brand,text,sentiment
Intel,Why does this need an article? It's a tweet by an official account praising their own product.,Negative
Intel,"The B580 has 200W TDP, in a perfect world and TDP scales linearly, the B770 would be 50% faster, that would put it around the 5060Ti/9060XT.  If the price also scales linearly, that would be around 375€, seeing that the 9060XT is going for 350€ now, it's gonna be tough competition.",Neutral
Intel,Im really looking forward to panther lake X. 4-4-4 core configuration and Xe3 iGPU with sr-iov is perfect for running a Linux-Windows mixed vm environment without having to get a gaming laptop with a dedicated GPU for virtualisation.,Positive
Intel,I hope the Linux driver support and performance is good in these,Positive
Intel,"Intel ARC needs to maintain their momentum. They have an excellent pricing strategy and genuinely compelling features, it's time they released a card that competes in the midrange. And no, I don't count the A770. As a B580 owner, increased ARC adoption rates will be sure to benefit all cards in the range, so I really hope that intel is committed for the long-haul here. They are not in the position to be burning consumers anymore",Positive
Intel,"Releasing a GPU more than 1 year after the B580 came out seems weird to me. Unless this is a new architecture, or is using Intel's own process, and fabs.",Neutral
Intel,"4070 performance for $350-400, I'm calling it now.",Neutral
Intel,Hopefully they've seen Nvidia and AMD fuck things up by having two VRAM configurations and know not to do that.,Negative
Intel,"300W? Sounds like they're chasing the big boys. Hope the performance justifies the power draw, leaks can be misleading.",Neutral
Intel,"Hello Revolutionary_Pain56! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,They wouldn’t need a B770 or mystery GPU if they actually released more than just a B50 to the masses.,Neutral
Intel,"I don't know what the driver situation is like a year later, but B580 was anywhere between a 4060ti and a 3060 (or less if the driver really choked), so comparing B770 to a single Nvidia point of reference probably isn't the whole story.  Intel has been selling a big chip with a lot of hardware relative to what they charge, so when the drivers work Battlemage can punch way above its price class. I expect the same this time.",Neutral
Intel,"It's been deleted, so it might even be inaccurate.",Negative
Intel,Ad revenue.,Neutral
Intel,Trying to apply logic or rules to the internet is a waste of time.,Neutral
Intel,"Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds. That however will mean a bigger die and viability might be questionable (considering they're already massive for the performance).",Negative
Intel,"Price doesn't scale linearly because die sizes make defects scale quadratically. so pricing is the same, 2 50mm\^2 dies are cheaper than 1 100mm\^2 die     However in GPUs there is a fixed cost for every GPU so there is a sweet spot",Neutral
Intel,"As always, TDP is a semi-arbitrary figure and has little to do with what the GPU requires.  Most GPU's of today have heavily inflated TDP's simply to try and juice benchmarks on review day as much as possible.",Neutral
Intel,"The BMG-G31 is supposed to have 32 Xe cores in 8 render slices on a 256-bit memory bus, compared to the 20 Xe cores and 5 render slices on a 192-bit memory bus for the BMG-G21. Unless Battlemage is seriously memory bandwidth-limited, it should be almost 50% more performant.  The only question is die size. If it's 50% larger than the 270 mm^2 BMG-G21, that would exceed 400 mm^2. The GB203 in the RTX 5080 is 378 mm^2 for context.",Neutral
Intel,"That ""perfect world"" of yours seems to violate basic physics though...",Negative
Intel,With tdp of 300 w it better be RTx 5070 or 9070 territory for much low price,Neutral
Intel,Intel never confirmed SR-IOV on Panther Lake - did they?,Neutral
Intel,"You can choose between high performance and crashes (xe) or low performance and stable (i915), and with Intel firing linux devs left and right I wouldn't expect much improvement any time soon.",Negative
Intel,That would be an amazing value proposition.,Positive
Intel,Rtx 5070 16gb for 380$,Neutral
Intel,I’d be happy if they didn’t gate the Arc Pro B60 behind bad distributors.,Negative
Intel,"So banking on the hope, that *everyone* ***else*** *somehow falls behind by accident*, only for Intel to succeed?  If that's their business-plan (looking at their foundry-woes, it seems it is), that's an awfully idiotic business-model.  ---- Last thing I heard, was redditors moaning about en masse that monopolies are bad. *Which one is it?!*",Negative
Intel,"The B50 is not a gaming GPU and actually underperforms in gaming tasks compared the the B580. They need to have an actual range of cards, not just a budget option, and even more budget option, and a server/workstation GPU. The B770 is essential to compete in the midrange",Neutral
Intel,"A year later the drivers are fantastic, seriously not even a single hiccup. Been playing Hogwarts legacy at 4k 60fps with Xess Quality upscaling, and no frame gen.",Negative
Intel,"> Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds.   The problem with this idea, is that this would cost them far more money, as you need more die space, which they already use relatively inefficiently compared to nVidia.  They can't really afford not to use every bit of die space they have for all that its worth.",Negative
Intel,"Battlemage doesn't have the ability to add more Xe cores per render slice, this is something Intel has changed for Xe3. The BMG-G31 will have 128 ROPs, the same as an RX 9070 XT, or more than an RTX 5080.",Neutral
Intel,"FWIW that is not how pricing structures work. It's a bit more complex than that.   E.g. defect rates scale with die size, that is true. But larger dies also have more budget for DFM structures, that can lead to fewer overall functional faults than smaller dies and more variability and binning opportunities. So although smaller dies tend to be on average cheaper, it is not necessarily that 2 dies half the size will be cheaper than the twice as large single die.",Neutral
Intel,"afaik it works on every iGPU since skylake, but the driver is not in the mainline kernel",Neutral
Intel,I'm using an Arc A770 right now in Linux.  With i915 performance was unusably (for me) low.  With xe it's been fine.,Neutral
Intel,"the driver is already open source right? i think it will get better over time on virtue of being open source, but relying on intel to fix it now probably isnt gonna pan out.",Neutral
Intel,"Well it kinda has to be, the 4070 came out nearly three years ago.",Neutral
Intel,5060 performance for twice the price isn't a good deal.,Negative
Intel,"I could see that. Nvidia really bailed out Intel by making the 5070 not much faster than the 4070 without using MFG to cheat lol   Edit: for all the Nvidiots downvoting, [the truth hurts](https://www.techspot.com/review/2960-nvidia-geforce-rtx-5070/#RT-1440p-png)",Neutral
Intel,This seems an absurd overreaction. All I'm saying is they don't do a 5060ti or 9060xt situation where there's a 8 gig model and a 16 gig model.,Negative
Intel,"It’s not but the B50 is the only Arc Pro that isn’t gated behind a bad vendor like Hydratech.    If they can’t properly launch the B60, why should I trust Intel or it’s partners with the B770 or some mystery GPU?",Negative
Intel,5060 is not nearly as performant as the 4070,Negative
Intel,"Interesting results. If this is representative for consumer laptops, Panther Lake is a much bigger upgrade than most here, including me, expected. But it almost seems too good to be true somehow.",Positive
Intel,is Geekbench a CPU or a GPU benchmark?,Neutral
Intel,"Hello LastChancellor! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,How does this compare to the Snapdragon X2 Elite?,Neutral
Intel,4 pcores  8ecores 4 lpcores..,Neutral
Intel,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",Negative
Intel,"Is Intel just ""squeezing the toothpaste"" again ? Even a low-frequency single-core 288V gets 2,700+ on Geekbench, while the 285H gets 2,600+ in single-core and 14,785 on multi-core. Therefore, TL;DR: I don't see Panther Lake being a huge improvement over the current Alder/Arrow Lake pairing. We will have to wait and see the power consumption, though.",Neutral
Intel,"I’m sorry, but that’s awful? Only 9% better single core when it has a better node and a newer architecture? Compared to what Apple and Qualcomm achieve every year, that’s pathetic",Negative
Intel,"Probably because GeekBench 6 only scales to a certain point, where more cores won’t help with improving performance compared to improving core IPC",Negative
Intel,"Fr, I really need to get a new light laptop (bc my old one's hinge is broken), but starting to feel  like I'd be better off waiting for Panther Lake than compromising with a bulky gaming laptop....",Negative
Intel,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",Negative
Intel,cpu,Neutral
Intel,"Probably one of the worst benchmarks out there for multicore tbh, I’d be more curious about the cb r24 scores",Negative
Intel,"Panther Lake doesn't bring any major changes to the cores. It's mainly about bringing node shrink, redesigned SoC, and new iGPU.",Neutral
Intel,"It’s obvious that this is the viewpoint of an outsider. Professionals would never look at it this way. Professionals first evaluate a processor based on its specifications, features, and process technology. Lunar Lake and Arrow Lake are *not* using some outdated process — they use TSMC’s then-most-advanced N3B node, Intel’s first time adopting it. Meanwhile, Panther Lake uses Intel’s own **18A** process.  Based on the current benchmark results, Intel’s 18A appears to outperform TSMC’s N3B by at least the same margin that **Intel 4** trailed behind N3B — which is an astonishing result.  Every day you hear people saying how much TSMC has advanced, how far ahead its processes are, how “outdated” Intel’s nodes are, how AMD’s processors using TSMC have excellent efficiency. These kinds of statements have been repeated endlessly over the past decade.  Yet today, Intel is using its newest process node to **clearly surpass** TSMC’s top process from just one year ago.",Neutral
Intel,"the real test will be how many watts the X9 388H needs to achieve its scores, because the 285HX needed like 90 watts to achieve its scores  so if the X9 could hit its scores while on its base TDP (65 watts) then thats a \~40% increase in efficiency, not bad",Neutral
Intel,"But it does that while clocked almost 6% lower, so the IPC gain is actually decent. Especially considering most people expected Panther Lake to be a side grade because of the small architectural changes on the cores.",Positive
Intel,"For the use cases of PTL, Geekbench (which is mostly consumer focused) is a good indicator.  It doesn't assume that its workloads are perfectly parallel, it assumes some threads are used more heavily than others, so its value in nT is influenced by its 1T.    If someone is using this for rendering or other highly parallelizable workloads they might want to look into a subtest or into an alternative benchmark, but for typical consumers it seems like Geekbench is a good approximation of their experience.",Positive
Intel,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",Negative
Intel,"Is macOS not an option? Because IMO, MacBook Air is *the* thin and light laptop to get, hands down.",Negative
Intel,"Couple of subtests leverage some new arm vector instructions and get huge scores, but those have limited influence to the overall score. Apple is better across the board, though the difference isn’t as big as the overall score suggests.   One difference is that since geekbench is distributed as binary it’s compiled more directly for apple architectures specifically while others use more generic targets. But that has very limited effect.",Neutral
Intel,But they have a gpu compute test too,Neutral
Intel,"Geekbench claims it's much more realistic than those multicore tests that scale nearly perfectly with tons of cores, and I think that's a fair take. It's not as if they didn't know how to create a benchmark that scales like other nT tests do, geekbench 5 nT does that.   I wouldn't call it worse, just different.",Negative
Intel,Geekbench runs common workloads as they are commonly implemented. It gives you a score on how well a multi core implementation of that workload would actually run in that CPU.   I think that is far more useful than some perfectly parallel workload measuring max power and core count.,Positive
Intel,"I have carefully compared the various models across Geekbench, PassMark, and the differences between Meteor Lake, Arrow Lake, and Lunar Lake. If my judgment is correct, the theoretical peak performance of the 484 in Cinebench R23 should reach around **24,500**; the 285H scores **22,500**. Compared with the 285H, it should be easier for the 484 to achieve high scores because its power requirements are significantly lower than the previous generation built on TSMC N3B.  Its peak performance will not be extremely strong because the frequency is not high. IPC is likely improved by around **10–11%**, but clock speeds drop by about **6%**. Overall, that means single-core performance should only rise by **4–5%**.  The improvement will be most noticeable in Geekbench. Since PassMark single-core also shows gains, the IPC uplift and resulting single-core increase should be quite certain. If Geekbench were the only source, it would still be questionable, but PassMark is more solid and has higher reference value.  Overall, in terms of peak performance, the uplift is average—around **10%**, close to that figure.  However, the real key is the **efficiency gains**. I believe they will be excellent. Compared with the 285H, which requires **65 W** to reach **20,000** points in Cinebench R23, I estimate that the **388H** may only need **40–45 W**.  I also estimate that the Cinebench R24 score should fall around **1300–1400**. Compared with Qualcomm’s X Elite 2 at **1950**, there is still a significant gap—but the two products differ drastically in scale.  Overall, Panther Lake’s greatest achievements lie in several aspects:  1. **Energy efficiency** — likely the best among all x86 products. 2. **Performance per mm²** — excellent. For example, the 484: if you look at its die shot, the total area of the CPU (including the CPU tile’s 4P and 4 LPE cores and all caches) is essentially equal to the die area of a traditional monolithic 8-core design. That means the 484 uses the same silicon resources as past 8-core chips, yet **no AMD mobile 8-core processor surpasses it**, either in raw performance or efficiency. 3. It also offers better performance-per-area than Qualcomm’s processors. The X Elite 2 has **18 cores**, including **12 “very large” cores**—similar in size to Intel P-cores—and **6 large cores**, each larger than Intel’s E-cores. The die area of this chip is **2.5× larger** than Panther Lake 484’s.",Neutral
Intel,"I mean, it does bring SOME changes to the cores, both are a next generation, it just isn't a more radical change like will be happening with NVL.  A mid single digit improvement is still pretty decent.",Positive
Intel,">The benchmark is very friendly to ARM and least favorable to AMD.   How so?   >The only valid reference is **same-generation, same-architecture comparisons**,  Geekbench is nice because it explicitly allows cross ISA comparisons. You don't have to take my word on it either, Intel and AMD themselves have used geekbench before to compare themselves to the ARM competition.   Same thing applies to spec and cinebench 2024.",Positive
Intel,My old 14900hx gets 35k multi core in cinebench r23,Neutral
Intel,"What's with the Cinebench fascination? At any rate. Geekbench 6 runs a raytracing test, and the 388H leak shows it at 29700 points compared to a 285H scoring 25300 points. That would place the Cinebench R23 scores at about 20% higher for the 388H  https://browser.geekbench.com/v6/cpu/15500755  https://browser.geekbench.com/v6/cpu/15474224  At any rate, the reason Geekbench doesn't scale perfectly with more threads is because a lot of workloads hit scaling limits due to Amdahl's Law, or memory bandwidth limitations. This applies to SPECint and SPECfp results for multiple threads as well.",Neutral
Intel,"lemme know once UTAU and Fighter Maker 2002 works on Arm macOS    (my point is that I work with a lot of old abandonware apps that barely even run on x86, so there's no chance in hell they gonna work on macOS)",Negative
Intel,"Thus useless to compare high CPU core counts.  If you actually need more than 8 cores you also have workloads that scale much better than Geekbench 6. It's especially dumb to claim this CPU is close to a 16 core, 32 thread zen 5 cpu based on Geekbench...",Neutral
Intel,I have workloads that scale fine with 16 threads and would scale fine with 32. People who actually buy high end multicore CPUs have a use for them.,Positive
Intel,It runs for far too short a time to reflect accurate multi-core performance.  People don't get a multitude of cores to run a task for a few seconds.  They do it for tasks that take minutes or hours to complete.  I'd argue it spends too little time on single-core tests as well.  I don't trust it to provide any useful information about anything other than transient performance.,Negative
Intel,I agree but the problem is it's being mindlessly used to compare MT scores as in this article.,Negative
Intel,Bro there's no need to spam this same comment like 4x in the same post's comment section T-T,Negative
Intel,"Why's the scaling ""problematic""? Its nT scaling is by design because GB6 is trying to replicate common consumer workloads which are rarely embarrassingly parallel. If you wanna see how well nT scaling for rendering is, there's cinebench for that.",Neutral
Intel,"was your 35,000 score achieved with power consumption above 100W? Can you try it now at 80W and see how many points are left? Also, limit it to 40W and check if it can reach 20,000 points. Because I estimate that the 388H has a chance to hit 20,000 points at 40W.",Neutral
Intel,"Ok, a simple no would have been fine.   Seems like those extremely old apps would run on any old POS x86 machine, if anything harder to run on modern hardware but hey what do I know. Best of luck.",Negative
Intel,"I mean, shortness is more of a problem if a device can cool itself properly or not rather than a problem of the CPU itself, unless said CPU in question is impossible to cool in that form factor",Neutral
Intel,"It provides useful information about the chip itself to real computer architecture enjoyers. Idk if gb6 changed it but geekbench has historically correlated with spec scores. Longer running programs like cinebench test the whole system including the thermal solution but geekbench gives a much better view into the pure performance of the cpu itself (and the associated memory system :/). Besides, you can always slap on a bigger cooler if thermals are that limiting.",Positive
Intel,Geekbench correlates with SPEC really well while taking a fraction of the time to run. Making it run for more minutes changes nothing,Negative
Intel,"It's being used for comparison because that's what we have. AFAIK, this is the *only* 388H benchmark we have",Negative
Intel,That looks like an AI post to me,Negative
Intel,It has been going hayway since SME just like GB5 had issues with AES Skewing results,Negative
Intel,Fair enough but I'd rather they kept something similar to GB5 multicore test in addition to their new 'more realistic' one.,Neutral
Intel,Incredible hardware news. Thanks for the share.,Positive
Intel,"This was sarcasm, by the way. A video from Usagi Electric on how computers count isn't hardware related but this is? OK mods.",Neutral
Intel,"Frankly, I wouldn't buy one for gaming, though I must admit Battlemage is pretty sweet for video editors thanks to 10-bit AV1 and 4:4:4 chroma on the HEVC side + you also get two codec engines (at least on the B580 with the same G21 core).  For perspective, you'll have to move up to Nvidia GB203 (RTX5070Ti), or better, to get your hands on two or more NVENC engines for the same 10-bit AV1 + 4:4:4 H.265.  If I was a serious video editor, this is *the* graphics card I would get.",Positive
Intel,The main bit that intrigues me about these ARC GPUs is their Linux gaming performance & how they compare to their windows performance.,Neutral
Intel,Not in the same system the 1080 ti was in.,Neutral
Intel,"One interesting data point is he's testing with 7500f. We have no comparison with contemporaries or higher end CPU to examine CPU bottleneck, but it's a realistic scenario and system for the card.  Interesting how that 1gig made all the difference in TLOU2",Positive
Intel,"idk why, intel gpu is so expensive in my country like bruh that gpu perform worse than cheaper nvidia/amd. those sucker trying to scam buyer just cause ""intel"" name in it.",Negative
Intel,Yeah intel's quick sync is very good at video editing and streaming as well. Even preferred over nvenc in streaming (no idea about video editing),Positive
Intel,"I would and I did (Intel B50 gpu).  So far, zero regrets and zero issues on linux.  Edit: Fedora for those that are curious.",Positive
Intel,Rumour has it Linus torvalds uses an Intel card because he wanted something on a budget that could drive dual 6k screens.,Neutral
Intel,"For desktop use Intel on Linux is great, but gaming performance and compatibility is horrifically bad.",Negative
Intel,"it's the retailers, they don't sell well and need higher margins",Neutral
Intel,Its not a rumor lol he did a video with Linus tech tips and specifically requested they put a b580 in the PC they built him  [link to video](https://youtu.be/mfv0V1SxbNA?si=jT_3dFy1H40vrVjk),Neutral
Intel,"Performance is a little worse than on windows, but compatibility is not horrifically bad. It's pretty much the same as on windows.",Negative
Intel,"I believe Linus Torvalds wanted an ARC Pro B50, but settled for the B580 because that's what LMG could get their hands on",Neutral
Intel,can you imagine a bunch of nerds whispering about which graphics card an old man uses?,Negative
Intel,"I'm curious as to how much worse. I've considered an upgrade to a B570 due to them being seen for £150 new, putting it into used RX 6600/6600 XT territory, but if the Linux performance of say a B580 on Linux falls closer to either or, then it's probably not a worthwhile choice over the used AMD options for me.",Negative
Intel,"TLDW:    GPU Models Tested: MSI Shadow 2X RTX 5050, Intel Arc B580 FE      16 games average:    1080P, High-Ultra Settings:     Native TAA: Arc B580 is 14% faster, 23% faster at 1% lows due to higher VRAM        DLSS 4 Quality vs XeSS Ultra Quality: Arc B580 is ~11% faster     DLSS 4 Quality XeSS Quality: Arc B580 is ~20% faster     DLSS 4 Balanced XeSS Balanced: Arc B580 is ~15% faster     DLSS 4 Performance vs XeSS Performance: Arc B580 is ~14% faster",Neutral
Intel,"""There was a time, about a decade ago when the $250 price tag offered solid products, but the world has changed""  Yep, inflation. $250 in 2015 money is $342 in todays money. And you can get a very solid product at that price tier, the RX 9060 XT is $369 on Newegg.  GPU prices haven't gone up, you money is just worth way less.",Neutral
Intel,"5050 really has no right to exist at the price it does. B580 is obviously being sold at near cost or even a loss however, it's not exactly a fair comparison but that doesn't matter to consumers.  If you just want to game then I can't see any reason to consider anything else at this price point.",Negative
Intel,I'd still probably go nvidia here as I don't trust intel's compatibility with older titles and the like.   Still it would probably be better to spend $20 more on a 9060 xt 8 gb or $50 more on a 5060 than either of these.,Neutral
Intel,"If UE5 games generally run this poor on Intel GPUs, there might be trouble ahead as there are lots of those games in the pipeline.  You still couldn't get ~~more~~ me to buy an Intel GPU, even if I was desperate for a cheap GPU right now. I'd just adjust my settings.",Negative
Intel,"The B580 is decent enough, but it might be better to just save a bit more and get a 16GB 9060 XT for $350 or something. That card is likely to last 10 years flat at this point, and it will definitely last at least 5.  And yes the 5050 is not good. Getting something with a half-decent iGPU would be a better use of your money at that point.",Negative
Intel,The biggest issue is that he did not test PCIE 3.0 vs 4.0 vs 5.0. Those GPUs are very likely to go into budget builds or as upgrades to older motherboards like the B450.,Negative
Intel,Imagine spending $250 on a GPU when you could literally just save $100 more for like a 100% percent more performance.,Neutral
Intel,Wait isn't xess a lower resolution per quality setting?,Neutral
Intel,"yeah there is a reason why there are 5050s for 210  the thing is a sub 200 dollar GPU, which matches it capability and vram well, its more or less the I want to step up from igpu deal",Neutral
Intel,Maybe for the low end but high end I can’t even buy a card at msrp outside of America.,Negative
Intel,"*ignores that this is a 50 tier product and should be compared with the 950 and 1050*  This kinda of ""but but inflation"" virtue signalling I'd very unhelpful to these kinds of discussions. It's as if you're saying people should stop complaining gpus are several times more expensive than they used to be with the actual low end market completely destroyed.",Negative
Intel,Tech is supposed to beat inflation. Look at monitors or TVs or SSDs (before now) or CPUs or ....,Neutral
Intel,All the tech tubers are just turning into old men shouting at clouds. They will probably all be replaced by younger people living in the now soon enough.,Negative
Intel,"First of all 250 euros bought way more gpu in 2015 than 360 does today. And the lower end and midrange gpus were much less cut down vs the high end chipa today.  A 5050 sits where the 750ti did when the 980ti was out. Now you get entry level performance for mid end prices  Have wages actually increased that much? Because that is the only useful measure of ""inflation"". Everything else is just corporate profits   If prices for everything go up but wages don't then that leaves less money for frivolous shit like ram and storage and laptops and consoles, not more.  Even in my country where our wages are automatically indexed to match inflation, our purchasing power has dropped because the actual cost of living isn't properly represented in whichever calculation is used for the inflation number.  Houses have gone up by 100+ percent since 2015, rents have gone up by over 60 percent, grocery prices have more than doubled, utility prices have risen sharply, public transport has more than tripled in cost.  Minor expenses like clothing or a tv you buy every ten years have stayed flat, but that isnt what people are spending 80 percent of their income on.",Neutral
Intel,"Shh, everyone knows that prices only go up on luxury goods due to evil corporations, after all how will people live without their computer not being 800% faster than last year?",Negative
Intel,"> Yep, inflation. $250 in 2015 money is $342 in todays money.   People really need to stop using CPI. I can bet you that GPUs don't make it to the market basket. Yes, your money's value has fallen but not by that much.",Negative
Intel,You're getting downvoted for speaking the truth.  The RTX 5050 should be a $150-180 GPU for the price and value it offers but unfortunately people are gonna defend the price tag that the card was set for by Nvidia,Negative
Intel,"For a while you could get them for $229, which would be more acceptable vs a 5060 for $299, making it the same FPS/$. But the 5060 is actually the one on sale right now for only $30-$35 more. 30% faster for like 12% more money.",Positive
Intel,"I'd say that the cheapest new GPU that I'd blanket recommend with no ifs, buts and caveats is the 9060XT 16GB, everything below that either struggles with outright performance, VRAM or software issues like Arc.",Neutral
Intel,"Intel checks all the right boxes on paper (generous VRAM, decent pricing compared to competitors, an alternative to the duopoly) but the recent CPU overhead stuff coupled with the crapshoot that is trying to play older games and it just isn't worth it",Negative
Intel,"Yeah they cover this at the start of the video https://youtu.be/lLe5AP6igjw?t=229   XeSS 1.3 shifts everything down a tier, so their quality scaling ratio is everyone elses balanced ratio.  Older versions of XeSS match DLSS/FSR scaling ratios.",Neutral
Intel,"I know quality is, not 100% sure about others. dlss quality preset uses higher resolution than XeSS and FSR quality presets",Neutral
Intel,Its fine for people who need a dGPU but not a beast for work. think stuff like CAD or Photoshop. It will also be fine for people who only play competitive multiplayer games.,Neutral
Intel,Nvidia cards are bellow MSRP here in eastern europe. AMD cards slightly above MSRP.,Neutral
Intel,"They are all selling below MSRP in the UK. £979 is MSRP for a 5080 and I can buy 3 in stock models for less than that price without much searching, at scan.co.uk.  If you are in South America its probably your countries insane import taxes, protecting their home grown GPU market lol.  29 upvotes from children who have not bothered to check or do any kind of reasoning.",Neutral
Intel,"> Tech is supposed to beat inflation.  And it does, wtf are you trying to claim?  $100 CPUs these days run circles around 6700K which was the flagship in 2015. A B580 is faster than a GTX 980 Ti, which was the flagship card of 2015.",Neutral
Intel,"It does. For the price of a 1993 CRT TV, you can get a flat-screen LED thrice the size and with 10 times the resolution.  SSDs? A 2 TB nvme is a fraction today than a 128 GB Sata one was a little over a decade ago.   What actually changed is inflation, and that the buying power of today's middle class person decreased significantly relative even to the 2000s.",Neutral
Intel,Gpus are way more expensive to produce,Negative
Intel,"> Tech is supposed to beat inflation.   It does, despite wafer prices increasing the last 10 years.",Neutral
Intel,Wrote a fucking who? The fact that TVs are cheaper in nominal terms than they were 15 years ago does not mean it has to be the same thing with every other tech product. TVs are not products manufactured necessarily on cutting-edge expensive nodes.,Negative
Intel,"It does. For the same amount of money, you get way better GPU(unless your braindead thinks the gtx970 has same performance of 9060xt)",Positive
Intel,"It does, but also, inflation has been extremely bad for 5 years.",Negative
Intel,"You are just making shit up at this point. NVIDIA GeForce GTX 760 (2013) release price: $250.   That was a shit card, arguably a worse product than the 9060 XT is today, when you compare it to contemporary rivals. How do i know it was shit? I had it.",Negative
Intel,TSMC inflation is FAR higher than CPI. You are half right,Neutral
Intel,"according to US bureau of labour staticstics that measures the CPI it includes  all personal computers (desktops, laptops, tablets) and related equipment (printers, monitors, smartwatches, smartphones). It does not look at GPUs specifically, but the effect of that will be visible.",Neutral
Intel,I doubt ppl are gonna defend the 5050 considering a 5060 or an 8GB 9060XT is not much more and a fair bit faster.,Neutral
Intel,"i mean, the 5050 off of amazon rn is 210, so it is getting there as a sub 200 dollar GPU for improving over iGPU right",Neutral
Intel,I pretty much but there's nothing else people *trust* in the category because apparently Arc cards are for professional nerds or something whereas I haven't had a real bad driver issue in over 2 years with my A770  People are also conditioned to fear older gen GPUs so 6xxx and 7xxx parts are sitting on shelves waiting for blowout discounts. People would still rather spend more on a basic nvidia from the 5000 series.,Negative
Intel,B580 user here. I coupled it with a R5 5500 and as of the recent updates the card just seemed to run much better vs when I got it last July.  There was a video before which also revealed that the CPU overhead is now being addressed in subsequent updates.  https://youtu.be/gfqGqj2bFj8?si=PyAfB2NhqZKWWVXY  I’d say it’s getting better and that I’d recommend it over a 5050 since the overhead is now fixed/negligible.,Positive
Intel,Intel is in their second GPU generation. Its going to take a lot longer to catch up with the institutional knowledge and practical application in videogames that the others were developing for over 20 years. The CPU overheard was not an issue in Intel iGPUs and Alchemist because GPUs never got fast enough to matter. It is only now that they noticed that issue since the GPU is far enough to create it.,Neutral
Intel,And that only in terms of native resolution and does not mean equal final image quality.,Neutral
Intel,\>dlss quality preset uses higher resolution than XeSS and FSR quality presets  Not exactly. FSR and DLSS are evenly matched in internals at all quality presets.,Neutral
Intel,I’m in Australia lol. The msrp for the 5090 is 1999 USD which translates to 3011 AUD The cheapest 5090 is 4800 AUD. That’s not even close at all to the msrp…,Neutral
Intel,"It sure is funny every time all those 6700k/8700k era CPU's pop up on used parts sites or FB Marketplace and still expecting close to initial prices.  Who even buys them anymore? At least a Q6600 has retro value ,but those are just obsolete.",Negative
Intel,"Not if you consider how the workloads being run on them have also changed. A GTX 970 ($450 inflation adjusted) would have run 2015 games better than how a RTX 5060 Ti ($429) or 5070 ($550) run 2025 games.  In other words, demand for performance has outstripped performance improvements, and those improvements are not felt as much.",Neutral
Intel,"A 100$ CPU in 2015 would easily run 2015 made software. A 100$ CPU in 2025 would barely run the electron JS slop. This includes Windows.  In 2015 my run-of-the-mill laptop (cost 300-400 bucks at that time USD equivalent) instantly opened the control panel when I clicked or file explorer even with a old HDD. Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  Yes yes yes it is very good on benchmarks but I don't stare at benchmarks all day. I use my computer for things you do at computer. Don't force my CPU to crunch how much digits of Pi it can compute.  A 980 Ti can easily run top 2015 games. Now? My laptop barely runs modern AAA games without looking a blurry mess. I simply can't fucking understand how you people look at the glorified motion blur and call ""yup it is the pinnacle of computer graphics"". How the fuck majority of modern AAA games look any better than RDR2 can anyone fucking tell me?",Neutral
Intel,"Why aren't you comparing relative buying power of 2014/2015 vs now then?  $650 got you what in 2014, a GTX 980TI?  $330 got you what in 2014? How close to the top end are both these things?    That's $900/$455 today, thereabouts.  What does $900 get you today?  Does that buy you anywhere near the top end?  And how does that product compare in relation to others above and below it?  Because the $330 product in question ($455 today) got you about ~75% to top end performance for ~half~ MSRP of the 980TI.  How does a $455 product of today square up relative to the top end?   Why don't we throw in a GTX 980TI vs a GTX 280 comparison while we're at it.  Make things really interesting.  I'll let you fill in those blanks (along with the $330 card in question) hoping you actually learn something in the process here.  The bar is very low, try not to trip.    The underlying point that user was making was pretty obvious if you read the comment they responded to.",Neutral
Intel,"cmon man, give him some slack, he just made shit up cause it's convenience for his argument.",Negative
Intel,How is it comparable? The 9060 XT is a very good card for 250. Ideally it'd be around 200 or below but for 250 you get a card that's a bit overkill for even 1080 P gaming.,Positive
Intel,In quite a few countries the 9060 XT is at or below the RTX 5050s MSRP.,Neutral
Intel,Then why did the comment above mine get multiple downvotes? It's Reddit and that's how it goes unfortunately,Negative
Intel,6x and 7x are priced far too high for old stock and are poor value compared to nvidias 50 series. They really haven't had a good price/performance low-mid end card since the 6700XT which are extinct at retail.,Negative
Intel,"I think they must have to go into every game, and adjust that t fix it, because it's not a universal fix it seems. Maybe per-game optimizations .",Negative
Intel,yep. XeSS 1.3 is closer to DLSS 3 rather than DLSS 4 in terms of image quality. Its good enough to game on in my opinion.,Positive
Intel,"> Who even buys them anymore?  The best SKUs on sockets have always demanded a premium in the used market. Since that's where people upgrading old machines will go.  And many machines from OEMs are not readily upgradable with just new boards/CPUs combos. Since they use custom form factors etc. So it's either a in socket upgrade or replace the whole machine. The socket 6700k is on is also especially affected by the ""premium"" factor. Since there's no lower end SKU with 4C/8T. You either get the 6700/7700 variants or are stuck with lower thread count.",Negative
Intel,"> would have run 2015 games better than how a RTX 5060 Ti ($429) or 5070 ($550) run 2025 games.  I think your memory is impacted by the expectations at the time. And the problem of reviews often using older titles inflating numbers, ffs some are still benching with GTA V to this day.   The [970](https://tpucdn.com/review/nvidia-geforce-gtx-1060/images/witcher3_1920_1080.png) couldn't even get 60 fps in witcher 3. Which was released in 2015.  And the performance it got in Witcher 3. Was not much better than what the 5060 Ti got in [Black Myth Wukon](https://tpucdn.com/review/msi-geforce-rtx-5060-ti-gaming-16-gb/images/black-myth-wukong-1920-1080.png)  Which even including 2025 titles. Is one of the hardest/heaviest titles with the worst performance. You can expect much better performance in almost every title. Just like the 970 was doing better than it did in Witchers 3.   But to argue that we got a lot better performance back then in the games releasing at the time, that is just false.",Negative
Intel,Yes 3.5GB of memory in 2015 was soooo much better than 12GB today /s,Positive
Intel,"All the GPU makers are betting on you using DLSS/FSR/XeSS as part of your usage to play games. Maybe even frame generation along with Relex, and all the other tech they ship GPUs with. They used to only rely on you using regular AA techniques.   If you ignore all those options you have today, and pay like it's 2015, it might be worse a lot of the time. If you use those options, you're generally way ahead of where a GTX 970 would fall. So it depends if you're willing to adopt new rendering tech, or rejecting it.",Neutral
Intel,"No, not really, in 2015, you happily accepted 45 FPS on not the highest settings at 1080p",Neutral
Intel,"This don't sounds like CPU problems at all, more like either Win11 is a vibe coded pile of bugs, or ACPI problems.",Negative
Intel,"> Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  Something is wrong. I keep reading people's experiences of stuff like this and I haven't experienced it, I'm not doubting it but I'm so curious as to what is wrong.  In particular I read a lot of people saying Windows Explorer takes forever to open etc",Negative
Intel,"> In 2015 my run-of-the-mill laptop (cost 300-400 bucks at that time USD equivalent) instantly opened the control panel when I clicked or file explorer even with a old HDD. Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  That's Windows for you.",Negative
Intel,it does not matter how close to the top GPU is. its a completely useless comparison.,Negative
Intel,I'm not sure what you're saying. The 9060xt is $250 only in 2013 money. They are arguing it's better value than a GTX 760.,Neutral
Intel,"Ppl up/downvote kinda randomly, doesn't really mean much post can go from +/-20 to the opposite real quick sometimes.  Anyways It's at +8 currently was at +something(2 maybe?) when i commented so who cares.",Neutral
Intel,"Nvidia and AMD do a lot of per-game optimization in the driver as well. In some cases very brutally, for example Nvidia is known for grabbing all games DX12 drawcalls and rearranging them in driver because the way game handles it is inefficient.",Negative
Intel,I remember upgrading to a 970 in 2016 and still being unable to max Witcher 3 at 1080p60 but got close enough,Neutral
Intel,"funnily enough, GTA 5 Enhanced Edition can be quite a benchmark for ray tracing nowadays. But it took to this year for it to be released. I think we can consider it a testbed for whats going to be implemented in GTA 6.",Neutral
Intel,"Bringing up 1080p no RT Wukong benchmarks sort of makes the point for me: the only way these cards look comparable is if we pretend features and standards are the exact same they were a decade ago.  High res and high refresh monitors were a luxury in 2015, now they are a dime a dozen. RT was not a thing in 2015, now it is and Nvidia marketing really wants you to use it. It's like you're comparing Witcher 3 on Ultra settings to Wukong on Medium or High settings, and acting like it's apples to apples.  The moment you take modern displays and features (including DLSS to be fair) into account, it paints a picture where technology has moved on, developers and players would love to move on, and GPUs are struggling to make that jump.",Neutral
Intel,"if you run out of memory today the game swaps textures and continues running, it just looks uglier.   If you run out of memory in 2015 it starts using the superslow 0.5 GB and everything breaks.",Neutral
Intel,Nvidia is certainly expecting DLSS+FG to be the typical use case. The vast majority of their benchmark and marketing material is with those two.,Neutral
Intel,"I've got nothing against DLSS, I use it whenever I can, but sometimes it's just not enough to bridge that gap.  Another user brought up Witcher 3 and Wukong as an example of a graphically advanced 2015 game vs a graphically advanced 2025 game. The 970 would get 50+ fps on Ultra settings Witcher 3. Max out Wukong on a 5060 Ti and no amount of DLSS will make that card stop crying and screaming.",Neutral
Intel,"Like, Debian has no issues running on a n150 with multiple docker containers without instantly spiking the cpu to 100%.",Neutral
Intel,"I found a way to sort of kinda make file explorer slow. But its really a perfect storm thing. Have multiple screens, one of which is running in HDR and another in SDR. Have the file explorer tree open. Have a HDD, slower the better.  When you browse folders it refreshes the tree. When it refreshes the tree it asks connected devices if they are online, including the HDDs. Now move the window back and forth between your screens. When the explorer moves into HDR screen, it gets redrawn. Same when it moves to SDR screen. I suspect but cannot confirm there is a bug where the old instance is not cleaned correctly. So now when you browse it asks all devices if they are online 10 times. 100 times. At some point youll start noticing actual delays in opening folders.  Works even better if you havent restarted for a month.",Neutral
Intel,"the opening notepad thing, if you use taskbar it has a bad habit of not actually opening notepad until it finishes the online search for apps called notepad or whatever you typed. Disabling online search in start makes it fly really fast.",Negative
Intel,The 9060 XT 8GB is currently retailing for 250$ in many areas. I'm saying that the 760 isn't an ARGUABLY worse product. It is a worse product for it's time straight up.,Negative
Intel,"Fair point, but isn't lower and competitive prices good for us?",Neutral
Intel,">  and acting like it's apples to apples.  Apples to apples would be comparing W3 performance for both cards.  Wukong even without RT is a CONSIDERABLY more advanced game graphically than original W3.   >High res and high refresh monitors were a luxury in 2015, now they are a dime a dozen.   And? Better monitors showing up doesn't change the laws of physics and basic economics. It doesn't make scaling with die shrinks suddenly increase. With your logic the 970 was a terrible deal vs some early/mid 2000s GPUs that delivered better FSP at 1024x800 in games released at the time.   And before you start harping on about die sizes. The die in the 5060 Ti is actually more expensive than the die used on the 970. Wafer price increases more than compensates for the size difference.",Neutral
Intel,"Looked it up. Wither 3 got 52 FPS at Ultra settings, no Nvidia Hairworks turned on, for a GTX 970. Wukong gets 42 FPS at the cinematic preset native resolution, which is actually intended for cinematics, but developers allow people to enable anyways. As Digital Foundry has said, they maybe shouldn't.  Gets over 70 FPS if you turn the preset down 1 notch to high. No upscaling, or frame generation, or hardware RT, which is like what Nvidia Hairworks was for Witcher 3. It's really not hard to get Wukong to run at 90 FPS on a 5060ti with some minor tweaks.",Neutral
Intel,According to TPU review maxed out Wukong with DLSS got 42.3 fps. Not exactly the 50 fps you remmeberr for witcher but close. Heres a link to the review: https://www.techpowerup.com/review/black-myth-wukong-fps-performance-benchmark/5.html,Neutral
Intel,"That's very interesting thank you, I can definitely see why I haven't experienced it.  You genuinely wonder how Microsoft are testing these days.",Positive
Intel,"> With your logic the 970 was a terrible deal vs some early/mid 2000s GPUs that delivered better FSP at 1024x800 in games released at the time.  Except in practice from 2005 to 2015 you got considerably more advanced graphics *and* higher resolutions *and* generally higher framerates too. Now either you pay up or you gotta pick one.  As for the rest of your post, it's more of a digression. All I said is GPUs are failing to keep up with the rest of hardware and software, not that there are no valid reasons for it.",Negative
Intel,"> As Digital Foundry has said, they maybe shouldn't.  hard disagree. As someone who does not have a lot of time for videogames and often end up playing older games with newer cards, those beyond high settings are great as it allows me to make use of my newer card and make the old game look better.",Neutral
Intel,"I disagree with comparing RT to Hairworks, when the visual impact as well as the emphasis put on it by Nvidia is so much bigger. I also disagree with using 1080p as a reference for Wukong, when high res and high refresh rate monitors are as cheap and plentiful as 1080p was back then.  Imagine you went back to 2015 and told the GTX 970 guy he's supposed to play his games at 2005 resolution and turn off antialiasing, how do you think he'd react?",Negative
Intel,">All I said is GPUs are failing to keep up with the rest of hardware and software, not that there are no valid reasons for it.  Why complain about something that there are valid reasons for lol",Negative
Intel,"It just makes such a small difference in UE5, it's really not worth losing 30% performance over for this engine. They would agree with you for a lot of other games, and Avatar Pandora kind of has a hidden setting, they'll maybe make available in menu at some point. Right now you need to modify a config file to enable it. Maybe they just need to wait until the final patch of a game to show those settings, years after launch, or just name them ""next gen"" or ""experimental"" with the setting below called ""ultra"".",Negative
Intel,"You can use DLSS and frame generation to play at higher resolutions. That's their intent. Especially UE5 games, because TSR was developed by Epic for a reason. The games on UE5 are really never intended to be run at a native resolution. I don't tell people to run UE5 at a 2013 resolution, but I also don't tell them to have the 2013 mindset that everything has to be run at native resolution, and that's the only way to play it. 1440p Balanced DLSS should give you around 50-60 fps without frame generation.",Neutral
Intel,"The performance loss does not mater for future (re)plays.  The config settings are usually hidden because during testing there were instabilities found that they didnt think was worth fixing. There were some games that had settings beyond ultra with names like ""Extreme,"" ""Nightmare,"" or ""Insane"".",Negative
Intel,"3050 performance at half the power, can't argue with that.  I do wonder how long it will be before the low-end dGPU disappears entirely from laptops, suspect these new chips probably aint it due to price but it's presumably coming.",Neutral
Intel,"While Geekbench's OpenCL isnt super popular for GPU testing;  I still cant believe a mainstream integrated GPU is actually competing with laptop 3050Ti, and these results still aren't with final optimizations!  by the time Panther Lake comes out, Intel claiming that its as fast as laptop RTX 4050 might actually be true lol",Negative
Intel,Are these better than Ryzen igpu? How do they compare to RTX 4050 AND 5050 in non demanding game?,Neutral
Intel,"I'm excited sure but if the claims are true how can anyone here expected these to be reasonably priced and sell well? If it competes with a laptop dedicated GPU it will be priced 2-3 times that laptop. The 285H isn't even that popular and its expensive, hope the 385H is affordable AND the claims are true.",Neutral
Intel,"The score is slightly above a GTX 1650 Super which in turn is slightly above a GTX 1060 which is barely matched by current most performing 128 bits iGPU (Lunar Lake / Strix Point)  Even if we expect 50% improvement compared to 890M, that's still half the performance level of a full power mobile RTX 3060. (roughly half a PS5 too)",Neutral
Intel,This would still no where be close to M4/M5 in single core and GPU,Neutral
Intel,I sincerely hope Samsung will work on their camera and make it better in this version.,Positive
Intel,"Hello 6950! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"The problem with buying a device relying on Intel graphics drivers, is that you're getting a device relying on Intel graphics drivers.  I'd rather not.",Negative
Intel,"iGPUs have already killed off the MX series, suppose it’s a real possibility other low end dGPUs also get killed off",Negative
Intel,"More likely what is considered a low end dGPU will shift, just as a current low end dGPU would have been an high end model a decade ago.",Neutral
Intel,"iGPUs is why low end desktop parts dont exist anymore, its going to do the same for mobile parts.",Neutral
Intel,"It'll happen eventually but they have a few issues.  First is memory bandwidth, they'd need to put a wider bus on it to address which is costly.  Second is the memory itself, they need something with higher bandwidth (but that might be addressed with LPDDR6).  To address the bandwidth issue they often need to put on extra cache, the 12Xe3 core is basically maxing out the bandwidth of the 'standard' chip, and if they went to say 20Xe3 cores (which would be 4060+ level) then they would need a bigger memory bus and more cache (Strix Halo put 20mb of MALL cache to try and address, and a 256 bit bus) and that is more design work.  They'd also have an issue with the socket - the APU would be too big to fit into a standard socket and would need a custom one which means a custom motherboard which increases costs.  Another is heat - while more efficient overall, all the heat is in one spot so the computer needs to be thoughtful about moving that out.  A think that in 2028 big APU solutions can compete well against XX50 series and most XX60 cards.  It isn't happening before then however.",Negative
Intel,Apple's M5 already beats the 3050Ti though.,Neutral
Intel,I mean the early Iris pro onboard GPU’s traded blows with gtx 650m at a lower power draw. It’s been done before. Still nice to see but nothing ground breaking,Neutral
Intel,We will have to wait for the actual laptops to release & see how they do in real games; performance in synthetic benchmarks does not always line up in real tests.,Neutral
Intel,PTL should have comparable or possibly even lower production costs than ARL. There's some room for optimism.,Negative
Intel,this GPU is also coming into the 355H btw (more specifically there's a Ultra X7 version of the 355H with the full 12-core GPU)     and iirc last time the 255H was pretty popular,Neutral
Intel,"Way we should think about it in my mind is that it's got 12Xe3 cores, and 1 Xe2 core = 2 RDNA 3.5 cores.  Strix Halo has 40 RDNA 3.5 cores (though with a bigger bus and enhanced cache) which is kind of like 20 Xe2 cores (maybe a bit more), and Strix Halo basically ties with a 4060.  If you assume a 5-10% improvement from the architecture going Xe2 to Xe3 to offset the bus and cache (which isn't *as* limiting with only 60% of the hardware), then you're probably at about 60% of a 4060 power, which is more or less where this looks to fall.  4060 mobile passmark = \~17,400  4050 mobile passmark = \~14,300 (81% 4060)  60% 4060 mobile passmark = \~10,500  3050 mobile passmark = \~10,100  That puts it pretty much at a 3050.  With a little sprinkle of cutting edge upscaling and a bit of frame gen it'll be just fine for light titles, but won't be a dedicated monster.  It's just a good solid chip for light-weight long-battery general purpose use that can play some titles on the side (or play older titles with confidence).  I suspect it'll review pretty well, since the reviewers tend to be excessively games-focused and this will appeal, but it's not a games machine - it's a surprisingly zippy long-life chip that's a good fit for all-day laptops.  EDIT: 8060S (Strix Halo) is \~18,000 for reference.  EDIT 2: 890M (strix point's top end) about 8,100, so the PTL 12Xe chip will smoke that.  I'm excited about it, it looks like a great chip!",Neutral
Intel,"I swear, the only issue i have with the book 5 is the camera, this shit doesn't deserve the book 5 hardware",Negative
Intel,"It is not 2023 anymore. Intel Arc drivers work quite well, and Lunar Lake graphics are already pretty solid in the low power category. No need to keep repeating this nonsense when you haven't even used the devices.",Neutral
Intel,"Only sticking point seems to be cost I suppose, implementing a truly competitive iGPU looks like it will be expensive if Strix Halo is anything to go by.  I think the Intel/Nvidia partnership might at least come close to making it a reality to have a 50/60 tier integrated GPU in a laptop that isn't much more expensive than one with a dGPU currently... in theory it could/should be cheaper.",Neutral
Intel,Not really? Even optimistically speaking this won't be close to a 5050 laptop when accounting for bandwidth limitations in real world tasks that use the cpu and gpu at the same time.,Negative
Intel,"It's a good point. There really isn't any truly low end dGPU options out there now as far as I know, Nvidia abandoned the GT line and someone up there mentioned MX is dead etc.",Negative
Intel,I’m assuming the caveat to posts like this is “running a version of windows/linux”,Neutral
Intel,"and the 780M when fed enough power also comes within range, and the 890M also beats it.",Neutral
Intel,"It could beat a 5090, it would still be useless until the bootloader is open.",Negative
Intel,Wasn't the previous Intel igpu really good for games and efficient?,Positive
Intel,"Even if what you said was true, I know enough about Intel to know that even if it worked great, they'd find a way to fuck it up eventually. That's what they do.",Negative
Intel,">if Strix Halo is anything to go by.  I think Strix Halo skews how expensive a competitive iGPU could be. Strix Halo was very low volume and was also, imo, overbuilt. I think it's definitely possible to have a more cost competitive big-iGPU SoC",Negative
Intel,Prices for Strix Halo are rumored to fall in the new year; I'm curious to see if it ever reaches 1000-1500 dollar laptops like I've seen suggested it could. Seems like a ridiculous idea right now.,Negative
Intel,And in gaming.,Neutral
Intel,I mean...technically Asahi Linux exists for Macs. Though not the M5,Neutral
Intel,Then there's the 8050S & 8060S,Neutral
Intel,"How so? It is not even beating, at best barely even with 3050 35W (non-Ti).",Negative
Intel,I'm sure thats the first thing that comes to mind when people purchase a thin and light.,Neutral
Intel,"The Arc 140V like in Lunar Lake 268V? it's about 3050 Mobile (35W) sort of performance in real tests. There's a 140T in some chips too, I think it's less powerful than the V.",Neutral
Intel,Strix point is the better comparison,Neutral
Intel,"I hope so. I was a little surprised by the price but as you say, they clearly have a market for that thing and it isn't particularly price sensitive.",Neutral
Intel,they are not really comparable to the traditional APUs,Neutral
Intel,These are 256bit bus devices and have even fatter GPUs .....,Neutral
Intel,"Wait, is the 780/880m close to the 3050? I had heard that it was close to the 1050 at some point... As an 780m owner, it sounds like I can suddenly play way more games than I imagined?",Neutral
Intel,yes that one,Neutral
Intel,"Right, but Strix Point suffers from the same problems that PTL-X will: 128b bus chocking the memory bandwidth.  If iGPUs want to truly replace entry level dGPU, there needs to be wider bus variants",Negative
Intel,"Might seem so, but mobile 3050 is way slower than let's say 1660 laptop.  Still, you can play many games, just at lower res and settings. You can look up benchmarks/tests for handhelds with this gpu (or ryzen z1/z1 extreme)",Neutral
Intel,"No doubt , I meant in terms of availability",Neutral
Intel,Why can't they have a low core count CPU but still keep the full iGPU?,Neutral
Intel,"While this has pretty much been known for a while, you really do get the feeling when looking at these frequency and power stats that Intel hoped for more from 18A.  It's a good node and a good chipset, I think it's targeted and designed well and will do well in the market, but it doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.",Positive
Intel,"6 different top models with just 100mhz between them is so stupid, that should be 1 or MAYBE 2 models for that core configuration  But just hope the 356H is cheap then",Negative
Intel,Is this desktop or laptop?,Neutral
Intel,">16C (4P + 8E + 4LP)  I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  Two clusters make sense. What am I supposed to do with three?!  Unless I'm mistaken, AMD seems to be sticking with full fat Zen 5 cores in both Strix Halo and ""Fire Range,"" and frankly, it sounds like Intel is just trying to keep up in the core count race with its plethora of 'baby' cores.  Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.   I use my laptop for convenience and my desktop for compute heavy work.",Neutral
Intel,"16 cores, but only 4 of them will be fast. The rest will be slow cores.",Neutral
Intel,"Hello Geddagod! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"Nice. My a770 still slaying 4k ultra gaming in 2025. No stutters no freezing just responsive beautiful gaming. Ofcourse it’s paired with 14900k so my igpu technically has identical specs but I keep temps down this way. Also have an RTX 50 series. I love arc graphics. Bang for buck AT HIGHER RESOLUTIONS — that’s where it’s at. And I’ve had 4090 (sold), amd 9070(sold). Never would I pay what they are asking for a 5090. Just because I wasn’t 100 percent thrilled with price per FPS with the 4090. Once you have had all of them you will see Intel is the way to go with 4k gaming(unless you like paying thousands of dollars). Arc doesn’t have shadows and artifacts . RTX does. Bad. Blurs my picture even on best settings possible.",Positive
Intel,Upselling,Neutral
Intel,IMO it's probably not economical for them to do it.   Gaming laptops/handhelds are already niche enough as it is -- doesn't make sense for them to package both and for OEMs to carry both for just for a ~$1-200 delta between the units. Those who want the full GPU will just pay for it.,Negative
Intel,"You gotta remember these CPU’s don’t have multi-threading. AMD gets 16 threads out of their 8 core z1 and z2 extreme but with these core ultra’s you get one core and that’s it. A full on physical core is better than a virtual core, but the same amount of cores with multi-threading is much better than just a same core-count CPU without it. Intel are offering up to 16 cores which is 16 cores/16 threads, but generally should offer better multi-core performance vs an 8-core 16-thread. If anything it’s nice Intel have the ability to improve core count offerings without significantly raising the power requirements.   I for one veered away from the MSI Claw 8 AI+ as a standalone PC solution because it lacked the multi-core performance I needed with it only having 8 physical cores and no multi-threading. A 16-core offering is exactly what I need.",Neutral
Intel,Because nobody would buy it.,Neutral
Intel,"yeah, sucks cause they said handhelds was a priority for them",Negative
Intel,"Not great, not terrible. In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.",Negative
Intel,According to jaykihn0 it's a heat issue: BSPD and transistor density is flattening the VF curve on the high-end. The ST power curve is substantially improved at lower frequencies over Lunar Lake and Arrow Lake.,Neutral
Intel,"If anything, seems strictly worse than N3, even compared to older iterations. That's not great for a node that was supposed to go toe to toe with N2. Especially if the production cost is more in line with the latter.",Negative
Intel,"18a is initially launching with its lower power variant, which is why the only nova cpus that will use it for the compute tile are the laptop ones. The higher power variant comes later",Neutral
Intel,"Jaykihn, whose tweets these leaks are based on, says that the design characteristics cannot be extrapolated to the node itself.   So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.",Neutral
Intel,"> […] It doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.  Well, we already had rumors of Intel struggling to hit actual intended frequency-goals, no?  Though especially the actual performance-metrics of the node will be quite sobering I think.  The power-draw of these SKUs is supposed to be a TDP of *officially* 25W – In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4) and 65/80W (4+4+4 and 4+8+4) respectively.  So it must be seen, if these parts are any more efficient in daily usage, or just a side-grade to Lunar Lake.",Negative
Intel,"Panther Lake is pretty much a pipe cleaner, so it should still improve a bit.  Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD. If that is the case, it will only really be allowed to shine in high powered desktop and server chips, while power limited chips won't be able to reap as many of the benefits of 18A, since laptops tend to target the efficiency sweet spot, rather than clocks.  My understanding is that 18A will scale much better with extra power than N3 and N2. Once you pass the sweet spot, 10% extra performance on N2 might take 30% extra power, while 18A should continue to scale much more linearly for a good while longer.  So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones, especially in use cases where SRAM density is less of a concern. AFAIK Intel CPUs aren't as cache-dependent as AMD, so SRAM density might not be the highest priority, whereas slugging it out with AMD and trying to beat them on frequency might?  From what I've heard, Intel is still just about matching AMD in terms of IPC, while Arrow lake is very efficient, just not great for gaming. If Intel wants the CPU crown back, simply pushing more power could go a long way in clawing back the deficit, assuming 18A does indeed scale well with more power. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.",Neutral
Intel,> 6 different top models with just 100mhz between them is so stupid  They've always done that because of the OEMs who like a million segments.,Negative
Intel,Laptop.,Neutral
Intel,Since when is any user determining what cores they want to use and when?,Neutral
Intel,It makes sense to have three as the low-power cores prioritize keeping the power consumption low for battery life but to achieve this they aren't attached to the ring bus which hurts performance so 8 regular E cores are also used,Neutral
Intel,">I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  From a user's perspective, CPU's are a blackbox and the inner-working details are irrelevant. What matters is results. They want, typically, a balance of performance, battery life, heat / fan noise, and cost, placing greater emphasis on one of these categories over the others.  The idea is that P cores = performance optizied  E cores = Area optimized  LP-E Cores = power optimized.  The theory being to have different core types focus on different parts of PPA.  E cores and LP-E cores are the same microarchitecture. Just LP-E cores are off-ring so the ring (and rest of the cores) can be powered down in light-load scenarios.  Intel's biggest problem is that looking at each of the cores, the P cores perform their designated role the worst. 200% larger than E cores for \~10% more IPC and \~15% more clockspeed is a pretty rough trade off - and by the time you've loaded all P cores and need to load E cores too, that clockspeed difference diminishes hard.  Intel's better off (and all rumors point this way) to growing the E cores slightly and making that architecture the basis of the new P and E cores (like Zen vs Zen-C)",Neutral
Intel,"> Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.  You are the niche use case. The average/median user is using H-class laptops.   These are meant to beat Strix Point and its refresh, which it will do and at better economics for Intel than Arrow Lake H.",Neutral
Intel,"And with all the issues Windows already has with the scheduler, I fully expect it to never be able to properly utilize the different cores. I expect a similar situation as with previous versions, where it's sometimes necessary to disable the slow cores to eliminate stuttering in games, because they sometimes offload tasks to them which slows the whole game down to a crawl.",Negative
Intel,"> I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  > Two clusters make sense. What am I supposed to do with three?!  Ain't these Low-Power Efficiency-Cores aren't even usable by the user anyway in the first place (and only through and for Windows' scheduler to maintain)? As I understand it, LPE-cores are essentially placebo-cores for marketing.",Negative
Intel,Darkmont should really not be that far off Cougar Cove. Certainly not *slow*.,Negative
Intel,The E-Cores are plenty fast for most use-cases by now and there are 8 of them in there. Aren‘t they even ahead of the skylake IPC by now? 8 will be doing a great deal with a good clock frequency.  Only the 4 LP-Cores are kind of weak. But they aren‘t attached to the ring-bus and really only used for offloading low req Backgroundtasks and an idling machine. So that you can turn off the faster cores when they aren‘t necessary.  But for performant gaming and productivity it is basically a 12 core machine.,Positive
Intel,I hope there's a 4+0+4+12 for handhelds but they might not consider adding a whole new tile configuration packaging line worth it over just giving the OEM a discount.,Negative
Intel,I find that weird. Who buys a 20 core CPU for GPU performance equal to like an RX 580?,Negative
Intel,"Yeah as of now only MSI looks committed to using Intel for their handheld, and not only are they not that successful but they had also branched out to AMD for another line of their gaming handheld.",Neutral
Intel,Gaming handhelds are niche but not gaming laptops (relative to gaming desktops). Steam hardware survey reveals a large number of mobile gpus very high up in the charts.,Neutral
Intel,Both Apple's M series and LNL use decent iGPUs for their 4+4 parts.,Positive
Intel,"This is a minority use case, but is a very valid one. I run a home server (/r/homelab) and could use some extra GPU power for video encoding and some basic AI stuff. Though this is just a home server so I don’t want to put in a dedicated GPU due to the desire to not spend all of my discretionary income on my power bill. Right now I have a 9th gen i7 that is good enough for the basics of what I am doing, but I’ve been keeping my eye out for a replacement. I’m not alone in that need, but I do get this is a minority group. I’m sure there are other use cases though. Saying no one would buy it is wrong.",Neutral
Intel,"IIRC Arrow Lake's compute tile is fabbed on N3B, which is pretty much TSMCs earliest N3 node for mass fabrication.  Its other tiles are on a mix of mature 5nm- and 7nm-class nodes.",Neutral
Intel,"> In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.  Arrow Lake was the pipe cleaner for 20A which in turn was deemed unnecessary to move into production because everything was going so well that they could use that groundwork to get a head start and move straight to 18A. That's right from Intel's mouth. Now the goalposts shift once again.",Neutral
Intel,"> Not great, not terrible.  For what it's worth, I'm glad and somewhat relieved, that the never-ending charade of 18A (and ultimate sh!t-show it eventually again amounted to, after their blatant Vapor-ware 20A-stunt) finally is about to come to some lousy end, after years of constant shady re-schedules, even more bi-weekly Intel-lies and basically +2 Years of delays again.  So it will be a bit of … 'consolation' I guess, not having to constantly read that 18A-sh!t in every darn Intel-news.",Negative
Intel,"I'm curious to see the core's power curve itself. Or the core+ring, I forget what exactly Intel measures here for their software power counters.   Intel claims PTL has outright lower SoC power than LNL and ARL, so if the power savings are coming from better uncore design and not the node gains themselves....",Neutral
Intel,"> BSPD and transistor density is flattening the VF curve on the high-end   He doesn't make this claim. BSPD in particular was supposed to help the most at high voltage, and the density does not appear to meaningfully differ from N3, which it still regresses relative to.    Best case scenario, the node, for other reasons, can't hit as high voltages as N3, and the top of the curve is just capped. But you wouldn't expect thermals to be such an issue then.    Doesn't seem to be any evidence of the core-level VF curve benefiting from the node either.",Neutral
Intel,"Jaykihn attributes only density to the supposed heat issues, not BSPD. Which makes sense because the process guy and Stephen Robinson both talked about having to space out the signal and power wires when working with BSPD.  If they prioritized density, then it is possible that frequency could not reach the theoretical maximum due to crosstalk or heat issues.",Neutral
Intel,"18A has always been low density, but expected to compensate for it with very efficient frequency scaling past peak perf/W. It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Wait for the desktop/server chips before you call it. With fewer thermal constraints and much higher power budgets, they should be able to push well past the perf/W peak, where it should continue to scale well and for a long time before it starts hitting severely diminishing returns.  I'm not talking 300W monster CPUs, but the scaling between a 65W and 105W 18A CPU should be more significant than on current nodes, where you might gain like 10-15% additional performance from that 50% power bump. I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.",Neutral
Intel,> So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.  Why would they suddenly decide to underclock it to such a degree?,Neutral
Intel,"Tbh, LNL battery life (note: not the same thing as loaded efficiency) with -U/-P/-H perf levels and market reach would still be a very good thing. It's just a shame to see Intel finally straighten out their SoC architecture only to be hamstrung by a subpar node.",Neutral
Intel,">In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4)      Intel's existing 2+8 parts have a PL2 of 57W, so seems to me OEMs are permitted to target the same PL2 they've been targeting on U series.  >and 65/80W (4+4+4 and 4+8+4) respectively.  Which is a fairly large drop compared to existing H series. ARL-H PL2 is up to 115W, PTL-H PL2 will be up to 65/80W",Neutral
Intel,">So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones,  NVL-S being external adds serious question marks to this.   I wonder if post NVL-S they go back to using further iterations of 18A though for even desktop, if they feel confident enough that they can hit high enough Fmax (even throwing away power and density) on those compute tiles.   Maybe for RZL or Titan Lake in 28' (going off memory for codenames lol).   >especially in use cases where SRAM density is less of a concern.  On paper, Intel has caught up to TSMC (even N2) in SRAM density.   >AFAIK Intel CPUs aren't as cache-dependent as AMD,  Generally speaking Intel uses way higher capacity caches than AMD, AMD uses smaller but faster caches.   >. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.  This seems *extreme*. Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?",Neutral
Intel,"I actually suspect otherwise - speculation on my part, but I suspect that they were running into unexpected process issues when they pumped the frequency and/or voltage higher so they had to scale back performance.  It almost feels like a process bug that they discovered when making the actual chips.  They've also talked about kind of weak yields on 18A on their latest earnings call.  That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  If they have a 'double upgrade' opportunity in both fixing the discovered 18A issues and implementing the originally planned 18AP improvements then it could end up being a somewhat bigger jump.  Don't think it'll be an N2 beater though.  18AP *might* beat the best of N3 in some applications, we'll see.  For that you'd want to look ahead to 14A where the CFO mentioned on the earnings call (and he was somewhat realistic in other areas so it's more believable here) that 14A has been a positive surprise and they're actually pretty excited about it.  It's possible that if Intel gets more experience on High-NA versus TSMC that they might start having an edge later on.  Maybe.",Neutral
Intel,"> Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD   And yet we see the exact opposite here. The leaker claims these go to to 65W, even 80W TDP. There's plenty of power to hit any ST boost the silicon can handle, yet it *regresses* vs N3B ARL. And that's with a year to refine the core as well.    > If that is the case, it will only really be allowed to shine in high powered desktop and server chips   Server chips are low voltage. Even lower typically than mobile chips. High-V only matters for client.    > My understanding is that 18A will scale much better with extra power than N3 and N2   There is no reason to believe that at this time. Notice that Intel themselves are using N2 for their next flagship silicon, including desktop.   Edit: Also, for the ""pipe cleaner"" rhetoric, remember how they cancelled 20A claiming 18A was doing so well and ahead of schedule? Even more obviously a lie now.",Neutral
Intel,Having to roll the dice on the scheduler doesn't make things better.,Negative
Intel,"Since release of Windows 7, which allowed users to easily set which cores are used on per application basis in first party software (task manager).",Neutral
Intel,"It's been a thing for a while as a means to circumvent the Windows kernel's shitty scheduling, especially on processors with asymmetric core or chiplet arrangements. Tbf, Windows has slowly begun to catch up and run cores a little more efficiently, but some people still swear by apps like Project Lasso for manually controlling their process affinity.",Neutral
Intel,"Users aren't ""determining"" anything because they don't have to.   On smartphones, they've virtually zero control over their own hardware.   And on the x86 side, I don't think people are buying big.LITTLE hardware in droves, and even if they are, I doubt the multiple core clusters are their deciding factor.   And if I had a big.BABY CPU, I’d definitely be tempted to play around with it, and yes, I don’t see the big idea behind having three clusters, aside from marketing gimmicks and artificially inflated MT benchmark scores.  From what I've heard, Windows Scheduler has trouble dealing with just two clusters as it is. On Linux, the experience is, at best, tolerable.  At the very least, I've certain issues with Intel marketing these CPUs has having ""16 cores."" I know a lot of 'normies,' first hand, who have fallen prey to this deceptive marketing tactic.",Negative
Intel,"> E cores and LP-E cores are the same microarchitecture   In this case, they're even the same physical implementation. 100% identical to the compute cores. Also, for MTL/ARL they were neither power nor area optimized.",Neutral
Intel,The average user is using U series,Neutral
Intel,">, which it will do and at better economics for Intel than Arrow Lake H.  Doesn't seem like this will be the case till the end of 26' though. At least from the earnings call a few weeks ago.",Neutral
Intel,"The average user runs apps like Teams, entirely off of the LPE core of Lunarlake and Pantherlake",Neutral
Intel,"LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  4 Darkmont LPE, in therory, should be a significant improvement.",Negative
Intel,"They are definitely more than usable if it’s anything like LNL, which basically defaults to them.  If it’s more like ARL or MTL, it’s placebo except for S0 sleep.",Positive
Intel,"The LP cores in Meteor Lake/Arrow Lake were too weak to be usable as multithreaded boosters, this isn't the case with Lunar Lake and Panther Lake will be no different",Neutral
Intel,"Windows are supposed to be using these LPE cores for lighter task such as web browsing or Word documentation or idling so that the they won't have to activate the rest of the cluster -> Reduced power draw and therefore good battery life. Meteor Lake originated with these but they were so slow they were pretty useless outside of idling. Lunar Lake also had 4 LPE cores functionally and we see how good those cores are, and these Panther Lake are supposed to be built on that but with additional 8 E cores for people that really value multi-core performance and to also solve the one major weakness of Lunar Lake, if it is even that major in the first place.",Neutral
Intel,>Aren‘t they even ahead of the skylake IPC by now  They're definitely ahead of Skylake now. Cougar Cove IPC vs Darkmont is gonna be like 10% better,Positive
Intel,> Only the 4 LP-Cores are kind of weak   Not for LNL or PTL. They're not crippled like the MTL/ARL cores. Should be full speed Darkmont. That's like 12th or even 13th gen big core.,Neutral
Intel,"The LPE cores being weak is probably the point anyway, or at least partially. It doesn't need to usurp an insane amount of wattage to get to full speed because it doesn't scale well, but it stays fast enough at lower wattage to default to that power profile -> Improved battery life.",Neutral
Intel,They are ahead of Raptor lake P cores and Zen 4 in IPC under 40W,Neutral
Intel,Honestly I wonder if the 4 e-core cluster on the compute tile is outright more power efficient than the 4 p-cores.    I wish some reviewers would do power efficiency testing with different core count configurations enabled on LNL and ARL. Just for curiosities sake.,Neutral
Intel,"At this point, I just hope they have a handheld chip with just 8 e cores (ensuring the CPU tile is as efficient as it gets) and a decent GPU. Maybe in time for an Intel + Nvidia chip, which could be a dream handheld chip.",Positive
Intel,Gaming laptop with iGPU should be niche. Most people just get Nvidia in their gaming laptops.,Neutral
Intel,Apple gets their margins in upselling memory and storage they don't need to worry about the added complexity of more SKUs for the sake of price laddering,Neutral
Intel,"Perhaps fully mature was an overstatement, but it wasn’t the first product on N3B, the node was already in HVM for over a year at that point IIRC.",Neutral
Intel,"We all know that's a lie and 20A was a disaster. Why repeat it now? 18A may not be perfect, but it exists and these chips are comming soon.",Negative
Intel,What delays? 2025 node in 2025?,Neutral
Intel,">BSPD in particular was supposed to help the most at high voltage  Says who?  BSPD improves signal integrity, lowers IR Drop, and helps improve density, at the expense of heat dissipation.   Idk how a technology that is known to increase heat density was supposed to *improve* fMax",Neutral
Intel,"> but expected to compensate for it with very efficient frequency scaling past peak perf/W   Quite frankly, the only people ""expecting"" that seem to be people on the internet unwilling to admit Intel underdelivered with 18A. There's been no objective reason to believe that was even a target for the node. If anything, the exact opposite. This was supposed to be the node where Intel focused more on low voltage for data center and AI.    > where you rarely push past the best perf/W due to thermal and battery constraints   That is not the case. ST turbo in -H series laptops has always been high, nearly on par with the desktop chips. At the power budgets being discussed, there's plenty available to hit whatever the silicon is capable of, and Intel's never been shy about pushing the limits of thermals.    And again, when you adjust for the power/thermal envelope, you *still* see a clock speed regression vs even ARL-H. The best possible outcome for Intel is actually that they can't hit the same peak voltage but look ok at low to mid.    > Wait for the desktop/server chips before you call it   Server is typically low voltage, or mid voltage at most. It's lower down than even laptop chips. And what desktop? Intel's using N2 for their flagship chips for NVL, which should really have been an obvious indicator for how 18A fairs.    > I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.   Intel had numbers in their white paper. BSPD, all else equal, was maybe a couple percent at high V. It's not going to produce dramatically different scaling, and again, we have direct evidence to the contrary here.    In general, Intel's had a lot of process features over the years that look a lot better on slides than they end up doing in silicon.",Neutral
Intel,"> 18A has always been *Xyz* …  There's always some excuse for Intel's stuff the last years, to accidentally NOT perform as expected, no?  > It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Hear, hear. Isn't that all to convenient …  > Wait for the desktop/server chips before you call it.  … and when is that supposed to happen? Nova Lake will be TSMC's N2.  What *Desktop* CPU-line will be on Intel 18A anyway then?",Neutral
Intel,Give reasons why you claim this is an 'underclock' that was done 'suddenly'.,Neutral
Intel,"Of course, Lunar was quite competitive, yet it was mainly so on performance due to Intel basically sugar-coating the living benchmark-bar out of it via the on-package LPDDR5X-8533 RAM. That OpM for sure *majorly* polished its efficiency-metrics by a mile …  Though looking back the recent years, that's how Intel always masked rather underwhelming progress on their process-technology – Hiding the actual architectural inefficiencies and shortcomings behind a invisible *wall of obfuscation* by only ever deliberately bundling it with other stuff like newer PCi-Express versions of 4.0/5.0 or newer, faster, crazy high-clocking RAM-generations.  So Lunar Lake while very strong, was mainly so due to being propped by OpM, and TSMC's processes of course.",Neutral
Intel,"The article claims it's ""TDP"", which would typically refer to PL1. Might be leakers getting their terminology mixed up, but I wouldn't necessarily assume a large drop in power limits.",Neutral
Intel,AFAIK Intel was at 165W in mobile back then …,Neutral
Intel,"> Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?  [6%](https://web.archive.org/web/20240901235614/https://www.anandtech.com/show/18894/intel-details-powervia-tech-backside-power-on-schedule-for-2024/2)",Neutral
Intel,"I wonder if the issues Intel's facing with 18A partially explain TSMC's very conservative approach to adopting ~~it~~ BSPD.  I image in reality, 18A (and AP's) fMax limits are almost entirely the reason for N2 in NVL-S",Neutral
Intel,"> That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  There's no doubt that given enough resources they can fix the node to the point where it's good enough for HVM in high performance chips. It's what they did with 10nm after all but it took them several years to go from poorly yielding Cannon Lake/Ice Lake to okay Tiger Lake and good Alder Lake.  If we take the talk about 18A ""margins"" from the recent analysts call to be a euphemism for something like node yields and a stand in for when it will be ""fixed"" then we could be talking 2027 before 18A gets to that point. A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.",Positive
Intel,"Yeah I know they talked about yield issues, but everything indicated that it was not a case of manufacturing defect density, but rather something else.   That would imply issues with hitting target frequencies, which as you say, they should be able to iron out in due time. That is as long as it isn't a fundamental issues, which I have a hard time believing, given how unconcerned they seemed about it.   I'm fairly confident it will be ironed out before Clearwater Forest hits mass production.",Neutral
Intel,"There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery. It offers better voltage control and less current leakage, leading to higher efficiency at any given voltage, but especially past the point where current leakage starts becoming more of an issue, i.e. at very high frequency.  A pipe cleaner running at low frequencies is to be expected. Wait for the desktop chips and you will see P cores hitting well over 6+ GHz advertised all core boost, possibly 6.5 GHz. I wouldn't be surprised to see some users hitting 7 GHz stable.",Positive
Intel,The modern CPU cores schedule all cores equally with E core priority because nowadays the difference between E core and P core is less than between AMD X3D CCD and higher clocking CCD,Neutral
Intel,"Windows Scheduler has come a long way but I mean come on, this is pretty pedantic. The commenter above is clearly not using a tool to assign cores to specific tasks they are just being annoying.",Negative
Intel,4 skymont already seem pretty good in LNL,Positive
Intel,> LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  Makes you think why Intel even went all the way to integrate those and waste precious die-space doing so …,Negative
Intel,They could be useful if they could check your email and stuff while the laptop is effectively sleeping.,Neutral
Intel,PLT's LP-E cores take after LNL. ARL/MTL's LP-E cores design is dead,Neutral
Intel,I hope those are powerful enough to eventually act as the low-power booster these were once supposed to.,Neutral
Intel,> Meteor Lake originated with these but they were so slow they were pretty useless outside of idling  The last info I had on the back of my mind was that these couldn't even be associated by the user and were basically reserved for Windows itself. Is that still the case now?,Negative
Intel,"Yeah after being reminded that the LPE cores are also Darkmont like the E-Cores I looked up the layout in detail and the differences is simply the P and E cores are on a large Cluster with L3 Cache and a fast ring-bus, but the LPE cores are seperated from that Ring-bus and don't have any L3 Cache (just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster).     They are for sure also much more limited in power-budget and frequency, but they can be very fast in general. The main usage is being able to disable the large cluster in Idle or low cpu usage to heavily reduce power draw.     The communication in between the clusters is much slower. So anything that relies on synchronisation and communication between cores is only really fast (latency-wise) if it is kept within the same cluster.      The LPE cores therefore can be very fast actually, but only if the task stays within the cluster. Modern demanding games will therefore 99% soley run on the large cluster, because they would run slower if they spread across all of them. The small cluster could be used to offload background tasks to themselves (if the powerlimits aren't hit), which could at least improve 1% lows and fps stability.",Neutral
Intel,"It's not just more power efficient, but also much faster at low power which you kinda want since you'd be mostly GPU bottlenecked. The good thing is that they've made that part of their thread director tech.",Positive
Intel,Yes it is. Memory uses power and IPC of LPE should be past Zen 3,Neutral
Intel,Intel makes e-core only CPUs (N100 etc) but at this point they're based on several gen-old e-core design. They're good enough for what they are so they might not update them for a while still though.,Neutral
Intel,"even 4 e-cores is enough, dont forget we use to pair up i7-7700K with GTX1080/1080Ti.  4 e cores + even stronger iGPU probably a better combo for handheld.",Positive
Intel,Apple absolutely does price ladder their SoCs.,Neutral
Intel,"True. But the node itself may be kinda mid. It seemed much, much more complex than N3E, and it's not as if the node was any sort of highlight in the products Apple used it in.   Going back to A17 reviews and such, there were serious questions presented about N3B vs N4P.",Neutral
Intel,I'm fairly sure that's what u/ProfessionalPrincipa was also implying.,Neutral
Intel,> We all know that's a lie and 20A was a disaster. Why repeat it now?   Probably in response to all the people assuming there must be some kind of upside.,Negative
Intel,"It was supposed to be a 2024 node. And given the perf downgrade and admitted yield problems, seems more like they're only delivering the originally promised metrics in '26 or even '27.",Negative
Intel,"> What delays? 2025 node in 2025?  What del— *Seriously now!?* Where you living under a rock the last years by any chance? o.O  18A is neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: *'cause Yields again!*) will be 1H26 and most likely even end of first half of 2026.  So it's a 2H24-node, which Intel is only able to offer actual volume basically +1–2 years later. That's called *»delay«*.  **Edit:** And not to mention the actual massive performance- and metric-regression u/Exist50 pointed out already. 18A is basically 20A in disguise. Since 20A wasn't actually knifed, but just relabeled as ""18A"" instead.",Negative
Intel,> Says who?   Intel. It was part of their own published results about PowerVia on the Intel 4 test chip. The gains range from ~negligible at low-V to around 6% at high-V.    Also explains why TSMC is not adopting it in the same way.,Neutral
Intel,Nova Lake is full on N2?,Neutral
Intel,"It's literally a regression from the prior gen, for a node that was supposed to be ""leadership"", mind you.    I don't know why it's hard to acknowledge that 18A is simply underperforming.",Negative
Intel,"Nah, credit where credit is due. LNL made a ton of fundamental design changes that PTL should also benefit from. Yes, they also benefitted a lot from both having an actually decent node and on-package memory, and no, they're not on par with the likes of Apple or Qualcomm, but merely making ARL monolithic on N3 would not have delivered these gains.",Positive
Intel,They definitely have their terminology mixed up. There's no chance PTL-H has an 65W/80W PL1,Negative
Intel,Which was never the PL1 but rather the PL2.,Neutral
Intel,"Didn't TSMC delay their BSPD node too? Ik they changed it from appearing in N2P vs A16, but unsure if there was a timeline shift in that as well.   Honestly, what's going on with N2P/N2/A16 seems to be kinda weird, timeline wise.",Negative
Intel,2027 just seems like hitting baseline parametric yields. Being able to get to an actual improvement in terms of performance is a whole other thing. Not an easy road.,Negative
Intel,">A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.  How it can be 24 product in 27 when it was 25 node in 25 with products on shelfs in january 26, rofl?",Negative
Intel,Intel themselves claim they won't be hitting industry acceptable yield till 2027 for 18A.,Neutral
Intel,"> There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery.    No, it's not. The main long term advantage of BSPD is to reduce the pressure on the metal layers as they do not scale like logic does. Better PnP is another advantage, but secondary. No one cares much about high-V these days.    > A pipe cleaner running at low frequencies is to be expected   As a reminder, ARL-20A was supposed to be the pipe cleaner, and they claimed they cancelled it because 18A was doing so well! PTL was supposed to be the volume product, a year after 18A was nominally supposed to be ready.    > Wait for the desktop chips   What desktop chips? PTL-S was cancelled long ago, and for NVL, they're moving the good compute dies (including desktop) back to TSMC on N2. A decision which should demonstrate the node gap quite clearly...   Also, ST boost for -H chips is in the same ballpark as desktop ones. They're already at the high end of the curve.",Neutral
Intel,"They aren't the same though, hence the roll of the dice to see if you actually get the performance you paid for.",Neutral
Intel,Because they still improve battery life under very light loads.,Positive
Intel,Yeah I think you had to utilize tool such as Project Lasso. I tried to assess the LPE cores to HWINFO64 and it was rather painful to use so I understand why these cores are never used otherwise lol.,Negative
Intel,"Yeah. The efficiency and overall battery life will probably comes down to how Windows manages threads, which seems to be pretty decent now with Lunar Lake seeing as they got pretty good battery life with the same design philosophy.",Positive
Intel,"> just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster  It's also accessible by the large cluster, fyi.",Neutral
Intel,"For those games back in the day, yeah. But modern games can and do use more than 4 cores.   Depends how low end/power you want to go, of course. 4 cores + eGPU cuold be enough too depending on your games: [https://youtu.be/XCUKJ-AgGmY?t=278](https://youtu.be/XCUKJ-AgGmY?t=278)",Neutral
Intel,"The e-cores are unlikely to be hyperthteaded though. 8 e-cores gives you 8 threads just like a 7700K did, though none of them share cores. It feels like a sweet spot for a handheld in 2025 imho.   This is while AMD competitors use 8c/16t of full Zen. An 8 e-core solution would remain weaker but more than sufficient on the CPU side, while allowing enough power for what truly matters (a capable GPU). If paired with an Nvidia iGPU, that could be the perfect handheld chip, if their partnership produces one like that.",Positive
Intel,"Those ""people"" are either bots or trolls. No sane person thinks Intel is crushing it with 18A.",Negative
Intel,"Whats the name of the close the gap initiative?  5 nodes in 4 years... announced in 2021  2021+4=2025  You can even read 3rd party articles from 2021 talking about 18A in 2025.  https://www.tomshardware.com/news/intel-process-packaging-roadmap-2025   >Intel didn't include it in the roadmap, but it already has its next-gen angstrom-class process in development. 'Intel 18A' is already planned for ""early 2025"" with enhancements to the RibbonFET transistors.",Neutral
Intel,">neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: 'cause Yields again!) will be 1H26 and most likely even end of first half of 2026.  How it aint 25 node when there will be products on shelves in january 26?",Neutral
Intel,FMax at same power consumption is higher true. But the temperature is just too high. Both temperature and power consumption are important considerations,Neutral
Intel,"High end NVL is N2, low end is 18A. At least for compute dies.",Neutral
Intel,"That's what we know so far, yes. At least the performance-parts. The lower end is supposed to be 18A, I guess?",Neutral
Intel,"> Nah, credit where credit is due.  I said verbatim, that Lunar Lake was quite competitive, and I meant it, unironically. It took long enough.  > LNL made a ton of fundamental design changes that PTL should also benefit from.  I haven't disputed that —  The modular concept was surely ground-breaking for Intel, and urgently needed!  Though, it's kind of ironic, how Intel all by itself proved themselves liars, when it took them basically +6 years since 2017, for eventually coming up with only a mere chiplet-copycat and their first true disintegrated chiplet-esque design …  For a design, which *in their world-view*, Intel was working on their tiles-approach already since a decade, which is at least what they basically claimed when revealed by 2018 – I called that bullsh!t the moment I first heard it. Surprise, surprise, they again straight-up *lied* about it …  They most definitely did NOT have had worked on anything chiplets/tiles before, if it took them *this* long.  Just goes to show how arrogant Intel was back then, letting AMD cooking their Zen in complete silence since 2012/2013, for their later Ryzen. *Still boggles my mind, how Intel could let that happen* …  > Yes, they also benefitted a lot from both having an actually decent node and on-package memory […]  In any case, we can't really deny the fact, that Intel basically cheated on Lunar Lake using OpM, eventually creating a halo-product, which ironically was quite sought after, but yet expensive asf to manufacture.  If AMD would've been to cheat like that (using OpM) in a mobile SKU, while dealing some marked cards in such a underhanded manner, it would've been ROFL-stomped Lunar Lake …",Neutral
Intel,"Willing to believe that, though then I have to wonder why it would be so hard to cool at such a substantially reduced PL2. Also if/how they could pitch this as an HX replacement.",Neutral
Intel,"Yes, I think it was 45- or 65W-parts PL1-wise. The spread was still obscene and outright insane.",Negative
Intel,I'm excited for 18A-P. Intel subnode improvements have always seemed to bring decent uplifts (Intel 10SF being a notable example).,Positive
Intel,"The difference is no longer stark. As I said, its now smaller than between 2 CCDs in the most popular CPU line ever made in actual raw performance. ie it's not something entirely unique at the scale the differences are",Neutral
Intel,"How even, if these weren't even used with MTL!?",Neutral
Intel,"Yup, pretty much paper-cores for marketing-reasons alone basically.",Neutral
Intel,We already have games tested on Skymont E cores. They are very fast,Positive
Intel,"Yes we know modern games can use more cores, but having smaller amount of cores means saving power budget. Those power consumption saved are better off use to juice iGPU.   Lets not forget the E-Cores here are skymont, not skylake. Skymont is definitely more powerful cores. The exact same reason why i5-7600K is more power than i7-2600K.  given how iGPU for handheld are no way near GTX1080 yet. I think it is better to pump more juice for better iGPU.",Neutral
Intel,Go to the intel stock subreddit lol,Neutral
Intel,"You'd be surprised how many people are willing to take Intel at their word on it.  Just look at previous threads here. Every time Intel posts nonsense slides, lot of people come crawling out of the woodwork.",Negative
Intel,"They are doing about as much as the ""sane"" people expected from 18A.",Negative
Intel,"> 5 nodes in 4 years... announced in 2021  It was supposed to start with Intel 7 in '21 and end with 18A readiness in H2'24.   https://img.trendforce.com/blog/wp-content/uploads/2023/10/17144859/intel-4-years-5-nodes.jpg  And now in the way end of '25, we're getting something more like what they originally promised for 20A, *if that*, and now they're saying yields won't be ""industry standard"" until as late as 2027. It's a disaster by any objective standards. Their failure with 18A literally got the CEO fired...",Neutral
Intel,"> How it aint 25 node when there will be products on shelves in january 26?  It seems, you don't actually grasp the concept of the term »difference«. *2025 and 2026 is actually NOT the same!*",Negative
Intel,so the igpu is propably tsmc too?  do we know if thats because of poor performence or poor manufacturing capazity?,Negative
Intel,I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   I think they'll probably have ARL-R for HX,Neutral
Intel,"Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage. Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.",Neutral
Intel,"The LP-E cores were a failure in MTL/ARL's design.  They're very useful in LNL and that trend follows with PTL/NVL.  They're definitely not just ""placebo cores"" - LNL uses them more often then the P cores and they can be activated and used in all core workloads as well.",Neutral
Intel,"They were, just not as often as Intel would have liked.",Neutral
Intel,"For MTL and ARL, yes. For LNL and PTL though, I haven't test them out personally but seeing the battery results of LNL chips made me think that the LPE cores of those things seem to be legit.",Neutral
Intel,"Oh year, right, I think I missed (or forgot) that. Darkmont should be even better but I'm guessing not by that much.",Positive
Intel,Kinda making my point.,Neutral
Intel,"Bro xd  Node readiness and product readiness and product on shelves are 3 different date  Node must be ready before product, product must be ready before being on shelves.  Officially 18A was ready around Q2 25",Neutral
Intel,"> so the igpu is propably tsmc too?  18AP, iirc. Good *enough* (or at least, assumed to be when originally planned) not to be worth the extra cost of TSMC.  > do we know if thats because of poor performence or poor manufacturing capazity?  Poor performance. Intel can't afford to be constantly a node behind. It's just too big of a gap.",Negative
Intel,"> I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   But that's exactly it. They said 65W vs 80W was in relation to cooling challenges, but either is more than sufficient for max ST boost. Idk, maybe reading too much into too little information.    > I think they'll probably have ARL-R for HX   At one point it sounded like they wanted to pitch PTL as a partial replacement. Guess it's not good enough though.",Neutral
Intel,"> Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage.  Of course, I already knew that. Binned Desktop.  > Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.  Well, up until 2018/2019/2020 Intel actually didn't really uncapped any of them and those parts were hard-limited to only pull their max 45–65W TDP.  Only later on when they were trying to hold pace with AMD, Intel insanely increased the TDP to pull to insane numbers (90W, 135W, 165W) … Other than that and prior, you had to resort to modded BIOS.  So until even Apple burned their thick skin on some i9 (**Cough* i9-9980HK in the 16"" MacBook Pro 2019), you didn't had such insane TDPs to begin with anyway (except for BIOS-mods).",Neutral
Intel,"As said, I owned a MTL-machine back then, and back then you couldn't even associated these cores after all, since you couldn't pin anything on them – So yes, *back then*, these were placebo-cores and they were in fact pretty much 'on paper-cores for marketing-reason' and basically useless.  Seems Intel managed to improve their performance a lot and make them actually useful, which is good!",Negative
Intel,"Yes, MTL's LPE-cores were basically a dud, LNL was fairly workable and PTL will be hopefully potent.",Neutral
Intel,haha,Neutral
Intel,> Officially 18A was ready around Q2 25  Which is still at least 4 quarters *past* its due-date actually …,Neutral
Intel,"That's not strictly true, the (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  https://www.ultrabookreview.com/27215-asus-rog-g703gx-review/#a5  >That comes with a few drawbacks, though, and one of the most important is the noise development. Asus provides three different power modes for this laptop, which you can use to juggle between performance, thermals, and noise:  Silent – CPU limited at 45 W and 4.2x multiplier, GPU limited at 140 W, fans only ramp up to about 40-43 dB in games; Balanced – CPU limited at 90 W and 4.2x multiplier, GPU limited at 140 W; Turbo – CPU limited at 200 W and 4.7x multiplier, GPU limited at 200 W, fans ramp up to 55-56 dB in games.  It was always up to the laptop OEM to configure the power limit and there was no hard cap enforced by Intel.",Neutral
Intel,"The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, negating the entire point of them.  In addition, MTL/ARL had the LP-E cores on the SoC tile which made them functionally useless for full nT load.  LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks *can* stay entirely within the LP island, *and* they're on the same compute tile, so they're also used in full nT workloads.",Neutral
Intel,"> That's not strictly true …  Yes it actually is.   > The (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  Dude, you're basically confirming exactly what I wrote, by literally picking a **i9 9980HK**-equipped notebook of 2019, which is even exactly the very infamous SKU I was talking about …  I was talking exactly on (among else) that very CPU, even Apple couldn't tame and had to undervolt (still without being able to prevent the later sh!t-storm) until eventually abandoning Intel altogether.  ---- All I'm saying is, all the years prior with only quad-core, Intel \*never\* (nor any OEMs for that matter) went above and beyond the official TDP – The only lone exception from this, were those super-bulky Schenker Desktop-replacements.  That only suddenly changed by 2017–2019, when Intel suddenly increased core-count in mobile swiftly from Quad- to Hexa- and ultimately Octa-cores, while pushing the TDP in quick succession  into insane territory.  *That was the time, Asus took about a year to manage applying liquid-metal en masse, for Intel-notebooks!*  You are right with your sample here, yet you only confirm what I said before. Only past quad-cores it was.",Neutral
Intel,"> The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  Do you happen to know, if that (ring-bus related) was the same principle on MTL?  > The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, **negating the entire point of them**.  Yeah, talking about throwing some completely untested sh!t onto the market, irregardless of the fact, if the product makes sense or not — A true classic, I'd say. Just Intel being Intel.  Since dropping those LPE-cores on MTL, would've actually made the mask and thus needed die-space *smaller*, which in turn would've actually *increased* their yields \*and\* in return profit-margins already …  But muh, benchmark bars and ""AMD has moar corez!!"", I guess.  > LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks can stay entirely within the LP island, and they're on the same compute tile, so they're also used in full nT workloads.  Is there any greater penalty (latency or cache-flush-wise) for moving threads off the LP-island to P-cores?  *Thanks for the insides so far though!* I can't really see through anymore to all these constant arch-changes.",Neutral
Intel,"I recently picked up an MSI Claw 7 AI+, and have been pleasantly surprised with the current state of their drivers. I would absolutely be interested in a Celestial GPU if they have anything targeting 9070~ or higher performance level. Here's hoping!",Positive
Intel,I'm glad they are doing these followup videos. Gotta give Intel credit where due - their cards are getting so much better over time.,Positive
Intel,The Lunar Lake handhelds would be really popular if they were just a little cheaper.,Neutral
Intel,They said their high idle power is an architecture issue so they can't fix that,Negative
Intel,God forbid Intel supports Day 1 GPU drivers longer than 5 years,Neutral
Intel,"true but thats not really that big of a deal, and can be fixed with some tweaks [https://www.reddit.com/r/IntelArc/comments/161w1z0/managed\_to\_bring\_idle\_power\_draw\_down\_to\_1w\_on/](https://www.reddit.com/r/IntelArc/comments/161w1z0/managed_to_bring_idle_power_draw_down_to_1w_on/)",Neutral
Intel,So just like AMD then.,Neutral
Intel,"They usually do tho? We dont know about their dGPUs yet because it hasn't been 5 years.  Their Xe Laptop gpus are 5years+ and are supported as of this month.  No idea why you're hating on intel GPUs mate, i'm used to everyone cheering the underdog not the otherway around.",Neutral
Intel,"I think that guy's a misinformed user that is saying that AMD 5000/6000 series won't get driver updates anymore lol, when AMD just said that those gens aren't slated for game optimizations that use their ""latest technologies"" (FSR4 at the time of writing) anymore. Those gens still will get driver updates but AMD gave up on trying to implement FSR4 on them, like the leaked scripts earlier this year had performance reduction from 20% to up to 30% FPS to use FSR4 for 6000 series cards since they don't have the hardware and just using brute force software for FSR4.   I'd agree that my 6700 XT never getting FSR4 sucks, but the misinfo circling around is that we'd never get anymore driver updates lol.",Negative
Intel,"so the A, B, C, (etc) Series is just branding? and they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name? … but Xe3 plus will be enough to call it C series.",Neutral
Intel,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",Positive
Intel,Everything about Celestial I've heard seems to be regarding mobile. Is there actually going to be dedicated discrete graphics?,Neutral
Intel,The Intel driver downloading a shader cache for games from Intel sounds very noteworthy.   I'm really skeptical tying GPU drivers to a manufacturer's online service.,Neutral
Intel,"They don't care. These will be mid tier chips at best. The RTX IGPU ones will be their flagships. These are still damn impressive though, just have to make that clear.",Negative
Intel,"The mega brains of this community really struggle with product naming lol, its so funny to watch supposed intelligent people straight faced say they can't work out what product out 6 to buy because of their product names, most of them are in different market segments so you can't even accidentally buy the wrong one.",Negative
Intel,"They may divide ABC depending on ISA/hardware capabilities used for example. Like AMD used ""RDNA 3.5"" on mobile as full RDNA 4 would pull more power due to more silicon dedicated to AI (including FSR 4 -\_-). RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4. So Xe 3B will be better than current but Xe 3C will do better at DXR and/or AI frame gen/whatever new tech they added.",Neutral
Intel,"This will help you to understand: https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake  This is not brand new architecture, it's refinement on battlemage to work more efficiently. Thinks of it as Xe2.5 but for some reason marketing is calling it Xe3.",Positive
Intel,">they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name?      No what he said is that the Arc B Series hasn't been around for all that long and the B580 has garnered a very positive reception so they felt in terms of branding and marketing moving in Panther Lakes Xe³ into the B series brand is a better marketing move than throwing away the B series branding and starting the C series branding now. In others words they want Panther Lake to ride on the momentum and positive vibes the B series branding has garnered.   Xe³ is a Xe³ based architecture like Xe³P is architecturally but there is more to selling a product than the engineering, keep in mind there's strong rumors that there will be a B770 launch, Intel also just released the B60 and later this year will provide a big software expansion to Bxx cards so the B series brand is still full of life. Xe³P will launch much later and at that point Intel feels there will have passed enough time to leave the B series branding behind.",Neutral
Intel,An alternative explanation could also be that Celestial dGPUs *are* launching as Xe3P and they would want the launch of C series iGPU and dGPU to be around the same time and the same architecture.,Neutral
Intel,">so the A, B, C, (etc) Series is just branding?  Wasnt it always so? Xe is the architectural names, ABC was just branding for customers.",Neutral
Intel,"No public announcements yet, but logically it would make all the sense for Intel to launch them, and do so in larger quantities so people in the market can get them at MSRP. Battlemage proved there is a lot more demand than there was supply, and it'd take too long to ramp it up once it became clear they were received well. With proof of a market test pass, they've now got a chance to do it right.  They are excellent value products that would allow Intel to get a chunk of that market, given they are available in sufficient quantities at intended price points.",Neutral
Intel,Is shader caching on first use a real problem? Sounds like bored people needing something to cry about again.,Negative
Intel,>RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4  Do you have a source for this? I vaguely remember 3.5 wasn't really bringing particular changes,Neutral
Intel,That's exactly what everyone's new GPU generation is.  An entirely new architecture is extremely rare for any company.,Neutral
Intel,That's not what Peterson was talking about context wise when he addressed this in the video.,Negative
Intel,"They would be extremely stupid not to continue to release discrete graphics cards, even if they decide to release 2-3 gaming SKUs per new generation; entry/budget, mid-range, high-end, etc.  - they win. until they keep iterating up, maturing driver, increasing install user base, improving software, until the point they can target higher segments (enthusiast tier)  Gaming GPUs remain the flagship component that pushes the limits of silicon design, features like ray tracing, AI upscaling, and real time path racing all debut in gaming before trickling into pro and AI workloads. Every research report shows that demand for high-end GPUs is still strong. Steam HW survey shows the most popular GPUs are still mid-range gaming cards, showing that discrete GPUs remain central to the PC ecosystem.  And yes, gaming GPUs are now marketed as AI accelerators too, Nvidia and AMDs GPUs both highlight AI compute as much as gaming.  Furthermore, if you look at the Steam HW survey, nobody gives a CRAP about 4k/8k gaming, meanwhile 1440p is growing! Nvidia, the leader in GPUs has been marketing for 4k gaming since 2013 with the 780 Ti/GTX 980 era, then 2016 with 1080/1080 Ti, 2018 with 2080/2080 Ti, 2020 with 3080/3090, and marketing for 8k gaming since 2020 with 3090, 2022 with 4090, and this year with 5090. So from 2013 to 2025 Nvidia has been marketing their GPUs as ""4k ready"" and ""8k ready"" but reality is that majority of gamers don't blame games at those resolutions. 4k adoption is only ~2-3% of users, and 8k is virtually nonexistent. So all Intel has to do is really focus on 1080p Ultra and 1440p High/Ultra gaming, above entry‑level (B570, A580) and below enthusiast (future B770/B780?) and keep optimizing and improving ray tracing and XeSS.",Neutral
Intel,[AMD RDNA 3.5’s LLVM Changes - by Chester Lam](https://chipsandcheese.com/p/amd-rdna-3-5s-llvm-changes),Neutral
Intel,"There are more to that, if you look at RDNA 4 and blackwell for example they had changes to the SMs, enc/dec, etc.   Here it using basically the same cores, but changes are made to better utilize those cores. I think this sentence summarize it the best:  ""Intel classifies Xe3 GPUs as part of the Battlemage family because the capabilities the chip presents to software are similar to those of existing Xe2 products"".  Now I'm not saying it's a bad thing, Intel has a lot of inefficiencies with Battlemage, a lot of time you expect it perform better judging by the amount of hardware. By going this route the basically giving pretty good improvement only year after initial release with basically almost the same drivers. It's just not a new generation but something in the middle between a refresh and a new generation.",Neutral
Intel,"Not rare, but more like it only happens when there's a paradigm shift in graphics that necessitates it.   GCN -> RDNA or Pascal -> Ampere are the most recent ones.",Neutral
Intel,"its not extremely rare. It happens once every 7-10 years or so when they redesign it for significant changes. but yeah, most generations will be refinement.",Neutral
Intel,"In the end it is all matter of marketing, filter it for a moment and understand what it's. As he said ""leverage all the good work from battlemage"". As I read it and looking the architecture it's battlemage with some enhancement that will come to future product celestial. Something like RDNA 3.5. I think they messed up the naming - they want to present it as something new and different (it's in some aspects) but it's not the real next gen in their roadmap.",Neutral
Intel,By discrete iam assuming you mean client graphics SoCs? The margins are not enough on those - you make much more money on Pro cards/ HPC.,Neutral
Intel,"No this is not the case. The decision is purely branding. Xe3 and Xe3P are part of the same generation architecture wise unlike Xe2 and Battlemage, he addressed this.",Neutral
Intel,It's this. IGPU's will be for gamers and DGPU for data center. They basically both have said this :) People just don't notice anything. What do folks think RTX IGPU's on intel chips are gonna be for. LIKE LOL!,Neutral
Intel,"Then why he said ""when we move to our next architecture...""? Meaning they are not the same (Xe3 and Xe3P), also look at the slide they're grouping Xe2 with Xe3 under battlemage.  What you say contradicts their statements, and the article. The decision is of course marketing but nowhere they say Xe3 and Xe3P share the same architecture. This is why I said before it should've been called Xe2.5 or something of that sort like with RDNA 3.5.",Neutral
Intel,Next architecture is just rearrangement of existing cores into more effective set up. No contradictions.,Neutral
Intel,"When we will see deep dive into X3P celestial we will see what have changed. Until then it's just speculation of the differences between Xe3 and Xe3P. My bet there will be more changes than mere ""rearrangement of existing cores"".",Neutral
Intel,"Love a good TAP interview, hopefully he'll do some podcasts too once the embargo lifts.",Positive
Intel,"I like how TAP said ""expect even bigger investments in graphics in the future"". I hope this means what I think it means, because Intel \*must\* continue heavy investment in their own graphics. Accelerated compute is the future.",Positive
Intel,MLID must have an aneurysm seeing the guy still employed at Intel,Neutral
Intel,"Tom's an absolute gem. It's so frustrating to see the most passionate and effective communicators locked up behind embargoes and NDAs, so fingers crossed that this marks the next wave of tech exploration and deep dives. Always nice to see him pop up in a random GN or PC World video.  Credit where it's due, Lex (Alejandro) has also always been attentive and forthcoming in his Reddit interactions, even factoring in the usual embargo/NDA constraints. Hopefully he'll dust off his u/LexHoyos42 account; I'd love to see some mini-AMAs as Panther Lake trickles out.",Positive
Intel,Igpus not discrete gpus,Neutral
Intel,"I would not attempt to read much into claims like that. This is marketing, not an investment roadmap. I'm sure if you pressed them on it, the lawyers would have some words.   Remember, they've yet to even *talk* about dGPUs past BMG. And the big-iGPU NVL is reportedly cancelled (likely in favor of the Nvidia partnership).",Negative
Intel,Hahaha I can hear Tom saying “fucking Tom Peterson has been *lying* to you” as you say that,Negative
Intel,Problem is we don't know the real roadmap is for Intel. I really want them to put out a roadmap like they used to. But things in flux probably :s.,Negative
Intel,I think only way for intel to enter consumer GPU space is to start via iGPU at this point.  They just need to make bigger faster iGPU to make Nvidia RTX xx50/RTX xx60 redundant.,Neutral
Intel,I don't think partnership Nvidia iGPUs will be ready until TTL/HML circa 2029/2030.,Neutral
Intel,"Agreed, but then similarly I would also not attempt to conjecture on their roadmap outside of an investment roadmap... lack of a public roadmap update on graphics in a while is certainly saying something by not saying anything, but it's also not saying cancellations, etc...",Negative
Intel,There are just too many Toms in tech media. Tom of Moore's Law is Dead needs to thin out the competition.,Negative
Intel,I don't think even Intel knows what the real roadmap is anymore. They're so far behind on the previous ones that it's not surprising that they're not releasing new ones.,Negative
Intel,We are not competing with higher end GPUs. It seems they want to capture the budget segment and then move up the ladder.  But I don't like that iGPUs has more priority than dGPUs. Hope celestial comes into life in future,Negative
Intel,"Probably, but no sense offering an in-house big iGPU for just one gen. It needs to be a long-term commitment.",Neutral
Intel,"I mean, the layoffs tell a story. The dGPU group, and client graphics SoC in general, was gutted even before Lip Bu came on. To say it's likely a skeleton crew is probably doing it a favor.",Negative
Intel,You stole what I was going to say... take my upvote.,Neutral
Intel,"thats how they are doing it.  iGPU is the one that killed Nvidia MX, or GT xx30 series. So their next target is the RTX xx50.",Negative
Intel,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  One thing seems clear, this solution will be cheaper than standard AI data center products.  Hopefully, Intel can get and maintain footholds in medium-sized AI research market",Neutral
Intel,No way that power usage is at idle. Even if there was only one performance state like Tesla GPUs they wouldn't consume that much power.,Negative
Intel,that's insane vram density,Neutral
Intel,"This thing only has a 1Gb Ethernet port? I don't know much about the use case for this thing, but that seems surprisingly low. Simply from the use case of uploading training data to this I feel like something faster is necessary.",Negative
Intel,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Is there a fork of chrome that runs on gpus,Neutral
Intel,"Now do it with pro dual version of b770s, 64gb each. Could be a far more economical inference solution than what AMD is providing",Positive
Intel,but is that faster than a single 5090?,Neutral
Intel,Is this enough VRAM for modern gaming?,Neutral
Intel,Nvidia: ill commit s------e,Neutral
Intel,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  If they rely on PCI-E for interconnect, bandwidth will be anemic between cards for training purposes. Might be ok for inferencing, but for training, you need the bandwidth.  That's not unclear at all.  This thing is probably not useful for anything, but serving many users with small models for inferencing.",Neutral
Intel,"I doubt we'll see any UALink this year, but perhaps  on the 160GB  Crescent Island card next year.  Intel hasn't announced anything, but it seems obvious.",Neutral
Intel,"You think Intel is going to maintain these footholds any longer than they would need to?  This push of their dedicated GPUs into the professional space, is just a mere attempt to get rid of the unsold inventory of ARC-GPUs at the highest possible price-tag right from the beginning – Dumping those into the enterprise- and datacenter-space, in noble hope that someone clueless might bite upon the Intel-brand, and then dip into it at high costs *at non-existing support software-support* Intel pretends to deliver some time in the future.  Since as soon as the bulk of it was sold, Intel will cut the whole division and call it a day, leave anyone hanging dry.  That's what's planned with the whole professional-line of them – Buying this, is a lost cause as a business.",Negative
Intel,"I think they've just taken the 400W per card TDP and multiplied it by 16 to get the 6400W figure, so I think they're trying to say minimum power draw at full load, not at idle. ""Minimum power draw"" is definitely really odd phrasing for that number. Surely you'd think they'd say ""Total GPU TDP"" or something less confusing",Neutral
Intel,I don't think servers are supposed to stay idle for long.,Negative
Intel,"> the 768 GB model uses five 2,700 W units (totaling 10,800 W), while the 384 GB version includes four 2,400 W units (7,200 W)    And an insane power consumption.",Neutral
Intel,You basically always put in dedicated NICs in those kind of servers. The onboard ethernet is rarely used.,Neutral
Intel,Could that make it very cost effective for any particular use cases?,Neutral
Intel,"Even the fancy NVIDIA interconnects are a bottleneck for training performance, as are the memory bandwidth and even L2 bandwidth. One architects the algorithm around that. A tighter bottleneck is obviously not ideal, but it might still make sense depending on the price of the hardware.  As long as a copy of the full model and optimization state fits on the 24GB bank the PCI-E interconnect isn't really too limiting for training. Small models, like a 1B parameters voice model, will naturally fit. The larger the model you want to train, the more complex the architecture might be, but there are ways to train even models over 100B parameters on such hardware, like routing distributed fine grained experts. Not that I think it's a good idea at this point.",Neutral
Intel,It's VideoCardZ so they probably got information from a tweet or Reddit post and copy/pasted without doing any thinking whatsoever.,Neutral
Intel,"I assumed that would be the case, I just didn't see any room for expansion. It's pretty crowded with 16 full sized cards.",Neutral
Intel,"I don't really see any, unless you can find a workload that will fit the card and multiply that workload for 32 users, but as each chip is performance wise less than a 3090, it has to be a fairly light workload.",Neutral
Intel,"1B might work for a voice model, but useful LLM models ARE 100GB+. Smaller models are fun little toys.",Positive
Intel,At least its a human hallucination and not AI hallucination.,Neutral
Intel,Can also be bad translation.,Negative
Intel,"according to the datasheet theres two free pcie 4.0 x16 slots on one version and two pcie 5.0 x8 slots on the other one. Both say ""support high speed NIC"" so I assume there is at least enough room for that",Neutral
Intel,"I wonder if these might make good controllers for ai powered robots. Something highly specialised, like a fruit sorter or something. I don’t know. I’m just hoping Intel can find a buyer because it’s good for the market to have more competition",Neutral
Intel,"It's a thing where the next generation of this card would be a viable competitor for the generative AI crowd, but not this one, where it can't compete with a 5 year old 3090.  For robotics, you just want inference. There are much better options focused on low power, small form factor NPUs custom made exactly for that, and they can be paired with small SBCs like a Raspberry Pi.",Neutral
Intel,No word on OMM so I guess that's reserved for Xe3-P. Wonder what other changes that architecture XE3-P will have.  The new Asynchronous RT Unit functionality in Xe3 is very cool and now Intel also has Dynamic VGPR. Neat.,Positive
Intel,"It's interesting they lump it in with B-series, given the architecture is pretty distinctly different from Xe2 used in Battlemage. Note that BMG/Celestial names have been explicitly used for dGPUs, however. They're not synonymous with Xe2/Xe3.",Neutral
Intel,"It would be nice if Intel released a small, budget friendly, passive cooled, PCIe powered dGPU with multiple QuickSync units, made specifically for AV1 encode and H.265 4:2:2 chroma with 10-bit depth.   It's unfortunate that one has to cough up $2,000+ (at least around here) for an RTX5090 to have three NVENC units at their disposal, and the 5090 still can't compete with a passive-cooled Apple Macbook that draws a couple of watts at peak.   You know things are crazy when Apple is the only company that caters to your specific needs!",Neutral
Intel,"Hello Dangerman1337! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Very bleak chance that Intel will continue with discrete gpu line up. Pretty shit decision tho,Negative
Intel,"So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In&nbsp;card under this precisely defined term.  Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – *In favour of Nvidia-GPUs?*",Negative
Intel,Xe3P needs that feature parity with RDNA 5 and RTX 60 overall since its a 2027 product.,Neutral
Intel,"hopefully that means NVL-AX is not getting axed. at least, the 24 core die. heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch. super weird, not believing too much into it. curious about Druid, people seem optimistic, idk why lol.",Neutral
Intel,yeah super weird -- but that does bode well for the next dgpu being based on Xe3,Neutral
Intel,"Arc Pro B50 was pretty interesting to me, it was a modern 75W card that isn’t gimped.",Positive
Intel,With a board with lots of PCIe x16 slots you can stack A310's or A380s and get along fine. I do 3 transcodes of av1 great on a single one right now.,Positive
Intel,The reason they'll never make this is the same reason they don't put SR-IOV on consumer cards.,Negative
Intel,Can you do this work on a $600 Mac mini? Or do you need something more expensive.,Neutral
Intel,"Yeah prob, but by how much. Looking forward to the post CES reviews and Xe3 better be a major leap over Xe2.",Positive
Intel,They will if they have xe cores that gen. They'll only fully kill it if they abandon that core development entirely to exclusively use Nvidia graphics tiles,Neutral
Intel,">Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  What in the world are you talking about. This roadmap says nothing of the sort.",Negative
Intel,"No, they are saying the GPU in Panther Lake is Xe3, but instead of classifying it as a new-gen architecture, they are saying it's just an extension of Xe2, so they've put it under the B series product family.",Neutral
Intel,100% but I fear this is another botched gen so it will at best be on par with current gen offerings.,Negative
Intel,"Intel buying chips from Nvidia won't be as cost efficient as them developing their own chips. Intel can't abandon GPU development in the short term or it'll be left behind in the long term (Nvidia can easily sell their Intel shares and pivot away from Intel leaving Intel with nothing).    If Intel will stick to making dGPUs and expand to competing in gaming with said GPUs they benefit greatly from the experience and talent that dGPU development yields, this makes dGPU development a no brainer. If AMD can afford to make dGPUs Intel definitely can as well.",Neutral
Intel,"> heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch  Would be very interesting if so, though I'd be surprised if projects get resurrected in this spending climate. Competitiveness might also be tight in '27.",Neutral
Intel,Celestial was based on Xe3p.,Neutral
Intel,"Then that can't just be a minor tweak, but perhaps it's just market segmentation.",Neutral
Intel,I don't think Mac minis support hardware AV1 _encoding_. Apple only recently supported hardware AV1 _decoding_ so their devices aren't the best suited to AV1 media,Negative
Intel,> They will if they have xe cores that gen  Why do you believe so? They made iGPUs without dGPUs before.,Neutral
Intel,"They won't Nvidia tiles are for specific market segments, they aren't a one size fits all solution.",Neutral
Intel,That's not what my colleague's say,Neutral
Intel,It wouldn't be a Helpdesk_Guy comment if it didn't include insane made up bullshit to disparage Intel.,Negative
Intel,"Exactly. *Good Lord are y'all shortsighted!* Are you falling for their shenanigans and typical tricks again?  Crucial is, what Intel does NOT mention – That is anything future dedicated graphics, as in *Add-In&nbsp;cards*.  They're purposefully declare PTL as de-facto dedicated GPU, yet any thing of future **d**GPUs is ABSENT.",Negative
Intel,"Congrats, you looked at the pictures. We all did. Now look at it *again* …  Now explain to me, why ARC alchemist is separated (since it's a line of dedicated graphics-cards, duh?!), while Battlemage gets PTL's X^e Graphics iGPU-tiles put alongside, despite such tiles classically does NOT count as a graphics-card but rather a iGPU (no matter how large).  The actual answer is, that this on the slide was surely NOT done slide by accident, but it figuratively puts PTL's X^e Graphics tiles of PTL's iGPU *on par* with their dedicated Battlemage-cards, which (at least in Intel's eyes) shall from now on signal a ""continuation"" of  Battlemage itself. Thus, there won't be any further dedicated graphics-cards like Celestial as classical Add-In cards, which still is way overdue anyway …  Since if not so, then WHY are PTL's iGPU-tiles labeled as ""Intel ARC B-Series""?! *You see the virtue signalling?*    You're also aware, that Intel recently dropped the Intel Iris&nbsp;Graphics theme and now calls all iGPUs ""ARC""?",Positive
Intel,">Intel can't abandon GPU development in the short term  Well Intel now uses the same architecture across both their iGPU and dGPU, so even if their dGPU products get killed, iGPU demand(indirectly, through Core Ultra processor demand) can fund continued GPU architecture dev. Intel still has the majority market share in the iGPU market, so they have the demand to justify continued R&D into GPU dev for iGPU alone.",Neutral
Intel,Xe3p is a significant architectural advancement says Tom Petersen.,Neutral
Intel,"Care to elaborate where I was blatantly wrong and it didn't actually turned out mostly or even exactly as I predicted prior when popping the prospect of Xyz, often months to years in advance?  A single example would be sufficient here, mind you.",Negative
Intel,"They did not declare PTL as a de-facto dedicated GPU. By definition it's *not* a dedicated GPU and they never said it is.  They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs. You're trying to fill in missing info with a nonsense interpretation.   What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".",Negative
Intel,"So much buzzwords, yet it sounds like a stroke.  You need help.",Negative
Intel,"Alchemist had both desktop discrete and mobile discrete. Intel's roadmap isn't a 'graphics card only' chart, it's an IP roadmap. That's why Panther Lake's Xe3 iGPU shows up under Arc B-series, they are saying Xe3 is an extension of Xe2, where as Xe3P (Celestial) is the next Arc family.",Neutral
Intel,"> A single example would be sufficient here, mind you.  Sure, here you go:  > So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In card under this precisely defined term.  >Their non-show is basically code for: “We're canceling everything dedicated, and our eGPUs we consider now dGPUs.”  >So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – In favour of Nvidia-GPUs?",Neutral
Intel,"> They did not declare PTL as a de-facto dedicated GPU.  Except that's exactly what they did – look at the damn pictures my friend, Intel purposefully group it into the same category as Battlemage's dedicated GPUs and Add-In graphics-cards and thus, iGPU-tiles as on par as dGPU cards.  > By definition [e.g. Panther Lake's iGPU-tile] it's not a dedicated GPU and they never said it is.  \*Sigh I *know* this, well all do! Yet Intel wants to pretend, that PTL's on-die eGPU X^e Graphics tile is now basically declared, what *Intel* now considers a **d**GPU. The implication is strikingly obvious here.  > They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs.  Ex-act-ly! You really don't get it, don't you? They deliberately leave everything what *classically* would count as a dGPU open and without ANY path forward, meanwhile their X^e 3-tiles are now considered dGPUs by Intel.  > You're trying to fill in missing info with a nonsense interpretation.  No, I don't. I'm just one of those being able to decypher Intel's typically backhanded tricks and try to explain that they're virtue signaling the worst: ARC as a line of dedicated graphics-cards is dead and won't get a follow-up, instead, Intel now considers it replaced by iGPU X^e Graphics-tiles (like the one in Panther Lake).  > What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".  Their trickery here, that they basically announce the discontinuation of ARC graphics-cards through the back-door.",Negative
Intel,I wonder why they're using such naming. If Xe3 is just Xe2 enhanced then why not call it Xe2.5 or Xe2+. And if Xe3P is a new architecture why not call it the real Xe3?,Neutral
Intel,"Just wait and see. I sadly have a horrific on point streak to remain in the right with anything Intel, even if I call things months to years in advance.",Negative
Intel,"The implication here is incredibly obvious:  ""Intel Arc B-Series consistes of Battlemage discrete graphics *and* PTL iGPUs"".  That's the implication here. It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL",Neutral
Intel,Because it's most likely still the same foundation. Xe3 just have additional optimizations and HW blocks along the lines of OMM (confirmed) and whatever new tech Intel decides is worth pushing for higher end parts.    But perhaps it's more of a RDNA 3.5 situation. Some early Druid tech backported to XE3P.    Yeah I have zero clue just guessing xD,Neutral
Intel,"> It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL  Of course not, since it would instantly tank their reputation and will kill all further ARC-sales?!  Do you actually think, that Intel is that stupid to plain announce the death of ARC-series graphics-cards? Do you think they'd even sell a hundreds cards after such a announcement? Who would by a dead-end line of graphics-cards, which won't even get any further driver-support and still have disastrous drivers like Intel's ARC? NO-ONE!  Of course they virtue signal it through the backdoor here, to preserve the sell-off the of the rest of pile of shame of their B-series graphics-cards as long as possible (leaving users with none support afterwards), and *only then* pull the plug afterwards when they went through their inventory …  Since when are you following Intel? Intel has always done so, and only announced a sudden discontinuation once the inventory was sold (leaving customers in the open with no further support overnight), despite virtue signalling a continuation all the time (to preserve a proper sell-off).  ---- Wasn't this exactly the same story with **Optane** back then? Each and every question upon a future support was answered in the affirmative, only to pull the plug overnight again as usual …  I'm not another piracy of cons here, it's Intel pulling the same Optane-stunt with ARC – Happily selling off their cards, knowing darn well, that they will pull the plug, yet refuse to actually play with open cards again.",Negative
Intel,"> > It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL >  > Of course not  Great, we're all agreed that you're making shit up when you said  > Panther Lake now Intel classifies as a dGPU",Negative
Intel,Just letting it be known that the [original source](https://www.reddit.com/r/IntelArc/comments/1nomuhk/driver_built_xess_frame_generation_might_be_on/) is over at r/IntelArc,Neutral
Intel,"Intel ""ExtraSS"" frame extrapolation tech has been in development for quite a while. I really hope it really does come with B770. Will be exciting to see how the implementation differs from MFG.",Positive
Intel,Could be? Wasn’t one of the lead developers for Asahi Linux’s graphics stack hired by them for this specific purpose?,Neutral
Intel,"Yet more evidence Intel is likely not scrapping arc, as if we couldn't guess they know the trajectory of past nVidia semicustom affairs.",Neutral
Intel,"Nice to see. Still pretty happy with my Arc card, good enough for game and resolutions I’m playing and the AV1 hardware encoder is just excellent.",Positive
Intel,Wonder if well be getting ray reconstruction andmaybe even a transformer model soon.,Neutral
Intel,we are witnessing the downfall of pc gaming in real time,Negative
Intel,Why should they? They are going to buy NVidia GPUs for everything now.,Neutral
Intel,"Hello brand_momentum! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,This could be awesome more completion The better,Positive
Intel,Doubt anyone would use it for practical application. FSR Frame Gen is awful to use.,Neutral
Intel,"Multi-Post News Generation, with three articles interpolated per source.",Neutral
Intel,I'm excited for Intels new GPU,Neutral
Intel,"Microsoft has been way too slow in the DX12U era, and Kronos is even slower. Microsoft's new APi that integrates AI upscalers isn't even out yet and it doesn't even cover Frame Gen or AI denoising. By the time MS moves to integrate those they'll be behind on something else.",Negative
Intel,"They aren't, though. FSR Frame Gen works on Nvidia Hardware",Neutral
Intel,Honestly we are gaining more from proprietary features than we lose from them being locked behind certain brands. Currently them being proprietary gives Nvidia an incentive to push the envelope and the head start they have isn't that big so they're incentivized to keep making new features   If everyone was required to share their new features then the incentive for innovation disappears,Neutral
Intel,Competition *,Neutral
Intel,Obligatory article quoting reddit post quoting another article quoting original reddit post.,Neutral
Intel,"Fake frames, fake articles! /s",Neutral
Intel,Sometimes I prefer vendor solutions today than waiting for microsoft for years like Direct Storage,Neutral
Intel,"I mean, surely it depends on when they have to share it. If they can release with the feature exclusively first, but then have to share at or after release, then it can still help the current generation of their cards until it's time for the new gen.",Neutral
Intel,"DirectStorage 1.1 (the actually usable version unlike 1.0) was supported by Windows 9 months after PS5 release. They werent that far behind on that, noone just was interested.",Neutral
Intel,"patents expire after 15 years, they will have to share it then.",Neutral
Intel,Nvidia controls 90% of the discrete GPU market on PC.   Anything proprietary that they make essentially becomes the standard going forward.,Neutral
Intel,"Huh?  1. RT in games is now done through DirectX APIs, which are vendor agnostic.   2. For GPU features, NVIDIA usually introduces them through a DirectX standard, they are just usually the first vendors to support it until others catch up.   3. RT is limited because current gen consoles are not good at RT. AMD took multiple years to adopt RT or AI acceleration cores.",Neutral
Intel,I actually have not seen an open source or even cross vendor solution with any reasonably quick uptake. Its a theoretical myth at the moment,Negative
Intel,the issue with RT isnt standard. Its that consoles/amdGPUs simply didnt support it for away too long.,Negative
Intel,"...It's down to 15 now? I thought it was 20.   Although. Software is more copyright than patent anyway, and copyright is, like, 70 years after the author dies, or 95 after publication for corporate works.",Neutral
Intel,"Depends on country/industry. Copyright also depends on country, but for most civilized word its authors death + 95 years (thanks disney). Copyright however has many exceptions to it, well, at least in the west. In Japan it does not and technically you can get sued for saying Zelda is fun because you broke copyright by saying Zelda without permission. Its why Nintendo is able to get switch reviews removed from youtube.",Neutral
Intel,The DirectX RT spec and SDK came out in 2018. The blame lies entirely on AMD. Imagine Nvidia waiting for AMD to agree,Neutral
Intel,I would imagine intel has to at least get celestial out the door before cutting off their GPU line. it's been so long in development that it would be more of a waste of money to cancel it then finish it and see how it sells. at least getting it to the public means some kind of ROI and a solid position in the lower end of the GPU market that AMD and Nvidia are abandoning to the used market.,Negative
Intel,"Okay then, what's the Xe GPU roadmap looking like then?",Neutral
Intel,"Hopefully Intel still thinks it needs discrete GPUs to compete in DataCenters and such.  Also GPUs for the ""edge""",Neutral
Intel,I can't claim to know the future or fate of Arc but I think if Nvidia wanted to kill it they could just price it out of the market instead of buying a stake in a company with a broadly uncertain future,Negative
Intel,They barely lived on before the deal.,Negative
Intel,"It'll live on our hearts, yes.",Neutral
Intel,Lol if you believe that I have a bridge to sell you in Brooklyn,Neutral
Intel,"You have to look at this from both sides.  This would give NVidia the ability to offer OEMs a tightly integrated on SOC gfx solution vs a separate discrete gfx package as it is now. This would lower costs for NVidia's customers as well as improve performance and Intel's cpu's get to ride that train.   Intel has a long track record of using integrated gfx so it's certainly right up their alley to take this on.  Could Xe gfx coexist with NVidia? Yep, I think so.  Xe could be the budget igfx option and iNVidia is the premium integrated solution. That's basically how it is now with Intel Arc/Battlemage & GTX.   On the DC side, Intel has absolutely no gfx solution at this time and won't have anything for at least two years.  Intel already sells NVidia when a gfx solutions are needed so tighter integrated products will benefit both sides in that DC space as well.",Positive
Intel,"What's weirdest to me about this deal is. They specifically mention NVIDIA chiplets being packaged with Intel CPUs.   Now, with Meteor Lake and Arrow Lake, the CPU and the GPU are different tiles packaged together. But for Lunar Lake, the GPU was included in the same tile as the CPU, called the compute tile, because that was more efficient.   Supposedly, Panther Lake is using a different tile setup, with the GPU being separate again, or with the memory controller being on the tile with the CPU, but.   Either way, it feels to me like relying on separate NVIDIA chiplets for the integrated graphics is a bit of a limitation to how the overall architecture of the SoC can be designed.   My understanding is that, if another company buys Intel, that messes up the cross licensing agreement, and Intel might lose x86-64. So, even if NVIDIA keeps buying Intel stock, the most they can get is 49%.   And NVIDIA is too big now for Intel to buy, so the other way around wouldn't work. And even if NVIDIA wants to keep Intel's fabs open, I doubt they want to run them either.",Neutral
Intel,Wonder if this investment is purely to keep intel going to avoid a monopoly issue in a few years.,Neutral
Intel,AMD is not going to release a next gen gaming gpu it just leaked and that’s what I’m worried about intel arc are ramping up.,Negative
Intel,"There's also a better then not chance this alliance would have gone the same way as most other nVidia semicustom things by the time Druid tapes out anyhow, and the leadership at Chipzilla likely know this.",Neutral
Intel,"While that might be true in the long term, it's not necessarily true for whatever quarter the execs are caring about at any particular moment.",Neutral
Intel,"Probably a chronologically changed version of what is known now.  No one with any savvy expects this alliance to last with nVidia's track record, so carrying it on makes sense in both being ready for the inevitable burnt bridge and possibly pushing it back.",Neutral
Intel,It's looking for 6 years of Xe3 in integrated graphics I'd guess.,Neutral
Intel,"Always selling out actually, they just can't produce that much",Negative
Intel,And I have another if you think nVidia is capable of keeping this deal running for that long...,Neutral
Intel,Intel will hopefully split their fab business from the rest of the company either way.,Neutral
Intel,"Im not sure what you are implying, but nvidia partnerships have historically not broken from their side as far as I know.   Apple gave them the boot, not the other way around.   AMD are the ones who rejected the Nvidia-AMD merger back in 2005.   Tesla kicked nvidia out as well, because they did not want to keep paying them to develop the tech.   If you are referring to AIB Partners, then that can hardly be called an actual partnership. That's more of a contracted labor and distribution agreement. Even then, XFX only got blacklisted because they were a premier partner who were seen as jumping ship when the going got a little rough with nvidia stalling between 2009-2010.",Neutral
Intel,> they just can't produce that much  because they are losing money on them,Negative
Intel,Trade bridges.,Neutral
Intel,Intel's arc is dead with or without nvidia deal.,Neutral
Intel,"Microsoft went with Nvidia for Xbox and it ended in a lawsuit.   Sony went with Nvidia once, and never again.  Apple ceased to bundle Nvidia GPUs because Nvidia sold them (and HP and Dell) huge quantities of faulty 8000M GPUs.",Negative
Intel,There is a reason why every nvidia partnership ends up breaking apart pretty quickly...,Negative
Intel,"Nvidia seems to make working with Nvidia too difficult for their partners to continue. It's the business version of someone emotionally bailing, and abusing you until you leave them.  They did that with smartphones, consoles, Linux, Apple, and likely countless other projects I didn't follow in as much detail.",Negative
Intel,"That really is up for debate. That they are losing money on them when you account for RNE and RnD amortized by unit, sure. That I have no doubt about.  But that is not the same thing as losing money on each card sold based solely on the bill of materials and distribution cost etc. Which is what actually matters when looking at if a product is ""sold at a loss"" or not.",Negative
Intel,"Hardly. Even after the bubble, Intel needs in house GPU, for both APUs and data center... which means they'll make a gaming version if the work is already mostly there.",Neutral
Intel,"In the case of Intel and Nvidia chipsets, it was Intel that decidedly ended the relationship, paying Nvidia off to exit the chipset business so that they could make integrated graphics.",Neutral
Intel,"Yeah, if nVidia has an Achilles' heel, it's their company culture's near total inability to do semicustom without fuckery, and it's the reason AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops.",Negative
Intel,Well yes. But you do need to amortize the tape out and dev. costs over the production life cycle of the product. And with low volumes I'm not even sure AMD is accomplishing that. AMD is saved due to the console business funding dGPU tech development.,Neutral
Intel,"They've already cancelled every data centre GPU after Ponte Vecchio (which was forced out by DOE contract) despite reassuring us Rialto Bridge and Falcon Shore were on track as little as months before the chop, and their next is apparently Jagure Shore in 2028.   I have 0 hope that will actually happen.",Negative
Intel,">AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops  And how much money is there in your ""everything else outside of ..."" vs Nvidia's entirety of consumer prebuilt desktop/laptop?",Neutral
Intel,">Well yes. But you do need to amortize the tape out and dev.   If they are selling above the manufacturing and distribution cost, they are amortizing those costs. Might not be much, but it beats not selling them.  They might still be ""losing"" money on them when all costs are included. But it would still make sense to make more of them and sell them, since each one sold still plugs that black hole of externalized costs. And would not be the reason for lack of availability.",Neutral
Intel,"*looks at effectively every current gaming system that isn't a PC, Intel claw, a phone, or the Switch*  A fair bit, and frankly, even if it's not that profitable, it defines more of the dynamics of this sector then you'd think if you're a PC type.",Neutral
Intel,"Not only is it not profitable for the chipmaker, the revenue isn't even anything to brag about looking at AMD's gaming section of their financial statements. 1.1 billion for Q2 2025, and that includes whatever miniscule sales there were for Radeon dGPUs.",Negative
Intel,This is the card Intel should have released instead of the gaming version(s). Took them 2 years,Neutral
Intel,Far more than I expected them to come out at. Damn.,Negative
Intel,"Not worth it at $599, slightly worth it at $499  This has 1/2 the memory bandwidth of a 3090 and 70% the compute performance.",Negative
Intel,I’m getting one when it releases in Australia,Neutral
Intel,Thats great and all but when will there be stock? (Canada),Positive
Intel,Pytorch libs are getting better for intel and amd support but there are still a lot of 3090 cards on the used market and those don’t need hoops to jump through to get compute working.  If I was handed a budget this low by a boss I’d just ask them to buy a lot of 3090 cards.,Neutral
Intel,I'm disappointed.  My order was canceled,Neutral
Intel,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"The gaming cards require significantly less robust driver and application support. Releasing them first makes sense when they still needed to work on said driver and application support, the timeline likely wouldn't have meaningfully changed.",Negative
Intel,b580 should've allowed SR-IOV upon release. That's all the card really needs to sell.,Neutral
Intel,"Even at $599 it's significantly cheaper than a used 3090, comes with a full warranty, and can be bulk purchased new.  If you just need the VRAM, which is what this is targetting, I think it can definitely make sense.",Positive
Intel,"This is worth it at any price point.  I am getting tired of gamers not understanding that SR-IOV support and 200W are very important to some people. If there were a 24GB AMD card out there for 200W and $600 I would buy that too regardless of SR-IOV support.  Neither Nvidia nor AMD have a card at this price point with this feature set. You are looking at $600 for 24GB of VRAM and SR-IOV support. AMD's SR-IOV offerings are in the ""thousands of dollars"" and Nvidia's cards in this segment do not meet the 200W and 24GB of VRAM targets. The closest thing is the P40 and it's a decade old, has way higher power consumption and is about to be end of lifed.  Please tell me the card that exists in the same segment as the B60. I will wait.",Negative
Intel,"This is $150 cheaper than NVIDIA's competing A2000, which has less VRAM.  3090s do not get certified drivers, which are a business requirement for certain workloads.",Neutral
Intel,The 3090 does not have ECC on it's VRAM nor certified drivers,Neutral
Intel,Totally not worth it.,Negative
Intel,it's worth it for enthusiast collectors who want to own a rare Arc GPU. Especially if Intel abandons dGPUs. I'm sure Raja Koduri will buy it.,Positive
Intel,"I'm more looking forward to the B50, but obviously local pricing is everything.",Positive
Intel,"Yeah, the website now says: ""This GPU is only available as part of a whole system. Contact us for a system quote.""",Neutral
Intel,"It has SR-IOV, certified drivers and other professional features...",Neutral
Intel,"Graphics cards are in fact seperated into two broad categories, consumer and professional, used by all the main manufacturers. They have completely different approaches to their design, different use cases, different drivers, different memory etc.",Negative
Intel,">The gaming cards require significantly less robust driver and application support  Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  It was like that since A770 release. When some games straight up don't work or have unplayable bugs, you are literally paying for own suffering.  That said, B50/B60 are good, though i wish B50 worked with ""Project Battlematrix"" that is Intels NVLink. B60 was also expected to be for 500$, not 600$, but it is what it is.",Negative
Intel,Both nvidia and amd only take a few months between releasing gaming and pro lines of new gpus. Intel managed to do the same with their first gen (only a few months). How come the second gen takes 1 year!?,Neutral
Intel,"...Do integrated graphics support SR-IOV? Do any of Intel's iGPUs support the ""pro"" version of their drivers,   Like, obviously iGPUs have less compute power and less bandwidth than anything dedicated, but. Part of me feels like iGPUs should actually have the features of the ""pro"" cards, given they aren't replaceable, with the actual pro cards providing performance.",Neutral
Intel,"Are used 3090s that expensive?  Here in Chile they go for about 550-600USD. Got one (luckily) at 450USD some weeks ago for AI/ML, without the box and such but it works fine.  Asked a friend from Spain and it seems there they go for about 600 EUR there.",Neutral
Intel,Do not underestimate the lack of CUDA.,Negative
Intel,"Bought my 3090 for MSRP 4.5 years ago and honestly would never have imagined that it would hold 50% of its value.  I want to get another one when they drop to $200 or so, so that I can finally be disappointed by SLI and make 14 year old me happy by finally clicking the bridge onto two GPUs and firing up a benchmark, but who knows if/when that day will come.",Neutral
Intel,A used 3090 is only $100 more.,Neutral
Intel,"It barely makes sense, at $600.  3090s are $700 now and the new 5070Ti Super (faster than a 3090) is expected to come out at $750 soon. And you’d have CUDA for either the Nvidia 3090 or 5070Ti Super.   I strongly believe that $600 is the price Intel set to ward off the scalpers. The niche of the B60 is pretty tiny at $600.   It’ll make a lot more sense at $500, which I suspect is the “real” price a month or two after it goes on sale.",Negative
Intel,Used 3090's are cheaper than this lol. Did you check the sold prices on Ebay at all? I can literally buy them at buy it now prices that are less than this.   94+ upvotes lol. 3090's sell for about the same price as 4070 supers as they have the same gaming performance.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1,Neutral
Intel,yeah it's got a good feature set for its price. people looking at it from a gaming product frame of reference rather than a professional product frame of reference are doing it wrong,Positive
Intel,"SR-IOV alone makes this a substantially different offering than a used 3090. If you know, you know.",Neutral
Intel,Atlas 300i duo,Neutral
Intel,"Also 3090s are $200 more, are used, consume twice as much power (400W card BTW), and do not have SR-IOV support.",Neutral
Intel,"It's cheaper than Nvidia's RTX Pro 2000, but it's also slower, but have more VRAM.",Neutral
Intel,Businesses aren't buying these cards these are destined to hobbyists home labs. The extra money to buy a CUDA supporting card is trivial for businesses they aren't going to buy Intel.  Certified or not the 3090 works wonderfully with all AI software out there while Intel does not.,Neutral
Intel,Yeah it was very scummy imo,Positive
Intel,"> R-IOV, certified drivers and other professional features  Did Intel ship the SR-IOV part? Last I read (been a while) it was ""coming soon"".",Neutral
Intel,It was meant to be a joke. Not so funny I guess.,Negative
Intel,"B580 needed to get out very early to beat RTX 5060 and RX 9060 to the market. Nvidia's and AMD's cards outperform it in gaming raster performance thanks to having been in the market for much longer. They have the more efficient architecture and software for gaming. It wouldn't be great for marketing if Arc came out at the same time with similar pricing, yet got beaten in the FPS benchmarks.  But productivity cards could wait. Arc's always been very competitive in productivity tasks. Maybe because Intel has more experience in this department. So reliable driver is the focus here.",Positive
Intel,">Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  Good driver and application support is not possible to deliver without a significant install base of your product in the wild. PC configurations vary wildly and usually driver issues are about some bad luck combination of hardware, driver versions, and application versions.  The incumbency advantage in this market is immense. Devs will validate that their stuff works on all manner of configurations including an Nvidia GPU, so they get the perception of having bulletproof drivers.",Negative
Intel,AMD and Nvidia have been having extensive driver support for years  ARC is a much newer platform requiring more careful hand holding,Neutral
Intel,Why does this matter?,Neutral
Intel,"Some iGPU do support SR-IOV, I think 12th generation and newer. Motherboard, driver, and OS dependent.",Neutral
Intel,Was seeing one sold locally recently for $800-$900. They very much are still expensive here in the US.,Negative
Intel,"Yup 600 Euro is about (or slightly above) what used 3090s are sold for here (Sweden) as well. I would rather buy a significantly better card used than a worse one new, if sold at approximately the same price point.",Neutral
Intel,"Used site jawa.gg shows 3090s priced from $750 to $950 (and of course, some people asking $999 or $1200)  https://www.jawa.gg/shop/pc-parts-and-components/geforce-rtx-3090-KNRXB30U",Neutral
Intel,Used market is freaking insane. It is better to grab new or open box.,Negative
Intel,Over here used 3090s are sold for 500-600€.,Neutral
Intel,"Well sure if you specifically need cuda then this won't work, but there's also a large subset of people who just need vram for as cheap as they can get it. And for that purpose this Intel card is interesting at least.",Neutral
Intel,Dude that feeling is so worth it though. I did that same with 2 Titan Xps a couple months ago and the build just looks awesome. Great performance on pre 2018 / SLI supported games as well.,Positive
Intel,Just keep on digging the hole man.  A 5070Ti or 5070Ti Super or 5090Ti Super Extreme Titan do not have Support for SR-IOV. You do not know why people want these products.  > Muh niche  **Gamers** are the niche. Have you been living under a rock for the past 5 years?!,Negative
Intel,the super gpus are not expected to release soon?,Neutral
Intel,"Why have you posted this same garbage about a 5 year old used card so many times in this thread? It's a valid point, to some people I'm sure, but it's like you're weirdly obsessed with making it until someone hears you.",Negative
Intel,"....are you mistaking the listings of broken cards as new, working ones?  because when you take those out you'll see pretty quick that all of the 3090's here are selling for FAR above $600 lol.",Negative
Intel,Used 4070 Supers dont sell for nearly $700+ on the low dude.,Neutral
Intel,Yes running business of used cards is how its done.....,Neutral
Intel,no fee SR-IOV makes this a substantially different offering then AMD or Nvidia.,Neutral
Intel,Typically run at 250W though to be fair.,Neutral
Intel,"They are less not more, no idea why people are making up used prices when we can check ebay sold listings.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1",Negative
Intel,not on the Vram like professional cards,Neutral
Intel,Q4.,Neutral
Intel,I thought it was funny ¯\_(ツ)_/¯,Neutral
Intel,The b580 is still good at $250 and it’s only recently in the past month the 5060 and 9060xt 8gb dropped near or at $250 in some instances.  It’s a good card at 1440p aswell and it has 12gb of the other 2 8gb cards. Only problem I have it is the windows 10 drivers being abysmal dog shit if you’d want to play any current game with stuttering and crashes.,Positive
Intel,"Yeah, apparently a year’s worth of it",Neutral
Intel,Because they're super late to the party.,Neutral
Intel,"Wow, 800-900USD after 5 years is more than I would have expected.",Positive
Intel,"No one on hardware swap is willing to go above $600 for a 3090, trying to sell one now. This reminded me to just put it up on Ebay.",Negative
Intel,I have no idea why people make up stuff like this when we have ebay sold listing search.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  Most recent one sold for $439.00 including delivery.  Again why are people making up used prices?,Negative
Intel,For what exactly do they need vram without cuda ?,Neutral
Intel,"Look at my profile. I have 0 comments in gaming and hundreds of AI comments. What do you know about running inference locally? Why do you think I mentioned memory bandwidth? Gamers don’t care about that. I haven’t played a game in years.   This is not a good deal for AI running consumers at this price, and a mediocre option for enterprise.",Negative
Intel,Gamers are not niche. They are 20 billion a year business.,Neutral
Intel,"False, they’re releasing in december or jan. So about 3 months from now",Neutral
Intel,"> Typically run at 250W though to be fair.  At which point we have to derate the performance. You know, to be fair.",Neutral
Intel,"The 3090 Ti did, but the standard 3090 did not.",Neutral
Intel,"It's a really good general purpose card. Perhaps not the best card if you want to squeeze out the most fps out of games, but it performs really well in a wide range of tasks including gaming. And yes 12GB VRAM really comes in handy.  It can easily be found for under $250. B580 is still cheaper than 5060 and 9060 XT and even RTX 5050 and RX 7600, especially in the UK where you can find a B570 for only 20 pounds more than RTX 3050. That's way cheaper than RX 6600, it's a no brainer. But the prices could only go down thanks to economy of scale mostly. It's been produced for a while, that's why.  Also one of the big reasons it's still selling now is thanks to good publicity. Imagine if it came out as the same time as RX 9060 XT. HWU would tear a new hole regardless of how much of an improvement it is over the Alchemist cards and the impressive productivity performance. Those guys literally don't care about anything other than pushing a dozen more frames in a dozen personally chosen games. Gamer Nexus and Igor too to some extent. It would be bad publicity.",Positive
Intel,"If intel felt they could have released this safely earlier, they would have",Neutral
Intel,People do not understand AI.  People buy up used 3080s and use 4 of them in an AI Rig to run MoE models. 96GB of VRAM can run you basically anything that's not the full 500GB models like Kimi K2. The power consumption doesn't matter because they're making more money off of the inferencing than the cost of electricity and it's cheaper than cloud providers.,Neutral
Intel,"Where did you find that? FB? Im saving for a GPU here in chile too, was aiming for a 5060ti 16gb but the model I want is 630k in ... Idk the top result in solotodo",Neutral
Intel,3090s will be much less attractive when the 5080 super and 5070ti super launches.  If the 5070ti super is 750- 800 with 24gb for a new faster GPU with a warranty it hel used market will have to shift.  The 3090s have nvlink but I'm unsure how much that matters for most people.,Neutral
Intel,https://www.ebay.com/itm/156720971107  That's a 3080 not a 3090 lol.,Neutral
Intel,"rendering, working on big BIM / CAD models, medical imaging,...",Neutral
Intel,CUDA isnt the only backend used by AI frameworks,Neutral
Intel,"I use opencl for doing gpgpu simulations, this card would be great for it",Positive
Intel,You mean your 0 profile history because you have it private?,Neutral
Intel,"And AI isn't the only use for a workstation card either, jfc.  3090s and 5070 Ti Supers don't have SR-IOV  3090s and 5070 Ti Supers don't have ECC VRAM.  AutoDesk for example will tell you to go pound sand if you try to get support and you aren't running a workstation GPU with ECC VRAM. So for businesses running AutoDesk software in a professional capacity, or businesses needing SR-IOV, you could offer them a 3090 or a 5070 Ti Super to them for $100 and it still won't be an option for them, because for their needs they might as well be paperweights.",Negative
Intel,I thought we were talking about actual hardware that was available in the referenced country (US) and not a banned Chinese product that may or may not even exist and if it existed would cost nearly 10x as much as this product.,Negative
Intel,Spoken like a true gamer.,Neutral
Intel,last leak on them is that they’re delayed. Haven’t seen any reports since then that they’re releasing in q4,Negative
Intel,You still get about 85% performance compared to stock settings.,Neutral
Intel,"Of course, but that’s the point. It took them an excessive amount of time to “feel” like they could have released this",Negative
Intel,What are they doing that's making them money? Or are theu selling the compute somehow?,Neutral
Intel,"People do not understand Ebay sold listing search showing them selling for way way less than the values you guys just pulled out of your assholes?  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  They sell for roughly the same price as 4070's as they have roughly the same performance, AI don't matter gaming is still king in this market.",Negative
Intel,"Sip, marketplace.  Si buscas por RTX 3090 vendidas, es una PNY 3090 a 450K, y todo perfect.  La 5060Ti es más lenta que la 3090 para juegos, la 5070 es un poco mejor o igual.",Neutral
Intel,">rendering  Rendering what? Because if you're talking from a 3D art perspective, all our software is leaps and bounds better with CUDA or outright requires it. You can't even use Intel GPUs with Arnold or Redshift, so good luck selling your cards to a medium scale VFX house that heavily use those render engines",Neutral
Intel,>CUDA isnt the only backend used by AI frameworks    It is not the only backend for all AI frameworks; it is the only backend for the majority of AI frameworks.,Neutral
Intel,It is the only functional one.,Neutral
Intel,"It's the only one that's always well supported and expected to work with decent performance.  We're months after RDNA4 was released and ROCm still doesn't have any winograd kernels for it, which means 3x3 convs, the most common kernel size, are 2 to 3 times slower than they could potentially be...  I've close to no experience with OneAPI and SYSCL, but I'd imagine AMD should be ahead considering they've been playing the game for way longer… And if ROCm is a nightmare then I can only expect the experience with Intel to be as bad, if not worse.",Neutral
Intel,"Oh good, I’m glad I set that private. You can literally google the username anyhow and see 1-2 example posts that underscores my point.",Positive
Intel,0 people are using autodesk with an intel arc gpu lmao,Negative
Intel,"You know nothing about the ai market, huh?  https://www.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/",Negative
Intel,"Oh no, im part of 60% of world populatioin. how terrible.",Negative
Intel,Which is 36% higher performance per watt.       ... to be fair.,Neutral
Intel,The reason they didn't is because it would have been worse to release it earlier,Negative
Intel,Software Development,Neutral
Intel,Why did you spam my replies about cherrypicked results. The cards are worth $800-$900 and the listings you linked literally prove htat besides a few outliers that are $600-$700 to tank the prices.,Negative
Intel,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",Neutral
Intel,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",Neutral
Intel,"Depends on what software you use, but your right that NVIDIA with Cuda is safe bet when is comes to rendering. I use Enscape and that works great in my AMD gpu",Positive
Intel,Which ones?,Neutral
Intel,"Intel is a bit better for common models as they actually pay frameworks and model makers to optimize for it, unlike AMD.  I use flux and sd models and it’s much better on my arc than my Radeon 9600Xt",Positive
Intel,Private profile = Complete troll.  You're not an exception to this rule.,Negative
Intel,I too like to promote Chinese propaganda which isn't available in the United States due to the whole AI cold war going on and the fact that Huawei products have been banned in the US for what? 15 years?,Negative
Intel,"Right man, my point is that it shouldn’t have been",Negative
Intel,EILI7? How are software engineers using GPU ~~imprints~~ inference on their home machines for software development? What am I missing?  E: a word.,Neutral
Intel,"Hell, one of them was just the cooler without the actual GPU.",Negative
Intel,"En ese claro entonces tal vez valga la pena esperar a la 5070 Super, que supuestamente tendrá 18GB de VRAM.  Sobre comprar usado, todo bien en general, pero siempre hay que tener ojo y ojalá siempre probado. Yo fui a la casa del vendedor para revisar y todo bien, pero igual puede ser peligroso, por eso revisar bien los perfiles.  He comprado hartas 3090s usadas por FB Marketplace.",Neutral
Intel,for example every local image and video generation system I've seen.,Neutral
Intel,"you’re wrong about cost, availability, performance, and just plain wrong overall lol",Negative
Intel,"The Chinese are busy making hardware, not propaganda. These are old cards that have been around for a while, I can assure you they are quite real.",Neutral
Intel,According to whom? With what evidence?   Intel spends more on r&d per year than nvidia and amd combined   Do you think they are just being lazy?,Neutral
Intel,"The many hours of trial and error getting things to work, and building expertise with said labor would be my guess.  This isn’t home software engineers buying up all the inventory, its people getting around sanctions.",Neutral
Intel,Getting good LLMs crammed onto GPUs for coding is a popular topic on /r/locallama.,Positive
Intel,And the one directly under that was the actual PCB...,Neutral
Intel,Im running Comfy UI + Flux on my Strix Halo right now without issue 🤷‍♀️,Neutral
Intel,Propaganda is 90% of chinas economic output though.,Neutral
Intel,r/homelab,Neutral
Intel,"The 9060 xt should be 30-35% faster than the b580 depending on games and resolution. To me, that's definitely worth a 14% increase in cost.  8 GB does mean the card won't age as well, but it doesn't mean the card is suddenly worthless. There are very few games where 1440p simply won't run on 8 GB of RAM. With the RAM crisis only going to get worse, my hope is that developers will put a little bit of effort into optimizing vRAM usage.  (edit: and based off your pricing, the 16 GB should only be about $50 USD more. That is probably worth it because not only will it perform better now, but you'll get a couple years longer out of it, and it will have higher resale later.)",Positive
Intel,"Idk about these specific cards but a few years ago I got a 1650 4gb and my buddy got a 1060 3gb  When he played games the fidelity was great but had some stutters here and there, just cruising at 70ish percent  Mine would be screaming at 99% usage with lower fidelity but boy oh boy was it significantly smoother  I'd go with more vram",Neutral
Intel,"VRAM usage is based on your resolution, mostly, in conjunction with game settings at that resolution.  If you plan on gaming at all at 1440p, do **not** get a card with 8GB of VRAM. This isn't a future issue, this is a now issue. It currently does not have enough vram for 1440p gaming in nearly any new heavy titles, including battlefield 6 (just as a recent example).   Hell, at 1080p my 8gb card was over capacity on battlefield 6. There's an argument to be made that it isn't enough for future 1080p gaming as well.  Can you do plenty of gaming on an 8gb card? Yes. Can you run every game at max settings? No, not even on 1080p. You'll have to turn settings down. Which frankly I'm in the boat of ""not usually a big deal"", but it's not good when you pay 300+ bucks for a new piece of hardware that can't run your games at full tilt.   It is more powerful than the B580, yes. The B580 will last longer because of VRAM, also yes.   My suggestion is really think about how long it will be until your next upgrade and make your decision based upon that. Longevity, go with intel. More power now for just a couple of years (2 or 3), consider the 9060XT/5060 (at 1080p only, skip entirely for 1440p). Or look at their 16gb variants.   You could also look at used options. You can find a 6750XT 12GB for 200-250 USD typically, as an example.",Neutral
Intel,"in that price range I think you can find good used deals. also maybe take a look at the rx 6800 xt (or generally gpus from 1 or 2 generations ago). I have one since 3 years or so and works like a charm (you will lose fsr 4 compared to the 9060 xt, but it has 16gb vram and better performance) in my country (in central europe) I saw some sold for about $250",Positive
Intel,"It depends on what games you play and refresh rate not just resolution.  As you mentioned, it's not just a matter of the amount of VRAM, otherwise a 1070 or 1080 Ti would still be used -I bring it up because I just upgraded from a 1070 :)  I would spend the extra $40. I think the RX 9060 XT 8GB would also have more resale value.",Positive
Intel,16gb,Neutral
Intel,"9060xt 16gb, is the one you should get if you are planning for 1440p in the future. Get that one that future you will thank and appreciate in the long run.",Positive
Intel,"definitley wait and save up for the 16gb of the 9060xt, it will be worth it",Neutral
Intel,If you're on PCIe 3 the 9060 is the only choice here.,Neutral
Intel,"So recently I bought a new prebuilt that came with a 5060 TI 8GB. For my old system, shortly before, I'd purchased a 9060 XT 16GB.  I haven't had an AMD card in many a year, and I was inclined to be generous to NVIDIA, thinking ""Hey, they've got DLSS, they've got multi-frame gen, yadda yadda yadda...""  So I'd been playing Borderlands 4 on my old rig with the 9060 XT when the 5060 TI system came in a few days ago. I tried playing Borderlands 4 on the new PC with DDR5 RAM and a much better processor, and for every game I tried (Witchfire, Borderlands 4), it was a much worse experience.  DLSS frame gen treated me well on my old 4060 so I thought  multi-frame gen would be rad and close the 8GB-16GB gap, but holy shit the lag was like improperly configured Lossless Scaling. It was awful, and I am NOT generally sensitive to latency. I really learned that without enough native frames to feed frame gen, it's not even a feature worth using.  I suspect had my new PC had the 16gb version of the 5060 TI it probably would have run everything as well as, if not better than, the 9060 XT.  As an example, on my new PC with the 5060 TI, Borderlands 4 was running like less than between 35-50 fps native on medium settings. With the same processor, same RAM, but a 9060 XT 16GB, it runs natively in the 70s and 80s on high settings.  Now, I do only game at 1080p, mind you, so that factors in as well, I suppose.  I was a true believer that 8GB was still enough in late 2025, especially with AI assistance, until this past weekend. I would urge you to go with more VRAM. I know, personally, I'll never buy another GPU with 8GB VRAM again after I see how impactful it is.",Neutral
Intel,"The whole vram issue is valid. But, ultimately, all you can do is look at the games you want to play. For the vast majority of current games, the 9060xt performs better and not by a tiny amount. I don't get why you'd take 30% worse performance now for the hope that in 3 years' time you might get better performance.",Negative
Intel,Especially if 1080p is still your target then I'd go with the 9060 no doubt about it.,Neutral
Intel,"Neither, get the 9060xt 16gb version. Stay away from 8gb.",Neutral
Intel,Tell OP they can get the 9060Xt in 16gb,Neutral
Intel,"Having same issues, having 40-60% decrease in fps across games. Along with stutters, and fps drops. Hard to run even easy to run games like roblox and LoL. Insane that a windows update caused this somehow.",Negative
Intel,"I think I have an restoration point, but damn it sucks",Negative
Intel,It's working perfectly fine after I rolled back the latest security update.,Positive
Intel,the H6 looks really great with 140mm fans in the bottom. are those reverse blade fans or are they installed backwards?,Positive
Intel,"thanks it took me so long to find them, i got them off aliexpress lol",Positive
Intel,Your list is private,Neutral
Intel,"It looks like you may have posted an incorrect PCPartPicker link. Consider changing it to one of the following:  * [Use the Permalink](https://i.imgur.com/IW0iaOm.png). note: to generate an anonymous permalink, first click [Edit this Part List](https://i.imgur.com/uqDIcdt.png).  Or, make a table :    * [new.reddit table guide](https://imgur.com/a/1vo0GHH)   * [old.reddit table guide](https://imgur.com/C86vdxB)         *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/buildapc) if you have any questions or concerns.*",Neutral
Intel,Your list is Private,Neutral
Intel,Ur list is private,Neutral
Intel,"Your list is set as private, so we can’t see it.",Neutral
Intel,"No use getting a 9800X3D (high end) CPU with a B580 (low end) GPU.   If this is for gaming, I would move some money from the CPU and the motherboard to the GPU.   What is your total budget? Do you have a Micro Center nearby?",Neutral
Intel,"Everything but the gpu is fine.  I saw in a comment you are close to a micro center.  Consider dropping to the 7800x3d bundle or even the 7600x3d bundle and bumping the gpu up to the powercolor 9070xt IF you are hard stuck at $1500 budget   If you have a little wiggle room, you could go with all micro center parts:  7800x3d bundle is 580 (580) -7800x3d -ASUS Tuf gaming b650e -gskill flare 32gb ddr5-6000  Powercolor rx 9070XT is 580 (1160)  Inland QN450 1TB nvme ssd is 100 (1260)  Corsair RM850 psu is 110 (1370)  Peerless assassin is 50 (1420)  Leaves you 80 for a case.  I like the Montech XR. Just used it for my kids build (almost identical to this one) and it’s got good cable management and was easy to work in.  Only 70 bucks too.   Just make sure whatever case you get fits an ATX psu, an ATX board, and the longish GPU.",Neutral
Intel,I fixed it,Neutral
Intel,fixed it,Neutral
Intel,fixed it,Neutral
Intel,I have a budget of 1.5k and yes I do have a micro center nearby. what gpu and cpu should I get instead then,Neutral
Intel,"I think this is pretty unbalanced, in that you have a budget GPU but a really high end CPU. You should either spend less on the CPU and keep the B580 for a lower cost overall, or you should spend money from saving on the CPU to upgrade the GPU. Also the RAM you selected, it's nice that it is cheaper, but CL46 is quite slow, imo worth to get more expensive 6000CL36 (lower CL is better).  I'd perhaps do it like this, maybe switching out the GPU to a 5060 Ti 16GB: https://pcpartpicker.com/list/PpRqYd",Neutral
Intel,"I would get a bundle from Micro Center (https://www.microcenter.com/site/content/bundle-and-save.aspx). Get the 7600X3D one for $400, which comes with a 16GB RAM stick, and you can get a ***matching*** stick on top of that if you want. That will be much cheaper. That CPU is very, very good for gaming thanks to its extra (X3D) cache, which also mitigates any RAM speed/latency concerns one could raise.   With the savings from that bundle, you can beef up your GPU ***significantly***: RTX 5070 Ti > RX 9070 XT > RX 9070 > RTX 5070 > RTX 5060 Ti 16GB > RX 9060 XT 16GB... You can use the relative performance table on this webpage to get a feel for which GPU is faster: https://www.techpowerup.com/gpu-specs/geforce-rtx-5070-ti.c4243  PS: Make sure your PSU supplies enough power considering your changes to the build. Cybenetics rates PSU quality on its website. You should be able to find a solid PSU for $100 or less.",Positive
Intel,"With the current RAM market, there’s no need to be picky about RAM speed and latency. Especially if sticking with an X3D CPU, whose extra cache will negate the speed/latency gap in gaming.",Neutral
Intel,how good of a build is that?,Positive
Intel,ty for thr advice,Neutral
Intel,"If you're talking 6000CL30 to CL36, or like 5600CL36, sure. But 5600CL46 vs 6000CL36 is a very big difference. And in my comment I do also say not to spend for a X3D, because a 250 dollars cheaper regular is a much better combination with the GPU.",Neutral
Intel,"I get you. I just wanted to specify that, if OP sticks to X3D (e.g., a 7600X3D from Micro Center), the RAM speed and latency gap will have ~~zero~~ minimal (3–4%) impact on gaming performance on a high-end GPU. ~~Zero~~.   *Edit: Corrected my assertions.*",Neutral
Intel,"Yes and no, there is a performance difference still of a few percent, but with a low end GPU probably not very noticeable.",Neutral
Intel,Fair enough... about 3–4% on average when using a high-end GPU... I will correct my comments accordingly.,Neutral
Intel,B580.  VRAM matters.,Neutral
Intel,"easily an rtx 3070. the 3070 destroys b580 in pure rasterisation to the point where vram doesn't make a difference.i would instead look at the 9060xt 8gb instead of the b580, which is newer than the 3070, performs slightly better, uses less power and has more features like fsr 4",Neutral
Intel,B580 is hard to beat at its current discounted price.,Neutral
Intel,A 3070 would slightly outperform i believe  But b580 being newer and more vram may be useful in the future,Neutral
Intel,"B580 and if you’re into one of the 4 free games, that’s an added bonus",Positive
Intel,If you plan to use a linux distro then B580.,Neutral
Intel,B580. Do not support NVidia for their predatory market manipulation.....,Neutral
Intel,"Get an NVIDIA GPU, probably the RTX 4060. The GeForce cards before the 4000 series don't support Frame Generation, while the RTX 4060 will support it.   NVIDIA DLSS + Frame Generation together is awesome.",Positive
Intel,"Where are you located (i only deal with locals)  If youre in canada, im going to be selling my rtx3070ti for $250 cad.  Never overclocked, was used in my gaming rig.",Neutral
Intel,He is buying used it literally doesn't effect nvidia,Neutral
Intel,"True, true",Neutral
Intel,">my budget is 300 and the Rtx 3060 is getting expensive and out of stock, what should i do?  RTX 3060 is out of production for a long long long time, prices of new pieces might be high. What is the currency and how much does B580, RTX 5060, RX 9060xt 8GB and 9060xt 16GB cost?",Negative
Intel,"With the 5500 you're better off getting an RX 9060xt   The 8GB version is $287, good enough for 1080p gaming  The B580 is great if zen 4/5",Positive
Intel,"Unfortunately, the 5500 is not a very good zen 3 cpu. The L3 cache is neutered, and it has reduced PCIE lanes.",Negative
Intel,You need the latest BIOS update but you should be good with 5000 series. This also depends on the chipset... if you have the newest one for AM4 then you are going to have an easier time than older ones.. Some of the really early ones dont work at all.,Neutral
Intel,Idk about 5500 but I know it works very well with 5600 and people are saying don't go lower than 5600 for it,Positive
Intel,"They're basically the same CPU, Intel makes 3 chips:  * H0 - 6 fast cores, 12th-14th Gen  * C0 - 8 fast + 8 slow cores, 12th-14th Gen * B0 - 8 fast + 16 slow cores, 13th-14th Gen faster DDR5 support   Both are B0 so it's the same silicon, they just disable some cores and tweak default settings. But it's a K CPU, so tweaking settings isn't blocked unless you have a cheap motherboard.   It's not unreasonable with a 5070 Ti, especially with fast DDR5 which is now unreasonably priced. I did a build for a friend with a 5070 Ti and 14600K(F) with one where we used a 48GB 2x24GB DDR5-7000CL32 kit with Hynix dies from Patriot for about $135. Unfortunately today, that same RAM is about $550.   At higher resolutions like 1440P-4K, the difference between CPUs/RAM will become more negligible.",Neutral
Intel,"14600K is sufficient, especially considering the huge jump in price to the 14700K. Unless the prices are similar, the 14600K is the better pick.",Neutral
Intel,"My bad, I should have provided more specs. I'm not planning to overclock. I have an MSI B660M MAG Mortar DDR4 motherboard. So your take is that the 14600K will be enough?",Neutral
Intel,Problem is that my platform is ddr4. So my thinking was that maybe i7 can compensate ram weakness somehow 😅  In my place i5 is about 270$ and i7 is about 420$ so yeah it's bit pricey,Negative
Intel,"Let's put it this way.   Over-simplified the 14600K(F) is like a 150 FPS class CPU while the 9800X3D is like a 250 FPS class CPU.   A 5070 Ti isn't typically good for 150+ FPS in modem games unless you're doing 1080P and/or low-medium settings.   And if you have a 120-144 Hz monitor, then it doesn't even matter. If you have a 240 Hz 1080P monitor, then your may have a bottleneck.   But more than the 14600KF, the DDR4 may also be a bottleneck. However it's not DDR5 > DDR4.   It's Good DDR5 > Good DDR4 > Bad DDR5 > Bad DDR4.   So if you have decent DDR4 RAM, there's a small performance impact. If you have bad RAM, there can be a bigger performance impact.  You can use a RAM latency calculator:  https://notkyon.moe/ram-latency2.htm  Generally:  * Below 9ns is top tier  * Below 10 is excellent quality * 10 is decent * 12 is ok * 13-15 is low quality * 16+ is garbage tier",Neutral
Intel,Hmm only to a certain extent. Better to put that price difference into actual ram instead haha. The future proof choice is actually to just go ddr5 right now.,Neutral
Intel,"Thank you for such a great, meaningful answer. I'm going to dive deeper into this. Happy holidays!",Positive
Intel,"At 1080p? You'll want the 9800X3D to push the frames, assuming you have a high refresh rate monitor. You be better served with a 9070 XT than a 5070 in terms of raw performance as well.   Why aren't you considering the 5070 ti as well? It delivers 85% of the 5080s performance for 25% less. It trades blows with the 9070 XT, but if you're set on Nvidia, it's a better value than the 5080.  If you're not planning on any productivity use in your PC, the 5070 ti with a 7800X3D would also be a great pairing within your apparent budget. The 7800X3D is only about 5% slower than the 9800X3D in gaming scenarios.",Neutral
Intel,5080+7800x3d,Neutral
Intel,"5080 is a huge a waste of money for this build. For the price of the 5080 you can get a cpu, a 9070xt, and a new 1440p monitor.  If it’s in the budget get the 7800x3d, if it’s not then a 9700x or 9600x",Negative
Intel,For 1080p u can honestly get by with way less than a 5070 or 5080. 5080 is really a 4K card. I’m using a 5070 for 1440p and 4K depending on the game. So idk your budget but I’d say go with the 5070 option.,Positive
Intel,i think this might be a joke,Neutral
Intel,9800X3D + 9070/9070XT,Neutral
Intel,Ah okay thanks. Well i was just comparing two prebuilts I saw that were both on on sale. So of these two which would probably give me the most fps and performance at 1080p?,Neutral
Intel,Both of these for 1080p are way overkill,Negative
Intel,"Both overkill for sure. The 7900x system will run into a CPU bottleneck before the 9800X3D system runs into a GPU bottleneck, most likely, but it would also be cheaper to replace a CPU than a GPU down the line.",Negative
Intel,"That’s fair, he’s most likely gonna be monitor bottle necked at the end of the day lol, good problem to have",Neutral
Intel,It's likely unless OPs holding out some info about a 240hz 1080p monitor.,Neutral
Intel,I’m sorry I’m a PC noob. I do have a 240hz monitor that’s 1080p,Neutral
Intel,Yeah I have a 240hz 1080p monitor,Neutral
Intel,"Nah it’s all good lol, that’s great, I think the 5070 build would make more sense unless you think you’ll upgrade to 1440p eventually",Positive
Intel,"If you wanna save money, you can also do a DDR4 build with intel 14th gen. Then you could keep your RAM. Ultimately, I would upgrade both the CPU and GPU.",Neutral
Intel,probably the gpu,Neutral
Intel,Would probably gain the most performance at that resolution with gpu upgrade. CPU is definitely going to show its age regardless but if you get a modern GPU with access to new DLSS models and frame gen you’ll get a substantial uplift in performance. Outside of that unless you have a microcenter near you where you can take advantage of their bundle deals I wouldn’t recommend upgrading anything else until the ram prices settle.,Positive
Intel,"Ehh it's hard to say. If you can find a decent AM4 Motherboard/CPU, I'd say go with that. But you'd probably want to look for a good deal used.   If that doesn't get you anywhere, then GPU would be next.   Your GPU is really good, just running on 1440p is your big drawback. 8GB on 1440p just doesn't cut it for a lot of games, but if you upgrade your GPU, that 9700k is gonna start holding you back just as much anyway.",Neutral
Intel,"The best bang for gaming at 2560×1440 right now is a GPU upgrade. Something in the RTX 40/50 series or AMD RX 7000/9000 class, depending on budget and availability.",Positive
Intel,"2077 is very cpu heavy at 1440p, has my 11600k and 3080 running both above 90% with gpu close to 100%. Very high graph everything ultra RT ultra dlss quality fsr frame generation 70-90fps depending how many NPCs are around. If RT off then 120fps. Even more with very high medium settings",Neutral
Intel,Ngl the CPU can last op a good while but the GPU should definitely be upgraded,Positive
Intel,"Make sure you have resizable bar turned on inside your bios and hopefully you got a pretty solid CPU. Other than that, this is one of the better gpus for 1080p gaming.",Positive
Intel,"My current cpu is 5 5600g but I'll upgrade it next year probably, for now it's not that bad ofa bottleneck I hope. I will turn resizeable bar as soon as my new mobo comes, I got the Asus Rog Strix B550-A gaming. Also my monitor is 1440p but from what I've seen, this gpu is perfect for that resolution.",Positive
Intel,no need to be very solid. an entry level cpu suffices  i'm using 225f and most of the time during gaming the gpu is 100% and the cpu is 33%,Neutral
Intel,"$200 is too much money for either of these cards. That said, i'd probs go 3060. There is only a 2% performance difference between them on average and the 3060 has 12gb vram. Unless you are doing a Linux build for a steam os or bazzite machine, in which case id go smd.  But again, $200 is to much for both cards and if you're going to spend $200 on a used card, you can either spend $250 to get a new Intel arc b580, or, my strong recommendation: a 6700xt. I've legit started to see them go for $160 on fb marketplace now.",Neutral
Intel,"If this is US pricing, these arent a good deal at all. You can get a card like an Intel ARC B580 12gb for 220usd right now new. Second hand comes with alot of risk with GPUS. I bought a 3070 the other week and its temperature was at 90C+ and throttling performance even after I tried replacing the thermal paste. Arc Raiders and Helldivers could benefit from something like a 9060 xt16gb, but I think the ARC B580 might be the perfect one for you, its like 60% more powerful than your two choices. The 3060 would be the worse of the bunch too.",Negative
Intel,Where are you seeing $220?,Neutral
Intel,Mustve been a sale I saw a week ago. The cheapest i can see now is [https://www.newegg.com/onix-lumi-8346-00278-arc-b580-12gb-graphics-card-double-fans/p/N82E16814987002?Item=N82E16814987002](https://www.newegg.com/onix-lumi-8346-00278-arc-b580-12gb-graphics-card-double-fans/p/N82E16814987002?Item=N82E16814987002) for $239.,Neutral
Intel,"Oh gotcha, thanks!",Positive
Intel,"If you want 2 pcs do this, if not just buy a used 5800x3d, also for cooling just get a peerless, it’s like 30 bucks and great for cooling",Positive
Intel,"Are you in the states? $1400 for that build is nuts. I just bought a prebuilt on sale for $1500 that had a 9800X3D, 5070, 2TB ssd and 32GBs ram.",Neutral
Intel,I can’t find a 5800x3d for anything less than $500 used. Even 5700x3d are a bit tough to find for anything under $400.,Negative
Intel,"I’m in the states…. And I think the price is mostly because of the 4 sticks of 16GB RAM. I’m going to try to talk him down in price but we’ll see. I agree that it’s a bit over priced but if I can knock it down a few hundred then it seems fair. 3080 is old, yea, but it’s still a good card. The 5800x3d seems impossible to find as well so the price could be from that. It’s also a ton of storage which is increasing in price lately too. I think that’s why the price is up there",Neutral
Intel,"Also, where did you find this prebuild?",Neutral
Intel,I found for about $250 around me I got lucky,Positive
Intel,"Costco online, they were selling them at $1500 for a Black Friday sale. I think the sale is over as I saw it was back to $1900 when I was there the other day. Either way worth a look.",Neutral
Intel,"Geez, really? That’s insane now days. How long ago was that?",Negative
Intel,Oh I think I actually remember seeing them sell a few pre builds on sale. I just didn’t look that closely at the components. I also wasn’t really thinking much about upgrading around Black Friday lol. I’ll have to keep a closer eye out when they do another price drop,Neutral
Intel,Early December,Neutral
Intel,Honestly prebuilt is the way to go with component prices unless you have a microcenter nearby for their bundles.,Neutral
Intel,"Dang, yea that’s lucky. Maybe I should keep an eye out for lower prices. It seems like that’s just not gunna happen. RAM and storage are next to increase.",Negative
Intel,"Yea, that’s what I’m seeing when I try to spec something out on pc part picker. I’m in Oregon and don’t have a microcenter remotely close. I really hope one day we get one because I’d drive a couple hours for one lol.   Until then, I’m looking at used rigs with solid components to swap around and get a second pc out of the mix. Even I buy a pre build, it just gets me a second pc without any upgrades. But my 5600x still gets me pretty good frames at 1440p… I’m just a sucker for MORE! lol.",Positive
Intel,"Oh man, I know how you feel. Sucks to live in a country which gives you the ""3rd world tax"".   In mu country, a 5080 will be ~$2,200. I bought it in the U.S. Amazon store, shipped it into this 3rd party company's warehouse which will ship it to me. Takes 3 weeks in total. But, at least, I paid MSRP for the card + shipping and tarrifs that totaled $1,200.",Negative
Intel,"Late but I chose a B580 and have absolutely 0 regrets. Very stable performance, have been playing some lighter new games at 4k 60fps, and I know I'm going to be futureproofed vs the other cards in its price class due to the 12GB of VRAM. Yes the 9060XT 8gb is technically a stronger performer, I will trade a few frames today for longevity over time, which should also translate into much stronger resale value on the B580 in the future.  But it seems you are finding the 9060Xt for actually significantly cheaper, like $70. That difference might make it worth it to go with an 8gb card. It's not going to be unplayable of course, but I think you may have to upgrade a bit sooner than with the ARC",Positive
Intel,"I feel the B580 to in quite a sweet spot at the moment and I kind of don't see a reason not to go for it. It may not be the best card out there (neither is the 9060 8GB), but in 2025 I would 100% go for the 12GB card over a tad more performance as that will last longer when games start to get more demanding VRAM wise. Sad to see the prices you have to pay for one at the moment, but I also see them rising here and hovering around $325-385 while most of them were under $300 last week (I'm not in the US btw).",Positive
Intel,I would take Rx 9060 Xt even 8 GB over intel arc 580.,Neutral
Intel,"Hey man, fellow Brazilian here. I know things are tough down here. I have not tried it yet, but everywhere I see right now, Intel is the king of cost x benefit. You can also help yourself by selling your card for about 600-700 reais, making your new card ""cost half"" if you go with the arc.  In the u future this will also help when you want to upgrade so your new card ""costs less"". I would go for the 12gb card because with the current landscape, games are demanding more vram and because of AI prices will shoot up.  If you need any help, dm me and I can try to help you out",Neutral
Intel,"Yeah, can't even do that here... It's possible that you would have to pay 92% more when it gets to the country or something...",Negative
Intel,"Yeah, it's possible that I'll stay with the GPU for some time... So, the 12 GB having more longevity, even if the 9060 xt has more raw power... Maybe the safe thing is to get the B580.",Neutral
Intel,"Right now sure... But what about longevity? I said that it's possible that I'll have this GPU for years, so... Won't 12 GB age better than raw power with 8 GB? Especially at 1440p.",Neutral
Intel,"Oh, I actually want my card. I have a home server and an NVDIA GPU would be nice for some stuff like transcoding. Also, I may be building a PC for my brother soon, so maybe the 1650 Super will go well in his PC for now. And if I buy another GPU in the future, I can pass the one I'll have for him too.",Positive
Intel,"Maybe I am wrong and the 9060 turns out to be the card that lives the longest, but if I had to make the call in that budget range personally, it would be the B580. Like you I don't really see a reason to go 8GB anymore. If Intel pulls out of the GPU market tomorrow, then I was wrong. If 8GB is not enough anymore by some release in June 2026, I was right. If you somehow could push the budget to the 9060 16GB, then that is a no-brainer as well. Difficult, but I do feel the B580 is very compelling.",Neutral
Intel,"I think raw power will come in more handy right now, and 8 GB is not the end of the life. Or spend a bit more on 16 gb.   You will need to make sacrifices when raw power runs out too.",Neutral
Intel,"Oh... It's not ""a bit more"" it's 1 THOUSAND reais more... At least...",Neutral
Intel,It is hard for people not here to understand that a difference of 180 USD is almost one month minimum wage for us,Negative
Intel,"I would take 8 GB 9060 Xt. Not a great choice for new AAA titles, but will rock everywhere else",Positive
Intel,I have a hard time affording upgrades too. This is why I would get the cheaper GPU which is also faster in most cases.   Is it possible to bring GPU when you visit Brasil?,Negative
Intel,"Makes sense. Thanks for the input, but ""not a great choice for new AAA"" titles makes me scratch my head, because I'm seeing the card being able to run new demading games on Ultra with more than 50 FPS and some games like Doom Dark Ages with more than 100 FPS max settings... You may have higher standarts, but that's absurd triple A performance for me...",Negative
Intel,"I am gaming at 4k if possible on RTx 5070   My last card was Rx 6650 Xt with 8 GB, and Rx 9060 Xt is much better. I think it is a good card, unless you run into rare cases where ram is not enough. So many great games where 8 GB will be plenty",Positive
Intel,"Makes sense, thanks for the input!  I agree, a lot of games that 8 GB is enough, but like, how the market is going, it seems like 8 GB is not being enough even today.  And I'm scared of having the card in 2 years or even more and being extremely limited by 8 GB.",Neutral
Intel,"if you have legos, you can sometimes use them to build a structure that holds a separate case fan close to the GPU, that can just be plugged to mobo for fan control. that's how i've cooled an entire GPU with a custom heatsink in the past, with static rpm. toilet paper or whatever to reduce vibrations if needed.",Neutral
Intel,"Hotspot can comfortably reach high 80s, don't worry",Negative
Intel,"Gpus are smart enough that if they start getting too hot they’ll power limit themselves, and eventually just shut down, to avoid damage. However if you want to reduce temps you’ve got a few options, easiest is to cap fps, so the gpu doesn’t work as hard, or you can undervolt, or just put a case fan right under the gpu to blow air past the broken fan",Neutral
Intel,Even with the thermal limit at 83?,Neutral
Intel,Fans are generally super easy to replace right? Just wanna make sure I’m not buying a fan that is stupidly impractical to install,Neutral
Intel,"Yes, hotspot temperature isn't what thermal limits are for",Neutral
Intel,It depends on the specific card but they tend to be a fairly straightforward repair,Neutral
Intel,They are for core temps? Okay thats good go true because I was getting unclear answers from other sources about that,Positive
Intel,"Okay. I did find a replacement fan from gigabyte that is identical to mine (I know gigabyte sucks but it’s what I had to work with), I just wanna make sure it’s a simple replacement if I have the tools",Neutral
Intel,"You’d have to look up the procedure for your specific model of card, cause different models may be built differently",Neutral
Intel,"Okay, it’s a good thing I’m waiting for my friend to be able to help then",Positive
Intel,"And again, just to be sure, the procedures might differ but even gigabyte fans can be replaced if you have the tools and exercise carefulness? I just wanna be sure it’s not one of those situations in which the fans are designed to be impossible to replace even if I purchase a replacement fan from the company",Neutral
Intel,Going to be a downgrade obviously. Intel has Been getting to the point those cards are actually good but it’s still going from high mid tier to low tier,Negative
Intel,you got a free game from intel,Neutral
Intel,as a low tier card it's quite decent. i use it to play games at 4k,Positive
Intel,Yep went in expecting a downgrade but I don't game too much lately haven't found the latest games all that good and mainly playing older games at the moment so the trade off for now isn't huge for me.,Negative
Intel,Yep Battlefield 6 key.,Neutral
Intel,How?,Neutral
Intel,Very cool Exxazy,Positive
Intel,"![gif](giphy|Idg2rAVGS3xMZtBdhu|downsized)  It’s fine, plastic doesn’t conduct electricity 🫣",Neutral
Intel,![gif](giphy|xT5LMtbEtZnbbCE08g),Neutral
Intel,It lost its sparkle  ![gif](giphy|63VH5uoslatcA),Neutral
Intel,[https://www.enostech.com/sparkle-intel-arc-b-series-graphics-cards/](https://www.enostech.com/sparkle-intel-arc-b-series-graphics-cards/)  Is this your card? So it's like a design / badge of the brand,Neutral
Intel,Remove it before it falls in the fan and fucks it up.,Neutral
Intel,"Each GPU chip is unique. Some are better at overclocking or running cooler, causing stock performance differences.  It's called the Silicon Lottery",Neutral
Intel,What did you need 6 B580s for? AI farm?,Neutral
Intel,"The best card here is about 2.3% better than the worst. I'd chalk that up to run-to-run variance especially with only 3 runs each. I'd be willing to bet some of these individual cards had a similar spread in results.  This is entirely normal. No two cards are entirely identical. One might have a tiny bit better cooler mounting or need a tiny bit less voltage to hit top clocks, and that one will be a tiny bit faster.  If you were to overclock or undervolt and chase a record, that top performer is the first one I would try first, but they are all so close that any one of these could be the technically best one.",Positive
Intel,Any idea on how the temps vary with those scores?,Neutral
Intel,"That doesn't look like too much of a variance to me, I bet nvidia and amd cards would act the same. Not all chips are equal, the silicon lottery determines what you get",Neutral
Intel,Seems within margin of error,Negative
Intel,pretty close. was thinking of a sparkle b580 as my upgrade as well.,Neutral
Intel,"Neet data. Not many people test a bunch of the same card against itself.  I know with cpus they talk about the ""silicone lottery"" - extreeme overclockers would look for the best CPUs and get a few percent extra performance. I'd guess thats whats happening here.",Neutral
Intel,"Furmark is known for a *lot* of run to run variance. You'd need to do something like ten runs, discard the outliers, and average them.  It's not very useful as a benchmark, doesn't run long enough, which is why you won't see many folk using it as a benchmark: It's a stress test.",Negative
Intel,"So we're talking about about Standard deviation of 78.2 and mean of 9187.6, which means less than 1% deviation from the mean for about 84% percentile of the cards. While sample size isn't to big to draw any conclusions and tests should be more thorough and controlled, this is not high variance.",Neutral
Intel,Average variation +/- 66 with maximum of -113  Max is a 1.22%,Neutral
Intel,thanks. now i can tell if mine is good or bad. wiat wait wait... no ya didn't benchamrk it on a core ultra 9 100series. thats a laptop only chip!,Positive
Intel,"NO ONE NOTICED THE CORE ULTRA 9 ""100"" SERIES?!?! I think he meant 200.",Neutral
Intel,Fucking ai people and their retarded local llm bs,Negative
Intel,"And not just GPUs but CPUs, vehicle engines, electric heaters, gas boilers, light bulbs, batteries etc  Basically anything will have a “tolerance” where the “output” will vary between x and y and is not a constant.",Neutral
Intel,"and also if you get a higher tier gpu then youre guaranteed to get a really good chip out of this lottery, such as gigabyte aorus xtreme cards",Positive
Intel,Yeah I was planning doing some local llm inference,Neutral
Intel,Good question.  I wonder if they are being thermally restrained or power limited.,Neutral
Intel,Very good GPU IMO the sparkle one looks better and clocks higher than stock,Positive
Intel,Hmm ok while I still have the cards not deployed in anything what benchmarks should I do like what exact testing are y'all looking for,Neutral
Intel,Yes I benchmarked it on my mini PC it is a mobile chip,Neutral
Intel,"What I do with my money and my hardware should not upset you. When people say ""AI people,"" we are referring to large corporations and data centers buying up all the GPUs and RAM and using the AI to ruin our lives even more. What **local LLMs** are doing is to bring that power back into our hands, the consumers. The everyday person. Would you like it so that all things to do with AI belong to Google and OpenAI, or would you not want a world where us, the consumers, can fight back with our own models?  if you dont know what to say about a matter its best to enlighten yourself before shitting on something",Negative
Intel,"And also humans, it's called genetic lottery.... Usually pcmr members don't fare too well on that. 😥",Negative
Intel,"Nvidia bins their chips into very tight bins. Board partners aren't even allowed to sell a chip as overclocked when Nvidia deems it to not be, so there's less of a lottery going on than there is in other places.  That being said, there will still be sample variance.",Neutral
Intel,"Why is this guy getting downvoted? Local LLM inference is really cool and you get full control over every part of it. Maybe people are thinking ""AI bad"" but the random hobbyist buying $2k of hardware is not the same as the large corporations buying the entire world's DRAM supply.",Neutral
Intel,Why go for Intel and not AMD or the one who shall not be named? Is Intel best?,Neutral
Intel,what?!? how did you fit the gpu?,Neutral
Intel,"Yep, they have 2 chip models with one permitting factory of and one not. And the ones permitting also tend to get you higher speeds  But for example if you buy an aorus card, chances are you can't go any higher because the chip couldn't make it to be an xtreme model. On the other hand, an xtreme card probably will go up a bit extra",Neutral
Intel,Nuance is too hard for most people.,Neutral
Intel,Most pple dont care they are like robots them selves they hear ai and immediately down vote some pple even believe that someone buying a few GPUs directly means they are taking away from gamers people dont care as long they have something to hate on,Negative
Intel,"From what I saw when I was looking, you can get decent amounts of vram for not too bad, I grabbed an intel b580 for my home server cause it was mega cheap and it can rip transcodes pretty ez",Positive
Intel,as others have mentioned currently the best bang for buck is intel for vram and also i got all of these cards for super cheap around 200-240 each also i dont want to support nvidia at all they can go fuck themselves and as for amd there is very little llm support on their cards,Negative
Intel,"that is 6 b580 carts at 12GB each right now they are selling for like 260 USD each at some retailers so that is 1,560 USD all before tax that is.  And you get 72GB vram to use as long as you have a system for using this.  And 3060 12gb are going for about 300-350 and the 5060 ti 16gb is going for about  400-450  then the 9060xt 16gb is going for about 380-450.  so the 3060 12gb is out, it is way to much  Then lets say he got 5 9060 XT 16gb at 380 that's 1900 and that would be 80 GB of vram or 4 cards at 1520 with 64GB of vram.  Then their is the 5060 ti 16gb it would be even more.  With Vulkan API working with different LLM stuff it is easy to use just about all cards now, aside from the compile process if you want to get maximum performance. Other wise LMStudio will get you going with Vulkan fairly decently.  EDIT: My mistake you can get the B580 right now for 239.99 from B&H photo.",Neutral
Intel,Nvidia tends to be the most performative for AI but also most expensive. Intel is the best bang for buck if you're going for high vram counts. It'll be slower but at least the model fits. Amd isn't really a player in the AI space yet,Neutral
Intel,How about an EU chip since you declared people should be independent of american ones hmm,Neutral
Intel,beelink gti 14ultra look it up it has a full pcie slot,Neutral
Intel,"It is hard to predict how chips will overclock. We've had generations where CPUs were apparently binned very conservatively, so even way down the range chips would overclock like crazy (Intel Core 2nd gen) or allow you to unlock whole extra fully functional cores (AMD Phenom triple cores), but we've also seen generations where most headroom was already 'priced in' (Intel 13th/14th gen) or arguably even exceeded. We've seen generations with more and less headroom on the GPU side as well.  That being said, now that consumer GPUs are competing with AI chips, they'll want to squeeze every inch out of each wafer, so I suspect they'll bin things as tightly as possible.",Neutral
Intel,"Yeah thats a fair point, some generations can be liklier to overclock than others",Neutral
Intel,"> TLDR in CPU intensive games, if your CPU is maxed, it holds the GPU back with it, which leads to some pretty significant drops, but don't sweat it. A 5600/5600x is a suitable CPU upgrade and will very well balance the system  i'm using intel 225f to pair with the b580 and they work well",Neutral
Intel,so ur tldr is that if you get bottlenecked from ur cpu the gpu doesnt perform well? sorry for the sarcasm but who wouldv thought! merry christmas,Negative
Intel,"Nice, the 12400f is definitely a solid pairing with the B580! Way better than my old 8400 for sure. How's it handling CPU heavy stuff like open world games? Been thinking about upgrading myself but trying to squeeze more life out of this setup first lol",Positive
Intel,"Lmao ur right it's stupid But I was doing research for this specific combo and couldn't find much, so I wanted to post my findings. Also, this is a problem specifically with arc GPUs, where the card requires a lot of CPU overhead to work well. Officially intel says not to pair this card with less than 10th gen, but it works so oh well 🤷",Negative
Intel,>How's it handling CPU heavy stuff like open world games?  does assassin's creed shadows count as open world? i think it's not so cpu intensive though. 225f+b580 can produce 4x fps at 4k medium quality  https://preview.redd.it/3ilcoxnptm9g1.jpeg?width=3840&format=pjpg&auto=webp&s=61369f28fca9c392917690bc631e2c0ecb667faf,Neutral
Intel,"Tengo el mismo micro el i5 8400 y me interesa la b580 lo recomendas? Tengo un core 225f nuevo, esperando a que pueda comprar las ddr5 para poder usarlo xd , pero me llama mucho la atención la b580",Neutral
Intel,"I don't really have anything bad to say about arc. It works, it's priced pretty decently. So I guess uh....not enough blue?",Neutral
Intel,That will be a substantial upgrade. Congrats!,Positive
Intel,"Great catch, this is like 2012 GPU pricing!",Positive
Intel,I thought so! Thanks! The price looks good too!  ![gif](giphy|m745dTCAxerHa),Positive
Intel,So cool you got a intel card they trying to step in for us gamers but will see if Nvida fully turns it backs on gamers and truly focuses on AI but cheers for coming into to the pc world…right on,Positive
Intel,"Yeah, I feel good that I asked my uncle before the ram situation got too fucked. Plus he got the AIO free with the graphics card",Positive
Intel,26% faster you say? Is this the GTX 1650 mom said we have at home?,Neutral
Intel,"Damn, this is cool as shit. Cool post",Negative
Intel,"I don't know much about this, so, out of curiosity, I'll ask, how long could this run before the freezer wouldn't be able to keep up with the heat produced from the card. Are long sessions or too-short breaks in between something you have to worry about?",Neutral
Intel,"So the next step is a shunt mod, I would hope.",Positive
Intel,I need to lay off the porn. Read a particular line in your post wrongly.,Neutral
Intel,I think a lot of that goes to show how Devs don't optimize anymore,Negative
Intel,Cool but this post has the same flow as AI 😂,Neutral
Intel,"Cool as shit, cool post, and cool GPU with the coolant.   Maximum coolness!",Positive
Intel,"Thanks mate, this one was a load of fun.",Positive
Intel,a freezer the size op described wouldn’t heat soak in the slightest. your biggest concern long term with set ups like that is condensation leaking into the chips and shorting them out.,Negative
Intel,![gif](giphy|Ugdyy18UaZj2),Neutral
Intel,"Would it be possible to isolate the components in a low moisture or vacuum chamber while allowing the pump tubes to pass through so that wouldn't be a problem?  Also, thanks for answering my question.",Neutral
Intel,"i mean i guess in theory it could be possible to do that yeah, i’ve never personally seen someone run a super chilled system in a vacuum. for day to day use it doesn’t seem super realistic, not that op’s test is either lol. plus then you’d have the vac pump frequently kicking on to maintain vac more than likely would be noisy/annoying… but i guess if you pulled all the air/moisture out of the surrounding area. in theory that would work yeah.",Neutral
Intel,"Yeah, I guess it's not reasonable for a daily build. It's interesting to think about, tho. Again, thanks for bothering to answer.",Neutral
Intel,happy to help :),Positive
Intel,"Have you tried uninstalling key software 1 by 1?   I know you said you reinstalled windows but if you then reinstalled all the programs you are used to running with, it may be one of those (eg: Adobe, discord, browser, etc).  You could also try to run Linux, on a usb (without installing the OS) just to see if you still experience crashes. That may help narrow down if it’s software (eg no crashes on Linux) or hardware related (eg you still experience crashes).",Neutral
Intel,"Without checking, it could be thermals, stability or even power issues. I would start with HWinfo and check to see if anything it running beyond the thermal limits. You can google what the running temps should be for GPU, CPU and motherboard chips. HWinfo will also give you a look at how much power is being used. Some adding up may give you an indication if it is power spikes causing shut downs.  If it is not down to overheating then check stability. I always check memory first, then CPU and finally the GPU. I use Kharhu for memory. This is not a free app, but you can use HCImemtest which is comparable. I have yet to see anyone use memtest in any version that gives credible results. It may have improved in recent years, but I tend to avoid it from its history.  CPU testing just run cinebench on a loop for 10 minutes. That will tell you if you have heat or stability issues.  GPU tends to be games as they tend to utilise the most assets in the GPU. If the CPU and memory come back clear, then running different games may narrow it down.  When stability testing though, there is no 100% guarantee on the results. Because you are allocating resources to run the programme, then it is impossible to test 100% of any device.  Good luck sorting it.",Neutral
Intel,oh people say amd cpus are good at gaming,Neutral
Intel,i'd take b580 over the 3060 any day.,Neutral
Intel,"For 300 USD you can get 5060 or 9060XT 8gb, and if you fine with used GPUs you can try to find 7700XT for that money.",Neutral
Intel,"Bro the 5060 and 9060XT don't even exist yet lmao, those are just rumors. For $300 right now your best bet is probably a used 6700XT or 7600XT if you can find one. The B580 should work fine with a 5500 after the driver updates but honestly I'd wait for more reviews before pulling the trigger",Negative
Intel,"The B770 feels like it is going to disappoint people. People want a 5070 TI competitor, but it seems more like a 5070 competitor, which could be great for the market.   Their tech suite is starting to feel a bit dated though. Frame gen is fine, but no RT denoiser like AMD and Nvidia is something that could use resolving, but the biggest is XESS XMX slipping into a relatively distant fourth place behind DLSS transformer, FSR ML, and DLSS CNN. A new transformer based architecture could really deliver a boost they need.    As for panther lake it is promising. An efficient yet reasonably powerful architecture with ML hardware for XESS is quite compelling in the gaming space with laptops and handhelds, but also low power laptops. Great encoding and decoding with WiFi 7 is quite nice too. Seemingly it will deliver better performance than stuff like the z2 extreme in the handheld market which is something we desperately need since it was such an incremental jump. Hopefully more big manufacturers consider Intel.   Nova lake is a lot of question marks right now, but it might be reasonably competent which is what we need from Intel. Even if they can’t match the 10800x3d or whatever they might be really viable competition for something like that 10700x or if they managed to do a really big jump the 10950x3d mega chips. I really want to see Intel succeed.  It isn’t good that AMD are starting to creep into the Nvidia style position for high-end CPU. Much like Nvidia in the GPU market has basically sole dominance over the 5080 and upward calibre of product. AMD is kind of dominating when it comes to their x3D processors for high-end gaming performance. That is never a position you want companies to be in.",Negative
Intel,"There is a terrible situation in the CPU's, AMD is dominating, and I will never trust Intel while having the government funding them publicly. Wish Nvidia did something cool in that, but I doubt that, they will probably make some shady movements there too.",Negative
Intel,"You might not trust intel, but I don’t know many people who would care about government subsidies and funding if they offered good value for money CPUs.",Negative
Intel,arrow lake is good  but the ram speed may not be fast enough?,Neutral
Intel,"Yep, its good, whole hate for it on internet is unjust for me, only serious downside is closed upgrade path, RAM? with  current prices you must take what is avalible for reasonable price",Negative
Intel,"The rule to follow here is: if one game is playing badly, it's probably the game, updates/files, it could even be on the games back end like the servers or w.e the devs are doing...  If all games play badly, it's the PC. Test more games to see a clearer answer.",Negative
Intel,i played for the first time in a while yesterday and the performance was quite a lot worse than when i last played i think because of the update which changed the entire menu and stuff idk if thats related or not,Negative
Intel,"Huh, thats quite literally the same issue I have if you check out the post Ive made, I assume it was some Windows update, as I can recall that there was one around the time it happened, because theres no way my performance would just suddenly drop like this by other causes, and I do know it happened on early december, I just shut down my pc, and then on the next day, I started having noticeable performance issues, I still cant find the culprit though, I keep looking for any more solutions but this might just be a Windows issue.",Negative
Intel,E sports titles are generally very CPU heavy and if you're sure it's not the GPU or the ram that leaves the CPU.  Check what hwinfo says about your CPU temps and vrm temps while running the game.,Neutral
Intel,Ever since the junkrat mythic update my performance has been actual trash.,Negative
Intel,Windows moment,Neutral
Intel,"I'd try using DDU (Display Driver Uninstaller) to clean all Intel GPU drivers then reinstall the latest. If you are unfamiliar with the tool, I recommend following their guideline for the process.  Disable internet > boot into safe mode > run DDU and select your platform > clean and restart. Internet should stay disabled until you install the drivers so that Windows doesn't auto install something before you do.   P.S. Close all 3rd party GPU utilities (Afterburner/Rivatuner/etc) during this entire process. I'm not sure this applies to Intel but Nvidia warns that installing drivers while these utilities are active can lead to performance degradation.",Neutral
Intel,"Yes, the game might have gotten an update that made the performance worse so its not your pc but if all games having worse performance, something wrong with your pc, os/driver or hardware related",Negative
Intel,"I'm pretty sure it's not game-specific, I left that out of my post but i added it in as an edit now, but neither games got an update when the performance dropped and they both dropped performance at almost the exact same time as far as i can tell.",Neutral
Intel,"probably not, 1st weekend of december was before the S9 update, after patch 8.12 it was fine but only a few days after is when it started acting up but thanks for thinking with me",Neutral
Intel,"interesting, happy to know im not the only one, i did notice a small windows update a little bit after my performance drop and installed it just in case, but maybe it updated on its own and thats what caused it. Hope it gets fixed soon for you as well",Positive
Intel,https://preview.redd.it/fsvhs4c2b19g1.png?width=288&format=png&auto=webp&s=3db8dcfdc91168aa9326999b0321f4091adb36a1  my CPU temps seem pretty good i think,Positive
Intel,"So, nothing i can really do except get a different operating system? It's still weird to me that it worked just fine for a few months",Negative
Intel,"Thanks, I thought about this but i ruled out driver issues because i didn't update any drivers before the tank happened, but I'll try it and see if it changes anything",Neutral
Intel,hey thanks idk why you got downvoted but it actually worked,Positive
Intel,it's likely something in the OS.   I had some weird compatibility issues with axis & allies (the really old version) yesterday.,Negative
Intel,"yea that's what i'm thinking as well, im 80% sure its pc-related, i just need to figure out what exactly is causing the issue",Neutral
Intel,"Yeah I hope it gets fixed, knowing how Windows can be, its hard to ever give them any credibility. Maybe they'll release a update at some point that will magically fix it, but who knows. Best to still keep looking for things to try that could possibly work. Good luck though, ill still keep an eye on this post, maybe more people will come up with something that could benefit us.",Negative
Intel,Yes     One thing no one here is talking about is windows OS     Look at your os like the stuff that loads up on startup every update adds bloat or re install's something.      So i would look at what is running in the background,Neutral
Intel,"Im just commenting on the fact that Microsoft progressively going through their OS code and replacing it with AI generated rubbish that in the last few updates have caused SSDs to brick, games to break and GPUs to cut perf in half. It's a sinking ship ruined by corporate greed and I left it a month ago and not looked back.",Negative
Intel,"Finals just gor a new season, so the game update could Be the cause",Neutral
Intel,"Hey i used a DDU like someone recommended and it totally fixed my issue, if you haven't tried it yet you could go for it, just make sure to look up some tutorials and stuff, hopefully it helps for you",Positive
Intel,"I don't think so, the performance already tanked before the new season (between the launch of S9 and the latest patch of S8), also Helldivers performance dropped around the same time.  But you're right it was definitely a bit worse after S9 launched (but they mostly fixed it the next patch with the performance improvements)",Negative
Intel,"The Radeon RX 9060 XT offers the highest raw frame rates at 1080p, outperforming the competition by roughly 4-5% on average.  The RTX 5060 provides nearly identical performance but adds the advantage of DLSS 4 for superior upscaling and image quality.  While the Intel Arc B580 is the slowest card, its 12 GB of VRAM allows it to handle Ultra settings that cause the 8 GB cards to stutter.  Ultimately, the video recommends the 16 GB version of the RX 9060 XT as the best long-term choice for modern gaming.",Positive
Intel,Had to sell the 6600 XT and went for the 9060 XT 16GB to play at 1440p. I’m loving it,Neutral
Intel,I got my 8GB 5060Ti open box excellent BestBuy for $309. It was brand new.,Positive
Intel,Personally out of the 3 I'd pick the 5060. Transformer model is but better than FSR4 at 1080p,Positive
Intel,"Bought a 9060XT 8GB for 247e (renewed on Amazon, Black Friday stuff) and sold the temporary 4060 non-TI 8GB for 220e on marketplace. Good deal...",Positive
Intel,Only compares 8GB cards from teams red and green since it’s only considering <$300.,Neutral
Intel,😮🫳🍿,Neutral
Intel,"I found an openbox 9060 XT 16GB at Microcenter for $305 and jumped on it. Very impressed so far, especially with undervolting.       I have the Powercolor Reaper model and it is legitimately impressive that they were able to make it that small.",Positive
Intel,"I feel like the HUB guys are going too hard with their VRAM crusade. Why recommend a GPU that's slower now just because it might be faster in the future?   A slight downgrade in render resolution or texture quality is hardly even noticeable, and with looming shortages I feel like most studios are going to start optimizing for lower VRAM further reducing the long term disadvantage of 8GB GPUs.",Negative
Intel,"The real answer, buy a used 2080ti. Usable VRAM, DLSS4, it still is 250W so it can run on most PSUs.  It is the most balanced option if you can't afford a 9060XT 16GB.",Positive
Intel,"all of them are power hungry junk, where are good cards?",Negative
Intel,real hero here,Negative
Intel,"Intel is on the right path, but they need to start using 384-bit memory interfaces on 12GB cards instead of the 192-bit memory interface they used on this card.",Neutral
Intel,"The 5060 will crush, without less than a 40 percent difference, from dlls alone. Then add frame gen. WOW I can't believe you can get away with this.",Positive
Intel,5060 then cuz way better in AI  5% performance cut to gain 2x-3x AI speed,Positive
Intel,What processor are you using with 9060 xt?  Is it the same as you were using with 6600 xt?,Neutral
Intel,"Can you tell me how well it runs games at 1440p? Have you played some of the demanding ones like Black Myth Wukong, Stalker 2, etc? Do you play at medium? high? I assume FSR is always on.   And also what's your target FPS? Would really appreciate the feedback, because I have the same card and I'm thinking on switching to 1440p but I don't know what monitor would be good refreshrate-wise",Neutral
Intel,"Well yeah, the cheapest RX 9060 XT 16GB is [$380](https://pcpartpicker.com/products/video-card/#c=596&sort=price&page=1&P=11811160064,51539607552) and the cheapest RTX 5060 TI 16GB is [$430](https://pcpartpicker.com/products/video-card/#sort=price&P=11811160064,51539607552&c=593). When you're comparing $300 GPUs, you're not going to bring up a GPU that's nearly another hundred dollar.",Neutral
Intel,"That would be a completely new phenomenon if you look at the past. Sure, some (probably indie) studio will optimize their games, but they would have done so already because they care about their customers.  Nothing will change with the current devs or tech, it's just a temporary issue that memory is that expensive. The prices will be lower in 2027, or we'll get used to it and buy more expensive stuff.",Neutral
Intel,"6 ish year old product that is out of warranty from some rando, is not exactly comparable here and definitely not a ""real answer""",Negative
Intel,"Dunno why you're being downvoted, the 2080 Ti is still very good value for the price and often has good OC headroom. Beats 5060 in most cases and you're right about 11GB being decent",Positive
Intel,The real answer is to stop being cheap and spend money on your hobbies.,Negative
Intel,you tell us,Neutral
Intel,Why do power requirements matter?   Electricity costs pennies,Neutral
Intel,So many think memory bandwidth matters more than it does. The 5060 ti has less bandwidth than the B580. Architecture matters a lot.  More bandwidth would do next to nothing for it.,Negative
Intel,AMD cards have upscaling and framegen as well...,Neutral
Intel,"Can you elaborate what do you mean by ""AI speed""?",Neutral
Intel,No. Dlss and frame gen is much less impactful in terms of performance boost at the low end and the latency is more noticeable. It’s also half the vram.,Negative
Intel,WOW 25 so far for the TRUTH. HUB and fooling now.,Positive
Intel,"Yes, same processor, 5600x",Neutral
Intel,"yeah seriously, here b580 is noticably cheaper for example.",Negative
Intel,"Yeah, I just wanted to point it out because there’s people like me to whom prices in dollars means nothing (or who don’t read the title) and then waste time watching an irrelevant video (though I skipped to the conclusion so not that much time).🙂",Negative
Intel,"Point being? If a cap blows because it's old any repair shop can fix it, If it's a fan dying you can fix that yourself.  On the other side there's not much the warranty can do for running out of VRAM.",Negative
Intel,"In this economy? It doesn't make any sense to not keep perfectly usable hardware that does the job just fine out of a landfill.   A 2080ti or a 3070 or AMD equal is more than enough performance for most people. Easily, and is way better bang for your buck.",Neutral
Intel,"In a time of global economic uncertainty, it's a horrendous time to overspend on hobbies.",Neutral
Intel,"heat, noise, size, messy cables",Neutral
Intel,"Correct, but they do not have commercial dlss support. How many games do you not have the ability and ww do?   Thanks for the dowmvotes nvidia. Amd brainwashed.   Just to let you know: you have all been played. Look closer.",Negative
Intel,"They're seemingly referring to the speed of running LLMs locally using that GPU, unless I'm also out of the loop. A good sub to look into that stuff would be /r/LocalLLM   I wouldn't recommend doing that with a 5060 but the 16gb version must be the best choice in that price range and would handle the very small models easily and the small ones with a little slowness.",Neutral
Intel,"News flash: We're always in ""this economy"". I know someone who works at a fucking McDonalds, has a kid, and spends more money on his hobbies than you do.",Negative
Intel,nothing global about it,Negative
Intel,"So you prefer low power for lower heat and smaller size.   I'm not space conscious, so those things don't matter.   What's with messy cables? The PC sits under the table, so it also doesn't matter how ugly it is.",Negative
Intel,"Well actually people have been modding games to put FSR where neither AMD or NVIDIA added official support.   Pretty much every game had amd nvidia and even intel upscaling these days.   In fact, when i still had my 3080ti, i was able to use AMD’s framegen in many games (cyberpunk, dying light, talos principle) because NVIDIA didnt provide any option for 3000 series.   I still bought nvidia because amd doesnt offer any cards at 5080 level, so no brainwashing here. You’re completely uneducated blinded by consumerism",Neutral
Intel,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,They sound irresponsible.,Neutral
Intel,"If you spend more on your hobbies than the things you actually need, you might be financially irresponsible",Neutral
Intel,"the unhinged extra power cables attached to the card itself, it makes everything harder to handle  and heat isn't just about the size of the case, it's noise and comfort of the room  and no, AC doesn't solve that as it's another source of noise that is even worse than the PC itself, only to be used when the weather is too bad to live otherwise",Negative
Intel,"Oh yeah? And who do you think had went through hundreds of accounts taking about that mod?   Came out DEC 22 2023 I remember the day I went from 22fps in Alan wake 2 to 50 (3070). I posted in this site non stop ban after ban just to try and get you this information. Go on the forums you will pick me out if you look back.   I, in all seriousness, would not be surprised if you know about that mod from ME.   Therefore, I am not blinded. I simply understand that an entire company propped up by manipulation on social media (and GPU) should not exist, and a real competitor would be in their place. There I have just demonstrated that not only am I not blinded, it might be you. Good on you for knowing about that mod (serious).   I will tell you another secret, maybe not meant for you. If you don't mind using dlss and mfg? 5060ti 16gb all day for 1440p or lower. Not only is it hundreds of fps, it has valuable vram on it that will see the card rise in price since it's discontinued.   Hows that for an uneducated prediction?",Neutral
Intel,Gotcha.  What's your preferred card?,Neutral
Intel,from this generation that'd be RTX Pro 4000 Blackwell generation SFF with replaced cooler  personally I own a passive A2000 SFF that replaced my modded 1650 (KalmX was released too late),Neutral
Intel,"Hopefully I am wrong but there is no aftermarket cooler for the RTX Pro 4000 SFF, right ?  https://n3rdware.com/gpu-coolers",Neutral
Intel,"unfortunately no, nothing ready to use that I know of  if you have access to measuring equipment machining a shim isn't even that expensive, haven't seen any publicly available projects for it yet",Negative
Intel,"Hmm that sounds tricky.  I’m thinking about getting PCI express extensor and a GPU holder to be able to use it with my MS-A2, keeping the GPU externally til the n3rdware cooler is available.",Neutral
Intel,"From r/radeon   * Ray Caching: Only available in Warhammer40K today, more games next year. * Ray Reconstruction: Only available in Black Ops 7 today with more games next year. * AI Frame Gen: Available in Black Ops 7 today with 40 games by end of 2025.",Neutral
Intel,It's almost 2026 and AMD keeps reinstalling the AMD Install Manager that I do not want and have to keep manually uninstalling. Stop this AMD.,Negative
Intel,What is fsr redstone? and which games use it?,Neutral
Intel,"I got a notification for the update in AMD Adrenalin Edition, but it does not appear in the actual install manager lol",Neutral
Intel,I just tested the release on four machines (76X&78XT/78X3D&79XTX/97X&9070XT/75F&76XT). Every system still suffers from crashing drivers when hardware-accelerated apps are used (Chrome/Discord/etc.).  Please fix. :),Negative
Intel,so can I open adrenalin on this one with a rdna 2 igpu and rdna3 gpu or is it still broken like the last version,Neutral
Intel,<--- Int8 rdna2 enjoyer,Neutral
Intel,did they fix enhanced sync and noise suppression yet,Neutral
Intel,Did this driver fix purple visual glitches with the RX 7700 XT? It's a known bug that appeared after the driver 25.4.1,Negative
Intel,"The ignorance by amd of Rx 7000 users is astounding tbh, but this is 2025 AMD not prior AMD where they would try to appease a larger user base.  It's going to make me rethink my loyalty for future gfx purchases",Negative
Intel,So we cant test path tracing performance yet on Cyberpunk? Lol,Neutral
Intel,"This is a very underwhelming update for RDNA 4 users I get that this technology needs to mature, but they should already be at a point where the implementation is across more wide array of games. My fallen RDNA 2 and RDNA 3 brothers will be remembered. The only reason AMD gpus are still relevant rn is price, nvidia tax is crazy. GG",Negative
Intel,"Thanks for nothing again, AMD.  Signed, 7900 XTX user.",Negative
Intel,So is there any point to installing this if I'm on RDNA2 and don't have any of the issues that they fixed?,Negative
Intel,This is the worst driver amd made 9060xt for me. 2 games instantly crashes. Indiana jones and silent hill 2. With this driver if you enable ray tracing game hangs and give error.i already report bugs in 25.12.1 and same with 25.11.1 and amd does nothing. every ray tracing titles works ok with 25.10.1 driver and this is bad. amd does not listen users anymore. anyone has any crashes happen like me?thanks...,Negative
Intel,Nothing on Oblivion Remastered crashing? Intermittent application crash or driver timeout on 9000 series when playing Battlefield 6?,Negative
Intel,AMD Software still crashes randomly,Negative
Intel,#AmdNeverAgain Give Fsr4 on rdna3,Neutral
Intel,New update new problems,Positive
Intel,"The adrenalin app just auto updated my 9070xt mid game, now my screen is black with no signal output to my monitor but my music is still playing lol. I waited for 10mins then I had to hard restart my computer for it to say the update failed",Negative
Intel,Pretty dissapointing ngl,Neutral
Intel,Should I get the RTX 5070 ti or 5080 at msrp? I am currently selling my XTX after radio silent news about FSR 4 int 8 on it.,Neutral
Intel,Everything is RDNA 4 exclusive? awesome /s  RIP finewine.,Positive
Intel,Please add the broken noise suppression to “Known Issues”.,Neutral
Intel,"If  this driver update keeps crashing my gpu im not leaving 25.9.2 for a while, im also starting to think about selling my gpu and get nvidea, and really black ops 7 why not a real game like cyberpunk i dont want to waste 70 euro for fifa with guns",Negative
Intel,"Can confirm on my 9060XT that Silent Hill 2 is still crashing and Avatar Frontiers of Pandora currently has a bug when FSR4 is enabled where the entire screen starts flashing like a strobe light, shadowy areas seem to trigger it. This is with both games fully patched & up to date.",Negative
Intel,"Let me see - all the new ""Features"" will be available for Cyberpunk 2077 in at least 1 year time and ONLY with RDNA4 ??",Neutral
Intel,AMD NoiseSuppresion still broken. Since September!,Negative
Intel,"Are pink artifacts fixed on RX 7700 xt, anyone ? It was bugged in 25.11.1 driver last month.",Negative
Intel,Where‘s support for 7000 series? Wtf is this dead meat,Neutral
Intel,I’m on a 6000 card is there literally no reason for me to download this,Negative
Intel,"all i want is to be able to capture clips in my games but for whatever reason amd either doesnt understand im in the game, recognizes the game wrong (battlefield 6 shows as elder scrolls online which i dont even have).",Negative
Intel,It's december and still no FSR4 for vulkan.,Negative
Intel,Is this worth updating to from 25.11.1  Is it more stable?,Neutral
Intel,"I had to downgrade to 25.9.1 to have some level of stability, can somebody confirm that the new driver is safe to upgrade to without it messing stuff up?",Neutral
Intel,Still no fsr 4 support for rdna3 🙄,Negative
Intel,"Guys calm down. RDNA3 being moved to maintenance mode is part of their new strategy, no longer ""Fine Wine"", the new approach is Stale Ale. That way their products remain DOA after launch and people won't keep them very long.",Neutral
Intel,idk why I find it so funny that a specific Roblox game got called out in the patch notes,Negative
Intel,Did they fixed the amd noise supression not turning on?,Neutral
Intel,"Anyone know why Cronos: The New Dawn has been showing [""FSR 4""](https://i.ibb.co/nqW2VMng/Cronos-The-New-Dawn-2025-12-04-02-28.png) for me on a 7900 XT for a few weeks? At first it was 3.1.  I know it can be modded in but this is on a new Windows 11 install and I haven't done any modding.",Neutral
Intel,"Looks like new chipset drivers, too.",Neutral
Intel,"I thought the application freeze fix might have stopped monster hunter wilds from crashing on me but nope still does it (DXGI_ERROR_DEVICE_REMOVED,)",Negative
Intel,/u/amd_vik are you aware of assetq corsa evo vr not working on AMD cards since 25.9.1 ? It displays the left and right eyes out of alignment and therefore fails to show a cohesive single image.,Negative
Intel,so no fsr4 support on Vulcan still? this is getting ridiculous,Negative
Intel,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.   Thank fucking god.,Negative
Intel,Still experiencing 100% gpu usage almost constantly as soon as you boot up BF6 on newer drivers after 25.10.1 and higher temps in general  I'm locking my FPS to 144 but the older drivers is showing better overall temps and less gpu usage for me 🤔  [https://imgur.com/a/ctbMCx7](https://imgur.com/a/ctbMCx7),Negative
Intel,25.11.1 was dog water driver timeout city for me I'm just gonna assume this new one will also be the same.,Neutral
Intel,BF6 crashing after a few minutes in game with that driver on a 6800xt,Negative
Intel,"ever since 25.9.2 still same bug is present even now and now it causes even more problems because ML based FSR and FG fails when it happens: Adrenalin app just shuts down randomly even when idle, no errors, no driver timeout, no dx12 trimeout, just adrenalin itself gets shut down in random times. why wont you guys do something about it finally? Seriously its been so long now... im on 9070XT Steel Legend Dark Edition from ASRock, 80% of your users or more report the same issue FIX IT for the love of GOD. I tried everything hoping its on my side but windows reinstall, DDU and AMD cleanup app and fresh driver install nothing helped its still here",Negative
Intel,"Both 25.12.1 and 25.11.1 drivers have the same bug on RX 9060 XT. When my screen goes blank and later i wake up screen, i have two mouse cursors on the screen, until i launch some app and then will second, fake cursor disappear.",Negative
Intel,I hope this fixes the many crashes I've had since the last update...,Negative
Intel,"Still enjoying the piss out of the 7900XTX on 25.9.2. It chews through everything I throw at it at the settings I choose, don't care about new driver releases unless a new game I want to play doesn't play well on whatever driver I currently have installed.",Negative
Intel,"Even tho I have a 9070xt this is still so underwhelming… We waited 6 months and got basically nothing yet. Sorry for all rdna2, 3 users.  Fun fact: Its been years now that the adrenaline software cant be opened, the only fix ist to press win+p and select only main monitor. Than start it, than swap monitor profile again…   Definetly buying nvidia next time, not supporting this big company anymore, which is behind in every aspect. Image you just want to play alan wake 2 (looks beautiful).",Negative
Intel,"ass. no support for rdna2/3, no new features for rdna2/3, rdna4 have only one game that support all of that, redstone framegen almost identical to 3.1 framegen, frame pacing still there.",Negative
Intel,hardware unboxed tested it and frame facing is broken when amd frame gen is on sadly,Negative
Intel,So the HDMI crashing issues should be fixed in this version yes?,Neutral
Intel,Any news on fixing the gpu vram leak issue on bf6? Sorry I’m lazing not reading the patch notes,Negative
Intel,25.12.1 does not even install on my Minipc (780M) + 6650XT eGPU Setup.   I thought I might fix 25.11.X not opening in an eGPU Setup.   Guess I will be running 25.9.2 for another few Months.  God why something always break? I thought it would be better going all AMD for the eGPU setup.,Negative
Intel,"Yeah I'll still be with 25.9.1 until the texture flickering is fixed in BF6, also instant replay just didn't work in 25.11.1 for me.",Negative
Intel,Will this help Warzone not look so blurry on 7900xt? Game is unplayable,Negative
Intel,So there seem to be two links - going through support>picking GPU(9070XT in this case) downloads the 25.21.1 win 11-b.exe file meanwhile going from this release note article it downloads the win11-c.exe . Any difference?,Neutral
Intel,"im using 6800xt the driver page has the win11-a version and article have win11-c version. which one should i choose i really dont know and this ""different builds"" confusing a lot of people",Neutral
Intel,"Genuine question, why all the hype and rush to release this today when it has just two games to showcase the benefits?",Neutral
Intel,Jesus how long has that Cyberpunk Pathtracing crash been in the known issues. It feels like it's been more then half a year.,Negative
Intel,Installed with no issues,Neutral
Intel,"I can’t play Warzone because I can’t update my bios, there doesn’t seem to be a recent bios update available for my Acer Nitro 5, using Adrenaline. Anyone know if this will help?",Negative
Intel,Doesn't look like they fixed the bug with Enhanced Sync not working properly with Freesync.,Negative
Intel,Any fix planned for 9070 users who cant enable Hardware Lumen on Oblivion Remastered? Game crashes as soon as we turn on the option.,Negative
Intel,Still no fix for Battlefield 6 for those with AMD 6800M GPU. I swear my next setup is going away from AMD if this is not resolved anytime soon.,Negative
Intel,u/AMD_Vik      In 2022 AMD made changes to OpenGL Driver. So since 2022 the extension gl\_ati\_fragment\_shader is missing in the driver. It cause problems in older games like Call of Duty 1 from year 2003. Stutter on some maps and broken water rendering because the games can't use the extension anymore.     Our Community is waiting since 3 years for a fix.,Negative
Intel,in black ops 7 only 25.9.2 driver work better even new 25.12.1 much worse fps drops,Negative
Intel,Very unstable for me (7900XTX). Driver keeps crashing even when I'm just watching videos. Reverting to 25.11.1,Negative
Intel,i just had to roll back to 25.9.2 because 25.12.1 kept crashing my system with poe2   even GGG straight up said don't use 25.10-25.12,Negative
Intel,"After observing you guys for a few days xD, 25.12.1 was installed along with new chipset driver on my system.  To my surprise, unlike previous 25.11.1, Adrenalin interface now runs properly with igpu enabled.  I need to test it out with real games, but for now, I've dodged instant roll back.  FYI, If you're using two or more GPUs, including igpu, on a single system with muti-monitor. Download the C package(1.65GB one including rdna1&2+3&4).",Positive
Intel,"NoUnfortunately, they don't work (( Random crashes remained + In some games, the inability to use frame generation through drivers was added (( Sad ( R5 3600 32gb ram Rx 7700 xt ) Rolled back to 25.9.1 everything works with it",Negative
Intel,"I had a very weird issue:     My PC would just crash when i did an Windows Defender Scan (only Full Scan, it worked fine with QuickScan or other programms like Malewarebytes) like the power was cut. I did a number of things even rollback the chipset driver but that didn't help. Then i rolled back to 25.9.1 + the newest chipset driver and everything worked fine again.   In case somebody had a similiar issue",Negative
Intel,"Anyone having problem with AMD overlay with this update? Somehow not showing at any game even if enabled, if I click to different monitor, it shows up. But when I click back to the game it disappear again.",Negative
Intel,AMD Wattman settings don't apply for the first time they're set. They have to be changed and applied to a different setting and then to the desired one back and forth to get them to work. I use wattman to set my custom fan curve and it's been glitchy since 25.11.1.,Negative
Intel,Should I download the new driver version if I have 6800XT? There is nothing in the patch notes about this series... And if yes - why?,Neutral
Intel,"Error code 182 for my AMD Radeon™ 780M integrated GPU on my Ryzen 7 8854HS CPU.  All other driver updates before 25.12.1 worked fine on my Lenovo Legion Slim 5 Gen 9, but this one says my GPU is incompatible, even though AMD's driver download page is providing [this download link](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe) to the installer:  https://www.amd.com/en/support/downloads/drivers.html/processors/ryzen/ryzen-8000-series/amd-ryzen-7-8845hs.html",Negative
Intel,7000 Series web browser glitch? and sound glitch? huh,Neutral
Intel,"Is it safe to update, 25.9.1 is stable for my 9070XT and causes zero crashes with the timeout bullshit from clock speeds going to 3300+ MHz",Negative
Intel,What does fsr Redstone means ?,Neutral
Intel,"Is this driver more stable than 25.11.1 it was causing driver time outs and i even got a blue screen. I rolled back to 25.9,2 and now im scared to update to this one lol",Negative
Intel,These comments are all over the place is it better than 25.11.1 or not? 😂😂,Negative
Intel,"So in short, still no support for 7000/6000 series, yipee",Negative
Intel,"Idk what happened but after this update my game crashed and then my PC crashed and when I turned it back on AMD Adrenaline disappeared from my PC, completely gone. What did you do lol.",Negative
Intel,For Sale: 7900 XTX - $50 OBO  I know these are no longer desirable due to being left in the dust by AMD after only a few months of real support but hopefully it will be at least a good paper weight for someone.,Neutral
Intel,So now driver frame gen is gone? Unless the game specifically supports it? And the overlay as well? Both are completely gone now after the update...,Negative
Intel,What about the fixes for the 7900xtx crashing all the time?,Neutral
Intel,"«#AmdNeverAgain” Where’s the Christmas gift in the form of FSR 4 for RDNA 3? In the new 2026 year, it might be time to think about switching to Nvidia.",Neutral
Intel,"Toujours pas de FSR4 pour les séries 7000 ? C’est mort. Pour ma part, je n’achèterai plus de cartes AMD. Si Nvidia continue à proposer son DLSS pour les anciennes cartes, alors mon choix est fait.",Neutral
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,"Downloads ""whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe"" for 9070XT, ""whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe"" for 5700 XT  What does it mean?",Neutral
Intel,It took a while for DLSS 4 to get implemented in a good way on 40 series cards too but a version made it there. Give it time. Now if they can just start prodding developers to incorporate that as well it’ll be worth it. Not enough games yet but here’s hoping!,Positive
Intel,Any update the in fact that and adrenaline software is not working when second monitor is connected? Especially using iGPU for second monitor ?,Negative
Intel,Omg I think they fixed the LG oled tv reboot bug,Neutral
Intel,Should i install it directly or should I use AMD cleanup utility first?,Neutral
Intel,some one have problem with instaling?,Neutral
Intel,Does this fix the driver timeouts that were happening with Edge? I had to revert the November update because of that problem,Negative
Intel,Any fix or still need iGPU disabled for 7000 and 9000 cards?,Neutral
Intel,The update is still not showing up in install manager,Negative
Intel,Honestly this software was the bane of my card for the longest time. Not having it anymore stopped so many weird bugs and crashes.,Negative
Intel,Does AMD's Instant Replay record still bug out?,Negative
Intel,Anyone tried the new fsr redstone yet? I am hoping for a big improvement over the old fsr,Positive
Intel,do you guys remove the old drivers before you install new ones? or just install ontop,Neutral
Intel,The path tracing crash STILL on Cyberpunk is absolutely wild to me. Finally AMD has a card capable of playable raytracing but we can't use it on the 'Crysis' of modern times to even test it out.,Negative
Intel,Adrenalin doesn't show this update for me yet lol,Negative
Intel,"> Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.   Glad for this, it was annoying that we were stuck in 25.9.2",Negative
Intel,Are the issues with SecondLife fixed? Last driver that didn't break textures was 25.9.1,Negative
Intel,9060 non XT 8GB can do the math 7900 XTX Nintendont,Neutral
Intel,"I’m at work, so I cannot check for myself: does this fix the constant crashing in Oblivion when hardware lumen is turned on?",Negative
Intel,Has anyone tested Marvel Rivals on 25.12.1 version of the driver? The only stable driver that works without crashing on that game is the 25.8.1 version.,Neutral
Intel,Adrenalin Panel not showing bug still present… :-((((,Negative
Intel,Am I the only one who doesn't have the new option in the drivers for frame generation with a 9070 XT?  https://i.redd.it/7r86hslx3g6g1.gif,Negative
Intel,"why are there 2 versions, b and c, 1.65Gb and 991Mb, release notes and through the support page, and is it stable or shall i just keep 25.8.1 as any other seems to crash call of duty, regular, other games seem fine,  ryzen 9 7950x3d/rx7900xtx",Negative
Intel,Did it fix the god of war 2018 checkered shadows?,Negative
Intel,"I spent all this time with 25.9.2 on my 9060xt because the following ones were disgusting to me, I will give this new update a chance and let's hope everything improves a little!!",Negative
Intel,Arc raiders crashes are fixed or not?,Neutral
Intel,"Makes my 9070 XT to constant run on 100% load in bf6 no matter if i play or sit in the menu. Cause device hung, graphic glitches and high temps.   Same with all drivers above 25.9.   25.9.1 works flawless with no errors and the load varies depending on the scenery as it should.",Negative
Intel,Noticing in Hogwarts Legacy with the new FSR and FG enabled over a period of like 30 seconds my 9070XT will go from \~250W used and 200 FPS and then drop down to say 120W used and 90 FPS and then after a short period go back up again. With FG disable it stays consistent 140 FPS-ish,Neutral
Intel,"FYI for ""Driver Only"" guys, 25.12.1 still have an issue to install this option.  l've open ticket to support team for last 2 versions. but I can't follow their request to observe the issue.  Don't know how long to keep using extracted file method. lol  Will see how 25.12.1 ""driver only"" perform.",Negative
Intel,oh nice! they fixed the FSR4 Quality Presets artifact issue,Positive
Intel,"When AMD finished Orange, Yellow Green, PurpleStone, can we unlock FSR Infinity?",Neutral
Intel,"Is it worth updating to this latest driver? I am not planning to use frame gen, is the image quality better or are there any fps improvements in games?",Neutral
Intel,"Updated to 25.12.1 now, before I was on 25.8.1, have a Rx 6800 XT and a Ryzen 7 7700X. Also updated my Chipset-Driver today. Haven't testet much yet, played now for like 1 hour Space Marines 2, watched some Youtube vids since I updated. So far looks ok. Only thing that worried me first was that I found in my Reliability History, 2 critical entries of LiveKernelEvents of code 1a8. But these were written down by Windows on the time, while I was updating my driver. We will see, if anything happens I will keep you updated.",Neutral
Intel,"Despite the device ID-based driver update blocking set in August, it has worked until now. The windows tried to install some driver on the 6700XT just now, and unfortunately, it also replaced the software itself somehow. threw an error message too.  Manual update would not go through unless i removed the driver update block.   What a sad situation.",Negative
Intel,"Anyone else has problems with CS2/Fortnite? Started happening after i updated drivers to 25.11 My whole PC would randomly freeze for like a minute with the ""AMD software detected that a driver timeout has occurred"" error. Once the PC unfreezes i must kill the game from task manager.",Negative
Intel,Does it fix the arc raiders dxgi crash of the previous driver?,Neutral
Intel,"How do I downgrade from this driver?   I’ve tried four different older drivers and all of them give me error 182 – GPU is not supported (RX 9070 XT) during install.   I’ve already used DDU and the AMD Cleanup Utility, but the only driver I can install successfully is 25.12.1.",Negative
Intel,pc started to crash 7900xtx... reverted to 25.11.1,Negative
Intel,Hi me and other people I know. Also forums and Facebook pages . Have had an issue with the frame gen after 25.9.2 . When they released new features we have all had issues where its drops fps and is completely unplayable. Has this been fixed in 25.12.1 I have 7900 xtx 7800x3d. Friend has 9070xt 9800x3d Both have issues. And im on windows 10 he's on windows 11. I used ddu and tried all settings on frame gen and other settings to fix it. Not to mention the drivers where stutters and lower fps without frame gen. Thanks,Negative
Intel,"When I enable V-Sync in the game, I experience lag; it only runs smoothly with V-Sync enabled when I also activate the performance overlay. This problem has existed since driver version 25.11.1.",Negative
Intel,"I have a second card from amd. And both cards have driver problems. Now I have an rx 9070 xt oc. I don't do any undervolting. Everything is at factory settings including the bios. I had 4 driver failures in 7 hours. What good is FSR if the driver doesn't work? It would be good if you finally solved this problem. I can stand it for a while, but if it continues like this, I'm leaving AMD.",Negative
Intel,"Wish they would acknowledge the bug where turning on GPU scaling and integer scaling adds more input delay, so for example the mouse movement will feel sluggish.  Been having this issue for 3 months now since nya bought a a 9070 XT",Negative
Intel,"On the RX 7600S graphics card, Adrenalin does not launch at all, and during installation it removed the driver PCIVEN_1022&DEV_15E2&SUBSYS_15131043&REV_60.",Negative
Intel,"How are those with a Cezanne CPU supposed to install this?  Selecting the 5750G from the drivers download page offers 25.21.1, yet none of the 3 variants of the installer support it.  * whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe (Vega, supposedly?) - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe (combined? ""Systems with RDNA series graphics products"") - nope  Each of them return Error 182.  Even the minimal web installer, amd-software-adrenalin-edition-25.12.1-minimalsetup-251207_web.exe, only offers 25.8.1.  VEN_1002&DEV_1638 is nowhere to be found in the .inf for any of the 25.21.1 variants.",Negative
Intel,I'm still hesistant to upgrade on this driver until they resolve these driver timeouts hell I'm even on 25.9.1 still experiences time to time TDR's.,Negative
Intel,"Is it worth for my 9060XT to go from 25.11 to the latest, Im having some problems where ghost of tsushima crashes.",Negative
Intel,Driver is making valorant run like crap for me idk why .,Negative
Intel,im using an rx6600 and up until today i was fine avg 200fps on r6 today the game says its at 22 usaeg when it avgs 1-4 and now it has major fps drops/tears,Neutral
Intel,This driver constantly crashes call of duty for me. Whatever windows update installs(which seems to be 25.10. something) is the most stable there is. 9070XT.,Negative
Intel,"Parabéns, fiz a atualização para 25.12.1 e agora não consigo jogar nada sem travamentos, além do google estar lento",Neutral
Intel,Anyone see if this fixes the issue of the graphics sliders not working at all and being stuck?,Negative
Intel,Still crashes. They will never fix it. Just buy nvidia or intel.,Negative
Intel,"u/AMD_Vik It says ""Intermittent application freeze when using the in-game Radeon™ Overlay."" in fixed issues but I've actually had my whole system lock up because of what seemed to be adrenalin having issues with the performance overlay....  I noticed one thing that pointed towards the overlay specifically: I was going through Adrenalin and when I was on the recording tab I switched to performance; it seemed like Adrenalin froze so I clicked Smart Tech. to see if it would respond.  Initially it didn't, before eventually swapping to the smart technology screen. I then went back to the record tab and tried again: same results.  That's about all I've got for specific steps. I closed Adrenalin and went back to doing whatever and I noticed my fans turned on and like two minutes later when I went to close my browser my cursor stopped before I got to the corner of my screen and I needed to hard power down my system.  Not sure if this is at all related to that issue. But i had it happen on the last driver as well, and came here trying to see if there was a known issue...",Negative
Intel,FSR Redstone support? Will my minecraft machine run faster now?,Neutral
Intel,Gonna be able to play modern titles on my HD5750 thanks to redstone !,Positive
Intel,All that build up for dog water. Built my first PC in March and went with a 9070xt full of hope. I'm beginning to understand why AMD is so widely despised.,Negative
Intel,Windows just installed the June   update from AMD. The fck is this,Negative
Intel,I'm not seeing the new update in AMD Install Manager,Neutral
Intel,so in 2028 10 games will have it like FSR4 XD,Neutral
Intel,Anyone knows if it fixed the crashes with Oblivion Remastered and Silent Hill?,Neutral
Intel,"""Intermittent application crash or driver timeout may be observed""  This is not an issue tied to a few games.... is a wide issue for me with a 9060 XT whenever i rise my monitor refresh rate.",Negative
Intel,"Ray Caching in 40K?  Not sure how they got this to work on the tabletop in real life but sounds awesome  In all seriousness there are a large number of games in the Warhammer 40K universe, any chance they are saying which one?  Space Marine 2 Darktide Battlesector   Etc",Positive
Intel,"so pretty much nothing for today, shrug...",Negative
Intel,Is there a partial list of the 40 games with the new frame gen? Is it something different from the fg we already have?,Neutral
Intel,There are well over 100 warhammer 40k games. Did they not specify?,Neutral
Intel,they wont,Neutral
Intel,"It's so annoying.  I would keep it if it didn't constantly pop up trying to get me to install ""AMD Chat"" and ""AMD Privacy View"".  I don't want your shovelware AMD, take a hint.",Negative
Intel,"There should be an option during install to exclude it, it can't be that hard to do. Same as you, u/MihawkBeatsRoger , I also uninstall it afterwards.       Notifying u/AMD_Vik",Neutral
Intel,"This.   Why I took it out are my own reasons and quite frankly, irrelevant. It's my PC and I don't want it. So please AMD, listen to me and keep it off.",Negative
Intel,"Focus on serious matters, this is a joke. If you do not want it feel free to install the driver only version, and be happy u have that choice. If you want the full features of adrenalin, well install manager is one of them.",Negative
Intel,It's a rebranding of the entire FSR ecosystem. What's new today is machine learning enhanced frame generation for RDNA4 cards. You can enable it in the driver for any game with FSR 3.1.4 or newer.,Positive
Intel,It adds denoising for Path tracing. In theory it should look way better now,Positive
Intel,All the games that don't use bluestone,Negative
Intel,"Only one , the new call of duty ATM. So if you enjoy shitty games , have at it",Negative
Intel,"Same, and I'm still on the October drivers",Neutral
Intel,You can download it from the website. The app release notification always lags behind the site. This is nothing new.,Negative
Intel,9070xt i see brave or discord freezing and lagging when watching a YouTube video still. I dont understand how hardware acceleration bug hasn't been fixed yet. Wtf are they doing.,Negative
Intel,Yup same here. Had to roll back to October to fix again,Negative
Intel,25.9.1 works on my 9070 XT. Everything after that is a mess for me,Negative
Intel,Tagging u/AMD_Vik  so they are aware of the issues.       I encountered the same problems on my 6800xt. Figma on chrome is causing random BOSD. The system will just restart without notice. Every single web app seems unstable on my system and memory usage is all over the place. Rolling back to 25.9.1 doesn't fix everything but it eliminates 70% of the issues..,Negative
Intel,Oh well. :/  Funny thing is I rebooted my PC again for a Windows update. The first thing that greeted me after opening a web browser was the driver giving up the ghost.  On 25.11.1.,Neutral
Intel,Ive been wondering what this seemingly random crashing has been. Thanks for this comment!,Negative
Intel,"9800x3d, 6950xt, no issue with either chrome or discord or firefox with hardware accelerated set",Neutral
Intel,"Me too.  Installed 25.12.1, whenever I use YouTube in Full Screen, the whole system freezes, while the sound is still audible, then I have to hard-restart my PC. Happened three times, decided to downgrade to 25.11.1 again.",Negative
Intel,"This should be fixed, I'm not sure why it was omitted from the release notes",Negative
Intel,<--- inte 8 rdna3 enjoyer,Neutral
Intel,"How do I set this up, can't find any info",Neutral
Intel,"I can't speak on enhanced sync, but noise suppression is still busted and not working =/",Negative
Intel,"I'm piggybacking, because I need that info too",Neutral
Intel,Ok I thought I was the only one having the enhanced sync issue because no one replied to any of my posts about it. I use it because then I can lock my fps to 120 (on a 4K OLED TV) and use enhanced sync instead of Vsync because of the screen tearing when locking to 120. Now I have to do the frame lock to 117 which is fine but just annoying me I'm missing out on 3 fps lol it was causing issues in a few games where it would stutter like crazy and it all came down to enhanced sync. I don't use noise suppression so I don't know what's up with that.,Negative
Intel,"Been using it for a few hours with the 7900XTX, so far so good.   Hopefully it's 100% fixed.",Positive
Intel,I hope they fixed it. I will test it now,Positive
Intel,"Did the typical test that I usually do and it didn't show up for me and I'm on the RX 7700XT as well. So hopefully, it's fixed.",Neutral
Intel,"AMD stopped giving a shit about it's fans once the company was saved and they started raking in the money. The change in tone was clear as day. That said, I'll still buy their GPUs because I hate Nvidia far more and I don't see that changing.",Negative
Intel,"yeah my next one will be Nvidia, better features, better performance espacialy with RT/PT   And apperently longer support... and AMD cards in a simmilar performacne bracket don't even cost THAT much less sooo.... jeah I am mad aswell",Negative
Intel,"I agree. AMD has shown poor support for 7000 series owners. If there was a FSR4 int8 leak, AMD should officially release FSR4 for 7000 series owners.  I bought my 7800xt only 2 years ago before RDNA4 cards came out.  Nvidia provides DLSS4 upscaling to their older generations like rtx3000 series",Negative
Intel,"Your system is almost exactly like mine, did you also have crashing problems while having the Xbox Gamebar DVR feature turned on? I would have constant driver timeouts until I turned it off.",Negative
Intel,looking back rn i think it wasnt worth the 100 dollars gain i gotwhen my 7900xt does consume more than rtx 4070ti and i do have shit features even the antilag+ scam that was one of the main reason i bought the GPU isnt here anymore .,Negative
Intel,And just like that comment and user deleted themselves 😂,Negative
Intel,"If you're referring to the app crashes with RTPT reflections enabled, we're working with CDPR on a fix",Neutral
Intel,Signed /another 7900xtx user,Neutral
Intel,"I came over from NVDA last March, bought a 7900xtx, RMAd it a few weeks ago due to pink/purple pixelation that would randomly happen. Now it's non stop driver timeouts and random performance issues every time I boot my PC or games. I am never buying another AMD card. I'd rather get ripped off by NVDA and not have constant headaches.",Negative
Intel,"Which driver are you currently on? I'm just curious; personally, I'm on 25.9.2, and surprisingly, I have 0 problems, unlike with previous versions. Should I try 25.12.1?",Negative
Intel,"Unterschrieben, Gigabyte 7900 XTX Benutzer.",Neutral
Intel,"Nope. Generally if the driver does not massively increases performance in some game, or you don't have any issues or the issue you have isn't fixed, then it's not worth updating, unless there is some new feature you want.    I reverted back to 25.9.1 (from the top of my head) because with any newer driver BF6 crashes randomly, and neither DICE nor AMD seem to give a damn about it.    And before someone asks, I tried any other fix on the internet for Battlefield and nothing else worked.",Negative
Intel,Same here. Anything above 25.9.2 crashes ray tracing games like Silent Hill  2 and Oblivion Remastered.   Ihr never had a more crash prone GPU than the 9070XT.,Negative
Intel,"Try this, taken from another comment branch https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",Neutral
Intel,Might potentially be fixed by a recent Windows update?  24H2 (and an earlier mini-patch that included this) apparently resolved a lot of crashing for folks.  See [here](https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/),Neutral
Intel,"Yeah I was really hoping they'd have got buy in from a decent number of devs with updates to big RT showcase games like Indiana Jones, Alan Wake 2, Cyberpunk, etc. But Black Ops 7 and Warhammer 40K... and that's it (for the RT features)?",Neutral
Intel,5070ti fs  basically a better 9070xt,Positive
Intel,"I’d wait on 5080ti with more VRAM but these are going to be obscenely expensive knowing nvidia + current RAM prices. Both 5070ti or 5080 are more of a sidegrade than upgrade, not worth the hassle IMO.",Negative
Intel,Get a 5070ti. I never thought I would say that. But this is what is is.. 9 months after release and the drivers are still D.S.,Negative
Intel,What about a secondhand 5070ti?,Neutral
Intel,"I mean, I wouldn't get either. 5070 TI is a sidegrade from the XTX, and 5080 is only slightly better. DLSS and RT would be the only reason.",Neutral
Intel,Sidegrading for an upscaler sounds like a joke.,Neutral
Intel,"I think Linux developers are doing some experiments As of now, FSR 4 (FidelityFX Super Resolution 4) does not officially support RDNA 2 or RDNA 3 GPUs, even on Linux. However, thanks to Develer’s work on VKD3D-Proton 3.0, there is partial and unofficial support for RDNA 3 under specific conditions.  RDNA 3: Partial Support via Develer’s VKD3D-Proton  - Develer’s VKD3D-Proton 3.0 includes support for FP8 (8-bit floating point), which is required for FSR 4. - This means RDNA 3 GPUs (like RX 7600, 7900 XT/XTX) can run FSR 4 in some games via Proton, even though AMD doesn’t officially enable it. - Global override toggles in AMD’s 25.9.1 driver can bypass the FSR 4 whitelist, allowing it to run in FSR 3.1-compatible games.  I hope they succed it will be a slap in the face.",Neutral
Intel,This has been announced for months.,Neutral
Intel,Yeah AMD refusing to port features to any card released before the 9 series makes supporting them really hard.,Negative
Intel,Say thanks they haven't demoted 7000 series to only game drivers,Positive
Intel,"Your best case is your RX 7900 turning into Balsamico, whatever that means.",Positive
Intel,"Its because RDNA 4 added hardware that 3 and 2 don't have. Now before I get kicked to death by angry people, there is a version of FSR Redstone that uses and INT8 path that is compatible and will work on 2 and 3, however that has not been launched today and AMD have not confirmed it will be.   That isn't to say they won't do it, but right now it's not been announced. Perhaps there will be enough noise to get AMD to change their mind or it might be that they want to get it out on their latest cards first before complicating matters with older RDNA support.  Only time will tell",Neutral
Intel,"Bro the AI accelerators completely got revamped, upscaling technique isn't usually the indicator for 'fine wine', it is when non-upscaling raw performance numbers improve.",Positive
Intel,Same boat here. Tired of trying.,Neutral
Intel,Thanks for testing. Have you perhaps tested Oblivion Remastered?,Positive
Intel,Finally fixed! It's a christmas miracle!!!,Positive
Intel,I regret getting this 7800xt,Neutral
Intel,Any card released prior to the 9 series.  Amd could give 2 shits as they chase the AI bubble (jokes on them if I was an exec I'd double down on the consumer market to insulate from the impending bubble burst),Neutral
Intel,sadly,Neutral
Intel,"Yep, I go back between 23.9.1 and 25.9.2. I couldn't be happier.",Neutral
Intel,"If it's any consolation, I was on an NVidia card for 2+ years where I wasn't getting the DLSS updates. Then they actively removed features when they went to the NVIDIA app.  Looking at AMD's roadmap, RDNA4 looks like a stopgap anyway until RDNA5 (prob will be called UDNA?) comes out. So in another year and a half I'll be in the same situation with my 9060XT.",Negative
Intel,"Use OBS, replay buffer",Neutral
Intel,Was just thinking of giving a shot for Indiana Jones and the Great Circle - I guess not anymore since FSR4 doesn't work with it..,Neutral
Intel,"Microsoft had bugs also causing hanging crashes. Everyone loves to blame GPU drivers immediately, but check this out:  https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",Negative
Intel,I also want to know this.,Neutral
Intel,"I'm wondering the same thing, 25.11.1 is still the most stable for me!",Negative
Intel,Wondering too. I bumped back down from 25.11.1 because it was unstable on my machine.,Negative
Intel,Stay on 25.11.1 if you are on RDNA 1 or 2,Neutral
Intel,squash hard-to-find sharp reach memorize fade husky divide subsequent plough   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"~~It messed up my audio, now everything sounds 8-bit. If you're on RDNA4, avoid this update.~~  EDIT: it's not the drivers, after much tinkering I was about to deduce that it was my monitor. So it should be ok to update",Negative
Intel,I can't use anything above 25.9.1 on my 9070 XT,Negative
Intel,forget it they gave u the middle finger move on fuck both amd and nvidia,Negative
Intel,won’t happen,Neutral
Intel,"Fine wine is only a thing for very few and specifics types of wine, typical wine still goes bad over time.",Negative
Intel,What is the source for this or is it trust me bro?,Neutral
Intel,They should just remove this feature. It never worked from day 1..,Negative
Intel,what is the difference,Neutral
Intel,Latest windows update yesterday fixes that.,Neutral
Intel,"Can you tell me if this is also applicable to 25.12.1? There are several (frustratingly unlisted) VR-specific fixes aligned, one of them closely relates to what you've just described",Neutral
Intel,Same here. 25.9.1 makes my problems go away,Positive
Intel,"That was a terrible driver for me also. New one has been night and day improvement, give it a shot.",Negative
Intel,Same for my 9070 XT. Device hung error,Negative
Intel,"Thanks for reporting, had that once with 25.11.1 + 9070XT (W10) before reverting to 25.9.1 (since then, it never reappeared).",Neutral
Intel,i think your sorry should extend to people with rdna4 cards because this is pretty underwhelming,Negative
Intel,Do you get a firmware update pop up? Is this one?  https://i.redd.it/w46j86mnct6g1.gif,Neutral
Intel,"I'm familiar with this impacting United Offensive, I don't believe we're reintroducing this old vendor specific extension, however. I do have a ticket for the performance issues though; I don't believe this is related to the missing extension.",Neutral
Intel,"Tested for 2 days(1day and 22hrs uptime)  No crash, No BSOD for me so far. Nothing strange.  MS Edge, Google Chrome video playback, youtube...etc all play nice while gaming on main monitor.  Diablo 4, MSFS 2024, Doom dark age, Forever winter(UE5), Witchfire(UE4)...etc All run fine.  Lossless scaling runs fine on spicy vids to all of the above games xD  HWinfo64 and MSI Afterburner, RTSS all run as they should.  (Win11 25H2 uptodate, X670E, igpu(98x3d)+7900xtx+6400 3gpus, 2 monitors, hybrid mode)  Edit) rx 6800 + r7 7700x on win11 25H2, X670E, Single monitor, igpu-disabled -> runs fine.  rx 6700xt + i7 8700k on win11 25H2, Z370, Single monitor, igpu-disabled -> seems good.",Positive
Intel,"25.11.1 had pink artifacts glitch on chromium browsers with 7700 xt but i installed 25.12.1 yesterday and no issue so far, i did not see artifact pink glitches or sound issue so far ?",Negative
Intel,My 9070 XT hate every driver above 25.9.1,Negative
Intel,I updated to this driver and immediately got a BSOD. Rolled back to October 25.10.2 again,Negative
Intel,offer steep theory scale straight obtainable physical ad hoc selective test   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,thats what I wonder too! Is it more stable??,Neutral
Intel,haha r u fr,Neutral
Intel,la même. C'est scandaleux,Neutral
Intel,"Means that they've created separate driver packages tailored for the specific gens (A rdna1/2, B for RDNA3/4, C - combined fat package that contains both drivers for systems that might have both gens on the same machine (igpu + dgpu) )",Neutral
Intel,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Cleanup utility first, always!",Neutral
Intel,I also wanna know best way updating drivers. DDU kinda annoying but maybe must be done i don't know,Negative
Intel,It's doing it there too.,Neutral
Intel,Never seen any crashes on it with latest driver prior to today 9070xt w11,Neutral
Intel,"Just tested it tonight, and for me it's working fine, 9060xt here, windows 11 with the latest update, although i play with the ""medium"" preset which disables ""lumen"", can't say it might work for you but you can try it if it still crashes constantly",Neutral
Intel,Might want to check [https://www.reddit.com/r/radeon/comments/1pjeonb/fyi\_fsr\_ml\_framegen\_requires\_windows\_11/](https://www.reddit.com/r/radeon/comments/1pjeonb/fyi_fsr_ml_framegen_requires_windows_11/) :|,Neutral
Intel,Everything after 25.9.1 is a mess for my 9070 XT,Negative
Intel,nope. I still crash,Negative
Intel,"I fixed my arc raider crashes (mostly in blue gate map load) by running DDU, installing 25.10.1 (down from 25.11.1), and deleting shader caches (dont know if the shader cache delete helped or not). I upgraded to the newest drivers the day after they released and haven't had a single crash since in arc raiders, including w overlay.",Neutral
Intel,"for me, DDU in safe mode, disconnect internet, install 25.9.1 fine for me(9060XT).  I've tried 25.10/ 25.11 and revert back to 25.9.1 with this way. Now observing 25.12",Neutral
Intel,Yes. I downgraded from 25.11.1 because of the crashing. Now been on 25.12.1 all week and havent had any issues come up. You also get proper fsr4 upscaling now.,Neutral
Intel,"I just want to say I think I found the culprit. It also happens on the winupdate one too, because it started crashing all the time.  Core clock boosts itself WAY past what it is declared on the card(I got a Sapphire 9070XT Nitro, supposed to be 3060MHz). Here's the moment before it crashes to a black screen:  [afterburner screenshot](https://i.ibb.co/3YpJFtzM/Screenshot-2025-12-21-030537.png)  The dip in clocks is the moment it crashes. As you can see, it is running well above boost clocks. Hence, freezing in a few minutes, proceeded by a black screen, and a crash. The ups and downs are from me alt tabbing in the graphs, by  the way.   This is with core clock -200mhz applied in Afterburner and no crashes, boosts to just above declared boost clocks. Here the dips in up and down on power are probably me toying around how much exactly -mhz is needed.  [afterburner -200mhz](https://i.ibb.co/YTQfGJtc/11111.png)  All of the crashing behavior so far is replicable in COD, CS2, Cronos New Dawn.  u/AMD_Vik",Negative
Intel,thanks for reaching out - funny timing; I noted that on the internal ticket for this issue yesterday having seen other accounts of end users noting this issue persists even with 25.12.1. Perhaps the fix aligned to that point release somehow slipped.,Neutral
Intel,I still have my 5670,Neutral
Intel,"No problems with RX 9070 xt in ARC raiders, i have win10",Positive
Intel,It’s for Darktide apparently,Neutral
Intel,any game with fsr 3.1 fg also has the new fg since drivers override it. it’s also why they stopped versioning fsr. any game with fsr 3.1 should just automatically have any new version of fsr when the drivers update,Neutral
Intel,"It's for Darktide. But it's not even ready for launch there, either.",Negative
Intel,I forgoed any amd software entirely  Use more clock tool  10x better with 0% of the bloat   ^^ helped me get my 4th in world furmark score (7900xtx user),Positive
Intel,If you want to be in control of what’s on your computer then Windows is not the OS for you,Neutral
Intel,"Dumbest take one can have, since installing only the driver won't let you manage the settings at all.  Which has nothing to do with this useless launcher no one wants or needs.",Negative
Intel,Found the install manager dev lol,Neutral
Intel,Thanks.,Positive
Intel,Unfortunately Redstone FG is bugged with poor frame pacing,Negative
Intel,Nice to see the innovation continuing on,Positive
Intel,But only on the 9060 and 9070 right?,Neutral
Intel,Yeah same,Neutral
Intel,"Remember when you could click ""Check for Update"" inside the AMD Software and if there was an update, it would download and install it for you?  Glad they fixed that awful experience, and we have the Installation Manager now.",Neutral
Intel,I remember this mentioned since the  GCN 1.0 days. Lol,Neutral
Intel,"On my end, the driver crashes. Most of the time it manages to recover (sometimes it will crash a few more times before stabilising). Sometimes it doesn't recover (leaving only 1 of my monitors working), so I had to reboot. Then after rebooting, strong chance it'll crash again the moment I open my browser.",Negative
Intel,"\+1 on this. Most games crashed drivers with any newer drivers except 25.9.1, but poe2 i cant play with vulkan or Directx 12 only with Dx11",Negative
Intel,"My experience with switching to amd was so smooth and perfect until 25.9.1. Everything after that just caused stutter issues in games, programs randomly crashing, drivers crashing completely causing my pc to reboot, this is so sad i hope they fix this soon and bring back a stable version asap. Rolling back to 25.9.1 now aswell until that happens.",Negative
Intel,přesně zustávám na 25.9.1 všechno jiné crash,Negative
Intel,"I had been having the absolute worst time with drivers when I first bought my 7600XT, but finally found stability with 25.8.1 (and turning the Xbox Gamebar DVR off...) but I'm so paranoid now to update my drivers again. The only reason I decided to check on updates now though is a sudden appearance of my screen flashing black at random times.",Negative
Intel,Are you able to tell us what the error code is on the BSOD? I don't suppose you have a kernel memory dmp pertaining to one of these failures over at      C:\Windows\MEMORY.DMP,Negative
Intel,Thanks will give it a try after I finish work,Positive
Intel,"Wait, AMD Customer Support told me that 2 monitors connected to iGPU and dGPU has never been officially supported and that this configurations breaks performance… so they told me bullshit?",Negative
Intel,Any update on three Oblivion Remastered and Silent Hill  2 Remake crashes? A lot of us are still with the September drivers because of them.,Negative
Intel,<--- Ditto,Neutral
Intel,Optiscaler lets you inject it. Do not use in multiplayer games though.,Neutral
Intel,it cannot possibly be this difficult to fix when there’s already community workarounds,Negative
Intel,both are still broken somehow,Negative
Intel,both have been broken since 25.10.1. enhanced sync just makes your display run at an extremely low framerate when freesync is on and then noise suppression just doesn't even turn on. I don't understand how they haven't fixed either of these yet. they haven't even acknowledged it,Negative
Intel,"I have to do the same. My monitor is  240Hz and the TV  120Hz and I have to use Chill, which sometimes will cause stuttering, because enhanced sync always causes stuttering.   Man I'm starting to miss the NVidia setting of just putting vsync on in the driver and everything just working.",Negative
Intel,I did some testing AND as far as I can tell I do think it's actually fixed finally,Positive
Intel,I would continue buying their GPUs if they gave me something to buy.  The XTX has no upgrade path on RDNA4.,Neutral
Intel,"I had Nvidia for years, the main reason I switched was that the drivers went to shit last year. I'm just sick of them in general, too. The 7800 XT I bought has been one of the most trouble free cards I ever had, aside from Adrenalin randomly closing in certain versions.",Negative
Intel,"If I could get my hands on a 5070 Ti I’d happily switch. AMD likes to take advantage of the underdog, for-the-people image whenever it’s convenient but they’ll just as quickly throw us under the bus and fuck us raw once they’ve got the bag.  Is Nvidia a gang of greedy fucks? Sure. But at least the bullshit’s right out front where you can get a good strong whiff of it. You know what you’re in for.",Negative
Intel,"I purchased a 7700 XT and a 7600 8gb I'm March and while I'm satisfied with performance, it would definitely be awesome to have FSR 4 on both cards as FSR 3 and 2.2 (overwatch )leave alot to be desired",Positive
Intel,It's been so long bro :( Hopefully the fix comes with ray regeneration support?,Negative
Intel,"Hey Vik, is there any info for FSR4 Vulkan support?  It's quite sad to see that there still isn't support for it as it has been 9 months by now since the release of the 90 series  Also is there any info about the EAC issue with Star Citizen and the latest drivers?",Negative
Intel,"Amd Noise Supression doesn't work, when I try to turn it on, nothing happens, but in 25.9.1 it works",Negative
Intel,"Hey amd\_vik is amd Aware of the 1 year on going Darktide issues with amd  ( GPU , and specially X3D cpus ? ), and that even the Dev of Darktide ( Fatshark ) seemingly gets ghosted by amd ?  heres some more info specially first links includes a few Dev comments  [https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462](https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462)  [https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f](https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f)  [https://forums.fatsharkgames.com/c/darktide/performance-feedback/97](https://forums.fatsharkgames.com/c/darktide/performance-feedback/97)",Neutral
Intel,"Vik, weren't you on holiday leave? xd",Neutral
Intel,Any fixes for the SecondLife issues we've had the last few months? last driver that didn't break textures was 25.9.1,Negative
Intel,Will this update fix some of the artifacting I’m seeing in cyberpunk with fsr enabled?,Neutral
Intel,Also getting driver timeouts in Cyberpunk with RDNA3 with raster or RT. I did not have these problems with my RDNA2 card.,Negative
Intel,"The AMD FSR ML-based Frame Generation option in the Radeon panel disappears in Windows 10.  So I have a question: Is ML-based Frame Generation no longer usable in Windows 10? This option is available in Windows 11, but not in Windows 10.",Neutral
Intel,Can I join if mine's just an XT?,Neutral
Intel,What GPU are you using?,Neutral
Intel,Try reinstalling Windows. That fixed it for me.,Positive
Intel,"This doesn't work. We are talking about games that crash with or without it, the only difference being the older AMD driver working.",Negative
Intel,I already install the latest update before update drivers its not update related. Vulkan driver is the problem in indina jones and silent hill 2 after windows update 25.11.1 not crashing ray tracing enabled but in 25.12.1 its broken again. So driver is the problem...,Negative
Intel,"You need to compile VKD3D with specific flags to enable this, it's not enabled by default. Based on the changelog, it appears that AMD has blocked support in the standard build.",Negative
Intel,"They said earlier in 2025 they were working on FSR 4 support for RDNA 3, and then it leaked in September with the INT8 version...",Neutral
Intel,"They might as well have lol, they aint getting no new features",Neutral
Intel,They also promised features to the few of us who bought 7900 XTX. Good luck defending them when it's your turn to be disappointed.,Negative
Intel,I expected them not to abandon their king card lmfao. Who does that,Negative
Intel,"Not really, they teased the possibility of including other architectures.",Neutral
Intel,Maybe next time you should read the whole thread before replying.,Negative
Intel,"It's also related to getting new features in generations other than just the latest one, ""bro"".",Neutral
Intel,"I have the 7800 xt hellhound i F love it, tbh i care less about this redstone thing but its frustrating why a 2 year old lineup is abandoned all of a sudden",Negative
Intel,"> I'd double down on the consumer market to insulate from the impending bubble burst  If that bubble bursts nobody is going to have much money to spare for consumer goods. That bubble bursting will tank the entire economy along with it.  *Long* term that might work out better, though.",Negative
Intel,further reminder amd is not your friend sadly,Negative
Intel,Same for me but Doom Eternal. I play at 4k and it needs upscaling at that res.,Neutral
Intel,What if I'm on RDNA 4?,Neutral
Intel,Yeah there are no good choices,Positive
Intel,https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html,Neutral
Intel,[https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html),Neutral
Intel,Adrenalin is for GPUs.   Chipset is for CPU & mobo.,Neutral
Intel,Awesome! I will try that 🤞,Positive
Intel,"No, it's not because it's a driver issue. AMD needs to act up",Negative
Intel,"Yeah im sorry for all of us, already shopping for a 5080 rn…",Negative
Intel,Yes that’s the one. I have no idea where to turn lol,Neutral
Intel,Sad news. Nvidia still supporting old extensions.,Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",Negative
Intel,oh wow i haven't had any issues yet but that doesnt mean much. 25.11.1 i didnt have issues for a week or so.,Negative
Intel,"Interesting, I’ll test it today. I was crashing non stop on 25.11.1 so hopefully this update fixes it",Positive
Intel,"cod crashes saying driver doesn't meet requirements, so i upgrade to the ""optimised for BO7"", crashcrash crashcrashcrash, roll back one by one with the same results get to 25.8.1 and stable again, crash once every 2/3/4 weeks before this one it was 24.12.1 crazy, a dozen updates every year and only one or maybe 2 are stable enough to enjoy shooting zombies for any length of time",Negative
Intel,Same,Neutral
Intel,"I have the same model GPU inconsequentially boosting well above the advertised clocks (nearly 3.4GHz) in both windows 10, 11 and fedora 43 with no issues.  This has been discussed several times on this community; whilst the clock behavior may surface other issues or instabilities on the system, it's not in itself the cause of problems.",Positive
Intel,"I actually have one more potentially related thing for you!   During the game I tried to turn the overlay on using my hotkey. Noticed it didn't. Since I've seen this before (we can call this a ""soft lock"") I tried to open the full screen experience with the hotkey. Which brought up my mouse (was using a controller in game before pressing the keys) but I could not move it...  My workaround has been: ctrl+alt+esc to task manager, tab to the search bar, type ""radeon"" and force kill the host service.  The instance I reported before this was a ""hard lock"" that I've noticed while trying to use my browser over a borderless game running, before this time where it was when the gpu wasn't under any actual load as far as I knew.  Glad to hear it's a known issue and not my hardware though... Thanks for getting back to me!",Neutral
Intel,6970 here,Neutral
Intel,Literally the one game I don't play lol,Neutral
Intel,"Yay, I own that one",Neutral
Intel,"It's not even out for Darktide yet either. Fatshark clarified that it's experimental and needs more work, so it's not in the live build",Negative
Intel,So it's under the umbrella of the fsr4 override if I understood this correctly. For the fsr2 and 3.0 games I can use optiscaler right? Sorry I just bought a 9070xt coming from nvidia so I need to get used to these things.,Neutral
Intel,I used to do that but a few games can use the FSR4 in driver upgrade.  The enhanced sync was nice too when it worked.,Positive
Intel,"Unfortunately I play games and run software that require Windows so I have it on a separate drive. When I do switch to it (and I update the driver to take advantage of new features), this shit typically happens along with a slew of forced updates.  You are right though, I do primarily run CachyOS.",Negative
Intel,found the linux user,Neutral
Intel,You're talking nonsense.  Engineer managing 2k endpoints and several hundred servers.,Neutral
Intel,Wasn't the dude's claim it has been always bugged with AMD,Neutral
Intel,🌍👨‍🚀🔫👨‍🚀,Neutral
Intel,It's barely an improvement.,Negative
Intel,It's branding,Neutral
Intel,"Yes, RDNA4 refers to the RX9000 series.",Neutral
Intel,"Uninstalling the install manager brings back the ""check for updates"" functionality until you update again (and have to re-uninstall the install manager)",Neutral
Intel,I have one of these captures if you want it (error code 0x00000119). I've been having a TON of driver timeouts and BSOD for the past couple of driver versions and I've had to roll back to October to resolve them. Seems like any app that has hardware acceleration enabled causes it and exasperated when viewing the system via RDP.,Negative
Intel,Let us know how it goes!,Neutral
Intel,"I don't know how much of an impact this could have on perf since it's not something I've measured. I personally wouldn't do this, though. With a dGPU installed I keep iGFX off.",Negative
Intel,"performance wise it should only be a couple frames of latency, when doing rendering on dgpu and going out through igpu it'll just copy over the frame buffers.   Main impact is on pcie bandwidth as it'll use up quite a lot there, and to a smaller degree RAM load, so you definitely don't want to run some other dynamic load on the igpu when gaming to overwhelm its pcie link. I think on 7000/9000 it's x8 so it may be fine? But I'm really not sure could be x4 too",Neutral
Intel,"We're tracking a failure in silent hill 2 remake, I believe a fix is aligned to a future release. I'll need to check in with oblivion remastered",Negative
Intel,"Do you have to do that convoluted setup and download the drivers from Limewire, or has Optiscaler wrapped it in to their application?",Neutral
Intel,"So, no official release... ;(",Neutral
Intel,Any tutorial for a noob on RDNA2?,Neutral
Intel,what workaround?,Neutral
Intel,"Same issue with fsr4 on rdna1-3.   It shouldn't be this difficult, it's in a perfectly working state made possible by like one guy's few days worth of work.   And yet AMD just doesn't do it...",Negative
Intel,FUG,Neutral
Intel,Enhanced sync makes games super stuttery even in  25.9.2.,Negative
Intel,So I tried out enhanced sync and the game that I first noticed the issue on is no longer an issue. I've checked like 5 other games and no issues so far. Maybe it's fixed?,Positive
Intel,I have my 5070ti build hooked up to my 240hz Ultrawide Oled because of Multi-Frame Generation. I was hoping that would be part of Redstone but it's not.,Neutral
Intel,"Such a relief, but i am also annoyed because they are ignoring 7000 series... I can literally use FSR 4.0.2 on my 7700XT and it is WAY better than FSR 3.1....",Negative
Intel,I hope it is fixed for me as well 😭🙏. Thanks for the info.,Positive
Intel,yep would have upgraded but with an XTX.... you can cut your vram in 2/3 and have less Raster performance for a good upscaler and better RT performance it's such a stupid fucking problem....,Negative
Intel,"That's not something I'm privy to, but it could be worth reaching out to them to request looking into if they're not already.",Neutral
Intel,"I'm not privy to any of the FSR stuff - that's a different team to mine. I can pass on the feedback.  The Star Citizen EAC issue should be addressed, please let me know how it is.",Neutral
Intel,i still am!   so many fixed issues out of the release notes that I felt the need to stick around and help clear things up in the communities I frequent. I'll go back into hiding again soon,Positive
Intel,"I've seen something like this over at OCUK Forums but weren't given enough data to work with. We've attempted to reproduce a corruption issue but apparently we've not been successful.  Can you give me a step by step breakdown on how to hit this, as well as a clear depiction of the issue?",Negative
Intel,"No, XT peasants needs to form their own group.",Neutral
Intel,6800XT.,Neutral
Intel,Some of their marketing said they would like to get it working if possible.,Neutral
Intel,"There are already third party options, but it would be nice to see if Steam Machine drives INT8 FS4 support since it runs on RDNA 3 tech. Let's see what happens in 2026.",Positive
Intel,Yeah there are going to be serious consequences as major retirement funds have invested in all these AI stocks because they have made so much money.,Neutral
Intel,"Give it a try, for my 6800xt it's crashing in almost all games...  ![gif](giphy|QMHoU66sBXqqLqYvGO)",Negative
Intel,Where does that say rdna 3 is in maintenance mode?,Neutral
Intel,"No, it didn’t 😣",Negative
Intel,"Sorry, out of curiosity, if you close it, it won't let you play? What do you get? Could you send me a photo so I can understand?",Negative
Intel,"I agree. Please can you raise a ticket requesting support for this over at our GPUOpen and ask other end users and developers to upvote it and leave a comment registering their interest? (please share a link to it here if you do) https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues  As far as I'm aware, the impacted titles are: IL-2 Sturmovik: 1946, Neverwinter Nights Diamond Edition and Call of Duty. If there are any others, I would really appreciate you letting us know.  E: I believe it's posted here: https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues/80",Neutral
Intel,"Just an update - I ended up running DDU and re-installing the latest update and now things are pretty stable, no driver timeouts from hardware accelerated apps either. Could be something to do with the architecture change between driver packages - but doing a complete removal between updates seems required now.",Neutral
Intel,I never seen 1 crash on 25.11.1 although I did use the preview update for windows 11 last week which fixed some amd gpu related crashing and that solved my arc raiders random crashing,Neutral
Intel,"My apologies then - it seems latest driver on Windows seems to be the source of issues then, seems more people have issues posting on /r/AMDHelp , also with 9070XT's. Seems all device hung errors and timeouts recently posted are with 25.12.1. I had no issues on cachyOS (Hyprland) running CS2 too, latest amdgpu.",Negative
Intel,"Also it's considered pretty poor Raytracing. People are usually snobbs who go ""I can't tell the difference"" when it comes to raytracing as a coping mechanism but darktide is the one game I can see why, it's not very noticeable and not worth it.",Negative
Intel,"yes, 3.1 is where AMD adopted the same modular approach as nvidia so any game at fsr 3.1 or above just runs at whatever latest fsr version your driver supports, which is currently 4 although now the versions aren't numbered anymore",Neutral
Intel,Hell yeah 👍🏻   Impressive you can run that on a 5x86,Negative
Intel,"Since you're already an advanced user, perhaps you could block it from installing by selectively blocking AMD in your hosts or pi-hole? It's not a dumb solution, but it's better than having to deal with push-installs.",Neutral
Intel,I might be an ass but I’m not wrong,Negative
Intel,Sorry  If you’re a **consumer** and want to be in control of what’s on your computer then Windows is not the OS for you,Negative
Intel,"Yes, If you mean the bad frame pacing when fps is lower.  I still opt to spent 1-200 hrs of my gaming session with FSR 3 frame gen, 7900xtx.  It's not that bad when the output is close enough to monitor max hz, similar to what hardware unboxed did in thier test.  The generated frame still comes out too early but it has to wait for the monitor's nest refresh which is consistent.",Neutral
Intel,ty,Neutral
Intel,"u/amd_vik it sounds like this person doesnt want the manager to install again, but I am pretty sure you can do custom option to uncheck it. If you do express of course it will put it back sschuler.",Negative
Intel,can you run analyze -v in windbg or fire it over to me via your preferred file sharing method?  I personally like to use https://send.vis.ee,Neutral
Intel,Can confirm this issue is fixed for me on 9800x3d + 9070xt (I had this issue on 25.11.1 and reverted to 25.10.2 until today) 👍,Negative
Intel,"Seems to be working fine, though when I was installing the driver my igpu showed up separately from the dgpu in the installer with a download link. But when re-running it they both show under 25.12.1  Should I be installing some separate older driver for it to keep things like hw accel working or was that just some hiccup?",Neutral
Intel,Oh great will also test after work it’s been headache since last driver update,Positive
Intel,Thank you for taking the time to respond. This has been very frustrating.,Negative
Intel,"I'm sorry to comment directly to you here. Do you have any report about monster hunter wilds performance drops in recent GPU drivers ?    I'm using 9070xt.    I have to use version 25.3.1 to play wilds with no stutters, anything newer gives a lot of stutters in many places.",Negative
Intel,"Yeah you still have to download it on your own, the creator of Optiscaler already said they aren't going to bundle it probably due to the whole legality around it.",Neutral
Intel,"i saw a post that detailed how to essentially replace noise suppresion with the working version in newer drivers, you can probably find it here somewhere",Neutral
Intel,worked fine for me idk,Positive
Intel,still busted for me,Negative
Intel,"Also from what I can tell, enhanced sync is fixed at least on my end.",Neutral
Intel,Thank you for this! been waiting for a fix with Star citizen.,Positive
Intel,Yeah SC seems to be working for now.,Neutral
Intel,"Bonjour, pour le moment sur Star citizen le problème avec EAC fonctionne pour la 7900XT. Merci d avoir réglé le problème. Bonne fêtes de fin d'année.",Neutral
Intel,That's good to hear. What about Noise Suppression not working since 25.9.2?,Negative
Intel,Hmm let me try. So pretty much having installed the latest driver (25.12.1) I just open SecondLife. I look closely at my avatar/character and my skin looks like this  [https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4](https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4) (excuse my outfit but just easier to show)  this is how it's supposed to look and also does on 25.9.1 [https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4](https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4)  I've heard that this doesn't occur on linux but only windows (But I don't have linux so can't say for sure)  I think you need PBR / Materials or some reflection on your skin to see the issue.   If you fly up to around 2000+ meters above ground it becomes easier to see  These are my settings [https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png](https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png)  I have an rx 7900XTX,Neutral
Intel,"Hello! I am actually one of the developers on the client team for Second Life, and I have been trying to figure out how to get in touch - we have found at least one nasty bug on some of the Strix Halo chips with the current drivers.  Can you send me a message here so we can exchange emails?",Negative
Intel,😭😭😭😭,Neutral
Intel,"I had a similar issue with my 6800xt and the other thing that helped was to sit it to fullscreen or borderless and swap back and forth. Now I'm only playing in fullscreen (which is annoying), but it doesnt crash anymore.",Negative
Intel,I have the same card and exactly the same problem. Can't install newer drivers or BF6 just constantly crashes.  I'm on 25.10.2 tho,Negative
Intel,"And it is, and they did, we have the leaked int8 version from September... Just needs official driver implementation now.",Neutral
Intel,Ive had zero crashes for 2 weeks on arc due to the preview update which rolled out to the public this week. You could try underclocking your gpu if it boosts over the limit.,Positive
Intel,"Before the Black Ops 7 (which I don’t own) integration to Warzone, I could click off it & carry on. But since the integration it just closes the game.",Neutral
Intel,"Yes, i have created this github issue.",Neutral
Intel,"If those failures are avoided by clock limiting the board, the problem area could be a different domain entirely (CPU, memory, power, etc.).  The linux remark is interesting, it kind of calls back to similar failures with NV31 in certain apps like Helldivers 2; we had a little internal discussuon about how the amdgpu kernel driver managed to mostly avoid such issues, though I dont recall the outcome.  If you get the opportunity, I'd recommend a suite of system integrity routines as a sanity check; please take a look at [one of my older posts](https://old.reddit.com/r/Amd/comments/1l9ox9r/amd_software_adrenalin_edition_2562_optional/nn3yuay/) for some background.",Neutral
Intel,"Depends on where you look at it. The Morningstar has a bunch of pre baked lighting effects, which make the difference completely unnoticeable unless you study the frames  In mission it makes a pretty significant difference, but wether or not that's something anyone cares about in a fast paced game is another thing entirely.",Neutral
Intel,"They aren't numbered in the sense of like 4.0.2 or like there won't be an ""fsr 5""? Thank you very much btw, very helpful info!",Positive
Intel,Like a charm. :D,Neutral
Intel,"I probably could, but AMD (and any other company, really) should be following the users preference anyways. It is a band aid fix and doesn't solve the problem.  Not a bad idea though.",Negative
Intel,I've been using computers since dos 3.  You're a spanner.  I'm sure MacOS is soooooo much more open.,Neutral
Intel,"Thank you for the idea, I just tried a custom install during an update, was given 2 choices (update/dont update driver and install/dont install privacy view). After installing drivers, step 2/2 was installing the install manager.    After updating through adrenaline using the custom option, I attempted reinstalling again using the auto-detect, custom install. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.   Installing via the WHQL package, custom install follows the same steps as above. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.",Neutral
Intel,Here you go: [https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw](https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw)  I did run in windbg but I have no idea how to save the output unless you just want a copy + paste of it here haha,Neutral
Intel,Appreciate the feedback,Neutral
Intel,Thank you for confirming.  That interesting though. I think the most seamless way to support products from both branches is to use the AMD auto detect tool. Can you tell me how the iGPU is represented in Windows' Device Manager?,Positive
Intel,"[https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html)  https://i.redd.it/3vxsa8yave6g1.gif  If you suspect the installation is incorrect, download the package that includes the IGPU driver using the link provided above. The basic version does not include the IGPU driver, but provides a separate download option during installation.  Anyway, it seems like a lot of bugs have been fixed in this version.",Neutral
Intel,"if you can find it, you will be the goat",Positive
Intel,"Yeah everything was fine until I tried to play Resident Evil 2 Remake, then the issue was back but not in the other games. This is a very odd issue.",Negative
Intel,Nvm the issue I was having just happened in Resident Evil 2 Remake.,Neutral
Intel,Thank you for letting us know 👍,Positive
Intel,appreciate the info. I'll ask our technicians to check in with the settings you've provided,Neutral
Intel,I can confirm there is no issue in linux. A windows version running under proton in linux has no issues as well.   In the video there is flickering on head and body. I see only flickering on the head (when running it on the windows pc)  But my body has no layers attached - the body in the video usually comes with layers. But all heads have multiple transparent layers. The problem occurs even when that layers are not in use and are fully transparent.   Probably related.,Neutral
Intel,"Hey there, thank you for reaching out!  I don't suppose it would be possible for one of our devrel folks to contact you via a linden lab email address like business@lindenlab.com?",Neutral
Intel,"So if you click dismiss, the game closes, did I understand correctly? It doesn't let you enter the COD HQ ? I'm telling you this because I too should update the bios, in fact it happens to me too, but I click dismiss and it lets me play anyway.",Negative
Intel,"for CS2, it was the newest driver that caused crashes exclusively, but on that driver I also got stronger boosts off the bat, hence it crashed faster. Now on 25.10.1(from windows update), COD still crashes with a black screen then tab to desktop with a driver timeout detected. Looking at afterburner(just using it to monitor clocks, no OC/UV applied or anything) the moment the GPU touches 3300+ I get thrown to the desktop. Can't even finish the training course even with ""speedrun strats"" before it crashes. It boosts [momentarily to 3300+](https://i.ibb.co/bgLFC0dp/coreclockcrash.png) and I get a screen freeze, crash, and sent to desktop with a driver timeout.   [These](https://send.vis.ee/download/103635cf66bdb907/#t2lRq409eeNwv6AaafhKJA) are both my crash report submissions. I'd go tomorrow over the stress tests, but I have managed to complete Time Spy/Steel Nomad without issues. And like I said, my system has has 0 issues before on a 2080ti.",Negative
Intel,"FYI, I passed [everything.](https://imgur.com/a/WyB9FeE)  This leaves the driver only. I made sure windows update didn't download its own driver this time, installed 25.12.1, still getting driver timeouts and crashes in games. I don't know what to tell you. Memtest86 also passed without any issues.",Neutral
Intel,"I just tried it myself and no I still stand by my statement that there's barely much of a change, I guess it's just subjective on this game. Agree to disagree.     I also still stand by the fact that I don't think enough will use RT to warrant it updating to use caching.",Neutral
Intel,"there won't be an ""fsr 5"" because any game with fsr implemented from here on out should, in theory, be compatible with every future version of fsr made, so numbering them isn't as meaningful. they're probably just going to stick with unofficial codenames like redstone for diffrentiation. Nvidia still uses versioning for DLSS despite it using the same system because it's good for marketing and diffrentiation so I'm not sure that dumping the version numbers is a wise decision but it also makes sense",Neutral
Intel,"I agree with you wholeheartedly, but super users do what they do best - sudo that shit. x)",Positive
Intel,"I've never had AMD Chat or Privacy View force install, I hate they show up in the available software to install when updating, but I just dont click to install them lol, just update the gpu/chipset drivers",Negative
Intel,I guess a snippet of the faulting component from the output would work.  This is a minidump. Do you have a kernel memory dump>?,Neutral
Intel,"sorry i missed this, seems it had expired. maybe someone downloaded it before i did?",Negative
Intel,"Right now in devmgr with re-running the driver installer from the site things look like this https://u.numerlor.me/2faMBA . I also remembered adrenalin has full driver details and everything looks fine there https://u.numerlor.me/w1Snxw https://u.numerlor.me/EOclpA so I think it was just the installer being a bit confused.  Compared to the installer on the first screenshot, when doing the actual update (from inside adrenalin) the Radeon Graphics was a separate item, and had a ""Download driver"" or something along those with the link I mentioned",Neutral
Intel,What about the combined exe? It's still available? That will install both gen but was bugged with control panel disappearing on previous driver.  The combined exe is around 1.6GB.,Neutral
Intel,This might be it? Worth a shot I suppose.  Edit: This worked for me on the latest driver  [https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),Neutral
Intel,geenz@ but yes,Neutral
Intel,any news? SL are not updating their customers with anything constructive and it is affecting most of us.,Negative
Intel,"Hmm, when I can, I’ll have another look! Thanks!",Positive
Intel,"I see. Is this specific to CS2 or does it occur with other apps on your end?  We're presently tracking and working on TDRs in that game specifically, though I'm kind of worried in a way that clock limiting works around this failure.",Neutral
Intel,"I do not, only the minidump but I've uploaded it again here [https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN\_IxLREw](https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN_IxLREw)",Neutral
Intel,"Yes it should be fixed under that scenario, and the combined package is linked on the release notes:  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html  https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe  Kind of guessing here but I believe the '-c' towards the end of the file name denotes a combined package spanning RDNA support.",Neutral
Intel,This worked for me btw - did it a few days ago before these drivers dropped. When I update I'll be using the same method.,Positive
Intel,thanks a bunch. I'll pass this on to my ISV contact and see where we get with that.,Positive
Intel,You can find it here [https://github.com/secondlife/viewer/issues/5048](https://github.com/secondlife/viewer/issues/5048),Neutral
Intel,news ?,Neutral
Intel,"COD is the greatest offender - I can't even get through the training course for Zombies without a black screen>driver timeout message, even if I try to speedrun it in a way (because I've attempted it so many times) it is inevitable it's going to crash, that one crashes with this [error](https://i.ibb.co/KjxynXH5/image.png).  Again, NO OC is applied. Other than the ram running at 2666, which as stated with both mem tests successful and went through both by Karhu's test and Memtest, have no issues. Including no issues with my previous GPU,2080ti, again. CS, I can't even start a match with friends because it'll inevitably crash randomly, sometimes it is within 5-10 mins, sometimes it is near instant in a couple of minutes. Tried everything from 25.12.2 to 25.9.1. PSU is a RM1000e, using the 12pin cable natively from the PSU. It is all the way in, this PSU I specifically even got for this GPU as I didn't want to use an adapter to power the card from all the experiences I've read with the 12pin + adapters.  Here is also a [video](https://www.youtube.com/watch?v=cSkaI6WSfJY) of it happening.",Negative
Intel,"huh, that's odd. Do you have any larger files over at       C:\Windows\LiveKernelReports\WATCHDOG\",Neutral
Intel,"Installed the c one. And seems to be working fine. 780M and 6800 here. Still when selecting a specific GPU for a specific app, both energy saver and performance show 6800. This bug has been forever. And it's probably just a registry key when the driver install. Win11.",Neutral
Intel,No updates there,Neutral
Intel,I do actually have one in there that's 17MB from a BSOD yesterday caused by the AMD driver,Neutral
Intel,"My PC won't wake up after sleep on previous 25.10.2 so I have downgraded to 25.9.2. It seems that AMD has added into Known Issue in 25.11.1.  >Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.   Ahhh. It is caused by HDMI 2.1... I can't use a DisplayPort on my LG C2 42"" TV sadly so I'll have to stay on 25.9.2 for now.",Negative
Intel,"wait, so the branching did not happen? for RDNA2? I was under the impression that it was already in effect.",Neutral
Intel,"That last known issue is what a lot of us experience. Not fixed, can't use this one either.  Leaving your pc for long enough, like 25 minutes and your system just BSOD quickly into reboot.",Negative
Intel,I just did a clean DDU install to 10.2 last night because something weird was going on. Of course 24hrs later a new driver drops,Negative
Intel,anyone know if the low gpu usage was fixed for Battlefield 6? I had to roll back to 25.8.1,Neutral
Intel,"After install i cant open Adrenalin app, I get starting up for a few seconds and then it's closed and the tray icon is gone, too 😿.",Negative
Intel,I'm going to wait to see if others find it stable before I move on from 25.9.1 I think.,Neutral
Intel,Anyone can tell me if the new release fixes the Adrenalin Panel not showing when trying to open it? I’ve spent 1 entire afternoon try every solutions given by Google but today the problem is still there…,Negative
Intel,"So what's the issue with Cyberpunk 2077? It's been present for quite a few updates now.   I'm asking because I've owned the game since day one but I haven't been able to play it cause my old 1060 6 GB was struggling hard with it, since then I've upgraded to a 7700 XT and for one reason or another I haven't gotten to play it yet but every time I update my driver and check the patch notes it's always a problem with it.   Can anyone with it installed and on RDNA3 tell me if it's playable?",Neutral
Intel,So does this mean Arc Raiders will stop randomly crashing in Windows?,Neutral
Intel,Just installed these zero issues so far!,Positive
Intel,"I dunno what people are expecting from Redstone?  It's on the game devs to implement, Blops 7 has the AMD Ray regeneration element of Redstone baked in.  It's not gonna be some driver toggle and all of a sudden you've got ray regeneration across all games.  Also all the people claiming ""not every game needs an optimized driver, Arc didn't get one"" when the RDNA 2 controversy happened, look, there it is, the optimisations were just late, hopefully it'll fix the crashing some people had in the game.",Neutral
Intel,Anyone know if this fixes the pink artifacting on Chromium based applications for the 7000 series GPUs? Can't check myself because I'm at work.,Neutral
Intel,There was a long delay with the blank screen. Made me a bit nervous,Negative
Intel,At this point i'm sure that cyberpunk will never be fixed.,Negative
Intel,Apparently I'm staying at 23.9.1 because I wanna keep using FSR4 INT8 with my RDNA 2 card.,Neutral
Intel,No fix for being unable to enable Noise Suppression...,Negative
Intel,When does Linux get this,Neutral
Intel,"Unfortunately after updating to 25.11.1 (even with a fresh install after using DDU), on a 9800X3D + 9070XT I am no longer able to open the Adrenalin software at all. It's the same issue as described in this post, caused by some conflicts between having both the RDNA3/4 drivers and RDNA1/2 drivers installed at the same time: https://www.reddit.com/r/radeon/comments/1okhlbw/_/  I had to roll back to the 25.10.2 combined driver which works fine with this setup without any issues, but yeah what a shame. Really hope AMD can resolve this issue shortly in future updates. You would think having both the latest gen AMD CPU and GPU would play nicely with each other, but alas...",Negative
Intel,"Why does the AMD install manager never find the updates for me? The AMD install manager only ever says the AMD Chat is available for install.  To get the updates, I have to uninstall 'AMD install manager' which allows me to manually check for updates in Adrenalin.",Negative
Intel,"Guys, I think I figured something out for those experiencing crashes. My RX7600 was overclocked in the default setting. I created a custom profile that matches the old default and seem to have achieved what appears to be stability in BF6.",Neutral
Intel,i am stable now in BF6 on AMD Adrenaline 25.11.1 ... and the game feels super smooth with FSR on 4k Ultra... so it was the AMD Driver 25.10.2 which was crashing ... annoying :-)  7800X3D + G.Skill Trident Z5 Neo RGB 64GB (2x32GB) DDR5-6000 CL30 + ASUS ROG Crosshair X670E Hero + 7900 XTX + Corsair Shift Series RM1200x,Positive
Intel,"Why is enhanced sync still broken? That was a feature I used to mitigate latency while capping frames to 120 and helped get rid of tearing. Now having it on causes major stuttering in a lot of games. I did come to find out that Vsync can be enabled globally and I'm not sure how long that's been a thing. Since being on the 5700 XT, 6900 XT, 7900 XT and now 9070 XT, I was never able to use it globally but it's been quite a few drivers since I've checked if it worked. I use Vsync and Gsync on my 5070 ti build and notice little to no added latency so I'm glad this is doable with Vsync and Freesync but I did like using enhanced sync and capping the frames to 120 better but this will do.",Neutral
Intel,"It's been almost 6 months and the 9060XT still crashes in DX12 UE5 games, especially with FSR4.  FFS AMD how long is a fix going to take?",Negative
Intel,Windows update keeps trying to update my driver.,Negative
Intel,I'm still having problems with FreeSync stuttering with the RX 7900XTX and this driver. Only when I revert to version 25.9.2 are the stutters gone.,Negative
Intel,No FSR4 on RDNA3 no care,Negative
Intel,"For how long ? One year already...Cyberpunk 2077 not fixed yet.   ""Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.""",Negative
Intel,Yes thank you AMD for fixing Arc Raiders. I had to revert to 25.9.1 to stop the exception access violation issues. This would happen mid-game and I'd lose my entire loadout. Here's to hoping it works.,Positive
Intel,Awesome no stated support for Outer Worlds 2.... I guess driver timeout while playing it is not a driver problem...,Positive
Intel,hardware ray tracing crashes both oblivion remastered and the outer worlds 2 after 5 minutes to an hour of play and it seems completely random on my 9070xt.   To even get it to last that long I had to set my core clock -300mhz and turn off variable refresh rate and hardware accelerated GPU scheduling,Negative
Intel,"I noticed that this fixed my Geekbench scores     When I got my 9070 XT a couple weeks ago, my Geekbench 6.4 scores for OpenCL and Vulkan were about 185K and 187K.  Then they mysteriously dropped to around 135K each, and I believe it was after updating Adrenaline.  Now I just updated to 25.11.1 and I'm back at 184K and 189K for OpenCL and Vulkan.",Neutral
Intel,"> The AMDRyzenMasterDriverV30 service failed to start due to the following error:   The system cannot find the file specified.  Source: Service Control Manager, Event ID: 7000",Negative
Intel,This driver was way better than the version before it(for me at least).,Positive
Intel,"Since switching to this version, I've been experiencing constant driver crashes. i use RX7700XT sapphire pulse GIGABYTE B650 Gaming X AX V2 with 6000mhz 32 gb ram. I don't know which driver caused the constant pink artifacts when I have graphics card acceleration enabled, but it's getting worse with every update. Why AMD? -.-",Negative
Intel,"Sorry, I'm not very expert, I installed AMD version 25.11.1 from 0, before when I started the PC AMD was already open now instead the icon appears but if I press it says ""Amd adrenaline starting up"" is everything normal?",Neutral
Intel,For me the driver just times out randomly during normal stuff like youtube shorts. Today I opend steam and the driver timed out. That never happend with 25.10.1.,Neutral
Intel,"The Adrenalin Software instantly closes and restarts if I try to click on the ""Record & Stream"" tab (no crash/error report, it simply closes and then restarts in background).       Dunno if it's from 25.11.1 or not, it was the first time I was going to try it. Didn't tried a DDU full reinstall either, just a simple reinstall of the driver but for no use. Guess I will just use other software for recording so whatever but I'm curious if it's really a driver issue since I got no report pop up at all.  Gpu is a 9060 xt 16 gb.",Neutral
Intel,"I am an RX 6800 user, so on the RDNA branch of the driver,  25.11.1 introduced a severe performance regression on a DX9 game (Fallout New Vegas) , with framerate basically getting halved over what I had before  Downgrading to 25.10.2 fixed the issue , not sure if replicatable (I use 70+ mods) , and not sure if it affects any other DX9 games other than Fallout: New Vegas  (My cpu is an intel i5 10400F in case that matters, in New Vegas, on the 25.10.2 Driver, cpu utilization is almost always between 70-100% on the primary core that the game uses, while on the 25.11.1 driver , it was consistently at or below 50% , which leads me to suspect the new driver caused a regression in CPU utilization in DX9 and/or old single-threaded games, downgrading to 25.10.2 completely fixes the issue )",Negative
Intel,"Still no Redstone update. Well, wait for 25.12.1 just started. Hope AMD end the year with a bang.",Negative
Intel,"Brooooo, they didn‘t fix the flickering in BF6 when recording…",Negative
Intel,* Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.  * Fucking LOL.,Negative
Intel,25.10.2 completely broke vsync... not even a mention about this in the notes?,Negative
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,There is new AFMF features too.,Neutral
Intel,Did they fix the crashing for Outer Wolds 2 on 25.10.2?,Neutral
Intel,bf6 fps drop fixed?,Neutral
Intel,What about the cursor lock when pressing hotkey for adrenaline overlay? Is this fixed,Neutral
Intel,"Hopefully they fixed the anti-aliasing this time...  Nah, i'm sure they didn't.",Negative
Intel,"Don't know if anyone else has experienced this in Battlefield 6 on the last 2 drivers but whenever I uses those my GPU usage always stay at 100% load even with 144 fps cap on a 9070 xt. I revert back to 25.10.1 and then it stops doing that, reaches maybe 80% max in menu",Neutral
Intel,"I've been getting black screens since a while ago on my 6750xt, could be after a random alt-tab when gaming or after logging into Windows, on my tv connected via hdmi it looks green but my displayport monitor it is black, what is weird is sometimes  I can win+L and see the login screen again but if I log in it goes black, also while it is black and pc hasn't frozen yet I use an app called chrome remote deskop and I can see and do stuff from my phone, weird. Tried DDU, new drivers, nothing fixed it.",Negative
Intel,How is the driver ? 7700 XT here.,Neutral
Intel,Finally a potential fix for CPU metrics? Look forward to seeing if it’s true!,Positive
Intel,Didn't they just release something already? Now we're getting another like. Do I have to update my rx 9060xt,Neutral
Intel,do yall use ddu for every driver or do yall just update it with the app?,Neutral
Intel,"New AMD update 👏👏👏👏, I'll install it! Send Redstone as soon as possible!!!!! Thanks AMD!",Positive
Intel,I just can’t wait for the instant replay to be fixed. Ever so often when I save a clip the infame notification starts glitching and I know that means the video I’m saving will have graphical glitches as well. It looks like big-ish squares of the image af slightly out of sync with the rest.,Negative
Intel,"There is a bug with Minecraft when using embeddium/rubidium or any forks on the latest driver. Many textures don't render at all. Launching through curseforge fixes it, which is very strange...",Negative
Intel,I'm not seeing this on my 6600xt. Only say 10.2 is available. Do I have to upgrade to that then upgrade to 11.1?,Neutral
Intel,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.    Well that probably explains the crashes from the last driver whenever I locked my PC but that workaround is not an option. Good to see it's been recognised and being worked on at least.,Negative
Intel,Has anyone else been able to get FSR4 to work again with BF6? Worked for me before the season 1 update.,Neutral
Intel,Think this broke Vulkan in POE2,Neutral
Intel,"Hi [u/AMD\_Vik](https://www.reddit.com/user/AMD_Vik/), other players and myself are having problems using DXVK with latest drivers. From driver timeouts to black screen. I'm particularly having problems with Fallout New Vegas, but there is reports in other games. How can I help in fixing these issues? Examples: [https://github.com/doitsujin/dxvk/issues/4999](https://github.com/doitsujin/dxvk/issues/4999), [https://github.com/doitsujin/dxvk/issues/5204](https://github.com/doitsujin/dxvk/issues/5204), [https://github.com/doitsujin/dxvk/issues/4851](https://github.com/doitsujin/dxvk/issues/4851)",Neutral
Intel,"After installing this update neither Cyberpunk 2077 or The Witcher 3 will launch through Steam anymore. The hitting play just attempts to launch the game and then turns right back into the play button. Only CD Projekt games, no issues anywhere else.  UPDATE: Actual games run fine if launched directly from their install location. It's the CDPR launcher that Steam usually auto opens that broke after this update.",Neutral
Intel,Hope this patch will fix the driver crash while playing Arc Raiders. It crashes in a way that slows my computer so bad and i have to restart it. Temps are fine. Everything is off except image sharpening.,Negative
Intel,I mean Arc Raiders runs perfectly fine even on 23.9.1. Game optimized new drivers are a joke.,Negative
Intel,Hi u/AMD_Vik  Thanks for the VR refresh rate fix. However a bug that I though was related but still isn't fixed are the Beat Saber VR game wall shaders as they are still broken/distorted. Those shaders work on 24.12.1.  Could you investigate this u/AMD_Vik?,Negative
Intel,I never updated to the most recent driver but when I open up adrenalin and the update manager it only shows the previous one 25.10.2  I had to go to the link to get the newest. Does it always work this way?,Negative
Intel,"Wish i could use this software propelry for my 7800xt, everytime i have adrenaline installed after couple hours of gaming the whole pc black screens and gpu driver crashes running with default settings on the gpu. You have to use DDU to get it back running, gave up with adrenaline and installed only the bare bone gpu drivers without adrenaline and installed msi after burned, it has been running for a month just fine under very excessive loads.",Negative
Intel,"25.10.2 already have Terrible Fps spike and stutter in Gaming, this update did not fix the Problem (wth happen amd??).. 25.9.1 is Still the Stable one",Negative
Intel,Did this fix the insane fps drops in 25.10.2?   Reported here https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps and here https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,Neutral
Intel,"Installing this on Windows 10 got me a system shutdown at the first go xD. Luckily, the second one went just fine.  Also, since you seemingly cooperate closely with Activision on CoD, can you fix CoD:WWII constantly crashing? I just bought a month of Game Pass to play the game, but it's basically unplayable.     It'd be nice if you somehow fixed CP2077 situation - FSR implementation, bugs etc. This game is a showcase every single reviewer runs, not Call of Duty...",Negative
Intel,subtract strong cats brave outgoing husky coordinated important rustic juggle   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,I can't even install it anymore as it doesn't recognize my iGPU (I have an R5 7600).,Negative
Intel,"The new version 25.11.1 still has the same problem that I had with version 25.10.2, that is, if while I'm in the game I press the Windows key on the keyboard, then when I return to the game the mouse cursor no longer works and I can't do anything anymore, which forces me to restart the PC, another problem is that when I open any game the overlay of the active Adrenalin techniques no longer appears in the top right, for the rest it seemed ok, but given the big mouse problem I mentioned above, I am forced once again as it was also for 25.10.2 to go back to version 25.9.1 which to date is the best and bug-free for my configuration with RX 9070 XT.",Negative
Intel,"Drivers fine for me on Arc Raiders so far, not had any issues with AMD drivers using a 9070",Positive
Intel,I'm glad the CPU metrics are showing again,Positive
Intel,"When I install this driver, I can't open the AMD Software any more. The start-up splash screen is shown for about a second and then it closes again. Doesn't matter from where I try to launch it. So it isn't the right click -> open bug.  There is no event in the Event Viewer.  I've reinstalled it, with prior DDU cleaning and disconnecting the internet connection, three times now... to no avail.  Anyone else?  Edit: Reverting back to 25.10.2 and it works fine. I'm tired of all these little quirks and annoyances I've had since I went for an AMD card...  Edit2: Tried 11.1 again and it worked now. The software started... once. The next time I tried to open it I got:   Download failed: Please visit [AMD.com](http://AMD.com) to download the compatible version of AMD Software.    I'm at the end of my rope here AMD... really getting tired of this",Negative
Intel,"Went to do the usual ""Leave AMD Experience Program"" after uninstalling the Installation Manager, but the option is gone.",Neutral
Intel,The update did not help. The problem with the driver crash remained (( ( Rx 7700 xt ),Negative
Intel,Any ideas for when the crashing when playing NBA2k25 is going to be fixed?,Neutral
Intel,"Is there another work around for system locking up?  My PC monitors never even turned off. I woke up this morning and the PC was simply frozen, had to turn off the power supply switch and turn it back on for it to work.  Couldn't even just do a hard reset.",Negative
Intel,"Im still having issues with easy anti cheat, rust game keeps crashing after a few minutes, maybe 2 or less",Negative
Intel,"2.5.11.1 fastest reroll for me to date, well done.   Booted arc raiders which now have support.  Game does not boot, instead I get a message frem arc davs that the driver has issues and want me to reroll to 25.9 😅   What a fucking joke",Negative
Intel,Shits been crashing my system since the update :( sapphire 7900xt,Negative
Intel,"Tested the new driver on 7700 xt, pink artifacts in browser and some weird flickering, some old bugs are fixed but  there are new issues instead, honestly it is not worth to update drivers at all if you find one driver that works without issues.",Negative
Intel,Still weird artifacts on COD MW2 game. Turned back to 25.9.1.,Negative
Intel,"With my Taichi RX 9070 XT OC after updating to the latest version 25.11.1 I still encountered the same problem that I encountered with the 25.10.2, which is that while I play Battlefield 6 press the windows button to go to the desktop and then return to the game my mouse crashes and I can't do anything, the only thing I can do is click ctrl+alt+delec and the mouse works and then I can restart the PC from there",Negative
Intel,"With this new driver, Adrenaline isn't automatically detecting Epic Games Store games (Steam games work fine). I tried with Fortnite, and it only adds to the games tab after launching it, but it doesn't work with ARC Raiders. I tried adding it manually, but I couldn't get FSR4 to activate. Is anyone else experiencing this?",Negative
Intel,"GV-R9070XTGAMING-OC-16GD    I have problems with Graphipcs since day one i bought from amazon.de. There is no driver that prevents some games from black screen and crashing on desktop, also Adobe Effects alongside Fortnite and others. The error is always amd software has detected a driver timeout on your system. Everytime i send logs, they updated drivers every 14days but no driver helped. I also made registry fix with increasing timeout from default 2s to 8s. Tlddelay did nothing, random black screens and app crashes to desktop. Also tried another cable DP instead of HDMI.    What can i do ???",Negative
Intel,"If anyone from AMD sees this. The recent drivers cause The Division 2 to consistently crash. It sometimes happens after 15m. Other times a few hours but is GUARANTEED to happen at some point. When it happens it's a hard reset case and doing so after about 10 times eventually wrecked my boot/login so I had to re-install Windows (bad AMD, spank!).   I assumed it was an issue with thelatest Windows update like the one that broke the other UBI Assassins Creed games a while back but when I installed 25.9.1 (with factory reset) I haven't had a single crash since.     7900xtx & 98003d.",Negative
Intel,Dose Arc raiders works now or not on new release 25.10.1 had prob with that game could not run it must go dx11,Negative
Intel,"Since this driver update my pc is unusable, only one screen loads the other stays black and after the os loads the screen just freeze, I can hear os sound like connecting and disconnecting of USB but the picture is frozen, I have the rx7900xt I tried to completely take the GPU off the motherboard and connecting back (after reinstalling the driver with factory reset when connected to the cpu display port) and it worked for some hours but after shutting down the pc and booting the next day it came back, you're saying the only fix for now is to roll back to previous driver?",Negative
Intel,У меня Мультимедиа контроллер выдает ошибку. Для этого устройства отсутствуют совместимые драйверы. (Код 28),Neutral
Intel,Noise Suppression still broken. 3rd release without that functionality in a row.,Neutral
Intel,"Hay un bug que me suele pasar con varios de los ultimos drivers... cuando desintalo los drivers, la pantalla no vuelve, y no me deja saber cuando la desinstalación del driver termino, debo reiniciar la PC. Con RX 6800 XT.  Le eh pasado DDU, pero el error sigue estando.",Neutral
Intel,"Not sure if anyone else is experiencing this, but after this update, Adrenalin acts like BF6 isn't open so I can't force frame-gen thru the driver. On both 25.10.1/25.10.2 and 25.9.2, enabling frame gen in game doesn't work, so I've had to do it through the driver. On 25.11.1, NEITHER are working, frame gen completely non-functional. Tried DDUing/factory resetting 25.11.1, didn't work. Rolled back to 25.9.2 and works normally again...",Negative
Intel,"for some reasons, whatever game i play it either closes itself or looks so bad visually that the games (yes, games) are unplayable so i have resorted to uninstalling all AMD graphics software (drivers and applications) and am going to try to re-install it and see if i can choose a previous driver",Negative
Intel,is the horrible stuttering/flickering (feeling like dynamic hertz and micro stuttering) experience from the 25.10.x drivers fixed? If not I have to stay on 25.9.2,Negative
Intel,"This version, perhaps even the previous one, installs the AMD Adrenalin Edition software even if I select Driver Only when installing the drivers. Can you solve it?",Negative
Intel,"Adrenalin 25.11.1 terminating itself right after launching.  Can't run Adrenalin UI(App, Program...)  Seems like iGPU & multi-monitor related problems.  for more info [https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin\_25111\_not\_opening\_after\_update/?sort=new](https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin_25111_not_opening_after_update/?sort=new)  I'm going back to 25.10.2",Negative
Intel,Anyone else having trouble even getting the software to open since the update?   I've done a clean uninstall and reinstall of the drivers and software twice and Adrenaline won't even open.,Negative
Intel,"Hi u/AMD_Vik  im still waiting more than month legion go 2023 amd vga driver get released update latest for arc raiders,bo7,but asus rog ally x and xbox rog ally x yesterday updated already.is there a chance,we receive an update for legion go?also amd chipset driver very old for legion go.thank you if you answer me 🙏",Neutral
Intel,"Am I the only one seeing this bug in the metrics overlay? there are two ""gpu temp"", one would be that of the cpu,but written wrong. while the other metrics are written right,I already tried a clean reinstall with ddu, but nothing",Negative
Intel,"Anyone else getting per game Settings not being able to be changed? It sticks to just one whenever you click on a slider, this update and the last had it. Apparently older versions didn't and the only other fix is through screwing with the BIOS which i'd rather not.",Negative
Intel,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable,Neutral
Intel,Still not working AMD NOISE S,Negative
Intel,New Game Support: ARC Raiders  I updated to this driver thinking it would be better for ARC Raiders since that is what I am playing right now but my game is crashing if I try to load into the Blue Gate map. I tried restarting my computer and it still happens. I almost lost all my gear because it took me a bit to roll back my drivers to 25.10.2. Come on AMD do better! It's a supported game on this driver! I surprisingly never had crashes in 25.10.2 unless I toggled the Adrenalin game overlay.  Running a 9060 XT and a 5900X.,Neutral
Intel,"I updated from 25.10.2 and saw the bug report tool pop up after restarting. I had no idea what caused it. I launched Battlefront 2 and the entire system froze, with WinAmp trying to play audio which sounded horrendous. I uninstalled Adrenalin, used DDU to clear everything then did a fresh install of 25.11.1.  On restart, the bug report tool showed again. This time Wallpaper Engine failed to show on the second monitor. I quit the program and started it again and boom - system freezes entirely.  So, this driver apparently has big issues with Wallpaper Engine. Uninstalled, DDU'd, fresh install of 25.10.2 and smooth sailing ever since.",Negative
Intel,I still have issues with the combo : 9070XT + PSVR2 + F1 25 in VR.   The image in VR is still bugged. The only version that is working is still 25.4.1,Negative
Intel,Software doesn't open at all for me. Used DDU but still doesnt work. Deleted CN folder from appdata aswell.,Negative
Intel,"After update I can't open the app. It just loads and crashes. I fully reinstalled windows and the error still persists. I can't update drivers or access the config, I can only download drivers externally. Any insight of what it might be?",Negative
Intel,"@AMD_Vik I still do not see CPU metrics after update via Adrenalin software, what to do?",Negative
Intel,"I'm new to this, i'm on 25.10.2 do i update? the only issue i have is fps drops on fortnite but other games i play are alright (r5 9600 igpu) it usually runs at 60 fps on performance mode but since last weeks of last season it started doing that",Neutral
Intel,"Unfortunately, version 25.11.1 does not start with Windows.",Negative
Intel,Is AMD going to come up with another driver soon?,Neutral
Intel,"My RX 7900 XTX now no longer run Frame Gen. The game becomes completely unusable even reporting 200+ fps, it still stutter like crazy.     Without Frame Gen all good, with Frame Gen, completely unusable for me :/",Negative
Intel,Over a few days after installing 25.11.1 on my 7900 gre system I had a few driver timeouts followed by a major screen-freezing crash which corrupted my drivers. DDU'd and rolled back to 25.10.2 and haven't had any new ptoblems.,Negative
Intel,"After installation 25.11.1 (from 25.10.2)  black screens entered the chat. After DDU and rollback to 25.10.2 they stayed, and after rollback 25.9.1 the same... RX 5700 XT. Sadly 😞.",Negative
Intel,"is there 25.11.1 for windows 10? the filename that i downloaded from AMD website is ""whql-amd-software-adrenalin-edition-25.11.1-win11-s"" where usually its filename includes windows 10 along the lines",Neutral
Intel,"they need to fix the BF6 texture corruption glitch, it's annoying af. had to roll back to 10.2",Negative
Intel,Any word on fixing the driver timeouts on the 7900xtx its a bloody joke worst gpu i have ever bought,Negative
Intel,Any of you also have issues with afmf2 and the game not opening adrenalin software or showing performance counter after enabling it?,Negative
Intel,"this shit was fucking with my PC, DDU current drivers and reinstalled 25.10 straight from Gigabyte Program and everything works again",Negative
Intel,getting bsod randomly since 25.9.1 sad..,Negative
Intel,"I started having an issue since the 25.11.1 update with unreal editor where all of my tools menus instantly close, nothing else changed except for this driver update and I've heard of Nvidia having similar issues with driver updates in the past so I think it may be the cause, Going to revert to an older driver and see if it works",Negative
Intel,"I've spent the last few days uninstalling, reinstalling, DDUing, doing everything I could think of to get Adrenaline to start/work. It would show the splash screen and then quit. No way of re-starting it. Couldn't open anything that used Vulkan and got errors. Couldn't install the Windows Store version cause ""driver error"". I eventually used DDU one last time and uninstalled everything AMD and was able to just install the driver through MyASUS. Now I'm able to open all the software again that wasn't starting before. I'll be holding off on installing Adrenaline again anytime soon. Sucks cause I want the features, but I couldn't use the programs anyway. I miss having nvidia.",Negative
Intel,"It seems on the latest Radeon driver that freesync is broken within CS2 when running fullscreen windowed. Freesync works initially when the game starts. But as soon as I alt tab, freesync breaks and I get screen tearing. I rolled back to 25.9.1 and I can confirm it works again as expected. So it seems this is a recent regression. Can we get this addressed please? u/AMD_Vik",Negative
Intel,"Been having issues with VLC freezing and stuttering during playback (video only, not audio) since anything after 25.9.1. Guess I'm gonna roll back to that until it gets figured out.... really frustrating.",Negative
Intel,Substance Designer won't start with this one. Access violation with amdvlk64.dll. Adrenaline won't start either,Negative
Intel,"Sorry but for me the drive give me crash pop up message every time i boot up my pc. Also just right now i got a freeze, black screen to all my monitors.",Negative
Intel,The worst driver this year so far,Negative
Intel,"Still havent fixed the noise cancellation lmao, guess its another month+ of old version :) Thanks amd, truly doing wonders.",Negative
Intel,CS2 crashing with driver timeout after tabbing out or watching streams on 2nd screen 7900xtx,Negative
Intel,"When is 25.12.1 coming out? I have read only bad things about 25.11.1 here, so I wanted to skip this one.",Negative
Intel,"Can we use FS4 on rx6000 series now without it crashing now on this driving now, or do i always need to keep downgrading my driver ?",Neutral
Intel,So no redstone yet,Neutral
Intel,FSR AI frame gen??? Didn’t they say that’d it would also have a driver toggle?,Neutral
Intel,Did AMD ever add support for Cronos?,Neutral
Intel,Well Star Citizen will load now!  Now some longer term testing....,Neutral
Intel,Anybody tried this with Anno 117 yet? I’m hoping it helps performance,Positive
Intel,Problemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemas,Neutral
Intel,NO LA DESCARGEN ES MAL LAGGGG EN LOS JUEGOS,Neutral
Intel,So are the issues with Arc Raiders fixed? I had to roll back to 25.9.1 because 25.10.1 kept crashing my game. Did they actually fix it?,Negative
Intel,Disabling ULPS seem to fix the crash. My pc crash pretty often when i wake the screen or turn it on if i dont disable ulps with msi afterburner. 9070xt,Negative
Intel,Doom: The Dark Ages does not lauch with the latest driver. I had to rollback to 25.9.2 in order to play. Please fix,Negative
Intel,Last driver crashes Apex Legends every joined game. Im fuckin over amd.,Negative
Intel,Support for Anno 117: Pax Romana - Incorrect. I have needed to downgrade back to 25.9.2 to play Anno 117: Pax Romana without crashing. The last updates have been a joke. I now cannot use adrenaline due to needing to play on this earlier version to be able to play any new games. Does anyone know of where I can express my complaints?  Edit: This is also the same for Arc Raiders.,Negative
Intel,"Here we go again, jetzt stürzt Battlefield 6 wieder ab. Mit dem Treiber davor hatte ich es in den Griff bekommen außer XMP war aktiviert, dann stürzte es dennoch ab.    Also es scheint definitiv ein AMD Treiber Problem zu sein.    Gut das bei euch die Kunden die Tester sind und nicht ihr das übernehmen müsst.  PS: Gespielt wird mit einer 7900XT und einem 7800X3D.      Das war definitiv meine erste und letzte Karte von AMD. So viel Probleme hatte ich mit Team Grün nicht.",Neutral
Intel,Yeah same here LG c5 42inch 😰,Neutral
Intel,"Having system crash issues after putting the PC into sleep mode. Samsung 57"" Odyssey Neo G9. Now I know who to blame.  Reinstalled AMD drivers and changed the settings so that my PC never gets into sleep mode (turns off the screen, but doesn’t sleep or hibernate). This fixed the issue temporarily for me :(  Also from the system crash minidump, it's very clearly an AMD driver issue  **IMAGE\_NAME:  amdkmdag.sys**",Negative
Intel,"I have this but on display port, HDMI works fine",Neutral
Intel,"DDU with full uninstall of all AMD related things and then chipset driver install, fresh GPU driver install fixed the crash from wakeup for me.",Neutral
Intel,"I had to go back to version 25.9.2 but I no longer have AMD Adrenalin. If I try to install it, it reinstalls version 25.11, which crashes my game. Is it necessary to have AMD Adrenalin? I have a 7900 XTX and a Ryzen 9 7950X3D.",Negative
Intel,Could you try a DisplayPort to HDMI adapter? I wonder if it works in this situation =D,Neutral
Intel,I have the same issue with display port but it’s okay with hdmi :/,Neutral
Intel,"Honestly, I plan to make sure my next display has Display Port in it. Mostly for linux though.",Positive
Intel,"There will already be branching inside the code of the driver. This has been the case already, to various degrees, for years. It's not new.  It's just whether AMD wants to formally spread those branches out on a file / compilation level and distribute different packages. And then publicly whether they commit to updating all branches of code or only some.  For any particular bug / feature / optimisation, there will be some cases where it's the same code path for practically all RDNA versions, you fix it once and it applies to everyone. For some, it might be very similar but not the same, just some slight tweaks and what gets fixed in RNDA3 can also be applied to RDNA2. For some, there's some hardware feature of RDNA3 that would make the fix easier there and it will be much more work to adapt the same to the RDNA2 branch. Of course, we have very little outside insight into the exact spread of these cases that AMD wants to pay their engineers to work on.",Neutral
Intel,There is a separate code path for specific things bit both are in those combine driver. Its mostly about certain ray tracing extensions   There are also fp8 and fp16 codepaths    People misunderstand and thought its like pre rdna stuff.,Neutral
Intel,"V25.10.2  here… I have both CPU and GPU by AMD and if you download the specific package, they install drivers only for the specific hardware (and they have different dimensions). For installing both drivers you have to download the AutoDetect package.  EDIT: typo",Neutral
Intel,"I'm one of the 5 people still running a Vega 64 and for years we've had a separate driver ""branch"" despite being able to install new Adrenaline versions. Bf6 beta wouldn't run without spoofing my actual internally installed driver, which hadn't been updated since they dropped support.",Negative
Intel,combined again it looks like 🤷‍♂️,Neutral
Intel,the display team are working on this with priority. Hoping to have this out in the next release. There are two similar display issues on their radar which are both P1.,Neutral
Intel,"what is triggering this? I can let my amd pcs run all day without any crashes. (7900XT,5700XT and 6900XT)",Negative
Intel,"Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  Not criticizing, just curious as to use-case.",Neutral
Intel,You try install last chipset driver ?,Neutral
Intel,"Bummer you're having issues. Hopefully AMD gets it straightened out for you. Out of curiosity, why did you decide to use HDMI on your monitor instead of display port? I thought HDMI was mostly used for TVs nowadays.",Neutral
Intel,So it's the driver that's why that happens 😡 and it's not fixed?,Negative
Intel,Thank you for your service,Positive
Intel,"I've had the ""device_hung"" error and the best drivers are those you are on.   AMD needs a lot of help with their drivers...",Negative
Intel,Any update mate?,Neutral
Intel,"No, radeon drivers update cannot upgrade your CPU so that it would stop bottlenecking your GPU.",Negative
Intel,"Had the same issue, just ddu and reinstall the drivers manually, problem now is with my hardware monitor screen not working, I have an aorus board with an internal hdmi, that is not being detected, will try to reinstall chipset drivers",Negative
Intel,"multi gpu? got the same problem with rx 6400 + 7900xtx. plugging in my secondary display to 7900xtx fixed the issue, but what's the point of secondary gpu if its not working properly...",Negative
Intel,"Workaround:  Win+P and disable the second Monitor in iGPU, then start adrenaline and you can enable the second monitor again  software should be okay then",Neutral
Intel,"I'm the opposite, I just want adrenaline app to stop everytime I right click on desktop or open file explorer.... Bruh",Neutral
Intel,"yeah same, my experience with 25.10 was terrible, had to DDU it once to get back my CPU metrics in Adrenalin, then DDU’d it again to go back to 25.9.2 since games were stuttering.",Negative
Intel,Same.,Neutral
Intel,"Not stable, ddu install 11.1 and lag in old dx11 mmo game(BDO)  Same as 10.2, 9800x3d with 7900xtx, i can keep 144fps in town , 11.1like  fps 60-65 and lag spikes  Im go back to 9.1",Neutral
Intel,"Both 10 and 11 are shit with 7xxx series, myself and most others have gone back to 9.1 or 9.2. Might be alright with the 9070, might not",Negative
Intel,I'm playing it on 7900 GRE with FSR4 INT8 with no issues.,Positive
Intel,na it works the problem only occures when using PathTracing and you are not going to play on an AMD card with PT anyways and especially not a 7700XT,Negative
Intel,"I played recently the latest patch of Cyberpunk 2077 with RX 7700 XT with drivers from February this year, 25.2.1 and had no issues, no stutter, no lag, no crashes, the problem is with path tracing or ray tracing something, you don't have to use that even and it lowers fps probably for very little visual gain.",Positive
Intel,If it still crashes set RTX Global Illumination to Static.,Negative
Intel,"pretty sure anti lag was causing my system to freeze up when in arc raiders on 25.10. turned it on and issues, turned it off and haven't had issues since",Negative
Intel,"Optimizing has nothing to do with fixing crashing, generally, that would be bug fixing.",Negative
Intel,"The only Redstone component that will work through the driver for upgrading existing features (like FSR3 FG) will be ML FG, but only for games that have FSR 3.1.4+(and FSR FG), as they’ve already announced in a GPUOpen article quite a while ago.   Ray Regeneration (RR) and Neural Radiance Cache (NRC) require dev implementation since they’re much deeper in engine code/inputs.",Neutral
Intel,"Does BO7 have ray tracing? Where is that feature mentioned, would be keen to read about it 🤓",Neutral
Intel,"Itll probably work through optiscaler, but a game like indiana jones has locked dlss inputs, so you need dev implementation",Neutral
Intel,"And all my USB devices dropped for a while, far worse experience than the usual double screen flicker & it's done.  Edit: seems like that was because of iGPU driver for my Zen4. In the end and after 6+ reinstalls, I'm stopping on 25.9.1 and have Vulkan working again.",Negative
Intel,What is wrong with CP? Just got my 9070 after 10+ years of nvidia :o   Ah nvm i see it.,Negative
Intel,Ugh,Neutral
Intel,"Most likely when your distro provides a kernel update.  I'd give this a quick read: https://www.gamingonlinux.com/guides/view/how-to-install-update-and-see-what-graphics-driver-you-have-on-linux-and-steamos/  Pretty much the default is that you will be using the Mesa driver collection and they were last updated yesterday.  The fixes there do not correlate with the ones in this thread (I think).  As long as you are on a ""cutting edge"" type distro then you can expect that update in the next few days/weeks.  If you are on a ""stable"" distro that doesn't update often, you may be waiting a realllly long time.",Neutral
Intel,"Get what exactly? Features? Bug fixes? Optimizations?   Every 2 weeks there are new Mesa driver updates with bug fixes and performance optimizations. As for features, you have to wait a few months for versions with big numbers (25.2 is soon updating to 25.3, adding Anti-lag 2).   Depending on your distribution, you can get these updates faster or slower.",Neutral
Intel,"Linux doesn’t work like windows. You get updates directly via Mesa stack. When your distro provides mesa package update, that’s when you get driver updates, and they’re completely different from windows branch.",Neutral
Intel,just uninstall it I prefer manual check myself.,Neutral
Intel,So AMDs default driver overclocks and doesn’t reflect that in the values?,Neutral
Intel,Same issues here i underclocked it but this new update just made it worse,Negative
Intel,ok it is still crashing ... complete reboot :(,Negative
Intel,"I feel like that crash is more on DICE's side, since Nvidia users get the same exact crash, although less often.  I tried everything I saw on the internet, nothing really works. Sometimes I can play for hours on end, other time game just crashes randomly after 10-15 minutes.  I am going to try to downgrade to 25.9.1 and see how it fares, since I remember that driver being really stable for me (6800XT).  Edit: been playing for 4 hours, no crash yet. Never had such a long session without the game crashing.  Will update in the next few days.  Edit 2: haven't crashed once, been playing at least 2 hours every evening.",Negative
Intel,Okay.,Neutral
Intel,I’m hoping Valve’s new steam machine will push them on that since it’s RDNA3 based.,Neutral
Intel,The RT/PT reflections leading to CTDs issue? We're working with CDPR to resolve this - it's not caused by the driver software.,Negative
Intel,"Stuff like this is honestly bugging me.  First AMD card and while it is okay to have some driver issues, stuff like that shouldn't be a thing at all. It can't be that some bugs keep existing for multiple months let alone more than half a year.",Negative
Intel,A much more fundamental thing like playing video in a window without stuttering took 3 or 4 driver updates.,Neutral
Intel,"> One year already...Cyberpunk 2077 not fixed yet.  On year, I did not know.  I was thinking of a 9070XT earlier in spring, but choose... differently",Negative
Intel,welcome to amd,Neutral
Intel,Yeah hoping it works. I guess the point about crashing with easy anticheat was the fix? Doesn't specifically mention but it was with raytracing enabled I was getting the crash. I suppose we shall find out.,Neutral
Intel,"Weird. I was running 10.2 and things were running fine for the most part. I kept getting a pop-up on launch saying there was a known issue with that driver and Arc Raiders though, so I rolled it back to 9.2 and had more issues on that release than the one with the stated problems.  My fan control profile broke and stopped tracking gpu temp so last night I did a clean DDU install back to 10.2.  Of course 12 hrs later the new release drops haha  Hopefully Fan Control continues to play nice if I just update",Negative
Intel,Same. Never even had Ryzen master installed.,Negative
Intel,"I'm receiving the same error in Event Viewer, but I have installed Ryzen Master. Most likely it's also a component of the Adrenalin drivers for system tuning and monitoring.  Registry search shows two keys for ""AMDRyzenMasterDriverV30"" (in both CurrentControlSet and ControlSet001): Computer\\HKEY\_LOCAL\_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\AMDRyzenMasterDriverV30  The ImagePath points to: C:\\Windows\\System32\\AMDRyzenMasterDriver.sys and the file exists. It's valid.",Neutral
Intel,25.2.1 had no pink artifacts on my Pure 7700 xt Sapphire but i switched to 25.11.1 and got it now also and also some flickering.,Neutral
Intel,What is redstone?,Neutral
Intel,What's weird is Black Ops 7 has ray regeneration.,Negative
Intel,"I'm afraid it may be released in January like they drop new major this year like AFMF 2 upgrade, FSR, etc..",Neutral
Intel,vsync issue fixed with win 11 KB5068861 update.,Neutral
Intel,had no issues with vsync on 25.10.2,Negative
Intel,works fine for me,Positive
Intel,I agree.  I also have this issue  Weird is if you turn on amd overlay it fixes itself,Neutral
Intel,"That it did, lol. My only complaint.",Neutral
Intel,"Not a problem on Linux with Wayland. Ever.   Main reason I ditched Windows 11, constant screen tearing and input lag made me go crazy.",Negative
Intel,The new AFMF features were added in 25.10.2 - Still waiting for AFMF 3.0 at this stage,Neutral
Intel,Suggest if you are using powertoys to stop... After their last game update it was crashing left and right. Powertoys was cause. I only crashed once and I believe it was cfeen blanking was cause for that. I was away too long.,Negative
Intel,"Fps drop over time? That's a game issue, it's got a memory leak",Negative
Intel,"What kind of FPS drops? I was having dog shit frame pacing, and hardcore drops to like sub 60 FPS at 1440p medium/high with a 6950XT. I went through every setting and found that anti-lag was the culprit. Once I turned that off it all went away. Then with future frame rendering it got even smoother. Now I can hold 100+ FPS at all times, sometimes peaking over 200 FPS in some scenarios. And frame times are smooth as butter.",Negative
Intel,I’d settle for bf6 going one entire game without drivers crashing the game and freezing pc,Neutral
Intel,"I'm also having this ""problem"" with driver 25.10.2; I haven't tested it with 25.11.1 yet.",Negative
Intel,Crashes?,Negative
Intel,I have this problem in all games.,Negative
Intel,"Hello, I've been having this issue and I have exactly your gpu and cpu, whenever I played valorant and I alt tabed many times the screen goes black and keyboard become unresponsive but I can still hear friends in discord and they can't hear me, after conctacting valorant support and messing with alot of settings I think  what fixed it for me is to add these in windows defender exclusions : C:\\Riot Games\\VALORANT\\live\\VALORANT.exe   C:\\Riot Games\\VALORANT\\live\\ShooterGame\\Binaries\\Win64\\VALORANT-Win64-Shipping   C:\\Program Files\\Riot Vanguard\\vgc.exe   C:\\Program Files\\Riot Vanguard\\vgm.exe   C:\\Riot Games\\Riot Client\\RiotClientServices.exe   I hope this helps",Negative
Intel,"DDU was only ever for switching GPU families, e.g. Nvidia or Catalyst to Adrenalin.",Neutral
Intel,"Sad to say I am having the same issue. Did you find any workarounds, or just downgrade?",Negative
Intel,"You don't need to make it work on Adrenalin, Battlefield 6 already has FSR 4 support integrated into the game, so you don't need to override, just turn off FSR 4 from Adrenalin.",Neutral
Intel,Works on 25.10.2. I have been seeing ppl said it doesn't work on this driver. It also didn't work on 25.10.1 for me as well.,Negative
Intel,Epic version runs just fine.,Positive
Intel,Cyberpunk GOG last version patch runs fine on this driver.,Positive
Intel,"Hey there, can you give an example of how this looks now versus how it's supposed to?",Neutral
Intel,are you able to install driver only with the newest drivers? When i choose minimal or driver only it still install full version of the software anyway. Only 25.9.1 works perfectly.,Neutral
Intel,Yeah... it can happen. I was already locked outside windows after a bsod with my old nvidia card. I had to use a linux usb drive to fix it through the command line.,Negative
Intel,It's most likely the same as I've described here:  [https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/](https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/)  But yeah unfortunately seems like all you can do for now is either roll back to 25.10.2 or disable your iGPU if you don't use it,Negative
Intel,probably because those issues are build specific.  I've not had any freezing or locks on any drivers this whole year.,Neutral
Intel,"The game is booting, this message was for the 25.10 they just didn't removed it",Negative
Intel,I don't know your system specifications but in many cases setting the BIOS to PCIe v4 could help.     Try change in your bios,Neutral
Intel,no I hit alt R opens right away.  And i did an upgrade install over top of 25.10.2 no DDU or AMD clean up utility.  When you say you did a clean install was that just from control panel and remove?,Neutral
Intel,"First time yes, i downloaded with -s letter, but the last time i downloaded smth like -combined(1.6 gb). All two's is for WIn 11.",Neutral
Intel,"To be clear, are you able to confirm that VRR is disabled after you alt-tab? Do you have a display-side OSD to verify?",Neutral
Intel,"Good call, it caused nothing but problems for me and pretty severe. Were talking driver timeouts with black screens and even a couple bluescreens.",Negative
Intel,"Not even 7000s have support for FSR, you'll have to stay on 25.9.1",Neutral
Intel,My 9070 xt crushes while I try to use fsr 4 on new drivers,Neutral
Intel,Why don't you try it and let us know if you can. Would be helpful for lots of us,Neutral
Intel,It's in Redstone. Still not out yet,Neutral
Intel,Didn't work for me...,Negative
Intel,Wait until you see how much your browser's cache is churning...,Neutral
Intel,Why cant you use Adrenalin? I'm using it on 25.9.1,Negative
Intel,I just received a windows extension update for my LG monitor. If you can boot up go check.,Neutral
Intel,The last time I had this problem it was a RAM issue.,Negative
Intel,I have this for my ThinkPad laptop with its internal display. Good to know I'm not the only one.,Positive
Intel,"if it happens, fill in the bug report that pops up. more reports will help AMD identify the issue better.",Neutral
Intel,I have DDUed the driver before upgrading. I still face the GPU crashes during display standby.,Negative
Intel,"Wait, chipset drivers uninstall too? I'm going to have to fix that on my system...",Negative
Intel,Do u reintall already up to date chipset drivers?,Neutral
Intel,You should use the Factory Reset installation in the AMD Driver Installer so it removes the newest AMD Adrenalin and install back the correct Adrenalin driver.,Neutral
Intel,"I don't have a DisplayPort to HDMI 2.1 adapter to try, sorry.",Negative
Intel,doing so (separation) will create a freak out shitstorm part 2.,Neutral
Intel,> People misunderstand and thought its like pre rdna stuff.  It is. They've simply bundled two separate drivers together. RDNA1/2 are still stuck on the same 21033.x branch while RDNA3/4 have moved first to 22021.1009 and now 22029.1019.,Neutral
Intel,Is it combined though?  7900 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov.exe (25.20.29.01 ?)  6800 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-s.exe (25.10.33.03 ?),Neutral
Intel,All it takes is one random Reddit comment with some upvotes spreading misinformation and you will see people posting articles about it,Negative
Intel,"You have two differnt drivers installed and running at the same time, one for APU and the other for GPU? is that what you are saying?",Neutral
Intel,Combined only as far as the two driver packages are now in one file. RDNA1/2 are still stuck on 32.0.21033.x branch. RDNA3+ are on 32.0.22029.1019.,Neutral
Intel,Last driver was branched right? I never updated coz I had just fidled with the files to make FSR4 (INT8 or whatever its called run) and didn't wish to do it again...,Negative
Intel,Can we please have a fix for the 7700 xt artifacts showing? It's been ignored for so long. Using hardware acceleration on chromium (and not just that) causes pink artifacts everywhere.,Negative
Intel,Thank you for communicating,Positive
Intel,Unfortunately happens to me too. So for me it’s a big issue as I can’t update to this driver until it is fixed 😰,Negative
Intel,"I have a dual PC setup, one system with a 9070xt where I have seen this bsod related issue, mostly when toggling HDR.  The other system has exclusivly an RTX 3090 that also has the same HDMI 2.1 related system bsod, but Nvidia driver instead.   It idles, display sleep, 1 in 5 ish chance it locks up or bsods on wake.  Issue goes away using a non 4k 240hz display.     I believe this system crash is deeply related to DSC on Windows.  I only got these two PC bsods when I bought a 4k 240hz display.  Returned a monitor (bad oled) and the issue went away.  Got a new oled a few weeks ago and now I have these bsods again.     Never had a bsod before I got these 4k 240hz displays.  Fresh Windows 11 installs too between both PCs and between my first and second oled.  Systems are both solid and stable.     Linux works fine, but no HDMI 2.1 support thus no 240hz, which makes using the oled pointless imo.   While digging into my issue, I wanted to see if DP2.0 / uhbe20  compatible display would have these issues, but my current display only has HDMI2.1 and dp1.4.  Hopefully someone else had experience with them on 4k 240hz.",Negative
Intel,Thank you AMD my bad for getting upset,Positive
Intel,Thank you.,Positive
Intel,Any word on when noise suppression will be fixed? I would actually lowkey love a right up and why it’s failing. Would be cool to see the technical details if that’s possible. (I’m actually more interested now on why it’s not working vs just getting it fixed).,Neutral
Intel,Thank you!,Positive
Intel,There's a long-time standing issue with Chrome and hardware acceleration. Will that ever be made a priority?,Negative
Intel,Redstone when?,Neutral
Intel,"LG OLED CX and newer are good monitors.  I am using it with my PC and PS3, PS5, Blu-ray 4K Player...  I must say that that was my very first actual driver issue with AMD.",Positive
Intel,Not always true. HDMI 2.1 has more bandwidth than dp 1.4. I have a ASUS pg32ucdm and HDMI looks better and uses a lower dsc ratio. I have a Nvidia GPU in my main PC however.,Neutral
Intel,"Yes, my 9070XT is paired with a 5600X hooked up to my TV for couch gaming. I have a separate PC with a monitor for desktop gaming. My TV doesn't have DisplayPort, very few did when I bought it.",Neutral
Intel,"I currently run a gaming monitor as my primary one, with DP, and a plain old Lenovo office monitor as my second one. It only has HDMI, so i wouldnt be able to use it with DP.  I assume this may be the case for a lot of people. Getting a ""higher spec"" 2nd monitor really isnt a priority for me, as i just need a second screen for productivity when working instead of gaming.",Neutral
Intel,Non pc monitor tvs are sometimes cheaper especially for larger sizes. I’m on lg c5 oled 42inch and it only has hdmi…,Neutral
Intel,"Not universally true, hdmi and displayport depending on the version supported are very comparable.   Dp 1.4 Vs hdmi 2.1 is where hdmi has more bandwidth, before this dp was the easy choice.",Neutral
Intel,"> Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  ~~Tell this to Valve, who are about to bring out a gaming-focused mini PC which **only** has HDMI 2~~",Negative
Intel,"This was true prior to HDMI 2.1 but no longer the case. However, monitors with HDMI 2.1 do still cost more than those with DP1.4 which has similar albeit slower bandwidth.",Neutral
Intel,"Yes, I am on a television. ""HTPC gaming (and turbo tax) from the recliner master race""!",Neutral
Intel,> Are y'all playing on televisions?  Do you guys not have phones?,Neutral
Intel,LG OLED TV as a monitor. I have LG OLED CX which comes with HDMI 2.1.,Neutral
Intel,My game keeps crashing due to the same. I want this fixed ASAP. You'd think they'd test the biggest recent releases before publishing an update.,Negative
Intel,"I am also having a ton of crashes in BF6 RedSec, but I'm not getting any errors. Game just freezes, then crashes.",Negative
Intel,"I got a new pc 2 days ago and i literally cannot downgrade my drivers without amd forcing me onto the latest, even if i download the more stable drivers directly. Got any advice?",Negative
Intel,"I kinda fixed it by turn off XMP/EXPO running the ram the lowest bus, still crash to but 1 crash every 2-3 hrs still better than 15 minutes.",Neutral
Intel,Why does it seem like driver quality/support has gotten substantially worse this past decade? Are we running out of skilled software engineers or is hardware just getting too out of hand?,Negative
Intel,there are multiple reports of lower gpu usage with 25.9 and above.. after going back to 25.8 I have 15%+ fps  I guess the constant crashing with rdna1 cards is also not related to these drivers,Negative
Intel,"Tried but didnt work, tried the 25.9.1 and get error that no Software is installed tried the auto update to 25.11.1 again and same error with open and shut down again",Negative
Intel,"Thanks a lot mate, i tried everything and adrenalin not worked but after yours advice it's fine. Thank you!",Positive
Intel,"open powershell as an admin and paste this in. (type powershell in the search bar and there will be a choice to run as admin)  Get-AppxPackage -AllUsers | Where-Object {$\_.Name -like ""\*AdvancedMicroDevicesInc-RSXCM\*""} | Remove-AppxPackage -AllUsers",Neutral
Intel,OK thought I was the only one. 25.10 is bad bad,Negative
Intel,"25.10.2 just straight up didn't work for me when trying to play Arc Raiders (constant UE crash when loading into a match), in fact it actually gave me a popup when launching the game to downgrade to 9.2.  Only driver since switching back to AMD to give me issues.",Negative
Intel,Thanks for testing it,Positive
Intel,"I posted before in an arc raiders thread, but I've had zero issues with my 7900gre on 25.10.2. Playing since launch. I don't use any sort of overlays and play in borderless windowed always. Performance is locked on 140+fps native res 1440",Positive
Intel,I thought FSR 4 was only on RDNA 4? 🤔,Neutral
Intel,My thoughts exactly. Thanks.,Positive
Intel,Ohh okay. Yeah I don't use any of that lol. Hopefully it won't give me any issues. Plan on playing it after FF7Rebirth.,Neutral
Intel,I tried that before and amd antilag off. I tried all the suggestions.   Been really happy with Bazzite so haven't been back in windows for 2 weeks,Positive
Intel,I tried that and all the other suggestions at the time. Nothing worked.  I'm running the game in Linux now and have 0 issues. I can't even crash it when I try,Negative
Intel,"There's not much documented about it  Videocardz has an article   AMD launches first feature from FSR Redstone with Call of Duty Black Ops 7, only for RX 9000 GPUs - VideoCardz.com https://share.google/VHJiZgwO6eqkMmHV3",Neutral
Intel,"Well currently none of the FSR 4 tech works in vulkan titles, through Optiscalar or driver override",Neutral
Intel,Pink path tracing and crashing after a few minuets of gameplay. If you don't enable path tracing the game seems to work just fine.,Neutral
Intel,What are you seeing? I played last night on latest driver and everything seemed fine to me. Playing Ultra RT with Auto on FSR4.,Neutral
Intel,"Yes, modern AMD cards use various telemetrics to push past the rated max boost frequency when headroom is available. Even on default. Problem is, not every board runs stable under these circumstances...  This issue is known to AMD but they don't seem to care.",Negative
Intel,"Have you tried undervolting your GPU?   I've played BF6 since launch with AMD drivers before the game ready one and currently still on 25.10.2 and I've had 0 crashes while playing the game.     Since I got this 7900 GRE that I've ran it with 2703MHz min / 2803MHz max and 1010mv, power limit -5%.",Positive
Intel,Would be absolute insanity to release a brand new rdna3 product if its not about to recieve fsr4,Negative
Intel,"But that's Linux based, So while we may get FSR4 in Linux, not necessarily in Windows.",Neutral
Intel,I guess you can't drop any hints as to whether this work with CDPR also involves adding Ray Regeneration to the game 👀?,Neutral
Intel,Fun fact - i am dual booting and on Linux this bug is not existent...:)),Positive
Intel,"On 10.2 the only crashes I was getting was if I pulled up the Radeon overlay while in game. My guess is that the anti-cheat doesn't like it, but as long as I avoided doing so it ran fine for me.",Negative
Intel,Hi. Did you ever resolve this? I'm getting the same error. Thanks.,Negative
Intel,"Yeah, I'm really starting to lose track of what's going on with AMD. I actually love AMD products, but I'm getting fed up. I've been waiting for a fix for over six months, and it never comes. Instead, new features are added, and bugs persist. AMD is making it really hard to continue relying on their products, which is sad. :/",Negative
Intel,https://www.amd.com/en/products/graphics/technologies/fidelityfx/super-resolution.html,Neutral
Intel,It's a thing you can search for on Google,Neutral
Intel,will it work with current version? Or is it based of some pre-release alpha driver for redstone?,Neutral
Intel,"They promised redstone as a feature for 25h2. So if they dont, that could technically be grounds for a lawsuit",Neutral
Intel,ahh i'm on Win 10 so probably why I didn't see it.,Neutral
Intel,Weird. Why is it a Windows fix?   I also got vsync bug in specific games and the weird thing is that I needed to turn on the and perf overlay in order to work,Negative
Intel,"Yes, but was it in the previous WHQL driver ? I'm not sure.",Neutral
Intel,there is an another bug appears in 25.10.2 drvier. 25.10.1 works fine.  [https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows\_25102\_driver\_performance\_for\_battlefield\_6/](https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows_25102_driver_performance_for_battlefield_6/),Neutral
Intel,It's not only BF6  https://www.reddit.com/r/radeon/comments/1oj98iy/amd_software_adrenalin_edition_25102_release_notes/nm4hh37/  https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,Neutral
Intel,"I tried everything I saw online: meshes on low, XMP lower/off, chipset drivers reinstall and other stuff. Nothing worked.  I downgraded back to 25.9.1., haven't had a crash in days.  Kinda miss the improvements for AFMF they brought with 25.10 for other games, but eh I'd rather play BF6 without it crashing randomly.",Negative
Intel,"Didn't test long enough to see if it crashes as I reverted back to old driver when I noticed my gpu was permanently at 100% load which it never was before on old driver, doesn't feel like it's supposed to do that with a 9800x3d & 9070xt + 32gb ram, It's moving between 70-95% usage on old driver",Negative
Intel,Either launch with curseforge or rollback,Neutral
Intel,"Damn, didn’t work for me last driver either. I can get FSR4 to work in other games just not BF6",Negative
Intel,Sorry for not replying in time with the pictures but I just saw that on Twitter that Beat Saber and AMD are now aware of the issue. The distorted flickering issue on the walls.  https://xcancel.com/BeatSaber/status/1993629046802882685  However there's another issue. I had not actually tried to use an Index at 90Hz until the other day. I discovered that the latency bug is back for 90Hz mode. As in I have to adjust the photon latency to ~5ms in the Steam debug commands to make it usable but not fixed. Just like in the the drivers before 24.12.1.   120Hz mode still works fine.,Neutral
Intel,"Even the rollback doesn't fix it for me now. It being related to the combination of iGPU and GPU makes sense though. Sadly I actively use the iGPU, so thats not an option for me.",Negative
Intel,You 100 procent sure on this?,Neutral
Intel,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable  I will also change in BIOS and completely disable integrated GPU in BIOS,Positive
Intel,"I had auto update on (my mistake), so it did install over the previous driver. I've downgraded for now and it seems to be working just fine.  But no, after uninstalling from control panel I booted into safe mod and used DDU.   Then I downloaded the newest installer from the website. Restarting where needed during this process.   Didn't matter how I tried to access the software. Wasn't active in the system tray, not there in task manager.   Right click on desktop and open from there, click the exe, or go into a game and try Alt R. Didn't open with any of these methods.",Neutral
Intel,Driver with -s letter after black screen and reboot PC tells me that this driver isn't for my graphic card🤡,Negative
Intel,"I had randomly black screens with 24.2.1, this was annoying as hell. Had to DDU the Driver and went back to 23.11.1, after this everything was fine.",Negative
Intel,Does fsr 4 work on 25.9.1? I thought the only driver that works without having to change any files and risk a ban online is 23. 9.1?,Neutral
Intel,"I had to rollback to driver 25.9.1, start up Star Citizen, switch the renderer to Vulkan, exit game, start SC again to verify it's working with Vulkan, then reinstall the latest driver again. Now it works, just don't switch it back to DirectX. No idea who dropped the ball on this one but Easy-Anticheat sure doesn't like the atidxx64.dll.",Neutral
Intel,I'm not sure. However when I install the driver it states that I cannot use adrenaline and gives me a link to the newest driver. Wondering if there is an earlier version of adrenaline I need to install.,Neutral
Intel,What do you mean extension update??? Do you mean lg firmware update or something Ina  windows update? Where do I find this?,Neutral
Intel,They do not.,Neutral
Intel,"Well, I used the system before the latest updates normally, for maybe half a year. Installed every update without DDU. I wanted to triple boot, formated the drive, left windows to install stock updates and drivers, went to install chipset and GPU driver and voila, every wakeup, just before I could type the pin, kernel panic and shutdown. DDU, just the driver, installed again, black screen. Went into DDU again, wanted to see if I missed anything and without reading, uninstalled everything AMD related, went into windows, installed chipset drivers from vendors site (ASUS) and then the latest GPU drivers, and havent had issues. I have a monitor connected via DP and a TV via HDMI.",Neutral
Intel,Dont buy an adapter. Most of them are trash and dont put out the advertised resolution and refresh rate. Not to mention most of them dont support vrr or HDR.,Negative
Intel,The one linked in the release notes is combined. (in the sense that it's two drivers in one executable) (And 1.6GB)  .....amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov-combined.exe,Neutral
Intel,"AND is taking away one additional driver feature per day, you say?",Neutral
Intel,"Yes, I’m talking about 25.10.2 (posted the other day here: https://www.reddit.com/r/AMDHelp/s/nNunS1QlFX ). If you go to AMD download page and select “GPU” you get a file that has a different dimension from the one you download if you choose “CPU”. If you do the following steps 1) DDU (or AMD CleanUp Utility), 2) install CPU drivers => GPU not recognized or with a generic driver. If the you install the GPU driver you get the GPU correctly recognized and a yellow esclamation mark on the integrated. To have both VGAs working at the same time I have to download the AMD Auto Detect package (the file name ends with “minimal_install), but Adrenalin App does not open.",Neutral
Intel,Thank you for explaining it before the rage baiters go nuts.,Positive
Intel,I've been following this one pretty closely. Unfortunately the fix *just* missed 25.11.1 release cycle; we hope to get this out in the next one.,Negative
Intel,"I have a 7900XTX and have this issue at times. It normally clears quickly on its own, but seeing pink pixels on the screen worried me that I had an issue with the actual hardware",Negative
Intel,"I totally understand. if you're running with an affected display config, you should be fine on 25.9.1 or the 25.10.1 preview driver for BF6. We'll get this out as soon as we can.",Neutral
Intel,"Appreciate you reaching out with this - that's an interesting data point to consider.  In this case, the quirky behaviour with HDMI 2.1 + FRL was attributed to a change made in the display stack, I don't think it explicitly involves display stream compression to repro, just high enough display bandwidth (though I could be mistaken)",Positive
Intel,"Interesting. I've been experiencing random BSODs on my PC too on a fresh Win11 install too, I was also suspecting it was something to do with auto HDR toggle but I didn't know for sure.  I'm running 180hz 1440 LG panel but I don't recall if it's using hdmi or DP. I've also never experienced BSODs until the last week or two, since i did a fresh install and I've had the same monitors/PCs for a long time.",Neutral
Intel,Don't apologise - this one is worth getting upset over. Hope it's fixed for you all very soon.,Negative
Intel,Was yours the DisplayPort config or HDMI? I may have a fix for this ready if you're available test,Neutral
Intel,Can you provide me some context? Are there any posts detailing the issue either here or at the chromium project side?,Negative
Intel,Already launched in COD 7,Neutral
Intel,"Im currently using dp 1.4 on my 7900 gre.  My monitor has 2.1 hdmi, so you’re saying i shoulf switch to hdmi?",Neutral
Intel,Some of us are using DP 2.1 though. DP 1.4 is a thing of the past.,Neutral
Intel,Are you sure? The steam machine has HDMI 2.0 and DP 1.4 listed in the specifications,Neutral
Intel,"Ah ok, that definitely makes sense if you're using a TV. Hope they straighten it out for you soon.",Neutral
Intel,What I did is get the 25.8.1 drivers shut off the fluid motion frames and in Windows set bf6 to run full power on your graphics card. Try some of those things and see if that helps you.,Neutral
Intel,"I tried to reinstall the chipset drivers, the same error, tried to install the drivers from gigabyte site the second screen worked but uninstalled adrenaline and it ran the auto update that disabled the secondary screen and the issue of no adrenaline opening came back, going back to older version, thanks AMD",Neutral
Intel,With the compiled leaked DLL you can use it on RDNA3 as well.,Neutral
Intel,"Wait until December, there's the 5th anniversary and they'll supposedly drop something new.",Neutral
Intel,For me playing it with path tracing on eventually the game crashes. Had no issues playing with ray tracing: psycho though.,Negative
Intel,Amen to that. Not a single crash since the game released on CachyOS. (9070 XT),Negative
Intel,Thank you! Exciting keen to see what it’s like,Positive
Intel,"It has nothing to do with the lack of vulkan support, even when fsr4 comes to vulkan, the game still needs ray regeneration implementation from the devs, you cant use optiscaler through dlss inputs, like you probably will be able to in fx alan wake 2",Negative
Intel,I suppose is going to be fixed as soon as Amd Redstone go out. Basically is going to add every tecnology that Path tracing needs to work. Off course the devs would have to implement them in the game too,Neutral
Intel,"The issue is if you try to use path tracing. Which to be fair, you probably shouldn't unless the miracle of them getting Virtuous to implement Ray Regeneration in Cyberpunk happens.",Negative
Intel,yes i tried it ... maybe it is because of the 7800X3D ... i really dont know... i will wait for the next patch ... should come tomorrow 18.11. ... but i have read so many threads and i am not the only one...,Neutral
Intel,Hmm fair. For me on 9070XT if I had raytracing enabled it would crash most regularly on loading back to the menu after extraction which was annoying but at least not game destroying lol. But yeah annoying enough that I turned RT off entirely.   I hope this means I can turn it back on because the game runs well enough for it.,Negative
Intel,"Yes. So far, so good. I'm not 100% sure what fixed it.      I uninstalled both Adrenalin and Ryzen Master standalone applications. Deleted the ""amdryzenmasterv"" keys. Rebooted.  Then I installed Adrenalin and used the Ryzen Master installer in Adrenalin (Performance > Metrics > Install Ryzen Master).  I think this problem might have something to do with a handshake breaking between Ryzen Master and Adrenalin, after upgrading just Adrenalin.   From now on, I'll probably do clean installs, removing and reinstalling both Adrenalin and Ryzen Master, through Adrenalin Performance tab.",Neutral
Intel,"My last two processors are ryzen amd, my last three video cards are amd radeon and next one will be amd also, nvidia has driver issues as well, to be honest.  But yes, some issues are too annoying and updating drivers is lottery at this point. I would advice against updating drivers unless you need it for specific new game or have the latest video cards - rx 9070 \[xt\]  or 9060 xt .  I should have sold the 7700 xt and bought 9060 xt 16gb instead but too late for that now and it has problems as well, so lottery as i said, its a risk.",Negative
Intel,"I see, btw I'm just telling the natural AMD trends on its major update/features on its GPU (kinda). Wish it coming sooner..",Neutral
Intel,lmao chill out dude go touch some grass,Neutral
Intel,Could be grounds for lawsuit… That’s funny!,Neutral
Intel,Because of MPO.,Neutral
Intel,yeah same with 25.11.1 25.9.2 works for me,Positive
Intel,"25.10.2 was the previous WHQL, so also yes :P",Neutral
Intel,"Huh, seems to be running in DX12 for me, as reported by MSI AB, and FSR4 is working",Neutral
Intel,Which driver version and does it still crashing?,Neutral
Intel,OK I will install it now and test it and get back to you. Give me 10 mins.,Neutral
Intel,Yuppp. It does not work on bf6 but it works on ark survival ascended for me. Seems it might be a bf issue. I saw ppl saying they were getting more fps with this driver in BF6 but I think they didn't realize it's using FSR 3.1 and not 4.,Negative
Intel,I'll work with the engineer from that ticket check if that issue has somehow regressed.,Neutral
Intel,We've not been able to reproduce this internally so far. Can you remind me which GPU (was this a 7900XTX?) + connectivity method you're using?,Neutral
Intel,Try using DDU to uninstall your current drivers and do a clean install of the 25.10.2 combined drivers if you havent already (you can find it here):  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html,Neutral
Intel,"Yup just need to say ""No""",Neutral
Intel,"whew thanks, good think i noticed it first before updating. i have 25.10.2 and 25.9.2 here and they both have windows 10 along their filename so i might as well asked.",Positive
Intel,I don't see how it would work on 23.9.1 lol,Neutral
Intel,"Did you use DDU to uninstall the drivers and then reinstall using a driver installer from the AMD website? I found this guide yesterday and it was extremely helpful, specifically step 8:  https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps/",Positive
Intel,I did it this morning before the new driver and confirm chipset drivers were untouched,Neutral
Intel,Agreed the variability and stability of adapters can be quite hit and miss. Just wait it out unfortunately and don’t use the latest drivers. At least AMD owned up to it so I can’t be too upset but hopefully they really do fix this soon as new users may not understand what’s happening and think their card is bad.   We are already seeing this in other Reddit post from other users. I have been playing some damage control and telling them to downgrade and suddenly it’s stable for them and they don’t think their card is dead doh. So yeah AMD really needs to fix this soon.  More people use HDMI than what people think and those TVs don’t always have a DP connector at all.,Negative
Intel,"ah, that explains it. Thanks. :)",Positive
Intel,"What if one does not check release notes? V25.10.2 point to different packages, for installing both CPU and GPU you had to download the AutoDetect package (named “minimal install”). Obviously I’m referring to AMD driver download page.",Neutral
Intel,"Okay, as long as they have a workaround for that... I was planning to get a 9060XT, but also keep my 6700XT for lossless scaling frame gen, was worried the branching would affect that..",Neutral
Intel,"What a mess. I have rdna2 and 3 on the same PC, so combined driver is the solution for this? Or auto detect is the only way?",Negative
Intel,"Damn, that's a huge relief to hear. Most of the frustration came from not knowing whether the issue was even acknowledged.   Thanks a lot for the update.",Positive
Intel,What about Noise Suppression not working since 25.9.2?,Negative
Intel,Finally. The last driver without pink artifacting was 25.4.1  It has been 7 months. AMD Matt acknowledged the bug on [the amd forums](https://pcforum.amd.com/s/question/0D5KZ000011WpJJ0A0/pink-square-artifacting-chrome-discord-and-steam) 5 months ago.  Please let it be fixed with the next driver update. So many 7000series users are on the verge of swapping to Nvidia.  https://i.redd.it/ywrg9at3lp1g1.gif,Neutral
Intel,"what about whatever changed post 25.3.1 for multi monitors? Any driver 25.3.1 and older i can run my main at 200hz, my side at 165, and my third at 60 without issue. Any driver newer than 25.3.1 and running my main at 200hz then playing a game will cause the second monitor (165hz) to flicker, glitch, and freak out the entire time, until the game on the main monitor is closed. Dropping to 165 on the main monitor down from 200hz \*fixes\* the issue but, i paid for 200hz and would like to be able to actually use it again.  (7700xt system)",Neutral
Intel,Hell yeah 🙂 amd I appreciate you see the issues Iv been a fan boy for a long time and I know it will be fixed,Positive
Intel,"Hi. Sorry, I thought this was pretty common as I've had this issue with two different systems and 3 AMD GPUs across 2 generations. [Here.](https://www.reddit.com/r/radeon/comments/1jayump/chrome_full_screen_freezes_after_switching_to_amd/) [Or here.](https://learn.microsoft.com/en-us/answers/questions/3890720/parts-of-screen-freezes-when-using-chrome) Chrome (or other Chromium browsers, I've had this happen with Edge too) in fullscreen, with graphics acceleration enabled.   Best way I can describe it is that very often the program fails to update its display information?? Most of the screen freezes to a certain frame and only small, sporadic parts of it - never the same ones, sometimes more, sometimes less - continue to be updated properly as I scroll or take other actions. Making the window smaller fixes this instantly. It seems to be tied to graphics acceleration as it just doesn't happen at all with it turned off. The issue seems to get exacerbated when the GPU is being stressed.   And in the same vein, Chromium browsers do not seem to mesh well with AMD hardware? I've had plenty of weird behaviors ranging from TDRs, WoW freezing in the background, crashing and restoring itself which do happen less often if I'm running Firefox in the background instead of Chrome.",Neutral
Intel,"Keep in mind this only actually matters if one protocol or the other is **actually** limiting bandwidth for the [resolution * FPS] you want to display on your monitor  For instance, DP 1.4 can do ~25Gbits/s, which translates to roughly 1440p @ 240fps, or 4k @ 120fps  If you have proper cable into HDMI 2.1, it should be able to do ~42Gbits/s, which would instead be 4k @ 190fps  Read up the wikipedia tables for bandwidth specs and resolution / fps / bit depth rates for more",Neutral
Intel,Yeah most people are still have dp 1.4 monitors though. No way I'm swapping my monitor just for dp 2.1 as it's just too awesome. Although these tandem OLED screens look interesting... Hmm lol,Positive
Intel,"lol, listened too much to linus  https://youtu.be/g3FkuZNSGkw?t=261",Neutral
Intel,That was my very first actual driver issue I experienced with AMD.,Negative
Intel,Oh that's nice! I'll look into it when I get the chance.,Positive
Intel,Cool. Thank you,Positive
Intel,I might turn on ray tracing in like a medium or low setting just for shits and giggles or screenshots but I don't plan on playing with it on all the time since I'll also play at 1440p so I don't wanna have a really bad experience 😖,Negative
Intel,Exactly! It's insane. I can't believe running through a translation layer is more stable than running native,Neutral
Intel,"but i dont know something is also wrong with 25.11.1 ... AMD Adrenalin randomly vanishes from tray     i dont want to roll back to 25.9.1 :-(    but this version was the last which was good so far      maybe when we have luck amd is able to provide next time a ""perfect"" driver without any issues?",Negative
Intel,"Could be it, nothing hardware wise at all and just software too.     Yeah, you're not the 1st I've read that says they have issues on the game.     If you're crashing and the system reboots it does make me think of some stability issues and not really just GPU, but I'm guessing you've already tried stability tests and using the system without OC+A-XMP/EXPO disabled.     Once upon a time I remember reading some people, on other instances and other games, having crashes like that (complete reboot) due to PSU instability as well.     Good luck!",Neutral
Intel,That's strange. My game defaulted to everything maxed out including running ray tracing and I haven't noticed any issues. The game runs incredibly well.  I've got a 5800X3D & 9070XT as well.,Positive
Intel,"Fair enough, and yeah sooner the better for all of us",Positive
Intel,It absolutely could be.  Europe has some pretty good consumer law and promising features that could impact your purchase decision are part of this law.  It can fall under misleading advertisement,Neutral
Intel,"It's not freezing, it's just that the GPU usage is practically at 100% all the time, even with V-Sync enabled. With the 25.9 drivers, this doesn't happen, and the clock speed varies normally, increasing and decreasing as needed when V-Sync is enabled. These 25.10 drivers are causing the GPU to consume more energy unnecessarily.",Neutral
Intel,Fingers crossed,Neutral
Intel,"Thanks for attempting to retest.  It's a 7900XTX with an Index connected via DisplayPort. I am on the latest 25.11.1 driver.  I run a monitor at 4k 120Hz 10bpc with HDR Off, which uses DSC, as my main and only display. I tried disabling DSC in the monitor settings which runs at 4k 120Hz 8bpc with HDR Off but I don't think I noticed a change in latency. I thought that DSC on and off on two different devices might contribute to the problem but I'm not sure.   I have also tried running the Index under a RX480 on another PC and I fairly certain the latency looks different under 90Hz and looks similar under 120Hz. Can't play much to test though as an RX480 runs the Index at a very blurry setting. Getting around to doing this test is what took me so long to reply.",Neutral
Intel,Were you able to find the issue?,Neutral
Intel,"Allright ty, will Install new, any differences in performance?",Neutral
Intel,"im running 25.11.1 on win10 7900xt. no problems besides afmf2 breaking the performance overlay, which ive had for multiple updates now",Neutral
Intel,I don't think you understand what I mean.. If you downgrade to that you can get fsr 4 to work on 6000 series without having to change any files.,Neutral
Intel,Thank you for this. This was very helpful. Got adrenaline working fine now.,Positive
Intel,"I wish my LG C4 42"" had a display port. Its my primary monitor.",Neutral
Intel,"Yep, AMD is not so quick in fixing their driver download page, I still can remember when X870E chipset came out but you found only X670 option. The drivers were the same, but man it’s your flagship chipset! Anyway, drivers install but Adrenalin App does not work for me… and have zero time to reinstall Windows.",Negative
Intel,"Don't do that, i'm suffering with both 7900XTX + RVII (and even with RX6400)",Negative
Intel,"I feel like AMD can do a bit of a better job communicating this, despite Vik's great efforts in this subreddit. Just saying: hey guys, we know about this, it sucks, bear with us please can make things better in the long run in my opinion.",Negative
Intel,"We've not observed anything like this internally, and I've personally not seen this at all in the field. This one may be specific to your display combination. I'll talk to a colleague in our display team about what we can do to learn more about that behaviour on your side - we might ask you to capture a special kind of log file during a state when your secondary display is blanking with gameplay on the primary",Neutral
Intel,Can you tell us what content you have on the secondary panel when it starts misbehaving? Is this behaviour affected if you disable VRR on it too?,Neutral
Intel,"I see. I recall the partial display freeze being related to DWM & multi plane overlay in Windows. One of our community testers encountered this and remedied with the overlayminFPS DWORD here: https://www.reddit.com/r/Windows11/comments/1kgp7ar/cause_and_solution_to_windows_24h2_related/?sort=new  If you're getting TDRs, with chome(ium) + gameplay, pass us over a kernel memory dmp and we'll take a look into it",Neutral
Intel,"Oh yeah I noticed that too when watching it, kinda funny of him to say it as it's on screen",Positive
Intel,I've had a really stable 60 fps experience even with path tracing (minus the crashes) but I'm on a 9070 with fsr 4 on quality,Positive
Intel,Can't comment on the vanishing of the tray since I have mine configured to hide it from tray always.,Neutral
Intel,also i have coil whine since this driver 25.11.1. ?!  also in idle sometimes...  very strange driver...,Negative
Intel,Every roadmap since forever has a small print disclaimer at the bottom at the page saying that release times are subject to change.,Neutral
Intel,"I upgraded from 25.9 because it started crashing, it was not crashing until yesterday, also I have limited gpu usage, clockspeed it is working sometimes, sometimes not.",Negative
Intel,We've still not been able to reproduce this unfortunately. I'll need to check in when I'm back at work next year,Negative
Intel,Didn't really paid attention to it :( but at least my game doesn't crash anymore!!,Negative
Intel,"did you download the same filename with the one i mentioned? i tried downloading windows 11 link and it also gave me the same filename, lol",Neutral
Intel,No you can't.,Neutral
Intel,"I don' think you understand either, 25.9.1 is that driver, 23.9.1 was released in 2023 before FSR 4 was even a thing.",Negative
Intel,"Glad I could help, the crashes on Arc Raiders were pissing me off big style so I wanted to share what helped me.",Positive
Intel,"They are TV's, not pc monitors. Buy the right tool for the job",Neutral
Intel,"I couldn't find the DWM key so I made one, we'll see if that fixes it. Thank you for replying. But please, forward it to someone. There must be people out there on AMD hardware that are not tech savvy and this just breeds bad reputation.",Neutral
Intel,"Nope, that didn't fix it. Chrome continues to forget to update its display. And WoW continues to crash in the background leading to massive stutters when panning the camera around quickly, along with plenty of other odd behaviours.",Negative
Intel,"Yeah, I'm facing the same issue on RX 9060 XT   Is it a GPU driver issue, or a Windows issue that Microsoft needs to fix?",Negative
Intel,since last BF6 Update i had zero crashes also on 25.11.1,Negative
Intel,"That's not how it works. Small print won't save you from stuff like this, you think that they can stop releasing this software for 2-3 years because some small print said 'Subject to change'?   European consumer law would absolutely support the consumer in this case. If you promise a feature by a certain date, you will need to deliver it. If the development takes way too long, you are in for a ride",Negative
Intel,What about 25.11.1?,Neutral
Intel,Yeah same for me. Considering how similair win10 and 11 are under the hood i just went with it. Still absolutely no problems sofar.,Positive
Intel,Can't what? I have been using fsr 4 on my 6700xt for a month now using this tutorial   https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1,Neutral
Intel,"> 23.9.1   Come on man, it's clearly a typo and you know it. That's why he's confused.",Negative
Intel,"Look online for fsr 4 on 6000 and 5000 series, you will understand,    Edit :I will just link you a tutorial to understand...      https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1",Neutral
Intel,Did you reboot after setting that key? Is the display with chrome still only partially updating?,Negative
Intel,"Haha. Sure thing buddy. If that would be the case, that a small print on a roadmap can't save a company, we would have hundreds of lawsuits in EU. You must be new to the hardware scene.",Negative
Intel,thank you,Positive
Intel,"Uh huh, yeah I did it too. You still have the overwrite DLLs. Hence the flipping tutorial. Completely irrelevant to this topic, anyways.",Negative
Intel,"Not a typo, I was asking about something else and he missed my point...",Negative
Intel,"Yes, it can't.   They could add that they can sell your organs, doesn't make it legal though.   Also lawsuits usually don't happen because of a feature or if they happen, in a very small amount that isn't meaningful. Class action also doesn't exist in Europe, so obviously you won't hear about it like in the USA.",Negative
Intel,"But flipping the dlls could get you banned from online games , that's why i was wondering if it's finally fixed in this update and we wouldn't have to flip the dlls anymore..",Negative
Intel,"You literally claimed we could do this without overwriting any files, that was obviously false.  Anyway, no. I would not expect AMD to bring FSR4 to RDNA3 anytime soon, if at all.",Negative
Intel,"What a disgusting build, I love it",Neutral
Intel,the content we crave,Neutral
Intel,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",Neutral
Intel,What GPU are you using in your build?  All of them,Neutral
Intel,you're one hell of a doctor. mad setup!,Positive
Intel,The amount of blaspheming on display is worthy of praise.,Neutral
Intel,Brother collecting them like infinity stones lmao,Negative
Intel,I'm sure those GPUs fight each others at night,Neutral
Intel,Bro unlocked the forbidden RGB gpus combo,Neutral
Intel,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,Neutral
Intel,What the fuck,Negative
Intel,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,Positive
Intel,Yuck,Neutral
Intel,Wait until you discover lossless scaling,Neutral
Intel,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,Neutral
Intel,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",Negative
Intel,Now you just need to buy one of those ARM workstations to get the quad setup,Neutral
Intel,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,Positive
Intel,Love it lol. How do the fucking drivers work? Haha,Positive
Intel,What an amazing build,Positive
Intel,wtf is that build man xdd bro collected all the infinity stones of gpu world.,Negative
Intel,You’re a psychopath. I love it,Positive
Intel,This gpu looks clean asf😭,Neutral
Intel,The only setup where RGB gives more performance. :D,Neutral
Intel,Now you need a dual cpu mobo.,Neutral
Intel,Placona! I've been happy with a 6700xt for years.,Positive
Intel,absolute cinema,Neutral
Intel,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",Neutral
Intel,"Brawndo has electrolytes, that's what plants crave!",Neutral
Intel,"You have always been able to do something like this! Though cross manufacturer has not had ""benefits"" until Vulkan in some spaces for games, now lossless scaling, but for anything requiring VRAM or software level rendering, it's been there as an option.   Unironically, ""SLI and crossfire"" I'd argue, is back, but not in the traditional sense.  I did this with a 6950XT and a 2080ti, simply because I wasn't able to jump on the 3090/4090 train in time tho.",Positive
Intel,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",Neutral
Intel,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",Neutral
Intel,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",Neutral
Intel,Team RGB,Neutral
Intel,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",Neutral
Intel,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here: https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",Negative
Intel,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",Positive
Intel,"OpenCL works on all of them at once, and is just as fast as CUDA!",Positive
Intel,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",Neutral
Intel,Start here with FluidX3D: https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md 🖖,Neutral
Intel,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",Negative
Intel,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",Neutral
Intel,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (I’m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),Negative
Intel,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,Neutral
Intel,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,Neutral
Intel,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",Negative
Intel,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",Negative
Intel,Thank you so much for the very detailed response!,Positive
Intel,Well worth it!,Positive
Intel,Thank you my man!! Looking forward to run some tests once I get home.,Positive
Intel,That's awesome!,Positive
Intel,"Yes, but SLI is a bad description for it.",Negative
Intel,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",Neutral
Intel,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",Negative
Intel,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",Negative
Intel,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",Neutral
Intel,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",Neutral
Intel,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",Neutral
Intel,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",Neutral
Intel,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",Negative
Intel,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  ​​​  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",Positive
Intel,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",Neutral
Intel,Why are you connecting the monitor to the gpu and not the mobo?,Neutral
Intel,"👍   thanks for the info, this'll definitely come in handy eventually.",Positive
Intel,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,Neutral
Intel,No worries mate. Good luck,Positive
Intel,"For some reason I switched up, connecting to the gpu is the way to go. I derped",Neutral
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,It's alive. Rejoice.,Neutral
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",Neutral
Intel,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",Negative
Intel,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,Positive
Intel,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,Neutral
Intel,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,Negative
Intel,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,Positive
Intel,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",Neutral
Intel,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",Negative
Intel,I'm fairly sure they use dxvk for d3d9 to 11.,Neutral
Intel,Could just be a cache issue,Neutral
Intel,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,Neutral
Intel,Glad you brought up Nvidia as I didn’t know this had improved until the testing around Arc showed it had gone.,Positive
Intel,"According to the graphs, AMD has slightly less overhead than NVIDIA.",Neutral
Intel,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",Negative
Intel,"Lowest with DX11 and older, but not with the newer APIs",Neutral
Intel,And when is the last time HUB did a dedicated video showing the improvement in overhead?,Neutral
Intel,or it's just a cache/memory access issue,Neutral
Intel,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",Neutral
Intel,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",Negative
Intel,"Intel uses software translation for DX11 and lower, so it does matter for them.",Neutral
Intel,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",Neutral
Intel,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",Negative
Intel,That's not true. Intel's issue is being too verbose in commands/calls.,Negative
Intel,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",Negative
Intel,HUB used DX12 games that also showed the issue.  It's something else.,Negative
Intel,"The comment to which I am replying is talking about nVidia, not Intel.",Neutral
Intel,"I’m pretty sure HUB doesn’t like Nvidia *or* AMD. They’re calling it how it is, these parts are too damn expensive.",Negative
Intel,That's actually... just worse news.,Negative
Intel,I always dreamt of the day APUs become power houses.,Neutral
Intel,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",Neutral
Intel,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",Negative
Intel,Damn Why is AMD even involved in iGPU,Negative
Intel,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",Positive
Intel,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",Neutral
Intel,almost there,Neutral
Intel,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",Positive
Intel,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",Positive
Intel,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,Negative
Intel,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",Neutral
Intel,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,Negative
Intel,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",Negative
Intel,"It depends on what your goals are for a laptop.  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!   I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",Negative
Intel,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",Neutral
Intel,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",Neutral
Intel,yes its so bad. better go buy some steam deck or ally x,Negative
Intel,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,Neutral
Intel,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",Neutral
Intel,How are they going to feed all those CUs? Quad-channel LPDDR5X?,Neutral
Intel,That's considerably faster than an XSX.,Neutral
Intel,>That's tapping on 4070/7800 levels of performance.  What is?,Neutral
Intel,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",Neutral
Intel,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",Positive
Intel,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,Neutral
Intel,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,Neutral
Intel,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",Neutral
Intel,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",Negative
Intel,It's called satire. You're just salty because you're the butt of the joke.,Negative
Intel,throw it in the next steamdeck and I’ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,Neutral
Intel,Praying the blade16 gets it.,Neutral
Intel,"This is the rumor, if you’re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",Neutral
Intel,256 bit bus + infinity cache.,Neutral
Intel,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,Neutral
Intel,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",Positive
Intel,"Yes, it’s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",Neutral
Intel,The rumored 40CU strix halo chip. Not the actual chips released this week.,Neutral
Intel,7500mhz ram and the 780m,Neutral
Intel,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",Neutral
Intel,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",Neutral
Intel,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",Neutral
Intel,Literally where did you see 40-60% uplift at half the power?,Neutral
Intel,> 40-60% performance uplift at half the power  Source?,Neutral
Intel,"i chuckled, then again im not a fanboy of anything",Neutral
Intel,Dont expect 40CUs in a handheld anytime soon,Neutral
Intel,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",Negative
Intel,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.  The system should have an actual TRUE quadcore memory setup.  Many of these systems have currently (and will absolutely continue to have) dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.  This matches my system that runs a 780m with 7500mhz lpddr5x.  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.  This will continue to hamstring these APUs regardless of how many cu they throw at em.",Neutral
Intel,"“Absolute monster”? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.    AMD has a long way to go before claiming “Monster” class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet aren’t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Max… let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",Neutral
Intel,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,Positive
Intel,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. That’s far from desktop 4070, never mind 4080.",Neutral
Intel,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,Neutral
Intel,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,Neutral
Intel,">AMD has a long way to go before claiming “Monster” class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",Neutral
Intel,"APU is one of AMD’s biggest markets. You are kidding if you think they don’t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they don’t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games aren’t the only thing APUs are used for so PS5 isn’t wholey in the conversation. PS5 also costs monthly to play online and their games aren’t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",Negative
Intel,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",Negative
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",Neutral
Intel,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,Positive
Intel,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",Negative
Intel,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",Negative
Intel,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,Neutral
Intel,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",Neutral
Intel,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",Neutral
Intel,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",Negative
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,Positive
Intel,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,Positive
Intel,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",Positive
Intel,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",Neutral
Intel,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",Negative
Intel,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,Neutral
Intel,"Installs beta software, proceeds to complain about it",Neutral
Intel,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,Negative
Intel,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",Neutral
Intel,What Ghost of Tsushima issue?,Neutral
Intel,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",Neutral
Intel,"This was a graphics card, not a ‘GPU’ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",Neutral
Intel,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",Negative
Intel,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,Negative
Intel,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",Negative
Intel,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",Neutral
Intel,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,Neutral
Intel,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",Negative
Intel,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,Negative
Intel,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,Negative
Intel,The documentation for it would still be in their archives,Neutral
Intel,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",Neutral
Intel,"Pretty annoying how everything follows the same linear fps/price curve, there’s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",Negative
Intel,Wish Arc cards were better. They look so pretty in comparison to their peers,Positive
Intel,Thats actually a pretty solid and accurate breakdown.,Positive
Intel,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,Neutral
Intel,3080 still looking good too,Positive
Intel,What they have peaceful then 4k series?,Neutral
Intel,Just get a 4090. I will never regret getting mine.,Neutral
Intel,i miss old good times where radeon HD 7970 as best single core card cost around 400$,Neutral
Intel,"Damn, the A770 is still so uncompetitive...",Negative
Intel,"It's like the free market priced cards according to their relative performance. How weird, right?",Negative
Intel,How is that possibly annoying,Negative
Intel,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,Positive
Intel,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",Positive
Intel,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",Negative
Intel,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,Positive
Intel,8gb perfectly fine today :),Positive
Intel,"Ah yes sure, now where did I leave my 1500 euros?",Neutral
Intel,"I don’t mind free markets, I’m just saying the state of the market is less fun now than it used to be.",Negative
Intel,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,Positive
Intel,"Yeah, i like the black super series.",Positive
Intel,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",Negative
Intel,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",Neutral
Intel,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,Neutral
Intel,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",Negative
Intel,"""The ability to “turn it off and on again” should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",Neutral
Intel,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",Negative
Intel,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",Positive
Intel,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",Negative
Intel,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",Negative
Intel,"Yo, I saw the title and thought this gotta be Gnif2.",Neutral
Intel,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",Negative
Intel,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",Negative
Intel,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",Negative
Intel,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",Positive
Intel,"I’ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, it’s usually a day late and a dollar short. Blender on Linux still can’t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that there’s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but we’ll have to wait and see.",Neutral
Intel,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*™ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",Negative
Intel,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",Negative
Intel,Long but worth it read; Well Done!,Positive
Intel,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",Neutral
Intel,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,Negative
Intel,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",Neutral
Intel,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,Negative
Intel,I’ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,Negative
Intel,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",Positive
Intel,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,Neutral
Intel,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",Negative
Intel,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",Negative
Intel,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100ºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",Negative
Intel,Lmao as a recent AMD intern I feel this in my bones. I still can’t fathom just how little effort is put into software stability these days.,Negative
Intel,100% all of this...  Love looking glass by the by,Positive
Intel,How does say VMware handle this? Does it kind of just restart shit as needed?,Neutral
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",Neutral
Intel,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",Positive
Intel,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",Negative
Intel,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,Negative
Intel,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",Negative
Intel,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",Neutral
Intel,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",Neutral
Intel,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",Negative
Intel,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",Negative
Intel,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,Neutral
Intel,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",Negative
Intel,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",Negative
Intel,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",Negative
Intel,TL;DR. **PEBKAC**.,Neutral
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",Negative
Intel,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,Positive
Intel,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",Negative
Intel,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  😤😭,Negative
Intel,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",Negative
Intel,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,Negative
Intel,"Thanks mate I appreciate it, glad to see you here :)",Positive
Intel,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",Positive
Intel,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",Negative
Intel,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",Neutral
Intel,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",Negative
Intel,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,Positive
Intel,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,Negative
Intel,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",Neutral
Intel,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,Neutral
Intel,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",Positive
Intel,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",Neutral
Intel,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",Negative
Intel,"Excellent post, very informative. Would take issue with this though:       ""Speaking of VRAM, The drivers use VRAM less efficiently. Look at any side-by-side comparison between games on YouTube between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",Neutral
Intel,"Funny, I saw the title and thought the same too!",Neutral
Intel,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",Negative
Intel,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",Negative
Intel,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,Neutral
Intel,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",Neutral
Intel,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",Negative
Intel,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",Negative
Intel,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",Negative
Intel,ursohot !  back to discord rants...,Neutral
Intel,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,Negative
Intel,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",Negative
Intel,"I dunno man. I’ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I haven’t used anything by team green lately, so it may all be the same shit , different pile.",Negative
Intel,Lol same with me tbh I haven't had any problems 😂 but I guess some do idk 🤷. I have crashed less with AMD than my old  Nvidia card.,Positive
Intel,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",Negative
Intel,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me £540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent £200 more on a second hand 3090 instead of this.",Negative
Intel,"It doesn't handle it, it has the same issue.",Negative
Intel,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),Neutral
Intel,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",Neutral
Intel,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",Positive
Intel,Me neither. I use a RX580 8GB since launch and not a single problem.,Negative
Intel,Because they're talking absolute rubbish that's why.,Negative
Intel,You are one of the lucky ones!,Neutral
Intel,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",Negative
Intel,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",Negative
Intel,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",Positive
Intel,lol your flair is Please search before asking,Neutral
Intel,"Hey OP — Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,Negative
Intel,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,Negative
Intel,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",Negative
Intel,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",Neutral
Intel,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,Neutral
Intel,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,Neutral
Intel,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol 🤣🤣🤣🤣   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",Negative
Intel,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",Neutral
Intel,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",Positive
Intel,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",Negative
Intel,"""NVIDIA, it just works""",Neutral
Intel,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,Negative
Intel,What is the AMD Vanguard?,Neutral
Intel,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",Negative
Intel,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,Negative
Intel,You misspelled $2.3T market cap....,Neutral
Intel,"Okay yeah fair enough, hadn't considered this. Removed it from my post",Neutral
Intel,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",Neutral
Intel,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",Negative
Intel,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",Neutral
Intel,This is not a fix. It's a compromise.,Negative
Intel,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",Neutral
Intel,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,Negative
Intel,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",Negative
Intel,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,Neutral
Intel,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,Negative
Intel,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",Negative
Intel,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",Negative
Intel,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",Negative
Intel,The comment I quoted was talking about people playing games having issues.,Neutral
Intel,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,Neutral
Intel,The thing I quoted was talking about people playing games though.,Neutral
Intel,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",Negative
Intel,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",Negative
Intel,"Idk, I don't use Linux",Neutral
Intel,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",Neutral
Intel,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",Neutral
Intel,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),Neutral
Intel,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",Negative
Intel,Because adding a feature for a product literally gives users more control for that product.,Neutral
Intel,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,Neutral
Intel,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",Negative
Intel,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,Neutral
Intel,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",Neutral
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",Negative
Intel,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,Negative
Intel,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",Neutral
Intel,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",Neutral
Intel,*wayland users have joined the chat,Neutral
Intel,You're falling for slogans.,Neutral
Intel,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",Positive
Intel,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,Neutral
Intel,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),Neutral
Intel,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",Neutral
Intel,Honestly after a trillion I kinda stop counting 😂🤣,Neutral
Intel,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",Neutral
Intel,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",Negative
Intel,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",Negative
Intel,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",Negative
Intel,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,Negative
Intel,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",Neutral
Intel,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",Neutral
Intel,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",Negative
Intel,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",Negative
Intel,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",Neutral
Intel,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",Negative
Intel,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",Neutral
Intel,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",Negative
Intel,Oh then just ignore my comment 😅,Neutral
Intel,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",Positive
Intel,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,Negative
Intel,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",Positive
Intel,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",Negative
Intel,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",Neutral
Intel,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,Negative
Intel,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",Negative
Intel,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,Negative
Intel,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,Negative
Intel,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",Negative
Intel,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",Neutral
Intel,"Too soon to tell, but hopes are high.",Neutral
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",Negative
Intel,"Agreed, they cannot rest on their laurels.",Negative
Intel,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",Neutral
Intel,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",Neutral
Intel,console gamers know pc’s are better and don’t really complain about upscaling and 30fps.. you’re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldn’t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,Neutral
Intel,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,Neutral
Intel,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",Negative
Intel,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned off   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",Negative
Intel,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",Negative
Intel,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",Neutral
Intel,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",Neutral
Intel,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,Negative
Intel,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,Negative
Intel,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,Negative
Intel,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",Negative
Intel,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",Negative
Intel,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,Negative
Intel,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",Negative
Intel,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,Neutral
Intel,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete. And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",Neutral
Intel,When did they do this switch? I remember years ago when I configured that their windows drivers weren’t being so nice to the card detected in a VM.,Negative
Intel,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",Neutral
Intel,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",Negative
Intel,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",Negative
Intel,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",Neutral
Intel,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,Neutral
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",Negative
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",Negative
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",Negative
Intel,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",Neutral
Intel,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",Negative
Intel,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",Negative
Intel,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",Negative
Intel,Oh and XE also have bug feature reporting.  Omfg!!!!,Neutral
Intel,Nobody is 100% right ;),Neutral
Intel,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),Neutral
Intel,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",Negative
Intel,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",Negative
Intel,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",Neutral
Intel,What about using a DP to HDMI 2.1 adapter for that situation?,Neutral
Intel,"2021 my guy, it's right there on the date of the article.",Neutral
Intel,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,Neutral
Intel,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,Neutral
Intel,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",Neutral
Intel,And I guess infallible game developers too then. /s,Neutral
Intel,So you decide what criticism is valid and what not? lol,Neutral
Intel,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,Negative
Intel,"Yup, but do you see them making a big press release about it?",Neutral
Intel,that is not how it works but sure,Negative
Intel,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,Neutral
Intel,>whine about Redditors.  The irony.,Neutral
Intel,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",Neutral
Intel,learn to comprehend.,Neutral
Intel,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,Neutral
Intel,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",Negative
Intel,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",Negative
Intel,"No, that would be you obviously /s",Neutral
Intel,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,Neutral
Intel,"Yea, given the state of XE drivers every major update has come with significant PR.",Neutral
Intel,Why not ;),Neutral
Intel,Go word salad elsewhere.,Neutral
Intel,"I have replicated the issue reliably yes, and across two different systems.",Neutral
Intel,If discord crashes my drivers.. once every few hours. I have to reboot,Negative
Intel,Discord doesn't crash my drivers  I don't have to reboot.,Negative
Intel,Really love how the 6000 series radeons look.,Positive
Intel,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",Neutral
Intel,That's a good looking line up,Positive
Intel,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",Negative
Intel,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",Negative
Intel,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",Positive
Intel,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,Positive
Intel,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",Negative
Intel,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,Negative
Intel,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",Negative
Intel,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",Negative
Intel,That's suprising. Generally the 70 series and AMD equivalent cards are more popular in Germany. Is 4060ti discounted heavily?,Neutral
Intel,"Always happy to see ARC succeeding. We need a third competitor, and they've got what it takes.",Positive
Intel,Im more confused about how do you guys still buy RX 6xxx series. Only RX 66xx series that are still widely available in my country. RX 67xx and above is just OOS.,Negative
Intel,That 7900xtx sale number is insane,Negative
Intel,That just shows that most people that buy GPU's don't know a thing about them.,Negative
Intel,"4060 Ti is not discounted well at all here.   390€ and 460€ for 8/16GB versions, really bad deal which is why I am absolutely baffled to see it this high in the list. Might be christmas shoppers who just buy the highest nvidia card within their budget without doing any research - that's my best guess actually.",Negative
Intel,best discounts were 6750xt 6800 and 7800xt,Positive
Intel,4060 ti is selling as a surrogate Quadro for AI/ML. $400 for 16gb is not awful in that context and you get full CUDA support,Neutral
Intel,"Intel is putting in the effort on the software front as well. Arc cards are great value, especially with the improvements intel has made with driver updates. With talk about XeSS being integrated into DirectX with Windows 12 (among other upscalers as well) I hope that intel GPUs will be even more competitive.",Positive
Intel,"Battlemage needs to be stable and competitively priced.   First impression matters so much these days, it cannot be overstated imo.",Neutral
Intel,"They're not out of stock there, duh",Neutral
Intel,Yeah the Germans love the 7900xtx in particular for some reason. It was really high up considering its price in last week's summary as well. I guess they don't appreciate the 4090's price.,Positive
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"I have so many (adult) friends who always bought NVIDIA, will always buy NVIDIA and don’t do a single bit of research.   Had one friend even ask me, I recommended the 7900Xt for her budget and she got the 4070 Ti because she didn’t trust my word or AMD  It’s such a big habit issue. Shopping season really showed me that nvidia could literally sell a brick and habit shoppers would buy it",Negative
Intel,">390€ and 460€ for 8/16GB  That equals to $350 and $415 before taxes, a lot lower than in USA ($390 and $450 before taxes)",Neutral
Intel,"They are pretty much gone in Germany as well:  6950XT - only XFX for 599 at Mindfactory   6900XT - gone   6800XT - only XFX for 499 at Mindfactory and Asrock for 580 at various   6800 - only XFX for 435-440 at Mindfactory and some   6750XT - a bunch of models for 373-405 at various   6700XT - a bunch of models for 344-369 at various   6700 - XFX and Sapphire for 298 and 322 from one retailer   6650XT - gone   6600XT - gone     So the high-end is gone except that XFX stock that will probably be gone in the next 2-3 weeks as well, the lower end is gone. Only 6750XT and 6700XT is still there a bit more and of course the 6600 because that has no replacement at it's current 200-220 price, the 7600 is way more expensive at 270-320.",Neutral
Intel,"I know, but i mean, both 6750 XT and 6700 XT still has more than 300 stocks each in single week (if i read the stat correctly). That's a lot of GPU. I thought the stock is already thin globally which in turn make stock in my place nonexist.",Neutral
Intel,"I thought I saw some 8GB models as low as $330 and 16GB models at $400 but even then I would much rather get another GPU like a 6700XT  (you can still get at here and we even had one down at 299€ for a while, think they are back at 330€ tho)",Neutral
Intel,the card is pretty bad if you missed that somehow,Negative
Intel,AMD probably ships leftover to countries in which they know it will sell,Neutral
Intel,"Sure, but I was replying to you saying ""4060 Ti is not discounted **well at all** here""",Neutral
Intel,"Because most people are not willing to pay that much for GPUs that weak. Sure ""discounted well"" is subjective anyways, I got your point that they are below MSRP but I am pretty sure you know what was implied when reading my comment.",Negative
Intel,"As you notice the photoshop version differs, so you can't compare them really",Neutral
Intel,I also another insanely high score:     7900X + RTX4090 [Photoshop score 8995](https://benchmarks.pugetsystems.com/benchmarks/view.php?id=168538),Neutral
Intel,So basically PC games are never going to tell us what the specs are to run the game native ever again.,Negative
Intel,"Well, folks, the game is basically software ray tracing an open world regardless of settings at all the times using compute shaders from what I gather. I mean, what'd you expect from something pushing consoles to the brink? That it would not be equally demanding on PC?",Neutral
Intel,The true crime here is needing FSR to reach these requirements.,Negative
Intel,"Oh wow, this is probably the first time I'm seeing dual channel listed in the RAM section of specs requirements.",Positive
Intel,im more baffled as to why they are comparing a RX 5700 to a GTX 1070. Id have thought they would have said a RX 590 (instead of 5700) or RTX 2060 (instead of 1070),Neutral
Intel,> Enthusiast   > 1440p [...] with FSR Quality [...] 60 FPS  1440p + xxxx Quality upscaling is just another way to say 1080p you cowards. It's 2023 and enthusiast level is 1080p @ 60 FPS.   Looks pretty though with the amount of vegetation.  I'm surprised that they put the RTX 3080 and RX 6800 XT side by side in a game that has forced ray tracing. I guess that's the advantage of being cross platform and AMD sponsored.,Neutral
Intel,"well, at least the chart is easy to read, not a complete mess",Neutral
Intel,This gives me hope that Star Wars outlaws will also have FSR 3. 🤌,Positive
Intel,60fps with balanced upscaling on a 4080/7900xtx … That’s not a good sign,Negative
Intel,It's joever. We are officially at the point where devs are using upscaling as the standard to reach playable frame rates instead of optimizing their games.,Negative
Intel,"If you look in the top right, these seem to be based on ray tracing being enabled? If that is the case, these specs actually seem reasonable.",Neutral
Intel,"FSR and DLSS were supposed to be optional, now they seem to be mandatory in any graphics setup.",Neutral
Intel,Yay finally a game that'll have both fsr 3 and dlss 3 so it'll work perfectly on either gpu brand at launch.,Positive
Intel,"Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. The requirements should all be done without any upscaling or frame generation tech to be upfront and honest about how the game will run. Upscaling will then just be an available option for users to further increase performance.",Negative
Intel,"Actually not to bad for current standards, 1440p 60 with fsr quality on a 6800xt/3080 is great if high includes RT, if it doesn't then it sucks",Negative
Intel,"So with my 6800xt, which should be able to do 1440p high settings at over 60fps easily, I'll have to use FSR just to get 60fps? Great, another unoptimized game headed our way.  So sick of the awful PC ports and such. 6800xt is still a very powerful card yet I'm seeing more and more games require it for 1440p 60fps high or even 1080p 60 fps high. Ridiculous.  The reliance on upscaling tech instead of properly optimizing the game is becoming a problem. And it seems this game has forced RT which is idiotic. RT destroys performance and should always be an option.",Negative
Intel,"If you notice, all the mentioned AMD GPUs are significantly better than Nvidia GPUs, so does that mean bad performance for AMD GPUs?",Neutral
Intel,So i bought an 1200€ card (rx 7900xtx in my region and at the time of launch) and can now enjoy new games at 60 fps with BALANCED fsr upscaling. We love to see it. /s,Positive
Intel,so I'm doomed to use FSR even at 1080p ? what a shame pc gaming has become.,Negative
Intel,"I honestly don't get this upscaling outrage. It's not a fixed requirement. If you want to play native, just turn off upscaling but you will have to turn down settings to compensate. Everyone wants to have their cake and eat it too. People want super low requirements, super high graphics with high FPS.  The thing is game devs/publishers want to highlight all the ""eye candy"" in games to show their advancement and that's how they want to advertise it. They have a certain level of visuals they want to present for the game. So they crank up the baseline and focus on making it look as good as they possibly can and by using upscalers to get to the playable FPS.  Other option is they ditch making games look any better and just keep it restrained to good FPS at native. But then you'll have the other subset of complainers that will complain about the game's graphics looking like X many years ago.  Take Alan Wake 2 for example, The medium setting looks better that some High-Ultra setting games from a few years ago. Running a AAA title 2-3 years ago at 'medium' preset may not equate to running a current AAA title at medium preset.  The games are getting too demanding because they are starting to push the limits visually (partially because on AAA titles, that's something they really focus on for advertising), if your card can run it, great, if not, adjust your settings to match your cards capabilities. If you bought a 6700XT 2+ years ago to play native 1440P on high setting, guess what, it's not always going to play 1440P at high settings as the years go by. You have to adjust your expectations of the card's capabilities as time goes on. Medium preset now is likely better than High preset then. If you want the ""same"" performance you used to get, match the visuals and see how it does.  Seems like the only way to get people to stop complaining is to just halt all visual/graphical progress and stick to limiting the game's visuals so they run well on mid/low tier cards.  People keep throwing out ""OPTIMIZATION"" but on one knows what/how they want that done. WHAT exactly do you want optimized? How do we know the game isn't optimized to the graphics/performance it gives?  What if they just made the ""High Quality"" preset look like games from 2018, would that now be considered ""Optimized""?  (I'm not talking about this game in particular, just in general, it was same outrage w/ AW2 and that turned out better than expected)",Neutral
Intel,I hope they will bundle this game with CPUs/GPUs,Positive
Intel,holy hell so for 1080p you need a 1440p card because not even a RX 7600 is on par with a 3060ti or a 6700XT,Negative
Intel,Recently upgraded GPU and CPU and crazy that newer games are already requiring the newest GPUs to play them. Personally I don't think it's a horrible thing but at the same time it just screams horrible optimization.,Negative
Intel,This game better look like real life with those specs. I does looks beautiful!,Positive
Intel,"Another garbage with forced upscaling at baseline. When upscaling winded up - everyone was ""Hell yeah, high refresh rate AAA gaming""  Fast forward to today: ""FSR performance just to hit 1080p 60fps - yay..""   I fucking knew it.. it was so obvious upscaling will turn into this and replace game optimizations. Why optimizing shit, when you can tell people to turn on fucking FSR / DLSS / XeSS performance??   What a fucking shitshow... And few weeks back when I said I'm not gonna buy AMD GPU again because upscaling is getting mandatory and DLSS is just objectively superior - I was laughed in the face. Every major AAA game onwards will be shoving up upscaling as base line performance - so your typical 60fps marks (and 30fps - yes apparently that is still a thing on PC in 2023). There's no escape from this.",Negative
Intel,Damn. My 5700XT did Mirage fine on Medium/Low shadows. Now its basement quality.,Negative
Intel,This is starting to get scary and not cool. Every stat is not native performance and a 6800xt with fsr at 1440p is only pulling 60fps?!?!? What the actual fuck is going on?!?!? Midrange is definitely on its death bed if this is the new norm...,Negative
Intel,I love it how absurd these things are these days.,Negative
Intel,Really tragic this IP ended up with the most anti-consumer publisher in the world.  Layers upon layers of DRM.  Timed Epic exclusivity. No achievements.,Negative
Intel,Does anyone know if it supports SLI or crossfire?,Neutral
Intel,Source  https://news.ubisoft.com/en-gb/article/G0eJBcH8NcvNO2MPqb1KV,Neutral
Intel,"Hopefully this is another Alan Wake situation where the game performs better than what their system requirements would suggest. With this being a based on a movie game, I don’t have super high hopes that that’ll be the case though. Going from movie to game and vice versa almost always has bad results.   Also, displaying system requirements with upscaling is a bad joke. This is basically saying you’re not getting native 1080p 60fps without a 6800xt or 3080. Upscaling is definitely becoming the crutch a lot of folks feared it would be.",Negative
Intel,I thought upscaling technology is for low end gamers to get more FPS or people who work on 4k monitors and use upscaling to get better frames on them. but it seems to be used as an excuse for bad game optimization. What a complete joke!!,Negative
Intel,They are so bad this days that if the say the specs without the upscaling it would be 4090s all across the board,Negative
Intel,Why in the f*ck is upscaling included on a specs page?,Negative
Intel,no more software optimization and full upscaling  bleah,Neutral
Intel,Now THIS is what I call a reasonable spec sheet for a next-gen game 👏👏👏,Neutral
Intel,"Surprisingly good specs. 1080p high 60 FPS on 3060ti/6700XT is what we can expect from 3 years old cards in open world games.  Yes, I know it's with upscaler. But if the footage stays - it's understandable. Ubisoft is always about open world games afterall.",Positive
Intel,"I've never even heard of this game, nor care about it, but these system requirements offend me.",Negative
Intel,seems like graphics reached its eak with ps4 and games dont look that much better naymore and requires 4k dollar pc to run with dlss,Negative
Intel,"What about steam achievements ? :), i want to show off perfect games on my steam profile :)",Positive
Intel,"Telling me my $1,000 GPU will only get me ~1200p at 60fps tells me you don't want me to buy your game.",Negative
Intel,How about support them with games that people actually PLAYS ? Like Alan wake 2 🤣,Neutral
Intel,Fuck FSR and DLSS. This isn’t how they are meant to be used.  Rather than making a good experience better they are being used to make unplayable trash playable. Papering over shit programming.,Negative
Intel,Nice,Positive
Intel,"If Anti-Lag is still locked by then , please enjoy your frame gen input lag.",Neutral
Intel,"The game has a huge graphical potential, it's the right time to release that much potential environment in this era .   But it wasted its potential by considering lower class GPUs there by reducing polygonal complexity and complexity of environment including terrain and vegetation , animal complexity ,etc.....  Imo, it needs to have 2X polygon than what it seems.  It seems ultra poooooorly optimized as per the recommended requirements coz it's a bad graphics.",Negative
Intel,Native gaming died or what ? Wtf they turning pc gaming into console gaming,Neutral
Intel,"60 FPS WITH Frame Gen AND FSR balanced? God, that's gonna be a horrible experience.  I really don't like the direction we are heading.",Negative
Intel,4k balanced FSR means 1200-1240p ... lower than 1440p ! And 4080-7900 XTX for that ?,Neutral
Intel,Their supporting FSR 2 and 3? what the hell does that even mean? Unless they plan on releasing b4 FSR3 becomes drops.,Negative
Intel,"Omg, it would be a graphic master piece or  bad optimized thing.",Negative
Intel,First time I see matches recommendations for nv and amd GPUs...,Neutral
Intel,Rip laptop rtx 3060 6gb,Neutral
Intel,"How do FSR 2 & 3 work with tech like ReShade?  ReShade has a pretty cool shader that will make any game into stereoscopic 3D.   When it works it works well, but it doesn't work in every game.  I'm curious because I would like to try it for this, since like 99.97% of Avatar's appeal (shtick?) is the 3D.",Positive
Intel,Upscaled 1080p low settings and only 30fps with A750? At least the game has xess.,Neutral
Intel,*NATIVE* resolution gang ftw!,Neutral
Intel,"I have been wondering with a few releases lately (Forza Motorsport, Cities Skylines) what GPU the dev machines they are using. I have a 7900XT and it ain’t enough to get above 60FPS without variable resolution at 1440p, seems kinda crazy that one of the top 5 cards from the latest gen can’t push enough pixels for these games.",Negative
Intel,Looks capped at 60fps?,Neutral
Intel,"Funny, how it only tells you the settings for FSR but not for DLSS and XeSS even though those 2 are also supported.",Neutral
Intel,How come 6800XT is mentioned? But not the 7800xt? As recommended specs for 1440p?,Neutral
Intel,"Rip my 780m and rtx 4050m. I don't understand why upscaling is ""mandatory"" not ""supplementary"". Do we need a SLI rtx 4090 for native 4k@60hz?",Negative
Intel,Is it using UE5?,Neutral
Intel,"Ubisoft, rip on launch.",Neutral
Intel,guessing no DLSS3 then?,Neutral
Intel,I guess that fsr3 will be added in an update at some point since everything mentions fsr2. It will be interesting to see it running on my gtx 1060 since it seems to be a big upgrade on older cards.,Positive
Intel,"Another game with upscaling baked into all presets. ""Free performance"" they said. I expect frame hallucination tech to be included in these presets in 1-2 years tops",Neutral
Intel,"Can't wait to be ridiculed by Alex at DF again for wanting PC requirements that don't include upscaling.   Upscaling should be an assist, not a requirement.",Negative
Intel,Farewell 1660ti… looks like it’s time for an upgrade,Neutral
Intel,well my 3300x is now obsolete for these new AAA games...,Negative
Intel,4k ultra right up my alley 😏,Neutral
Intel,fsr3 frame gen but in the specs themselves FSR2 is stated everywhere. What? So there's FS3 FG bu no FSR3 upscaling?,Neutral
Intel,7900xtx will do 4k 120fps with FSR 3 then I guess?,Neutral
Intel,What must one do to achieve a higher rank than an enthusiast? A demi-god?,Neutral
Intel,"I think we're at a point in gaming where DLSS / FSR will be mainstream in pretty much every game now. I mean even my 4080 can't get 60FPS 1440p in the Witcher 3 or Cyberpunk maxed out with ray tracing, have to use DLSS to increase to above 60fps. I do think DLSS / FSR is actually good technology, but for those enthusiasts who like to play native, I feel like having a $1200 GPU that can't run 4K or even 1440p ULTRA settings at 60fps depending on the game, is just a cash grab.",Neutral
Intel,Is vrr fixed with frame gen then?,Neutral
Intel,"Will someone please get a message to developers that FSR/DLSS should be *optional* and used only if you want to sacrifice some visual fidelity in order to run 144 FPS+ on high refresh rate displays, and let them know FSR/DLSS should never be *required* to get a 90GB bundle of spaghetti code to *barely* reach a blurry ass 60 FPS?  K? Theeenks!",Negative
Intel,"Yet another AMD sponsored game without DLSS....      EDIT: Nm, it appears this game will offer DLSS thank god.",Positive
Intel,Hoping with my 7800x3d and 7900xtx fsr 3 4k maxed is around 120fps for my tv refresh rate. To be honest at my age I barely notice the quality difference of any input lag.,Neutral
Intel,They recommend upscaling even at 1080p.. disgusting ew,Negative
Intel,pretty much this we all knew they would start using upscaling as a crutch.,Negative
Intel,Welcome to modern times and how you can’t just brute force everything anymore. Or you can go buy a 4090 and not complain about how expensive that is.,Negative
Intel,"Yeah, I'm not crazy about using upscaling techniques on everything to gauge it's performance and requirements. I think more time needs spent on optimization but I could be ignorant for stating that.",Negative
Intel,Thanks to all of you that were screaming dlss looks better than native lmao.,Positive
Intel,"I tend to be much more picky than those around me. A group of people I play with were all playing Remnant 2, and I FSR was driving me crazy, they all have way less powerful computers than I do, and none of them cared at all FSR was on. In fact they were impressed with how well it ran and how it looked. I think there is a general 80% of the PC population that doesn't mess with graphic settings and just wants the game to play. I'm a min/maxer when it comes to graphics where I constantly tweak it until I get the perfect output for me.     I'd love to have real data from games like this on how many people actively change graphic settings.",Neutral
Intel,and a year ago i kept saying this here and i got downvoted to hell   its so obvious that the game engine devs and game companies pressured both nvidia and amd for this because they can release games faster and somewhat unoptimised,Negative
Intel,"Upscaling, once a promising and beneficial tech, now abused by almost every dev cause they are lazy to optimize. :(",Negative
Intel,Nope we as a community abused a nice thing,Positive
Intel,"Alan Wake 2 (and others) is proof that this doesn't matter any more. DLSS Quality looks and runs better than DLAA native rendering as shown a number of times and documented by tech power up and actually observed by those of us playing it  It's about time people stop crying the ""omg no native rendering"" beat as this is a mentality that is irrelevant in 2023. It's pretty clear that too many games come with performance issues and as such ""native"" rendering has the worst performance in those titles. At least upscaling offers a means to get better performance and in the vast majority of titles, better image quality as a result too with intricate details rendered at a higher fidelity thanks to image reconstruction. The likes of Digital Foundry showcase those benefits often in their videos. Plus the fact that all of the consoles upscale yet nobody bats an eyelid, yet for some reason a small portion of PC gamers seem to demand it no matter what, and then complain when it doesn't run as well as they had hoped in various titles.  It's a hard pill to swallow, but it's \[upscaling\] progression and the key driving factor as to why we can path trace and use other modern engine tech with such good performance and visual quality. This is probably the wrong sub to say such things, but this is the truth, and I fully expect pitchforks to be mounted.... But don't take it out on me, point the finger at AMD and ask them why they are falling so far behind.  At the end of the day if it looks good and runs good then nobody needs to bat an eyelid on whether it's got native or upscaled rendering. Everyone benefits as a result. Fact is the vast majority of modern games look and run better when using DLSS with only a handful of FSR exceptions (Callisto Protocol, for example).  Personally I want improved image detail, sharpness and performance, not just AA that DLAA brings to the table since it's just a more modern AA method vs ancient TAA/MSAA/FXAA etc. I don't care if there's no native option, I can just run with DLDSR if I want to render at a higher res and then apply an upscaler to that output to get the best of both worlds.",Neutral
Intel,Didn't take them long to make upscaling worthless.,Negative
Intel,i smell a burgeoning cottage industry of game spec reviewers!,Neutral
Intel,"I've hated upscaling since the beginning.   Now, it's being proven right in our faces that companies are no longer making 100% they're making 45% done games, and then they rely on upscaling and fake frames to do the rest.   Why have a fully optimized game that runs well natively when you can just lower the resolution by 3x then have frame generation help?   Full force ahead to frame generation= Every game will be optimized so badly that you're forced to upscale",Negative
Intel,"It’s literally right there in the hardware specs. 1080p fsr2 quality is 720p input resolution, so at native the game runs at 720p low on a 1070 or 5700.  It is the inverse of what reviewers have been doing with their benchmarks - if you are a fan who really has a stick up their ass about native res, just do the math yourself. They’ve given you all the pieces you need to calculate it out yourself, but that’s not how the game is supposed to be run so they aren’t going to put native res in the official marketing material.  Games are optimized around upscaling now and it is unhelpful and misleading to pretend otherwise. Rendering 4x as many pixels is *obviously* going to throw things off with more intensive effects, and lead to a generally unplayable experience.",Neutral
Intel,"Judging by how games work on 4090/7900 xtx, these cards turn from 4k 60 fps to 1080p 60 fps",Neutral
Intel,requirements have been a joke for over 20 years at this point i dont know why people are surprised.   on the other hand crappy devs using fancy upscaling and fake frames to get the game playable instead of just making a well made game is infuriating,Negative
Intel,yeah this is the new standard,Neutral
Intel,"RT is the future. With this game and Alan Wake 2 using software RT at all times and hardware RT for anything beyond ""low"", RT is the future.  At some point GPUs will be so powerful and game engines will have RT in the bag that RT will be used in basically all indie games...and then suddenly nobody will talk about RT anymore because RT is in everything for years.",Positive
Intel,and with fsr now standard on consoles it(or a similar tech) will be standard on pc :(,Neutral
Intel,"people also forget both AMD and nvidia didn't really offer a generation leap in GPU performance this gen, sure it's popular to call games ""unoptimized"" (I know many AAA releases are, not denying that) but the issue compounds with the fact that game devs couldn't predict a GPU generation to be this bad caused by inflated crypro-boom margins a generation prior",Negative
Intel,"As with almost every game release recently, expect the game to perform a bit better than the requirements.    Sometimes the hardware can achieve 60 FPS most of the time but drops below that in other times, so the devs just say it's guaranteed 30 FPS and call it a day. If you look at the 5700 vs 6700XT, the gap is almost 50% in most titles, this means that if the numbers for the 5700 were accurate then the 6700XT will be getting around 45 FPS at 1080p low (unless the game utilizes special hardware only available on the 6700XT) but the game targets 60 FPS at 1080p high on the 6700XT which leads me to believe the 5700 is gonna perform much better which should leave room for turning off upscaling.",Positive
Intel,"It does state ray tracing in the top right, we don't know if that's included in the requirements or not. I would definitely reserve judgment until we find out.",Neutral
Intel,Don't forget about Frame Gen too. IMO that's the worst part because that means it's 30 fps without it. Imagine the input latency...,Negative
Intel,The actual true crime here is even having fsr to begin with. It should just have dlss,Negative
Intel,Seems like they tried to cover every basis with these.,Negative
Intel,"Yeah it's kinda weird, even games that had a massive difference of performance when using dual channel RAM never listed it on their requirements :/",Negative
Intel,"I've noticed that many developers do this, for some reason developers seem to have some kind of secret passion for the 1070 lol",Positive
Intel,It's because the minimum settings is with software RT enabled and thus they're using the fastest cards without hardware RT that can reach 1080p30 FSR2.,Neutral
Intel,"GCN support is over, RDNA1 is the lowest currently supported arch.",Neutral
Intel,"In 2023, RX 5700 performs equally or better than the RTX 2060 Super/2070. The 5600XT is more like a 2060 competitor.   Either the game runs like shit on AMD hardware, or the system requirements are wrong and they just write down the components that they tested with.   Another thing is that the game is doing is software based RT by default and that might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.",Negative
Intel,"They're not, those are the cheapest/slowest GPUs that have 8 GB of VRAM. Clearly game devs are done pandering to people who bought Nvidia's e-waste.",Negative
Intel,What do you mean forced raytracing?,Neutral
Intel,1440 + upscaling looks better than 1080p because you're on a 1440 resolution monitor.   Do you even 1440p?,Positive
Intel,"Honestly RT has a lot of fluff and the majority of which is the most costly for the least amount of noticeable fidelity increase. Try path tracing in Cyberpunk with reduced rays & bounces and you'll still get great soft lighting and GI without most of the performance hit when full tilt. If games only leverage certain aspects of RT then the performance is still comparable, that's also why the 3080 and 6800XT perform similarly in Fortnite with full RT/nanite/lumen settings.",Neutral
Intel,It's actually 960p :(,Neutral
Intel,"can advertize pushing all the triangles and RT if you dont stick with a low resolution, certain things launched WAY before they should have in the consumer space.",Neutral
Intel,Support FSR3.0 and being able to use FSR3.0 at launch gives me a bit of skepticism lately,Neutral
Intel,We'll see in a year.,Neutral
Intel,AFAIK  &#x200B;  It is with RT,Neutral
Intel,Seems pretty good to me given it is at 4K with RT,Positive
Intel,"Likely includes the RT features , its also 4k",Neutral
Intel,"Its ubishit, what do u expect, however, since the game has RT reflections, shadows and GI, then i would say this is based on AMD shit RT performance, so lets wait and see how the 4080 is gonna perform.",Negative
Intel,"I wouldn’t mind so much if it was exclusively a 4K thing, but now devs are leaning on upscaling *at 1080p*, where it still objectively looks like ass and blurs everything in the background.",Negative
Intel,"Everyone asked for this. ""iTs tHe fuTuRe"".  ""LoOks bEtTeR tHan nAtiVe"". ""BiGgeR nUmBeR bEtTer"".",Positive
Intel,and we were called [names that frankly should have resulting in bans] when we said this is where it would lead to. and we still get shat on for saying the same about fake frames,Negative
Intel,"I think the top right table is listing the available features/settings the game supports. Depending on the presets, RT may be enabled by default or it may be a separate option from the presets. I guess we'll find out.",Neutral
Intel,"Nothing will stop DLSS / FSR nor RT in new games at this point.  The problem with the rising hardware requirements is, that it will keep a lot of 1440p/4k dreaming gamers with 1080p for a very long time and FSR doesnt really look good enough in 1080p.",Negative
Intel,I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.,Neutral
Intel,Exactly! It even got xess so Intel users also can use xess,Neutral
Intel,">Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. T  its not much different for consoles , like alan wake 2 uses Low to mid settings and FSR balanced on PS5 even on switch titles use scaling and no man sky even FSR and stuff.",Negative
Intel,"I agree with this, that’s not that outrageous if this does include RT. Non RT would be pretty bad to need fsr but that is the quality mode.",Neutral
Intel,This is going to be the exact same situation as with Alan Wake. The game is going to come out and the requirements are going to make sense. People just aren't used to seeing games with mandatory rt.,Neutral
Intel,I would say pressured bc they announced it over a year ago and promised we will get it in september. Thats the problem with games an tech they all make promises and then cant keep the deadline. Just make smth new and when it works then announce it and make the last fine adjustments. If you make promises to early you set yourself up for failure.,Negative
Intel,"Maybe on the lower end, but for enthusiast and ultra... no, they're not ""significantly"" better. These are pretty on-par comparisons (outside of RT).",Negative
Intel,"How so? The 4080 and 7900xtx are neck in neck, same as the 6800 and 3080.",Neutral
Intel,"It's 4k, it will take another 5-8 years to be mainstream.  1440p slowly creeping to 10% marketshare give it another 2-3 years to be mainstream",Neutral
Intel,"Consoles use fsr as well, and not just fsr but often at low precision making the upscale rougher. Its an industry wide trend.",Neutral
Intel,">what a shame pc gaming has become.  Could be worse , Consoles use mostly FSR balanced like in Alan wake 2 the PS5 uses Low to mid settings and FSR balanced.  its likely the trend of going from specially home made engines  catered to the games needs to "" Can do everything , but nothing great"" multi tool engines like UE.",Negative
Intel,"> what a shame pc gaming has become  I mean, PC gaming is in the best state ever. So many amazing games that run so well. Just don't buy all these shitty games, not like there is a shortage.  Tbh this game looks like standard movie cash grab shite anyway. Solid 4/10 game I bet.",Negative
Intel,Which means if all games will require upscaling it will force AMD to improve there's. I'm on board with you though will see way less optimization going forward which is not what we want.,Negative
Intel,"Absolutely agreed my friend. Time to leave pc gaming and focus on building yourself. The gaming industry is corrupted to the extreme, and will continue to do so as long as players continue to praise lazy developers. In the pussy that whole shit.",Negative
Intel,"Mirage is PS4 game, Avatar is PS5",Neutral
Intel,"Update: I must've read that wrongly somewhere! Apparently, it uses their own Snowdrop Engine 😳  Well AC: Mirage is nowhere near a next-gen game. Avatars of Pandora, on the other hand, uses Unreal Engine 5 with hardware ray tracing. So it's only natural it needs more horse power. But it'll look 10x better than AC: Mirage. So there's that 😉",Negative
Intel,Timed epic exclusivity? Aww man.,Neutral
Intel,Was guaranteed to happen when up scaling was announced... It's an excuse for less optimisation or to push fidelity more.  I saw a video of this avatar game it got an insane amount of plants visible at all times.,Negative
Intel,You... are.. joking... right..?,Neutral
Intel,Because it’s not coming to steam. It’ll be an epic games exclusive so everyone will pass on it until it hits steam in 18 months. At which point no one will buy an 18 month old game for full price and will wait for a 50% off sale at the minimum.,Negative
Intel,Kind of a messed up opinion.  Maybe games don't look much better than PS4 on your PC but they sure as hell do on mine and that would still be true at a lower resolution and framerate.,Negative
Intel,Ubisoft no longer employs achievements on new titles.  Don't get your hopes up.,Negative
Intel,Try ubisoft achievements :),Neutral
Intel,"It's doing rt at all levels and 4k? Ye... It isn't mainstream yet, heck 1440p is like At 10% market share",Neutral
Intel,"Iam using afmf daily, the input lag introduced by afmf is between 5-12 ms for me.  Doubt any can literally feel this.  I have afmf enabled in all possible games & emulators.  If you think it's bad for you, you don't need to use it",Negative
Intel,Pretty much yeah... Was inevitable that Devs will use up scaling as excuse.  Same for taa being a absolute shoddy aaatleast up scalers do better aa :/ . But even consoles run now heavy fsr like the PS5 uses fsr balanced on WLAN wake on mostly low settings,Negative
Intel,We don't know the settings.  If its like cyberpunk psycho rt on release which 3080 and 3090 easily needed dlss balanced or performance for on 1440p and 4k...    Then I can see the same case in this game for 4080 and similar cards  Also they dont speak about frame gen just fsr2 ( which doesn't exist here because they support fsr3 which does up scaling and frame gen it's not split like dlss 2 & 3),Neutral
Intel,"The fans of avatar, or people that like fantasy games or just games to have fun in.  Luckily games don't need to be for everyone.",Positive
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Really comes up to how far the rt and lumen stuff can be pushed, if it goes to cyberpunk psycho rt levels? At release it needed a 3080 and 3090 with dlss.   So it could need top end gpu of this gen if its a similar case",Neutral
Intel,yup that is why I will play 1440 UW native with a 7900XTX.,Negative
Intel,I guess they are confused with dlss.  Dlss 2= up scaling Dlss3 = frame gen  So they thought its for fsr3 too. ( as in fsr2 up scaling and 3 frame gen)   But fsr3 can do frame gen or not.  Fsr3 is up scaling and frame gen.,Neutral
Intel,"Likely mid range gpu and a suit yells in the background ""30 fps with up scaling is fine""",Neutral
Intel,"Likely the same, they didn't want to make the graph even more annoying, hence why fsr3 and rt and xess are mentioned top right.  They got anyway already confused with fsr3 and fsr2 ( cause 3 is fsr frame gen and up scaling it's not like dlss where 2 is up scaling and 3 is frame gen)",Negative
Intel,Very similar performance I guess,Neutral
Intel,Ye,Neutral
Intel,They've been using it since forever. The difference is that PC GPUs were 10x faster than console GPUs so there was no need for it on PC.,Neutral
Intel,"It mentions dlss, fsr3 and xess support, doubt they limited it to dlss2.",Neutral
Intel,"Likely, they were confused because dlss 2 and dlss3.  So they thought its fsr2 and 3 too.",Neutral
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,Honestly... Maybe fsr3 frame gen can help you but... A 3300 Was already outdated when it came out.,Negative
Intel,Most likely the upgrade part called FSR 2 and the frame creation part called FSR 3,Neutral
Intel,If they didnt get confused between FSR 3 and 2 then yes fsr 3 roughly doubles fps same for AFMF.,Neutral
Intel,"Nah usually enthusiast in hardware did mean "" max settings no compromises high frames"" but let's say... This changed the last 3 gens sadly.",Negative
Intel,"with AFMF it works , didnt test with FSR3 now.",Neutral
Intel,Reading before raging :),Neutral
Intel,"It is even low quality with 30fps and still needs 5700 ~~XT~~. That is pretty bad :D  *Corrected to 5700, which I think is still a lot for FSR2 at low fullHD, but definitely cheaper GPU.",Negative
Intel,That's with 5700 and 1070.... WHAT exactly are you expecting those cards to do in visually demanding AAA titles in 2023?  People have some crazy expectations. Everyone wants the eye candy but no one wants to pay the performance cost.,Negative
Intel,For 30fps even lol,Neutral
Intel,"Especially recommending FSR.  DLSS at 1080p? Sure why not?   FSR at 1080p? Disgusting, absolutely horrific.  Anyone who downvotes this is a fanboy, end of story. Nvidia's DLSS simply is better at reconstructing an image. At lower resolutions that is SIGNIFICANTLY more important.",Negative
Intel,Or be happy your shitty video card is still supported.,Positive
Intel,and we still get shat on for saying they are or admitting its a bad thing. corpo fanbois are so cringe,Negative
Intel,Even 4090 users use DLSS because it fixes TAA problems. And has a sharpening slider for easy sharpening adjustment.,Positive
Intel,When did I ever complain about how expensive gpus are. I've always been in the category that they are luxury item and vendor can charge whatever they want for em. So you aint never hear me complain about gpu prices.,Negative
Intel,I can't market a game for running with 120fps 4k with raytracing on on a RTX 4060.  But I can market 400 completely identical missions that take up 150 hours of your lifespan and empty out your soul.,Negative
Intel,Yea I know nothing about optimizations and if it could or couldn't fix this or that. I just want them to tell me exactly what I need to run the game at X resolution a Y preset with 60 fps. If that calls for a 14900k and a 4090 then just say so.,Negative
Intel,"Nah, I think we're really beginning to see a trend here with abusing upscaling for free mileage.  Not that upscaling isn't great but...  Apart from RT, on medium settings, I can't really see    massive improvements on visual quality that would warrant such a massive increase on requirements.  UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  We'll probably see this continuing to be a trend for a while, and then as tech novelty wears out, some devs will come out with more optimized and scalable titles.  With hardware prices being as they are, and with 60fps now being highly requested in consoles as well, it's pretty much a given.",Negative
Intel,"People accepted upscaling with open arms, so now devs know they can rely on people using it. Thus they don't have to care about optimizing for native res anymore. Oh you have a 4k card? Okay well we'll make sure it can run the game at 1080p so you can upscale it.",Neutral
Intel,"No average consumer wanted to come to this point. The market leader started pushing upscaling instead of just making stronger GPU’s and there’s nothing AMD can do to win them, only join them.",Negative
Intel,Turns out TAA is worse than AI TAA with AI upscaling.   Of course devs are gonna use it to cover their own asses. And they get perf boost!  Let's face it. The FUTURE is AI everything. Nobody is gonna talk shit about frame generation and upscaling when it looks perfect each frame.,Negative
Intel,But it does in many ways.,Neutral
Intel,"I don't know who those people are. I play all my games on native, whether I'm using an amd or nvidia card.",Neutral
Intel,"dlss looks better than native with TAA because taa is so bad, but dlaa/fsraa at native >>>> upscaling",Negative
Intel,you think engine devs can pressure AMD and even Nvidia? Come on man. Be real.,Negative
Intel,"Ever notice how multi-GPU vanished the moment DX12 shifted the onus from AMD/Nvidia over to the developers? It shows just how much effort they put into getting SLI/Crossfire working when developers instantly drop them the moment they have to do the bulk of the work (significantly less, I might add) themselves.",Negative
Intel,"If I relabeled ""Medium"" preset to ""Ultra"" preset and changed nothing else and told you that you can run this game at Native 1440P Ultra on a midrange 7800XT card and get 60+FPS, would that be ""optimized"" now?",Neutral
Intel,speaking facts my guy,Neutral
Intel,You shouldn't have downvote dood wtf redditors ?,Neutral
Intel,People with AMD cards dislike upscaling more because FSR sucks ass lol.,Negative
Intel,"Uhm, me btw...",Neutral
Intel,"> Who actually plays games at native these days, if it has upscaling?  I do.",Neutral
Intel,"I won't judge your preferences, but at least using native rendenring isnt the clown show we have here. Native 4k look the same no matter what gpu render it (performances aside). Now with all the quality preset, we can't figure what quality to expect. 4k60fps at balenced preset? What's that crap even supposed to mean as thoses presets vary from title to title, from NVIDIA to amd. And btw, I'm still using fxaa over taa and fsr when i can, don't ask me why.",Negative
Intel,"You're even seeing this on console, where Spiderman 2 now has RT reflections in every mode. Even Unreal 5's Lumen renderer is a software raytracer that can be hardware accelerated.  RT is the future because there are scenarios that rasterized rendering is just not capable of resolving. Screen space reflection artifacts, light leaking, weird glow on indirectly lit models... I find these so distracting and immersion breaking.",Neutral
Intel,onto absolutely nothing.,Negative
Intel,"Wait what? No performance improvement this gen? Bullshit, RTX 4090 is literally 2 times faster than RTX 3080 and RTX 3090 is only 10% Faster than the 3080  you do the math. RTX 4080 is 50% Faster than RTX 3080.",Negative
Intel,Or the game performs at 50 fps at 1080p low on 5700 but 50fps is a weird number to target and they can't say 60 so they just say 30fps and call it a day.,Negative
Intel,Not if it's using mesh shaders like Alan wake. In that game the GTX 1650 outperforms the 1080ti.,Neutral
Intel,The game is raytracing only with a ridiculous ammount of foooliage.,Neutral
Intel,"I'm so glad this is finally getting more main stream spotlight.  I do a little bit in Dota 2 and the amount of people that simply have a terrible experience because of a single stick of ram in that title is staggering.  Sure, it's an outlier, but a lot of esports/CPU bound games see similar issues.",Negative
Intel,i wonder when dual rank and dual channel will start being listed too. since dual rank does help gaming,Neutral
Intel,Honestly my 1070 still runs great for what it is. It’s in my sons rig now because it couldn’t keep up with 1440p depending on the game and definitely not 4k. Still a fairly solid 1080p card.,Positive
Intel,False.  guy blocked me lmao,Negative
Intel,"I thought that was on Linux, though I might be wrong",Neutral
Intel,1070 doesn't have hardware RT though.,Neutral
Intel,According to the technical director there's no non-raytracing mode. I'm assuming that means software ray tracing for the minimum spec.  Source: Skill Up video - https://youtu.be/v91y87R5iDk?t=332 (5m32s),Neutral
Intel,Spoken like someone who has never upscaled 1080p content on a 1440 screen. The pixel ratios are all off. It's fucked.  1440p is perfect for watching old 720p HD content though.,Negative
Intel,Completely agree.,Neutral
Intel,Damn my brain must have been on auto pilot. I'm so used to 2160p + FSR quality neatly equalling 1440p.,Negative
Intel,I see the 5700 and the GTX 1070 on the minimum specs. Are those cards going to be RT capable for this game? (It's 30FPS LOW with FSR but still.),Neutral
Intel,"Sometimes the DigitalFoundry reviewers use their meme powers for good (eg:fighting shader compilation stutter), but their  >""it's better than native!""   memeing about upscaling seems to have gone sideways.",Negative
Intel,"Dude tells truth, go to Nvidia sub, and you fond every second post someone telling that DLSS looks better then native, and every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote.",Negative
Intel,It's purely on the devs for targeting low performance.   We could be getting 240fps at 4K using those upscaling techniques.,Negative
Intel,"Derp, derp.",Neutral
Intel,Sounds like John on Direct Foundry Direct every week.,Neutral
Intel,"""Avatar will be a title that will only appear with ray tracing. But we're developing the game so that the quality and performance will be scalable.""  Seems I was right.",Positive
Intel,">I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.  Facepalm...  Avatar Frontiers of Pandora very literally uses a custom engine, not Unreal Engine.",Neutral
Intel,Yep. This company learned something from all the failure game launches that happened this year. The game companies can't just implement one gpu company's tech and not the others bc then it'll only work on the one gpu that you implemented the tech for and it won't work on the other gpus from other companies. Meaning for example if a game company only implemented fsr it's only going to work on amd gpus and not on Nvidia and Intel gpus and vise versa is true too for both Nvidia and Intel gpus for dlss and xess.,Negative
Intel,Avatar is RT only. There is no non RT mode.,Neutral
Intel,This is mad with snowdrop engine. It's Ubisoft they don't use unreal...,Negative
Intel,"Yep, I missed that. Good call! Makes sense why the specs are a bit higher then but if it is forced on, that's going to be rough on any one with a more mid tier card.",Neutral
Intel,">  Consoles use mostly FSR balanced  heck sometimes they use performance or less, heck a lot of the time even. Especially with RT or performance modes",Neutral
Intel,They have really long way to go. Take for example Lords of the Fallen - UE5's TSR is better than FSR. Take Alan Wake 2 and you get ton of shimmering on the edges. It's also not like nvidia not improving - so while AMD makes a step forward so will do nvidia..   There's also question how much they improve without dedicated accelerators that both Intel and nvidia is using. Intel jumped late to the party and they're already ahead - which just shows how much AMD cares. It's rally embarrassing considering both XBOX and PlayStation are also relying on FSR - ant that's bigger market than what nvidia has.,Neutral
Intel,I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?  I mean the facts are the facts. I need to play on lower settings till we get a 5070 or 8800xt.,Neutral
Intel,"It uses their own in-house engine, Snowdrop. The last game this developer did was The Division II which was excellent.",Negative
Intel,"Took a look at some gameplay on YouTube, and it does look nice enough, though water looked like ass, and as mentioned already, it's an in-house engine (which is nice to see TBH).",Positive
Intel,You will never see a high production game from Ubisoft ever on the Steam store ever again at initial launch.  They are the anti-christ of the gaming industry seriously.,Negative
Intel,Consoles are the baseline in most multiplat games and they've been using upscaling since forever. The real reason is just that we don't have GPUs anymore that are 10x faster than consoles like last gen. You can't bruteforce high resolutions in games that run at like 720p on consoles with GPUs that are only 2-3x faster.  If upscaling didn't exist on PC the minimum here would just be 720p at the minimum and 1080p-1440p at the max.,Negative
Intel,Next-gen doesn't mean that current gen can run it well. It means it's so far ahead that most curren-gen hardware can't run it properly + top-end hardware can only barely run it well.,Negative
Intel,"Exactly. These paid exclusive games launches only make me care less whenever they actually release.  It's not just Epic, Meta has done it with several games (Moss Book 2) and when a proper looking pcvr port comes a year later it sells bad because people forgot about it.",Negative
Intel,"Is that with significant RT? I assumed it was ""normal"" ultra settings.  Still I can play Far Cry 6 and RE4 with AMD sponsored RT at 4k native with 100fps. This game will have to look amazing to justify what... a quarter of that performance?",Neutral
Intel,"You are correct. I overlooked the part where, under Ultra settings, it said that was with FSR2. Now it doesn't seem as bad. I mean, it's still not good because it's rendering somewhere between 1080p and 1440p, but at least it's not 30 fps without Frame Gen/60 fps with it like I thought.",Neutral
Intel,"It might be, AFAIK neither Nvidia nor AMD still support their implementation of it in their drivers.  But I've tried it on a few games and it is pretty cool to me, much stronger than almost any modern 3D movie (IMO why 3D has gone into hibernation), and I gather that Cameron designed the franchise around 3D.  Plus the Avatar 1 game was in 3D.  FWIW.",Positive
Intel,This is what I am thinking too.,Neutral
Intel,"You say that, but they specifically mentioned FSR3, and just DLSS, so i have a feeling DLSS3 will have to go via a mod again.",Neutral
Intel,"i have a believe that even 4C8T will survive for the decade, like the case of that 4790K. But turns out these newer UE5 games are wrecking even newish CPU like the 3600. i mean cmon it's still a 4 years CPU old which i think it's not old yet",Neutral
Intel,Let's hope so. 30fps is far from recommended for FSR 3,Neutral
Intel,Yeah I heard it works with that hoping it works with fsr3 now,Positive
Intel,"I've noticed that for some reason newer games look blurry at 1080 even without any kind of upscaling and I'm not sure why, older game look much sharper at 1080p",Negative
Intel,"Just lazy optimisation, that’s pretty much a PS5 equivalent GPU and you can be damn sure that the console version won’t be 1080P 30fps with FSR",Negative
Intel,"No it says 5700. NOT 5700XT, and yes there is a difference. Quite literally the difference between a 1070 and 1080ti. Or a 2070 and 2080.",Neutral
Intel,no no my 7 year old card needs to be able to play at high settings. so does my low end card from today. gotta be high or its just bad,Negative
Intel,Crazy expectation? A game released in 2023 should be able to run at 1080p low 30fps on a low end card like rx6500xt (lowest end modern card you can buy now).  Optimization matters.,Neutral
Intel,"Yup, these cards came out around the time of ps4 I think..that's a long time ago in the graphical quality of games we are seeing.  People should be happy to a point that those cards can even run some of these games if upscaling helps them run that's a bonus as it means they don't need to upgrade if those graphics settings are acceptable to them.",Positive
Intel,"I have a 5700 and it does better at 1080p on these games than what is normally posted. Now it does need quality fsr but its not the worst thing, anything lower than quality is. Bottom line is these are not 100% correct and never have been.  I have zero interest in this game but the specs seem fine even though I wish upscaling wasn't what was relied on for optimization.",Neutral
Intel,"well you dont know what they mean by 30 fps. It could be de 1% lows or the average, we dont know. The easy assumption is that it will be 30 fps average. but that's crazy that a game at 1080p low cant be run by a 5700. It better be a new Crysis.  edit: apparently it uses raytracing only for lighting which would explain the hows and why it crushes the framerate so bad especially on a gpu that doesnt have hardware raytracing like the 5700.",Negative
Intel,"Or in the case of games like Alan Wake 2, you will get unplayable fps if you try to running it at 4k native max settings. Even with a 4090, you will get a glorious 20fps.",Negative
Intel,Assassins creed origins and odyssey side quests/collectibles oh my god,Positive
Intel,Yeah it doesn't give us a actual gauge to understand what we need for set native parameters. I mentioned optimization out of concern that they rely too much on upscaling and it seems like it's the lazy way out of something. It's unrealistic that a usable gaming experience be in a top tier hardware setup that costs $2k+ and top tier hardware of just the last generation are just barely cutting it anymore.,Negative
Intel,I appreciate your insights and opinions. Thank you.,Positive
Intel,">UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  Because they market how ""good"" it looks, not how ""well"" it runs. There's a big influence from marketing perspective to place high focus on ""eye candy"", it's what sells.  The how ""well"" it runs is usually restricted to the small % of us gaming nerds. ![gif](emote|free_emotes_pack|flip_out)",Neutral
Intel,console games started upscaling way before PCs .,Negative
Intel,This sub is rife with crazy conspiracy theories. I don't think it was this insane a couple years back.,Negative
Intel,you forgot who owns one of the most popular engines out there?,Neutral
Intel,"I guess technically it would count but then everyone would flame it for looking bad. But seriouly, what happened when Ultra settings ran real smooth on top of the line hardware? Gone those times.",Negative
Intel,downvoted by devs lol,Neutral
Intel,> RTX 4080 is 50% Faster than RTX 3080.  The msrp is 71% higher too.,Neutral
Intel,"Yeah at 1200 bucks and above you got one, that's true but many cards below didn't offer much.",Neutral
Intel,that's stoopid but it's somewhat more understandable. Why a 5700 would struggle so much since it doesnt have hardware raytracing,Negative
Intel,Yep it's like a 10/15% performance uplift in some games but I'd guess they would prefer to keep the requirements sheets somewhat simple.,Neutral
Intel,Why is there a dialog message about unsupported hardware when you try and run a 390X?,Negative
Intel,It can do software based RT just like every other modern GPU out there.,Neutral
Intel,Well then no wonder rx 5700 can't manage 30 fps lol,Neutral
Intel,"Wait so we are finally getting games made only with ray tracing in mind? This is actually great, 2023 is officially the year the new generation started",Positive
Intel,Avatars uses a Lumen like software RT solution.,Neutral
Intel,"yeah they never finished the sentence for some reason. The full sentences is ""its better than native, when native has forced taa!"" which is why dlaa/fsraa at native look so much better than the upscale versions. normal basic TAA is just so bad, that even upscales not using it look better than native using it",Negative
Intel,"The AA side of things might be better, but I swear no matter the resolution textures become a blurry mess even with Quality mode etc, and it irks me that most people don't seem to notice that.",Negative
Intel,Some people seem to have very low sensitivity to added blur. See also *depth of field blur* and *motion blur* being on by default in every game. It's funny how much people rage against FXAA/DoF/Motion blur but ignore DLSS.  Of course only one is tied to someone's purchasing decision :),Neutral
Intel,"Yeah, thatsl happened.",Neutral
Intel,">every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote  Well, yes, you are using shitty FSR which is literally unplayable no matter resolution instead of godly DLSS that is literally better than native even at 720p/performance. The only way you could have seen the ever unmatched DLSS for PC kings instead of the plebean FSR is through a compressed youtube video so your opinion is literally invalid. I hope I am being sarcastic enough.",Negative
Intel,"The problem is some (if not most) games have such a bad TAA it might as well be true, and if often cannot but turned off.",Negative
Intel,"""We could"" but not profitable. Just let imagine these games does not exist in our reality.",Negative
Intel,You didn't understand. It's another Swiss knife engine,Negative
Intel,"It is indeed not a great situation. Things like this shouldn't be forced on, and it leaves me feeling like they haven't given the devs enough time to optimize the settings.",Negative
Intel,It works because most consoles players are noobs they don't care about the tech. They just want to sit on the couch and play. I'm hoping we see them start working more on FSR3 and upgrading older games to it. And you are right nvidia will keep moving forward but so will AMD. They may never catch them since they have a head start.,Positive
Intel,AMD just released drivers for Alan Wake 2 that fixed the flickering while using FSR.,Neutral
Intel,The PS5 has a 6700.,Neutral
Intel,">I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?   Not exactly. It's a custom quasi-RDNA2 design (has the improved RDNA2 compute units, but it's missing a few features such as fully-DX12-compliant mesh shaders and VRS-capable ROPs).  For most purposes, it's pretty comparable to a 6700 (non-XT), with the same number of CUs (36), and similar clockspeeds.",Neutral
Intel,"Oh, wow. I didn't know that. But it'll likely be on a similar level as UE5, right?",Positive
Intel,Next gen DOES NOT MEAN that your CURRENT GEN hardware can run it well for sure... Why don't people get that?,Negative
Intel,That reminds me to grab moss 1 and 2. I just got a quest 3.   Agreed though. I’ve been waiting for kingdom hearts 3 to come to steam for so long that I’m basically going to skip it at this point. It’s still stuck on the epic game store.,Neutral
Intel,Far cry 6 and re4 are older and or lower demanding games.  And yes any Mode here does either Hardware or Software rt from. Min to max.,Neutral
Intel,It's a 4 year old cpu yes... But how it was built (4c) was like 10 12 years now old.  6 cores are since a few years already base and more and more 8 cores are standard.,Neutral
Intel,TAA,Neutral
Intel,Temporal anti aliasing.,Neutral
Intel,"TAA is the devil. it forces a blur on every frame, and only gets worse as the resolution goes lower. its why fsr, xess, and dlss can look better than """"native"""" because it doesnt use the horrid plain taa solution so many games do. and why dlaa, and fsraa are so important and need to replace taa completely",Negative
Intel,"Ahh, welcome to r/FuckTAA",Neutral
Intel,Even 4K looks blurry with some implementations of TAA,Negative
Intel,they're giving you the bare minimum until your upgrade!,Neutral
Intel,Because they are built with upscaling in mind so they have a blurry looks intentionally as it works best with aggressive upscaling.,Neutral
Intel,"Not even close, you're completely out of your mind. The 5700xt is 9% faster than the 5700. The 1080ti is **32%** faster than a 1070 and the 2080 is **19%** faster than the 2070.   Source from techpowerup: [https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339](https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339)",Neutral
Intel,"Well the difference is only 256 shaders more on the 5700xt. The 5700xt has 2560 shaders and the 5700 2304. That's 11% more. That's not that much.  The 1080ti has 86% more cuda cores than the 1070, as well as more and faster memory. I would not compare now so haha",Neutral
Intel,"The 5700 and 5700xt is more like a 1070 and 1070ti situation. It's not that big a gap, and you can close it even tighter because the majority of RX 5700 cards can be flashed to a 5700xt bios tightening the gap farther.",Neutral
Intel,"The 1070 was released in 2016. Its already the end of 2023. That card belong to ps4 era. Wake up.   People in this sub are delusional, really.",Negative
Intel,Those are some dumbass expectations. Maybe you should correct them instead of blaming the developers for it.,Negative
Intel,"The 6500XT offers $200 2016 performance. In fact its often worse because of how gimped it is interface wise. Its one of the worst products released in recent years.  Its not the game developers fault AMD pretty much rebadged the RX 480 4 times and they can't be expected to sink their optimization to that level.  RX 480, RX580, RX 5500XT and RX6500XT are virtualy the same product performance wise. RX 5500XT is the most consistent one of the lot offering a somewhat more modern arhitecture in its 8GB variant. The 6500XT on the other hand while on a newer arhitecture is an unmitigated disaster that is on par at best and sinks even below the RX580 when memory, bus and PCIE interface constrained.  The console baseline is around 6700XT/2070 Super/3060ti level. People better get used to that as its gonna become or already has become the norm for 1080p high settings gaming. The 5700 is upwards of 30% slower by default while also having less hardware features. Its gonna become the minimum norm going forward alongside the 2060's of your time.",Negative
Intel,"the 1070 was 2016, the 5700 was 2019. While both not new its not 10 years old like the PS4 (2013)",Neutral
Intel,This is literally only true if you use path tracing. I think you're trying to insinuate the game is unoptimized when it just scales really high with visual features if your hardware supports it. It straight up runs great for the visual features it uses.,Neutral
Intel,"That was exclusive to console releases, no? And never to the point of replacing AA sharpening and rendering the game from 720p…",Neutral
Intel,No I havent. AMD is bigger than Epic.,Neutral
Intel,"Right, you'll have different complaints but complaints none the less even though the new Ultra might still look slightly better prior gen games and offer high FPS but people would complain that the graphics are not advancing fast enough, they look the same etc etc.  Yah those days are likely gone but it's hard to say if it's because the game is unoptimized or if the dev is really pushing the limits of what they can do graphically and use upscaling as a buffer because what they have in mind, or what they intend to show is too taxing for current hardware. We won't really know until a game is released  So screaming game is ""unoptimized game, lazy devs"" seems to have just become the norm before anyone's even played it. This is why I've never in my life ever pre-ordered a game. I still don't understand why people do it (esp now that games are digital) but lot of people do. Then again I've never really paid for a ""skin"" or some other visual add-on so maybe that's what attracts people.  I'm not saying games aren't unoptimized, those do exist, they offer nothing more but run like crap. We know what those game are, it's not hard to tell. But then you do have games like AW2, which before release, we were hearing the same exact complaints and it looks amazing and the minimum requirements were on the conservative side. The game looks better at ""medium"" than high/ultra presets of some games, heck you can even argue that at some low settings it looks better than some games at medium/high presets. The presets are all relative anyways. There's no universal settings that define what each preset represents.  I just don't get the outrage on these minimum spec sheets compared to what the game is showing graphically. At least wait until it releases and we have some hard data.",Negative
Intel,"I'm also a dev dood sometimes it's literally a ""foutage de geugle"" I had a friend who talk about how shit devs in EA (the people who's making the engine) code and doesn't optimise",Negative
Intel,"The commenter I replied to said there wasn't a big generational improvement for GPUs, while I said there was. But I agree that 4080 is expensive for what it offers.",Neutral
Intel,It was the same price as a 3090 at launch and offers 90% more performance.  It was definitely a generational leap.,Positive
Intel,It's software ray tracing which isn't accelerated by hardware.,Neutral
Intel,and i guess it can be hard to tell if ram is dual or single rank with just two sticks of it too.,Neutral
Intel,>might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.  I was pointing out that 1070 doesn't have hardware RT either.,Negative
Intel,So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?,Negative
Intel,"Why not make it toggleable though, you can choose other graphics settings so it makes no sense. Most people don't play games with RT enabled",Negative
Intel,Still kinda weird that they put the RX 5700/A750 on the same ballpark as the 1070... Being that the newer GPUs actually perform closer to a 1080(Ti).,Negative
Intel,Sounds like improperly configured negative LOD bias. If you have Nvidia you can fix it with nvinspector. On AMD there's nothing you can do IIRC if Devs mess it up.,Negative
Intel,"Exactly what I'm telling them all the time, that I can see difference in textures, in hair, fences, powerlines etc. And every time I get downvoted by the army of ""but it looks better then native"". I feel like majority of that sub needs to go get a prescription for new glasses",Negative
Intel,"So if you stopped measuring who's bigger, why do you think I have no access to DLSS? Tested on rig with 4080 on a C2 42"" screen. Once you stop thinking with the tip of your head where probably Nvidia's logo is tattooed or maybe Jensen's face, who knows, you could come to realization that one person have access to more hardware then only his personal rig.  Do I say that FSR is better then DLSS? No! Both of them have their issues, but none of them is better then native. Keep on fanboying mate, instead of providing normal convoy as grownups",Neutral
Intel,I don't think YOU understand what you're talking about.  Did you seriously expect Snowdrop to be something used in like one game and then discarded completely?,Negative
Intel,"The thing is: creating good looking lightning with RT is much easier than with prebaked illumination. I can fully understand, why developers of new games are using RT as the basis of their game.",Positive
Intel,Minimum requirements go up as games get/look better. They probably made this game with the series s as the minimum. Low settings probably looks better than high in most games.   Rt for the last few years has been added on top of already complete games. Now games are being made with rt as the standard and they would have to manually do a ton of extra work to support 7+ year old hardware. It's the same as having to drop support for last gen consoles to be able to deliver a better game.,Neutral
Intel,"And still - superior upscaling makes it worth paying extra, RT on higher end is worth paying extra. AMD even this gen doesn't undercut price by enough, especially in EU where power efficiency also play a role. For example, assuming I'm gonna use GPU for 2 gens, then I save ~90-100€ with my usage just on electricity alone. I know it's over period of 4 years, but still it's price difference that will recoup over time, even if I have to pay extra upfront. AMD is simply no longer a value proposition.",Neutral
Intel,"They did not, lol - that fixes completely different thing, not the FSR caused shimmering edges.",Negative
Intel,Both excellent 👌 Down the Rabbit Hole was another solid one.,Positive
Intel,"RE4 came out this year.  Anyway the difference between us is I don't think any lighting effect is worth a $1,000 GPU using 1080p resolution and 60fps, but I know many disagree. That's life.",Neutral
Intel,"3600 and the 3300x was built with the TSMC 7nm if i recall, that was 2018 cutting edge tech. and somehow in gaming it's already obsolete. i was planning to replace mine with 5700x  but i cancel it because it's still very much capable for what i do now. but now after UE5 i think i'll reconsider it again, maybe even as far as 5800X3D",Neutral
Intel,It is crazy that I see everyone saying that modern games look blurry. I've known for years taa does this but it seems like I see more people than ever dissatisfied by it.,Negative
Intel,I have to turn it off in borderlands 3. Ugh.,Neutral
Intel,r/FuckTAA,Neutral
Intel,"My friend nicknamed it after his eyesight. When he wears glasses he has 20/20, but when he doesn’t he says that he sees the image about the same as without glasses",Negative
Intel,"Out of curiosity what resolution do you usually game at and at what distance? I find taa usually looks pretty good in modern games, but I'm also playing on a TV (48 inches but I'm not at monitor distance anymore). When I played on my 1920x1200 monitor I HATED TAA, well, not as much as fxaa, but moreso than smaa, and I missed msaa or SGSSAA with a passion. Now on the TV at 4k (lol, usually not on a 6700xt), 1440p, or even 1080p I find TAA can do a pretty decent job paired with varying levels of the Radeon sharpness adjustment in the adrenaline drivers. But again, probably due to viewing distance.  Looks leagues better than FXAA and I prefer it to SMAA now. But again, I'm sitting further away and tweaking sharpening to my liking. FSR2 seems to be a wash most of the time but the performance gain is worth it.",Positive
Intel,You dumbass do understand that even the console version of the recent games are total mess? Alan wake 2 have to run on reduced settings to even reach 30fps on xbox/PS. Callisto protocol used to have frametime issue on consoles. Console ports still targets 30fps on quality settings with upscaling while target should be 60fps with 120fps for performance mode.,Negative
Intel,"Yes, seems you are correct, it was the ps4 pro that came out in 2016, still, I wouldn't expect that to be capable of running some of the games we are seeing now without some serious compromises.",Neutral
Intel,I never said that Alan Wake 2 is unoptimised. I just said that even a 4090 can’t max out the game at native 4K resolution.,Negative
Intel,epic is tencent...,Neutral
Intel,"In terms of absolute numbers sure, 3090 always had a stupid price tho for being 10% faster than a 3080.   In terms of performance per dollar the 4090 even at 2x 3080 performance was worse value since you pay 1600 not 1400.",Negative
Intel,"Yes, you either have to get the spec sheet of the RAM modules (That even then it may not specify if it's a single/dual rank module) or test it in a system and use HWinfo/CPU-Z. Not as straightforward as dual channel that's mostly just using two identical modules.",Neutral
Intel,You don't need RT hardware to do software RT. That's what I'm saying.,Neutral
Intel,"For this particular game, there is a software fallback. Also, even if we did get a game that mandated hardware support for RT they'd still likely build it with console in mind so even RDNA2 GPUs shouldn't have issues",Neutral
Intel,"The first RT/DLSS capable cards are 5 years old now, way older than the current consoles. You are definitely not being left behind if you're only outdated by 1 gen lol",Neutral
Intel,I mean the last non ray tracing cards were released 4 years ago (1650/60 and 5000s from AMD),Neutral
Intel,Games running poorly on computers weaker than current gen consoles isn't exactly anything new.,Neutral
Intel,"No, you have multiple generations of games from prior to raytracing to enjoy.  Games shouldn't be held back forever just because someone wants to game on a GeForce2 MX400.",Neutral
Intel,You are stuck playing older games. Unless someone can find a way to disable it with a mod.,Negative
Intel,> So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?  A gen old is second gen RT e.g. rtx 3000.  We're halfway through the third RT/matrix generation right now.,Negative
Intel,Yes hah,Neutral
Intel,"The 3090 or 6950xt will very likely run this game extremely well, they're a gen old.",Positive
Intel,"For the AMD GPU, it's probably because minimum will be the max it will support. Medium probably requires Hardware RT.  For Intel, I don’t know. Perhaps it signal not so great optimization for Intel GPUs...",Neutral
Intel,DLSS is very strong in thin lines like fences & hair. Much better than most TAA solutions. Not sure about FSR in general.  An example: https://pbs.twimg.com/media/E6erA6BVkAAuf_M?format=jpg&name=large  Only DLSS resolves the lines properly.,Neutral
Intel,"I mean, it's not that black and white. Quality upscaling to 1080 will obviously be more noticeable than 4k. Maybe you *can* tell the difference between 4k native and dlss quality, but people aren't gonna believe you unless you prove it with some kind of blind test, but I doubt you'd be willing to go out of your way to do that.  Even if you could, the benefits of better aa might outweight some negligible drop in textures or whatever it is upscaling reduces. Though that's irrelevant when DLAA or NAA are options too, I guess.",Neutral
Intel,"Test, which one looks better, explain.  https://imgsli.com/MjE3MzEy",Neutral
Intel,Oh man the hair... its so crazy how much upscaling kills the hair..,Negative
Intel,i mean they already arent the smartest bulbs considering they went team green.,Neutral
Intel,Reading comrehension dude.,Neutral
Intel,"I do, but thanks for your interest.",Positive
Intel,That will depend on the user. Power is cheap here in canada so none of thar factors into my buying decisions. And rt will depends on the games you play also not everyone plays tripple a games.,Neutral
Intel,"You need to get 23.20.17.05.  Fixed Issues Intermittent flicker may be observed on some textures while playing Alan Wake 2.  Fixed for most people, also has a nice performance boost of about 10fps from what I'm seeing.  That being said I always prefer native resolution unless absolutely necessary. So far only had to use it in Martha is Dead.",Positive
Intel,Nice. It’s on the list. Thanks man,Positive
Intel,Yes.... 7nm was but 4 cores the actual important thing was obsolete like 10 or 12 years ago.,Neutral
Intel,Probably because TAA is (unfortunately) more prevalent than it ever was.,Negative
Intel,"Nah, most people are FINE with it. Its people on FuckTAA that are mad.  The thing is, you have sharpening filters today. You can adjust the sharpening for TAA. You can make it look oversharpened if you want. There's driver level sharpening for both AMD and NVIDIA. Both also have software sharpening filters.  And DLSS and FSR bot have sharpening sliders.  There's 3 ways to adjust how blurry and how sharp the game looks for everyone. It's a non issue and even Digital Foundry acknowledges the benefits of TAA for game devs vs things like the blur. At the end of the day, these games are looking great...in this era, with exceptions. Not all games are blur. The whole blur thing was like in the past 4 years since upscalers started and TAA became the norm. But with all the sharpening options, and with devs actually used to making TAA not so soft looking, things are definitely getting a lot better for TAA.",Neutral
Intel,Oh my bad if I read that wrong,Negative
Intel,And I'm saying 1070 doesn't have hardware RT so it's just as gimped as AMD cards when doing RT.,Negative
Intel,Still pretty annoying nonetheless. Until it's stable no way should it be forced upon a game when someone's willing to run it normally at native.,Negative
Intel,"and the 2080ti is still winning, best purchase in gpu i made in a long time.",Positive
Intel,imagine being mad that 4 year old cards arent high end,Neutral
Intel,I'm on a 6700XT. 🙇. I just feel very shocked that I would probably need a gpu upgrade soon. With the demands of new games. Also 1440p ruined me.,Negative
Intel,"Technically makes sense, but they can just make RT something you can toggle between Software/Hardware and they also could just add the Vega 56 in the requirement sheet being that was AMD's competitor to the 1070 or they could've added the GTX 1080(Ti)/RTX 2060(Super) into the sheet.  On Intel's side they only have the A580 as the closest GPU to a 1070, even when it's too similar to the A750 performance wise :/",Neutral
Intel,"It is that black and white for me. I can immediately see the added image softness from upscaling, and it's especially prevalent with DLSS 2.5+ unless you enable a sharpening filter. DLAA also has weird issues like blurring textures (almost like it reverts to initial mipmap) while moving, usually at mid-distance from camera. It's too distracting and I have to go back to regular TAA. Immediately goes away.  1440p to 2160p is better than 720p to 1080p, but it's still noticeable. Detail is immediately lost as lower resolution image is scaled to higher resolution. Is it terrible? No. Actually changing monitor resolution outside of native and having all UI elements scaled looks terrible. That was what we used to do and upscaling is much better than that alternative. LCD panels need to stay at native signal resolution and GPU should scale render resolutions.",Negative
Intel,"And you look at static image all the time while gaming? It's different story when movement is involved, that's where you find all the artefacts you get. Static image doesn't tell a thing when it comes to gaming",Neutral
Intel,"Yes, and that's driving me crazy. I really couldn't stand it in Cyberpunk (in other titles it's either hit or miss), that I play it in native 4k instead.",Negative
Intel,"Wouldn't phrase it like that tough. I had that choice too in January, and actually by going 4070Ti instead of 7900XT I would be able to play Cyberpunk in higher fidelity and better frames then on mine, because AMD cooked FSR3.0 almost a year in the oven, and God knows how long it will take for CDPR to add it to the game, if ever considering that active development on game is ended and they are on bug fixing sprint right now. Also AMD needs to fix FSR in general, because how it works with smaller details like hair and fences is awful. But don't get me wrong, I love my card, just feel I git sold on features that would come handy right kow instead of when I don't need them anymore",Neutral
Intel,"Sorry mate, guess I need to read comments early morning instead of late night, just saw that last bit. My apologies. I'm just tired of upscalare wars whatever sub I go, and green team fanboys screaming that ""FSR is shit, DLSS is better then native"" and then throwing around static images for comparison",Negative
Intel,"It's for textures, not object edges from FSR use, lol. Two completely different things",Neutral
Intel,"people echoing the same shit like ""ooh 4C is obsolete, you need to upgrade that geriatric cpu"". 4C is still solid for gaming (ofc no UE5) and literally anything beside doing compute intensive professional workloads.",Negative
Intel,"Sharpening makes the image less blurry, but it doesn't recover the texture detail lost from TAA blur.",Negative
Intel,"""Even Nvidia foundry""   \---   they are like chief proponents of blur-tech and they don't care about how sharp and easy-to-see the image is. TAA flattens the image so you can't see which objects are far and which are near, you have to strain your eyes and mind to determine that which is unbearable to many. Just give us option to disable it even if it breaks some effects (which is just lazy thing to do on the side of devs)",Negative
Intel,"Sharpening fixes blur, not clearity. That is TAA destroys+add significant ghosting no matter the implementation.      Sharpening looks was worse than the normal cleairy of games, it's not the same thing at all.      >Its people on FuckTAA that are mad.  We are mad because it being forced 1 and 2 it's being used to hide lazy effects that break without temporal frame smearing. People are now punished for turning off forced frame smearing.",Negative
Intel,And I'm saying that maybe the GTX 1070 is better at even software RT compared to AMD cards with no RT hardware. GTX 10 series card did get RTX support through CUDA even though they were horribly bad at it. Older AMD cards with no RT hardware didn't get any support like that from AMD.,Neutral
Intel,"You're not forced to use upscaling though. Unless you replied to the wrong person this is about ray tracing, which can be run at native just fine. If your GPU can't handle RT at native res, then just use upscaling.",Neutral
Intel,The minimum requirements for this game includes de 1070: a 7-year-old gpu from 2016.,Neutral
Intel,"I'm not really shocked, and was out there telling people not to buy those cards on day 1 in large part because of their deficiencies in RT performance and complete lack of matrix accelerators (which Nvidia and Intel have both made a large part of their architecture and got shockingly good results with - DLSS and matrix-XeSS are twice as good as FSR)",Negative
Intel,5700 was probably the lowest AMD card they had to test with.,Neutral
Intel,"It also looks better in gameplay, less ghosting, less shimmering, less aliasing.   So which one looks better? You clearly see the difference in textures/fences/powerline, right?  I even made it easier for you, both images are 1080p output.",Positive
Intel,I personally didn't try DLSS even once but FSR 2.0 does have shimmering I notice to various degrees. It was very noticable on fur in Ratchet & Clank cutscenes for example. Overall I am just happy my 5700xt could still keep up with 1440p.,Neutral
Intel,Hmm yeah whatever you want to believe and let's you sleep.  Have a nice day.,Positive
Intel,AMD CAS is aimed to restore detail,Neutral
Intel,"They aren’t chief proponents of anything, TAAU has been popular for well over a decade at this point and it’s the default approach for consoles (checkerboarding has moire and aliasing issues) and digital foundry simply recognizes this is the reality of how the console world has settled on doing graphics.",Neutral
Intel,The problem with Nvidia is they are massively overpriced. One 4080 would probably cost the same as my whole build or more.,Negative
Intel,"Yes I see which one looks better, and as I said before it's a static image. While I didn't use upscaling in this same exact example, I can go by my own experience and tell that it doesn't look better then native let's say in same Cyberpunk or any other game. And I will beleive my own eyes on my own screen then static images tossed around. TBH, in my opinion it all depends on screen and screen size too, as to me that difference between upscaled and native was less noticible on my old 27"" 1440p screen then it is now on 42"" 4k. DLAA tough, that's another story",Neutral
Intel,"I tried both, and neither of them are better then native as people tend to claim. In my testing FSR have big problem with smaller details like hair, fur, fences, powerlines and as you mentioned, you get severe shimmering effect. I really hope AMD will keep on working on it, an try to get away from that. But DLSS also adds shimmer, it's just less visible then in FSR (atleast in my testing on Cyberpunk pre 2.0), and whatever upscaling method I've used I also could see that image is upscaled (lower res textures here and there etc.). I like idea being upscaling tech, it helps prolong life of GPUs, and for more powerful GPUs to run higher settings with more fidelity, but there's still work to do on them as they aren't perfect yet.",Negative
Intel,Yeah but a 3060ti didn't,Neutral
Intel,"In Cyberpunk you are not comparing upscaling vs no upscaling at the same settings though. You are comparing low RT native vs path traced upscaling and at 4K, the decission there is absolutely clear. On my 4k OLED screen, upscaled PT is so clearly the way to go, there is no competition.",Neutral
Intel,"The gameplay looks exactly like the image + your typical TAA/DLSS smearing. And as I said, in my example DLSS also looks better in gameplay, because as you see... native has a ton of aliasing which is super noticeable while playing (shimmering). There are so many objective tests, which show why its most of the time better than native. (up res to 80% instead of 67% and its always way better, even at 1080p)   Well when did you try DLSS the last time? Newest versions are miles ahead of the ""old"" 2.0 version and pretty much always ahead of standard TAA.  DLSS is essentially a way better TAA solution + AI to fight TAAs issues + lower res. Its so much better than your standard TAA, that youre able to reduce the res and it will still look better in movement. Think about it like codecs for streaming... there are codecs which need a way lower bitrate for the same quality than others. (av1)  Everything graphics related is not rendered at full res anyway (shadows/lighting/lod etc...), there is no line between native and upscaling. Talking about native is nonesense, just image quality counts and thats were DLSS or even FSR 2 (some games) gets better and better. There is a reason why consoles use it... it looks better than without it.",Positive
Intel,"If only Intel had stayed in the memory business!   They'd be enjoying Micron valuations and wild profits and performance from copackaged CPU+GPU+LPDDR of their own design and manufacture...     But no, they'd rather invest billions in buying donuts as a service, or whatever their crazy investements went into.",Neutral
Intel,"damn an iGPU using 32GB of vRAM, I wonder if they're testing a Panther Lake laptop with 48GB RAM or even more (since X7 & X9 Panther Lake only accepts soldered memory)",Neutral
Intel,"If Intel is really about to release a B770, honestly the **only thing that could make it competitive is the price**. (FOR ME, competitive in 2026 means <400€) From a performance standpoint, it would need to undercut existing GPUs quite aggressively to make sense, especially given how crowded the mid-range already is.  That said, I’m pretty skeptical about how realistic that is. **With the recent RAM shortages and rising memory costs**, pricing a new card competitively while still keeping margins doesn’t sound easy at all. Memory is a huge part of the BOM, and we’ve already seen how shortages can push prices up across the board.  So unless Intel is willing to take a serious hit on margins (which seems unlikely), I’m not convinced the B770 will land at a price point that truly shakes up the market. Happy to be proven wrong, but for now the pricing question is the big unknown for me.",Neutral
Intel,So there's a 20GB variant. A 28GB variant and a 32 GB variant?,Neutral
Intel,Optane was practically **built** for the type of AI workloads that they're shoveling money at.  If Intel didn't give up literally only a matter of months before GPT released and the bubble began in earnest lol,Neutral
Intel,"If Intel stayed in memory business, it would be long dead in the 80s and killed by Japanese memory companies. CPU remains the top niche area with less competition and deeper moat. See how China has quickly come up with their GPU designs? Well it will take at least another decade for them to make 2nm CPUs",Neutral
Intel,"are people high or something? intel was losing money on optane and their SSD business became irrelevant the minute regular memory manufacturers slammed the market. don't get me wrong, they were some of the most durable on the market, but they were no where near printing money on the memory business.  optane may have survived if their nodes were on schedule, keeping CXL support on schedule, but not because it was profitable.",Negative
Intel,CXL killed octane it’s that simple. No one wanted to be locked to just Intel. CXL was and is just better,Positive
Intel,"I feel like the price has to be more than competitive. If they can undercut competition cards of the same performance by 100 or so (or maybe offer rebates or freebies) they could potentially steal the market in that category. With Nvidia and amd cards being tried and true for many many years, I feel like their marketing needs to grab the attention of consumers in a somewhat drastic way.",Neutral
Intel,If the b770 is 5060ti levels even €500 is competitive,Neutral
Intel,Yeah sure it would’ve been perfect but CXL killed octane and offers pretty much everything it did while not being loved to just Intel lol,Negative
Intel,"it's not like any of this AI garbage right now is profitable for anyone except nvidia and the hardware companies anyway, it's not stopping everyone from shoveling money into it",Negative
Intel,"> With up to 192GB of VRAM across eight GPUs in a single system, Battlematrix positions itself as a relatively cost-effective alternative to other professional GPU ecosystems for AI inference workloads.",Neutral
Intel,Hope they do some image and video generation benchmarking as well. Nice to see someone testing AI rigs out there.,Positive
Intel,Wish they’d give prompt processing speeds. AI coding generates very few tokens compared to input. Nvidia seem to dominate here.,Negative
Intel,"How many concurrent users will this serve, 30 devs would be nice",Neutral
Intel,:),Neutral
Intel,"These 12Xe3 cores are pretty neat, and because it fits in a normal socket it isn't ludicrously expensive to make.  I suspect we'll see a ton of these different form factors for this chipset.",Negative
Intel,Mac Pro Trashcan 2.0 is crazy,Neutral
Intel,"> These 12Xe3 cores are pretty neat  have there been any leaked benchmarks or gaming FPS?  on paper they look good, but... some synthetic benchmarks suck",Positive
Intel,No one knows. Synthetics seem to put it roughly at a 3050m.,Neutral
Intel,"3050 to 3050ti mobile if leaks are to be believed. Could get a bit better than that if software is still not mature, so I'm calling a max of 3060M performance.",Neutral
Intel,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Negative
Intel,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Neutral
Intel,I hope it comes to desktop CPUs,Positive
Intel,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Positive
Intel,Yeah this headline doesn't add up based on my own testing,Negative
Intel,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Negative
Intel,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Negative
Intel,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Positive
Intel,concur  some benchmarks are biased,Neutral
Intel,lateral,Neutral
Intel,If it’s just “16% faster than 890m” it’s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Negative
Intel,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Neutral
Intel,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Positive
Intel,Answer to strix halo was the partnership with nvidia,Neutral
Intel,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Neutral
Intel,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Neutral
Intel,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Neutral
Intel,"I mean no offense, but Passmark is irrelevant.  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Neutral
Intel,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Neutral
Intel,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Neutral
Intel,"I guess we'll see more when we get actual info about the potential devices.  Right now, I haven't read about a device coming to market.",Neutral
Intel,"should have a 3y warranty on it. submit an RMA ticket  regarding the actual query though, the silicon is the same the 14900ks is just a slightly better bin. you wouldn't notice the difference at stock let alone normalized for energy consumption",Neutral
Intel,"I have i9 14900ks, what I did is that I reset bios settings to optimized defaults and then I limit pl1 and pl2 to 150w and enabled XMP, these are the only two settings i changed, the rest is default, and temperatures are in check, i still get the same performance, and it’s very efficient in gaming that way, the extra heat and power consumption of 253 or 320 are not worth it, I recommend just get the ks and make these two changes and forget it.",Neutral
Intel,The performance difference will be tiny and definitely not noticeable with a 3090. Go for the cheaper chip.,Neutral
Intel,"Get the KS for better silicon quality only limit power , set pl1/2 253w and set it to 350 or 325A, definitely want the better 14900ks silicon quality it’s overall better and better IMC as well. It’s a better bin and typically only the best 14900k will run stable 6.2ghz and at lower voltages even if you limit your chip to 6ghz",Positive
Intel,If it doesn’t cost you extra then get the 14900ks and lock all the cores to 5.6 and power limit 256w,Neutral
Intel,"A 14900KS is nothing more then a binned 14900K. Running a 320W/400A extreme setting is not advisable with a AIO. I run my 14900KS on custom loop with 320W/307A performance setting and it does not thermal throttle at all. If you get lucky, you could get a 14900K that can run KS settings. Performance in that case is( should be) identical. Without benchmarks i can't really tell the difference between 125/253/307 and 320/320/400 except the heating of my room.",Negative
Intel,"As an update - I went ahead with the 14900ks and also changed my cooler to a 420mm AIO.   Ensured latest bios update then set Intel presets (performance) but also went ahead and reduced PL to 150w, set temp limits to 70c, system agent voltage to 1.12, 307A, and I was blown away by the temps!! I am getting basically identical performance (+ few fps) to my previous 13900ks, but a whole whopping 30°c cooler in game!!! I would average 80-85, now it’s sitting super chill with same in-game settings on BF6 & ARC at 50-60c.   Thank you everyone for your inputs, I sincerely appreciate it and I’m extremely happy with the outcome!",Positive
Intel,I also PL1/2 at 150. My temps stay under 60c when gaming.,Neutral
Intel,"if the cooling wasn't sufficient disable HT(useless for gaming) and undervolt it this lower CPU temperature by 20c, in games the CPU temperature should be around 65c.",Neutral
Intel,Im using a duel air tower for my 14900k game temps are at 60 to 70,Neutral
Intel,"14900KS is just a better binned 14900K. All things equal, you should have lower temps/voltage/power draw for the same exact workload/clocks on a 14900ks vs a 14900k. How big of a delta between the two comes down to how well you struck the silicon lottery with the KS.",Neutral
Intel,"I have the K version only because of the onboard gpu. In case my GPU gives issues and I'll still be able to boot. But otherwise there is almost no performance gain. I ran my i9-14900k pl on 320 watt and did a cinebench benchmark, temps were ok: average 94c, max 98c with a 360 aio.  That said, go for the cheaper version if you don't need onboard gpu.  Edit: I have my pl on 253w now. No need to go any higher.",Neutral
Intel,5 years warranty on 13 and 14gens now.    I have the 13900ks. Run it at 253w. Clock locked at 5.5ghz. Temp 80c and cinebench 23 39k,Neutral
Intel,Why not keep pl2 at 253 and 1 at 150/185 ? Did you try undervolting? Most of them can take 50mv offset with 75 /85 needing a bit more stability testing. Can also cap the vr limit and iccmax. I feel like going 150 pl2 makes you miss some performance in games unless you had thermal issues and doing it to keep it from thermal throttle.,Neutral
Intel,"Thank you for the feedback. Forgive me for the dumb question; if I ran either a 14900ks or a 14900k at these settings, would they both have the same temps? Or would the KS still run hotter?",Neutral
Intel,"Dropped the voltage further down by -0.10000 and now I’m getting 58°c core temp and max 65°c package temp under load. Really happy with this, and with some tweaks to my in-game video settings I’m able to still maintain a framerate that matches my screen refresh rate.",Positive
Intel,This post is about the K and KS. Both have the same iGPU,Neutral
Intel,even better. i take it they extended tbe warranty period for those products?,Positive
Intel,For 5.5 39k in CB23 is a little low,Neutral
Intel,"I have tried and tested all my games, i saw absolutely no difference between 253w, 150w, 125w or even 100w, the fps were exactly the same, the only difference was in temperatures, performance wise i saw no difference between any of them, i was using 100w before but then I switched to 150w because I thought it was too low, even though the performance is still the same as 100w, just higher temperatures, my cooler is pretty good kraken elite 360, it never goes above 80 even on 253w but I just like to keep temperatures between 50-70 while gaming.",Neutral
Intel,The ks should run cooler because of the better bin. Less voltage being required to hit certain frequency points.,Neutral
Intel,"Oh shit, I thought only the K had an igpu! Should have gone for the ks version lmao",Negative
Intel,Yes because of the degrading issue.,Neutral
Intel,Lol stock is 5.8ghz lol and most stock after the update get 35k,Neutral
Intel,All core cinebench is not 5.8... I get 39k stock what are you talking about lol,Negative
Intel,Search on reddit on 13900-14900k.  After the code update stock most 13-14k can barely do 35k. Dont like to your ego brother.   So millions on reddit are getting those score and you are the special bin whose getting a higher score.   Mr 1 post and 7 comment history lollllllllllllllllllll,Negative
Intel,LOLOLOLOLOLOL HAHAHAAHAHAHAH ARE YOU DUMB? This really shows you don't have a 13900k or 14900k,Negative
Intel,"My guy what are you talking about? 5.5 ghz for 39k is a good score on 13900k. My 14900KS completely stock does 41.5k and downclocks to about 5.5-5.6 ghz with hyperthreading on. If he's got HT off, his score is even better.   You have to be rage baiting.",Negative
Intel,Talk to him not me... The guy said 35k is the score 😂😂😂😂😂,Neutral
Intel,Will be a interresting CES,Neutral
Intel,I'm half-expecting this to show up as a server-only AI-focused SKU with video outputs removed.,Neutral
Intel,Merry Christmas everyone,Neutral
Intel,"4070 performance for $400, I'm calling it. Would have been great if this had come out right after the wave of negative press that the 5070 received for only being 10-15% better than the 4070 with a mediocre 12 GB of VRAM, but I feel like Intel missed the boat again if the Steam Hardware survey is anything to go by, the 5070 has really made a comeback with recent sales.",Neutral
Intel,They can't even ship B60's.,Negative
Intel,"What is taking Intel so long?      It's already been almost a year after Battlemage's initial launch. And for what? RTX 5060 performance at the same price with some extra VRAM?  I had really hoped Intel would be able to gain ground on their competitors. At this rate, we'll get the ARC C770 to compete with the RTX 6060 in another 3 years.",Negative
Intel,Aren't they always,Neutral
Intel,give it some time...,Neutral
Intel,Merry Bitmas,Neutral
Intel,"4070 performance for... used 4070 price, now with driver issues and an objectively worse upscaler!  intel greatest hits",Negative
Intel,Sure they can if you search for it   B60  https://www.idealo.at/preisvergleich/OffersOfProduct/207972918_-arc-pro-b60-sparkle.html   Or b50 https://geizhals.at/intel-arc-pro-b50-a3584363.html,Neutral
Intel,"Intel's GPU division has been operating at a loss never mind Intel as a whole and ARC series cards aren't as popular as the enthusiast circles would have you believe. Coupled with how expensive R&D is for things like GPUs, it's hard for them to pump out a competitive product while remaining just profitable enough to undercut AMD and Nvidia.",Negative
Intel,What is taking Nividia so long with the super cards?,Neutral
Intel,"Battlemage gpu chips are made through TSMC and Intel is getting screwed on supply, this is why even if the B770 comes it will only be a small amount. Hopefully Intel can put together enough rare earth to pump out discrete Celestial Gpus but it takes time to ramp everything up. In addition Intel has their chiplet design, EMIB that could take off soon. They may be able to bring Apple back into the fold, but let us hope Discrete Arc lives on.  I have learned to not have expectations for anything that is outside my direct control, I do the best I can to just go with the flow. Whatever will be, will be.",Neutral
Intel,There's definitely been lame ones.,Negative
Intel,"I'm fully aware it's not a great deal, but that's my expectation when it comes to Arc.",Negative
Intel,"These are European links and will be out of stock. I found a mom and pop place back in my old stomping grounds in San Francisco, and they normally only sell B60s in prebuilt systems but a special order is possible.   Intel can't rely on TSMC for Battlemage supply, so let us pray that Discrete Celestial GPUs are made (entirely) at IFS and release in 2026 / 2027.   May your Bits Byte Hard, long live the Arc.",Neutral
Intel,"If CES 2026 comes and goes without any details for Discrete Arc GPUs then it could be awhile. The main thing being promoted is Panther Lake which should be made entirely at IFS, a step in the right direction. The TSMC monopoly is destroying the industry and it hurts companies here in the US.",Negative
Intel,Not sure how the link being European matters. They are European shops and the B60 is in stock.,Neutral
Intel,"We kind of know where it will land. It will be a 3050M level chip, maybe a bit better, but will have improve scaling and frame gen.   8060S is 40 RDNA 3.5 units. One Xe3 unit is about 2.1 RDNA 3.5 units. That put it at about 65% of Strix Halo, though it will have a worse memory bus and no MALL cache. Somewhere in that range.   So that 60% is almost bang on the 3050M. Maybe a bit better. It won’t be like the 4050 but 3050M isn’t bad for an iGPU that fits in a normal socket",Neutral
Intel,"> and no MALL cache  PTL does have 8MB of memory side cache, fwiw.",Neutral
Intel,"I don’t know how this score compares to the 3050M. I only know that this score is about 55% of the B580. And the B580, at 2K and 4K, is about 1.7–2 times the performance of the desktop 3050",Neutral
Intel,and also an iGPU won't be stuck with 6GB of vRAM 😅,Neutral
Intel,Really don't understand why they don't go with a larger cache.  Pretty sure they still have a bunch of cache chunks spread all around the SoC.,Negative
Intel,"in configs without die-to-die memory performance in general should be worse if bandwidth limited. despite not being specifically dedicated to onboard memory like Lunar, people are still planning configs with local LPDDR5x, though peak bandwidth is limited by a 128bit bus.",Negative
Intel,I see it personally as not really being at 20W if you’re asking much from the GPU. It’ll increase the juice dynamically if it gets demanding enough. So it’ll be hard to say unless you force the power limits way down manually.,Neutral
Intel,"I actually like the idea of discrete GPU naming scheme for the new iGPU, 300 series for integrated graphics is really makes sense but they should add 'M' suffix to make it clear.",Positive
Intel,"I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  Also looks like the 3\_8 and 3\_6 versions are differentiating maximum boost clocks, though I wonder if instead those may reflect that configurable upper TDP bound. Might make sense for 65W and 80W to be differentiated if that will coincide with anything about the ""experience-based"" PL1 behavior.",Neutral
Intel,There are 2 dies.  One for professional workload which will be mass produced.  Second is for gaming. Even a 10-12 core xe3 will be barely enough for modern 1080p. Lunarlake can only run alanwake at 1080p at low settings getting only 25 frames so even if this is 50-100% better this is the minimum for a 2026 product.   I see no point of a 8 xe3 core system when all people will do is just complain.,Negative
Intel,"quite confusing, Xe3 should start from C (celestial), if using name like B390  we think this  is a battlemage Card (Xe2)",Neutral
Intel,"I wonder if rumors about Zen 6 clocking way higher than current cpus turn to be true, and the 5.1ghz max on mobile PT mean Amd might have an edge in next generation   Only time will tell",Neutral
Intel,please add M for Mobile or i for iGPU  * Arc B390M Xe3 Graphics * Arc B390i iXe3 Graphics,Neutral
Intel,"A clock speed regression vs the prior gen on N3B, with a remark that it's difficult to cool, really isn't a good look for the process side. 18A branding with more like N4 performance...",Positive
Intel,Will the 10 core Xe be better than radeon 890m or worse?,Negative
Intel,"i mean i get this is a laptop part but man 16 threads is not much to phone home about when it comes to horsepower, isnt next gen desktop aiming for something like 48 threads?",Negative
Intel,Still weaker than x3d,Negative
Intel,Yes indeed we need that M&M. Mobile platforms are not a priority for me and are dedicated mobile gpus really comparable to Big Boy Discrete GPUs? It is very confusing.  Lunar Lake laptops should fall in price. Has anyone used Lunar Lake and if so which models? Buying latest gen is for guinea pigs and the rich!,Negative
Intel,Doesn't the B already serve that purpose?,Neutral
Intel,">I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  The 10 Xe3 core model is a binned down 12 Xe3 N3 die, and I doubt yields are so bad that they would even be able to find more dies where they have to disable more cores.   The other die is the 4 Xe3 Intel 3 die, so you can't go up from there.",Neutral
Intel,To be fair Alan Wake 2 low settings look great. This was covered by DF awhile back they said in some ways Alan Wake 2s low settings look better than some modern games high.,Positive
Intel,"I mean, but that logic, most of Intel's historical bigger iGPUs don't make sense. There are use cases other than AAA gaming. Media creation is another big one.",Negative
Intel,"Xe3 is not GPU family name but GPU core architecture, it's like Nvidia Ampere, Ada Lovelace. But Alchemist, Battlemage, Celestial is GPU family name.   Panther Lake 12Xe3 being B series GPU makes sense because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture. Intel confirmed Celestial will have Xe3P.",Neutral
Intel,"Think of it like AMD Zen X+ nodes. Ryzen 8000 is more or less a laptop only APU series on Zen 4+.   Xe3 is a half-generation, it doesn't get the letter upgrade to C, but it gets the 3, signifying a new architecture, but not a new generation.  Zen 4+ is a half-generation, it doesn't get the number upgrade to 5, but it gets to be 8000-series, signifying a new architecture, but not a new generation.",Neutral
Intel,"Mobile Zen 6 is likely to come around the same time NVL does. Both should use N2 and will presumably have similar frequencies, well above any 18A parts.",Neutral
Intel,"Considering the timing of 2nm, zen 6 would be around late 2026, and mobile zen 6 late 2027 wide availability. for whatever reason amd takes forever despite high mobile demand, but this quarter it looks like it worked out for them (maybe a big bump up from x3d sales).",Neutral
Intel,What're you doing on a laptop?,Neutral
Intel,"These are for thin and light office notebooks and light gaming. Think Lunar Lake. For CPU power, Nova Lake H will exist.",Neutral
Intel,It's for handhelds and office laptops not hyper enthusiast shit.,Negative
Intel,Nobody buys AMD laptops,Neutral
Intel,">Still weaker than x3d   Source?? Also Panther Lake is H series only, HX will be based on Nova Lake.",Negative
Intel,Yeah so it is weaker for gaming with a dGPU than the 0.2% of laptops currently sold that have either a 7945hx3d or 9955hx3d that makes up for less than 0.1% of all laptop users. What's your point?,Negative
Intel,"While I haven't used it daily or anything, and I've only done initial setup on the Lunar Lake, the feedback we've gotten both on Arrow Lake and for Lunar Lake (e.g. 268V and 265H) Dell models is that it's a big increase in battery life and performance. The integrated graphics (e.g. 140V and 140T) are very capable compared to a **workstation** grade NVIDIA Ada 500 GPU, but they are not even comparable to a gaming GPU like the GeForce 4060 or even a 5050.  The integrated graphics do however get used for 99% of all workloads unless explicitly specified because they are vastly more battery efficient and draw less power compared to a dedicated NVIDIA chip, meaning you can have a much smaller external power supply, and your graphics performance in those basic desktop workloads with one of these chipsets will be **much** better than previous generation Intel chips. Exceptions are obviously something like gaming or AutoCAD that specify to use the high performance dedicated graphics chip.  140V/140T are barely functional for modern AAA gaming, but if you stay 5-10 years back for AAA titles you might be okay. It will smoke most Indie games. Just look at the per title benchmarks for a 140V/140T and you can see if your game benches. You could probably get away with a lot of functional mobile gaming without a dGPU, but I wouldn't expect to be able to play a recent Call of Duty or Black Myth or anything with anything like an acceptable framerate at a decent resolution. This integrated graphics chip compares very favorably to its more common Ryzen 7 equivalent, I believe the 780M, and it's a very good APU for handhelds overall due Lunar Lake's power efficiency compared to other X86 chips.  You have to understand that for these next two generations Intel seems to be making big strides in terms of both power efficiency and integrated graphics for mobile, it's a very attractive option and the first time I've seriously considered a laptop without a dGPU. I think Panther Lake is going to be a very nice kit next year for both laptops and handhelds and give AMD a run for its money.   I suspect AMD genuinely needs a new APU graphics architecture implemented next year to keep up, which I expect them to. Not a bad problem to have.",Positive
Intel,"Yeah for sure. It's a small die and should be yielding pretty high. See also the number of 4+8+4 SKUs. Looks like the larger CPU tile is also yielding decently, so not a ton to cut down.  I'm partly saying that because a larger Intel3 die was certainly possible. Even if it was 6 Xe3 cores and built as half of the larger die (just one of the two render slices)  it would fill the void a bit more.",Positive
Intel,Which is rather silly. They should've named celestial Xe3 and the current Xe3 as Xe2P,Neutral
Intel,"> But Alchemist, Battlemage, Celestial is GPU family name.  Specifically, *discrete* GPU family name.   > because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture  That is simply not true. Xe3 brings much bigger changes over Xe2 than Xe3p does over Xe3. That's why they were named that way.   > Intel confirmed Celestial will have Xe3P.  No, they actually haven't said anything about Celestial (again, as a dGPU) at all. They said that C-series naming (i.e. NVL iGPU) will start with Xe3p.",Neutral
Intel,Highly immersive porn on the go,Neutral
Intel,These are H series chips. Even the U series chips don't go down to LNL min power levels.,Negative
Intel,"NVL-H is 400 series, *replacing* this, next year. Not supplementing this lineup.   Adding more cores won't do anything for gaming.",Negative
Intel,PTL extends up to the -H series too.,Neutral
Intel,"HX this year will still be Arrow Lake.   Nova Lake will be a full line up, with S, U, H, and HX, but end of 2026 / early 2027",Neutral
Intel,">Source??  You can't seriously be asking for a source for whether or not this part will be able to power dGPU gaming laptops better than X3D chips.   >Also Panther Lake is H series only, HX will be based on Nova Lake.  Not till late next year or early 2027. It's all arrow lake till then.",Negative
Intel,> Source?   Common sense suffices. It's a tick core with a clock speed regression at that.,Neutral
Intel,"Surely a cost decision. The 4Xe die, including the choice of Intel 3, is supposed to be the cheapest thing to deliver an acceptable mainstream PC experience. They need PTL to be a proper volume runner and start displacing the RPL that's still a large chunk of sales. WLC should hopefully finish the job.",Neutral
Intel,Just get a Vision Pro?,Neutral
Intel,"Tbh, more cores would just make that go faster, but 16 would already be plenty. Especially for something like that where it's probably going to be a linear analysis and ram constrained if they actually modelled the gas (which would not necessarily be required).",Neutral
Intel,Lmao this is funny we both responded to the same comments with the same things within like 2 minutes of each other.,Negative
Intel,I don't see anything wrong with asking for actual benchmark information especially when there isn't anything official. X3D is nice but it isn't the end all be all. I would be curious to see if Intel can manage to compete.,Neutral
Intel,"Oh I totally agree, but it would've been nice you know? Jumping to 6 Xe3 is a  significant area increase for a tiny tile. I understand exactly why the 4-10 gap exists, but I can't say I don't wish there was something to fill that gap if only because it looks weird.  I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.",Neutral
Intel,"PTL's main changes are fixing MTL/ARL's terrible SoC design, which should net a few % performance. It'll see a mild IPC increase, getting a few more % performance. And it'll lose a bit of clockspeed, erasing most of those gains.  Expect PTL to be very similar performance to ARL, but with lower power consumption, a much better iGPU, and most importantly to Intel: Using their own fabs instead of TSMC.  It absolutely won't be X3D in gaming.  Edit: Actually shocked that people think this would compete with X3D.  9955HX3D is \~16% faster than a 275HX in gaming...and a 275HX itself is easily 10%+ faster than a 285H in gaming.  Not even Intel themselves are claiming this. Their own marketing refers to PTL as ""ARL performance with LNL efficiency"". Nobody realistically expects PTL-H to see a 25%+ gaming improvement over ARL-H. The fact that IPC increase is less than 10% and clockspeed is slightly lower than ARL-H should make this obvious",Neutral
Intel,CGC is a LNC tick. This is well known at this point. And we see it's even a clock speed regression.    Even entertaining the notion it will close the gap to AMD's X3D chips is just delusional.,Neutral
Intel,"Oh, yeah, I get you. Wish they could give more granularity. Just personally think some sacrifices are worthwhile if it can condense Intel's mobile lineup back down to something sane again.   > I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.  Yeah, should be a good fit. Shame they don't have anything with a bit more CPU umph, though. 4+8+4 and only up to 5.1GHz is *fine*, but not great. Especially without an HX replacement.",Neutral
Intel,"Two options seems right, either you care about it or you don't.",Negative
Intel,"Its not ""delusional"" to want to see actual numbers instead of speculation. I have been in this game long enough to see plenty of speculation even with accurate information not give the actual numbers.",Negative
Intel,"Given how well ARL HX was received in gaming laptops, I think they may wait to have something from NVL take that top spot. 5.1ghz does seem low though. ARL-H will happily do 5.4 and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP.  I suspect these may not be totally final clocks though they do seem reasonable.",Neutral
Intel,"Honestly, surprised ARL-HX is doing as ok as it is. The deficits of the architecture in gaming are well known. If it could hit the same clocks and core counts, PTL should look a lot better still. And all that besides, ARL's cost structure is horrible. For Intel's own sake, the sooner they move on, the better.   > and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP  From the same leaker, these chips are at 65W or even 80W TDPs, so they're not merely power limited. It seems that 18A just significantly underperform some expectations, though in line with some rumors and the gist of the revisions Intel's been making to its projections over the last year or two.  > I suspect these may not be totally final clocks though  If they're defining SKUs and such, these clocks need to be finalized for all practical purposes.",Negative
Intel,"If I'm reading correctly those are max power limits, not the TDP,  though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  As for why ARL HX is doing well in gaming laptops, I think a good bit of that is also part of what made it lackluster on desktop. It doesn't really scale up that well with higher TDPs and power limits, but it does seem to scale down. The 285HX with its 55W TDP and 160W max limit doesn't perform far off the 125W TDP and 250W max of the 285K.  It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system. The 9955HX3D is very impressive, but quite a lot of laptop buyers seem to value the ability to do more laptop-like things with their gaming laptops than the extra frame rate. I'm hoping this gets shaken up as AMD adopts new packaging tech as seen in Strix Halo.",Neutral
Intel,"> If I'm reading correctly those are max power limits, not the TDP, though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  Not guaranteed given it's just a twitter leak, but I'm assuming the leaker is using the term TDP consistently with Intel's historical usage, i.e. PL1. For ARL, 115W is PL2. I would also assume there wouldn't be the disclaimer about it being hard to cool if they cut the PL2 so much, though PowerVia does create some interesting complications there, so maybe not quite apples to apples.  Either way though, don't think it should have much impact on ST boost. You're talking a good 70%-ish of power going to compute, so even at 65W PL2, that's still 40-50W available for one core. Should be *easily* sufficient to hit whatever the silicon is capable of.  > It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system.  Yes, and this is something I've very much looking forward to with NVL-HX. At this point, the biggest demerit of the -HX platform vs -H is the use of standard DDR5 vs LPDDR. That's because it's still based on the desktop silicon with the different SoC/hub tile. But with NVL using a shared SoC die, they should be able to offer an -HX platform with the core counts people expect (though probably limited to single die 8+16), but the power/battery life advantages of -U/-P/-H. In general, should help make the -HX more of a straight-up upgrade than the tradeoffs one faces today.  AMD has this situation even worse today, because there's a much bigger gap between their desktop SoC architecture and the mobile one. Though as you say, they may also bring them closer together in the future.",Neutral
Intel,Try the shunt mod,Neutral
Intel,"Cool, errr...  icy",Neutral
Intel,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Neutral
Intel,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Positive
Intel,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Neutral
Intel,did you use dry ice? how did you hit sub-ambient?,Neutral
Intel,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Neutral
Intel,Are you in the US? If so how were you able to get Maxsun?,Neutral
Intel,Oh... for sure 😁,Positive
Intel,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Positive
Intel,Great work dude! Only 200MHz to go 😉,Positive
Intel,Car coolant in the freezer 😁,Neutral
Intel,That's the way! Let us all know the results.,Positive
Intel,I am in Australia.,Neutral
Intel,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Neutral
Intel,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Neutral
Intel,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Positive
Intel,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Neutral
Intel,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Neutral
Intel,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Negative
Intel,Oh! You put the car coolant to run through a freezer? Wow! Nice,Positive
Intel,But Core Ultra 255Hx is almost $150 more than Zen 4 8945HX on lenovo Legion. That price gap is enough to upgrade 5060m to 5070m.,Neutral
Intel,"Can I redeem the codes to my accounts on a different Intel system? I bought a B860 motherboard and an Ultra 5 245k, but I won't be building that system till Christmas. I'm currently running an 8700K on a Z370.",Neutral
Intel,Idk if I did it wrong but redeemed my cpu but not my arc card on the website. Couldn’t contact support because it kept throwing invalid captcha at me.,Negative
Intel,just purchased a laptop from micro center with a 275hx but don't know how I would redeem this specific offer as I only get the one that lets you pick 1 of 4 games. Does mine not qualify/is micro center not participating?,Negative
Intel,"nope, you need to have installed Ultra processor to get promo game, because Intel used software to check it",Neutral
Intel,I really would like to know how it will compare to AMD Ryzen AI MAX+ 395 (Strix Halo) APU?,Neutral
Intel,That naming scheme really is complete and utter dogshit,Negative
Intel,Sounds like fooz will be getting a taste soon? Christmas or Q1 2026?,Neutral
Intel,Can we just toss random darts at tech words and numbers to assign a different naming scheme to each and every different Intel Product?,Neutral
Intel,I'm looking forward to check how those series will perform!!,Positive
Intel,"This isn't a ""halo"" product. An upgrade to Arrow Lake/Lunar Lake when it comes to iGPU, but not Strix Halo. It won't cost as much as well.",Neutral
Intel,look forward to new APUs,Neutral
Intel,Wait for Nova Lake AX. It will have 48 GPU cores instead of 4/8/12.,Neutral
Intel,Likely will be worse since it has only 1/3 the power usage and significantly smaller chip size.,Negative
Intel,"It's meant to confuse you in purpose, so you ignore it and go by the 3/5/7/9 scheme. Marketing success is dependent on the company leading the customers to the way they want it. So it needs to be complex and confusing.",Negative
Intel,"also why is everything in a lake. like really that's the last place you want your chips to be.  and why is it 255k, 285k and so on instead of just 250k and 280k.  and why is it arc xe3 and not just xe3 ahh  so many weird conventions really. no consistency at all. just pure confusion.  apple and nvidia get it right too, it's really not that hard.",Negative
Intel,"'Strix Halo' is lees about being a halo product, because it’s just an AMD internal naming scheme for the set of specific APU products. APUs that will follow 'Strix Halo' will be called 'Gorgon Point'. Since 'Strix Halo' is already out and the Panther Lake in question is about to come out it might be better to compared it to the next AMD 'Gorgon Point' APU line? But I still think it can be compared to 'Strix Halo'. At least I’m looking forward to see the comparison.  Edit: changed 'Strix Point' into 'Gorgon Point' since that’s what I meant.",Neutral
Intel,You’ll be waiting till 2027 on the amd side.,Neutral
Intel,All halo iGPUs are way too overpriced. Even regular iGPUs are overpriced going into $1K laptops.   And if you want to spend that money you can do it today with Strix Halo.,Negative
Intel,"Lesser power usage doesn’t always equals to performance losses.  See the Apple Axx SOCs and the Qualcomm snapdragon SOCs.  Even with the x86-CPUs over time they had more performance gains while maintaining almost the same power consumption.  Not anymore, but they could catch up with better chip design?",Neutral
Intel,>also why is everything in a lake. like really that's the last place you want your chips to be.  Easier to cool them,Negative
Intel,"Stop trying to find meaning in the naming scheme. Patience grasshopper, all will be revealed in time.",Neutral
Intel,Like how is every chip company so bad in naming stuff?  Intel names things from geographical stuff to avoid politics etc. So I get the lakes  The numbers I dont.,Negative
Intel,"> APUs that will follow 'Strix Halo' will be called 'Strix Point'.  What? No, that's not what any of these terms mean. The Strix lineup is already out. ""Strix Halo"" is literally their halo, big compute/iGPU product.  PTL will compete with normal Strix Point and its refresh next year.",Neutral
Intel,Really? This should be good for Intel in 2026.,Positive
Intel,These Halo iGPUs are meant for LLM first and foremost. If you just want to game just get a normal RTX laptop.,Neutral
Intel,They are like 20% more efficient.  You are talking about 200% more efficient if it hits the same performance.    They might as well make it a mobile chip if that is how good it is. Would be great on phones.  Would be better than most pcs right now on 5W envolope.,Positive
Intel,"It's intentional, and non specific. So most go by the 3/5/7/9 naming. In order for them to have max profit, they need to lead you to the chips they want you to buy.",Neutral
Intel,'Strix Halo' is a product line as 'Gorgon Point' is also/another product line. Both have different variants.  In the case of 'Strix Halo'  [Check](https://hothardware.com/news/amd-strix-halo-cpu-rumors)  Find the official lineup here  [AMD](https://www.amd.com/content/dam/amd/en/documents/partner-hub/ryzen/ryzen-consumer-master-quick-reference-competitive.pdf)  Edit: changed 'Strix Point' to 'Gorgon Point' since that what I meant.,Neutral
Intel,👍,Neutral
Intel,"Yes, Strix Point lives *alongside* Strix Halo. Strix Point is already out, and came out *before* Strix Halo at that. It is not the followup to Strix Halo.",Neutral
Intel,You are right. I meant 'Gorgon Point'.,Neutral
Intel,"Gorgon point is just a refresh of strix point. These terms mean less in the next gen anyway as the follow on products have quite different names. Medusa point isn't really the successor to gorgon point, for example.",Neutral
Intel,My brain hurts and I’m still confused,Negative
Intel,That's why you have guys that are completely ChatGPT-levels of confident when they are completely out of whack with the info lol. No worries.,Negative
Intel,"I think him saying ""unreleased products"" could mean it's still coming.",Neutral
Intel,I think B770 is locked in and to release shortly isn't it? sure I saw a leak of packaging details etc.,Neutral
Intel,"We finally got reviews for the B50 and it is a SFF gem! That said… I wish Intel would get better at promoting upcoming products, they seem to be slowed down by the restructuring.  I’m going with the flow, whatever will be will be. I’m waiting for Dividends to kick back in so I can retire with Intel.",Positive
Intel,"Boy, I am glad they picked an easy naming convention.   /s  After Xe, Xe1, Xe2 no doubt next would be Xe3, then Xe3p  After A-series, B-series, no doubt next would be C-series.    Too bad those generations do not line up",Positive
Intel,Intel is not serious with dGPUs,Neutral
Intel,"If it was coming, you expect them to talk about the future even a little bit. Instead, nothing.    They cancelled Celestial over a year ago now. Sounds like things haven't improved since.",Negative
Intel,"I think him saying he doesn't talk about unreleased products is 100% bullshit. He talks about upcoming tech constantly. This is not just unreleased, it's unannounced, unclaimed and nonexistent outside of pure speculation.",Negative
Intel,B50 is interesting once the software is there (planned Q4).,Positive
Intel,"Yes, they are literally just playing. It's all a game lol",Neutral
Intel,"They have released about the same number recently as AMD, I'd say they're pretty serious. The B50 is a pretty compelling product too, big features for the price.",Positive
Intel,They are as serious as AMD.,Neutral
Intel,This interview happened during the quiet period so I don't think he could talk about the future,Neutral
Intel,"They cancelled Celestial over a year ago now. Sounds like things haven't improved since.   MLID people showing themselves.  Tom Peterson previously said Celestial discrete hardware was already done and they were working on Druid. So if they cancelled it, it must have really sucked.  I am sure after they release Celestial 70 series beater, MLID will come and say they cancelled the 90 series beater and it's for sure dead from now on!",Negative
Intel,Where did you hear that it was cancelled?,Neutral
Intel,"We are supposed to get B60s also but they will likely be very limited and part of Battlematrix. Intel is moving very slow with Pro and Consumer GPUs and they can’t rely on TSMC for supply and obviously are not ready to manufacture through IFS? We are getting left in the dark, all we can do is wait.",Negative
Intel,There has been no update regarding celestial dGPUs internally.,Negative
Intel,AMD is skipping a generation to focus on the next. Intel has lost its focus on GPUs. These are not the same things.,Negative
Intel,Lol no. ARC was 0 margin product. Now it's fate depends on the whims of VPs not engineers.,Negative
Intel,"You could likewise point not that there was no word about dGPUs in the PTL presentation either. I think people need to accept that it's just not happening, at least for the foreseeable future.",Negative
Intel,"> MLID people showing themselves.  I'm not getting this from MLID.  > Tom Peterson previously said Celestial discrete hardware was already done  No, that's absolutely false. Actually watch the interview instead of reading reddit comments. He said Xe3 (specifically in PTL), not Celestial, was done. And this was after the PTL tapeout was announced, so that didn't even tell us anything new.   And as we now know, they don't consider that even in the same family as what would be Celestial.",Negative
Intel,Ex-Intel coworkers/acquaintances.,Neutral
Intel,>There has been no update regarding celestial dGPUs internally.  Do you have internal information?,Negative
Intel,Why would they? Battlemage is not even finished. Battlemage is not even 1 year old yet. They will still release B7XX gpus and probably B3XX.  I expect them to tallk about celestial by next year.,Negative
Intel,> Intel has lost its focus on GPUs  So despite them repeatedly telling you they have not... they have?,Neutral
Intel,"Battle mage is not a 0 margin product...  I know how much silicon cost etc due to my job. Believe me there is at least %30 gross margin in Battlemage and that is assuming somehow Intel got a worse price compared to my small ass company.     It's not profitable due to amount of R&D it takes to develop it, Intel earns a significant chunk for each Battlemage sold. They are simply not as greedy as Nvidia and AMD to earn market share.",Negative
Intel,Dude XE3 even has some test shipment reports etc. It's too late to cancel.   Sure if it's not good maybe we will only see B580 replacement.    But it's literally impossible and stupid to cancel it right now. Especially given how much gross profit they made from B580,Negative
Intel,"So, no news story has come out stating that?",Neutral
Intel,Xe3P-HPM suggests otherwise,Neutral
Intel,"Yes, through my ex colleagues",Neutral
Intel,"No, I don't listen to them, they have a nasty habit of downplaying bad situations. I'm going by their actions.",Negative
Intel,"> Dude XE3 even has some test shipment reports etc  Celestial wasn't base Xe3, and didn't tape out before cancellation. What test shipments are you referring to? PTL?  Btw, they still aren't saying anything about BMG G31, and that was much further along than Celestial was.   > But it's literally impossible and stupid to cancel it right now.  You can cancel a product at any point before it's released. Anything else would be sunk cost fallacy. Surely you're aware of the massive budget cuts and layoffs they've announced. Not everything can survive.   > Especially given how much gross profit they made from B580  By all reports, BMG still wasn't profitable for them. Hell, even if it *was* profitable, doesn't mean profitable *enough* for Intel to keep funding it in this environment. They're prioritizing spending reduction, not profit maximization.",Negative
Intel,Why would Intel tell you? It would just stall selling all current ARC cards.,Neutral
Intel,No. Or at least not from any reliable source. Obviously discounting MLID and his ilk.,Negative
Intel,What about it? That some reference exists in drivers?,Neutral
Intel,"You don't even know that?   Lip Bu has been hiring gpu designers not firing them.   Most of the Cuts are from foundry side and slightly from gaudi side.    Intel if anything is focusing on gpus to create AI inference gpus.    Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.      You should check your sources, Lip Bu would cut 14A before he cuts Inference gpu side.",Neutral
Intel,BMG was 0 margin product,Neutral
Intel,"Of which are recently implemented, as in 'in the past week' which would be exceptionally stupid to do for a cancelled project",Negative
Intel,"> You don't even know that?  What do you claim I do not know?  > Lip Bu has been hiring gpu designers not firing them.  Celestial was cancelled under Gelsinger, as well as several rounds of client GPU layoffs. If Lip Bu is hiring anyone, it's not to build the team back up again.   > Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.   I am specifically talking about client graphics, yes. There's shared work, and arguably should have been more, but they were quite different. Client iGPU, client dGPU, and server dGPU were basically all separate SoC designs.",Negative
Intel,They're also using Xe3p for NVL-P and that Island AI product.,Neutral
Intel,"It wasn't though?  As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  I don't believe there is nothing Intel can do convince you. I heard these news every single time.     Also just look at job listings, there is many for gpu development.",Negative
Intel,"Which means it is being used, produced, and cannot be disqualified yet, nor does anything, not one trustworthy source, show it is cancelled",Negative
Intel,"> As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  Go ahead and point out where *I* said ACM or BMG were cancelled. I have no idea who ""you guys"" are, nor do I care what others are or are not saying.  > I don't believe there is nothing Intel can do convince you.  Well yes, if they cancel a project, and I know they have, I'll say they cancelled it. The way Intel can convince me to say otherwise is by... not cancelling projects. And you *do* realize they haven't talked about client dGPUs past BMG in many, many months, right? It's not like Intel's really denying anything.  I'm not sure what you're looking for here. Should I lie and pretend not to know what I do? To what end? It's not like Intel's harmed by me saying they cancelled such a project. That's not something you can keep a secret indefinitely, and anyone who *really* cares already knows.  > Also just look at job listings, there is many for gpu development.  If you lay off 10 people, 5 more leave on their own, and then you backfill 4, that's still a net loss. They still need some people, but not as many as they had, and not for client dGPU.",Negative
Intel,"Intel have not claimed there to be upcoming Celestial, or more Battlemage or anything like it. How did their silence convince you of anything?",Negative
Intel,Actually after the Nvidia deal there's definite reason for Intel to cancel future ARC development.  They have a partner that makes better GPU than them. Why would they continue? If the Nvidia deal is successful I expect even their iGPU will disappear.  In no sense it makes sense to develop a line that's redundant with what your partner is doing.,Negative
Intel,"> Which means it is being used, produced, and cannot be disqualified yet   Again, Xe3p lives for various things. Celestial, the client dGPU planned to be based on that IP, is dead. I'm not sure what contradiction you think exists here.    > nor does anything, not one trustworthy source, show it is cancelled   Intel's refusal to talk about future gen client dGPUs and the mass layoffs in the hardware team don't tell you anything? Or what about Gelsinger's remarks about less focus on dGPU?    This is how Intel usually handles cancellations btw. They simply pretend it never existed, unless investors really demand to know. Even then they like to delay the acknowledgement.",Negative
Intel,Think pretty easy naming. Any X infront just automatically means better iGPU,Positive
Intel,"Looks pretty snappy at \~75% faster than the top-end LNL chipset and this is with pre-release drivers.  Granted TDP is probably higher.  If they get the drivers cleaned up then it might release a bit higher which makes it a viable (though still materially weaker) thin and light XX50 dGPU model alternative some some of the market.  Should do well addressing the 'I want a thin and light laptop, but I want it to have an ok GPU' crowd.",Neutral
Intel,"Panther Lake 12 Xe3 performance looks great to match RTX 3050 laptop performance because the entire chip only draw half power of RTX 3050. Seems like using 18A BPD really paid of to reduce CPU consumption by a lot with the helps of Tsmc N3E for iGPU.    Also it's so weird to see Asus Zephyrus G14 with Intel chip, usually it always has Amd CPU paired with Nvidia GPU. I heard G14 is pretty popular gaming laptop but this laptop during full load can use 120w+ power.    Using Panther Lake 12 Xe3 will makes this laptop looks even more appealing because it reduces power requirements from 120w+ to 45w but still giving about the same GPU performance which is insane. This is a massive game changer for people who use laptop as portable gaming machine and to those who travel a lot. I can totally understand why Asus this time use Panther Lake for G14.",Positive
Intel,"Nice to see the G14 with an Intel CPU. Thought the lineup was AMD only tbh, while the larger G16 laptops get Intel.",Positive
Intel,Naming for these chips are terrible,Negative
Intel,"Can Asus send me this laptop for review? I have 14 followers on snoozetube and 60% are probably bots, but bots are human too?",Neutral
Intel,Can't wait for 14inch laptops with actually good battery life and convenience than the cheap gaming laptops it's going to kill,Positive
Intel,"Nah, should've kept that info at the end like every other Intel and AMD CPU ever made.  But otherwise this branding really feels like AMDs APU line, where they had to emphasize their iGPU was better than average.",Neutral
Intel,"The possibility of being nearly 100% faster than Lunar Lake in some tasks, and minimum possibly 50% faster while being able to fit it into a sub 3lb/1.5kg design with a 80+wH battery is going to really nice. If the 4 LP-e cores scheduling work well and maybe a more efficient OLED panel you could easily get true 24 hrs use on x86",Positive
Intel,"If the game could be 50–60% stronger, that would be That would be a killer",Neutral
Intel,Its GPU part isn’t 18A at all — it’s actually N3E and 4Xe3 integrated graphics use Intel 3.,Neutral
Intel,"That’s been true for the past generations, but it looks like it will change this generation",Neutral
Intel,Still better than Ryzen 365 AI pro MAX+,Positive
Intel,"I disagree, GPU focused = X (like Xe3). Just takes getting used to , but otherwise it follows the same 3 7 9 scheme that probably didn't make much sense at first either :)",Neutral
Intel,https://browser.geekbench.com/v6/compute/compare/5050048?baseline=4771132,Neutral
Intel,The typical consumer doesn't know anything about the last letter. Having it in front will be much more successful to communicate to consumers the difference.,Neutral
Intel,Yea putting ai the model name is disgusting 😂,Neutral
Intel,I can't wait for the Ryzen 688S AI Pro MAX+++,Positive
Intel,"I agree with this and now snoozetube creators are doing 128gb reviews for the 365 AI Pro Max+ and glossing over the fact that it costs decent money but lacks any kind of power when compared to discrete GPUs.  Amd continues to pump out expensive APUs that are mediocre, while doing everything related to Radeon half heartedly.  Why is that?",Negative
Intel,"Nerds argue over names for tech products but will eventually figure out some kind of logic in why they named it that way. Entire generations need to be released and compared.   As for average users they will always be perpetually clueless and unfortunately will become influenced by an influencer with no integrity and or a store associate who has been trained on scripts that make the most money for the store.  God help us all, I pray for Jesus - just like Pat Gelsinger, who will get no credit for the Intel turn around.",Negative
Intel,Ryzen metaverse Ai max++ 3D Hypercache macroboost,Neutral
Intel,I think it's rather on point. the 395+ is a beast for running large MoE AI models. It's value for money in that respect is almost unbeatable.,Positive
Intel,you forgot the x3dx2    when both cpu tiles are stacked on 3d cache tiles.,Neutral
Intel,Ultra TypeR S-line AMG M Bi-Turbo CCXR LM Harley Davidson Edition,Neutral
Intel,"Very very few know anything more than that, usually completely unaware that there's a whole SKU number after that.  How many times do you hear stories about some user proudly boasting about having an i7, only to find out that it's like a 6th gen, and they don't even realize / believe that something like a i3-12100 is actually a better CPU.     The average user understands the difference between, say, a Core Ultra 5 and 7, because the ideal of 3, 5, 7, 9 being product tiers exist in plenty of industries, like BMW's product line. Bigger number = more performance. How? By how much? No clue to them.  So since the average user is going based off just the name 5, 7, or 9, having that X visible in a location they'll see is certainly very important. They'll notice the X.",Neutral
Intel,"It would be like a Chromebook named ""Chromebook CloudCompute+"" just because that's what those are built for",Neutral
Intel,I personally prefer them Name it Ryzen 3 / 5 / 7 / 9. It’s easy to understand and easy to compare to intels naming but sadly both companies have ruined it now.,Negative
Intel,That's honestly sounds even more cringe. Can you imagine Amd Ryzen 9 395X3DX2 AI Pro Max+? That's ridiculously bad LMAO,Negative
Intel,BMW Individual M760i xDrive Model V12 Excellence THE NEXT 100 YEARS,Neutral
Intel,I actually did my research and found out that core ultra 5 125u is not much different from core ultra 7 155u... Ended up buying ProBook with core ultra 5 125u and saved money for upgrading the ram and SSD,Neutral
Intel,"but i think its gonna happen aye,  i wonder if the RAM bandwidth needs for AI benefit from cache like games do, or are they better slapping more ram channels on it...",Neutral
Intel,"Yeah, all of the U chips within a generation are the same physical chip, just different bins (usually tiny clockspeed differences). I don't think they even have core count differences any more for the most part.",Neutral
Intel,Same core counts too,Neutral
Intel,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Neutral
Intel,"To summarize from the article for some folks:  6300 puts this GPU as 33.4% faster than the 140V, 71.3% faster than the 890M, and 54.3% of the 8060S. Just over half of Strix Halo's top config.  Bear in mind though, that this benchmark favors ARC GPUs compared to gaming results. The 140V and 890M are roughly equal and this benchmark puts the 140V as 28.4% faster.",Neutral
Intel,"Ugh, who is naming these products?!?!? Between the internal code names (which are now publicly used) and the actual product names, it's a mess. As bad as monitor naming.",Negative
Intel,"Sick, Panther actually sounds good, and Lunar/Arrow on mobile already sounded good to me. Please keep pumping those iGPU numbers Intel Arc engineer bros.  AutoCAD on integrated Intel graphics WHOOOOOO  AMD step it up next gen please, Intel is no longer a static target in graphics, kthx.",Positive
Intel,This sound more realistic and not as good as the 50% better Intel announced.   For Intels sake I am hoping the 50% is real,Negative
Intel,... with early drivers and probably no decent support yet. Its also probably an engineering sample? 😉,Negative
Intel,Wow pantherlake is looking enticing. Should I wait for novalake? Will novalake have same battery life like lunar lake or pantherlake?,Positive
Intel,"It's going to cost like $10,000, right?",Neutral
Intel,"The top lunar lake has 8Xe2 cores, and this has 12Xe3 cores. 30% faster seems… bad? Like why not 50%+ given 50% more cores and architectural improvements?",Negative
Intel,"We have seen how Lunar Lake on MSI Claw 8 AI+ performs on early firmware and drivers, Intel even managed to improve Lunar Lake performance up to 30% after that. I expect 50% performance improvement on Panther Lake is very possible.",Positive
Intel,seee ceerto seeeh,Neutral
Intel,"It really is a mess, someone should've been fired long ago.",Negative
Intel,"The only thing they want you to care about is 3/5/7/9.  If the naming is too clear, then they can't steer you to the higher margin/profit variants. At some level it needs to be unclear. Not defending them, but how things are.",Negative
Intel,Intel wasn't a static target since the 10th gen. They've been pushing (at least in terms of performance) since then. It's mostly the efficiency which stepped back. And they messed up their current lineup in terms of performance and pricing but they've improved for sure.  Next gen seems good for them though. Hope they price it well. AMD isn't zen 1/2 either. They're bumping core numbers too and introducing some new cores as well.,Neutral
Intel,"I mean, why?  Is AMD going to be coming out with anything more powerful than the HX 370 in that power range?",Neutral
Intel,because 50% more cores need 50% more power for 50% more performance.,Neutral
Intel,"There are so many variables here that we have no way of accurately saying that.  It's possible that Xe3 is simply treated more like RDNA in this benchmark, and that the Xe2 140V is unfairly biased towards in that way. If that is the case, the 890M and 8060S may be better metrics to base off, and we do see over 50% gains for a 50% wider GPU than the 890M.  It's also possible this is a power-limited scenario. Like for like on TDP, this would be a solid improvement given the GPU both has to be moved off-tile compared to Lunar Lake, and has twice as many CPU cores to fight for power.  This could be pre-release drivers not getting the true 100% out of the hardware or silicon with non-final clocks still being tuned. The point is there's no way to know for sure.",Neutral
Intel,I don’t understand why you people continue to use that MSI claw to demonstrate “intel improved lunar lake performance post launch” when there are millions of lunar lake laptops that never had performance issue of that one handheld.,Negative
Intel,"they're keeping their jobs because this is technically better than the past. back in the day you'd have model numbers exclusive to a retail store, much less specific to an OEM, because businesses wanted to feel like they got a bespoke deal.   These days the SKU naming is mostly for accounting purposes, while the ""real"" naming decisions are made by the OEMs. basically most people are buying the Thinkpad/Yoga/ROG ""brand"" rather than the specific processor model, which only a much smaller crowd bothers to comprehend. It's like how people buy the Steamdeck rather than whatever APU is in there, which is fairly old at this point.",Neutral
Intel,"Energy efficiency is huge in mobile but I was explicit in saying they were no longer a static target in graphics, where they were not making significant or impressive gains in iGPUs for many years, it seemed. Now they are making one of the best iGPUs on the market.",Positive
Intel,"Possibly yes.   Current AMD handheld chips are better than Lunar Lake performance. Assuming normal cadence. Next generation would be similar performance to Panther Lake.     I was hoping more of a leap frog, rather than similar performance. %50 would be a very clear edge.",Positive
Intel,Is the TDP fixed to be the same between LL and PTL in this test?  I genuinely don't know.,Neutral
Intel,But LNL's TDP is too low compared to H45 cpu,Neutral
Intel,"> because 50% more cores need 50% more power for 50% more performance.  insightful, and easy to forget given feature bragging    :)",Positive
Intel,FTFY: because 50% more cores need 50% more power for 35-40% more performance.  Cause it don't scale linearly.,Neutral
Intel,"Mostly because people with LNL laptops are less likely to games since gaming are not the point of those laptops -> less testing, whereas the Claw 8 AI+ is a gaming PC handheld, so it is mostly use for gaming purpose and thus have more people testing for its performance.",Neutral
Intel,"As weird as it sounds actually Lunar Lake improvement mainly comes from MSI Claw not laptop, that because majority people who use MSI Claw give feedback the most which is why Intel focusing on the handheld first then laptop.    Intel even use Claw as benchmark for Lunar Lake compared to laptop. You can read from this article :  https://arstechnica.com/gadgets/2025/04/intel-says-its-rolling-out-laptop-gpu-drivers-with-10-to-25-better-performance/   Also Claw got BIOS update way faster than any laptop with the same chip so it helps Intel to mitigate power and boost behavior to maximize the performance. There is so many bug reports on Arc forum, most of them are Claw users, that's why laptop got benefits too.",Positive
Intel,AMD isn’t coming out with a better next gen iGPU for mobile (Gorgon Point) since it’s a simple refresh. Same arch with same CU count based on rumors.  The generation after will be competing with Nova Lake,Negative
Intel,"Amd handheld with Z2E isn't better than Intel Lunar Lake, you can see the comparison on MSI Claw sub or even on youtube. Z2E in most game is 10% slower than Core Ultra 7 258V, it only won in the game where Intel GPU performs bad.    Not to mention at 17w Z2E losing badly to 258V, Intel is on their own league, it's not even competition for Amd.",Negative
Intel,Intel is gonna have better integrated graphics than AMD,Positive
Intel,I have no idea.,Neutral
Intel,"We don't know what TDP was run here, and both MTL amd ARL H have been 28W outside of the Ultra 9 SKUs. It's entirely possible this was run at 28W, which is also in reach of Lunar Lake's boost envelope.",Neutral
Intel,The lunar lake laptops are tested by reviewers all the same as the strix point ones. Regardless it’s misleading because it’s not “lunar lake” but rather the performance profile/boost behaviour of that specific MSI CLAW that was changed and people act like it’s lunar lake’s drivers doing “30% magic”.   It’s not.,Negative
Intel,"And is there documented so-called “large” performance improvements on LNL systems that already performed as expected on day one (it tied 890m on high power and was always better at low power, talking about real games not 3dmark)? Or was it only bringing the Claw back to where LNL should always perform?",Neutral
Intel,"True. I don't see how the next igpu from Amd going to use rdna 4, it won't even support fsr 4. Meanwhile Intel going to push their igpu tech even further with XeSS XMX 3 with Xe3 and Xe3P, they will be way ahead of Amd in igpu market especially when Intel Lunar Lake already beating Amd Strix Point and Z2E.   Intel also dominating mobile market. Honestly it's not looking good for Amd.",Negative
Intel,Yeah people act like strix point is in that segment..... It's not.,Negative
Intel,🫨,Neutral
Intel,Wouldn't be the first time.,Neutral
Intel,"Oh, I guess it be like that.",Neutral
AMD,Has AMD made any RDNA 4 GPUs for laptop?,Neutral
AMD,"AMD has to actually ship product (and support it) for OEM to use them.  There are other considerations, yes, but multiple manufacturers have publicly stated that AMD just doesn’t have enough available for them to warrant making more than a few models.  Nvidia on the other hand has more than enough capacity to guarantee deals so they can easily just stick with them for an entire product stack and simplify procurement.  (For dGPU products. Future of Nvidia’s business direction withstanding at the moment.)",Neutral
AMD,I imagine AMD don't commit the volume necessary to supply the OEMs   Look at the supply issues on the 9070/XT for most of the year. And the DIY market is small,Negative
AMD,"1. Nvidia has brand recognition and simply sells better.  2. Nvidia can guarantee OEMs as much supply as they can move. AMD discrete GPUs for laptops are as good as vapourware.  3. Nvidia GPUs are more efficient and that really matters in a laptop. AMD GPUs really struggle with idle power draw especially, so even before you start to run anything on them, you’re on the back foot  4. Nvidia has their entire laptop stack (with the exception of a couple of SKUs) available at launch. AMD drip feeds its launches so the hype doesn’t remain.  5. A massive proportion of gaming laptop buyers buy them to do work. Almost none of that works on AMD hardware. Their GPUs are straight up not supported in V-Ray and Corona. They absolutely suck in Blender. A 7900 XTX chugging 350W gets is arse handed to it by a 14 inch MacBook running on battery for example. Nvidia GPUs are better for video editing, especially with the new NVDEC of 50 series.   6. The value argument doesn’t hold for AMD laptop hardware. They have worse features and don’t tend to cost much less. And they all have the same VRAM anyway so that is also not a selling point.  TLDR: Because they are worse products and there is more nuance to the laptop market than there is to the DIY gaming desktop market.",Positive
AMD,"It's more likely due to AMD.    They simply don't ship that much volume and unlike nvidia who is constantly pushing volume of desktop GPUs, AMD seem to prefer the much more profitable AI/datacenter market.   Why integrate a worse, less efficient GPU at almost the same price?",Negative
AMD,"AMD isn't able to guarantee the same amount of supply as Nvidia (same situation with CPUs and Intel, although that's been improving)  And AMD has also been behind in terms of performance/watt since at *least* 2014, which is the most important metric by far when thinking about laptop GPUs.",Negative
AMD,Amd never released newer lineup this gen thats why.,Negative
AMD,Hard to ship something that doesn't exist,Negative
AMD,"They don't. AMD just hate the market for some reason and have never been good at supplying it.  Remember that a lot of laptop brands are largely made by a relatively small number of ODMs. Producing a modern laptop mainboard with discrete graphics is where an ODM leans heavily on their knowledge and relationship with Intel and Nvidia for circuit designs etc. Both companies are very focused on mobile and working with these ODMs. Both have a large lineup of products specifically engineered for mobile, AMD... not so much.  AMD hasn't been focused on mobile for a long time and ODMs have complained about the lack of relationship with the company, being left to figure stuff out themselves etc as far as putting together a mainboard featuring AMD products.  This generation they also just plain do not have the competitive products, there is no RDNA4 mobile chip which is strange. In the past there has also been complaints about inadequate supply of mobile dGPUs.",Negative
AMD,Power efficiency is everything in a laptop and AMD GPUs require a higher power draw to get a similar performance.,Neutral
AMD,"Likely more profitable to go with NVIDIA, whether that is in better prices on components from NVIDIA, or a lack of supply from AMD, or simply better brand recognition that attracts customers.",Positive
AMD,"1. AMD only has enough current mobile dgpu skus to count on 2 hands at most 2. AMD doesn't commit to actually shipping high volume of mobile parts 3. AMD tries to force ""AMD advantage"" with various hardware requirements that makes it a pain in the ass for OEMs to ship either cheap base models or use Intel cpus as an alternative if there is a shortage of AMD APUs.  No sane OEM is going to sell amd dgpus when they have min spec requirements for CPU, Memory, Storage, Screen, and wifi/bt.",Negative
AMD,"All these choices are done via the GPU maker’s business development team going and getting it done by selling into the laptop OEMs. The BD team is enabled by leadership committing manufacturing resources. It’s not like the laptop OEMs just go decide what to buy and put in the systems. I could also see the laptop OEMs going to the silicon companies and sharing “we have consumer demand for an AMD GPU, can you please make some available to us?”  In short, AMD either hasn’t made it a priority or they don’t have the capability. Or consumer demand doesn’t exist such that either party makes it happen.",Neutral
AMD,Because Nvidia makes 95% of all GPU products… so laptop makers need to be ensured that mass product will be available. AMD likely doesn’t have the capacity that some SI’s are looking for.,Neutral
AMD,"Because they are not good and AMD is well know for making half ass bios and then never fixing it, especially on the GPU side of the house.   For example we are all still waiting for AMD to release the Linux nvme raid driver for the built in raid, for ALL AMD CPU platforms. Been what? 10 - 12 years now?",Negative
AMD,"its the other way around, AMD only pretends to make mobile GPUs so they can lie to investors or something  seriously they have zero volume, idk why they even bother with it as a product stack. must have a government contract or something",Negative
AMD,"The whole concept of the APU, AMD, and Intel to a lesser degree, are choosing to beef the iGPU in their APU to compete in the low and medium performance gaming laptops against the Nvidia dGPUs.  I personally have the theory that the handheld PCs were a proof of concept of how far AMD and Intel can take their APUs performance wise vs Nvidia discrete solutions in the mobile market.   AMD probably are thinking that if a single chip in the form of an APU can deliver a similar performance to two chips (CPU+dGPU) for 50-80% of the power budget, then they can corner the low and part of the medium performance gaming laptops market, after all you don't see a Nvidia dGPU in any of the current or future X86 handhelds, way too power hungry and ARM alternatives like the Tegra family fall considerably short, not to mention the still messy compability layer to run X86 games on ARM systems.",Neutral
AMD,Supply Issues is most likely   \#1 Reason  \#2 They don't sale.   Its the perfect combo of why bother. R&D isn't cheap you need to recoup cost at the bare minimal for what you put into the device being sold. Companies aren't going to do each other any favors anymore then they do a consumer a favor.,Negative
AMD,"AMD doesn't have a mobile GPU lineup that has matched NVIDIA's for a while now (at least in the last 11 years), and what doesn't help is the efficiency at idle/low loads isn't great either which isn't a big deal in a desktop but much more important in a laptop, as you don't want heat to stick around in the laptop shell.  Also whilst the Nvidia mobile gpu shennagians weren't great with naming, they are generally more power efficient and performance ""per watt"" is better , especially if you are pairing it with a high end cpu or a thin laptop shell.",Negative
AMD,"AMD makes limited laptop level GPUs. Additionally, for laptops the market is a bit more split. If people want a gaming laptop, they generally really want a capable machine, and Nvidia has the more significant brand recognition there. If they don’t want that, they generally explicitly don’t want a dGPU. So as an OEM, it’s hard to justify the volume necessary to support an AMD SKU rather than just consolidate around an Nvidia line",Neutral
AMD,"I think that the APUs are pretty good for laptops, the RX6600M didn't really take off, I haven't seen any laptop with 7600. If they don't come with a 9060 non-XT for laptop now, then they never will.",Neutral
AMD,"Wondering the same thing, and I'd love to see an explanation.",Neutral
AMD,"The key reason is just that Nvidia sells better. For an integrated product such as a laptop its very expensive to give consumers the choice, only for 90% of them to go with nvidia.   The technical aspects are not super important, because AMD did price some of their laptop GPUs aggressively to offset a hypothetical inferiority and almost nobody adopted it other than minisforum I think.   Its not a god situation for the market, its really nice to see framework making it flexible, but for many OEMs it seems just not worth it. The nvidia + Intel bundle situation might make it even worse for AMD in the laptop space in the future...",Neutral
AMD,"As an oem manufacturer, why would I want to carry 2 sets of inventory?",Negative
AMD,Because AMD can't supply them realibly in any meaningful way that the manufacturers can design multiple lines across,Negative
AMD,"Since GPUs come with the laptop, and are not generally replaceable, it usually comes down to a matter of ""can you promise X sales at Y cost"" and NVIDIA being a fuckin trillion dollar company likely has the ability to offer much more competitive pricing. This is just one factor though as I'm sure there are many.",Neutral
AMD,"When it comes to laptops AMD dips a toe in, doesn't get enough of a bite and then takes its toe out, in and out over and over. The laptop industry wants long term deals where it can design a series and just upgrade the internals when manufacturer releases new variants, the in and out is costing them a lot in design and so very few companies do AMD in laptops and when they do they often have niggling issues.",Neutral
AMD,Costco Canada uses laptops with Ryzen/Radeon in most of what they carry.,Neutral
AMD,I've had a laptop with a 6800s.  It received one driver update. ONE.,Neutral
AMD,"Because they probably still get kickbacks from Intel, or other ""incentives"".",Neutral
AMD,"Lisa ""we're going for marketshare this gen by abandoning mobile, low end & high end""",Neutral
AMD,"AMD historically hasn't been able to supply these manufacturers consistently enough, they don't trust AMD.",Negative
AMD,Cause they like money and amd doesnt make them money,Negative
AMD,because market share is leverage and OEMs don't want to risk their nvidia partnership,Neutral
AMD,"You already got your answer from other users here, but AMD and Intel seem to not even really be trying to unseat Nvidia dominance in the laptop dGPU space anymore. Their currently strategy is to try and undermine it by beefing up iGPUs to the point where some more budget oriented people may question if they even need to step up to a x50 or x60 class dGPU.  AMD's first crack at this with Strix Halo may have been a flop, but the strategy is sound. Intel's releasing their X line of PTL chips in a few weeks. I'm sure AMD is gonna double down next gen with this plan as well.",Neutral
AMD,"Thats why I bought a Framework 16.  To have a Radeon GPU cuz Nvidia is utter dogshit under Linux, especially since most laptops don't use a fuckin MUX switch.",Negative
AMD,"educated guess: a laptop with a Ryzen CPU and a Radeon gpu, what a wonderful reason to scream ""monopoly""! ☺️",Neutral
AMD,I have a legion with a Ryzen 9 and a Radeon gpu..,Neutral
AMD,More like why AMD hates laptop manufacturers.  AMD needs to release mobile GPUs first and deliver them in necessary quantities generation after generation. Also they need to provide them across different pricing brackets.,Negative
AMD,"They don't hate them, AMD GPUs simply SUCKS for what Laptop requires most  \#1 Reason: EFFICIENCY, Nvidia destroys AMD gpus in terms of efficiency, not even close, and efficiency is everything in laptop, even 10W higher could become a problem. Nvidia just draws so much less power at the same performance level for many generations already ever since the first RTX. The 5700XT draws the same power as 2080ti while being way slower and has no feature, 7900XTX consume more power than 4090 while being destroyed in everything, even newest 9070XT consumes about 100W higher than 5070Ti even though it's still slightly worse, this might not be relevant in PC at all, but for Laptop this is super crucial, and AMD completely failed at this. Not to mention idle power draw is a problem for AMD gpu.     \#2: Laptops are always meant for WORK and PRODUCTIVITY, not just gaming. If you want gaming only just build pc or get a console, but laptops have to be able to do as many productivity tasks as possible, that's what laptop is made for, a portable pc for work, even gaming laptop is made so that people can work and game on them, and AMD failed this aspect as well, Nvidia not only offer far better feature sets, but also simply work way better and compatible with everything, never need to workaround or tinkering, everything works with Nvidia gpu, and this is massive for laptop     These are the 2 biggest reasons, it's never about stocks or lack of volume that some clueless comments here suggest. That's why AMD thrives with Console instead, because consoles don't need neither of the reasons I mention above, console doesn't care about efficiency since they always plugged in, and console is for gaming only, never for anything else, so AMD has no problem fulfilling the massive volume of console Gpus, because for console use case AMD gpus make sense, unlike laptop",Negative
AMD,Probably the same reason all the laptops are intel despite them getting clowned on. It's because their agreement says so,Neutral
AMD,Probably the same reason all the laptops are intel despite them getting clowned on. It's because their agreement says so,Neutral
AMD,NVIDIA optimus and better efficiency,Positive
AMD,Simple numbers game.  Nvidia - 92% market share   AMD - 7% market share   Intel - 1% market share    Which one are you going to go with if your goal is to sell the largest number of gaming laptops possible?,Neutral
AMD,"I actually saw this talked about somewhere. I think the people said the manufacturers have found that there is just huge demand for nvidia gpus from consumers. Dlss, framegen etc and the fact that they deliver very good performance with reasonably small power, it is a no brainer for them. Having an nvidia gpu in the laptop does free advertisement for them.",Positive
AMD,"Why do most people buy Nvidia?  OEM's follow what sells, Nvidia sell units. Just look at all the posts we still see asking if AMD GPU's will burn up, the number of times a AMD GPU was lower cost but OP ends up going Nvidia as it's the brand that everyone buys.  It's just sales, Nvidia moves laptops.",Neutral
AMD,"Why do orange basket manufacturers hate bananas?   I noticed that, while looking for a new purse.",Negative
AMD,because intel still lobbying  one exemple: [https://www.reddit.com/r/Amd/comments/16q44ar/eu\_fines\_intel\_400\_million\_for\_blocking\_amds/](https://www.reddit.com/r/Amd/comments/16q44ar/eu_fines_intel_400_million_for_blocking_amds/),Neutral
AMD,I know it’s bs. Yet virtually every handheld pc maker uses AMD 🤦🏻‍♂️,Negative
AMD,"They don't make a dGPU for laptops, just iGPU. Though their iGPU is pretty good for most tasks besides gaming. AMD probably thinks if you wanna game, buy a gaming system like a PS5 or Steam Deck which hosts an AMD chip.",Neutral
AMD,I have a last gen asus a16 advantage edition with a 7700s gpu. Rougly 4060M performance minus the nvidia rtx/ftame gen which i never use anyways,Neutral
AMD,"It's old practices, even when AMD started dominating in CPUs you look at the market place and it's NVidia/Intel everywhere... That eventually changed due to the lunacy of not having the best CPUs ""because reasons""... The GPU side of things is going to be very hard as the stigma that AMD is a budget brand is what drives these companies to avoid good AMD GPUs as they fear the budget stigma will rub off on their premium products.",Negative
AMD,"They used to run too hot for laptops and nvidia drivers used to be the more reliable ones  Companies are very reluctant to change  This is what Steve Jobs noticed in hardware manufacturing when it came to knowing/not knowing how many precisely how many units were produced per unit of time, among other problems and common practices. The answer to everything was “well because we’ve always done things this way”  With Ryzen, there was a clear reason to switch, it had better performance and lower temps, everything you’d want for a laptop  TLDR: no reason other than “it’s always been this way.” No one changes until someone changes.",Negative
AMD,"Because gaming laptops actually suck for gaming, and Nvidia has a stranglehold on the creative market (who actually buy high end laptops) with CUDA.",Neutral
AMD,"AMD sold their mobile Radeon brand to Qualcomm when they were at their lowest, hence why Adreno is just an anagram of Radeon. Because of that deal, they can't have dedicated mobile GPUs under their Radeon branding...",Negative
AMD,Nope. They wont launch a 9070M,Neutral
AMD,question answered,Neutral
AMD,"They made RDNA, RDNA2, RDNA3... almost no laptops.  Backroom deals, pressure or even manipulation from Nvidia might not be the whole story but I suspect it is going to be a significant factor...",Neutral
AMD,"Chicken and egg, AMD hasn't been shipping product because laptop manufacturers do not buy them. They're finding success in their strix halo APUs, and that's likely their future in the laptop space.",Neutral
AMD,"Yeah, this is by far the biggest issue discouraging adoption by laptop makers.",Negative
AMD,Well they supplied the entire console market pretty well and their cpu market too so their track record is actually good,Positive
AMD,"AMD doesn’t actually make anything, do they?  All their chips are made at other fabs.  That means they’re competing with everyone else for the same production capacity.",Negative
AMD,"True points.  Adding to them that Nvidia is more reliable in hardware and software for decades.  AMD drivers for RDNA used to be problematic and they had higher RMA rates. For a DIY enthusiast this is less of an issue, but for a system integrator/laptop manufacturer it means costs. Even the basic support request is a cost and you may have to service an entire system in case of a RMA, instead of just the dGPU for a DIYer. Combined with AMDs notoriously low margins, a single support phone call can mean a financial loss.  Meanwhile Nvidia is generally stable. While sometimes the drivers cause issues in specific scenarios, there was no general issue with them as their was with early RDNA.",Neutral
AMD,"7 Historically AMD's software stack had been so clownishly bad they were basically unusable. Until a couple of years ago you genuinely couldn't install the normal driver on their mGPU on many laptops, instead you'd get some branded locked down driver with missing features from the laptop maker from around the time they released the model and stuff just wouldn't run on a 2, 3, 4 year outdated driver. Sometimes they'd even have the older driver UI which hadn't been in use for literal years.  AMD being bad at software is not a meme.",Negative
AMD,"Why the heck are they always coming out with new codec’s lol  It always leaves AMD in the dust, makes it feel like Ryzen and Radeon will always be left behind when it comes to editing and streaming",Negative
AMD,> Nvidia GPUs are more efficient  This is not really true. RDNA4 is just as efficient as Nvidia. rx9070 topped GN's efficiency charts for instance. RDNA2 was also more efficient than Ampere.  AMD also has Radeon Chill which is another tool you can use for power efficiency Nvidia doesn't have.  I agree with your other points.,Positive
AMD,Who uses a laptop for blender or video editing? Literally cherry-picked use-cases to make AMD look bad.,Negative
AMD,And they barely released anything last generation either.,Negative
AMD,Efficiency at low loads is also poor too: https://tpucdn.com/review/sapphire-radeon-rx-9060-xt-pulse-oc/images/power-video-playback.png,Negative
AMD,Not since Vega,Neutral
AMD,"Actually, you could count AMD’s current generation laptop dGPUs on 0 hands, considering they don’t have a single RDNA4 laptop part this generation. We’ll see if that changes at CES next year (in ~2 weeks), but I wouldn’t hold my breath.",Neutral
AMD,Weirdly their mobile chips are far more likely to be found in mini-PCs than laptops. I'm not sure why that is but presumably they've built a stronger relationship with the mini-PC firms.,Neutral
AMD,"RX 7600S exists in a variant within the cheaper Asus TUF, same as the RX 7700S",Neutral
AMD,"I've been debating recently on which GPU to get alongside a mobo upgrade, what do you use to have it switch to the dedicated GPU?",Neutral
AMD,As someone that’s works with corporate GPU applications this is funny. If they were so bad why would they dominate the industry?,Negative
AMD,"the idea of a mux switch in a laptop is hilarious, hardware fix for horrible drivers",Negative
AMD,nah intel mobile cpus are just better rn,Positive
AMD,"These numbers aren't relevant. They look like dedicated desktop GPU market share. In reality, I bet Intel is first in laptops, followed by AMD just due to the fact that most laptops don't need, nor want dedicated GPUs. As for gaming space idk",Neutral
AMD,They’ve put double agents to make AMD mobile gpus suck (because they’re)?,Neutral
AMD,LOL,Neutral
AMD,"I call it the *OEM-factor*™ … Always gets immediately DENIED as non-existent, of course, since years.  Yet somehow we had *a completely heathy and quite balanced laptop-market up to the early 2000s*, where there were loads of potent AMD-powered offerings with AMD's *PowerNow!*-technology (Dynamic frequency scaling and power-gating for power-saving), and it was almost like fifty-fifty AMD vs Intel …   That was when Intel was still utter sh!t in that department and had even horrendous power-draw in mobile.  Until it all of a sudden all changed the precise moment Intel brought their infamous *Centrino*™ program to OEMs (paying system-integrators for equipping notebooks with Intel-chips)  — It has stayed as such since (+90% Intel).  Then the Intel-exclusive *UltraBook*-brand was the next, which a while ago just rolled over to be Intel *Evo*.  > Intel and Nvidia pay system integrators to use their own hardware over AMD's.  Yeas, and very handsomely at that. Still, *»Nothing to see here folks, just move along!«*",Neutral
AMD,"And despite the absolute surge of burned connectors since Blackwell, it is still AMD cards that get questions sbout catching fire",Negative
AMD,"Lmao Intel isn't lobbying OEMs to choose Nvidia GPUs.  And also literally read the first paragraph of the article: ""between 2002 and 2007"".  And thirdly, the biggest proof Intel isn't lobbying OEMs (with what money?) Is how many design wins Snapdragon got",Neutral
AMD,"Right... Intel is lobbying OEMs to use Nvidia products..  Maybe every once in a while you should actually read the articles you use as ""proof"".",Neutral
AMD,"AMD had to consistently deliver a better CPU than Intel for several generations before mindshare in the public caught up.  Its going to be the same for GPUs: until AMD offers the better *all around* GPU, several generations in a row, and at a lower price, it's mindshare just isn't going to change in GPU.  And dont forget the impact Halo products have on their downstream products. The fact that the 5090 is the undisputed best GPU gives huge brand prestige to the whole product stack.",Positive
AMD,what?,Neutral
AMD,"> Because of that deal, they can't have dedicated mobile GPUs under their Radeon branding...  Oh shit, you'd better inform AMD because they've been using Radeon branding for all their mobile dGPUs all the way up to this very day.",Negative
AMD,They’ll put a 9050m in laptops after they release RDNA6 in 2030,Neutral
AMD,Why would laptop manufacturers do this to us?!,Negative
AMD,Rdna2 was the only real competitive mobile showing in a decade plus. That was also when they got the most laptops including the flagship lenovo legion 7 in an AMD advantage model. Stop playing the victim and accept they have NEVER been competitive for more than 1 gen in a row and they do not make things easy for oems.,Neutral
AMD,"I fear ""success"" in Halo Strix is related to its limited supply: that is, the Strix Halo large APU price is so high, the demand is also pretty low → AMD can make enough dies to satisfy the small market.  Large APUs will find it tough to crack into the laptop gaming market in terms of market share; the only other large APUs are 1) consoles, which only survive on extreme optimisation, hundreds of millions of units, and they *still* sell at a heavy loss, and...  >As [IGN reports](https://www.ign.com/articles/microsoft-loses-between-100-and-200-on-every-xbox-sold), Spencer confirmed that the loss on each Xbox console is between $100 and $200 dependent on the model.   2) Apple's M-series Pro / Max / Ultra. These are also very expensive and kind of what one would expect a large APU to cost (+ the customary Apple tax).",Negative
AMD,"Chicken and egg doesn’t really work for B2B. The seller has to provide the product and push the deals, demand doesn’t just appear out of the air for products with (edit: particularly established) competition. The buyer doesn’t particularly care which egg it buys as long as it has enough of them to make what it wants to sell.  (And in this case the consumer doesn’t really care either, the overwhelming majority of any form of prebuilt including laptop is just “it’s in my budget, available, and the page says it’s good”.)",Neutral
AMD,"Strix Halo has been a commercial failure.  There is only one commercially available laptop of it. No major OEMs are even interested in it, and it is now being sold to Chinese brands at a discounted price for mini PCs and handhelds.",Negative
AMD,"Most people talk about volumes and while that certainly does contribute, the main problem is that OEMs basically expect cpu vendors to handhold them to designing most of the system. Yes you read that right, designing the system as in mobo reference designs, cooling solution and beyond just what you imagine to be normal SW support. When qcom started to double down WoA to promote snapdragon elite, the higher ups complained about how they had to provide massive amount of support to OEMs because they were to used that and they have to massively ramp up on that for OEMs to take them seriously and not just put their SoC in some half assly designed gimped models",Negative
AMD,"I think APU's will be a big deal for AMD in the near future, they will be fast enough for everything and will keep getting better for the limited power envelope of laptops",Positive
AMD,Supplying the console market is probably one of the reasons they don't have much stock left for laptops.,Neutral
AMD,The consoles are using old nodes tho,Neutral
AMD,"Right, now look at their GPU track record, the relevant market here",Neutral
AMD,"Desktop CPUs sure, but mobile CPUs they don't seem to supply a sufficient volume of either. Look at Strix Halo, a high margin performance leading part and a year after launch you can still count the number of laptops using it on one hand.",Neutral
AMD,But they choose how to allocate their capacity,Neutral
AMD,Cost wise its not effective either. Its the same reason why AMD fail miserably in the Pre Built space.  9060 xt and 5060ti. 350 v 420. $80 and 20% right?  But when you build a whole PC with the 9060xt for lets say $1000. The same with a 5060 Ti wil cost $1080. Only a 8% difference. A difference many would pay  And the difference gets smaller the higher your base pc is  Same thing applies in laptops where the sum of all parts makes the RTX GPU only slightly more expensive.,Negative
AMD,"They didn't. They simply were catching up to Apple and Intel who've had 10-Bit 4:2:2 HEVC decoding for a very long time. Even now, I don't think the 50 Series can decode 10-Bit 4:2:2 H-264 in hardware.   It is a big reason why most videographers and video editors use Macs. 10-Bit 4:2:2 HEVC is a very common acquisition codec now and quite frankly, it is laughable and unacceptable that neither Nvidia nor AMD supported them until this year. AMD still doesn't.",Negative
AMD,"Are you speaking to efficiency under load? Or while idle?  Idle power draw has been a weak spot for AMD GPUs for a while. I haven't looked into it since launch, but at the time initial reviews seemed to confirm this was still an issue for RDNA4.  High idle power draw is bad for laptops, obviously.",Negative
AMD,"I also agree in everything but that point as well. Don't know why you're getting downvoted, though not surprised since everyone here is speaking anecdotally. Also, while you posted an example from TPU there are other outlets such as [computerbase](https://www.computerbase.de/artikel/grafikkarten/amd-radeon-rx-9070-xt-rx-9070-test.91578/seite-9#abschnitt_leistungsaufnahme_gemessen_spiele_youtube_desktop) that backs it up (pretty much on par on idle, and daily usage/multi monitor)  I've actually made a post here: [https://www.reddit.com/r/hardware/comments/1l2vjuo/mostly\_positive\_reviews\_rx\_9070\_xt\_vs\_rtx\_5070\_ti/](https://www.reddit.com/r/hardware/comments/1l2vjuo/mostly_positive_reviews_rx_9070_xt_vs_rtx_5070_ti/)  Where I shared someone's finding on the matter with RDNA4 vs Blackwell (9070 XT vs 5070 Ti). At that time, when capping FPS, AMD by default simply does a better job with freq to hit a locked FPS target to save power, even doing as well or beating the 5070 Ti. In those comments you'll see (obviously so) that FPS capping saves you power anyways, but the matter is, RDNA4, especially when binned and power constrained (+drivers) can be effectively used in laptops.  Oh and btw,  >RDNA2 was also more efficient than Ampere.  IIRC pre-RDNA3 AMD only reported GPU power and not TBP (it was an [igors lab test](https://www.igorslab.de/en/graphics-cards-and-their-consumption-read-out-rather-than-measured-why-this-is-easy-with-nvidia-and-nearly-impossible-with-amd/))",Neutral
AMD,"Yep. RDNA3 was really the only botched generation efficiency wise (post Vega). But just like with drivers, people will continue to believe AMD GPUs have worse efficiency for the next 10 years, regardless of how each generation actually does.  inb4 but rdna2 had a node advantage. Yes, and?",Negative
AMD,"That’s like the majority of professionals who need a GPU in their laptop, that’s not that cherry picked.",Neutral
AMD,You can’t think of a reason why someone may want to do video editing or Blender work on the go?   Or have a device capable of being a video editing platform anywhere to allow for working from home?,Neutral
AMD,basically everyone using a Mac.,Neutral
AMD,"no Blender pros do buy them, as new laptop 40 and 50 series do very good in blender  new laptop “5090” GPU with 24GB VRAM almost matches a 4080 super desktop in blender performance at 175 watts   https://opendata.blender.org/benchmarks/query/?compute_type=OPTIX&compute_type=CUDA&compute_type=HIP&compute_type=METAL&compute_type=ONEAPI&group_by=device_name&blender_version=4.5.0  CAD professionals, with Recent Ai boom a laptop with CUDA support for ML and Ai it's an awesome buy  In steam charts 4060 laptop is the top……  Unlike what reddit thinks,   do you think companies pour millions in for gaming laptop manufacturing R&D to waste?  Gaming laptops do insane number of sales.",Positive
AMD,"They still do. RX 7600 is slower than a 4060 and needs 40-50W more power to run. To compensate, AMD had to reduce CUs from 32 to 28 and reduce power draw. On the other hand, the RTX 4060 is identical on laptop and desktop, down to the CUDA core count and power consumption. It’s the better product. And given that the 60 series is the most popular laptop GPU series, that really adds up",Neutral
AMD,They are less efficient than nvidia and suck way more power at idle which is a big deal in laptops,Negative
AMD,"I dont need to switch it myself, since it works in handshake with the AMD DGPU and IGPU with integrated MUX switch.   So basically, desktop idle and browser stuff runs off the IGPU, everything else, DGPU 7700S handles it.   Also, if ya have a 7745h cpu, not worth swapping to the HX since its half the full core size, plus compressed Zen 5c cores.  Dont like that alot",Neutral
AMD,Open source community distros running GUI desktops are a bit of a different experience than headless Linux servers running with enterprise licensed packages or Windows/Mac desktop.,Neutral
AMD,I was talking tech wise... also no. A quick gander on the passmark puts the top 10 laptop chips as 80% AMD with Intel taking the number 2 and 3 slots. So I'm going to gather that the intel mobile cpus are at minimum on par.. I'm sure theres a some variance in benchmarks but I'm lazy  https://www.cpubenchmark.net/laptop.html,Neutral
AMD,"Qualcomm showing up in more and more laptops. ARM is the future, Apple proved that. And Qualcomm is the company best poised to make the ARM chips in the quantity needed, because they’ve been doing it for smartphones for decades.",Positive
AMD,These numbers are the *only* thing that's relevant.  We're talking about gaming laptop GPUs.  Manufacturers are going to sell what people want to buy.,Neutral
AMD,"I know right! So pathetic what those companies are willing to do to put down AMD, but shoe is on the other foot now and AMD will stomp out intel in the CPU space and is gaining ground as Nvidia leaves gaming to propel the great AI scam.",Negative
AMD,"Yeah, it all changed on a sudden because the Pentium M was just so good.",Positive
AMD,"I gave up pointing that out, people want to buy Nvidia and you cant change there mind even if it's lower cost or not setting on fire.  GPU's are like SSD brands, you just plug one in and it works yet a lot of normal/light PC users still think it's dark magic to change GPU brand.",Neutral
AMD,"Exactly. My biggest piece of advice for tech illiterate people buying a laptop until about 7 years ago was ""make sure you get an Intel CPU"". AMD was producing some absolute donkeys that had no place in the midrange laptops they were going into. It took a while for that sort of advice to cycle out of people's minds, well after AMD started making decent mobile CPUs.",Negative
AMD,"https://www.qualcomm.com/news/releases/2009/01/qualcomm-acquires-handheld-graphics-and-multimedia-assets-amd  https://www.fierce-network.com/wireless/qualcomm-buys-amd-handheld-assets-for-65m  Instead of downvoting me, just do research...",Neutral
AMD,Do you mean AMD?,Neutral
AMD,"Why are there no notebooks with Strix Halo then?  Asus, the vendor AMD even gives preferential treatment, only made tablet - a device nobody asks for - out of Strix Halo so that they don't have to put it in a laptop. Hmm? Absolutely AMD's fault for not making the processor, right?",Negative
AMD,"IIRC Strix Halo has bespoke I/O and compute dies, so presumably either supply is high, there's a plan to productize those some other way later on, or somebody is losing money.",Negative
AMD,"Next gens RDNA lineup is fully dual use. The top spec goes into the workstation market and high end desktop, the second chip is the 9070XT replacement and a cut down version goes into the next xbox. Third chip goes midmarket gpu and Halo gpu die, and the small one for the ...point mass market apus.",Neutral
AMD,"No, GPUs aren't direct replacements in laptops. They have to develop two different boards for each model, if they want to offer both manufacturers. Customers are also very picky in gaming laptops. Almost all of them will pick an Nvidia version over AMD at a similar price. Just ask yourself how much cheaper a $1400 laptop would have to be to pick a 9060XT over a 5060ti? If the answer is >$100, then AMD would have to basically give away the chips for free.",Neutral
AMD,This is so massively ignorant to talk about in the PC space. People do care... It's why Nvidia owns 92% of the PC graphic card market.  AMD has been struggling to sell it's products for ages in the GPU space... Both to end users and to board and laptop partners and it's not because they can't produce product...   APUs are AMDs future in the laptop market because AMD CPUs are unmatched in the x86 space. Strix Halo sells because the okay GPU is bolted onto world class leading CPUs.,Negative
AMD,AMD has been betting on that for years but still have issues giving good supplies to OEMs  And they also take a long time to actually launch their products to OEMs,Negative
AMD,"somehow they have enough silicon to make strix halo IO dies with an entire mobile GPU integrated into the SOC  it is definitely more deliberate, especially since mobile GPU chips are literally just desktop GPUs but in a mobile package and power limited",Negative
AMD,It's hard to judge rn because of AI dram production which definitely is gonna cut into some production for some of the process nodes. Ideally though you'd have thought they'd be at least well suited to gaming laptops with their lower power console focused development.   Especially considering their APUs are almost best in class at this point so you'd thought they'd at least have it down in the lighter laptop selections.   I think its mostly just legacy volume contracts and risk adverse manufacturers keeping intel and nvidia dominating,Neutral
AMD,Yeah they are but uh track record is kinda always gonna be old. Plus I wouldn't say that the current situation is exactly a good example.,Negative
AMD,"I am, I'm talking about their console GPU manufacturing. I'm saying that they have good track record for chip manufacturing its at least a half decent indicator.",Positive
AMD,"AMD uses a single universal tapeout/photomask set to satisfy the vast majority of their entire desktop and server catalogues. Even a substantial proportion of AMD's available laptop offerings over the last 2 years have been -HX ones, which are just the desktop SKUs made into a solderable package. That's why it's always been relatively easy for them to maintain relatively consistent inventory, despite having far far fewer wafers coming in compared to what Intel can get.  The actual monolithic mobile SKUs are fundamentally different die designs to the other market segments. Strix Halo also uses a fundamentally different chiplet packaging method to server/desktop Zen, so neither the CCD or I/O die are compatible and makes it its own third, separate limited production assembly line.",Neutral
AMD,It's that actually production problems or do manufacturers just not adopt strixx halos for what ever reason they dont adopt AMD CPUs or GPUs in the laptop market?   Are there volume problems with those laptops? I can imagine strix halos packaging makes it hard to produce,Negative
AMD,"Does anyone expect them to prioritize the single GPU buyer, and let large customers get whatever is left, or does the other way around seem more sensible?",Neutral
AMD,"To sum up, when your one & only selling point is lower upfront cost, you will have a lot of uncertainty and areas of business you won’t be able to compete in.",Negative
AMD,Oh no wonder photographers and stuff use Macs,Neutral
AMD,RDNA4 idle efficiency is good too. You might be getting confused with RDNA3 big GPUs which were chiplet based. They use more idle power because they are chiplet based.  https://www.techpowerup.com/review/asus-radeon-rx-9070-tuf-oc/41.html,Positive
AMD,"Wouldn't call RDNA3 botched, it's just a price of doing chiplets. I'm sure they learned a lot from it.  rdna2 had a node advantage but it wasn't just the node advantage that made it more efficient it was also the infinity cache, the new cache hierarchy that improved memory bandwidth efficiency (AMD could have similar bandwidth with less memory bus width). Nvidia followed suite by adding a big L2 cache in the next generation.",Neutral
AMD,"I can think of plenty that do, and it always comes down to the same two reasons.    1. They (for entirely selfish reasons, unrelated to work) travel between multiple locations and need access to GPU computing power, while away from their (home) office.   2. They don't understand remote compute and streamline decoding. While using the worst internet sources while on the go.   It's not that other, faster and more reliable solutions exist. It's that they are typically ADHD gremlins that need simple, reproducable solutions that work, even if it takes 500x longer and has stronger caveats.   It's not a market issue. It's a user issue wanting a product to confirm to their lifestyle, that wasnt designed to. Laptops are not workstations, and don't have the P2Pr to even compete in the space. But so many refuse to be ""tied down to a desk"" that this is how things are.",Negative
AMD,"Laptops are for convenience. Video editing and blender require some amount of time sitting down to do what you want that it's better off being done on a dedicated workstation.  ""I'm going to do a little blender and video editing on my laptop at the library today"" - said no one ever",Neutral
AMD,"Nah, idle power on Nvidia is tragic if you more than a single monitor unless they have identical resolution and refresh rate. My 5090 goes from ~40W to 90W by just plugging in a second display while doing nothing",Negative
AMD,I have the 7940HS and was playing on going to HX 370,Neutral
AMD,That’s where it’s being used mostly. I know this your hobby but surely you can see the bigger picture.,Neutral
AMD,yes tech wise is why intel cpus are better. [https://imgur.com/a/CyxoUZM](https://imgur.com/a/CyxoUZM)  On paper the 9955hx and moreso the 9955hx3d look very good but in practice they fall short.   The 9955hx3d and 9955hx are simply lackluster in terms of battery life with the 275hx giving double the battery life and when unpluged the performance of the 9955hx/hx3d tanks which causes the 275hx to pull ahead when not plugged in. Additionally ryzen cpus have more sleep state issues which again hurts the battery life although that could be considered a windows problem and not on amd.   On top of this the intel chips have thunderbolt support and come with better wifi cards which amd refuses to use.   Then on the ultrabook side of things intel lunar lake is just better in terms of having lower powerdraw and a stronger igpu compared to AMDs lower power options although both intel and amd get smacked by ARM chips so not really that important unless you need x86,Positive
AMD,yeah I would say ARM chips are the best for most laptop users but for gaming you still want x86 at least for now.,Positive
AMD,"Yes, I know about that. It doesn't have anything to do with their desktop/laptop class products.  Also, what do you mean by "" they can't have dedicated mobile GPUs under their Radeon branding...""? There are literally dedicated radeon laptop GPUs available.",Neutral
AMD,Does AMD make things that go “woosh” when you need them to?,Neutral
AMD,Strix Halo is way too expensive and still sucks down too much while doing absolutely nothing.    OEMs find it cheaper just to use an Nvidia dGPU with any mobile CPU and the end result is an equivalent or better mobile product except in Cinebench and Blender. It's absolutely AMD's fault for making something that few people want.,Negative
AMD,You have to go ask amd why they don’t codesign laptop platforms with oems the way intel does with their evo platform. Nobody automatically deserves commercial success just because they exist. If intel invests money into oem platforms then that’s what amd must pay to do.,Neutral
AMD,"And Intel is closing in on the APU side as well. Lunar Lake was impressive but also a bit expensive and also seems to had some level of supply issue, but Panther Lake is probably rectifying that and being cheaper to make, too.",Neutral
AMD,Their problems existed far before AI was a thing. Whatever new thing pops up conveniently becomes their newest excuse.,Negative
AMD,"And the consoles are a completely different market    AMD do not commit volume to PC  gaming, look at the GPU shipments, a much better indicator    I didn't say they couldn't, I'm saying they dont",Negative
AMD,"[Based on Amazon's best sellers](https://www.amazon.com/s?k=amd+nvidia+gaming+laptop&s=exact-aware-popularity-rank), AMD's monolithic APUs are way more numerous than the desktop-repackage CPUs (which makes sense, they're kinda crap for a *laptop*).  But note that I had to put ""nvidia"" in the search to filter for actual *gaming* laptops, and not [""""""gaming"""""" laptops](https://www.amazon.com/NIMO-FHD-Gaming-Laptop-i7-1165G7-GPU-Computer-Fingerprint/dp/B0F549GSX4?th=1) with no discrete graphics and undisclosed CPU model.",Neutral
AMD,"Unless you are doing one specific job, which is LLM inferencing, no matter what the hype says, Strix Halo is very poor value performance wise. RTX 5060 laptops are faster and generally half the price.   Nvidia GPUs are then also way better supported in professional applications than AMD GPUs.  And if you really wanted to do LLM inferencing on a ""budget"", a MacBook Pro is more efficient and faster anyway.",Neutral
AMD,"No, but that's the reality    They don't commit the volume to compete in the GPU space, never have    Until that changes, Nvidia are PC gaming, a consequence of sufficient supply",Negative
AMD,"Well as an amateur photographer I can tell you it’s just one of many reasons. The display of a MacBook is always perfectly calibrated so the colours look correct. They have great battery life so you can work from anywhere. And they have the fastest single core performance of any computer and as a result, Lightroom works a lot faster",Positive
AMD,"On laptop the goal is to have zero power cost for having a GPU, this requires that the GPU get put into D3cold when not used. So idle figure from TPU's article isn't relevant because you wouldn't want to leave the GPU up in an idle state (with integrated graphics scanning out the display), you'd tear it down completely and power it off.  Doesn't work if you want to eg. load stuff in VRAM like running a LLM though, or hook up a display to a port directly connected to the dGPU :/",Neutral
AMD,"Oh wow, finally! That is actually great to read!      For more than a decade, AMD cards, like my old R9 280X, just drank down the juice in multi-monitor idle, with sometimes double the power draw of comparable Nvidia cards.      Even a 3080 is frugal in multi-monitor use compared to a 6800 for example.",Positive
AMD,Thx for providing a source.  TIL.,Neutral
AMD,Never worked on the creative/film/event management field I guess?,Neutral
AMD,"That’s a very valid student usage of laptops though, and you very conveniently leave out the work from home usage which does involve sitting down somewhere for some amount of time, while still needing the portability to bring the device between home and the office if needed.  A workstation would mean giving each employee two device for hybrid work, or forcing remote usage, neither of which would be an amazing option compared to just giving each employee a laptop to commute with",Neutral
AMD,Discrete GPUs are not meant to be used as display adapters. [Even Windows has CASO](https://devblogs.microsoft.com/directx/optimizing-hybrid-laptop-performance-with-cross-adapter-scan-out-caso/) now. Shut that pig off when it's not rendering something.,Negative
AMD,">My 5090 goes from ~40W to 90W by just plugging in a second display while doing nothing  Yea that's not normal, 2 displays have been fine on nvidia cards for... well as long as I've had an nvidia card so 2017, to stay in idle memory speed state and it's only couple of watts more. Maybe if you go high enough it changes like two 4k240 displays or one/both being 1440p500, but 4k240 + 4k144 brief moment i had that combo didn't do it on a 4070ti and was fully idle state. Or you have some power settings cranked up maybe those effect things. Obviously interacting with like browser stuff with 2 high refresh/res monitors does have more brief spikes than a single panel would, but that's not idle.    3 is a bit of mixed bag, but even that it much better than it used to be like ~4-5 years ago, when 3 monitors of any kind was max memory speed state and can still get idle at 3 panels if the refresh rates of all panels isn't high and even then it might go to half memory speed state instead of full speed on some specific combo. I do remember some talk about how the specific model also might affect things due to... something, rather than just pure refresh/res combo. thought that was mainly for 3(+) panels where it mattered, but maybe that could be what's happening even with just 2 panels.",Neutral
AMD,"Meh, not worth it tbh.  If it were a X3D version, yeah, but basic 370 is basically 3% faster",Negative
AMD,"I’m not sure what you mean by “bigger picture”, but the desktop GPU experience is why people in this sub would have the opinion that Nvidia on Linux is a poor experience.   It’s only recently (within three years) that Nvidia started releasing open source kernel drivers for their cards on Linux (because they moved their proprietary code into the device firmware). Prior to this (and even still) it wasn’t uncommon for Linux users to boot into a black screen after upgrade due to an Nvidia related problem.  This is different from large ML or HPC clusters which will use licensed drivers and CUDA libraries curated by someone like SUSE or RedHat for a hefty fee.   With regards to procurement, Nvidia has been popular because of how good CUDA is, how powerful/efficient  Nvidia cards have been since Maxwell, and how good their marketing is.",Neutral
AMD,">The 9955hx3d and 9955hx are simply lackluster in terms of battery life with the 275hx giving double the battery life and when unpluged the performance of the 9955hx/hx3d tanks which causes the 275hx to pull ahead when not plugged in.  The problem is that the laptops that those chips go into often don't really care much about battery life at all. Desktop replacement/thick gaming laptops don't really prioritize battery life.   Even more so for the unplugged performance.   For the thinner gaming laptops that might, such as the Asus G16/G14, you see the -H series be used, or even LNL in that one experimental Acer laptop, an entire mobile class of CPUs you ignore in your comment.   With ARL-H, Intel is on par or has a slight lead in battery life, but also has a good bit worse nT perf/watt scaling.   Overall I still think Intel has the better mobile portfolio, but I don't think it's as lopsided as your comment makes it out to be.",Negative
AMD,"Cant believe i needed to turn on my vpn for that. I did think there would be variance in the test, is there a reason the amd chip doesnt ever reach its max tdp or is it limited? I'd also like to point out the x3d chip isn't on your graph and they behave wildly different.   I guess the thunderbolt is okay ish? And idk what the wifi card thing is about considering how many laptops just use a separate WiFi chip.   >although both intel and amd get smacked by ARM chips  Not surprising its like the whole point of RISC to be specialised it'll only keep getting better.",Negative
AMD,They make the sound when the bicycle in the meme hit the ground,Neutral
AMD,"So you do know for a fact they don't do that, or is that just you making it up? It's funny that you on one hand deny one theory without much reasoning but take another one for granted.",Negative
AMD,"Laptops are closer to consoles in alot of ways, tight oem integration, semi custom is also more common, power envelopes and form factor are much similar too.   Also why should we not include AMD's largest consumer GPU segment? Surely if you want to look at whether they can do the volume for an oem you would look at their total track record for oem gpu shipments? Of which the largest will definitely be the shipments to Microsoft and Sony.",Neutral
AMD,You wouldn't put the strix halo in a laptop that could fit a 5060 in it i guess then. You'd be putting it in thin and light laptops where you don't really have the power envelope or space for a discrete gpu. So it doesnt really matter what its perf is vs a discrete gpu cause that laptop chassis isn't gonna fit one.  Also on a side note wasn't it really designed for Microsofts god awful Copilot+ PC rollout that flopped heavily i can imagine the inference based APU coupled with another discrete gpu is why?,Neutral
AMD,"""My boss is irrational and doesn't plan anything out. Just wings everything and expects me to fix it when it doesn't work, with this POS laptop that takes me 8 hours to render a 720p scene, that I should be mastering in 2160p and scaling to maintain quality. But they are too impatient, so everything looks like a streaming video at 240p on YouTube.""   Yeah, and the expectations in those toxic work environments are not based on reality but forced compromise. Because the people in charge hate that they have to use technology...",Negative
AMD,"I’m saying that the drum beat of bad driver support isn’t the whole picture. If they want something free with no official support they are gonna have issues.  But saying because of that NVIDIA is bad when they have 100,000s of Linux system running the world at the moment is short sighted.",Negative
AMD,[https://imgur.com/a/PmUFBnX](https://imgur.com/a/PmUFBnX) just over double the battery life compared to the 995hx3d and that is a best case scenario for the 9955hx3d because if you are turning the laptop off many times in the day the disparity will grow due to sleep state issues.  The wifi card thing is a hardware thing because intel makes the better wifi cards and while they do have multiple chipset versions(one CNVio that only works with intel cpus and one non-CNVio) Amd still doesn't use them probably due to cost but also there are some driver/hardware issues as even the non-CNVIO Intel BE200 is known to have issues with AMD cpus,Neutral
AMD,"....if they do then where are the results? It's not a ""theory"", evo platform is a documented thing intel does with oems.",Neutral
AMD,"They haven't released mobile RDNA 4, that's tells you all you need to know about commitment   If they actually competed on volume, don't you think it would make sense to actually launch some products?  AMD would rather sell them bundled with a CPU for a premium, why their APUs are so prolific",Neutral
AMD,"Even then you wouldn't though. AMD says the Max+ 395 can sustain 120W and boost upto 140W. That's not really much less than most 5060s do paired with a Ryzen 7 or Ultra 7.   I have an Omen Transcend 14 with a Meteor Lake Ultra 7 and a 4060. The total power budget for them is \~85W because the laptop is powered via a 140W USB-C power adaptor. The 4060 can pull upto 65W. The newer 5060 model can pull 75W because the CPU is more efficient. As a result, my laptop is slower than a full-power 4060 laptop.  But the same is also true for the ZBook Ultra G1A. Also a premium, high-performance 14"" HP. Also runs through a 140W USB-C power adaptor. Also has the same 85W power limit. Is also slower than the ROG Flow Z13 tablet that has a 180/200W power brick with 20-25W higher power budget. It is also not really faster than my laptop for the GPU.  Of course, the ZBook's CPU is a lot faster. I guess a more fair comparison in that sense would be the 385, but that has a slower GPU.   All of that would be fine until you realise that the ZBook costs a LOT more than the Omen and the laptops otherwise are very similar. Similar battery life, same OLED display, very similar keyboard, trackpad and speakers, similar build quality and general handling.",Neutral
AMD,theyre talking about desktop usage. nvidia lacks some features and gets performance drops in some cases in desktop linux.,Neutral
AMD,Oh what laptop was this test ? I assume it was amd vs Intel versions of the same one.   Also the wifi card thing doesnt matter all that much anyways the perf of intel vs qualcomm vs cnvio is really not going to matter much considering they are all compatible with the same standards so you can get a better one if you want,Neutral
AMD,"Yeah what happened to that? Was it power?  >If they actually competed on volume, don't you think it would make sense to actually launch some products?  >AMD would rather sell them bundled with a CPU for a premium, why their APUs are so prolific  I don't want to be cynical and blame AI but I think they were putting their production into datacenter stuff instead",Neutral
AMD,"The tdp for the 5060 in the transcend 16 is 140W which is more than the strix halo by itself which is what I really meant. I mean its the whole point of a integrated GPU less power, less space, less engineering complexity unified memory so on and so forth.",Neutral
AMD,yes same exact laptop the XMG Neo apart from ofc the superior wifi card on the intel laptop and thunderbolt.,Neutral
AMD,">but I think they were putting their production into datacenter stuff instead    As is Nvidia, who are committing more   But despite that, Nvidia still commit enough volume to supply the gaming market",Neutral
AMD,You seem really hung up on single port type and WiFi card huh,Neutral
AMD,"I mean for a company having volume problems 73% year on year revenue increase in that segment seems pretty damn good. I mean looking at their financials I'm finding your point hard to believe, they had record radeon sales and increased gaming revenue from 0.5 to 1.3 Bn yet they're having volume issues...",Neutral
AMD,? no I was just saying they are the same exact laptop spec wise except those two parts although those two parts aren't really important in terms of power draw which would make the comparision accurate.   If they instead had different screens like oled vs ips that would cause a difference in power draw.,Neutral
AMD,Please don’t tell me they’re going to slap the ugly HyperX logo on the lid,Negative
AMD,"Seems like the OLED displays and the inclusion of the 9955HX AMD chips are the big changes for the next year.     Shame the most powerful AMD CPUs are not going to be paired with the best GPUs and cooling in the Max systems.     HP used the 8940/45HX AMD chips in the regular Omen this year already but it's a strange mismatch putting that with a 5060 and a weaker thermal system that requires a power limit restriction. At that point, why bother with a 16 core CPU that's going to be hamstrung from the beginning?",Negative
AMD,"Kinda weird to have 15"" relaunching when they have dedicated 14"" and 16"" lineups",Negative
AMD,Tasteful laptop design.   Difficulty: nearly impossible.,Neutral
AMD,"Well, it's a lot of research and costs to put all that EDRAM on a CPU   And most of the gains were in gaming. So it made sense why Intel abandoned the idea.    But now it would make sense to bring back to their mobile chips that have to share bandwidth between CPU and GPU so much",Neutral
AMD,"Despite its drawbacks with lower CPU clocks, Broadwell/Crystal Well CPUs were ahead of their time. A well-clocked 5775C is beating out Zen 2 and early Skylake in gaming performance. It's a shame they didn't get the attention they deserved.",Positive
AMD,DDR4-3200 and higher was faster in both BW and latency. And the on-die logic for the EDRAM controller took up as much space as two cores.  There is a reason why it was discontinued.,Neutral
AMD,"The gaming perf these chips offered where unmatched thanks to the EDRAM. 5775c were matching and/or beating 6700K by a fair bit. They have ages miles better with the EDRAM than without. Consider as time progressed intel would have improved it than writing it off as ""It wasn't good for a first gen"" is stupid.",Positive
AMD,Similar cost for consumers does not mean the chip wasn't a lot more expensive for Intel to produce and package...,Neutral
AMD,"> 5775c were matching and/or beating 6700K by a fair bit.  That is outdated info. Reviewers at the time tested with slow DDR4.  Check out [this revisit of Intel CPUs](https://www.youtube.com/watch?v=KPWEdbfJ0oE) by GN. Usually 6700K wins, sometimes 4790K trades if workload benefits more from clockspeed.",Neutral
AMD,Yeah because Skylake supported both DDR3 and DDR4 and DDR4 at the time was barely faster than the fastest DDR3 - which went as high as 2133 MT/s - whereas JEDEC DDR4 started at that speed.,Neutral
AMD,"Okay and? even running 3200 skylake, a 5775c can match a 6700K...   Also most people by 2014, 2015, ddr3 was at 2400-2666 speed without ass pricing.   If you were building a high end rig with this chip you where running decent ram.",Neutral
AMD,"[Here](https://www.hardware.fr/articles/940-15/cpu-jeux-3d-crysis-3-arma-iii.html) is one of the launch reviews featuring Skylake and Broadwell-C, with Skylake on DDR4-3600  The actual difference, if any, is seen on the i5 parts because they lack SMT.   Games from that era were very much dependent on ST performance, where extra cache could alleviated the DX11 driver tricks that Nvidia drivers did - which you now see as ""overhead"" in DX12 and Nvidia which X3D alleviates better.",Neutral
AMD,"Did you even read the review you linked...? Your point even with 3600 is just false?   It's on par all through the gaming benchmarks lmao, the 5675c is beating the 6600K most of the time aswell. Congrats, you played yourself.",Negative
AMD,"Did you just choose to ignore the rest of my comment?  ""EDRAM helped when you don't have SMT"".",Negative
AMD,"And EDRAM is simply DRAM on the substrate - the ""E"" stands for embedded.  Intel already has its successors in Lunar Lake and Panther Lake - SLC/memory side cache.",Neutral
AMD,edram helps full stop... SMT or not. Even with a 13% deficit in clocks the 5775c is matching the 6700K with faster ram. Your point?,Neutral
AMD,">SMT or not  Yeah, you are incapable of reading data.",Neutral
AMD,And another point is that Intel was still the supplier to Apple back then and EDRAM was put there for primary use by the iGPU in mostly Apple specific parts.  The 5775C/5675C were the only retail SKUs that had it. The iGPU was branded as Iris Pro.,Neutral
AMD,"Okay and? What does that have to do with gaming perf, the point which I made is that it beats skylake.",Neutral
AMD,Will it come with vram,Neutral
AMD,"I thought this was gonna be UDNA, not RDNA5.",Neutral
AMD,"Makes sense why sony want release ps6 in the end 2027,iirc rdna2 released in fall 2020 as ps5",Neutral
AMD,I'd be surprised if it didn't.,Neutral
AMD,18 months out the rumour might as well be a nothing burger.,Negative
AMD,I’ve been hearing 2027 more often from the rumor mill sources and it’s incredibly disappointing. 2026 should have been a banner year for PC parts and now it’s looking like just the new CPUs are launching and nothing else,Negative
AMD,in 2 weeks : OpenAI just bought all RDNA5 GPU stock until 2030,Neutral
AMD,Completely depended on ram avaliabllity,Neutral
AMD,"RDNA6 rumored to launch in mid-2029. There you go videocardzzzzz, no need to post one more rumor about it. 🤣",Neutral
AMD,Oh neat.  More than a year and a half away.  Must be a slow news day.,Positive
AMD,"Cool, just in time for them to banish RDNA 3 and 4 to the maintenance branch.",Neutral
AMD,"Mid-2027 feels far, but at least it gives RDNA4 time to shine first.",Neutral
AMD,"Considering the current situation we are in, RDNA 5 GPUs will be BYOV (Bring your own VRAM)",Neutral
AMD,"Damn, it's gonna be the new Vega, isn't it.",Neutral
AMD,GPUs have a two-year cadence. It's obvious that the next generation will come H1 '27. Until then we might get some refreshes from team red and green while team blue is on track to actually deliver some proper mid range GPUs.,Neutral
AMD,4090 level raster perf and I box up my 4090s and buy these.  Nvidia drivers on Linux have me at wit's end,Neutral
AMD,"This will get pushed back. High speed memory will still be in shortage and expensive.  There will be a refresh of RDNA4. Most likely, with more and faster GDDR6 memory. That is doable.  9070XTX or 9075XT 24Gb with 22 or 24Gbps GDDR6 memory. They can use the 9700AI pro board, but just fit lower density memory to half the board 16+8 = 24. Still on a 256bit bus.  The low density ram won't be very expensive.   UDNA seems to be looking more and more like UDNA=RNDA5. AMD is putting a lot of effort into making RDNA the majority of the future for AI. The AI 9600Pro is a great example of that.    AI 9600 Pro with 64GB memory would be nice. That would push AMD into more data centre spaces.    I also think top end AMD graphics will be back on HMBe..",Neutral
AMD,So AMD lied about RDNA4 not competing on high-end in order to launch RDNA5 faster than Nvidia's 60 series,Neutral
AMD,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,That’s DLC,Neutral
AMD,"128bit is 4x 32bit chips to 4x vram controllers in gpu. 3GB gddr7 for 12GB.    192bit is 6 chips. 18GB.   256bit is 8 chips. 24GB.   384bit is 12 chips. 36GB.   512bit is 16 chips. 48GB.    They’ll do 256bit for the xx70 gpu and 128bit for the xx60 gpu as that is the most efficient design to keep the gpu chip smaller. The more vram controllers in the gpu, the larger itll be in area.   If they decide to go “5090” big, it’ll be interesting. They would want to go for 512bit gddr7 to compete but 384 may be possible if they dont wanna go that huge.   But more overall vram is likely, but more vram chips is not. Theyll use the same # of ram chips but at the 3GB density.   I dont know why i typed all of that nonsense… my brain got the tunnel vision and i had to do it. I need my coffee. Byeeeee",Neutral
AMD,yes. you can subscribe to 1Mb monthly vram for $50,Neutral
AMD,"It's FSR 5, which eliminates VRAM using AI.",Neutral
AMD,With a subscribtion for RAM. Unlock what you need.,Neutral
AMD,"Actually, maybe no. Some rumors say lower end parts will actually come with DRAM instead of VRAM. There  have been claims from like AMD or Mark Cerny or someone, that also said they are trying to drastically reduce memory bandwidth requirements of GPUs. So there might be truth tot hat rumor. It's also something they absolutely need to do if they want to create huge APUs for laptops, or other mobile gaming platforms like the Steam Deck 2 or rumored PS6 handheld. Maybe the PS6 even, will use DRAM instead of VRAM.",Neutral
AMD,"No, we’ll have to download it from OpenAI’s Stargate monopoly.",Neutral
AMD,"I wonder why AMD and NVidia do not just put the RAM in the chip? Doesn't the M5 chip and the Strix Halo chips have the memory right on the die? Seems like they should be able to just do this with discrete GPU's as well? I know it's not simple and would take years to do, just wondering why they have not done it already.",Neutral
AMD,Honestly don't know which is which at this point,Negative
AMD,From what I can tell udna and rdna5 are referred to interchangeably in leaks. We will likely have to wait for official confirmation to be reasonably sure about either.,Neutral
AMD,It's gonna be some kind of DNA,Neutral
AMD,No one knows not even AMD. At FAD they didn't even commit to a name. Nothing. This confusion will prob continue until Cerny does his Road to PS6 in 2027 :(,Negative
AMD,"UDNA launched already, as amdgpu-spirv. It’s AMD PTX, not an actual uarch.",Neutral
AMD,UDNA and RDNA5 are the same thing,Neutral
AMD,"Yes, let's hope the memory shortage doesn't ruin those plans.",Neutral
AMD,"You're on crack if you think the PS6 is coming in 2027 lol. Memory shortages mean Sony will be looking at at 12 month delay at a minimum, possibly more if things don't improve. You can't release a next gen console with the same, or maybe 24GB Vram, that would be a pathetic jump.  Sony will want this console to last 10 years at least.",Negative
AMD,"AMD Radeon alone is nothing burger, unless AMD increase their GPU production by 5 times. Given how AMD only hold 7% dGPU market share now, so yeah 5 times just about right.",Neutral
AMD,you can say that again,Neutral
AMD,There will be a horrible DRAM shortage likely for the entire year.,Negative
AMD,Why? They always release it in 2 years cycle. RNDA 4 is 2025.    Same for RTX 6000. Why everyone talks like it's a big news,Neutral
AMD,Why would 2026 be a banner year? RDNA4 and Blackwell just launched this year. Normally it's a 2 year cadence for new GPU architectures.    2026 will be great from a CPU perspective. We get Zen 6 and Nova Lake.,Positive
AMD,Would you prefer they launch in peak memory hellscape?,Neutral
AMD,And basically nobody in the DIY market will buy a new CPU without some new RAM to put into a new motherboard...,Neutral
AMD,"Going to be a ""slow news"" year. Youtubers are going to suffer with really desperate content in the next year, since there will be very minimal hardware launches.",Negative
AMD,"It's going into virtually everything from the PS6, and PS6 handheld, all the way to next generation Xbox. Maybe even Steam Deck 2. It can't be Vega.",Neutral
AMD,"Only Nvidia ever had any routine two year cadence and even they've gotten off that a bit with the 50 series.    AMD have a very mixed record and never followed any real routine.  Sometimes they'd release just like one new GPU in a year on a new architecture, and then the next year they'll only release low-mid entries on a new architecture, and so on.    There's no rule that says we couldn't start seeing more like 2.5 years between releases.  Or whatever.  There's no rules about this stuff at all.",Negative
AMD,">Nvidia drivers on Linux have me at wit's end  It's been pretty stable for a while, like two years.",Positive
AMD,">9070XTX or 9075XT 24Gb with 22 or 24Gbps GDDR6 memory.  How would they do that, if 3GB GDDR6 memory modules does not exist, and no one is investing in developing it?  What they could do is a create a whole new 500mm^(2) die with a 384 bit bus, 12 memory modules, and 96 CUs of RDNA4. They have claimed that RDNA4 is pretty modular in design, so it wasn't hard to double the 9060xt design, to create the 9070xt. It's really double the chip. So 96 CUs would not be difficult for them.",Neutral
AMD,"I feel like AMD teases the possibility of releasing before Nvidia every gen since RDNA2, and then they always do the Nvidia - $50 approach.",Neutral
AMD,"AMD will never, ever, launch their products before Nvidia, unless they know for sure how big the performance gap and Nvidia pricing is. Because it will be a disaster for their already weak sale, if they get undercut by Nvidia.    You already know this year they had to delay and then change the price of the 9070xt down to 600$ from 700$, because the 5070ti was 50$ cheaper than they expected",Negative
AMD,Where did AMD say that?  Got a quote?,Neutral
AMD,What are you even talking about? There is no RDNA 4 product that competes with high end Blackwell.,Negative
AMD,I emailed https://downloadmoreram.com and asked if they would add VRAM to their lineup to help with the shortage.,Neutral
AMD,PC brought to you by Paradox,Neutral
AMD,"People make this sort of argument in many generations that they'll use these 'in between' memory capacity chips and they just never really do.  Cuz of the lower volume, they tend to not be great in terms of cost per GB compared to high volume memory chips.    Basically, your whole premise relies a lot on Nvidia wanting to give us better VRAM capacity and selling us midtier GPU's with midtier naming and pricing(and low=low, high=high).  That's a lot of bones Nvidia will supposedly throw us in a time when everything is more expensive for them.   I just dont see it.  I think we'll get a continuation of what they're doing now, just with higher prices.  They're not gonna suddenly start to care less about having good margins and there's no indications consumers are ever gonna stop buying their GPU's.  They've got no incentive to be nice to us.",Negative
AMD,"I would love to see more VRAM but that would mean more expensive cards, which the original question was referencing due to the memory price surge.",Neutral
AMD,> 512bit is 16 chips. 48GB.   you really think you're gonna see $1000 worth of VRAM on an AMD card?,Negative
AMD,"Once you've gone with a giant bus for the top die it's difficult to go back since professional clients expect VRAM to go up every generation. 512 on die and maybe 448 for the 6090 if yields are bad.  Also would expect all the other buses to go down and the gap between top and mainstream to widen. If you look at rumored RDNA5 / PS6 / NextBox specs all the buses are down per tier. PS6 is down to 160-bit from 256-bit on PS5. Buses don't scale as well as logic with node shrinks so if you keep the same bus then more and more of the die will be the bus until the bus is bigger than logic. That's why the 70 class used to be 256 and is now 192, 60 class went from 192 to 128.  Also you can see this when Sony and AMD are talking about 'Universal Compression'. You need to work around this and make the uArch more bandwidth efficient.",Neutral
AMD,"There was a rumour that RDNA 5 would top out at 384 bits, matching with the rumour it would not be faster than the 5090.",Neutral
AMD,Screw GDDR! They can go with 512bit of LPDDR5X at this point. Just put some decent SRAM L2 cache onboard and get \~600GB/s but with insane capacity,Negative
AMD,That's Nvidia's plan.,Neutral
AMD,It increases in memory every month by 10% but if you miss a subscription it goes back down to 1Mb.,Neutral
AMD,"Maybe RDNA5 is just a video decoder you can connect to a data center with for cloud gaming!  But only for 100 hours a month, more would be an unhealthy habit.",Neutral
AMD,"It's basically a Zen strategy for RDNA5. Design costs are outragious on N3, so it's better to share GPU chiplets with mobile instead of designing seperate for each market. Makes sense to me.  Considering all the stuff they're prob working on for the first time mobile GPUs might finally not be completely BW choked and Medusa Halo could be insanely powerful.",Positive
AMD,Both have had HBM SKUs,Neutral
AMD,Expensive. It will be expensive. It has been estimated Rubin desktop comes earlier in 2027 which will see a healthy price increase across the board.,Neutral
AMD,"AMD was pretty clear on this when they announced UDNA last year. The exact wording was:  ""So, part of a big change at AMD is today we have a CDNA architecture for our Instinct data center GPUs and RDNA for the consumer stuff. It’s forked. Going forward, we will call it UDNA. There'll be one unified architecture, both Instinct and client [consumer]. We'll unify it so that it will be so much easier for developers versus today, where they have to choose and value is not improving.""  The only way RDNA5 makes sense would be as informal codenames for a group of products that were in the planning phase of development before this decision was made. I seriously doubt there will be any formal RDNA5 branding on anything, that would be like if they had called the Radeon 5700 XT ""GCN6"" rather than""RDNA1"".",Neutral
AMD,"Argh. They kept making this big deal about UDNA being this glorious future, they really need to commit.",Negative
AMD,"24GB is fine if they fully lean into nextgen paradigms (neural textures, neural rendering (MLPs) and procedural content) and API (workgraphs + clean slate API overall).   12.5GB -> +22GB if they include DDR5 similar to PS5 Pro. But rn most rumour suggest it'll be 30GB so yeah zero chance it'll be 2027. Sony can't commit to anything until this current mess has a real end in sight.",Neutral
AMD,"Sony could have signed contracts for components before the price increase, and the supplier will be obligated to sell them at the price at the time the contract was signed,at least for several million units",Neutral
AMD,Indeed. The big three are building capacity but the most serious chunk of that is being allocated to enterprise to ensure they can control the price of consumer memory.,Neutral
AMD,Well be lucky if by Black Friday 2027 RAM prices go back pre surge pricing.,Neutral
AMD,Won't be over even in 2028 but supply and logistics should be less bad by then.,Neutral
AMD,Make it the rest of the decade.,Neutral
AMD,"RDNA4 in 2025 already bucks the '2 years' cycle since RDNA3 was 2022.  People were kind of hoping that with RDNA4 being something of a 'stopgap' product line/architecture, that RDNA5 would have gotten things back on track to the same 2 year cadence as before.",Neutral
AMD,AMD bad.   Need more RAM.,Negative
AMD,gotta generate clicks and engagement somehow.,Neutral
AMD,There's no suggestion that things will be any better by mid 2027.,Negative
AMD,"Yes, because then that would allow a good half a year for prices to fall as few will buy at inflated prices. GPUs already aren’t selling now at inflated prices, and they certainly won’t in 2026 when AMD and Nvidia jack up prices due to claims of a memory shortage",Negative
AMD,50 series was delayed by 1 quarter.,Neutral
AMD,"Its what I got, it's enough for me for the rest of my gaming ""career"", and the drivers won't be an afterthought to the people with access to meddle with them",Positive
AMD,"I am happy to hear that, truly.   For three years it has been hit or miss.  There is always something off.  Currently its primary weirdness is my two portrait screens coming back from standby, or much less frequently upon first boot, in landscape along with poor CP 2077 perf.    The funny thing about Cyberpunk is it was fine until they got frame gen into the last Ubuntu driver update a number of months ago.  Now it's a stuttering mess.   there is a laundry list of shell game/whack-a-mole items that have come, gone, came back, maybe got fixed again.   I have this problem on a two separate 4090 rigs on two different hardware platforms (5800x3d/x570 and 9800x3d/x870e) and the jank is normally quite the sameness for a given distro and update level... i am bad about abating distros that have fallen into disuse.   I am hardly a linux pro but I have a comfort level with it.  I will happily shelve the 4090s when the aforementioned conditions have been met.",Negative
AMD,">How would they do that, if 3GB GDDR6 memory modules does not exist, and no one is investing in developing it?     There are several ways to do it.  * Use the Ai Pro 9700 board, but fit only 24Gb. 8 x 2Gb on one side, and 4 x 2Gb on the otherside. The issue of this is halving the bus size for that last 8 GB. So unlikely. * Use the Ai Pro 9700 board, but fit 8 x 2Gb on one side and 8 x 1Gb on the otherside.  1 Gb chips already exist. * Just software lock out the last 8GB.    The board exists, its already double sided, this would be easy and cheap to do. 1Gb chips would be pretty cheap I imagine. Partners already have this board in production and in supply, it would be very low risk for them. It uses parts that are already in the supply chain. Even if they fit it with faster memory, that likely be shared with other products and historically that is what AMD has done. Faster GDDR6 has been around for years at this stage. No new drivers, just a bit of bios code change. Chinese remanufacturers already do this with Nvidia products without manufacturer support.  A whole new die would be expensive. They can certainly do that but that would take years and create an expensive and niche product.",Neutral
AMD,"I dont think they've ever done any such thing.  Again, can y'all show me the quotes where AMD are saying these things?    I think y'all get all mixed up with rumors and somehow they turn into facts in your heads and somehow it's what AMD said and not just some random person on the internet.",Negative
AMD,"These guys are awesome, what with this crazy memory shortage I don't know what we would do without downloadmoreram.com.   If you're downloading some ram don't forget to use code F*cktheRamCartel at checkout for a sweet 13.37% off your order.",Positive
AMD,I can only find DDR4 there :'(   I wish they had DDR5 as well.,Neutral
AMD,3GB density chips are the new kid in town for gddr7. By mid 2027 it may be the primary density for mass production the same way that 2GB replaced 1GB chips a few generations ago and they stopped producing 1GB chips.   But if 2GB density is still available and significantly cheaper theyll do that and save the 3GB for their pro lineups. It does depend on the cost and whats got the most active production line for gddr7,Neutral
AMD,It's the new standard. I don't think they'll bother with 2GB except for very low end. 3GB modules are also much faster on path to 40gbps and a little further out we get 4GB modules. Not inconceivable that future x60 tier cards go down to 96bit bus. Really depends on how much AMD and NVIDIA can improve cachemem efficiency and little compute progression there is.,Neutral
AMD,I can see a 2028 RDNA 5 refresh with 4GB modules.,Neutral
AMD,"They'll just gimp mem PHYs and offer cards with a little more VRAM. Look at rumoured 18GB AT2 card, although there will prob be a refresh based on 4GB to satisfy the VRAM crowd as u/Dangerman1337 said.",Neutral
AMD,"Sure why not, they'll just put it on the pro lineup and charge double",Neutral
AMD,"Yea well no one predicted the memory shortage not even Samsung, Micron and friends.",Negative
AMD,"That's based on the cut down AT0 config. The full AT0 die caps out at 512bit like 5090, well at least according to rumours.",Neutral
AMD,No it'll be faster than 5090 but it won't be faster than 6090. The next gen Xbox is supposed to reach above 5070ti performance and that'll be a smaller chip on a 192bit bus. Plus RDNA5 is getting big architectural improvements and a die shrink unlike RDNA4 that used the same node as high end RDNA3.,Neutral
AMD,"There's no way the architecture is exactly the same imho, as the typical workloads on CDNA are too different from those on RDNA.",Neutral
AMD,It was written at one point that RDNA is still the gaming architecture (as used in APUs as well) and CDNA the computing architecture but the new platform combining both is UDNA (RDNA5 + CDNA4 = UDNA1) while GCN was a different architecture to RDNA,Neutral
AMD,"Yeah I still think the PS6 is Sony's last traditional 'console'. Everything will be cloud based beyond 2035-2040. They'll want to make this console last a really long time....ie more beefy CPU and RAM initially, with a GPU upgrade 4-5 years down the line.   With the PS5 still selling like crazy, what incentive does Sony even have to launch a PS6 at all until 2029?    I can't see one tbh, devs are only starting to really push the PS5 now.",Neutral
AMD,"They wouldn't have signed contracts more than 2 years out for a console lol. They're still designing the hardware, let alone ordering mass quantities of compliments for it. Again, don't expect PS6 till Q4 2028 at the earliest. Distinct possibility of it being a 2029 launch now as well.",Neutral
AMD,Very unlikely. The G7 memory chips they need have barely begun real HVM. Rn limited to RTX PRo cards and select Mobile on NVIDIA side + you don't sign LTAs this early on.,Negative
AMD,And if they are reinvesting everything - who knows what will happen once the bubble bursts.,Neutral
AMD,"Nah, this too shall pass. The current prices are too big of an opportunity, and no-one can lock up dram with patents. Either demand will fall, or supply rises until margins come back down.",Negative
AMD,RDNA3 was released in DECEMBER 2022. Calling it 2022 is misleading,Neutral
AMD,"AI isn't just unprofitable, it's bleeding money at a ridiculous rate. Another year/4 quarters of massive losses on AI should hopefully lead to *something* changing.   I can't imagine companies just throwing billions into a blender for longer than another year. Microsoft is already scaling back.",Negative
AMD,"Claims, lol",Neutral
AMD,"Nvidia already announced a next gen vera rubin successor to the rtx pro 6000 with 128gb vram for late 2026. At 512 bit, it should have 4gb gddr7 vram chips. leaked sk hynix roadmaps also showed higher capacity vram chips becoming a thing iirc so it makes sense.",Neutral
AMD,">3GB density chips are the new kid in town for gddr7. By mid 2027 it may be the primary density for mass production the same way that 2GB replaced 1GB chips a few generations ago and they stopped producing 1GB chips.  Standard chip capacity goes up in doubles.  3GB is new, but it does not mean it's going to be the new standard.  We had 1.5GB chips as well in between 1GB and 2GB.  These sorts of in-between capacity chips dont tend to get used in consumer GPU's.  Not saying they absolutely wont here, just not sure why people are so incredibly confident they will.",Neutral
AMD,It won't AMD are incompetent,Neutral
AMD,What makes you think any of this?did a reputable leak something about this,Negative
AMD,"That'll be very easy to accomplish. It'll prob match or beat 5080. GFX13 = Massive architectural overhaul, N3P and 70CUs.  Very interested to see what RDNA5 is about. Just a shame it's prob +2 years away unless VRAM mess sorts itself out in early 2027.",Positive
AMD,It won't be exactly the same but there will be more AI tech in the gaming version that would have normally been exclusive to cdna.,Neutral
AMD,"I was under the impression it was on a logic block basis. For example, in the compute market you obviously don't need video out, texture mappers, RT hardware and the like, but you do need SIMD cores, a memory controller, and matrix math cores. With the split architectures they were pulling these universally required blocks from two different bins. A unified architecture means advances for one use can still benefit the other (say a really awesome scheduler or something). If they really double down on chiplets and can optimize things in ways that aren't obvious to someone like me, they might even be able to share some amount of silicon (obviously not all, but I think it can be more than just SRAM).",Neutral
AMD,Then AMD does what NVIDIA does. Makes a shared foundation with extensions on top (logic blocks and cache customizations) for gaming and HPC respectively.,Neutral
AMD,"I don't personally recall seeing that and very well might have missed it, but it does seem to run counter to the UDNA announcement.",Neutral
AMD,"You're prob right, especially considering how good GFN has gotten and how hard most normie gamers have with distinguishing frame-gen on vs off in latency. Add a controller to the equation and that becomes even less relevant.  We'll see just remember that prob even more so than PS4->PS5 PS6 has a very long crossgen period extending well into 2030s before it can shine, even if it launched in 2027 which seems next to impossible rn.",Neutral
AMD,"> if they are reinvesting everything  They are not.  We know from the earnings calls of those big memory suppliers that they are very skittish about building out capacity.  Which also tells you that this shortage is, in their eyes, **very** temporary.  Otherwise, they'd be on the hook for huge lawsuits as shareholders could sue for the missed opportunity.  I have a hunch that DRAM pricing will be a _lot_ better end of 2026 and basically where it is today in mid-2027.",Negative
AMD,"Or DRAM makers suspect this to be a bubble and will gladly take the money now but won't speed up production. If you build fabs beyond what was already planned you have to know they'll still be needed 5+ years from now, not just right this moment.",Neutral
AMD,"DRAM makers do not want to overreact and build up a ton of new capacity to meet demand, leaving themselves extremely vulnerable if that demand ever drops.  Why put themselves at such risk when they can just jack up prices 300%?",Negative
AMD,"Exactly, we have seen this before just not to the extent it is now. Vendors will ramp up production and suddenly there will be a market change and prices will crash. This cycle will likely take longer but it will happen.",Negative
AMD,RDNA 3 was supposed to be earlier than December 2022 originally.,Neutral
AMD,"Oh good lord. smh  Either way, it's a terrible claim.  Nvidia are the ones that (used to) do a 2 year cycle.  AMD has been all over the place, often only doing like one or two new GPU's per new architecture, which could sometimes release every year or so.",Negative
AMD,"Prices went up before supply lost. This is similar to how the middle men of gas and energy will jack up prices of gasoline the same day that crude prices spike. That’s not supply and demand, that’s just market manipulation. We haven’t lost DRAM supply at all, either.",Neutral
AMD,Historically maybe...  If memory stays expensive companies would be more willing to produce in between capacities at comparable $/GB. Because the difference between boards populated with 3 or 4 GB chips would balloon.,Neutral
AMD,"Because there no mention of 4GB production or roadmap.  Right now, 3GB is live in mass production, and they’re already sampling higher speed 3GB chips at samsung, hynix, etc. they’re set on producing 3GB chips. They’re committed to producing 3GB for years. Nobody committed to 1.5GB that’s why it never was a thing it was better to go to 2GB. Nvidia and amd will choose whatever the ram conglomerates choose to make and right now, its 3GB.   https://www.techpowerup.com/news-tags/GDDR7  You can see the news links when scrolling. 3GB is going to stick around for sure. 4GB has no confirmed production yet",Neutral
AMD,SW and marketing wings are run by clowns. The HW team is very capable. Look at RDNA4 and RDNA5 will be the first time they actually bother to put in the work. More budget and for consoles so it'll be good.,Positive
AMD,"No MLID and then Kepler\_L2 didn't contradict any of it. Very early on, hopefully we get more certain and final leaks in 2027.",Neutral
AMD,"Basically what Kepler has already alluded to. Honestly surprised AMD took ML this seriously with RDNA4, but RDNA5 is probably the first time they built an architecture from the ground up around AI, so yeah a ton of learnings and low hanging fruits from CDNA for sure.",Positive
AMD,They could not be sued by shareholders by not increasing capacity. That’s not how fiduciary duty works as they are protected by the business judgement rule.,Neutral
AMD,"Good - And I hope we won't see a build up of coal and other trash sources of energy for ""AI factories"".",Positive
AMD,Same thing with RDNA 4. Really dissapointing how these launches keep getting pushed back.,Neutral
AMD,This ain’t a market spike on crude oil my man.,Negative
AMD,">4GB has no confirmed production yet  It does, Rubin CPX needs it.",Neutral
AMD,"Some of the ""software"" issues of RDNA 2 and 3 were hardware issues that software couldn't fix. It's not just a matter of one part of the team being good and the other bad. I can remember at least one issue with the video encoder outputting the wrong resolution being a hardware issue.",Negative
AMD,"Thanks for clearing that up, I always thought leaving an ""obvious"" money maker on the table would be something shareholders can pressure the C-level to do.",Neutral
AMD,I’m sorry you don’t understand what an allegory is,Negative
AMD,A lot changed with RDNA 4. But yeah prev stuff was crap on HW side as well.   I was mainly referring to SW feature set deficit and bad pricing.  RDNA 5 better be good. All that poached talent and hiring has to result in something meaningful.,Negative
AMD,"Do you not see the circular logic here? It's only obvious if it's obvious. Clearly it's not obvious, else people would be in broad agreement.",Negative
AMD,"The shareholders usually have the power to call a meeting, to ask the c level to course-correct or in rare cases replace them. They might then sue for damages that occurred while that process takes place, but unless there was obvious neglect, not that certain to be awarded.  Edit: to look at it another way, if they could sue for damages easily, where would the money come from? The c level is only personally responsible in rare cases (otherwise nobody would want to do the job). It could come out of the company, but that is just the shareholders giving themselves a dividend, weakening the company.",Neutral
AMD,"Apologies in advance, this is a long one. Some results below (give the site a visit to view all)  **nT**  |CPU + GPU + RAM Config|Score| |:-|:-| |9970 (PBO +200), RTX 5090|18,278| |9950X3D, RTX 5090, 64 GB|9,641| |14900KS, RTX 5060 Ti, 16 GB|9,563| |9950X, RTX 5090, 64 GB|9,235| |9950X3D, RTX 5080, 32 GB|9,166| |275HX, RTX 5090 M, 64GB|8,490| |265KF, RX 9070 XT, 48 GB|8,409| |265K, RTX 4090, 48 GB|8,316| |7945HX, RTX 4090 M, 32 GB|7,474| |M4 Pro (14C), 20C GPU, 48 GB|6,812| |14700K, RTX 4090, 32GB|6,425| |13700K, RTX 4080, 32 GB|6,206| |12900K, RTX 4090, 32 GB|6,024| |5950X, RTX 5080, 64 GB|5,846| |13600K, RX 9060 XT, 64 GB|5,271| |9800X3D, RX 9070 XT, 64GB|5,122| |9800X3D (65W), RTX 5080, 32 GB|5,104| |9700X, RTX 2070 S, 64 GB|4,875| |7800X3D, RTX 5080, 32 GB|4,134| |M2 Pro, MacBook Pro 16""|4,121| |11900K, RTX 5070 Ti, 64 GB|4,017| |M4 (10C), 10C GPU, Mac Mini|3,858| |13400F, RTX 4070 Ti, 32 GB|3,750| |M4 (10C), 10C GPU, 24 GB, Mac Mini|3,723| |5800X3D, RTX 5070 (OC), 32 GB|3,698| |X1E80100, Adreno X1-85, 16GB|3,504*| |5800X, RX 9070 XT, 32 GB|3,485| |11900KF, RX 7080 XT, 32 GB|3,417| |5700G, Vega 8, 64 GB|3,305| |5800X, RX 7800 XT, 32 GB|3,297| |5800X3D, RTX 5080, 32GB|3,027| |9900KS, RTX 2080 Ti, 32 GB|2,988| |5700X3D, RX 9070, 32 GB|2,799| |M3 (8C), 10C GPU, 16 GB|2,689| |7640U, 760M, 32 GB|2,367| |6600H (45W), 16 GB, Beelink EQR6 Mini|2,105|",Neutral
AMD,"Apologies in advance, this is a long one. Some results below (give the site a visit to view all)  **1T**  |CPU + GPU + RAM Config|Score| |:-|:-| |M4 (10C), 10C GPU, 24 GB, Mac Mini|677,0| |M4 Pro (14C), 20C GPU, 48 GB|667,0| |M4 (10C), 10C GPU, Mac Mini|653,0| |9950X, RTX 5090, 64 GB|567,0| |M3 (8C), 10C GPU, 16 GB|567,0| |265KF, RX 9070 XT, 48 GB|560,0| |14900KS, RTX 5060 Ti, 16 GB|559,0| |9950X3D, RTX 5080, 32 GB|553,0| |9970 (PBO +200), RTX 5090|537,0| |9700X, RTX 2070 S, 64 GB|533,0| |9800X3D (UV), RTX 5090 (UV), 64 GB|530,0| |9800X3D (65W), RTX 5080, 32 GB|528,0| |275HX, RTX 5090 M, 64GB|524,0| |9700X, RTX 5090, 64 GB|514,0| |14700K, RTX 4090, 32GB|510,0| |13700K, RTX 4080, 32 GB|495,0| |M2 Pro, MacBook Pro 16""|480,0| |7945HX, RTX 4090 M, 32 GB|473,0| |13600K, RX 9060 XT, 64 GB|471,0| |12900K, RTX 4090, 32 GB|460,0| |7800X3D, RX 9070, 32 GB|444,0| |11900K, RTX 5070 Ti, 64 GB|438,0| |X1E80100, Adreno X1-85, 16GB|438,0*| |13400F, RTX 4070 Ti, 32 GB|430,0| |11900KF, RX 7080 XT, 32 GB|417,0| |5800X, RX 9070 XT, 32 GB|396,0| |5950X, RTX 5080, 64 GB|390,0| |7640U, 760M, 32 GB|377,0| |5800X3D, RX 9070 XT, 32 GB|369,0| |5700G, Vega 8, 64 GB|367,0| |6600H (45W), 16 GB, Beelink EQR6 Mini|338,0| |9900KS, RTX 2080 Ti, 32 GB|333,0| |5700X3D, RX 9070, 32 GB|303,0| |5800X3D, RTX 5080, 32GB|272,0|",Neutral
AMD,"Apologies in advance, this is a long one. Some results below (give the site a visit to view all)  **1T w/ SMT**  |CPU + GPU + RAM Config|Score| |:-|:-| |9950X, RTX 5090, 64 GB|763| |9950X3D, RTX 5090, 64 GB|748| |14900KS, RTX 5060 Ti, 16 GB|742| |9970 (PBO +200), RTX 5090|740| |9800X3D (65W), RTX 5080, 32 GB|733| |9950X3D, RTX 5080, 32 GB|730| |9700X, RTX 5090, 64 GB|724| |9800X3D, RTX 5070 Ti, 64 GB|710| |14700K, RTX 4090, 32GB|680| |13700K, RTX 4080, 32 GB|647| |13600K, RX 9060 XT, 64 GB|625| |7945HX, RTX 4090 M, 32 GB|620| |12900K, RTX 4090, 32 GB|602| |7800X3D, RTX 5080, 32 GB|570| |13400F, RTX 4070 Ti, 32 GB|563| |11900K, RTX 5070 Ti, 64 GB|533| |7640U, 760M, 32 GB|518| |11900KF, RX 7080 XT, 32 GB|512| |5800X, RX 9070 XT, 32 GB|497| |5950X, RTX 5080, 64 GB|486| |5800X3D, RX 9070 XT, 32 GB|464| |5700G, Vega 8, 64 GB|462| |6600H (45W), 16 GB, Beelink EQR6 Mini|429| |9900KS, RTX 2080 Ti, 32 GB|420| |5700X3D, RX 9070, 32 GB|411| |5800X3D, RTX 5080, 32GB|304|",Neutral
AMD,"Apologies in advance, this is a long one. Some results below (give the site a visit to view all)  **GPUs**  |CPU + GPU + RAM Config|Score| |:-|:-| |9800X3D, RTX 5090, 64GB|166,885| |Ryzen 9 9950X, RTX 5090, 64GB|160,564| |9700X, RTX 5090, 64 GB|157,538| |14700K, RTX 4090, 32GB|153,707| |9800X3D, RTX 5090, 64 GB|153,422| |265K, RTX 4090, 48 GB|146,045| |7800X3D, RTX 4090 (UV), 32 GB|142,168| |9950X3D, RTX 4090 (315 W), 64 GB|140,077| |9800X3D, RTX 5080, 32 GB|124,739| |13700K, RTX 4080, 32 GB|110,815| |9800X3D (65 W), RTX 5080, 32 GB|110,233| |5800X3D, RTX 5080, 32GB|107,994| |275HX, RTX 5090 M, 64 GB|106,856| |5950X, RTX 5080, 64 GB|104,847| |275HX, RTX 5090 M, 64 GB (HP OMEN MAX 16)|103,947| |9800X3D, RTX 5070 Ti, 64 GB|101,697| |11900K, RTX 5070 Ti, 64 GB|101,600| |7945HX, RTX 4090 M (Legion 7 Pro)|98,094| |285K, RTX 5070 Ti, 128 GB|97,859| |13700K, RTX 3090, 64GB|91,266| |9950X3D, RTX 3090, 32 GB|84,942| |5800X3D, RTX 5070 (OC), 32 GB|78,660| |13400F, RTX 4070 Ti, 32 GB|77,851| |14900KS, RTX 5060 Ti, 16 GB|62,959| |265KF, RX 9070 XT, 48 GB|55,898| |5800X, RX 9070 XT, 32 GB|49,156| |9800X3D, RX 9070 XT, 64 GB|48,769| |7950X3D, RX 9070 XT, 32 GB|48,123| |5700X3D, RX 9070, 32 GB|41,898| |5800X3D, RX 7800 XT, 32 GB|40,789| |5800X, RX 7800 XT, 32 GB|40,769| |9900KS, RTX 2080 Ti, 32 GB|38,859| |11900KF, RX 7080 XT, 32 GB|38,213| |M4 Pro (14C), 20C GPU, 48 GB|36,881| |9950X3D, RTX 3060, 96 GB|35,615| |5600X, RX 6800, 32 GB|35,349| |5700X, RX 7700 XT, 32 GB|30,313| |13600K, RX 9060 XT, 64 GB|29,954| |7800X3D, RX 9060 XT, 32 GB|29,122| |9700X, RTX 2070 S, 64 GB|27,940| |5800X3D, RX 6700 XT, 32 GB|25,563| |14400F, RX 6600, 32 GB|18,313| |M4 (10C), 10C GPU, 24 GB (Mac Mini)|17,996| |M4 (10C), 10C GPU (Mac Mini)|16,054| |M2 Pro (MacBookPro 16"")|13,425| |M3 (8C), 10C GPU, 16 GB|12,268| |7640U, 760M, 32 GB|5,054| |5700G, Vega 8, 64 GB|0| |4750G, Vega 8, 64 GB|0| |X1E80100, Adreno X1-85, 16GB|N/A|",Neutral
AMD,"Their form + Maxon has it termed properly, but Computerbase graph suddenly mixes up the terminology. Maxon is accurate.  |Test|Maxon Terminology|Computerbase Terminology| |:-|:-|:-| |One Thread: 1T|Single Thread|Single Core| |Two Threads: 1C2T|Single Core|Single Core + SMT| |All Threads: nT|Multiple Threads|Multi Core|  We shouldn't use ""Single Core"" to mean two things. Maxon is much clearer.  And, before we have an endless debate: Maxon's Single Core test ***is*** a multi-threaded benchmark test. It limits the thread count to 2. Cinebench pushes *two* parallel instruction streams to the CPU.  Hopefully Computerbase and other outlets stick to Maxon's wording. Or just use the numbers: 1T, 2T (SMT), and nT.  An old but good read from AnandTech:  [https://web.archive.org/web/20221006033815/https://www.anandtech.com/show/16261/investigating-performance-of-multithreading-on-zen-3-and-amd-ryzen-5000?utm\_source=twitter&utm\_medium=social](https://web.archive.org/web/20221006033815/https://www.anandtech.com/show/16261/investigating-performance-of-multithreading-on-zen-3-and-amd-ryzen-5000?utm_source=twitter&utm_medium=social)",Neutral
AMD,"who cares, just use stockfish",Neutral
AMD,"SMT doesn't automatically mean 2T as there are implementations with more than two (even if not common).  Single Core+SMT is more accurate (but multi-core for nT isn't very good either) so I'd skip both terminologies in favor of being specific (i.e., 1c3t if it's a 3 thread SMT single core, 2c4t, 4c4t, etc.).  In the context of this benchmark, 1T, 1C (or SMT) and nT probably makes the most sense.",Neutral
AMD,">SMT doesn't automatically mean 2T as there are implementations with more than two  Fair, but in my experience, those *usually* use more specific names: SMT**4**, SMT**8**, SMT**16.** I imagine it's unlikely any of those CPUs will work with Cinebench 2026:  >Cinebench 2026 will not execute on unsupported processors.  But I agree with you: the best choice is just using the numbers of how many threads (and whether CB is setting affinities to ensure they're localised to one core or it accepts whatever the CPU allocates). The number of threads removes all confusion & is universally understood across languages.",Neutral
AMD,"Not impossible, but very unlikely. AMD is currently focused on upcoming Zen 6, and I guess their engineers are working on next gen CPU / chipset / AM port. Plus there are probably constraints with TSMC schedule, I think you have to ""reserve"" quite some time in advance for any order.",Neutral
AMD,"Probably the simplest would be to manufacture 5800x3d again.   (Newer cpus have ddr5 controller, they would have to redesign architecture.)",Neutral
AMD,No.  Considering the tech news from the last few months apparently there isn't much money in the consumer market for something like this.,Negative
AMD,"Maybe, but I'm starting to think they want people to sell off their old stuff. Just a theory of course.",Neutral
AMD,"the thing is you can still get 32gb of ddr5 for two or three hundred bucks. it won't be a great bin, and definitely won't have the overclocking potential of high end hynix a-die but frankly the vast majority of people can/will just run JEDEC and not be able to see the difference. That may be easy for me to say sitting on 64gb of a-die running tight timings, but you/I seriously don't need high end ram to build a pc and run games flat maxed out. Prices have gone up but generally if you could afford to build a PC before the rampocalypse you can still do so now, you're just going to have JEDEC speeds. Which are actually, totally fine if you're using your pc and not a hardware snob (which I am admittedly). ​",Negative
AMD,"Last AM4 cpu that got released is the Ryzen 5 5600F, which happened september. So only 3 months  I wouldnt be surprised if AMD comes up with something new or reintroduce 5800x3d and 5700x3d (these cpus only existed for a year or year n half. Which is very short)  Its been 9 YEARS since AM4 got released. AMD still releasing cpu's for it.. crazy",Neutral
AMD,No  Not enough volume for amd to restart production,Negative
AMD,"The production of the X3D variant of the core chiplets has been discontinued. Presumable it was done so that they can use those lines to produce Zen 5 X3D variant. So unless you want them to discontinue the current gen to put old already somewhat obsolete generation back into production, it's not viable idea.",Negative
AMD,"If I were a top AMD executive I'd be focusing on these things  1. Getting Radeon THERE for AI purposes   2. Making the EPYC line amazing for data centers   3. Finding ways to optimize costs and cut risks     Targeting budget customers is fairly low on any list I'd have.    There's probably some value in keeping Zen 3 dies in production but they'd get minimal priority for anything new or cutting edge. Minimal development efforts. Zen 4 is already getting ""old"" by industry standards. There's not much point to getting anything newer to work with AM4 IODs either.",Neutral
AMD,5950X3D? Or even 5950X3D2?,Neutral
AMD,"Not only is it not likely for reasons of moving forward instead of back, you'd also crash the ""market"" the minute there's any supply, and it would likely spike prices on AM4 boards and DDR4 ram. The lack of demand is the only reason AM4 boards and DDR4 are relatively cheap right now. Can't get good AM4 CPUs, so the rest of the linked prices slump. Bring back competitive AM4 CPUs and the prices shoot up to match, and now AMD is stuck trying to sell ancient and EOLed designs into a market that's already buying every AM5 CPU they make.",Negative
AMD,"First of all, never trust inflated prices on eBay. It's well known that scam artists who have hordes of collectibles or scalped/limited items who will orchestrate sales of items at inflated prices to drive up the market. Yes, they have to eat the cut that eBay takes, but if all of a sudden they have 10-50 items that they can sell for 20-100% more, it's worth it.   Like what people have been doing with VHS tapes on eBay for the last 6 years.   Secondly, AMD hasn't stopped releasing CPUs for AM4. They recently released the 5500X3D for the Brazilian market in June, and the 5600F in others.   The biggest issue is there's only a handful of AM4 boards still for sales and DDR4 RAM in the retail channels is almost entirely gone. Sure you can find used RAM, but minting new products that requires parts that are no longer in production is suicide.   Micron recently said they'd keep making DDR4, but I ain't seeing much available in the US. I assume much of that is still being [shuffled to the enterprise market.](https://prerackit.com/memory-markets-in-turmoil-how-chinas-exit-from-ddr4-manufacturing-triggered-a-server-ram-pricing-crisis-in-2025/)",Negative
AMD,Wafers aren't just sitting on shelf to be bought when in demand. They're order well in advance. It's unlikely they resume production of the 5700x3d which old got disconnected a few months ago (new AM4 cpu is completely put of the question) as it's a step backwards and a gamble that people would still be interested in AM4 several months time instead of shelling out a bit more for significantly higher preformance gains from their next generation of cpus in November.,Neutral
AMD,">It got me thinking, is 5000 series AM4 on an old enough node that AMD could restart production cheap? Cheap enough to sell a high end x3d chip to satisfy people holding on to their old platform and RAM while the shortage is happening?  Why this will probably never happen:  * The vast majority of people do not upgrade their CPU. The entusiaste market is probably less than 1%. There is not enough volume. * Selling this processor will cannibalize their own current product line. * The stacked L3 cache could be better utilized on their current CPUs. * All fab production is concentrated on storage, ram, new CPUs and GPUs. New and Old.  The echo chamber that is PC enthusiasts has seriously distorted their perception of the PC market.",Negative
AMD,Before the AI bubble I would have said no way. With the AI bubble making new unaffordable for the next 12-18 months at a minimum I'd say maybe.  People on AM4 aren't going to buy AM5 if they have to spend 200% of their budget to get it. DDR5 RAM prices are really going to cripple the PCbuild industry for the next year.,Negative
AMD,"There was some rumors that Zen4 was originally going to come to AM4. They could still do it. Maybe a lot of that work is already done. I'm skeptical there is still much DDR4 in production, and it'll soon all be gone. What we got on desktop was mostly left over stuff that servers didn't want. I don't think they would make these CPUs just for consumer when there is so much more profit in putting that silicon towards servers. If Mircon abandoned Crucial memory for consumers, that to me says a lot about where the real focus is for hardware makers. AMD right now just doesn't care much about desktop anything.",Neutral
AMD,"Yes, they have actively done so in the last year and are being pushed to re-release the 5800x3d by retailers.",Neutral
AMD,"Very unlikely and most probably something on the lower end I’d think.   But a 5950X3D would be incredibly cool though, and a nice upgrade for my 5700X3D. 😄  On the other hand, there are a lot of people out there, me included, on AM4 that aren’t planning to upgrade to AM5 any time soon. So that would be a way for AMD to keep selling CPUs to existing AM4 users of whom they otherwise wouldn’t make money from.",Positive
AMD,"From one side Zen 6 has to sell but RAM/(SSD) prices may block AM4 to AM5 upgrades (how many still on AM4?). Then if going AM5 is to expensive then people may wait till Zen 7 which may be AM6 and if it's revealed early that Zen 7 is AM6 then it will decrease AM5 interest even more.  Zen 6 will launch ""late"" 2026, so 2027 is the year it will be in mass availability. RAM prices may be more sane at that time. Especially when people will be waiting for X3D (if the base variants get rekt again by existing X3Ds).  Refresh of 5800X3D wouldn't hurt but I doubt they would be offer more without a longer development cycle (a new design) after which it may turn out it's all for nothing.",Neutral
AMD,they do release 5000 series x3d chips but using leftover chiplets that were binned too low for the expensive parts. ie the 5500x3d. its not in production anymore.,Neutral
AMD,No - production has already stopped for the 5000 series. It would be prohibitively expensive to have TSMC manufacturer last-gen chips in another run.,Negative
AMD,"Would be nice when the 5700x3D and 5800x3D would be produced again, but very unlikely.",Positive
AMD,Sure they could. AMD or Intel could make a modern chiplet CPU with a memory controller that runs DDR400.   There just isn't any financial incentive to do so.,Neutral
AMD,"they did release a ""new"" one this year, the 5500x3D for latam.",Neutral
AMD,I hope they do. Been thinking of shifting from Intel to AMD for a long time now.,Neutral
AMD,Why would they do that when they can just sell a new x3d chip at inflated prices and have every single one of them sold before they've even been shipped?,Negative
AMD,they just released the 5600F in September.,Neutral
AMD,"AM4 is just too good for me to ditch. My 6800XT holds it back in 4k gaming, 64GB 3600Mhz CL16 was dirt cheap and both single threaded and multithreaded performance with my 5950X is more than I really need.",Positive
AMD,"Considering they launched the Ryzen 5 5500X3D only 6 months ago, i would say they can, the problem is if they will. Unfortunately it can take a long time to design or adapt existing designs, test and then manufacture them in sufficient numbers for release, if they started now it would probably take a year or more to see them on the market. A 5950X3D does sound cool as hell, maybe i would upgrade from my current 5700X3D, though this CPU is perfect for my needs and the gaming i do.",Neutral
AMD,Imo the 5950X3d is the most likely option as the very last cpu on am4. dual 3XD ofc and with people starting to switch to more expensive and advanced packaging then Cowos there will be enough cowos allotment that amd can buy for older designs,Neutral
AMD,"Restart? They never stopped releasing them, with the latest being the  5500X3D released in June this year.",Neutral
AMD,"DDR6 is coming out soonish in 2027, so when that happens the cutting edge ai shit will all switch over to that. So ddr5 availability should go up permanently after that. It's gonna be a dry ass desert until then but it won't be forever. DDR5 came out in 2020 so it's already fairly old, making a switch back to 4 even temporarily quite unlikely.",Neutral
AMD,"The only one that would *maybe* make sense, and I stress it's a biiig maybe at this point, would be a 5950X3D. After that, no more AM4 anything. Otherwise, probably best to keep on cranking 5800X3Ds.",Neutral
AMD,"You can get a 5700x3D, which is a lot cheaper.",Neutral
AMD,"They could mix tiles, but would be a hard sell for motherboard partners. And why buy a am5 x3d when am4  x3d gives almost the same gaming performance?",Neutral
AMD,"They can restart Zen 3 X3D production, with a dual CCD V-Cache 5950X variant and should do so IMV.",Neutral
AMD,AM4 Zen 6 Fan Edition backport manufactured on an Intel node,Neutral
AMD,"Dont think they're advocating for proper new designed chips, just respinning up old ones.    As for TSMC, I'm sure if AMD really cared to do this, they could find some way to give up some more leading edge capacity for older node capacity.  Perhaps via agreement with some other company who would love to bump themselves up the waiting list.    I think bottom line is that AMD isn't gonna be overly concerned with things.  They're still gonna be making lots of money on all this AI stuff themselves selling CPU's and GPU's, they can take hit on the consumer side for a bit.    I also think paying high prices for AM4 processors is very stupid.  While DDR5 has certainly ballooned building on AM5 platform, the reality is that total system costs are still only gonna be like 15-20% higher.  Small enough difference where it will probably still be worth it to go with AM5 in the big picture.",Neutral
AMD,"Two years in advance, I believe...",Neutral
AMD,"seeing ""Zen 6"" is crazy when i still am thoroughly satisfied with my Zen 2 3900X. Although i suspect it's finally showing some wear.",Positive
AMD,"There are actually older Zen3 parts with the DDR5 controller, too. As I understand it, Zen cores are largely decoupled from the wider system with their IO die interfacing with the various external components, so that may be the only bit which really needs changed to support one platform it another.",Neutral
AMD,"Part of me wants to believe it would be possible for AMD to use the old DDR4 IO die and pair it with newer compute dies with Zen 4 or 5, but even if that was true they have no financial incentive to do it.",Neutral
AMD,Or variants of it we never got (5900X3D or a 5950X3D).   It's not like TSMC 7 and 6 have companies fighting each other for their wafers at this point.,Negative
AMD,Im down,Neutral
AMD,"Also, the current price explosion of ddr5 is a bubble by an overleveraged market with inelastic demand that will likely be mostly gone by the time any of these new models would make it to the market.",Negative
AMD,The consumer market is waiting for 8k monitors with high refresh rates. And cpus and motherboards with connections that support that.,Neutral
AMD,They might want it but the rest of us want DDR5.   Nobody's getting what they want in 2026 except the billionaires.,Neutral
AMD,"They want money, and they will do whatever it gives them. Old stuff is sold, makes no profit, so yeah they want more sales. Anyways, stick to ddr4 and if ddr5 demand falls then they will have to adjust.  W11 EOL+ Crypto+ AI ... That is tiny compared to 9.000.000.000 humans that use a pc.",Neutral
AMD,And X3D doesn’t have too much to gain from “good” RAM.,Neutral
AMD,"What about people who just want an upgrade? I would pay a few hundo to get best in-slot CPU on my current platform for a quick bump to give me another year or two.  Right now that's not possible, 5800x3d is no longer manufactured and 2nd hand is prohibitively expensive...  Why not fill that gap with brand new silicon?  The question is - is there fab capacity to make it?",Negative
AMD,It was pretty clear they were dumping and clearing out stock when the 5500X3D launched during the summer.   EPYC with Zen3 and X3D is probably EOL now. Which is the main reason why AMD kept the desktop parts around as well.,Neutral
AMD,"5100x3d in 2 years, trust",Neutral
AMD,"Gonna be plain, with IBM and Cisco starting to pivot, avoiding too much waste of Radeon dev time on AI crap may end up being wise.",Neutral
AMD,5500X3D is just a stockpile of chips that couldn't be sold as better chips.   AMD likely was building that stockpile ever since launch.,Negative
AMD,Assuming that the AI crap doesn't crash before then.,Negative
AMD,ddr5 is unavailable becuase dram manufacturing is not miang the ddr5 chips that go into desktops and increasingly not even ddr5 chips that go into servers. why do you think ddr6 would be any different?,Negative
AMD,Even these have gotten insane. Used 5700x3Ds used to go for 150-175 eur. Now they're 300..,Negative
AMD,u/bobalob_wtf there isn't enough demand to justify a new chip SKU for better raw performance or performance per watt.  It is more likely that they'll do a new manufacturing run of an old SKU.  Will it be cheaper? They may pass on the savings of the older process node to you if it is a competitive advantage.  If now stocks of current SKUs are unavailable then buy used?,Negative
AMD,"I think you're right. AM4 was a great, long-lived platform--I'm still on it myself--but I don't think anyone building now should really be looking at AM4.  Maybe build with 16 GB RAM if you're really on a tight budget, and start with an R5 7600, and upgrade to Zen 6 and more RAM when prices come back down out of the stratosphere.",Neutral
AMD,">As for TSMC, I'm sure if AMD really cared to do this, they could find some way to give up some more leading edge capacity for older node capacity  Is the 7nm node fully booked?  Would AMD have to give up anything to get more of it?",Neutral
AMD,"Nostalgia is one hell of a drug. We saw the same with people exaggerating the longevity of Sandy Bridge during the pandemic. Both SB and Zen3 have been absolute champs, and the latter is mostly great if you already have it, but some people are close to deluding themselves because of desperation over prices and lack of realistic options to build new in the current situation.",Negative
AMD,"Those 15% are a 200 dollar difference for a 32 GB computer. That can be a very significant fraction of disposable income, and the benefits of a DDR5 CPU aren't that big of a deal in this price range anyway.",Neutral
AMD,"the mobile chips have a bunch of zen versions (2,3,3.5,4,5) running ddr5/lpddr5",Neutral
AMD,"> There are actually older Zen3 parts with the DDR5 controller, too.  Ye and Zen 4 with DDR4 controller from Zen 3 as well. They both tested the new IO die with the old architecture and Zen 4 with the old IO die during development.",Neutral
AMD,"For the silicon itself no, but if you want X3D parts packaging is the bottleneck.",Neutral
AMD,The average consumer doesnt even have a gpu that can run 2k. What are you talking about,Negative
AMD,"Not really, it's closer to less than 2billion people that have a pc and almost all of that is gonna be a basic pc that isn't upgraded. Like with everything a company makes way more profit on a overpriced ""pro"" ai chip than a cheap user version and when you have a trillion dollar order for ai gpus you will have that 50k dollar ai gpu be prioritized over a 1k consumer gpu.  The real problem is that these orders are for speculative demand and so they are just selling all of their future product without regard to how much will actually be needed.",Negative
AMD,they definitely help a lot.,Positive
AMD,"where were you last year when they were practically giving away 5700x3d on ali express? it was very clear at the time it was a very limited time deal as the chips were out of production and AMD was just using up all the silicon that didn't bin well enough to be a 5800x3d. if you were ok with your current performance and it wasn't worth the time when fantastic AM4's were only $150 shipped, why FOMO and panic now? In any case anyone building \*now\* can still get a 7500F which will out perform any AM4 CPU for $150, paired with whatever mobo is cheapest and JEDEC tier ram, then upgrade the CPU and ram in the future and be sitting pretty. Fab space is booked up for years solid, scheduling a brand new run specifically for people who missed the boat on a cheap drop in AM4 life extension isn't going to happen and wouldn't be economical for anyone if it did.  Also, are you aware you can currently just get a 5900x from ali for $250 all in? That's your one step AM4 life extension solution right there, they're available, and quite good chips. 12 core, 4.8Ghz boost, they're more than enough to get you a few more years.",Negative
AMD,"> Right now that's not possible, 5800x3d is no longer manufactured and 2nd hand is prohibitively expensive... >  >  >  > Why not fill that gap with brand new silicon? >  >  >  > The question is - is there fab capacity to make it?  And would it be profitable for AMD to release it at prices that you and others wouldn't consider prohibitively expensive?  I took a quick look at ebay, and it looks like the 5800x3d is going for just under $500, and the 5700x3d just under $350.  What price would those CPUs have to sell for new for you to consider them a good buy?",Neutral
AMD,It was pretty clear they were making brand new silicon when the 5800XT and 5900XT dropped last year. They might have stopped this year despite several new AM4 releases but availability makes me think they never stopped at all.,Neutral
AMD,"If AMD handled 10% of nVidia's output, they'd basically 2x their market cap.   There are crazier gambits to take.",Neutral
AMD,"Yeah exactly, they were all the ""bad"" 5600X3D yields which are all just 5700X or 5800X bad yields too slapped with an 3D V-Cache die on top. In no world is AMD going to TSMC and purchasing wafers to make exclusively 5500X3Ds or any other old chip. They especially wouldn't bother purchasing wafers for some transient rise in DDR5 and to give customers on older platforms upgrade options. It also takes years to do that sort of stuff and thats using old process nodes and architectures.  Even backporting a new architecture to an old platform would require re-validating for a new platform and for DDR4 memory now, getting partners to release BIOS updates and to get them on board etc. It's not that simple to just move Zen6 to AM4 for instance by replacing an I/O die and calling it a day. I can't imagine AIBs being happy about losing new motherboard sales either.",Negative
AMD,They will only use ddr6 because it's much better but current producers will keep making ddr5 for a while because it's in demand and will sell. It's only a problem because we stopped making ddr4 because there wasn't much demand for it once ddr5 was cheap and widely available. If this crunch happened last year before ddr4 production was wound down it wouldn't be a huge problem either. This is the worst timing.,Negative
AMD,"a used 5700x3d is 300€ and a used 5800x3d is 350-400€. It is just not worth it, you can buy a brand new 14600k+a decent mobo for that amount.",Negative
AMD,"Hey Admirable_Bid2917, your comment has been removed because it is not a trustworthy benchmark website. Consider using another website instead.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Negative
AMD,"The exception is if you already have solid DDR4. In that case, it can be a pretty good idea to make a new AM4 build.",Positive
AMD,"> I don't think anyone building now should really be looking at AM4.  You're joking, right? As if anyone aside from Brad Pitt's, Elon Musk' kids or some wealthy politician could even afford actually *newly* released stuff these days, given all the price-increases …",Negative
AMD,"Maybe, maybe not.  But I definitely think they could figure out something to get it no matter, given leading edge manufacturing tends to be more desirable overall.",Positive
AMD,"I am pretty sure AMD still has 7nm booked for products like the console chips (PS5, Xbox Series) and some long running embedded designs. How much they could reallocate to old Zen 3 AM4 designs is an other question.",Neutral
AMD,"Sandy Bridge's longevity wasn't exaggerated, that arch absolutely slapped till 2020. Basically if you had an OC'd 2600K or something you were sitting on that till Intel 12th gen for a great improvement in ST perf. Or you bought Zen3. If you needed MT perf from Sandy Bridge, you likely upgraded to Zen2 or Intel 10th gen for a nice leap in performance. If you really needed MT perf gains, you were likely on Ryzen or Intel Extreme chips anyways almost every new gen.",Positive
AMD,>We saw the same with people exaggerating the longevity of Sandy Bridge during the pandemic.  Did we? Personally I was on Ivy Bridge until 2022,Neutral
AMD,"$200 difference in a $1000-1200 build isn't insignificant, but it's also probably not worth hobbling your system over, either.  Going AM5 gets you better performance for the entirety of its life, it gets you upgrade options in the future, and it also gives you better longevity so that no matter what, better stuff will be available by the time you actually do really want/need to upgrade again.  $200 over a five year ownership period is just $40/year(or like $3/month).  I think people should remember to consider this kind of perspective on things when buying a PC, at least for anybody who cares about getting good overall value.",Neutral
AMD,"I mean....that's pretty much true for everything now though, isn't it?  Given how Sammy's more or less fucked the entire market, not just DIY, and some hyperscalers are finding it pretty damn hard to jump to newer EPYC platforms, I can see there being enough demand to justify firing the Zen 3 X3D line up for a little bit.",Negative
AMD,"I am talking about upgrading from current consumer tech.   Current tech can easily run 2k monitors just fine, even with APUs. My point was not about GPUs specifically, nor about gaming, it was about 8k monitors. Dual resolution monitors are a thing, you know.",Positive
AMD,[TechSpot/HUB tested with 6 different RAM kits](https://www.techspot.com/review/2915-amd-ryzen-7-9800x3d/)  There was less than 2% variance between them.,Neutral
AMD,"5900x sounds perfect for my needs, thank you :)  Why are 5800x3d going for silly prices then? Just random market insanity?",Positive
AMD,People are gonna cry and want $200 5800X3D forgetting it had an MSRP of $450...,Negative
AMD,X3D is a separate production line with its' own constraints. There is limited packaging available for X3D that is not shared with the normal SKUs.  X3D being EOL is not tied to the normal Zen 3 SKUs. The normal SKUs they can shurn out as long as there is demand from both server and desktop.  X3D however might very well be supply constrained. And making lower margin Zen 3 variants might cut into scaling Zen5 X3D SKUs.,Neutral
AMD,"They'd be wise, in that scenario to focus on GPPU stuff and only have AI gains that come with overall uplift because, well, bubble.",Neutral
AMD,"Yep this is exactly the case some people find themselves in. They have something like an i5 6600K or 7600K which are 4c/4t CPUs, completely outdated in 2025. Yet they have 16GB or even 32GB of DDR4 that is still usable! Upgrading to something like a Ryzen 5500, 5600 or 5700X on a cheap B450 or B550 board is a huge upgrade for very little cash, and they spend $0 on RAM as they're reusing what they already have.",Negative
AMD,"Sure, but the majority of consumers for most of AM4's life cycle were on 16GB of RAM. [Go look at Steam Hardware Survey in May 2021](https://web.archive.org/web/20210512095214/https://store.steampowered.com/hwsurvey/Steam-Hardware-Software-Survey-Welcome-to-Steam?platform=pc) at the height of AM4's popularity and performance leadership (well into Zen3) only 12% of Windows systems had more than 16GB and people were still rolling with Intel too back then. You'd be hard pressed to find anyone with 32GB of RAM on AM4 really in their old systems if they're still using them. 32GB really only became very prolific with AM5 and Z690 thanks to DDR5 density. I guess if you're okay with having 16GB of RAM it would be okay, but at that point might as well just sit on one stick of 16GB DDR5 or 2 8GB sticks of DDR5 till this whole AI memory shortage blows over.",Neutral
AMD,If it's solid DDR4 it'll probably fetch a good price at the second hand market.,Positive
AMD,"There's a whole 30%ish of the American population (loosely college educated several years into their career and moderately successful small business owners) that are doing better than ever.   The median person (not highly educated, minimal to moderate career development) might be feeling financial pressure, but there's enough of the top third (100 million people) RIFE with cash to prop up entire industries.      There's a lot of people for whom a new computer every few years is something like 1-3% of their disposable income.",Positive
AMD,"They could reallocate whatever they like, I suspect.  But I think it's *very* unlikely there will be any significant production of AM4 X3D SKUs.  X3D was created for Epycs, with the consumer parts being essentially the bin rejects - and who the hell is buying Zen3-based Epycs now?  AMD will carry on producing AM4 CPUs for a while - they're cheap to make, thoroughly supported and more than good enough for a pretty wide range of use cases.  I'm still using one myself as a daily driver and it's absolutely fine.  But X3D is an expensive process and AMD aren't going to restart an obsolete version of it to produce low-price consumer SKUs.",Neutral
AMD,"And I'm still on Ivy Bridge today with my 3570k.    But I'm also under no illusions that my system is massively outdated and has been for quite a while and that playing most of the latest heavier hitting games is basically a total impossibility.    You and I were just being cheap/patient bastards.  It had nothing to do with our CPU's genuinely being great CPU's up through 2022, much less today.",Negative
AMD,"> I mean....that's pretty much true for everything now though, isn't it?  Not for stuff made with traditional packaging tech",Neutral
AMD,"You are vastly overestimating the prevalence of ""spec out the CPU socket type for the PC I'm building"" hobbyists as a percentage of the consumer market.  Most of 'the consumer market' buys common PCs off the shelf at big-box general retail store, and 8K dual-resolution monitors have never once entered their mind.",Neutral
AMD,"you can definitely squeeze an extra 2-3% by fully tuning hynix a-die, EXPO profiles are \*really\* loose in all the subtimings. But most people won't bother. As someone who spent a lot of time tuning ram the juice definitely isn't worth the squeeze for normal people.",Negative
AMD,"Just supply and demand. X3D's are hype (with good reason), the AM4 ones  are out of production, and people who have them aren't upgrading/selling because they are still very capable. The price isn't proportional to performance, the 5950x or 5900x for example are within 5% in performance but half the price since the x3d hype is so powerfull. Not that they don't deserve that hype, but other excellent options are overlooked as a result of how they dominate the discussion around cpus for gaming.",Neutral
AMD,"I bought a 5700x3d last fall for $135 including shipping and tax, and at that price it was a no-brainer to upgrade.  But I had already decided that $450 for a 5800x3d was too much when I already had a 5600x.",Neutral
AMD,"Much of the work on AI optimizations would also carry over GP-GPU.   At some level tensor multiplication is tensor multiplication.   There are cases where one set of tradeoffs is more important in one use case vs another but overall... a rising tide lifts all ships.     My suspicion for these is that much of the reason why nVidia started focusing on ray tracing and DLSS is that the uarch optimizations that happen to be somewhat useful for those are VERY useful for general AI training. I'd have to dig into details though.   I'd actually agree that using machine learning to do upscaling is an overall smarter and more efficient way of doing things than just brute forcing more raster calculations. Frames upscaled by DLSS are something like 200-400% more energy efficient (AI generated info take with caution) and the amount of die space dedicated to tensor cores is pretty minimal, just a few percent.",Neutral
AMD,"I’m not talking about blanket recommendations, please keep in mind steam hardware survey is not representative of commentators here.   Many of us may have picked up 32GB of fast DDR4 during the over supply for really cheap. Or even 64GB like me.",Neutral
AMD,Yup. I got into pc building again this year. Comfortably afforded a 5090/9800 etc etc. the poor are worse off than ever but the PMC middle class- especially the child free are doing just fine.,Negative
AMD,There is no other reason to upgrade. Consumers have other desirable products to shop for.,Neutral
AMD,"> I’m not talking about blanket recommendations, please keep in mind steam hardware survey is not representative of commentators here.  You never said that lol.  >Many of us may have picked up 32GB of fast DDR4 during the over supply for really cheap. Or even 64GB like me.  Okay and you're the minority of gamers/AM4 users.",Neutral
AMD,No one is buying this for 800  Stop the sensational headlines  You can get a 9800x3d and 32gb ddr5 cl30 kit at MARKET PRICE for under 800,Negative
AMD,"Another clickbait article, yeah no shit if you order by highest price sales on ebay it'll look like this. If you instead look at recent sales this month they're all around $390-530.",Negative
AMD,Trash fake news. Delete this garbage,Neutral
AMD,Glad I bought my 64GB of RAM months ago. Cost less that $100,Positive
AMD,How many people on am4 actually made the leap to 5800x3d than just the normal 3600 > 5600x/5700x and thats it or 1600x to 3600x to just 5600x?,Neutral
AMD,Why do we still allow Tom's Hardware articles in this sub at all?,Negative
AMD,There was a brief period these chips were an excellent buy and very cheap but that ended a while ago. Last time I checked at the middle of this year they were £350+ and now I see they are £450+.  I get that it allows you to max out an AM4 system and if you have a bunch of DDR4 that may be a wise investment still but the days of it being the undisputed price:performance king are long over.,Negative
AMD,"Hello I_Love_Cape_Horn! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,Honestly I want a 5600x3d with the lower power draw and TDP. My mATX system is somewhat small and I’m not playing the most demanding games on it but I’d like to get a few more years out of AM4,Neutral
AMD,"oh ye, i am selling my one for $69420...now go write another article",Neutral
AMD,"I upgraded my AM4 AB350 system from first gen Ryzen 1600 to 5700X3D and the gaming results lined up with the reviews. But these chips falter with high RT usage games that have even higher demands on the CPU and RAM.    For example, in Stalker2 CPU-limited scenario, 9800X3D was almost 2.5X of its performance. I doubt that non X3D would be more than 20-30% slower, so it'd be better to get AM5 instead.",Negative
AMD,"What? I mean if you'd actually wanted to spend 800$ on just the CPU, why not just get a cheap AM5/1851 one and then get yourself some overpriced RAM instead?  This makes no sense at all.",Negative
AMD,"A 7600X is faster than a 5800X3D in most games, if you are buying a 5800X3D for $800 you are an idiot.  A 5600x will still be GPU limited at the resolutions and settings people actually play games at.",Negative
AMD,No shit.  This happens with all older hardware as companies want you to buy new stuff.,Negative
AMD,I was thinking to sell my old DDR4 memory....,Neutral
AMD,This crap of supply and demand can stop anytime now.,Negative
AMD,The RAM I own will appreciate faster than gold in the next year or two woohoo,Positive
AMD,What enthusiasts are on 3 generation old hardware? I consider myself a pretty big enthusiast and never skip more than 1 generation.,Neutral
AMD,Jumping from am4 to am5 is impossible though because of insane prices too,Negative
AMD,I just bout a Ryzen 7 7000 series drum 220 brand new.,Neutral
AMD,Someone wanna give me $1000 for my... DDR3!,Neutral
AMD,Am4 is dead.   LGA 1700 is where it’s at rn. Having compatibility for either ddr4 or ddr5 ram is much better than am4.   And the lga 1700 CPUs especially the 13th and even more the 14th gen intel CPUs like the 14700k are way faster than any am4 cpu as they compete with am5 CPUs anyways.,Positive
AMD,"Maybe I should sell mine, I'm using it in an HTPC that I rarely game on anyway.",Neutral
AMD,I just sold mine for 375$ yesterday.,Neutral
AMD,"Hey I’ve got a 3800X, I’ll sell it for 3.8 million USD if anyone is interested.",Neutral
AMD,"and Ryzen 5 5500 is only $75. sure, having two more cores and 6x the L3 cache is nice, but not 10x the price nice.",Neutral
AMD,This is more sad than anything,Negative
AMD,something something... turntables.,Neutral
AMD,"Pleased my 5950x, 3090ti and 64gb of ddr4 are still keeping up just fine….",Positive
AMD,"There are still places that do sales on AM5 stuff. You can buy a 7800X3d , 32GB ram and motherboard for $580. There is literally no need to spend $800 for a 5800X3D",Neutral
AMD,Gamers would rather pay 800 for this than get Intel. The gamer brain rot is real.,Negative
AMD,Computers are quickly becoming a luxury like in the 90s.,Neutral
AMD,"they are trying their best to milk the market. Push prices up...   Fucking Tech websites, all in the pocket of other big corpo's making artificial scarcity.",Negative
AMD,"A couple of sales, (literally just a few) on ebay is no big deal...there's wacky people doing wacky things out there all the time.  The two or three people are pointing out on ebay are dwarfed by the normal sales flow from normal websites...     ""a thing happened ***once***!  let's all discuss it like it's a regular occurrence and changing the status quo! ""    it's all so tiresome.",Negative
AMD,"Check eBay.  [https://www.ebay.com/sch/i.html?\_nkw=5800x3d+cpu&\_sacat=0&\_from=R40&rt=nc&LH\_Sold=1](https://www.ebay.com/sch/i.html?_nkw=5800x3d+cpu&_sacat=0&_from=R40&rt=nc&LH_Sold=1)  $800, no not quite.  Over MSRP?  Yes.",Neutral
AMD,"Yep.   https://www.microcenter.com/product/5007092/amd-ryzen-7-9800x3d,-asus-b650e-e-tuf-gaming-wifi-am5,-gskill-flare-x5-series-32gb-ddr5-6000-kit,-computer-build-bundle  CPU, memory and motherboard $679.00",Neutral
AMD,> at [MARKET PRICE](https://youtu.be/5KXrQYWbbIs?t=17),Neutral
AMD,What motherbard ?,Neutral
AMD,"Yeah this headline is stupid. I see *new in box* 5800X3D selling for $500-600 on ebay, and plenty of used sales in the last few days between $380-500. The price has risen a bit, but not that much. It's been expensive ever since production of the 5700X3D ended.",Negative
AMD,"And even then, sales VOLUME is what matters. There will always be people who can't do basic arithmetic, who will then buy at these prices, instead of selling their old system and getting something brand new. That doesn't mean that $400 for a 5800X3D is sound market pricing, or that it is worth this much.",Negative
AMD,I built my PC a few years ago and since the cost difference was negligible in the overall build I went with 64gb of ram to fill up the 4 slots on the motherboard. When I was asking build opinion on a PC sub many people called me an idiot for wasting my money on all this ram.   Just kind of goes to show there’s no sense of asking Reddit for opinions.,Negative
AMD,"I bought a pair of 16 gig sticks in an attempt to beat the first round of tariffs. At the time I thought they were DDR4 and I wasn't paying much attention. They came in, I see they are DDR5 and think ""ok, future rig then."" They came in around 120 bucks. Fast forward to today, I check the price of the same pair of sticks, it's over 400. Genuinely considering selling the sticks for a better AM4 CPU.",Neutral
AMD,"I went from 2700X to 5800X3D and upgraded ram from 16GB to 32GB, so I'm good until Zen 6 at least. I got it for 340€ like 5 months after release in germany. If I sold it now I would make 50€ profit, its going for around 390-400€ used on ebay. Not bad.",Positive
AMD,"A 5600x will play games just fine, at the resolutions and settings people play games at you will still be GPU limited on a RTX 5070.",Positive
AMD,I went from the 5800x to the 5800x3d. Stuck the 5800x in my daughters PC instead of buying her a 5600x like I did for my son's PC.,Neutral
AMD,"It really only makes sense if you already have an AM4 motherboard. Otherwise, if you're doing a DDR4 build Intel 14th gen outperforms it.",Neutral
AMD,You can limit the power usage on the 5800x3d or 5700x3d as well if you find a deal on one of those.  I have the 5800x3d and the low power usage is great.  In a lot of games the chip is only drawing ~50W.,Positive
AMD,Because no one sane would do that... they simply took the most expensive offer on eBay and made a clickbait title around it... I may list my old PC parts for 10k; maybe they shall made another such article...,Negative
AMD,"Or it's discontinued and a sketchy seller is hoping to con someone who doesn't know any better. I've seen Ryzen 5 2600's going for 200+ on Amazon from 3rd parties while official channels were selling Ryzen 5 5600's for 130 and 3600's for 90.  Honestly, whenever I see a price on Amazon that's not remotely close to a whole number or 25 cent increment (eg. 137.53), I pause and see if something's fishy.",Negative
AMD,"Seconding this, a lot of flagship CPUs released in the past 20 years are still absurdly expensive. QX9770, FX-60, P4 EE, 6950X, 9900K, etc. some of them more expensive than others, but the top CPU for a dead socket is still a pretty penny",Negative
AMD,Could also be used by sellers to fulfill insurance replacements.,Neutral
AMD,I thought we all loved Capitalism?,Neutral
AMD,"You can still buy laptops for $200 on Amazon, they good enough to do most computing tasks.",Positive
AMD,I had a PC in the 90s didn't know it was a luxury back then guess I was a lucky kid,Positive
AMD,"Compute has never been cheaper and fairer. You can get a Mac Mini right now for $450, which will do all you need, including light productivity, sans gaming and heavy 3D rendering etc, for the better part of the next decade.   All the harder compute that you need you can rent and get the best value, instead of shelling out for hardware that becomes obsolete within 2 years:  * for inference either get the subscription or pay as you go on Replicate, OpenRouter etc.   * for gaming get GeforceNow or similar service for $10/$20 a month. On MacOS it runs natively in AV1 with Cloud GSync, there's virtually no lag and even in very dark scenes you can barely tell it's a stream. Hardware in the back gets upgraded every 2 years, and 5 years of the service cost less than the GPU it's running on right now.",Positive
AMD,"I get your whole point, but I just want to pinpoint the wacky part, maybe they have a reason for it. Like, one quick example, they have the whole built already working, and the CPU dies or gets fried, or god knows what, and they want the same CPU, idk. Which goes hand in hand with your other point of it being 2-3 sales. The CPU is not being made anymore, so Ebay is the only alternative. Maybe even a collector. There are wacky people out there thought :P",Neutral
AMD,">$800, no not quite. Over MSRP? Yes.  MSRP was $450 and in your link there are a bunch listed at $399  What am I missing?",Neutral
AMD,Still too expensive   Still the price of a non x3d zen4/5 or raptor lake + ram kit which outperforms this,Negative
AMD,Yea I sold mine for $425 two weeks ago after upgrading to 9800x3D for $440,Neutral
AMD,"People are insane, in what world does that purchase make any sense.",Negative
AMD,"A fair number of these purchases are probably going to end up with fraudulent charges (item not as described, etc) because eBay tends to side with the buyer.",Neutral
AMD,It's worth whatever it consistently sells at. It seems to consistently sell for >$400 used.,Neutral
AMD,"It's a lot like the ""car is worth $2000 but needs $2000 in repair"" work making it worthless but they're low credit high APR folks and have cash and $2000 in repairs makes sense to get it going another 2 years",Neutral
AMD,Did you ever use more than 32? How often do you need more than 16 really? I'm asking because I stuck with just 2x8 and never had any issues yet.,Neutral
AMD,And they were right.,Neutral
AMD,"Unless you play heavily CPU bound games, which many BRs, and simulation/factory games fall into.",Neutral
AMD,"""Games"" are not a uniform performance load. Tons of games will be CPU bound with *any* CPU, including the 9800X3D, even at 1440p.  As always, know your workloads and purchase accordingly.",Neutral
AMD,"For productivity yes, but not for gaming.",Neutral
AMD,"If you filter on Sold listings, people *are* paying $4-500+ for these.  It’s still insanity even if the headline is sensationalist.",Negative
AMD,Oh? I have a 6950X sitting in a closet…,Neutral
AMD,"For the older ones it might be people wanting to build top-tier rigs of INSERT_YEAR?  I should get on with my want of a top 2012-2013 build, but knowing me I’d probably go seeking for silly things like Titan Z or ARES 2/3.",Neutral
AMD,I had a 9900K in my workstation at the office back in the day.  Solid performer for the money.,Positive
AMD,Adjusted for inflation it’s around $7000. Yeah they were luxury,Neutral
AMD,"Got a used Pentium I in 1995 (my first PC) and it was about the equivalent of 80 USD, but that meant a lot more in local buying power.",Neutral
AMD,"Mac's can run games  (native ports) decently too, considering their loss power draw.",Neutral
AMD,Stop being logical   We need 64gb and a 5090 to browse reddit and watch youtube,Neutral
AMD,> for gaming get GeforceNow or similar service for $10/$20 a month.  Streaming is not and will not be a valid option for gaming.,Neutral
AMD,"Okay Sam, no need to advertise your shitty services.",Neutral
AMD,"Even then, it would be cheaper to get a whole new mobo, CPU, and RAM on AM5 and end up with better performance.  It's an absolutely ridiculous price, no matter what. You'd have to just be a blithering idiot to go for it.",Negative
AMD,"If that situation forces a sale at $800 though, that does suggest that the market is very shallow.  Generally one goes on eBay and picks the cheapest or 2nd cheapest that doesn't look shady.",Negative
AMD,"One just sold for $600  Sold Dec 19, 2025    Brand New  [51 product ratings- AMD Ryzen 7 5800X3D Processor - NEW in Sealed Box](https://www.ebay.com/p/4053561416?iid=168015596458#UserReviews)  **$600.00**  or Best Offer  \+$7.50 delivery  Located in United States  [View similar active items](https://www.ebay.com/sch/i.html?_nkw=AMD+Ryzen+7+5800X3D+Processor+-+NEW+in+Sealed+Box&_id=168015596458&_sis=1)  [Sell one like this](https://www.ebay.com/sl/list?mode=SellLikeItem&itemId=168015596458&ssPageName=STRK%3AMEWN%3ALILTX)  gangster1234484 100% positive (522)     Being kind and logical, that's what you're missing :) <3",Neutral
AMD,"The RAM kit alone is more than the 5800X3D.  People aren't buying the 5800X3D over a new AM5 processor. They're buying it over a whole new platform, which is ~$1000 including board and RAM.",Neutral
AMD,What?  I'm a Top Rated Seller and very happy,Positive
AMD,Theres also folks who do their own repairs so thats 2000 in repairs becomes 200 in parts + couple of your weekends gone.,Neutral
AMD,14700k slams a 5800x3d both in gaming and especially on productivity tasks.,Neutral
AMD,5800X3D is [13% slower than a 14700K with DDR5-6000 in gaming](https://www.techpowerup.com/review/intel-core-i7-14700k/18.html). Unfortunately I can't find a direct comparison with the 14700K using DDR4.,Negative
AMD,"For some upgrading to AM5 would indeed cost a lot right now, but I also feel like a lot of people don't realize that the slowest AM5 CPU (7600X) is just as fast as the 5800X3D. Including for gaming.",Neutral
AMD,Nice! Those go for around $125+ on eBay regularly. People want to max out their old rigs,Positive
AMD,As i am writing this reply from 32GB and a 4070S i can confirm we can do with a slightly less for reddit.,Neutral
AMD,"It absolutely is. It almost feels local. And on Mac it's AV1 so it even looks almost local. It for sure looks better than anything my PC can pull off.  I can play on my phone with a simple android controller. When Im visiting my folks, all I gotta do is download the app on their smart TV and log in, and pick right up since the saves sync via steam.  Back home I simply turn it on in my TV in the living room, on my laptop wherever, or in the TV in my room upstairs.  Not need to carry anything. Don't care if one TV is occupied. It's always dead silent, since theres no 500w machine that needs to be cooled.  You don't event need to take my word for it, you can try it for like 3 bucks for 48hrs.",Positive
AMD,">Being kind and logical, that's what you're missing :) <3  Great, but that still doesn't explain the dozen other listings for new in-box chips at under MSRP prices",Negative
AMD,"Pretty sure it's just not showing the ""Best Offer"" price it *actually* sold at.",Negative
AMD,What's so rude about what he said?,Neutral
AMD,No its not  Newegg has combo deals ram and good board for 400ish dollars  A crappy 7600 beats the 5800x3d,Negative
AMD,"There are edge cases where no CPU can touch the X3D chips (e.g. Factorio, Baldur's Gate 3).",Neutral
AMD,"Hardware Unboxed's 5800x3d vs 12700kf (with ddr4) comparison was pretty much a wash, particularly if you are >1080p. Nominal win for the 5800x3d for gaming if you are being charitable. (I have both, but haven't done any real comparison testing, particularly because I run 4k.) So add on top some clock speed gains and there you go.",Neutral
AMD,Yeah I don't really get it. When they were sub 200 or even sub 150 for a 5700x3d it was an awesome buy but now it just feels like people panic buying.   Maybe they were waiting for zen 6 and just are afraid that ddr5 will be screwed for many years but I don't see the point in paying that much when you can still buy a faster 7700x microcenter bundle  even with 32 gb of ram for 500.  Even with ram like this I don't see why you would pay over like 250 tops for a 5800x3d.,Negative
AMD,Something to understand anout the average /r/hardware poster is that they believe that X3D chips are magic performance improvers that are instantly better than every other CPU ever produced by impossible margins.,Neutral
AMD,"Not sure why you're getting downvoted. Micro Center too has brought back their bundles with RAM for around $500. And, yes, a 7600 beats a 5800X3D. A 7500f goes toe to toe with it.",Neutral
AMD,"Even in factorio once you go to [high spm comparisons](https://factoriobox.1au.us/results/cpus?map=9927606ff6aae3bb0943105e5738a05382d79f36f221ca8ef1c45ba72be8620b&vl=1.0.0&vh=) rather than the the low SPM ones, raptor lake/non X3D zen4/5 does fine.",Neutral
AMD,Basically if your primary worload is larger than Intel L3 cache but smaller than the X3D cache and results in high cache hit rates for AMD and not Intel you get crazy boosts. Works in a lot of MMOs for example.,Neutral
AMD,"Its not that deep. A person wants to build a PC but does not know much about hardware. So he goes and watch some youtuber techfluencer for advise. He finds a video stating DDR5 is getting expensive, buy DDR4 (recent HUB video for example). He also finds video 5800x3D best DDR4 CPU for gaming. He looks no further and just buys those parts.  Got to remmeber most people dont follow the hardware news like we do. They spend 30 minutes on youtube and make their purchasing decisions.",Neutral
AMD,Depending on what your usecase the x3D can be magic. For example in mmos that get very CPU bottlenecked when there are a lot of players in the same place x3D due to much higher cache hit rates can mean as much as triple the framerate in raids (when it matters most).,Neutral
AMD,Does Samsung actually have N2 equivalent node ready though? Since it's SF2 is basically just SF3 renamed.,Neutral
AMD,">As ebn notes, since 2nm currently represents the cutting edge, TSMC’s 3nm output in 2027 is expected to lag by at least two generations under Taiwan’s N-2 principle.  ~~14A~~ edit: A14 won't be out by 2027 though. Maybe if TSMC is counting A16 as a complete node jump, rather than what it sounds like as N2 with BSPD being only meaningful for HPC.  Problem is, and this applies to both Samsung and Intel, is if remaining what is realistically N-1 in 2027 is still enough for Samsung and Intel to actually benefit. Because I don't think the nodes they will have available then are going to be much better than the N3 family either.  >while Google’s TPU team reportedly visited the Taylor fab to discuss potential production volumes, as the search engine leader is currently moving to sell TPUs—previously used exclusively for internal workloads—to external customers such as Meta.  I would be very surprised if this is true, precisely because the reporting that Google is looking to sell TPUs to external customers, and still want to use what will likely be an inferior node to what other customers/competition may end up using on TSMC.  It's one thing to use a lower end node or less competitive processor for your own internal workloads, where the cost you save remaining internal can offset the disadvantages. It's different when you have to then turn around and sell that to external customers and make it worth it for them too though.",Neutral
AMD,"""Google Eye"" making my tired brain thinking they renamed their smart glasses.",Neutral
AMD,Samsung is killing it lately.,Neutral
AMD,TSMC onmy has N4 in the US currently and the N-2 rule would prevent them from producing N3.,Neutral
AMD,"I feel like both Intel and Samsung think theirs belong to the same generation of TSMC 2nm simply because it uses GAAFET, when actual pitch and yield matter...",Neutral
AMD,"On your second point, I kinda read that as in ""(implied) has contracts to sell"", because of the specific reference to Meta.  If they don't have to compete to close the client, then they don't need to be paying for top-of-the-line nodes, do they?  The needs for the end product are probably specified in said contracts if those exist, and if they don't (yet) exist, then they are subject to discussion and change anyway...",Neutral
AMD,Well that’s not cured my impotency.  Damn.,Negative
AMD,"Radeon subreddit is in shambles because it doesn’t support RDNA3, they’ve got pitchforks out and are claiming they’ll go nvidia next gen, lmao",Negative
AMD,"I wonder if GN or HWUnboxed will roast AMD for their [misleading ""performance"" charts](https://cdn.videocardz.com/1/2025/12/AMD-FSR-REDSTONE-VC-5_videocardz.jpg) like they did for NVIDIA and MFG. I have no problem with Upscaling performance, but once you start introducing Frame Generation [like AMD has here](https://cdn.videocardz.com/1/2025/12/AMD-FSR-REDSTONE-VC-4.jpeg), you're muddying the waters of what is ""performance"".",Neutral
AMD,Launched without 7000 or earlier support despite the leaks.  lol?,Neutral
AMD,AMD never misses an opportunity to miss an opportunity.,Neutral
AMD,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,"Marxist-Leninist-based Upscaling. No wonder they're called Team Red. /j  This is an improvement, but I'm more excited for the hardware of UDNA.",Neutral
AMD,The Ray Regeneration thumbnail is legit just a contrast filter lmao,Neutral
AMD,Would they be using their ML/AI cores of e.g Strix point/Krackan point for this too?,Neutral
AMD,Hopefully this leads to them FINALLY being competitive with nvidia in the high end again sooner than later,Positive
AMD,"It's really obnoxious that they released this but didn't roll it out to Adrenalin yet. I had to DDU and reinstall it twice, since Windows overwrote it immediately with 25.10.30 the first time.  AMD owes me 10 minutes of my life back, is what I'm saying. /firstworldproblems",Negative
AMD,does this work on RDNA 3.5? (890m specifically?),Neutral
AMD,"This wasn't the road I wanted AMD to pursue, the ""fake frames"" like Nvidia currently getting a lot of heat from.  But it just shows that AMD has no guidance except copying everything Nvidia does. Grow a pair and just make your technology better because it **IS** good right now, just not in the test metric Nvidia wants to push on consumers which is basically a big fat lie in promises and practical performance.",Negative
AMD,Still balding here....,Neutral
AMD,"It'll probably at least partially support rdna3 eventually, but it's pretty obvious that AMD just needed to get this out into the wild with at least rdna4 support asap.",Neutral
AMD,"I mean yeah, RDNA3 doesn't have the physical hardware for this ML-based stuff. If they bring Redstone features to RDNA3, it'll be entirely different.",Neutral
AMD,"I was on AMD for my last 3 GPU upgrades.  I had a Radeon 6800, then i saw AMD announcing that they'll put that card series into legacy support, which slightly pissed me off. They still make and sell 6000 series cards.  Then there's the fact that partial FSR4 support is possible on older cards, but not released or enabled by AMD.  I don't really care about the upscaling part of it, the 6800 chewed up any game i threw at it at 1440p without RT. I wanted it because TAA or FSR3 are horrid when it comes to image quality when you use them as AA solutions.   Playing something like Final Fantasy 7 Rebirth was a travesty if using TAA or FSR 3. Such a beautiful game that looks like a smudged mess with those solutions.  So i got an Nvidia 5080 for black friday sales. I basically just don't trust AMD's GPU division to not abandon even their 9000 series once they release a new series. And i'm done giving money to a  company division that is content in merely keeping their cards in ""Nvidia -50$"" price range for much lower feature sets.  I HATE AND LOATHE Nvidia as a corporation and hate that i gave them money, but i ultimately just picked the better product for my needs.  Their CPU division is banging and my 5800x3D looks like it'll keep chew anything i throw at it for a good while still, but AMD's GPU division can go fuck itself for now.",Negative
AMD,Anyone with a brain could look at RDNA3 and realize it wasn’t a major architectural shift over RDNA2. People read about a couple of low-precision math instructions and assumed RDNA3 had closed the gap with Turing. Honestly I don’t expect AMD to truly lock their feature set in until UDNA launches.,Neutral
AMD,Not so Fine wine now eh? Lol.,Neutral
AMD,"My problem whit the 9070XT, which isn't a cheap card by any standards, is the issues I came up against given the supposed 2.1a ports. The ports are not full bandwidth, I have multiple 4K monitors connected and get stuttering, freezing, and timeout issues constantly. I have to manage the displays as if I purchased a cheap card, lowering Hz here, color range there, etc. I can't run all my monitors at full specs at the same time! RIP I should have purchased a 5070ti...  Having to run my LG C5 and Alienware AW2725Q at 60Hz is crazy. Switching settings every time I want to play is such a pain.",Negative
AMD,I just build a amd pc after many years with just laptop and I saw Radeon subreddit. Is AMD really deserve that hate or Radeon subreddit is just that toxic :D?,Negative
AMD,"They shouldn't worry too much, as even RDNA4 doesn't support it in almost any use cases except for very specific games.",Negative
AMD,I went from 7900XTX to 5080 exactly because of this. And VR,Neutral
AMD,Some of us refuse to use FSR lol,Neutral
AMD,HUB is saying it sucks.   https://www.youtube.com/watch?v=LpAZF_-qsI8,Negative
AMD,"What's misleading about this? Nvidia specifically marketed their GPUs as ""4x"" performance or whatever and compared GPUs with FG/DLSS off with DLSS and FG on. These charts from AMD are specifically performance charts for FSR which means the point is FG performance.",Neutral
AMD,Gn have published a video but I have yet to watch it,Neutral
AMD,AMD's GPU market share has gone so down that most people actually don't care.,Negative
AMD,"I don't think they will do a ""AMD IS LYING!!!!"" with a stupid thumbnail like they did to Nvidia.",Negative
AMD,oh NOW frame generation is bad when AMD gets their hands on it  give me a break,Negative
AMD,"amd repeatedly said before this launch that it was exclusive to rdna4, people's own fault if they decided to assume they were lying",Negative
AMD,"I actually thought it wasn't leaks, but AMD's own statements that claimed something like them wanted to make this adoptable for multiple older architectures, and things out there. Maybe I misunderstood that. Either way, I'm glad I went with Nvidia last generation.",Neutral
AMD,"It looks dissapointing, so you didnt lose much",Negative
AMD,"Leaks from where? If it was some of the typical suspects, I'm *shocked* that they would make shit up.",Negative
AMD,They are RDNA 3.5 and ML Redstone is only for RDNA 4,Neutral
AMD,How can they be competitive with nvidia in the high end if they don't have any high end RDNA4 graphics cards and Redstone is exclusive to RDNA4?,Neutral
AMD,Lol. It’s not even 4x frame gen. How would they compete with a mid level card like the 9070xt?,Negative
AMD,Actually that Windows owes you time...,Negative
AMD,"Nope, RDNA4+/UDNA (we presume will also support this) exclusive.",Neutral
AMD,This is what AMD has always done.  It’s part of their origin story.  AMD literally copied Intel’s silicon to break into the CPU market back in the 70s,Neutral
AMD,Holy shit my teeth are straightened and I think one grew back,Negative
AMD,"FSR4 INT8 already runs on RDNA1-2 and 3. Even some RDNA1 and Radeon 7 on GCN5.  XeFG also runs on DP4a or SM 6.2 path on the same GPUs.  If Intel can, certainly AMD can. They just don't want to.",Neutral
AMD,"Yeah. I think many people have unreasonable expectations. Still, we know there's an int8 version of fsr4 upscaling out there which works pretty well. If AMD just officially published that I suspect a lot of RDNA 2&3 owners would be pretty happy. (Maybe with the added promise of trying to achieve something similar with FG and ray reconstruction)",Positive
AMD,Yes they do. Shader cores are more versatile than matrix cores and can do everything they can at a lower efficiency/performance. AMD gating FSR's newer versions and NVIDIA gating DLSS behind never hardware with that excuse is bullshit.,Neutral
AMD,Yeah but AMD claimed “Architectured to exceed 3.0Ghz”. It was a major architectural shift since even RDNA4’s boost clock did not exceed 3.0Ghz,Neutral
AMD,AMD Vinegar™️,Neutral
AMD,This myth is so heavily reliant on the R9 290X surpassing the GTX 780Ti its not even funny.   Probably users younger than those cards in here lmao,Negative
AMD,But muh drivers,Neutral
AMD,"not sure how old your card is, but is just got it recently and im running 120hz on lg c4 and lg ultrawide monitor(not sure exact model) on 240hz no issues.",Neutral
AMD,...at 60Hz? That don't sound right. You sure your cables are up to spec?    Even RX 6000 could already do 4K120 10b HDR.,Negative
AMD,"Yes, it was a good video, discovered what other outlets did not and [Tim also did mention that Frame Generation does **not** increase performance, just perceived smoothness.](https://youtu.be/LpAZF_-qsI8?t=793) Kudos to that video, they (or Tim) did great work.",Positive
AMD,It's still presenting the FPS increases without any context for how compromised the experience is compared to regular frames. But I agree that using it as a tool to lie about performance uplift of a new product compared to the old one is considerably more dishonest.,Negative
AMD,"> What's misleading about this? Nvidia specifically marketed their GPUs as ""4x"" performance or whatever and compared GPUs with FG/DLSS off with DLSS and FG on. These charts from AMD are specifically performance charts for FSR which means the point is FG performance.  What's misleading is they're making out that the FPS you're getting is more performance and making the game ""faster"" (says it in the top right of the chart, their words not mine), yet the latency is increased and delays inputs, that's anything but faster or more responsive gameplay. If this was just upscaling I would have no problem, but as I said, once you start adding Frame Gen as ""performance"" and making the game a less responsive experience, it's not faster, you're delaying inputs for perceived smoothness in the image. Both practices are dumb and misleading and I do not advocate for either what NVIDIA or Radeon have done with marketing their products. What NVIDIA has done is worse, but go to the root of both marketing strategies and both AMD and NVIDIA are pretending like Frame Generation = more responsive and more/faster performance.",Negative
AMD,"I saw it, he doesn't even mention the graphs being misleading from the slidedeck (it's clear from the video he has access to the slides), but GN does look at latency results. In the end, I wish he was harsh like he was with NVIDIA because honestly, AMD is just copying NVIDIA's homework and using the same BS playbook, pretending Frame Gen is increasing frame rate and [making the game's performance ""faster""](https://cdn.videocardz.com/1/2025/12/AMD-FSR-REDSTONE-VC-5_videocardz.jpg) (their words not mine).",Neutral
AMD,"DW bro [we got you caught in 4K calling them ""fake frames"" just earlier this year.](https://www.reddit.com/r/hardware/comments/1n0qtts/nvidia_geforce_rtx_5090_and_the_age_of_neural/navh6bm/) Before you say ""that's just one post or is a joke, [here you are doing it again in a separate thread](https://www.reddit.com/r/hardware/comments/1n67v2u/steam_hardware_software_survey_august_2025/nc3jscw/). Enjoy!",Neutral
AMD,"Bro lmao, every time I see you defending AMD like their white knight. Redstone is clearly DLSS 1.0 and AMD rushed to release it because they're getting stomped in software features.  Though in AMD fashion, they manage to incredibly disappoint as always and reviewers are just letting you know.",Negative
AMD,"There is a leaked INT8 path for FSR4 that works on older hardware and has been implemented by others (i.e., on Linux in Proton-GE).  It works pretty good already, which makes AMD's reticence to put it out officially *baffling* -- especially since they aren't putting RDNA4 into APUs for a while yet, and want to sell a bunch of those in gaming-focused handhelds and Steam Machines.  AMD has a good software product here for once and they need a broad installed base to drive developer adoption, but they don't seem to care and it's infuriating.",Positive
AMD,"No one cared about the new framegen (on older cards), what people wanted is FSR4 upscaling on RDNA3/2, which already has been proven to work well on Linux",Neutral
AMD,"They aren’t even RDNA 3.5, they are XDNA 2. That’s why I asked whether they will leverage XDNA 2.",Neutral
AMD,"4x frame gen is really niche, there are so few cases where it actually makes sense and doesn't cause an unacceptable amount of artifacting and/or latency. You pretty much need a 240hz+ monitor, for one thing. I don't imagine that being a major factor in almost anyone's purchase decision.",Negative
AMD,"4x FG is a gimmick at this point. Maybe FG progresses to the point that it's viable in the future, but it really isn't right now.",Neutral
AMD,Yeah that's fair. Windows loves to replace new drivers with shitty old ones against your will.,Negative
AMD,"The fact that the INT8 version got leaked the way it did says...something.  It is pure speculation at this point by anyone except AMDs management as to why they have not released drivers that include INT8 for older RDNA versions but I think based on the independent testing from that leaked version that not officially releasing it is bad.  It is one thing to want to sell new cards but it is quite another to have something like the INT8 be out in the wild and then try to ignore its existence for the owners of cards not really that old.  And given how they recently they tried to put cards that they still sell into ""maintenance mode"" it really does seem like some parts of AMDs management is not making good decisions for their customers.  Now maybe their data shows that such decisions are better for their quarter to quarter bottom line but I really do question if that is the case.  I'd have to see some *hard* data to prove to me that whatever additional profits they are making but implementing these decisions are adding value to the company/brand over these anti-customer moves.",Negative
AMD,"It's pure market segmentation. Even if it doesn't work the same, they could still allow older cards to get it.",Negative
AMD,People use fsr3 to play at higher frame rates. So why would they release something that almost entirely fails to do that on the most sold gpus which are the low end 6600s and 7600s?,Negative
AMD,"It runs, but performance impact is considerable and inconsistent and quality isn't on par with FP8 either, no?   Could be they decided against releasing it in this state because of it.",Negative
AMD,The unreasonable expectation that cards that are still being sold get feature support,Neutral
AMD,My 280X survived until Overwatch 2.,Neutral
AMD,"I have had enough discussions with them, and the thing they call driver is adrenaline software which is driver control panel, clearly not understanding what a driver is.",Negative
AMD,The only benefit I can think of is maybe better motion clarity by pushing the FPS to 200+ or something if you have a really good monitor (i.e. an OLED). But even then the hit on latency means it would only work well for games where latency is not hugely important.,Neutral
AMD,"If I recall, the main harshness on Nvidia was the way they were trying to push reviewers to only review performance with frame generation enabled, an inherently dishonest take.  To be fair, I also haven't seen the video on this yet either, but I think ""frame gen generally sucks in these ways"" is pretty well established and those factors are unlikely to change drastically",Negative
AMD,"I saw first 3 minutes, he did said he didn't had much time for this video  but still doesn't excuse the ""less harsh"" opinion on amd.  Will update this after watching whole video",Neutral
AMD,"Both are bad, at least this is being used to show \*gains* from having the feature on or off, rather than presenting it as \*gains* over the previous generation of cards for the purpose of representing a new product's performance as higher than it really is",Negative
AMD,Proton-GE doesn't implement the INT8 model it's the FP8 model running through the cooperative matrix extensions added to Mesa.,Neutral
AMD,"I mean, why did you expect this? They never said they will do it.",Neutral
AMD,Another reason to dump Windows,Negative
AMD,"I wish they would.  Isn't the NPU in the 90x0 an XDNA also?  That's what is doing this upscaling.  But, who knows if Windows is locking it down for CoPilot, or if it is accessible in the same way as the one in the GPU (since it is technically part of the CPU).  I bet they will figure it out though.  With Intel about to release their own competitor to it (maybe... no GPU benchmarks yet), they will want to use all they have to combat it.",Neutral
AMD,Yes everything is a gimmick until AMD releases a shittier version of it and then gets praised sky high.,Neutral
AMD,With a 4k240hz oled (or any other ultra high refresh rate oled) mfg 4x is literally transformative despite the 2 Steves telling you otherwise.,Neutral
AMD,"Seconding what the other comment says, it's great.",Positive
AMD,The market segmentation is stupid.  Why NOT want to sell more RDNA3 GPUs besides RDNA4? WHY make consumers reluctant to buy more AMD products? It's so stupid.,Negative
AMD,"Path Tracing, DLSS Transformer and Ray Reconstruction Transformer can run on laptop 2050. Or MX 570.  What's your point exactly?",Neutral
AMD,Performant impact isn't that considerable.  It's consistent.  Quality is almost identical.,Neutral
AMD,"Because it was already developed, exists, and all they had to do to get the easy win was launch it officially.   But they chose not to do so while still making every APU including $1500+ Strix Halo products that could use it their only option.",Neutral
AMD,"It’s a bit more complicated. Krackan point/Strix Point are APUs, is everything on the same chip so… why not?  Also I’m on Linux, firmware/drivers are there but there is no program/frameworks using them… So it’s purely a drivers issue from amdgpu to actually schedule compute on the NPU.",Neutral
AMD,You guys say this but people still consider FG a gimmick despite FSR3 supporting FG for years now (and doing a decent job with it too tbh unlike the upscaler).,Positive
AMD,Not every negative Nvidia comment is a pro-AMD comment. Stop promoting tribalism for 2 giga-corporations that don't give a shit about you.  I genuinely don't think 4x FG is a valuable feature at this time. The latency hit and the image degradation are not worth the smoothness.,Negative
AMD,"I really expected them to announce an MFG, you know damn well that they must have been working on it since the moment they caught wind of Nvidia having it.  It must be a lot harder to do as well as it's already being done by Nvidia than people expect - everyone seems to think everything these days is practically just checking a box off.",Negative
AMD,"You think shareholders are going to like that old product is cannibalizing new? This is exactly what happened with the 1080 Ti.  Sure, it's irrational from a consumer and engineer perspective, but nobody cares about them. They only care about the shareholders.",Negative
AMD,Doe dlss transformer only give 5 extra fps on a 2050?  And path tracing is designed to look good not help games run better. Your example would make sense if there was somehow less light bounce than in rasterized modes.  Fsr4 on rdna3 fundamentally fails at the thing its most used for.,Negative
AMD,"The xdna npu is a completely different architecture (based on xilinx IP) than the rdna4 ml extensions, which are a set of new shader instructions.  They don't really have anything in common in terms of architecture, I'd be surprised if the amdgpu driver ever ""supports"" both, as it'll be effectively adding an entire new driver stack beneath that interface for the npu, and much of that interface would simply be not relevant to the npu (and likely the npu will need new interfaces that aren't relevant to the GPU side of things either). It'll just be functionally 2 different drivers sharing a name.",Neutral
AMD,"Neither of the things you mentioned are even remotely true. Latency hit is negligible if you're close to base FPS of 60 or higher, Nvidia Reflex is far far better than AMD's Anti lag. And Reflex 2 will kill the latency debate with FG once and for all.  And personally I haven't noticed any image quality issues either. The FG model they trained is very good. It's an amazing technology for me, I have a 360 Hz OLED monitor and it's sublime to game in the  200-360 FPS range.   Also let's not pretend to not know fans of which corporate, Intel, Nvidia or AMD represents a literal cult.",Neutral
AMD,"Lol, the 'shareholders' do not care about Radeon consumer products. Most AMD shareholders probably aren't even fully aware of these DIY discrete GPUs, they are a rounding error in the business.  The amount of sales lost to people buying RDNA3 over RDNA4 due to FSR4 is essentially $0 in the grand scheme.",Negative
AMD,It would be hilarious if a Samsung chip has newer RDNA IP compared with AMD’s own APUs.,Neutral
AMD,Is Juno expected to be a mobile version of RDNA4?,Neutral
AMD,">PhoneArt reckons that the prime core will reach a maximum clock frequency of 3.9 GHz. A commenter, Erencan Yılmaz, reckons that this figure should be reduced to 3.8 GHz, due to power consumption considerations when looking at a 2 nm GAA-based design.  What does this even mean? Are they implying that GAAFETs have poor power frequency scaling at the highest end of the V/F curve or something?   Anyway, interesting to see the % Fmax gap between TSMC and Samsung based designs for the P-cores remain around the same.",Neutral
AMD,Very interesting. I would like to know more,Positive
AMD,"Like Google, Samsung has already taken steps to move away from ARM GPUs.  The end game is to replace the ARM CPU with a RISC-V one. These steps are just for readiness.",Neutral
AMD,It's actually quite probable since it actually features AI upscaling and frame generation and rdna 3.5 doesn't support it,Neutral
AMD,"> What does this even mean? Are they implying that GAAFETs have poor power frequency scaling at the highest end of the V/F curve or something?  If you look at the actual comments, they supposedly have their own leaks of the specs showing 3.8. The power consumption comment was a reply to some rando comparing clock speeds to Snapdragon.  Just lazy reporting.",Negative
AMD,"Money  Making GPU tech from scratch is stupidly hard. And it’s going to be hot garbage for a few generations until you polish it.  Look at Intel, despite making iGPUs for decades, still had tons of trouble making decent architecture for performance.   Qualcomm bought their GPU tech from AMD (was ATI back then).  Apple managed to do it, but I bet they were working internally for a few iterations before it was good enough to ship.   Just licence the GPU and not reinvent the wheel if you don’t need too.",Negative
AMD,"Apple didn't make their own gpu from scratch tho. It's more like a custom power vr gpu in it's first iterations. The first ""custom"" apple gpu is a power vr gpu with ""image block"". Which is actually a feature that no other tile based gpu had, only the adreno 840 released this year has this feature, imagination and Mali don't have it yet.",Neutral
AMD,"I think people forget a fully featured GPU arch is not just a collection of ""dumb"" SIMT compute elements.  There is a lot of arcane knowledge. Modern GPUs have so many specialized parts that are domains onto themselves - display drivers, video encoders/decoders, TMUs, RT engines, schedulers, memory & cache management....",Neutral
AMD,Yea I wasn’t even confident they did it either. Just further showcases how difficult it is.,Negative
AMD,"If you saw the similar Hardware Unboxed video from a few days ago, this one agrees with it and presents the info in a different way.  AMD urgently needs to fix this.",Negative
AMD,Very clear Redstone needed more time in the oven. Also it's going to struggle to gain traction unless they add support for older cards.,Neutral
AMD,The frametime issues are truly catastrophic. Looks worse than when I would force Crossfire on games that didn't support it even. I don't understand why they thought it was a high-quality release to represent the brand. WTF man.   At least it looks good and they can theoretically fix the frame pacing. They never did on FSR 3.,Negative
AMD,Maybe Nvidia was up to something with their Flip Metering stuff. The frame pacing of DLSS FG/MFG is flawless.,Neutral
AMD,FSR 3.0 all over again. Their framegen was also unusable on launch. They really never miss a chance to miss a chance,Negative
AMD,"V-Sync on driver level, cap frame rate -3 of your refresh rate   It mitigates the frame pacing in CP2077 and completely solved it on different games  I know it's a crutch but at least it's something until they solve it.",Positive
AMD,"It seems fixable by software (driver), so I do not despair as much as many others.  They needed something out to show their progress.  I'm hopeful it will get much better in the coming months.",Positive
AMD,"What confuses me about this is that the framerate and frametime graphs displayed by MSI Afterburner in many games tend to not be flat with DLSS-FG on my 4090. In fact, FSR-FG often appears flatter. However, the DLSS-FG tends to subjectively feel smooth to me (so long as it's not inheriting stutter from the rendered frames).  Does anyone have any explanations for this in light of the HUB and DF videos? Could the flip metering hardware of the 50 series be playing a significant role here (I think both HUB and DF used 50 series cards to compare FSR Redstone to)? Is there an issue with using MSI Afterburner's framerate and frametime graphs for this purpose (I can't seem to post a screenshot unfortunately)?",Neutral
AMD,"With nvidia bowing out of consumer GPUs next year, AMD is lining up the fine wine perfectly.   When they fix it next year, I hope the media covers it equally as positively as they were critical.",Positive
AMD,"Yeah, I think in Digital Foundry's podcast they called out the Hardware Unboxed content as excellent and basically said ""I am not sure we even need to make a video now, but we will"".  And I think it's good that they did, more attention on this can only be a good thing.",Positive
AMD,"They will fix it in RDNA5, just like how adding frame generation was a “fix” for FSR3 and it being limited to RDNA4",Neutral
AMD,It might be an unfixable hardware flaw.,Negative
AMD,Why urgently? It's an optional feature.,Neutral
AMD,"The ""time in the oven"" is releasing the feature in RDNA5, not in RDNA4. Just like how frame generation was a demo feature for RDNA3, Redstone is a demo feature for RDNA4 with the real version in RDNA5.",Neutral
AMD,Nvidia did it by adding ML hardware support to cards well before they were needed with the RTX 2xxx cards. It's surprising AMD waited so long to do it. They must have thought traditional algorithms would work just fine.,Neutral
AMD,"Yeah seems so. However, if they have to spend more engineering time/power on improving advanced features I would expect porting them to the older gens is pushed further down the timeline.",Neutral
AMD,What did you expect though? It’s AMD. Their software is always a couple years behind.,Neutral
AMD,Don’t they have flip metering on RDNA 4?,Neutral
AMD,"it's not flawless, unfortunately. for some games though. yes, it's miles better than FS FG, but there is still room to improvements  take Indiana Jones with path tracing for max GPU load, take RTX 5090, run it with 4xFG without frame cap and check msbetweendisplaychange with capframex. it will have the same sawtooth graph with some short lived frames, but to a lesser degree ofc. it will be very noticeable to the naked eye on OLED monitors because they will flicker due to those variations  reflex by itself (and reflex is forced on when FG is used) also adds not so perfect frametimes that can be seen with msbetweendisplaychange in some heavy games (Cyberpunk 2077 would be another example)",Neutral
AMD,"Has anyone done some god quality testing on the frame pacing of no flip metering vs flip metering? The only such coverage I recall finding is this is [this Gamers Nexus clip](https://youtu.be/Nh1FHR9fkJk?t=1922), but they only tested this on two games, and only one of the two showed an obvious framepacing improvement from the 4090 to the 5090.",Neutral
AMD,"Someone should remind them of that old adage: ""Better to remain silent and be thought a fool than to open your mouth and remove all doubt.""",Neutral
AMD,"Unfortunately, this only solves the tearing issue, but doesn't completely address the frame pacing issue. It also deprives us of the low latency benefits of Antilag2 and adds a sync delay, although not as significant as without the frame rate cap. The increase in latency can be easily verified using the reflex monitoring built into the Optical scaler.",Negative
AMD,"There are different statistics that you can use to populate your frame time graph, each of which are valid depending on what you’re trying to show.   Pure frame time measurement, as in “this is how long it takes to process each frame” is valid. But so is ‘ms between presents’ and ‘ms between display change’, the first being the timing of the frames being presented to the render queue, and the latter being the rate that the actual display updates and shows the new frame. Both of these measurements are captured by presentmon and frameview. I’m not sure exactly what measurement afterburner uses, but I suspect they are measuring pure frame time. But if you looked at time between display change it would probably show the issues that hardware unboxed and digital foundry showed.",Neutral
AMD,DF wrote their own software which physically inspects frames to measure frame pacing because they found the existing apps were insufficient. They talk about in one of the podcast episodes from 2022 or something like that. Its in the segment where they talk about automating the part that makes the graphs and it saving them a lot of time.,Neutral
AMD,Its rumored they will reduce production however nothing is confirmed. Nvidia still makes A LOT of money from gaming and wil lnot be giving it up. They were about $100 million short of a new record in gaming revenue last quarter.,Neutral
AMD,People said they will bow out of GPUs since 40 series just launched,Neutral
AMD,"With nvidia bowing out of consumer GPUs next year   People has been saying this for YEARS now and their gaming market share/revenue has only gone UP and UP. I don't know why anyone with enough sanity would believe this stupid narative   And let me ask you, if shortage hits Nvidia and forces them to reduce gaming GPU production, why would you think AMD will be safe from it?",Negative
AMD,Nvidia aren’t bowing out. They’re just reducing production on the 5000 series. They’re also going to be launching the 6000 series.,Neutral
AMD,lol,Neutral
AMD,"[amd isn't nvidia, they don't release slop. they want to make sure they realease quality products for gamers](https://www.reddit.com/r/hardware/comments/1nw18md/comment/nhdqwi7/)     \- you",Neutral
AMD,"> With nvidia bowing out of consumer GPUs next year, AMD is lining up the fine wine perfectly.  If they don't fix this ""optional feature"" (and future ""optional features"") AMD will be lining up a miraculous market share loss against no competition.",Neutral
AMD,If they leave things broken developers will ignore Redstone. Redstone is supposed to be AMD's answer to Nvidia's suite of DLSS features.,Neutral
AMD,"because all of this is hurting their brands even further, as if gating FSR4 (ML) behind RDNA4 didn't piss people off enough.",Negative
AMD,"There doesn't seem to be anything preventing RDNA4 from running this correctly, it has support for hardware flip-metering. AMD engineers just fucked this implementation up and need to fix the software.",Negative
AMD,"well, to be fair Nvidia utilized ML hardware in 2xxx cards a year after the release and before any other card releases. So the hardware adding was needed for their planned use.  Meanwhile, while Nvidia was already using ML for DLSS2, AMD publicly stated that ML and AI will cause Nvidia to go bancrupt and AMD wont do it. Well, we see the results and how long AMD took to change direction.",Neutral
AMD,"If I had to choose between them supporting my card fully and them fixing up and keeping Redstone competitive, I'd take the latter.  I bought my card for the features it had at the time of purchase. I didn't expect future new stuff beyond maybe FSR4. Making an official WMMA / INT8 version for games to fall back on would be more than enough, but I don't expect that to come.",Neutral
AMD,remind me who had gpu driver issues this gen again?,Negative
AMD,"Hadn't heard about it but they support ""Hardware Flip Queue Support"", which I think is the same thing?  But they [advertise it](https://www.notebookcheck.net/fileadmin/_processed_/2/e/csm_RDNA_4_Architecture_Press_Deck_page-0005_768d67dd27.jpg) with the following benefits:  1. Offloads video frame scheduling to the GPU 2. Saves CPU power for video playback  I don't think it has a role in frame generation, or even gaming, I think it mostly has to do with video playback.  Maybe it is the same thing and Redstone is just bad at frame pacing anyways?",Neutral
AMD,"As someone who came from a 40 to 50 series GPU, I can tell you it's amazing. It literally fixed VRR flickers for me and the pacing is flawless. The effect is exacerbated if you have an OLED display as it has near instant pixel response time.   The end result is a flicker-free smooth gameplay. It's hard to explain but it feels like I'm playing games on a thin fabric, it's that good.   So if anyone's on an OLED and hates bad frame pacing with VRR flickers, upgrading to a Blackwell GPU is the way to go, thanks to its HW flip metering logic.",Positive
AMD,I can only talk from personal experience but I have a 40 and 50 series gpu   I find Frame gen literally unusable on the 40 series card whereas I can literally not tell it is on with the 50 series   It felt like fucking magic to me   The 50 series is also a lot faster overall but I went up to 4K at the same time and am getting less frames so it’s not just more performance,Negative
AMD,You should apply this to yourself instead of typing it out bro,Neutral
AMD,"> But so is ‘ms between presents’ and ‘ms between display change’, the first being the timing of the frames being presented to the render queue, and the latter being the rate that the actual display updates and shows the new frame.  So I take it that the former is the time between frames entering a queue of frames to be sent to the monitor, while the latter is the time between those frames actually being sent to the monitor?  Anyways, I installed PresentMon, and using various metrics:  - FrameTime-Display - FrameTime-Presents - FrameTime-App - Ms Between Display Change  I couldn't notice any difference in these graphs between DLSS-FG and FSR-FG in Cyberpunk and Avatar (This is on 40 series, so no flip metering). With MSI Afterburner, the lines for both framerate and frametime appeared much flatter for FSR-FG in both games (even though it didn't subjectively feel smoother than DLSS-FG to me).  At times, FSR-FG has felt noticeably _less_ smooth than DLSS-FG in Avatar, but they both felt about the same on this particular occasion.  Others are reporting that DLSS-FG felt much smoother to them after upgrading from 40-series to 50-series, so I wonder if DLSS-FG isn't much smoother than FSR-FG on the 40 series. Also, I wonder if some of the FSR-FG framepacing issues are inconsistent, getting okay-ish frametimes on some occasions, but othertimes getting awful frametimes in the same game.",Neutral
AMD,so what does that mean for consumers if nvidia is going to reduce production but still wants similar gaming revenue?,Neutral
AMD,Wym they're clearly bowing out right now. They only shipped 11 million GPUs this quarter compared to amds 900k.,Negative
AMD,> They’re also going to be launching the 6000 series  That's a mid 2027 launch at best,Neutral
AMD,"40% is significant, and you know they're going to be top-heavy too, they're not going to waste hardware on budget 6060s. $3k GPUs is inaccessible for 99% of gamers that it's basically bowing out of the segment.",Neutral
AMD,Perfection,Neutral
AMD,"To be fair, if it's true Nvidia is cutting GPU supply by 40% soon, AMD will probably have no problem clearing their inventory if it's the only thing available at a reasonable price.",Neutral
AMD,"You realize most people pick up Radeon GPUs because they're incredible value for money. It's the raster and VRAM that is the attraction, Redstone is just a cherry on top.  Seriously, this is textbook example of loss aversion. Had there been no Redstone everyone would have been fine, now we get something as a bonus and although it's not quite ready yet it somehow diminishes the original value of the product?",Positive
AMD,"Don’t worry, I don’t think its an either/or. Its just an order of priority. Ignore the doomsayers, Radeon’s given every indication they intend to bring FSR4 to RDNA3. They aren’t even putting RDNA4 in their APUs in 2026, so supporting it going forward is pretty much a necessity for those lower power devices.",Neutral
AMD,"Literally right now, as we speak, NVidia drivers are mostly great while AMD struggles with whole host of problems introduces by fresh branch.",Negative
AMD,Adrenalin has been crashing so much on my windows clean install that I just said fuck it and installed a Linux distro to see if the problem is software or hardware lmfao  What a horrible purchase I made with my 9070 XT. I regret it SO MUCH. By the way I bought it because of FSR4 support specifically.,Negative
AMD,Dude the current 25.10 drivers are the worst they've been in years and some people are even using 2024 May drivers because for some reason later drivers cause hard pc shut downs in the Spiderman trilogy.,Negative
AMD,"Nvidia, AMD, Intel. You just hear about Nvidia's more because they sell like 95% of all GPUs",Neutral
AMD,Oh right i forgot AMD has the best software. Their upscaling tech is years ahead of everyone else.   I heard they give out free handjobs with each GPU purchase. They’re just that good! That’s why everyone owns one right?,Positive
AMD,"I can't remember if it was Digital Foundry or Hardware Unboxed, but someone mentioned it was the same thing. The video frame scheduling on GPU.",Neutral
AMD,"wow, that's good to know. honestly this makes me want to downgrade from a 4090 to a 5080, lol. frame gen on 40 series is almost unusable due to VRR flicker, i had no idea 50 series fix it.",Negative
AMD,On a 4070 Super I can't tell when frame gen is on. I used it in Cyberpunk to get above 60 FPS. The base framerate was in the 50's with all the cool path tracing stuff and I couldn't tell it was starting in the 50's.,Neutral
AMD,">This is on 40 series, so no flip metering  I believe both Hardware Unboxed and Digital Foundry showed the issues specifically on Radeon GPUs using FSR FG, No?  There are going to be differences between how AMD and Nvidia handle frame pacing, even outside of the FSR/DLSS conversation. There could be something about how Nvidia handles frame pacing that is better for FG, but is not a part of DLSS FG specifically.",Neutral
AMD,Nvidia will increase prices.,Neutral
AMD,So then you agree they’re still making GPUs then?,Neutral
AMD,Everything after your first sentence is pure speculation. Nvidia has like 95% of the GPU market. They aren’t just going to sell 6090s lol,Neutral
AMD,"Friend - they'd reduce production on top tier as  * AMD doesn't compete on that level * their OEM numbers are probably 60-65% of their volume  If anything, they will pump out 6060s to keep AMD out of Steam Surveys since the die will be miniscule thus high yield per wafer, in consumer eyes, and most important - in affordable products thus increasing their user base and indirectly influence.  They can afford for 6090 tier buyers to eat the cost - as they've done willingly for halo GPUs since reviews existed.",Neutral
AMD,Chip yields increase exponentially as area is reduced linearly. They will try to sell traditionally 50-tier sized chips in 70-tier cards and be not much less profitable than the huge AI chips while hedging against the bubble popping.  They could also do another generation of dual fabs where Samsung or even Intel produces consumer chips while TSMC fabs for their data centre designs.,Neutral
AMD,In 2021 Nvidia GPUs were all sold out and going for 2-3x MSRP (when you could actually find one) because of crypto and AMD still couldn't take advantage of the situation.,Negative
AMD,"And they are 'incredible' value for their money because they offer similar features. No one is going to buy an AMD card without FSR, FG, etc... in todays market.   Raster time is over",Positive
AMD,"Hmm no, If I wanted raster I would buy last gen used or on sale as usual, this gen was different because it's supposed to be the one that gets upscaling, ray tracing and frame gen right so NVIDIA tax becomes unjustified.  I'd get a new GPU to step up to 144fps and that requires both upscaling and frame gen, actually I could do without ray tracing but the reason to have a >60fps display is that frame gen is supposed to be ok at higher frame rates. Redstone is not.",Neutral
AMD,"I always love that response in a literal thread about AMD's driver/software issues/bugs.  I'm surprised bro didn't just say  ""I have a 6600 XT and have no issues.""",Positive
AMD,Sounds like a console fits better for you,Positive
AMD,This is no surprise. The 5070 Super 16GB will be the new 6080,Neutral
AMD,"you realize the dram shortage plays in AMD's favor right?  game devs aren't going to optimize their games, your best hope is nvidia figures out their fake vram neural texture compression just so you could have the privilege of paying $800+ for a xx70 gpu with MAYBE 6gb of VRAM in 2026/2027  and by then AMD will have ironed out Redstone, maybe even be on UDNA and have it backported to GPUs with 16gb+ of VRAM  raster will prevail",Negative
AMD,8GB at any rate with how difficult it will be to acquire VRAM,Neutral
AMD,"Most NVIDIA cards except for the 5070 have the exact same VRAM as the AMD counterpart, so I don't know how this plays into AMD's favour 😂  Also Redstone wasn't important according to you, now it's suddenly important",Neutral
AMD,"talking mid to long term, when nvidia cuts 40% of gpu production, they will recoup costs by selling $1000 midrange gpus while AMD will continue to provide GPUs with more vram at better value and feature parity  redstone will be fixed, that's the point.",Neutral
AMD,"Ah yes, because AMD is obviously not affected by a GLOBAL shortage and definitely doesn't need to cut production and raise the price  The fact that Samsung just reported that they have no stock at all definitely won't affect AMD but only NVIDIA",Neutral
AMD,"Last quarter shipping numbers were 94% to 7%  NV cutting it be 40% is only ~38% drop, still flooding the market >6:1 over AMD.  These people are over dosing on the kool aid.",Negative
AMD,But have you considered that AMD is our lord and savior?,Neutral
AMD,Venice is the next release. This would have leaked ages ago unless they are going for dual supplier.,Neutral
AMD,"AMD already announced they taped out Venice on TSMC 2nm back in April.  [https://www.amd.com/en/newsroom/press-releases/2025-4-14-amd-achieves-first-tsmc-n2-product-silicon-milesto.html](https://www.amd.com/en/newsroom/press-releases/2025-4-14-amd-achieves-first-tsmc-n2-product-silicon-milesto.html)  If they use Samsung at all, It is probably going to be something else.  That said, there are going to be Zen 6 EPYC SKUs that use different chiplets (there are at least two chiplet variants).  So it is possible that ""Venice"" is referring to a family of CPUs and there could be some Samsung based SKU in there.  But I think it is more likely that this headline is wrong, and AMD is contemplating Samsung for a different product.",Neutral
AMD,If i had to guess AMD is probably looking to offload lower end and mobile SKU's to Samsung to save on costs and get more TSMC production dedicated to more important datacenter/mainstream silicon. so Samsung might take over APU silicon for desktop and mobile along with motherboard chipsets and maybe some entry level desktop GPU's and most laptop GPU's.,Neutral
AMD,"AMD and TSMC have been pretty intimately close ever since AMD ditched global foundries. Also TSMC isn't too kind on companies that are unreliable partners, AMD just hopping to Samsung would be pretty shocking",Negative
AMD,Moving away from TSMC is always a good thing,Positive
AMD,"Ram shortage.. You need a deal for quota package memory and GPU, if not.. Who's gonna buy a new CPU.",Negative
AMD,TSMC ?: [https://www.tomshardware.com/pc-components/cpus/amds-first-2nm-chip-is-out-of-the-fab-epyc-venice-fabbed-on-tsmc-n2-node](https://www.tomshardware.com/pc-components/cpus/amds-first-2nm-chip-is-out-of-the-fab-epyc-venice-fabbed-on-tsmc-n2-node),Neutral
AMD,Why would AMD hamstring their very populair and performant server line by switching to a subpar node? Delays or price hikes at TSMC?,Negative
AMD,So they were unable to secure capacity at TSMC?,Neutral
AMD,I thought Venice was old news a long time ago.      [https://www.techpowerup.com/review/amd-3800-plus-venice/](https://www.techpowerup.com/review/amd-3800-plus-venice/),Neutral
AMD,"Probably not for launch, but filling up capacity later on.",Neutral
AMD,Possibly Samsung will be building the IMC,Neutral
AMD,Maybe lower end skus?,Neutral
AMD,Maybe iodie? Gpu?,Neutral
AMD,"Moving from TSMC's 2nm process to Samsung's 2nm would likely be a big downgrade. I believe AMD would have to compromise on frequency, power efficiency, or yield and even in the worst case it has to tweak the architecture.",Negative
AMD,The power efficiency throws itself out of the window,Neutral
AMD,"It goes both ways. if TSMC provides capacity to their latest nodes to every company that asks for it, giving AMD less allocation than they want, then AMD has to look elsewhere to fill the gaps. Can't rely on any one company for everything.",Neutral
AMD,They are business partners not spouses in a relationship,Neutral
AMD,"Unless you want to compete at selling the best high performance accelerators, then it makes sense why it is AMD and not Nvidia",Neutral
AMD,"Yea, at best it's dual sourced, or maybe it's going to be used for the IODs, if Venice actually uses Samsung (which I doubt).   FWIW, another prominent (though I'm very dubious about how accurate he is) twitter leaker, jukon, thinks it is for the PS6.   IMO, this rumor ends up going nowhere. We went though similar things with Samsung 4 and 3nm.",Neutral
AMD,"Because AMD has significantly more understanding and insight regarding the processes involved, than random gamers?",Neutral
AMD,"Gives them more leverage in negotiating prices with TSMC, also could be looking at Samsung as a second source; AMD is probably doing enough volume now in the datacenter market that they can justify the costs of adapting their design to Samsung's fabs, and then use the Samsung chips for lower end datacenter products.",Neutral
AMD,GPUs more important?,Neutral
AMD,Vertical integration in CPU/GPU/memory is now the hottest thing. Amd wants one contractor to provide all the comments and and can have a streamline input in the entire stack without delays in communication,Positive
AMD,Because TSMC has too much demand and can’t keep up? They have also raised their pricing a absurd amount as they had no real competition,Negative
AMD,"Why would Nvidia hamstring their own very popular GPU line by switching to a subpar node with their Ampere generation?   Not saying I fully believe the rumor, just stating that it's not a totally implausible suggestion.",Negative
AMD,"Writing is on the wall with China and Taiwan, AMD won't be the first to move either.",Neutral
AMD,Capacity for Venice was secured eons ago. They're already sending samples to hyperscalers...,Neutral
AMD,wow TPU is way older than I thought lol,Positive
AMD,"I remember that specific article! What a trip down the memory-lane … Thanks for this!  > Advanced Micro Systems (AMD) has released a new revision of their Athlon64 S939 […]  *Oh dear, the glorious socket 939 and its Athlon&nbsp;64* — Makes me a bit melancholic already.   *That was the time to be alive*. That was true journalism at heart from enthusiasts!   Explaining every short and abbreviation with the respective written-out long version and bringing out pieces for actual hardware-hits (instead of today's hit-pieces over the next refresh-cycle), where you could readily feel the joy of the editors themselves writing the article, describing new hardware between the lines.  Not the nonsense clickbait sh!t we have to day, which fabricate news around a single ~~Twitter~~ *~~X~~* *Twix*-post from some leaker no-one knows anyway …  Back then, you could go days, not seldom even a week without a single hardware-news, and no-one bat an eye, as it was only fueling anticipation and building up anticipatory excitement.",Positive
AMD,Athlon 64 3000+ was my first CPU that I got brand new. I upgraded from a very ageing Pentium 133MHz. 8 years of new releases brought 13.5x of clock speed increase as well as a lot more instruction sets. Now my current PC is on an over 9 years old platform...,Neutral
AMD,"Doesn't really make much sense given it'll go against their super successful scaling strategy with their CPU products.  They cant just swap TSMC for Samsung chiplets and have everything all work out the same because they're designing these Epyc packages based on a very specific die size for the CCD's.  This makes scaling super easy for them in so many ways.  Adding in some new CCD based on a wholly different process tech seems like it would throw everything out of whack, no?  They've never done anything like this before.  They've always had the same process tech for all CCD's of the same type, for all ranges of CPU's of that architecture.  Dense/C versions are a bit different, but those are also a strategically produced different line.  You wouldn't do that just for a different process technology alone.",Neutral
AMD,"I/O die only makes sense if they are getting wafers cheap as they don't benifit from the leading node density as much, but they may like it for low power.  Midrange GPU would make sense.  One of the mid/low end APUs would make sense too.  PS6 handheld or even the main console APU could potentially work as well, but that would require consistent parametric yields since they don't have opportunities for performance binning.",Neutral
AMD,Versus current gen 5/4nm?,Neutral
AMD,"Venice isn't their AI GPU lineup, and AMD already confirmed they will use TSMC N2 for their MI400 series IIRC.",Neutral
AMD,"This is about CPU's, where AMD is currently the top dog.  Much like how Nvidia used Samsung for Ampere GPU's while still being competitive, it's possible AMD could utilize Samsung for Zen 6 and still be very competitive.  Especially because AMD's whole chiplet scaling strategy is still a lot more cost effective than Intel's messy bullshit.  Epyc CPU's are also not typically being pushed to higher limits, so efficiency sweetspots matter a lot more, and that's not always gonna be such a huge difference in terms of performance per watt.  It might hurt them a bit more in consumer, but overall they're probably pretty confident in their architecture teams to maintain improvements without relying entirely on process node gains.  Intel's P-core team definitely has plenty of question marks surrounding it since Alder Lake.",Positive
AMD,"> Because AMD has significantly more understanding and insight regarding the processes involved, than random gamers?  Dude, don't you think such a assessment is a 'lil bit too much to drop casually?! o.*0*  *This is Reddit!* 90% of users here are armchair-generals or virtual CEOs, who actually know their sh!t …",Neutral
AMD,That and Nvidia and Apple bought up all of TSMCs 2nm production time.,Neutral
AMD,"This is just a meaningless appeal to authority with no logical backing. Moreover, any decision to make chips in Samsung, whether they turn out true or not, will also involve non-technical factors like cost. They could very well choose to go with a subpar process if it is way cheaper. Beancounters often hold significant decision-making power.     And historically Samsung has had a number quality issues versus the having the same architecture made at TSMC, so these concerns are pretty fair.",Negative
AMD,Perhaps this would make more sense if this wasn't:   * just a rumor  * no mention on whether this was for CCDs or IODs  * being fabbed at Samsung which has a horrendous track record   But sure.,Neutral
AMD,"that's not a reason to choose Samsung over TSMC, try again",Neutral
AMD,"FWIW that is not necessarily how prices are negotiated between designers and foundries.   The roadmap has more effect, and TSMC, Samsung, or Intel being in the picture is rarely used to drive prices lower.   Sure competition among fabs and packagers may set the ballpark of costs. But designers rarely use the threat of going with a different process to get a lower/better price from TSMC, Samsung, etc.   If AMD is going w Samsung it is likely because Samsung's roadmap may align w AMD requirements for a specific design.   The reason why this news may be unlikely is that AMD does not have an stablished silicon team with Samsung (that I am aware of). And that usually is how you can tell any type of significant volume from a large design team is going to be on a given foundry.   However Samsung does have a very nice roadmap for the type of large dies AMD has in their pipeline for their monolithic SoCs (mainly their APUs and GPUs). So there could be some alignment there.   For the CCDs and IO chiplets + 3D cache, it seems AMD is very aligned with TSMC and the packaging flow there.",Neutral
AMD,"I understand dual sourcing for lower end SKUs, but the article makes it sound like the entire new EPYC line is moving to SF2, a very likely worse node than N2. This would impact the performance and potentially allow Intel to close the gap further, so I’m struggling to understand why. If this article is true, there could be something off about N2.",Negative
AMD,"Most analysts have AMD as a top 5 TSMC customer, and Venice is a flagship product from AMD.   It's hard to believe Mediatek can tap TSMC N2P for their next smartphone chips, but AMD couldn't for their high margin server CPUs?",Neutral
AMD,Because samsung made them a dirt cheap offer and Nvidia had no competition at the time.,Neutral
AMD,"Every year there's new rumors about AMD using Samsung for their 4nm and 3nm node, and then people like you always say the same comments like this, and then it never ends up happening. But then a new Samsung node gets announced, and the cycle repeats.   *Sigh.*",Negative
AMD,"Yeah, the Chinese overtaking Taiwan we're told to happen the next Monday morning since the 1970s …  Even announcing another round of *»This is the year of the Linux-Desktop going mainstream«* has more credibility to it, and actually is becoming reality rather sooner than later, thanks to Valve's Steam now, also their SteamDeck.  ---- Intel just dug up that age-old Taiwain-spectre, to scare investors and governments into submission for subsidies, and y'all damn fools all bought readily into that nonsense and manifested a threat, China really couldn't care about less.  *China has a host of other problems of greater importance* — Taking out Taiwan, will inevitably result in vaporizing it for EVERYONE, which wouldn't help Peking/Beijing one bit anyway to begin with. It's a futile undertaking and they know it.",Neutral
AMD,I still remember being excited about getting a Denmark (Opteron 165 - basically dual core Venice) and OCing it like crazy.  Those were the days... when dual core felt like overkill and a 50% OC - clocking higher than the fastest available SKU - could be reached with the stock cooler on an entry level part.  today's CPUs a way better and way more boring. Almost zero point to OCing.,Positive
AMD,"> They cant just swap TSMC for Samsung chiplets and have everything all work out the same because they're designing these Epyc packages based on a very specific die size for the CCD's.  Hasn't they've done so before already? AFAIK a bunch of CCXs were dual-sourced (TSMC, Samsung) and AMD even opted for *a three-pronged strategy* on sourcing (TSMC, Samsung, GlobalFoundries) for a single design of their CCXs (Ryzen, Threadripper, Epyc), no?  > They've always had the same process tech for all CCD's of the same type, for all ranges of CPU's of that architecture.  You think that Samsung's and GlobalFoundries' 14nm back then were identical down to the last bits?  Yes, they were the same process, but GloFo surely made quite a bit of custom tweaks on their own, don't you think?",Neutral
AMD,AMD desperately needs a I/O die upgrade so this may be their solution.,Neutral
AMD,"> as they don't benifit from the leading node density as much, but they may like it for low power.   AMD has been wanting to jump up to DDR5-9000 for a while if you've been reading the tea leaves/what comes out of their PHY guy via AGESA update, but the I/O die simply isn't up to the task as currently built. Jumping up to 2nm for a faster switching frequency and just adding a <profanity> load of transistors to make it less fragile solves that problem adequately. It's not elegant, but what's the point of more advanced materials (or transistors, as it were) if you never use them?",Neutral
AMD,"> Especially because AMD's whole chiplet scaling strategy is still a lot more cost effective than Intel's messy bullshit.  Still baffles me how Intel can't let go of their big-die philosophy, tanking their margins and destroying yields that way since years while even crippling their overall volume — Still times higher manufacturing-cost than AMD.",Negative
AMD,LOL. They did not. Both AMD and Intel are also doing bring up of high volume SKUs on N2.,Neutral
AMD,Nvidia has no products on 2nm. And AMD was first to tape out on 2nm with this CPU. Meaning they get all the early production to themselves. Think.,Neutral
AMD,"Interesting. Yet, I am quite certain the people at AMD and Samsung (or TSMC for that matter) still have significantly more understanding and insight regarding the processes involved than random terminally online gamers. Just a hunch.",Positive
AMD,">I understand dual sourcing for lower end SKUs, but the article makes it sound like the entire new EPYC line is moving   Which makes no sense because AMD already confirmed that Venice will be fabbed, at least for some part of it, on TSMC 2nm.   >SF2, a very likely worse node than N2  You are probably right, but the article also mentions SF2P, which very well could be Samsung's next real node jump after SF2 having very minimal PPA benefits over SF3 GAP.   So maybe they could catch up to TSMC 3nm rather than being decently worse than it, as they currently are.   >This would impact the performance and potentially allow Intel to close the gap further, so I’m struggling to understand why.  TBF I doubt Intel and Samsung are going to be in much different places if Venice is fabbed on SF2P and DMR on 18A, and AMD's design side also just gaps Intel's, so they could still win at what could be considered node parity.",Negative
AMD,It definitely feels like an ego thing that they didn't just copy Ryzen's super successful and ultimately quite straightforward chiplet scaling strategy.,Neutral
AMD,Who goes first? :),Neutral
AMD,Late 2026,Neutral
AMD,Your own recent posts are out of touch even if you claim authority because you're a director.  These Samsung foundry rumors are like Intel foundry rumors. I'll believe it when I see it.,Negative
AMD,">Interesting. Yet, I am quite certain the people at AMD and Samsung (or TSMC for that matter) still have significantly more understanding and insight regarding the processes involved than random terminally online gamers. Just a hunch.  Maybe. However, that may be why u/heylistenman is asking why AMD is rumored to be switching foundries, to get insights from people more involved in the industry than random gamers.",Neutral
AMD,We’re not allowed to discuss this rumor as mere enthousiasts frequenting a forum about hardware? Why do you think AMD would move EPYC to SF2 instead of N2?,Negative
AMD,"> It definitely feels like an ego thing that they didn't just copy Ryzen's super successful and ultimately quite straightforward chiplet scaling strategy.  Well, I think Intel at least *tried* to copy AMD's chiplet-paradigm helplessly for the last couple of years …  It seems that Intel was taken totally by surprise on anything Chiplets and got caught with their pants down (again), when AMD brought it this quick to market, when having worked on them for a decade plus.  Though IF Intel would already worked on anything *disintegrated silicon* for said 10 years by then (like Intel claimed when announcing their heterogenous '*Mix and Match*'-approach around 2018), *Intel would* ***not*** *have needed +6 years for finally reaching a fairly comparable design-approach* only half a decade later in 2023 with Meteor&nbsp;Lake (also Arrow Lake, Sapphire Rapids, Ponte Vecchio).  Since sure enough, all these disintegrated designs \*somehow\* brought Intel truly massive troubles engineering-wise and they had tremendous difficulties to overcome those for years — The years-long horror-stories over validation and dead-on silicon past tape-outs on *Sapphire Rapids* and *Ponte Vecchio* for example are testament to that … *Meteor Lake* was also everything but a performer.  ---- The issue at hand for Intel was quite many-layered …  * Intel hardly knew how to do it (despite claiming otherwise for years). *Shocker!*  * Intel had no real equivalent to AMD's SuperGlue aka *Infinity&nbsp;Fabric*™   That's for sure the most striking one, obviously — The reason they had to wait for PCi-E 5.0 to become a thing, for ""Intel's"" *CXL* to finalize upon it (which in itself is basically just AMD's former *CCIX* in disguise as Copy-pasta anyway done out of spite).  * Intel haven't remotely had adopted a design-strategy by then, which would've offered so to speak ""intelligent"" chiplets/tiles, which could be freely thrown together randomly at will on a (PCi-E) bus (which AMD actually evidently can with their CCX and I/O-dies since ages).  So Intel's claims before the press in 2018, of already working on disintegrated silicon since years already (and that AMD *weren't* actually as spearheading as they became), was pure virtue-signaling, a blatant lie.  As a result, Intel's IP-blocks still remained virtually *dumb* (in the sense of *head*|*less*) for years on out afterwards and most of it had to be *re-engineered from scratch* in all this other trouble like lay-offs, down-sizing or Intel's road-maps being constantly thrown out again (only to start over once more).  The reason why AMD had a years-long edge on chiplets from the onset, was since Intel just couldn't place or handle the stuff as ***independent*** *IP-blocks* — Everything was grown intercoupled and -linked together.  AMD on the other hand already had a decade-long headstart, due to their already well-tuned modular concept of IP-blocks (building-block principle with IP-blocks to freely chose from) attuned for the console-era from back then for the console-contracts (those, Intel always made fun of since).  So Intel allegedly been working on chiplet-esque designs for years already, was utter bullsh!t, and that's actually the sole reason WHY Intel struggled so hard for years with anything Tiles — They started at 0.",Neutral
AMD,"> It definitely feels like an ego thing …  Well, yeah. It definitely IS a ego-thing for Intel — They called them *Tiles* purely out of spite.  Since you can't just call basically the very same what your competitor has, the same name, can you?   That's not how Intel rolls nor would have ever done anyway, even if doing so, would've saves them a lot of engineering-pain in the arse, even more teething-problems and cost them years of falling back behind their only lone competitor …  Truth be told, Intel's *Tiles* are basically in essence just Copy-Pasta: *A botched Copy'nPaste-job from AMD's chiplets*, yet relabeled as 'Tiles', just for Intel trying to pretend to have their ""own"" chiplet-esque implementation, even if it's basically the very same …  Wanna hear a joke? When Intel back then around 2018 out of the blue announced their heterogenous *Mix and Match*-stuff (pretending, they'd already worked a decade on this by then), [the actual effing PowerPoint-slides actually didn't even incorporated anything called 'Tiles']( http://web.archive.org/web/20210923105815/https://newsroom.intel.com/wp-content/uploads/sites/11/2018/08/monolithic-vs-heterogeneous-infographic.pdf) — *Called them* ***chiplets****!*  Yet some big weak weasel's ego was hurt in Santa Clara, and they renamed it *Tiles* shortly afterwards.  > … just copy Ryzen's super successful and ultimately quite straightforward chiplet-scaling strategy.  They joke is, due to both of them share a cross-patent agreement, Intel even would've had access to AMD's patented stuff over everything chiplets — Chances are Santa Clara chose not to over license-fees, or spite …  *That's how you're f–cked over by your own ego* — Rather throw years of potential lead into the gutter, instead of even *thinking* about having to share some meager percentage of profits of yours with others.  That's Intel for you. A bunch of braggarts and loud-mouths, wo always think they can do and know better, yet fail nigh every single time and still can't bring themselves to be humble for once …",Negative
AMD,"I believe  For N2; APPL A20/M6, NVDA Feynman, INTL Nova Lake   For N2P; APPL A21, MTEK 9600, QCOM SDE8G6, AMD Venice  AVGO, MRVL, AMZN, MSFT, GOOG have also volume contracts for n2/n2p production with several SKUs in bring up already.",Neutral
AMD,AMD is actually rumored to have the first TSMC N2 products out.,Neutral
AMD,Late 2026 is Rubin and it's on 3nm. 2027 is Rubin Ultra also on 3nm.,Neutral
AMD,"You're absolutely allowed to discuss the rumor. I am also allowed to point these discussions are way beyond the pay grade of a lot of people in this sub.  FWIW Designs for 2nm generation are already in production, so that ship has sailed. If AMD was to be planning on doing EPYC on SS 2nm, they would be already be at the bring up stage or close to it. Which does not seem to be the case.  Edit: However Samsung's foundry roadmap aligns with some of AMD's monolithic die roadmap, so it would make sense for AMD to at the very least explore their foundry options there. But for 2nm nodes, those negotiations should already have happened a while back.",Neutral
AMD,"AMD only ever claimed N2, which is interesting because Mediatek had no problem specifying N2P, though IIRC their original press release *also* only had ""N2"".   Given that AMD seems like they will launch their N2 products the earliest, I think they might be on N2 rather than N2P, but who knows. The differences between the two nodes seem very minor anyway.",Positive
AMD,"Is it strange that i feel like 3nm is just old tech now ;)  Google AI says  ""Nvidia is heavily invested in 2nm chip technology, planning its next-generation ""Feynman"" AI architecture on this advanced process node via TSMC, targeting mass production around late 2026 or 2027, following its current ""Rubin"" (3nm) chips, with its cuLitho tech aiding the shift for improved efficiency and speed, aiming to maintain leadership in AI hardware despite rivals also targeting 2nm""  Google AI wouldn't lie would it? ;)  Watch Nvidia 2nm get delayed until 2029 because no ram available ;)",Neutral
AMD,">You're absolutely allowed to discuss the rumor. I am also allowed to point these discussions are way beyond the pay grade of a lot of people in this sub.  I'm just baffled why you brought it up if you seem like you *agree* with the premise of the original comment anyway, other to just dunk on gamers? Lol.   >FWIW Designs for 2nm generation are already in production,  Only 18A and Samsung 2nm, both of which are likely not on par with TSMC's 2nm node. Meaning calling them part of the ""2nm generation"" is a stretch.",Neutral
AMD,Google AI is telling me the correct info: https://i.imgur.com/HPg7fhN.png,Neutral
AMD,"> I'm just baffled why you brought it up if you seem like you agree with the premise of the original comment anyway, other to just dunk on gamers? Lol.  It's their pastime.",Negative
AMD,Another AI miracle.,Neutral
AMD,"AI has fuzzy memory like humans. The way attention works in AI is based on context. Certain keywords increase or decrease certain likelihoods.   Vague prompts increase the likelihood of a hallucinated response.. In my screenshot you can tell it also provided the picture of Nvidia's official roadmap. This is called grounding.   Can't trust AI 100% time just how you can't trust humans 100% of time. But there are ways to increase accuracy via grounding, or prompts. We're still figuring out how to extract the best out of these models. They aren't perfect.",Neutral
AMD,"The rumor that Samsung will manufacture the PS6 APU doesn't hold up when you look at standard semiconductor timelines. If we assume a late 2027 launch, mass production needs to start by early 2027. In the semiconductor world, you don't select a foundry 12 months before production; that decision is locked in 2–3 years in advance during the design phase because the physical design is tied to a specific process node. At this stage, AMD should already be in the 'bring-up' phase with early working silicon in the labs. Switching to Samsung now would require a massive redesign that fits neither the timeline nor the logic of a major console launch.",Neutral
AMD,"*— Article itself is in Korean —*  The article states that according to South-Korean media reports, AMD is in talks with Samsung's Foundry-division about chip-manufacturing …  While AMD will expressively **not** leave TSMC behind, especially not in the imminent advent of their N2-ramping (the majority of AMD's setup is scheduled to move to N2 later on), AMD may adopt a two-pronged strategy for possibly having Samsung's foundry manufacture the chips for the upcoming PS6.  So AMD could eventually use Samsung's SF2-process for PlayStation&nbsp;6-chips, for the console which is scheduled for end of 2027, to split and shoulder the load of manufacturing, for ease of availability.",Neutral
AMD,"Samsung: ""Get an ambulance... but not for me!""",Negative
AMD,"RAM might be three times as expensive and rising, but at least we get to reap the benefits; for instance we all get to marvel at whatever the fuck this AI generated thumbnail is trying to convey",Negative
AMD,"Has Samsung ever made x86 chips? Is there any reason they couldn't, in partnership with AMD? Or is this also suggesting that Sony could be moving to ARM?",Neutral
AMD,From the article it seems they’re not actively replacing TSMC.   AMD is choosing to probably dual-source chip manufacturing to both Samsung and TSMC and treat Samsung as a secondary supplier.   Most likely it is because TSMC allocation is too full for them to make the amount of chips they need.,Neutral
AMD,You're assuming they will use Samsung at launch.,Neutral
AMD,"You don't have to lock in your foundry choice 3 years in advance, but 2 years is pretty close.  Three years in advance, you are kicking the tires on the PDK and making sure the node will achieve your goals for your design.  That is not so expensive you cannot do it for multiple foundries.  The next step, is finalizing the layout for masks, which culminates in the tape-out.  You could do that for multiple processes, but it does not make much sense, as you should have known which process you prefer before that step.  Masks are very expensive so you don't want to make them for two different processes.  So tape out is the last chance you have to make a foundry decision, but you should make it months before that.  As an example, AMD announced back in April that Venice chiplets taped out on TSMC 2nm.  Venice is going to be included in Helios racks that are being delivered in Q3 next year.  So maybe a little less than 1.5 years from tapeout to product.  Add in the time for doing the physical layout, and you get pretty close to 2 years.  So I'd expect that right about now is when the final decision on where to manufacture PS6 is being made.",Neutral
AMD,"> In the semiconductor world, you don't select a foundry 12 months before production; that decision is locked in 2–3 years in advance during the design phase because the physical design is tied to a specific process node.  I understand where you're coming from and the reasoning of year-long lead times for a given process is pretty much self-explanatory, but it's not *that* much like 2–3 years. It's 1–2 years in advance, if it takes that long.  Also, it's not that AMD (just like others like nVidia, Broadcom, Qualcomm et al) aren't used to foundry-hopping since decades due to the short cadences of GPU- and CPU-release cycles — *They all have extremely well-attuned process-tailored teams for such foundry-stuff, speeding up things massively*.  Taking GPUs as an example, these are AMD- and Nvidia-teams of hundreds to thousands of process-engineers, VLSI-magicians and all kind of specialists for EDA-tooling and whatnot else is needed, working closely on-site directly at foundries like TSMC, Samsung or GlobalFoundries and interoperate with the foundry's respective specialists for stuff like that …  So given the current time-line at end of 2025, it would be *virtually two full years* up to it, until a launch scheduled for end of 2027, which ought to be pretty sufficient and timely fitting — Pretty realistic if you ask me.",Neutral
AMD,"> At this stage, AMD should already be in the 'bring-up' phase with early working silicon in the labs. Switching to Samsung now would require a massive redesign that fits neither the timeline nor the logic of a major console launch.  Nah.. Remember the launch of Zen&nbsp;5 (Ryzen 9000), which AMD suddenly postponed, due to unspecified 'quality-control' issues at early production-runs? *They recalled and canned ALL already shipped batches prior to launch*.  Who knows what it really was, but it's possible, that it was comparable to Intel's oxidation-issues or comparable process- or design-flaws — It still didn't took them years to correct that, but just a couple of *months* …  I'd say, over the years no other companies became as time-efficient and extremely attuned to toss a design, fix it and redesign it ASAP, only to bulldoze through the whole initial pre-manufacturing process once again in a speed-run-like fashion all over again, as much as as AMD and nVidia are by now — I'd bet, that a complete design-change might take them barely more than 3–6 months total.",Neutral
AMD,"You're joking, but considering how much contracts and design-wins Samsung's foundry-division got over the months from others, I'd consider them the *»Foundry-winner of the Year«* for 2025.",Positive
AMD,I don't see why they could not make X86 chips. But this is suggesting that the PS6 chip could be manufactured both at TSMC and at Samsung to ensure that they don't have availability problems.,Neutral
AMD,The process is architecture agnostic.   They're probably doing the switch to free up volume for the best TSMC processes for AI and CPUs.   Samsung might also be offering a price competitive option.,Neutral
AMD,"Samsung can fabricate whatever their customers bring them. So yes, if AMD pays them to fabricate x86 chips, there is no reason they can't do it.",Neutral
AMD,Global Foundries 14nm node was licensed from Samsung. Their 12nm and 12nm+ was just refinements of that node. So their nodes have been used for x86 at least.,Neutral
AMD,"> Has Samsung ever made x86 chips?  Yes, they manufactured a bunch of Intel-designs over the years.  In any case, *processes don't care about architectures*. These are architecture-agnostic, as others pointed out.  > Is there any reason they couldn't, in partnership with AMD?  Nope. A foundry manufactures what the client asks them for, or don't, depending on contractual incentives.",Neutral
AMD,More like everyone wants tsmc chips so much they became too expensive and Samsung made a half decent chip at a lower price,Negative
AMD,"And even relatively inexperienced teams can pull off faster timelines than 2 years. I was following the crypto space early on when ASICs started being taped out.   And you had relatively small teams that manged to design, raise funding and get shit out the door within not much more than a year. Granted a Bitcoin miner is some of the easiest chips out there to design, so that part was not very time consuming. But you had some companies like KNC that launched on at the time leading edge nodes like 20nm planar back in mid 2014, so not like they launched on some ancient/quick node. And they were formed and started taking pre-orders in early 2013.",Neutral
AMD,Are you really going to write every comment using AI?,Neutral
AMD,"> But this is suggesting that the PS6 chip could be manufactured both at TSMC and at Samsung to ensure that they don't have availability problems.  I'd just see it as a repeat/extension of the first Ryzen-launch — AMD not just adopted a two-pronged approach back then with the original-Ryzen in 2017 (TSMC, Samsung), they even opted for *a three-pronged strategy* (TSMC, Samsung, GlobalFoundries) for a single design of their CCXs (Ryzen, Threadripper, Epyc) …  Still wasn't enough to satisfy demand anyway — Limited availability, as markets just gobbled up ALL of it in months.",Neutral
AMD,"That's not how it works.  You don't just bring a design to a silicon foundry the way you bring a blueprint to a factory.  You have to create the design using that foundry's software tools.  If AMD uses Samsung as a second source, that means they re-created the low-level design for Samsung from scratch.  None of the people who created the TSMC design would be legally allowed to take part.  That leaves Samsung-specific employees and the broader design team that creates the higher-level design.  It is true, of course, that there's nothing special about an x86 chip versus some other chip.  The point is, Samsung just runs the process.  AMD has to design the chip on that process, with assistance from Samsung being limited to issues well outside the scope of the high-level chip design.",Neutral
AMD,Pre 10nm Intel was generation ahead of Industry by a full node shrink they introduced FinFet before everyone was on HKMG,Neutral
AMD,"I'm reasonably sure Samsung has never manufactured any x86 chips.  Intel did use them for chipsets in the past, but not processors.    Actual Intel x86 processors have always been made by Intel in-house, with the exception of their recent use of TSMC.    AMD x86 processors have been made in-house, by Global Foundries (which was spun off from AMD), and by TSMC.",Neutral
AMD,Taiwan says China will invade in 2027.  That's why 2026 will see a massive shakeup in TSMC orders.,Neutral
AMD,"Yup, I think 2 full years is actually a ample amount of time for a experienced and well-attuned team, and being tightly integrated at given foundries on top of that, tremendously helps to cut months of time.  AMD, Nvidia and others are used to short, highly stressful stints like that, since it's virtually their daily work.",Positive
AMD,"is it really AI? using "".."" and space after ""—"" makes me think it wasn't AI generated.",Negative
AMD,"Good Lord, what have you people with your accusations of using AI for comments every time?!  You really become a pain in the butt with this sh!t over AI-stuff — *Just because I use hyphens?!*  You can browse my comment-history, and you'd see, that I'd quite likely already used hyphens on my first comments ever made here on Reddit about 8 years ago (a time, when AI wasn't even existing). It's a style of writing for me ever since I use since like two or three decades, as I got used to it, as many are from the scientific/technical-driven realms.  ---- And talking about y'all imbecile reasoning over em-dashes (or en-dashes for that matter); Did it ever occur to you, that using em-/en-dashes is actually quite common, actually often mandatory in the scientific field of applied since?!  Ever opened a effing book? Or read a news-paper the last decade? Em-dashes (—) are *usually used to bind two sentences of the same scope*, which belonging together analogously. Meanwhile a en-dash (–) is used for date-formats in everyday life and money-values et al and so forth.  In any case, in print media (newspaper, books, scientific works), dashes are heavily used, in the field of typography even as a stylistic element … and if you would've cared to look at the way I wrote my post;  I usually try to write while trying to maintain a pleasing appearance for *ease of reasoning*, and dashes tremendously help with that, that's also why I use horizontal rulers within posts (differentiate separate issues).  You can see the source of my posts using RES (Reddit-enhancement suite), and discover, that I often also use proper non-braking white-spaces (`&nbsp;`) for when it's needed, given proper HTML-entities like &times; (`&nbsp;`) for formulas or given specific brand-nams and whatnot (Intel**_**Core, Intel**_**ARC; AMD**_**Ryzen, AMD**_**Radeon; nVidia**_**GTX/RTX).  In short; *Shut up and learn about [Quad](https://en.wikipedia.org/wiki/Quad_\(typography\)) and that sort of stuff!* -.-",Negative
AMD,> None of the people who created the TSMC design would be legally allowed to take part  Really? I would be shocked to learn that people who worked on the physical design of a chip fabbed at TSMC couldn't also work on a design for a chip fabbed at Samsung.,Neutral
AMD,"> I'm reasonably sure Samsung has never manufactured any x86 chips. Intel did use them for chipsets in the past, but not processors.  That's not true. Just because Intel never \*admitted\* it publicly (so save face and protect their golden cow, their stock), doesn't mean, Intel never outsourced before — They did in fact, and they did in fact *en masse* for years …  Since back then during the self-inflicted 14nm-shortages (Meltdown, Spectre, Foreshadow; also 10nm-issues), Intel's CEO Bob Swan reshuffled much capacity and brought over almost all chipsets to Samsung being fabbed there.  Their low-end CPUs like i3s and Pentium were later manufactured at Samsung as well.  TechPowerUp.com – [Intel Turns to Samsung in Order to Resolve CPU Shortage on the 14 nm Process](https://www.techpowerup.com/256613/intel-turns-to-samsung-in-order-to-resolve-cpu-shortage-on-the-14-nm-process)  (2019)   TechPowerUp.com – [Samsung Scores PC CPU Manufacturing Order from Intel](https://www.techpowerup.com/261641/samsung-scores-pc-cpu-manufacturing-order-from-intel) (2019)  > Actual Intel x86 processors have always been made by Intel in-house, with the exception of their recent use of TSMC.  Just goes to shows that you ain't aware of the actually situation and not really informed. No offense though!  No, with all due respect, but that's pure nonsense — All throughout the time of Intel pumping the mobile market with Atoms (2007-2013, fighting the myriad of ARM-licensees like Samsung, Qualcomm, MediaTek), these very Atoms (especially each and all for the mobile market), have been made *virtually exclusively by TSMC* for several years in a row (and NOT Intel itself) since at least 2009.  True be told, I'm not even aware if Intel ever manufactured their own Atoms ever again past first Gen Atoms (Silverthorne) in 2008, when they basically showed over their whole Atom-business to TSMC for several years.  ArsTechnica.com – *Atom can’t feed fab monster;* [Intel outsources chips to TSMC](https://arstechnica.com/gadgets/2009/03/atom-cant-feed-rd-monster-intel-outsources-chips-to-tsmc/) (2009)  Remember, up until recently with TSMC after them announcing ""outsourcing-possibilities"" (to keep pace) in 2021, Intel **never** had ever admitted publicly upon any outsourcing, except back then as mentioned above.  This was done fully *strategically*, as it would've signalled an actual *admission of struggling and falling behind* (and necessarily having to contract outsiders), which Intel has been desperately trying to avoid for years on end since 2012 or so and through-out the whole fiasco of their 10nm™ …  Ever before, these were all 'rumors' Intel either vehemently disputed publicly (despite doing *the exact contrary*), or just let pass without comment, despite everyone involved knew from day one, that such rumors were 100% true.",Neutral
AMD,Is there a source for this claim?,Neutral
AMD,"It's laughable, as em-dashes (or en-dashes for that matter), have been used especially in typography for at least *the whole of the 20th century*. Actually, around 1850–1990 was the height of such usage in anything print.  Look at any newspaper-archive; Virtually every other older printed text (and even handwritten ones) the last hundred years, uses dashes for *separating the principal clause from the subordinate clause* (En-dash; –, HTML:`&ndash;`) like a counter-argument the writer dropped mid-sentence (for not trying to disrupt the readers' current train of thought), while embedded sentences either \*within\* a principal clause or between principal and the subordinate clause, is ought to be signified with a Em-dash before at the beginning and the end afterwards (Em-dash; —, HTML:`&mdash;`).  **Edit:** Specifically full dashes are also used in writing (directly at the point of breakage), to signify to the reader a sentence or written (direct) speech, which ended abruptly mid-sentence. The same goes for suspension marks (ellipses; …; `&hellip;`), if the recipient is supposed to fill in the blanks by himself.  I even often type them directly using the num-block (Alt+0150 for – and Alt+—). … but I'm using AI! -.-  I just despise wrong spelling or the complete absence of punctuation marks, that's all.",Neutral
AMD,"The use of ""—"" strongly hints at AI, since virtually nobody types that character.  99%+ of people type "" - "" instead, because ""—"" is a special character that can't be simply typed on any normal keyboard.",Neutral
AMD,"Bro, you wrote a novel in this post with pixel perfect formatting and bunch of formatting flourishes. Nobody does that. You are using AI. It's clear as daylight.",Negative
AMD,"There are a lot.  https://news.sky.com/story/taiwan-to-prepare-for-combat-by-2027-president-says-as-he-warns-china-is-preparing-to-take-the-country-by-force-13475504  Also, I'm seeing that China may surround Taiwan and quarantine it.  No matter how you look at it, this will damage TSMC.  TSMCs chip future is likely to come to an end around this time.  Samsung and Intel need to start ramping.  They have about a year.",Neutral
AMD,"Option+Shift "" - "" is pretty easy to type on a mac—although people usually don't bother.",Neutral
AMD,"For many linux users AltGr + Shift + - results in ""—"".",Neutral
AMD,Long press the - on an iPhone and it pops up as an option.,Neutral
AMD,"> The use of ""—"" strongly hints at AI, since virtually nobody types that character.  No, it does actually **not**. If anything, it just shows that LL-models (for training AI in the first place) were crawled (illegally) upon huge libraries of digitized text-corpuses to begin with …  I mean … *Where do y'all fools think, these AI-bots got those dashes from in the first place?!* Exactly.  In any case, if you happened to put open any decade-old newspaper, you'd know, since newspaper and articles (at least the ones being written by actual humans) to this day are full of hyphen and dashes.  Anyway, at least I do since decades (and many I know too), and so do many in the scientific field in general.",Negative
AMD,">Nobody does that.  At least one person does that, and did that years ago already. Seriously, look at the comment history",Neutral
AMD,"Well, no kidding. The U.S. constantly try to push it, no? I mean, aircraft-carriers in the Taiwan straight?",Neutral
AMD,"Or *Option*(+Shift)+*Minus* for different dashes (`⌥`+`-`), while *Option*+*Period* for ellipses is another (`⌥`+`.`), and (`⌥`+`+`) for the plus-minus-sign (±) — I picked those up on a Macintosh IIfx back then in the Eighties at the university and have been using those ever since …  Though it's not just Mac&nbsp;OS&nbsp;X which has been incorporating special typographic characters into keyboard-layouts since decades even under Mac&nbsp;OS&nbsp;9 and below (e.g. proper typographic quotation-marks [“…”] or Guillemets [»…«]), most Linux-distributions' default keymap are also filled to capacity with loads of special characters, of which most you can't even reach under Windows without Alt-Codes via Unicode through the num-pad — Just see Ubuntu's default keyboard-layout …  That's why the likelihood of layouters, writers, editors and content-creators (in the *classical* sense at least, journalistically speaking), often use Mac OS out of principle — The chaos and annoyance to even reach special typographic characters under Windows, is a complete mess and no-go for many.",Neutral
AMD,"Because sure, why not.",Neutral
AMD,Yeah because one cannot pay more for ram than cpu.,Negative
AMD,"As someone who doesn't have a single piece of Intel silicon in my build, I've never understood people cheering on their downfall. We need competition, people, or shit like this happens.",Negative
AMD,Bought 64GB memory and 4TB storage in June. Have bought a Ryzen 7 9700X and RX 9070 this Friday.  I feel quite lucky.,Positive
AMD,"I don't think this has anything to do with the memory shortage anymore, it's just pure greed.",Negative
AMD,"Of course they do. Black Friday prices aren't supposed to be the new normal prices. I'm sure Intel and Nvidia are doing it too, this is not newsworthy.  In the Videocardz article about it they even admit this is not raising the normal prices, it's just prices returning to normal after Black Friday:  https://videocardz.com/newz/amd-rumored-to-raise-ryzen-9000-and-older-cpu-prices-tonight",Negative
AMD,Won't someone think of the (checks notes) 355 BILLION dollar company!?!?!?!,Neutral
AMD,At this point I can see me running my 4670k GTX 970 build from 11 years ago until 2040 and beyond. Got my duct tape ready.,Neutral
AMD,A decent PC about to cost the same as a new car,Negative
AMD,"“Hey people are still buying ram at these prices, let’s raise our prices too!”",Negative
AMD,"They did not get the memo, that people aren’t building PC due to memory prices?  Good luck AMD, I managed to get 9800x3d for 399€ brand new and I was still on the fence about it. Like really on the fence, it was a stretch. Cause they blow up!  Not to forget I am an enthusiast. I upgrade GPU every gen and always get the latest platform.  If this was a strech for me, then 90% won’t even look at those if you increase the price, especially now.",Neutral
AMD,they’re ryzen the prices  i’ll go now,Neutral
AMD,"AMD: ""What are you going to do? Buy Intel?""",Neutral
AMD,"Nice, now it's gonna be rising prices for RAM, GPU, and CPU!",Positive
AMD,"RAM makers are not going to like this, they rather prefer CPU prices being low because now when they ship DDR5 memory to margin highs  they want consumers to not second ask themselves regarding PC upgrades like they already do.",Negative
AMD,people who built a pc at summer/spring must be happy af,Positive
AMD,"This is what happens when there is no serious competition around the horizon, it's not the first time that AMD has done this btw.",Neutral
AMD,I think AMD should reduce prices instead of increasing.,Neutral
AMD,"Intel is back, AMD should lower their prices. It would be better for their future.",Positive
AMD,"Just bought mine, sorry everybody. Hopefully RAM and motherboard prices will crash for everyone else to make up for it, now that I ovepayed.",Negative
AMD,man i was JUST thinking of upgrading from my 3900x....,Negative
AMD,I guarantee you they wont raise prices of their datacenter CPUs because you bet your ass their customers would switch to arm,Negative
AMD,"Proof that not having competition is bad.  Fast RISC-V chips soon, hopefully.",Negative
AMD,"If AMD holds the commanding lead in the retail market share with their Ryzen CPUs, there board and investors will probably be demanding more profit be taken.",Neutral
AMD,"Damn, didn't know CPU chips had tiny ram slots in them",Negative
AMD,Good thing I got my 9800X3D before this happened.,Neutral
AMD,And they will not drop them until Intel gets their shit together,Negative
AMD,"Hehe, glad I got my Ryzen 9 a few days ago.",Positive
AMD,I'm so glad I didn't wait longer to build my PC,Positive
AMD,"when intel was the top dog, everyone excused it by saying they had the best performance and stability, amd bulldozer was bad blah blah blah don't buy AMD  now that AMD is the industry leader all of a sudden competition is important  so obvious.",Negative
AMD,I mean sure why not . Their gpus finally hit MSRP 8 months after launch so something gotta give,Neutral
AMD,Glad I bought a 9800x3d last week I guess,Positive
AMD,"Instead of reporting on your lack of information, be a farking journalist and actually research the answers before writing about it. Even AI can match this level of journalism.",Negative
AMD,"Depends how much but I like to think this is my final amd product at this point. They have not been making the best decisions the past few years...   Like if intel is cheaper and more powerful at this point, might as well get that then.",Neutral
AMD,Because AMD is a damn corporation aiming to keep its margins while learning from the giants about consumer exploitations.,Negative
AMD,"The setup looks clean, but the price news definitely stings.",Neutral
AMD,"No, they're not:  https://www.tomshardware.com/pc-components/cpus/amd-isnt-increasing-prices-on-cpus-at-least-for-now-ryzen-appears-to-be-safe-from-the-ai-hysteria",Neutral
AMD,I think it’s way more likely rx 9000 has 0% than it does like 0.1% I mean it just makes sense 0 people bought it,Neutral
AMD,Cache memory is going up in costs so AMD has to make up for it. /s,Neutral
AMD,"You've been clickbaited. Yes, prices go up again after a sale. This is a non-story.",Negative
AMD,"This is not AMD raising prices. This is prices going back to normal after Black Friday is over.   The Videocardz article on this ""news"" says this is a return to normal, not the normal prices being raised:  https://videocardz.com/newz/amd-rumored-to-raise-ryzen-9000-and-older-cpu-prices-tonight",Neutral
AMD,"""Eeh, might as well"" -Lisa",Neutral
AMD,Very good example being nvidia vs amd. Nvidia can keep their prices outrageous just because there’s no meaningful competition.,Neutral
AMD,Intel pricing is pretty competitive these days. As soon as gaming isn't your top priority they aren't a bad choice at all.,Negative
AMD,What model wifi card or Ethernet controller do you use? I've had really good luck with the Intel ones.,Positive
AMD,"But you see, good guy AMD would never do such a thing!",Neutral
AMD,People cheer on the downfall of Intel for the same reason they'll cheer on the downfall of a rival sports team.,Neutral
AMD,"last time after amd 64 intel got back swinging, This time it looks really sad",Negative
AMD,"The people that cheered for Intel's downfall were probably computer users from the time when it was Intel or nothing, that wasn't a great time because competition was low.",Negative
AMD,intel was a bad actor and actively tried to sabotage AMD and shun them from system integrators. sorry can't sympathize with intel.,Negative
AMD,"I bought about 80 in the last ten years, I'm doing my part.",Neutral
AMD,"I hate to break it to you, but the existence of Intel does not prevent sales being ended and prices returning to what they were previously.",Negative
AMD,"> I've never understood people cheering on their downfall  They were almost a monopoly for a long time, excepting the AMD K8 era.  Intel's business practices and strategic decisions have been extreme short-term profit motivated since.. even before 2010? When they were dominating, they would just hold back from bringing tech to market to maximize profits. Deliberate decisions to rest on their laurels, to not invest into real R&D. It's been a slow motion train wreck, competitors with much more growth R&D momentum catapulting past them, leaving them in the dust.  It's kind of like a desire for justice? Cheering on an org reaping what they sow? Of course, in America companies like Intel, or certain industries, like in 2008, get bailed out anyway. No justice; moral hazards aplenty.",Negative
AMD,"Well just read a article about intel making their own version of x3d cache for their next cpu linup, which will have lots of cache memory. If they actually perform for once and not take 300 watts to do it, could be enough competition to push AMD to lower prices again.  A big will see though.",Neutral
AMD,"Jealous. I bought 5x20TBs two months ago from Amazon, and they finally acknowledge it was lost somewhere and gave me a refund. I can only buy 3x20TBs now.",Positive
AMD,I bought 64gb of ram 2 years ago for $130. 6000mhz,Neutral
AMD,"I bought my 7600X for $150 and DDR5 32GB kit for $80, and additional 1TB SSD for just $40. Now I see both the SSD and the RAM being 2 - 4x the price of what I paid for nowadays when I am browsing our local online marketplace. I can say that I feel the same.",Neutral
AMD,Damn.. I bought 64GB memory last week and a 9800X3D. Paid $380 for the memory.. at least I got the CPU before it increased.,Negative
AMD,"Yeah, I jumped a little earlier and I'm still sitting on AM4, but when it looked like tariffs were gonna blow up the PC component market I made sure that I had a 5950X, 64GB of Samsung B-die, an 8TB flash drive, and a 9070XT.  Planning to coast on this for the next few years and hope there's still a hobby on the other side.",Neutral
AMD,"I bought a 9950x3d and 192gb like 2 months ago, i am happy i did so ahahah",Positive
AMD,The memory shortage is just one symptom of AI chewing through supply and increasing prices.   The DRAM still needs a CPU…,Negative
AMD,"""Black Friday sales have ended"" is not in fact a sign that we have been overtaken by greed.",Negative
AMD,"its... capitalism, from a publicly traded company. When was it ever not about greed??",Negative
AMD,"Nope, there isn't. It's simply a power move by AMD Ryzen because they know exactly the consumers will still choose them anyway over Intel that is already dragged on the mud by the reviewers and tech enthusiast community in general.",Neutral
AMD,"Please boost this, the entire thread is being ragebaited by an absolutely garbage article writing about a complete nothing burger. In the article it reads  ""The timing follows Black Friday and Cyber Monday discounts.....return to standard pricing.""  The article puts in complete rumor gibberish, and baits with RAM drama to confuse the reader because its literally just saying ""AMD cpu's went on a discount for Black Friday, they are now losing the discount."" Except the college student who wrote this had to pad the word count to 1000 on a 50 word article.   Completely nothing burger written for interaction bait.",Negative
AMD,"I mean, their market cap is only a little higher than the GDP of Portugal.  They're only slightly too big to fail.  I'm pretty sure the other tech giants throw fries at them at the lunch table.  It's actually kind of sad, really.  NVIDIA could find the cash to acquire them between its couch cushions at this point.",Negative
AMD,Nah don’t worry new car prices are through the roof as well,Negative
AMD,"Yep, Ive been planning on building my dream PC since I can finally afford to.   Not going to happen if the whole industry decides to fuck us. I'll watch and laugh at them when AI pops and the market is flooded with their existing and planned hardware.",Positive
AMD,"It will be sooner than later I can feel. These companies will go extinct, they are too greedy to exist.",Negative
AMD,"Intel isn't back right now... Arrow Lake isn't getting much traction,  the refresh isn't getting much traction (though it's honestly pretty good), so desktop DIY seems to be AMD's market.  They hence have pricing power.  For mobile AMD isn't really a big player but you don't buy DIY laptop chips so it isn't relevant.  Lunar Lake is pretty good (it's efficient, though ARM solutions still beat it in performance per watt) and Panther Lake is sounding better (ARM still wins, but it's a good gap over AMD and most people ex-Apple want x86).  That's a different market than desktop though.  When Nova Lake comes out it'll be a much better chipset than ARL, we'll see if they have giant cache chips or not which is what a lot of DIY people want.  If that happens then maybe Intel can compete better and exert more competitive pressure on AMD.",Neutral
AMD,Intel is back in what? Panther Lake is only M2 level,Neutral
AMD,"They're not raising them. In fact, the 9800X3D is cheaper now than ever.",Neutral
AMD,This article is complete clickbait about Black Friday sales ending. It applies to all of their competition as well.,Negative
AMD,There are people that prefer their cabling doesn't spontaneously combust.,Negative
AMD,not to mention 3D in the title of the CPU...,Neutral
AMD,considering how many reviewers pick a sales price and base their performance/dollar graphs based on that you may as well see this as raise compared to the misinformation you are fed.,Negative
AMD,I guess we will know in less than 24 hours,Neutral
AMD,"prices going up above msrp 18 months after release isn't ""normal""",Neutral
AMD,"This is just factually wrong. Look at GPU price history from MSRP to market price for the last year. GPU launches were incredibly inflated until these last 2 months, so what you think is ""black friday deals"" and ""cheap"" is what was supposed to be its original MSRP lol. Now this price hike is just to artificially raise the price to maintain the same profit margins they've had from the past year out of pure greed. No reason for AMD to raise prices on existing products on shelfs except for pure greed. No reason for RAM manufacturers to not offer long term contracts to large customers and OEMs when they are purposely causing a shortage, except for pure greed. No reason for Nvidia (who does not produce a large quantity of their FE GPUs themselves and relies on AIBs) to not bundle Vram with their dyes to the AIBs that have a smaller profit margin than Nvidia, except for pure greed.  20% of this is speculation of a AI bubble pop, the other 80% is companies seeing blood in the water and hopping on the price gouging train to artificially raise the prices for the whole industry. When all the businesses collude and work together to raise the price of computing power, which is now an essential commodity now that it powers the world, who stops them? I can't see it being government as they are apart of the majority of the demand for these AI datacenters.  Source you can check out: GPU Prices Crater Before Inevitable Opportunity to Screw Consumers - Gamer Nexus",Negative
AMD,"Counterpoint: Competition only works if there’s checks and balances to prevent price collusion. The stupid SSD mafia colluding and keeping prices high (DAE remember the great fire sale of SSDs in late 2023?). There’s no reason why a 2TB 990 Pro should be that close in price to a 9100 Pro. Were they losing money then? I am hard pressed to believe they were.  The skeptic in me however is willing to bet the RAM prices are never going to go down, and this will become the new normal, and they’ll just pocket the difference (unless there’s something major that happens like upstart Chinese suppliers flooding the DRAM market forcing them to).",Negative
AMD,"and the solution to that isnt shitting on Nvidia, its for AMD to make better cards.",Neutral
AMD,"I mean, there is no competition for the high end. When it comes to medium-high performance, the competition is absolutely there. It's really just the 5090.",Neutral
AMD,"this is partially incorrect.  as there were lawsuits and settlements about amd/ati and nvidia price fixing.  if price fixing is happening, there is no competition.  it is a fake competition, just like the memory industry, where a memory cartel sets their prices through price fixing and unified supply control (let's all massively reduce production and increase prices for example)  BUT it can look to the average consumer to still be ""competition"" then.  is amd and nvidia rightnow price fixing?  well there sure as shit won't be an investigation into it rightnow. hell nvidia can triple down on fire hazards without a recall. and the pricing between nvidia and amd are surprisingly almost always very aligned.  what a coincidence.  amd is also not interested to sell anything aggressively, despite wrongfully claiming they would.  so there is no meaningful competition going on at all here anymore.  and it is reasonable to expect, that price fixing is going on  as well of course.",Negative
AMD,"Yeah, for productivity Arrow Lake currently offers better performance than similarly priced AMD competition. They also don't seem to be cooking themselves (so far) and don't suck back stupid amounts of power the way Raptor Lake did.   Intel's lack of platform longevity is still a pain point, but if that doesn't matter to you and you just care about getting the best bang for your buck right now I wouldn't fault anyone for going Intel.",Positive
AMD,"Even with gaming, I just picked up a 225f for $155 on Amazon.  While not exactly on par with a 9600, it is close enough to save $40 on the CPU and saved $50 on the same brand's Intel vs AM5 ITX board.",Positive
AMD,just purchased an i5 14600KF after selling my Ryzen 5 5600 just because I had DDR4 ram to utilize it with.  I believe there is a lot of misinformation online on how the 14th gen is still messed up to this day but I'm having zero problems (updated BIOS to be sure). Ran BF6 earlier (CPU demanding game) and CPU was running 70% - 80% at 58 to 60 degrees at stock lol,Neutral
AMD,"I've been out of the loop with pc components for the last 3/4 odd years. My build is primarily music production-focused but I do play games on it quite a bit (primarily PvE, I don't really need anything beyond 4K @60fps): AMD Ryzen 3600, Nvidia GTX 1660Ti, 32 GBs of DDR4 RAM, Asus TUF B450M pro-II. What should I be looking at if I wanted to upgrade without dumping half of my wage for DDR5 sticks?",Neutral
AMD,"For homelabbing, Intel seems almost purpose-built for this. Lots of cores for cheap (never thought I’d say this about Intel), decent enough single-thread, and that excellent Quicksync.",Positive
AMD,Isn't Intel in the middle of divesting themselves from their networking business?,Neutral
AMD,"I dunno, I just use the sharkfin antenna that came with my motherboard. Works fine.",Positive
AMD,"The article is clickbait. Yes, black friday sales are over, so prices will rise again. This also applies to their competition.",Negative
AMD,"Amd was better than intel around 2000s, then intel bounced back, now amd is top again, intels gonna bounce back again",Positive
AMD,"You forgot the (brief) period where AMD K7 was dominant.  Also, INTC did not purposely hold back development for years. It literally fell apart when 10nm was delayed. The chip and fab business was so tightly bound that any delays in the fab (10nm) caused the chip design business to stall and make silly workarounds (e.g., Coffee Lake, Rocket Lake, Ice Lake, etc).  You can point to the massive $100B in stock buy backs, but INTC during that same time also spent more on R&D than AMD and TSMC combined.",Neutral
AMD,> not invest into real R&D  [$8-14B/yr](https://www.macrotrends.net/stocks/charts/INTC/intel/research-development-expenses) on what?,Neutral
AMD,"In a similar situation just on a smaller scale. Bought a 20TB disk for 300€, the store sent it in just a cardboard box and it was obviously DOA.  The return department dragged their feet with the replacement for 2 fucking months until one day they just randomly closed the case with a refund. The price of the same drive is now 450€.  The store is Senetic btw, any European shoppers avoid it. Shit packaging, completely unresponsive for any support apart from the initial (legally mandated) return ticket and fucked me over in the end when it was in their financial interest to do so.",Negative
AMD,"Unless you really need them new, I'd check ebay refurbs for 20tbs. I got my 14tbs for like 180 a couple months ago. you might get lucky, however they were WD white enterprises.   i can send a link if you'd like",Neutral
AMD,"I bought 48gb 6000 cl30, Trident Neo Royal Z (the silver with sprinkles) and felt outrageously lavish for paying 320€ this September.    Now they're more than 500€. Jesus Christ. At least the 9800X3D just hit record low with 440€ where I live.",Neutral
AMD,"I'm glad I grabbed a 2x32GB DDR4-3200CL20 kit for my laptop earlier this year, paid $90 brand new",Positive
AMD,for what do you need 64gb?,Neutral
AMD,Did you read the article or just comment capitalism bad immediately when you saw the headline,Negative
AMD,They can control the diy market all they want. They still don’t have the real important markets of pre built computers and laptops.,Negative
AMD,"Genuinely, who is buying this shit? Average new car price is ~$50k now.  I make 6 figures and the most I've ever spent on a car was $22k, and even then I kinda regretted it (until I was able to sell it at a profit during the pandemic...)  Apparently several manufacturers are straight-up discontinuing base trims next year in an effort to boost that average sale price even higher.  This can only end in tears.",Negative
AMD,What do you mean intel isn't back? Intel has a 75% market share on CPU's.,Neutral
AMD,Thats a stretch but yes they aren't back yet,Neutral
AMD,Unfortunately I couldn't open the article. Thanks for clarifying!,Negative
AMD,Extra 3 dollars at least just for that.,Neutral
AMD,Source? There is nothing to suggest that's what's happening.   The Ryzen 9950X MSRP is $649.99 at launch (August 2024). I bought it in November 2024 for $599.99. It's currently $539.99 on Amazon. It would take a $110 increase to hit MSRP. This would be a MASSIVE increase and doesn't seem very likely.   My prediction: It'll hit $599 again at most,Neutral
AMD,"What's the MSRP? What's the current price?  That's all that matters.  Not whatever you imagine is ""supposed to be its original MSRP lol"".",Neutral
AMD,"Look, AMD is way behind Nvidia in market cap. By paying these increased prices, we're helping AMD stay competitive in the one arena that really matters. I think I speak for all gamers when I say that increased competition benefits us all.   /s",Positive
AMD,"RAM prices have to come down. The entire client market, especially OEMs, will be at risk of collapse otherwise. There's no reason to think a rapid 500%+ increase in memory prices due to a shortage (and the resulting panic buying) is permanent.   I dont believe we're about to witness the collapse of the client PC, smartphone, and tablet markets.",Negative
AMD,"The RAM situation is going to kill the entire PC market entirely if it stays this way.  That might be somewhat acceptable if AI demand simply never goes down whatsoever, but this really cant last.  Literally everything like PC's, laptops, smartphones, consoles, etc will all have to go up in price quite a bit.  It's not sustainable.   Also, SSD prices have been very reasonable overall for a while now.  And yes, the latest SSD's will cost more, but come down in price fairly quickly all told.  This is really not an issue.",Negative
AMD,I fully support China eating the revenue of these companies,Negative
AMD,"...so you're describing the lack of competition, aka anti-competitive price collusion. 😅",Negative
AMD,"Lay persons are going to be priced out of the market, we're going to all be on thin clients eventually as our hardware dies, paying monthly for a resolution/framerate package that doesn't meet it's advertised performance.",Negative
AMD,"> Counterpoint: *Competition only works if there’s checks and balances to prevent price collusion*.  That's what I'm saying since years now; Gamers blindly buying nVidia for the sake of it, ruined the GPU-market.",Neutral
AMD,"Even in medium-high segment, there is no competition, it's an absolute dominance of the 5070 and 5070ti against the 9070 and 9070XT. It's not even close.",Negative
AMD,"Brother, that's a whole lot of stupid.",Negative
AMD,"> Nvidia doesn't keep their prices high because of ""no competition""... they do it because they are greedy as fuck and have always been that way.  No, Nvidia keeps the prices high, because they CAN — People buy their cards, without thinking, no matter what.  > Even back in the day when AMD/ATI was crushing their shit GPUs by huge margin they still charged more because they handed out bribes to game developers for years and always got their BS software gimmicks like Phys-X in new games. They also spent way more on marketing because AMD had to spend huge amounts on CPU.  There you have it, the answer on why people buy Nvidia-cards. Not because those would be (always) necessarily better, but mostly out of the market's *brand-perception* — nVidia spent billions to fabricate their leader-image.  Same reason forwhy so many people still stick to Intel and answer questions about their CPUs with *""It's a i7!!""*.",Negative
AMD,I got another 3+ years out of my old PC simply by upgrading from 1700x Ryzen to 5600 Ryzen. That system is still viable I just wanted to upgrade to 9800x3d. Platform longevity should not be underestimated.,Positive
AMD,"Mmm I’m a bit confused about this, I thought due to AMD being on a smaller node they were more power efficient…",Neutral
AMD,"> Intel's lack of platform longevity is still a pain point […]  Kind of blows one mind, how Intel still sticks to their idiotic 2-CPUs-per-socket mantra no matter what …  As if it didn't cost them already a good amount of users switching sides, due to AMD have the better cards here.",Negative
AMD,And with the price of RAM right now those sorts of savings do mean a lot. 32GB+ of fast RAM has become as expensive as the GPU for a new computer.,Negative
AMD,If Intel comes out with something that is like the x3d chips then I'd consider them. That extra cache is too awesome.,Positive
AMD,"A big upgrade would be something like a Radeon 9060 XT 16GB, which is about 2x as fast as your current GPU. You can also get a used CPU, something like 5600X or 5700X should be available for not much money and will work, just update your BIOS first.",Positive
AMD,Ryzen 5000x3d is a drop in upgrade.,Neutral
AMD,Not sure. But I still use and have seen many Intel networking chips in the wild.,Neutral
AMD,Well Intel may be the one who made the chips that handle your wifi. They make a huge portion of networking chips.  So you very well might have a partial Intel system without knowing it!,Positive
AMD,"the situation looks dire for intel, they just sold everything off, a lot of engineers are leaving, the money cushion is gone",Negative
AMD,"I run multiple VMs, some docker containers, gaming, software development, etc",Neutral
AMD,"I was playing some Clair Obscur last night, had a few chrome tabs open (youtube etc) to look up certain builds or whatever, discord, etc. I don't even rice my desktop experience with widgets. But after a ~4hr gaming session my RAM was at 27GB. I have 64GB.  One thing I noticed in the past from my years of PC experience is that your system will generally use less RAM if its going to be approaching the limit. This suggests that theres algorithms that will use disk space instead and/or do memory reallocations. There is a performance cost to this.  That said I think 32GB is a good amount of RAM, I rarely approach it.  However you never know what apps or games are leaking memory so having a lot is nice.",Neutral
AMD,Consumerism just never stops.  It boggles my mind how completely thoughtless most consumers are with how they spend money.,Negative
AMD,> Average new car price is ~$50k now  Because of the high end dragging the average up. Entry level cars have been pretty consistently priced after accounting due inflation for quite a while now,Neutral
AMD,"A lot of people think they deserve a treat, the treat being a $75k new car. To the point that “paying off the car” is a common financial milestone.",Neutral
AMD,the carbrain insanity is very sad to see. People buying cars multiple times their yearly income in costs then wondering why they are getting fleeced. An average person spends close to half of lifetime income on a car.,Negative
AMD,"You say this as if their market share hasn't been steadily decreasing over the past couple of years.   What's even worse is that Intel's market share shrink has largely been slowed down by them pushing a bunch of high volume, low margin chips. If you look at revenue share in desktop, AMD is already at 40% share.   Intel's competitive position in desktop has only been deteriorating since X3D came out. Something which Intel's own executives have acknowledged, multiple times, in different conferences and earnings calls.   It's a lil insane people are still denying what Intel's leadership themselves have admitted is a problem.",Negative
AMD,I think they mean amongst youtubers.,Neutral
AMD,"They did something AMD can't yet, ARM level idle but in PPA,PPW and raw performance both AMD and Intel need to work on it.",Neutral
AMD,You get an extra fiddy from the 9850X3D,Neutral
AMD,">The RAM situation is going to kill the entire PC market entirely if it stays this wa  ""Hi, I'm Michael Dell""  ""and I'm Tim Apple!""  ""And together we're excited to announce our new industry-wide pricing initiative!""  ""To help alleviate the burden on consumer PC pricing, we're pioneering the launch of a new initiative that has been in the works for some time: 30-year PC Mortgages with fixed-rate APR!""",Positive
AMD,"> The RAM situation is going to kill the entire PC market entirely if it stays this way. That might be somewhat acceptable if AI demand simply never goes down whatsoever, but this really cant last.  We're lucky that internet infrastructure is so terrible in North America, otherwise Big Tech might succeed by sheer force in a pivot to replace local PCs (and ownership) with pure streaming clients.",Negative
AMD,"Yeah effectively. Competitors != competition. When the industry has super high R&D and setup costs (like CPU/GPU design and manufacturing) it's very difficult for competitors to enter and disrupt the market. Just look at Intel ARC, they are a billion dollar company and yet they are still having difficulty in GPU development.",Neutral
AMD,Its literally close according to every single benchmark but whatever.,Neutral
AMD,"Platform longevity has to have some kind of financial value placed on it.   The maximum value of long platform longevity is the price of a motherboard. And you maximize this value if you buy into the very first generation on that platform. So every subsequent generation you enter in, the value of platform longevity goes down.   You also have to consider what impact generational improvements have on platform longevity. Zen 1 -> Zen 3 was a huge jump. Assuming Zen 7 on AM5, I don't think we're gonna see that level of improvement with Zen 4 -> Zen 7.  Let's say someone upgrades from AM4 to AM5 with the launch of Zen 6. They receive less value from platform longevity than someone who entered at Zen 4. And someone who bought in at Zen 4 will likely receive less value than someone who bought in at Zen 1.  So yes, platform longevity does have value. It's better to have platform longevity than to not have it. But best case scenario, that value is the price of a motherboard, and we could argue on the whole that you subtract from that value based on the circumstances.",Neutral
AMD,"99% of users do not even know what a CPU is, let alone know how to upgrade one.",Negative
AMD,"1700x was already obsolete when it was released, my 4 cores non ht 4670k was superior for gaming with a basic overclock and it was released almost 5 years before the first gen ryzens. I see Soo many people braging about upgradability but all of them could've bought a 8700k a few months after the release of Ryzen and had a system stronger than 1000, 2000, 3000 series and slightly weaker than the 5000.",Neutral
AMD,"They were comparing to Raptor Lake. Also, ARL is on a smaller node than Zen 5.",Neutral
AMD,"AMD isn't on a better node. And power efficient is a difficult metric to measure because it varies wildly on context: ISO-Performance, ISO-Power, ISO-task that isn't time sensitive (web browsing). It depends on power profile (high-performance will sacrifice efficiency for performance). It depends on where in the efficiency curve you are. It depends on how high in the product stack you are (a 7500F is more efficient in gaming than a 9950X, but may be less efficient in highly threaded productivity apps, etc.)  edit: Downvoted for being factually correct. You don't just run CB24 at full power and divide scores to determine efficiency. That's way too simplistic.",Negative
AMD,"sure, the x3d makes a huge difference, but for my needs I didn't need the extra power or cost associated with an x3d chip for this more budget build.",Neutral
AMD,"Forgot to mention I edit video in DaVinci Resolve so CUDA cores are kind of a must for me, but yeah, the GPU and CPU are definitely getting swapped before anything else",Neutral
AMD,"Oh absolutely! The latest generation 10 gig chips from them are really good, very power efficient compared to prior offerings. But Intel networking stuff has (mostly) always been solid.",Positive
AMD,"Ah, I didn't know that. Not sure why I got downvoted for answering your question, though.",Neutral
AMD,"Wouldnt say that too early, amd suffered same fate like current intel and bounced back",Neutral
AMD,Yeah but everything seems to be more or less tracking with inflation except wages for some odd reason.,Negative
AMD,"Losing market share isn't great.  But they still have ~75%.  Saying a company with a 75% market share ""isn't back"" is dumb.",Negative
AMD,AMD's PPA is the best on the market. Just not for light workloads no one cares about. Run some database benchmarks.  Nobody cares what Geekbench gamers think.,Positive
AMD,Ninety-eight fiddy ex three dee,Neutral
AMD,"more like  ""Introducing out new lineup running Windows/MacOS SE, designed for just 4GB of DDR3! And it still starts at only $999!""",Neutral
AMD,"you could buy computer parts with a fixed rate loan for a very long time. You shouldnt though. If you cant afford it without a loan, you cannot afford it period.",Negative
AMD,you need to solve communication at faster than light speeds (good luck) to make everthing a streaming client. Anything that isnt local server streaming is absolute ass.,Negative
AMD,"Only if you ignore all the features. You know, the features buyers DONT ignore.",Neutral
AMD,I was talking about sales though?,Neutral
AMD,"Time is also a factor, the amount of time I spend swapping a motherboard is far more than the time I spend swapping a CPU.       Also potentially getting a bad motherboard if I was also swapping a motherboard (has happened twice).       That means loss of more time and effort to reinsert the old motherboard, RMA/return the new board, and then swap the board again again if I manage to get a replacement. Getting an RMA repair/replacement again isn't a 100% guarantee in this day and age with how stingy companies are. So I mean depending on how far you want to take this. By the end of this I could be out an extra $100-$200 in time.",Negative
AMD,"IDK about anyone else, but I had some bad experience with intel and wanted to support AMD. Helps I got the x370 Taichi motherboard as a gift. I still have 2x 32GB Kits of DDR4 that I upgraded to 64GB on two machines. I wonder if I should sell those. I think I should sell my 64GB Kit of DDR4 as well.",Neutral
AMD,Sorry I didn't mean to say it is a one size fits all solution. I'm just hoping that Intel figures something similar out to really push AMD down in price or get them to innovate further,Neutral
AMD,"In that case a 5060 Ti 16GB or something used, e.g. a 4070. I also use Davinci for editing on a 5060 Ti, for Fusion I'd welcome even more power tbh, but everything else works great.",Positive
AMD,"I guess it's the smugness of ""not a single piece of silicon from Intel"" when most likely you and most people do in fact probably have something from intel.   It's r/hardware anyway, most people here are teens that worship youtubers, dont sweat it too much.",Neutral
AMD,"Bulldozer era AMD was truly awful, their GPUs weren’t good, their CPUs weren’t good, and their server CPUs weren’t good either. Their turnaround is crazy.",Negative
AMD,[Inflation adjusted incomes have been trending higher for decades and are up about 5% in the last 5 years](https://fred.stlouisfed.org/series/MEPAINUSA672N),Neutral
AMD,"How can a company ""be back"" if you are literally losing share?   At best one can claim a company is ""back"" if they *stopped* the bleed, but even that hasn't occurred.",Negative
AMD,Geekbench is the opposite of gaming benchmark   And AMD wins ofc in benchmarks that rely on fat caches,Negative
AMD,and your specific device will be abandoned by the OS in 3-5 years,Neutral
AMD,"Good. I wasn't. You can clearly read ""performance' in my message.",Positive
AMD,I can't image the failure rate for a brand new motherboard that needs to be RMA'd immediately is higher than the failure rate of a motherboard that's by this point already several years old. Although I don't really have the data.  What's the actual time difference of install of just a CPU vs Mobo + CPU? 10 minutes?,Negative
AMD,I went from a Ryzen 1700 to now a 5800X3D on a x470. The value of platform longevity can not be understated!,Positive
AMD,"Yeah, I'm thinking that 5800XT + 5060Ti 16GB might be the best combo I can aim for until DDR5 is back to normal prices again.  How about ""higher tier"" cards compared to the 5060Ti? 5070s, 5080s etc.?",Positive
AMD,"No worries, it's just internet points at the end of the day, I was just a little confused when I went to respond to his comment and saw the vote count.  I wasn't even trying to be smug, it was more of a ""I don't own anything Intel related (so I have no reason to be going to bat for them), and I still think them failing to compete with AMD sucks for everyone"".",Negative
AMD,7970Ghz Edition was pretty beastly,Neutral
AMD,"game cpu king, productivity king, server? performance king but the industry is shifting to computer per watt",Neutral
AMD,"Intel isn't ""back"" because it never ""left"". They have always retained an extraordinarily high market share.",Neutral
AMD,"I didn't say geekbench was a gaming benchmark. But that people treat geekbench like a gaming benchmark and also that geekbench scores are easily gamed (benchmark runs so quickly that the CPUs can easily go beyond their thermal throttling point before the benchmark completes)..  Also plenty of workloads Zen excels at that don't get the benefit from cache. Like I mentioned database benchmarks, where SMT can give you 50% more IPC. Meaning better perf. and better perf/watt.  You know workloads that pay the bills. Not Geekbench. Of course Geekbench also pays the bills by misleading consumers.",Neutral
AMD,Weird. Considering we're in a price related thread. Whatever I guess.,Neutral
AMD,> What's the actual time difference of install of just a CPU vs Mobo + CPU? 10 minutes?  With all the stuff you have to disconnect/rehome/recable? I think for the average home builder closer to an hour.,Neutral
AMD,"Maybe for an experienced builder, but to me (someone who's only built 1 PC) the prospect of just changing the CPU seems like a far less daunting task.  If you just want to replace the CPU, you unscrew 4 screws to remove your cooler/AIO pump head, pop open the socket, put your new CPU in, repaste and remount the cooler, plug some fans back in if necessary, and you're good. Maybe remove the GPU if you're really pressed for space.  When changing an entire motherboard, you have to unmount the cooler, remove the GPU, unplug every cable plugged into the motherboard, possibly remove some AIO fans/the radiator (or just fans in general) from the case if they're getting in the way of the motherboard screws, remove the motherboard, and remove any SSDs you want to reuse.   Then you have to put everything (CPU, SSDs, RAM) on the new motherboard, put the new motherboard in, repaste and remount the cooler, remount any fans you had to take off and plug them back into the motherboard, plug all your PSU cables again, then put your GPU back into place.  You basically have to redo a big chunk of the entire build process. Not to invalidate what you said, maybe you are proficient enough that all that would only take you 10 additional minutes. But I would not bank on the average DIY PC owner being able to do all that so quickly.",Neutral
AMD,"> What's the actual time difference of install of just a CPU vs Mobo + CPU? 10 minutes?  Maybe if your build has basically nothing in it, mine has an AIO I would need to remove, PCI-e cards, GPU, 2 SSDs that would need swapping. I think it's more like an hour.  CPU, you just pop off the AIO, swap it out and put it back on.",Neutral
AMD,>I can't image the failure rate for a brand new motherboard that needs to be RMA'd immediately is higher than the failure rate of a motherboard that's by this point already several years old.  You might be surprised. Reliability of these things can very much follow a bathtub curve,Negative
AMD,>I can't image the failure rate for a brand new motherboard that needs to be RMA'd immediately is higher than the failure rate of a motherboard that's by this point already several years old.  The failure rate for a brand new board is thousands of times higher than the failure rate of a three-year-old board.,Negative
AMD,"QC for PC parts seems to have been rather lax lately, I received a board with multiple damaged pins, and another one with a bad bios, I also got a bad CPU. This was all on the build I did around this time last year. Because of these issues it wasn't up and running till February. I also remember when I was a young teenager I got a bad board from Frys in the late 90s. My friend also received an Asrock board a few months into 2025 that basically melted with stock settings.  For some on compact builds multiple hours. Multiple factors go into this. Swapping a motherboard can be a major PITA. Not to mention when dealing with multiple thousands of dollars of components I have to take breaks to deal with the stress.",Negative
AMD,"5070 has a reasonable price, but it's somewhat limited by 12GB of VRAM. 5070Ti is a great card at a still somewhat reasonable cost and 5080 is probably the worst offender, it's way too expensive for what it is.",Negative
AMD,"Productivity king is pushing it, intel still has the lead in that",Neutral
AMD,"productivity is clearly in Intels court right now. Unless you need AVX512, but the vast majority never will.",Neutral
AMD,"I agree, currently they’re the best in the very same categories they were awful at a decade ago. Productivity can be argued that Intel is still better for some things. Still, in a good number of use-cases, AMD is still fast than intel per watt. Most of the Core Ultras are using a shit ton more power to match AMD’s performance.",Positive
AMD,And are going bankrupt while doing so,Neutral
AMD,The term ‘Is Back’ refers to a positive directional change and not a current status. Their current status is collapsing market share and margins.,Neutral
AMD,"With a bunch of lower end, cheaper chips. They simply aren't competitive.   Here's a quote from the CFO of Intel:   >""As you know, we kind of fumbled the football on the desktop side, particularly the high-performance desktop side. So we're -- as you kind of look at share on a dollar basis versus a unit basis, we don't perform as well, and it's mostly because of this high-end desktop business that we didn't have a good offering this year,""",Negative
AMD,"10 more minutes vs a CPU swap, so not counting that time or cooler installer.   Move RAM over, move GPU over, move storage over. Cables are already routed and sitting right there.    Maybe 10 minutes is optimistic, but I think if its not your first build, an additional hour to swap the Mobo vs just the CPU seems pretty long.",Neutral
AMD,"I wouldn't want to do it that quickly, I like my cables nice and neat and shit.",Positive
AMD,"but ARM is winning in compute per watt, AMD is only a leader in x86 in that category",Neutral
AMD,"I was thinking swap as in same case.  Cable locations vary by mobo, although generally not by a lot.",Neutral
AMD,"ARM isnt winning in compute per watt, Apple design team is winning in compute per watt. Other ARM offerings arent any better than say arrow lake.",Neutral
AMD,Laptop CPU naming scheme is a real mess,Negative
AMD,I LOVE LAPTOP CPU NAMING SCHEMES THAT MAKE 2 GEN OLD CHIPS SOUND MODERN!!!!,Positive
AMD,4 core/4 threads vs 8 cores/12 threads.   AMD really went light on this CPU. Only one full Zen core.,Neutral
AMD,"I don’t understand why they pit them together: sure 10-30% more performance, _at 50% more power consumption_.  Why not comparing it to an AMD with the same power consumption? It may give better performance…  Edit: I also like that they say “Intel consumption is _slightly higher_”, and on the next sentence “…would draw 68W or _almost 50% more_”. Article is clearly written by Intel PR…",Neutral
AMD,Yeah sure not in a similiar configuration.,Neutral
AMD,Don't see any reason to buy anything on intel 7 unless at a large discount.   The intel stuff on tsmc 3nm is much better,Neutral
AMD,So there are two laptops at same price just one has the Ryzen AI 5 330 the other has the Core 5 210H which one is better? I just saw this review of a laptop with the intel [https://www.youtube.com/watch?v=q5OFCLvxA6U](https://www.youtube.com/watch?v=q5OFCLvxA6U)  and it looks very very bad for battery life not even very light gaming.,Negative
AMD,"I would like laptop without AI please, especially CoPilot, that product is hot trash",Negative
AMD,"What a silly comparison, they aren't even close in price... The AMD only has 4 cores, of course it's going to take a huge hit in multithreaded...   Also, I just realised this is a repost from 9 months ago, maybe prices were different then? Why post something so old now though that's not even relevant anymore?",Negative
AMD,The real budget performance winner is an m1 macbook air.,Negative
AMD,"Hello Balance-! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,Yup. Now it seems like Apple is the only company with good CPU names.,Positive
AMD,"Was gonna say price, but looking at Lenovo's website and I couldn't even find the Ryzen AI 5 330 in my country, but the option with Ryzen AI 7 350 is still cheaper than the option with the Intel Core 5 210H.   So this is a really odd choice for comparison.",Negative
AMD,50% higher power consumption for *50% higher performance (in nT benchmarks),Neutral
AMD,"68W is only the peak at the start, otherwise the chip stays at around 50-58W during gaming. Even the AI 5 330 can peak at 58W and 43W in games too according to their review so it isn't that much of a difference either way.",Neutral
AMD,The better comparison should be 210H vs AI240  13420h reskin vs 7640HS reskin  But that would be boring,Neutral
AMD,"And probably on par in consumption, since the node is smaller.  This comparison right here is like comparing an i3 to an i5 and saying the i5 has better performance…",Neutral
AMD,That's Alder Lake. Can't remember anything later on the Intel 7. There used to be some decent 12-14th gen CPU for it.,Neutral
AMD,"Eh, I'm not a fan of the Pro Max Ultra shenanigans.",Neutral
AMD,Well you can configure the cpu and GPU cores to be different and still have the same name so no they're not better,Negative
AMD,"My gripe with Apple is it’s TOO simple. M4, pro, max, ultra.   Doesn’t really say how may CPU or GPU cores, as it varies by device SKU (some got the binned cpu like the air got 1 less gpu core)",Negative
AMD,I see a $100 price difference in the US with the ryzen at $500 and Intel at $600 through Lenovo. However the thing is the core 7 240H model is only $1 more than the core 5 and the ryzen 7 with 860M graphics is only $525. So why bother with either model at that point?,Neutral
AMD,"Where I'm from, 330 is very sparse and for some WILD reason more expensive than similar 350 options.  At the same time, 350 is more expensive than 225h... (Edit: re-checked, 225h isn't cheaper anymore it seems)",Negative
AMD,"Yeah. That’s like comparing an i3 with an i7. An i7 will consume more, and will have better performance. That’s why the comparison is odd to me: why comparing a Core 5 with an AMD that has to be compared with a Core 3?",Neutral
AMD,yeah i've never even heard of the ai 330. Crazy they gave it the 5 specification when it has 4 cores and three of them are C cores with only one full zen 5 core.  [https://imgur.com/a/lMajlf6](https://imgur.com/a/lMajlf6)  generally speaking amd is better efficency wise although intel does make up for it with the lower idle draw and lower draw in low-medium loads. The higher binned hx stuff have better efficency but you have to power limit them bc out of the box they go like 100w+     edit: from what im seeing both intel and amd have given up on the i3/3 naming in the laptop space but I still wouldn't say it is a fair comparision bc u should compare it to the 226v which has the NPU like the ai 330,Neutral
AMD,Go ask why amd names it a ryzen 5 and not a ryzen 3? Hmm?,Neutral
AMD,No the 210h is on intel 7 same with the 240h,Neutral
AMD,"The 210H is ""Raptor"" Lake(Alder, since the P Cores have 1.25MB L2 instead of 2MB like actual Raptor Lake)   12450H -> 13420H -> 210H are the same CPU, with clock speed bumped up with each iteration",Neutral
AMD,Well they always list the core count directly in the full product title on their site,Neutral
AMD,"It's not.   The Core 210 is Intel's budget chip.   Both chips are priced similar. It's a Core **5** vs a Ryzen AI **5**  Both are positioned as ""5"".  Intel adding E cores to boost the nT performance on their products has been giving them the nT lead in the lower end segments for a few generations now",Neutral
AMD,Whatever that has to do with technical specifications…,Neutral
AMD,"That and this comparison also saw the Intel version having better gaming percent, but it is only like a 5-10% difference across the board, with more games favoring Intel and some other favoring AMD",Neutral
AMD,Why did YOU bring up i3 vs i5 then? Huh?,Neutral
AMD,"Because they are both from Intel, I was trying to convey the fact that they are not on the same category. I need to refer to the same family somehow…  Now, saying Ryzen 5, 7, 9, etc. makes only sense within AMD, same for Intel within Intel. But different manufacturers have different nomenclature, and even different ways to measure TDP.  TDP on Intel is true TDP at minimum power, for AMD you can go lower than the TDP they publish for PL1.  So, at same power, AMD is also competitive, just that they have different nomenclature.",Neutral
AMD,"They *are* the same category. The two companies just approach that category differently. Intel decided to have 4 extra E cores to boost nT performance, but at the expense of higher power consumption if you load those cores.  This is the lowest end """"""current"""""" gen CPU H series CPU from Intel.",Negative
AMD,"9950X3D doesn't really make any sense for a gaming PC, just get a 9800X3D.  Any NVMe would offer you the same experience as the 990Pro, get a cheaper one.   $335 is silly for a cooler, a $35 air cooler would work all the same.",Negative
AMD,"Save some money by going with a 9800x3d, same gaming performance",Neutral
AMD,This system wastes a good amount of money.  * The 9800X3D is actually faster for gaming * The cooler is really expensive for the purpose of having a tiny screen * There are cheaper RAM kits on the market. * There are cheaper 9070 XT models,Negative
AMD,"I don't quite agree with that - unless the 9800X3D has much better heat dissipation than the 7800X3D which I currently have with a Noctura NH-D15 - and even that idles at 58-50C. Plus under load, you can definitely hear the fans spinning up and down. That's why I was thinking AIO. I could get another Noctura, but wanted to try something different and potentially more efficient (and quiet).",Negative
AMD,Switching to 9800X3D.,Neutral
AMD,"Gotcha; I'll switch to the 9800X3D. You recommend a Noctura NH-D15 then? That's $140. Question is does the AIO cool better overall? I don't mind spending if it does.  Which RAM kits that are RGB and 6000 CL30 do you recommend?  I don't necessarily need ""cheaper"" on the video card; I want reliable quality with a good name - same with the RAM.",Neutral
AMD,Final update posted in body - thanks a ton for your help! Only thing now... where to get a legit copy of Win 11 pro without giving Mucusoft $200 \*sigh\*,Positive
AMD,"The 9800X3D moved the 3D v-cache to under the compute cores. This puts the compute cores directly on top of the stack, in direct contact with the heat spreader. This makes it easier to keep cool.",Neutral
AMD,"You have a 7800X3D and you want to upgrade already? But why? Upgrading every gen is a waste of time and money.  There's nothing to disagree with. 9800X3D runs under 100W for most gaming workloads, same as 7800X3D. You could run this with a stock cooler if you wanted to.   Idle temps mean absolutely nothing. Your CPU could run at those temps indefinitely, and idle isn't even an established state. Just because you aren't doing anything doesn't mean your PC isnt' doing anything. Max temps under load is what actually matters. Your idle temps also seem just fine.  The fans will spin up and down regardless. AIOs use the same fans, and run according to the same fan curves. In fact, you're adding a pump to the noise equation.  https://gamersnexus.net/cpus/rip-intel-amd-ryzen-7-9800x3d-cpu-review-benchmarks-vs-7800x3d-285k-14900k-more  https://www.tomshardware.com/pc-components/cpus/amd-ryzen-7-9800x3d-review-devastating-gaming-performance/4  Objectively, you're paying hundreds for only the aesthetics with that cooler  Also in case you weren't aware, there are air coolers under $50 that compete with the Noctua NHD15 these days. IE thermalright PA120 or PS120",Negative
AMD,I recommend the Thermalright Phantom Spirit. It will be more than enough cooling for the 9800X3D and costs $36.  This RAM kit is way cheaper and you will never notice the performance difference between CL30 and CL36. https://www.bestbuy.com/product/crucial-pro-overclocking-32gb-2x16gb-ddr5-6000mhz-c36-udimm-desktop-memory-black/JX8PSKC864?cmp=RMX&refdomain=pcpartpicker.com,Neutral
AMD,Ahh nice! Yah the heat of my 7800X3D always bugs me heh. But the system works great. No question.,Positive
AMD,"I'm giving my current system to my wife, and her system is going to a friend (11900k with a 3090RTX). Doing it now because all indications are RAM and GPU prices are going to continue skyrocketing in 2026.  What combination of ""perfect parts for optimal performance and cooling"" would you recommend for a 9800X3D RX9070XT buildout?",Neutral
AMD,Updated build posted in body.,Neutral
AMD,"Actually, there is a performance difference up to 5% depending on what you're doing between CL30 and CL36. I have the money, so I'm just going to stick with the CL30 for now for that ""up to 5%"" difference heh.  Updated build posted in body.",Neutral
AMD,"It's your money. Just know that for a similar budget you can get an RTX 5080, which is approximately 20% faster than the RX 9070 XT. Plus, 64GB of RAM.  [PCPartPicker Part List](https://pcpartpicker.com/list/zDyzDj)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 7 9800X3D 4.7 GHz 8-Core Processor](https://pcpartpicker.com/product/fPyH99/amd-ryzen-7-9800x3d-47-ghz-8-core-processor-100-1000001084wof) | $444.99 @ Amazon  **CPU Cooler** | [Thermalright Phantom Spirit 120 SE 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/GpbRsY/thermalright-phantom-spirit-120-se-6617-cfm-cpu-cooler-ps120se) | $35.90 @ Amazon  **Motherboard** | [\*MSI MAG B850 TOMAHAWK MAX WIFI ATX AM5 Motherboard](https://pcpartpicker.com/product/vjpD4D/msi-mag-b850-tomahawk-max-wifi-atx-am5-motherboard-mag-b850-tomahawk-max-wifi) | $189.99 @ Newegg  **Memory** | [\*G.Skill Flare X5 64 GB (2 x 32 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/6QcgXL/gskill-flare-x5-64-gb-2-x-32-gb-ddr5-6000-cl30-memory-f5-6000j3040g32gx2-fx5) | $659.99 @ Newegg  **Storage** | [\*Samsung 990 EVO Plus 2 TB M.2-2280 PCIe 5.0 X2 NVME Solid State Drive](https://pcpartpicker.com/product/hpqrxr/samsung-990-evo-plus-2-tb-m2-2280-pcie-50-x2-nvme-solid-state-drive-mz-v9s2t0bw) | $176.99 @ Amazon  **Video Card** | [\*MSI SHADOW 3X OC GeForce RTX 5080 16 GB Video Card](https://pcpartpicker.com/product/dQqNnQ/msi-shadow-3x-oc-geforce-rtx-5080-16-gb-video-card-geforce-rtx-5080-16g-shadow-3x-oc) | $1099.99 @ Newegg  **Case** | [Corsair FRAME 4000D RS ARGB ATX Mid Tower Case](https://pcpartpicker.com/product/Mjy8TW/corsair-frame-4000d-rs-argb-atx-mid-tower-case-cc-9011296-ww) | $99.99 @ Amazon  **Power Supply** | [Montech CENTURY II 1050 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/yJkqqs/montech-century-ii-1050-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-1050w) | $99.90 @ Amazon   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$2807.74**  | \*Lowest price parts chosen from parametric criteria |  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-12-18 17:34 EST-0500 |",Positive
AMD,"First, thank you for this and the time you took to put it together. A few things; I want a v5 SSD that can do 14,7 on the transfer - that's a 9100 Pro. The case would require at least one additional fan - but do we really need fans up top?: I think 3 in front and 1 in back with the Phantom Spirit is fine. I do agree after really thinking about this - AIO pumps always go bad after 5+ years. Air fans; just replace them as they start to make noise. Air really is the best choice, so that saves a ton of money.  I don't need 64GB. 32GB is fine.  As for the 5080, that would require a 1000w PSU. There's no question the 5080 is faster and DLSS is better than FSR. But is the 15% difference worth $400? Not quite sure TBH. Plus the 5080 you linked has pretty bad reviews. I'd want to pay a bit more for a better reviewed card.  Here's what I put together: [https://pcpartpicker.com/user/SkrunchyCakes/saved/#view=D2FCFT](https://pcpartpicker.com/user/SkrunchyCakes/saved/#view=D2FCFT)  Not sure how to share the list as you did.",Positive
AMD,BTW I meant to ask - why are you recommending 64GB vs. 32GB?,Neutral
AMD,"> I want a v5 SSD that can do 14,7 on the transfer - that's a 9100 Pro.  Why? A gaming system will never make use of those speeds. There's essentially no real world difference between PCIe 3.0 and 4.0. 5.0 is just overkill.  I included a 1050W PSU in my build. You picked an SFX unit designed for small form factor cases that is needlessly expensive.  > But is the 15% difference worth $400? Not quite sure TBH.  You said you were willing to pay $150 extra for faster RAM that improves your performance by 5%. I'm a litttle confused by your priorities.",Negative
AMD,Because a few applications can make use of it and you seemed not to care about costs.,Neutral
AMD,Ahh you're correct on the SFX. The difference is RAM speed impacts everything the computer does all the time. The video card only impacts performance FPS above a certain threshold tied to the refresh rate of the monitor (I'm currently 144Hz). You are correct on the SSD; PCIe 4.0 will work fine.  I'm also hesitant to use a brand (Monarch) I've never heard of and AFAIK hasn't been well established.,Neutral
AMD,Your choice. The Montech Century II is highly rated by professional reviewers.,Neutral
AMD,I assume I'd only need one additional fan to blow out of the back of that case? BTW curious - do you have any of these exact components you recommended?,Neutral
AMD,What resolution are you playing at? Those specs are a bit overkill for those two games.,Neutral
AMD,"Im aiming for 2k, and possibily I will change the monitor to my TV to sometimes co-op with my wife",Neutral
AMD,You should be fine with those in that case. Solid build.,Positive
AMD,"Sweet, thanks so much for your input man",Positive
AMD,Why not RX 9060 XT?,Neutral
AMD,5060 due to wattage and features.,Neutral
AMD,Why the fuck is it between the 5060 and 6800?,Negative
AMD,"If he's 1080p, you can still get away with 8GB well on 99.9% of games. Even at 1440p with upscaling only a few you might have to drop a couple settings i.e. ultra -> very high (mainly textures). I personally had a ton of driver related problems on my RX 6000 series AMD card over time, so I'd sooner go for Nvidia or possibly a newer AMD card instead.",Neutral
AMD,"that build no matter a new gpu is quite ""old"" granted probably will still hold up. best to switch if possible to a Am5 socket ryzen at least 7 series cpu. ddr5 recommend 32 gb fuckign windows 11 is chunking my 32gb ddr5 ram under minimal load as is. and i've ran debloat scripts turned off all auto start disabled fast boot i got xmp enabled a slight GPU OC like i got 7 tabs open in browser and discord on the side monitor damn near 35 percent my memory utilized at the moment. although the b650 platform was announced discontinued. well they brought it back. and now it would be a super viable option for your buddy if you have microcenter in your area i highly recommend seeing whats in the holiday bundles atm",Neutral
AMD,what about a 9060xt?,Neutral
AMD,"The 6800 isn't a great choice, since it's 2 generation old and needs at least a 600w psu.  I also wouldn't get a gpu with less than 12gb of ram, even if he only plays in 1080p.  If he's willing to search for it and spend a bit more he could look for a 9060 xt 16gb, it would work with his psu as it only has a tdp of 160w and it's also pretty good in 1440p.  He could look for a used 4070/4070 super, it would work pretty well.",Negative
AMD,I'd guess price,Neutral
AMD,"He suggested the 5060, and I suggested the 6800. It's not between the two, I'm asking if my recommendation is correct and if there's a better fit.",Neutral
AMD,"Yeah, he's running games at 1080p currently. I've heard about the RDNA2 driver issues, I was mainly suggesting the 16GB card because he'll need a CPU and RAM upgrade in a couple of years, and I wasn't sure 8GB would be a good recommendation for a 3+ year card.",Neutral
AMD,Shouldn't it be same or cheaper than a 5060? XD,Neutral
AMD,it should be between  1. 3070 Ti   2. 9060 XT   3. B580   4. 5060   5. 6750 XT,Neutral
AMD,"8gb version is, fair enough. And it does get at least the x16 pcie connector so less of an impact from going over vram. Might be one of the rare cases where the 8gb version makes sense",Neutral
AMD,"These are def more balanced options but the things that sucks is that this is strictly from a gpu standpoint, and having had my own issues with the 9060xt while using a 3700x, id say this removes at least the b580 and 9060xt. It might remove the 5060 too but i dunno how it handles with older amd hardware. I myself ended up upgrading to a 5800XT cause the 3700X couldnt quite keep up.  In this persons case, id get a 6750xt, maybe a 7700xt. Maybe.",Negative
AMD,"I don’t even really see 3070ti for reasonable prices, they are mostly going for like $600+ due to being discontinued based on what I’m seeing, you may want to update your list. You are probably better off buying a used 3080, though I’d generally not recommend buying 2 gen old hardware to better be able to take advantage of new features, efficiency improvements, etc.",Negative
AMD,What kinds of issues have you had with the 9060xt? He definitely can't upgrade both his CPU and GPU currently.,Negative
AMD,7700XT is pretty pricey,Neutral
AMD,who the fuck is selling a 3070 Ti for 600 dollars??? What the hell are you talking about? They go for sub 300,Negative
AMD,"For basic use, nothing. For gaming, i'd get screen tears, the occasional stuck frame in games with a lot of objects (survivor games or really heavy areas in elden ring) and I'm fos games like fortnite and marvel rivals, i'd get occasional latency issues.  Tbf, these went to the point where these games were unplayable. This wasn't like trying to run a 4090 with my 3700X. It wasn't a horrific imbalance, but it was enough that i noticed it and didn't like it.  Realistically if you want to future proof your friend pic, maybe do get a 9060xt first and explain to em down the line they may wanna get a 5600XT for like $150.  One more thing of note: i will say now that i game in 1080p. Gaming in 1080 puts more pressure on the cpu than the gpu. If your friend games in 1440p, the balance shifts more to the gpu, so in that scenario, their 3600X may be ok.",Negative
AMD,"New, yeah. But ive started to see em on the used market for less than $300.",Neutral
AMD,"Now I’m seeing used ones for those prices, not sure why it didn’t show up a minute ago, but new ones I’m finding prices are cranked I guess due to being discontinued for so long. Bit of a compromise buying used with no warranty though. Whatever the case though I don’t think a 2 gen card is a good recommendation. A new 5060ti is faster and has a warranty for like $60 more vs a used 3070ti.   https://www.google.com/search?q=3070ti&sca_esv=709dfd0b212bea1b&rlz=1CDGOYI_enUS1023US1023&hl=en-US&biw=428&bih=751&udm=28&shopmd=1&ei=b95GacP0Lub9ptQP9JH2wQQ&oq=3070ti&gs_lp=Ehxtb2JpbGUtZ3dzLW1vZGVsZXNzLXNob3BwaW5nIgYzMDcwdGkyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgcQABiABBgKMgUQABiABDIHEAAYgAQYCjIHEAAYgAQYCkjeRlC2CFinQXAAeACQAQGYAW2gAYQFqgEDNi4yuAEDyAEA-AEBmAIHoAKpBMICDRAAGIAEGLADGEMYigXCAggQABiABBiwA8ICChAAGIAEGEMYigWYAwCIBgGQBgiSBwM1LjKgB8IesgcDNS4yuAepBMIHAzktN8gH0aoUgAgA&sclient=mobile-gws-modeless-shopping",Negative
AMD,"He understands that it's either a CPU or GPU upgrade right now, and he's currently GPU bottlenecked in games so a CPU upgrade is secondary. He does play Marvel Rivals, though. Depending on how the RAM shortage plays out, he might end up going to a Ryzen 7600x in a year or two. The future proof way with a 16GB card was what I had in mind, it would suck if he got two years down the line and found that he needed both a CPU and a GPU.",Neutral
AMD,"I ain't really be seeing that, all I ever see is like $320",Neutral
AMD,"so they don't make 30 series cards anymore, so if you're buying one new, ofc it will be expensive, that's just supply  but, the 5060 Ti is BARELY faster and ""$60 more"" on budget builds is a LOT of money and it is not ""$60 more"", you can get a 3070 Ti for like $250, a 5060 Ti 8GB is $335, that is $85 more",Negative
AMD,"At 1080p? You'll want the 9800X3D to push the frames, assuming you have a high refresh rate monitor. You be better served with a 9070 XT than a 5070 in terms of raw performance as well.   Why aren't you considering the 5070 ti as well? It delivers 85% of the 5080s performance for 25% less. It trades blows with the 9070 XT, but if you're set on Nvidia, it's a better value than the 5080.  If you're not planning on any productivity use in your PC, the 5070 ti with a 7800X3D would also be a great pairing within your apparent budget. The 7800X3D is only about 5% slower than the 9800X3D in gaming scenarios.",Neutral
AMD,5080+7800x3d,Neutral
AMD,"5080 is a huge a waste of money for this build. For the price of the 5080 you can get a cpu, a 9070xt, and a new 1440p monitor.  If it’s in the budget get the 7800x3d, if it’s not then a 9700x or 9600x",Negative
AMD,For 1080p u can honestly get by with way less than a 5070 or 5080. 5080 is really a 4K card. I’m using a 5070 for 1440p and 4K depending on the game. So idk your budget but I’d say go with the 5070 option.,Positive
AMD,i think this might be a joke,Neutral
AMD,9800X3D + 9070/9070XT,Neutral
AMD,Ah okay thanks. Well i was just comparing two prebuilts I saw that were both on on sale. So of these two which would probably give me the most fps and performance at 1080p?,Neutral
AMD,Both of these for 1080p are way overkill,Negative
AMD,"Both overkill for sure. The 7900x system will run into a CPU bottleneck before the 9800X3D system runs into a GPU bottleneck, most likely, but it would also be cheaper to replace a CPU than a GPU down the line.",Neutral
AMD,"That’s fair, he’s most likely gonna be monitor bottle necked at the end of the day lol, good problem to have",Neutral
AMD,It's likely unless OPs holding out some info about a 240hz 1080p monitor.,Neutral
AMD,I’m sorry I’m a PC noob. I do have a 240hz monitor that’s 1080p,Neutral
AMD,Yeah I have a 240hz 1080p monitor,Neutral
AMD,"Nah it’s all good lol, that’s great, I think the 5070 build would make more sense unless you think you’ll upgrade to 1440p eventually",Positive
AMD,That's a very good price for RAMageddon. How much were the bundle and the SSD?,Positive
AMD,"The CPU, motherboard, and ram were bundled together for $579.99. I got the SSD at Best Buy.",Positive
AMD,"I put your build together on pcpartpicker and I cannot possibly get under $1700, even with the Microcenter bundle. How did you do that lol? $1560 is sick.",Negative
AMD,The 9070 XT was also from Micro Center. Here is my pcpartpicker list: [https://pcpartpicker.com/list/xDCtXR](https://pcpartpicker.com/list/xDCtXR). Looks like the price of the PSU went up a little already though.,Neutral
AMD,"Ryzen 9600X is a very good CPU, I could not even complain on my 7600X since it's so reliable on all I do with my PC. I am happy I switched to AMD on my new rig.",Positive
AMD,"Id just say since you are dropping cash on AM5 with these storage/memory prices, consider saving money somewhere like the motherboard to put towards a better GPU that will be more futureproof and match with the 9600x. Its a great CPU, and even a 9070 will be the eventually be the bottleneck (ideal). If you can get your price up to a 9070 - get a motherboard for $90 or something, then you will literally get almost double the frames in each game with a 9070 compared to a 9060 xt.  [https://pcpartpicker.com/product/KXkqqs/asrock-challenger-radeon-rx-9070-16-gb-video-card-rx9070-cl-16g](https://pcpartpicker.com/product/KXkqqs/asrock-challenger-radeon-rx-9070-16-gb-video-card-rx9070-cl-16g)",Positive
AMD,"For the Adata XPG SSD , please do check if any issues were reported or not. S70 blade has issues where the ssd would just die, so please check some reviews on S60. I would say if you can get something from WD or other brand , prefer that",Neutral
AMD,"The 7600X still seems like a good CPU, part of why I felt comfortable going with the 9600X.",Positive
AMD,"Thanks, I'll look into some cheaper motherboard alternatives and see If I can go up to a 9070.",Positive
AMD,"I see they've had some issues with stability which might have been fixed in recent firmware updates but I would rather not take the chance, I'll go with another brand.",Negative
AMD,"With all the intel fiasco atm, we are lucky we opted for AMD CPU",Neutral
AMD,"As far as i know, it was innogrit controller that has issues and it was used in other brands too. Other brands might have solved the issue but for S70 Blade i still see reports of failure coming out. S60 could be good as it has diff controller ( if you dont fine failure reports ) but at same specs WD SN5000 is also an option",Negative
AMD,"5600 or better. Zen 3 x3d has become unobtainable at reasonable prices in several markets now, but if you can still get a reasonably priced one go with the 5700x3d. Other solid options would be a 5700x, 5800x or the newish 5800xt depending on price and availability.",Neutral
AMD,"Hey guys thanks for all the advice, I just bought the AMD Ryzen 7 5800X for €175. I was looking around several webshops for the X3D variant but unfortunately all were not available anymore",Positive
AMD,"Grab a 5700X3D or 5800X3D and milk that AM4 platform like it owes you money! Your new GPU deserves better than that 3600, and DDR5 prices are basically highway robbery right now anyway.",Negative
AMD,"Maybe 5700x yeah cause its not too expensive (yet) and then crank your graphics settings in games to try and keep the GPU as the bottleneck. Im in the same boat with a 5700x and a 9070, none of my games are 100% usage CPU after I adjusted the settings.",Neutral
AMD,depending on budget i'd recommend anything from a 5700x to a 5800x3D,Positive
AMD,"Get a 5600x or 5700x, those are a cheap upgrade that should last you until prices settle back down from this lunacy.  If you want to splurge then you could try to track down a 5700X3D, but these are hard to come by these days.",Neutral
AMD,Already to late a 5800x3d is more expensive then a 9800x3d,Neutral
AMD,"Wait if possible, prices are inflated heavily for the last AM4 chips.  Rocking my Ryzen 7 2700x until the bitter end.  Just depends on your use case",Neutral
AMD,5800X3D is goated.,Neutral
AMD,"I think it depends where you are. I’ve just upgraded 2 weeks ago from AM4 to AM5. I’m in the UK and RAM prices while horrible don’t seem as bad as the US. CPU and motherboard prices have come down a little, about 10% down (I expect from reduced demand) and resale prices of AM4 and DDR4 has gone up so it wasn’t as terrible overall as it first looked.   My strategy. It seems solid state memory prices are only going one way and this situation is going to take years to resolve and likely won’t ever get back to pricing of 2 months ago. I’m now well placed to ride out the next couple of years and being on AM5 I will even have some future cpu releases and able to consider cpu upgrades but realistically I probably won’t need to as I went with a 9800X3D.",Neutral
AMD,A 5600 or 5700x paired with that 9070XT would be a massive upgrade.,Positive
AMD,If you can get a 5700x3d or 5800x3d you can stay in AM4 for a couple years,Neutral
AMD,X3d chips are ridiculously overpriced at the used market. 5800x would be the reasonable choice.,Neutral
AMD,"Just stay with am4 , before few days i upgraded my 3600x to ryzen 7 5700 and now my pc is so good fps in the games is better",Positive
AMD,"I'm still on AM4 with 5800X3D. I'm waiting for when AM6 drops. Which should be around 2027, when I will decide if I should move platforms.",Neutral
AMD,You're going to wait 2 years if you're lucky.  Look into combo deals from microcenter or newegg.,Neutral
AMD,Stay as you are. Don't listen to people advising for a 5700x3d or 5800x3d they have no idea of availability/pricing at the moment. Sure If you can find a 5700x3d at maximum $250 then go for it. But the cheapest u can get is $350+. And at that price the increase in minimum 1% fps u are gonna get is not worth that money.,Negative
AMD,"I’ve been running a 5950x, 128 gb DDR4, 3090 gpu. Personally I don’t see why I’d switch to DDR5. What do you feel like you need it for. The Ram alone on DDR5 isn’t justified.",Negative
AMD,Unfortunately there’s no indication prices will ever settle down. We are on an unsustainable and bleak path here,Negative
AMD,rtx 5060 ti paired with a 5700x3rd or 5800x3rd will do you wonders.. future proof for 4 more years easily,Positive
AMD,"Find a Z690 ddr4 motherboard and install a 12900k and your existing RAM. That should nearly triple the performance of what you have right now, for about $450-$500  AM5 with DDR5 would easily double that cost.",Positive
AMD,Get a 5800xt and you are good to go,Positive
AMD,"5700X and 32GB DDR4 and you won't have anything to worry about for several years, you can basically ignore AM5 if you're gaming on a higher AM4 platform",Neutral
AMD,Grab a 4k monitor or TV and let the gpu carry the load.  The 7900x 5080 prebuild I picked up last month isn’t that much faster than my old 5900x 7900 xtx at gaming but I couldn’t pass on the deal.  The older pc was good for 4k100 at high settings.  The new system is much faster at AI workloads that I needed it for though.,Neutral
AMD,"5700x3d can still be bought at trusted sellers, they are a bit overpriced, but still cheaper than 32gb of ddr5 ram.  I got one for 250€ last month and will now probably stay on am4 until am6 arrives.",Neutral
AMD,I'm super satisfied with AM4. This is a supercomputer I got. Just max out what you have until the market goes back to normal.,Positive
AMD,"had the same setup, but got rtx5070 and moved to am5. 5800x3d / 5700x3d current prices on aftermarket are crazy.",Neutral
AMD,I thought about this a year ago and Im glad i stick with am4. I upgraded my 3600 to 5700x3d paired with 6700xt. I sold my 3600 for $90 and bought the 5700x3d for $200. Now Im good till the am6 comes out.,Positive
AMD,I was in this same situation earlier this year! For the price of one of the microcenter 3 in one mobo/ram/cpu combos I got a 5800XT and a 9060XT (125 for cpu 325 for the GPU) and its been great.,Positive
AMD,"Nobody has mentioned it yet. Upgrade your ram to 32gb. Im staying on am4 and I'm not planning an upgrade until prices die down, probably 2027 or 2028  32gb is the new standard soon",Neutral
AMD,I know I'll be sticking with my 5600x basically until one of us dies as long as these RAM prices stay the way they are lol,Neutral
AMD,"Yeah on a very tight budget a 5600 is great, but if someone can spare the extra $20-$40, I'd generally recommend the 5800XT.",Positive
AMD,*if the prices settle down.,Neutral
AMD,No.,Neutral
AMD,Do you have a microcenter within driving distance?,Neutral
AMD,Am I the only one whose eyes immediately scan for what RAM they got?  CPU Banana  GPU Banana  RAM DDR4 3200 CL16 64 GB  Motherboard Banana  PSU Banana,Neutral
AMD,What are you playing? Most games would only offer marginal upgrades from the 3600,Neutral
AMD,used 5700x for 130 bucks - perfect upgrade,Neutral
AMD,I did the same and switched from my 3600 to the 5800x. But keep in mind. The 5800x gets way hotter than the 3600. That was quite a surprise to me and took me some time of adjusting the AIO and fans.  Temps around 75-80 degrees Celsius are absolutely normal.,Neutral
AMD,"When you try the new setup, give us some feedback. I'm currently running a somewhat similar setup to your old one, and i'm loking for upgrading it to a 5800xt +9070xt (or 5070 ti) combo.",Neutral
AMD,"If you'd like to refund that could sell you the 5800x3d for the same price, I just upgraded to am5",Neutral
AMD,Nice. You're gonna be happy I'm sure.,Positive
AMD,AM4 X3D CPUs are very expensive now. Best case is probably a 5800XT imo.,Negative
AMD,I dont know if that's even possible anymore. I was going to suggest someone get a 5800X3D and I literally had to look at the box for my own to confirm tbe model number is right. That thing is extremely scarce right now.    So much so that im debating selling my old machine instead of turning it into a media center.   Plus I have Samsung B-dies which are equally impossible to find,Negative
AMD,these are not in stock anywhere and people selling them second hand are in $400 range,Negative
AMD,Buddy and prices of X3D aren't robbery? It's much more worth it to just bite the bullet and buy ddr5 than 5800x3d.,Neutral
AMD,5800x3d costs more than a 9800x3d right now. Do not buy one.,Neutral
AMD,You are aware the 5700x3d goes for $300+ now right?,Neutral
AMD,I did exactly this; went from what you have to 5700X3D and 6700XT. runs great dead silent too.,Positive
AMD,"This exactly, doesn't even have to be an X3D even a regular 5700X is a huge upgrade over the current 3600 the OP is using.  Personally I went from 1st gen to 5th gen Ryzen 5s and the difference was night and day, doing it also helps avoid having to buy new RAM (Typically costs the same as an iPhone 17 now 😭) and new motherboard, while still providing a nice performance bump that will keep the system current for the next 5 years or so.",Positive
AMD,"Can't find any X3D AM4 CPU's in stock in Portugal.   I have the same R5 3600 as OP and would love the 5700X3D but it's no where to be found in stock.   I could get it from ebay, but the price I'd pay for that... is more expensive than an AM5 R7 9700X...  And even though buying the 9700X would turn out to be more ""expensive"" because I'd have to get AM5 motherboard + DDR5 RAM for a total of 700€-800€, I still feel like it would hurt to pay like ~300€ for 5700X3D.  Not sure if my logic is flawed, but it's how I feel. Doesn't make sense to me to pay ~300€ for AM4 CPU when I can get more powerful AM5 CPU for less even though the final cost (+ new motherboard + new DDR5 RAM) would be way more.",Negative
AMD,"If he is going to spend way too much on a part, it might as well be DDR5 RAM then. 5XXXx3D made sense at MSRP, but those days are over.",Negative
AMD,i feel offended now that i've upgraded to a 3600.,Negative
AMD,Haha this. I’m looking to get a 5080 when it comes available. And I’ll be rocking that 5800x3d hopefully for another 3 years. Only issue is finding one new,Positive
AMD,"I live in the Netherlands. Honestly AM5 CPU and motherboard prices are not the issue here. It's the DDR5 RAM prices. For example the Corsair Vengeance 32GB RAM went from €103,99 at it lowest to now €499. If I could use my DDR4 RAM on a AM5 motherboard I would have done it :(",Negative
AMD,Unfortunately not. I live in the Netherlands and you don't have a microcenter over there,Negative
AMD,"Im playing games like Battlefield 6 and Where Winds Meet atm, sometimes Planet Zoo. And with these games I notice my CPU UTIL reaching around 100% when it gets very busy with players/NPC's and such",Neutral
AMD,"Thanks for the tip. I'm going to reuse my Scythe Mugen 5 CPU cooler, hopefully that's enough. Otherwise I'm gonna also adjust some of my fans to keep the temps in check",Positive
AMD,Will do if I don't forget. I'll probably upgrade and test after new years when I have some spare time,Neutral
AMD,"Ngl that sounds pretty tempting, that would help me out a bunch. How about shipping and such? I haven't bought much 2nd hand stuff, so I don't know the entire procedure. I know that some services exist that have buyer protection",Positive
AMD,5800xt is the sweet spot because it’s usually only a little more expensive than a 5600 but the extra 2 cores can make a difference in a few games. For example simulation heavy games and even boarderlands 4 recommends the 5800x for the cpu.,Positive
AMD,"Even that one is just a few ups in the performance if you're on a 5600, we're basically at the ceiling at this point in AM4, unless amd makes them again, which is not possible.",Negative
AMD,"> AM4 X3D CPUs are very expensive now.  They are. But even at $350 for a used 5700X3D, it's still probably cheaper than a new AM5 platform right now, sadly.",Negative
AMD,sounds grt,Neutral
AMD,"Yeah, that’s worse than the price when I bought. The RAM I bought went from about £130-£140 2 months ago to £340 when I bought it.",Negative
AMD,"oh yeah you def need an upgrade lol. Try a 5700x, the X3D CPUs are really overpriced these days.",Negative
AMD,"I don't know if you already made a purchase, but check out 5700x. It is basically the same CPU as 5800x but much cooler and uses less electricity.   Look through a few reviews and benchmarks and see if it might be better option for you.  I don't know your local prices, but it might be even cheaper than 5800x.",Positive
AMD,Paypal G&S is what hardwareswap uses for buyer protection. I've had no issues,Negative
AMD,I actually had no idea about the prices of them. I bought my 9800X3d for $460 and the used 5800 is going for around the same price. I'm regrettably going to remove my offer apologies but this just makes my upgrade free,Negative
AMD,"OP is on a 3600, not a 5600. I mean you're right about the difference between a 5600 and a 5800XT but just saying for OP it'll be a larger upgrade.",Neutral
AMD,5700X3D is cheaper than AM5 but I think the value proposition is worse than saving up for AM5. If the X3D was still $150-200 in the past then the value proposition is way better,Negative
AMD,"Thanks for the advice, I watched some videos about it. I sent an email to the webshop where I ordered it from to cancel it. Hopefully I get a response on monday before it will be sent. The 5700x is only 5 euros cheaper, but having better temps for like a 1% loss of performance is much better imo",Positive
AMD,No worries,Neutral
AMD,"I think spyder was just stating to go with a 5600 instead of a 5800xt for an upgrade, but honestly the 5800xt goes on sale for about the price of a 5600 and the 2 extra cores can make a difference in some games and definitely makes a difference in multithreaded workload.",Neutral
AMD,You can always reduce power limit of the 5800x (95w) to be the same of 5700x (65w) if I remember correctly.,Neutral
AMD,"Yeah, it will work fine at gen 3 speed.",Positive
AMD,It will work fine at gen 3 speeds.,Positive
AMD,Yes.,Neutral
AMD,looks good,Positive
AMD,"depends on how much you're willing to spend tbh.  updating your bios and getting a 5600/5700x is a good idea  for the gpu, a used 3060ti/6700xt will be a nice upgrade",Positive
AMD,"GPU should be priority. Ryzen 5600 to 5800 is a cheap.buy meaningful upgrade. Unless you want to splurge for am5 platform .   People overestimating ram upgrade need. Especially at current prices, you then might as well splurge on anew platform",Neutral
AMD,"Just a heads up, some older am4 motherboards don't handle higher ram speeds well. I upgraded a 1600x to 5700x. Found the memory unstable at higher speeds.",Neutral
AMD,"Okay here is the kicker, as some have said - RAM. 3600 cl16 or cl18 with 2x8 or 4x4 would probably do you a world of good, but you most likely would not feel as much of a difference as upgrading the GPU.  The R5 2600 isn't necessarily slow, its just not very fast, but it is still capable at 1080p, and raising the resolution would actually increase your graphics fidelity while also lowering the load on your CPU.  Reference this chart, as it shows both the R5 2600 and the 5600 as to their maximum FPS they can deliver. [https://www.techspot.com/review/2135-amd-ryzen-5600x/](https://www.techspot.com/review/2135-amd-ryzen-5600x/)  On average the 5600x is about a 30% uplift in performance. F1 2020 is a good comparison of a high FPS light weight esports title.  What I would personally do if you can:  1. RAM. 2. 5600x - 5800x is a perfect cpu slot in upgrade. 3. GPU - If you are going to keep the 2600, you are looking at max a RTX 3070 (use tech powerup as a reference, if you look up 2070 super, you can see a relative scaling of other GPU's in reference), if you get a 5600x, you can go up to a above a 3090, like 4070ti super, 9070xt, or lower.  3.1. If you up the resolution, then you can always go higher, but we are realistically hitting the wall for GPU's a normal person could afford as 5080's are over 1000$.  If you are trying to spend the least money possible, you can slot in a 3060ti for 200$ (used usa ebay) and call it a day, see if the performance is good enough for you. Then go with the RAM + CPU combo if you need more.  If you want the maximum possible uplift across the board and money isn't a limit, go straight for CPU + RAM, and then get the best GPU you can get thats slower than a 5070 ti.  Don't listen to the people who say your PC is slow. Its not slow, its got headroom. Some ram and a gpu will take you a long time and you sound like you would be happy. As you said, you want bare minimum for more consistent frame rates - just buy a 200$ 3060ti. That is what I have, and it plays most of my games at 1440p with DLSS at above 100 FPS, even on my aging ddr4 platform and the CPU load is minimal. I also played a lot of CSGO/CS2 (I quit, F that game it takes my life away lol), it will do just fine. A CPU upgrade is if you really want to get more competitive frame rates (200+) to really get the response times down in esports titles.",Neutral
AMD,7 Year old used RX 580 recycled mining card: I'm tired boss.,Neutral
AMD,"I think you're on the right track. A 5700 or 5800 will be a very nice upgrade to your cpu. Bonus points if you can find a 5950 for cheap. Too bad the X3d chips are not available. Those would be the best.   You can also look at a GPU upgrade. I'm not sure what your budget looks like or what you have available to you. But for current gen, RTX5060ti or RX 9060xt. Most people recommend 16gb. But with 1080p esports titles, you could probably get away with 8gb. Last gen models could also be good if going on a budget.   Ram is probably the least impacful upgradefor you sotuation, and also very expensive. At this point, you might as well stay put. You can also look for a used 3200 or 3600 MT speed on the used market for cheaper deals.",Positive
AMD,"First, see if you can still get a 2x16gb or 2x8gb 3200mhz or faster kit (ideally 3600mhz CL16, but that’s likely silly money) of DDR4 for semi sensible money. If pricing is stupid for everything in your market, what you already have will work at the cost of a little performance.  Second, look at upgrading your CPU and GPU. Anything from a 5600 to 5700x3d depending on price and availability will be a nice upgrade, and would pair nicely with a 9060xt 16gb or 5060ti 16gb - or maybe a 9070xt if that’s in budget.  Upgrade your PSU if it needs it - a 650w-850w that scores a B or better on the PSU tier list depending on what components you get and what you may plan to upgrade in the future.",Neutral
AMD,I had your exact same build until last summer. I went with an Intel b580 you and an ryzen7 5800.,Neutral
AMD,What is your budget?,Neutral
AMD,I would save up and do an entirely new build. The least you can do is target a new cpu/mb/ram and get a gpu and psu to support it later.,Neutral
AMD,"5600x, at least 16gb ram, used 2070 super or greater. GPU depending on budget of course",Neutral
AMD,"As people have said, get 2x8 or 2x16 GB 3200 CL16 RAM, a Ryzen 5600X or better, and a 3060/2070 or better GPU (preferably 9060XT 16GB, if you can afford it). I'd go with CPU, then RAM, and finally GPU. That would get you a system that should easily last until the next platform shift, or at least until RAM prices normalise.",Positive
AMD,5700X3D and 9060XT/gtx5060.,Neutral
AMD,Ryzen 5600 + a faster 16 GB RAM kit and you'll be doing pretty well. I'd save GPU for dead last.,Positive
AMD,"I would get a GPU first as well with 5600, the difference in Zen Plus to Zen 3 is very strong.",Positive
AMD,"That's just utter horseshit lol, he's getting a straight up 3-5x performance increase by spending $100 on a GPU.  Your few% performance increase from going to a 'more optimal' (this is also completely false) DDR slot habitation is delusional.",Negative
AMD,"I am more inclined to go with your suggestion, I think it will cost me 200 ish bucks to fix my problem (prices in EU seems to be higher) compared to pouring 1k+ when I don't actually need that much",Negative
AMD,This 100% reads like AI.,Neutral
AMD,"Thank you so much for the breakdown!  I am in EU, and my local used market isn't that great, plus I have some vouchers that I can spend only on new parts so I am more inclined into going for the Fast Ram + CPU upgrade, and will keep an eye out for a deal on a GPU.",Positive
AMD,I never went that far with it :),Neutral
AMD,"I mean, I'm not budget constrained, I can afford buying a new one, but I don't think it's worth it considering the titles that I am playing, I think getting a faster RAM in dual channel and a better CPU fits my needs and it will by much cheaper than building a new PC altogether.",Neutral
AMD,Buying a $100 GPU is a waste of money. He needs 3600mhz ram anyway when he upgrades other things.,Negative
AMD,"Don't listen to him, switching your memory is utterly useless. 2400MT/s vs. 3600MT/s is +5% to your FPS on average :)  Your 4 sticks will be just as good as a 2 stick kit.  [https://www.youtube.com/watch?v=bDgDtz7ImGI](https://www.youtube.com/watch?v=bDgDtz7ImGI)",Negative
AMD,"Good idea! Honestly I would just stay on your platform, maybe upgrade to a ryzen 3600 or 5600, both are a decent upgrade you would notice and should fit in your current motherboard.",Positive
AMD,"The fast ram + cpu upgrade just won't change anything. You are heavily GPU bottlenecked. The fact that you can go from a rx 580 to literally to a card 2.5x that performance and not have issues, shows that upgrading your CPU won't actually change your frame rate in games.  Think of it this way. The GPU outputs frames, if your CPU can process all of them, then you are good. If not, then you upgrade the CPU. Upgrading the CPU now and looking for a GPU is fine, if you do intend to get a GPU, but if you think the CPU will increase your FPS by itself, it won't. PC's work together, there isn't a single component by itself that dictates your FPS.",Neutral
AMD,16-32GB 3600 DDR4  5600x  9060xt 16gb   This would be about as solid as you can get on am4 without a lot of gymnastics.   Most of these except the GPU you can likely piece together from used sources.,Positive
AMD,"I mean, if you want an upgrade, a ryzen 7600 would be a hell of an upgrade if you can afford and add. GPU. If you want to upgrade, I would say prices might get worse over the next year especially RAM.   Maybe pair that with a AMD 9070 GPU or similar",Neutral
AMD,"So you would rather get a 3600MT/s 2x8gb kit for $20 (including selling your old kit, what a hassle!) to get a 10% overall increase at best instead of switching out an RX 580? Modern iGPUs deliver better performance and believe it or not, esports titles do benefit from a better GPU.  Absolutely get an RTX 2060 or something else that isn't 10 years old instead of memory changes.  Found a great benchmark of this exact setup (although unknown RAM speeds): [https://www.youtube.com/watch?v=3cLOdq8au90](https://www.youtube.com/watch?v=3cLOdq8au90)",Neutral
AMD,"He absolutely does not \*need\* 3600MT/s ram at any point! You can find me some benchmarks to show otherwise, I couldn't. In most cases the difference is negligible, within 3%.  Can you also point me to some sources that show the negligible difference between an RX 580 and the RTX 2060 for an example?",Negative
AMD,"No no, he can upgrade his CPU. A 3600 will cost him just about the same as your proposed memory upgrade.  [https://www.youtube.com/watch?v=VRnZClBIYPw](https://www.youtube.com/watch?v=VRnZClBIYPw)",Neutral
AMD,Lmao upgrading to a 3600 is a waste of money when the 5600 exists,Negative
AMD,Yes it is! Did I not say that would cost him just about what that memory upgrade would?,Neutral
AMD,"Yeah, definitely! [https://www.youtube.com/watch?v=ghO-NCr4pQg](https://www.youtube.com/watch?v=ghO-NCr4pQg)",Neutral
AMD,"You can absolutely use ""Intel XMP Ready"" RAM with an AMD CPU, but you'll need to enable the settings via your motherboard's BIOS using AMD's equivalent features like [**EXPO**](https://www.google.com/search?q=EXPO&sourceid=chrome&ie=UTF-8&mstk=AUtExfCjJrYtIpcuhq2aGFonO-jHa3cT4P6E9HsHrjBPTFiuHFeJGTeYHQeH4e12cVCf0Ox1vq1_AAmV-zG8HGBcc2EhKFYP76eRnUtLupa2tu-UzU2Y_hs6L-RXx1yFcauzBj-eAyNyBZknNpWepqnaMrdvb0B_U0hv4E4cUswysXbLrTBi4dwt-CO5aT58KUI4n7qOcI4LrTa0Iqd72fMO1ZlmUpE60nduwBTAEmGbQz4maPa3A2f3zDLfHXPRW2CkWgaPm_Yo6OP-KpQjk41WNz2tEGb70pbTatgl6549CPfNsg&csui=3&ved=2ahUKEwj4pM7fu-CRAxUFQvEDHVCcD2IQgK4QegQIARAB) (for DDR5) or [**DOCP**](https://www.google.com/search?q=DOCP&sourceid=chrome&ie=UTF-8&mstk=AUtExfCjJrYtIpcuhq2aGFonO-jHa3cT4P6E9HsHrjBPTFiuHFeJGTeYHQeH4e12cVCf0Ox1vq1_AAmV-zG8HGBcc2EhKFYP76eRnUtLupa2tu-UzU2Y_hs6L-RXx1yFcauzBj-eAyNyBZknNpWepqnaMrdvb0B_U0hv4E4cUswysXbLrTBi4dwt-CO5aT58KUI4n7qOcI4LrTa0Iqd72fMO1ZlmUpE60nduwBTAEmGbQz4maPa3A2f3zDLfHXPRW2CkWgaPm_Yo6OP-KpQjk41WNz2tEGb70pbTatgl6549CPfNsg&csui=3&ved=2ahUKEwj4pM7fu-CRAxUFQvEDHVCcD2IQgK4QegQIARAC)**/A-XMP** (for older DDR4)",Neutral
AMD,Get ZenTimings app and check what frequency/latencies/whatnots you're running.,Neutral
AMD,"Enable expo/XMP in the bios. Your board will be able to read the XMP profile.   Then stress test with OCCT to verify stability. I believe OCCT will show the ram speed but if not, task manager will also show it.",Neutral
AMD,"You can check the speeds in the BIOS. RAM isn't specific to a CPU manufacturer, so just check the BIOS for your speed, verify that EXPO is on. You should be good to go then. If you're confused by this, check out enabling EXPO on YouTube.",Neutral
AMD,"Cool, I figured I already had to do this anyway. So does “Intel Ready” just mean it would work at the rated speed without playing with the BIOS if I was using an Intel CPU? Or would I have to change the setting even then?",Neutral
AMD,"No, if you had intel cpu bios tweaking is necessary too.",Neutral
AMD,Oh okay. So in what situation does the readiness designation really matter?,Neutral
AMD,"Marketing, so they can sell two separate SKUs for Intel and AMD.  I had the same confusion in the opposite direction after buying ""EXPO-Ready"" sticks with an Intel CPU. Majority of the time, they have profiles for both families.",Neutral
AMD,"If you have no space constraints i would recommend a ""closed"" dual tower where there is a shroud over the middle fan and you can't see it.  The performance is worth it.   But otherwise look for a Be Quiet Dark rock 5.",Positive
AMD,sudokoo SK 700V,Neutral
AMD,Arctic Freezer 36 is a good one. Noctua U12A also but you pay a premium for that.,Positive
AMD,"Something like Thermalright Burst Assassin and possibly replacing the fan. not sure if it cools enough under heavy loads though.  You could consider a setup with intake + outtake fans but at that point you would not save that much space, making a case for a dual tower.  AIO is usually a quieter option as you get more fans dissipating heat and you can replace the fans for better noise profile (it might not be a good price though).  I ended up with an unused 360mm AIO for my build. Maybe some other peeps end up with spares like me and you could score a good deal, leaving you some extra cash to upgrade the fans if needed.",Neutral
AMD,I have the noctua U12A silent and enought for it have since they lanched the 7800x3d iam very happy with it.,Positive
AMD,I have the msi core frozr aa13 and it seems to be chugging along just swell,Neutral
AMD,Arctic freezer 36 is your only proper option here.,Neutral
AMD,dark rock pro,Neutral
AMD,Deepcool AK500,Neutral
AMD,You can have twin tower heatsink with just the middle-sandwich fan installed.,Neutral
AMD,[https://wccftech.com/the-best-looking-cpu-cooler-is-finally-here-meet-the-deepcool-assassin-iv-dual-tower/](https://wccftech.com/the-best-looking-cpu-cooler-is-finally-here-meet-the-deepcool-assassin-iv-dual-tower/),Positive
AMD,Scythe Mugen 6.,Neutral
AMD,"I have the Arctic Freezer 36 for the same cpu, does the job nicely. ~75/80 °C under the most CPU heavy games, and thats with very silent fan curves.",Neutral
AMD,dont see why you would change anything. if you have the money go for it.,Neutral
AMD,You have no storage space my guy,Neutral
AMD,"Look for a B850, if doesn't necessarily cost more.   The RM850e is a decent PSU, but a very poor value. For example Corsair themselves recommends Cybenetics: https://www.corsair.com/newsroom/press-release/corsair-transitions-to-more-comprehensive-psu-certifications-from-cybenetics   The RM850e is $110 and is Cybenetics GOLD with A level noise: https://www.cybenetics.com/evaluations/psus/2630  [PCPartPicker Part List](https://pcpartpicker.com/list/nnws8Q)  Type|Item|Price :----|:----|:---- **Power Supply** | [Corsair RM850e (2025) 850 W Fully Modular ATX Power Supply](https://pcpartpicker.com/product/zBbypg/corsair-rm850e-2025-850-w-fully-modular-atx-power-supply-cp-9020296-na) | $109.99 @ Best Buy  The Cooler Master MWE V3 is Cybenetics Platinum with A to A+ level noise: https://www.cybenetics.com/evaluations/psus/2478  [PCPartPicker Part List](https://pcpartpicker.com/list/sYRPbL)  Type|Item|Price :----|:----|:---- **Power Supply** | [Cooler Master MWE Gold 850 V3 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/bTdG3C/cooler-master-mwe-gold-850-v3-850-w-80-gold-certified-fully-modular-atx-power-supply-mpx-8503-afag-2buv) | $87.59 @ B&H  That covers a motherboard upgrade:  [PCPartPicker Part List](https://pcpartpicker.com/list/fqzrPX)  Type|Item|Price :----|:----|:---- **Motherboard** | [Asus PRIME B650-PLUS ATX AM5 Motherboard](https://pcpartpicker.com/product/nGKKHx/asus-prime-b650-plus-atx-am5-motherboard-prime-b650-plus) | $170.00 @ Amazon  [PCPartPicker Part List](https://pcpartpicker.com/list/L3xpKq)  Type|Item|Price :----|:----|:---- **Motherboard** | [Asus TUF GAMING B850-PLUS WIFI ATX AM5 Motherboard](https://pcpartpicker.com/product/sRG2FT/asus-tuf-gaming-b850-plus-wifi-atx-am5-motherboard-tuf-gaming-b850-plus-wifi) | $189.99 @ Amazon",Neutral
AMD,What about gpu and whats you total budget as it totally depends upon budget,Neutral
AMD,I'm just ripping out storage from my old pre-built,Neutral
AMD,"Hands down, NVIDIA B200. It costs a bit more than other GPUs and performs much worse on games, but you immediately become an AI company and get RAM for normal prices",Negative
AMD,"5070ti minimum, 5080 if you can swing it.",Neutral
AMD,9070XT.,Neutral
AMD,What’s your budget?  I’d aim for 9070 XT or 5070 Ti.,Neutral
AMD,RTX 5080,Neutral
AMD,"Did you just recently get the CPU, if so why?  5070 ti or 9070xt are the best value/performace GPU.  5090 the strongest, but very expensive.  5080 is imo very overpriced and not worth it over a 5070 ti.",Neutral
AMD,I'd go with either a 5070 ti or 9070xt depending on if you care about retracing or not. if you want a little overkill go with 5080. all are excellent cards that will easily maintain your refresh rate at 1440p,Positive
AMD,9070XT,Neutral
AMD,What power supply do you have?,Neutral
AMD,If you're going up to a 9950X3D just get a 5080. There's no point being the top cpu and skimping on the gpu. And 5090 is just shit value from every metric.,Negative
AMD,"Or 5070ti. But you absolute high end cpu and play with 144hz dude. So a 5070ti is enough for that. But if you want upgrade your monitor in the future to a 240hz for example, get a 5080",Neutral
AMD,the most expensive one your budget allows you to.,Neutral
AMD,5070Ti,Neutral
AMD,5070 Ti handles 1440p 165 Hz on worse specs no problem (5800X3D).    I can confidently say that my GPU has nothing to do with the reason I suck at battlefield 6,Negative
AMD,9070xt or 5070ti and upgrade monitor to oled qhd 240hz,Neutral
AMD,5080 if you don’t do VR and 5090 if you do.,Neutral
AMD,"I bought pre-built one too (Skytech) from BestBuy. I upgraded the motherboard, cpu (9800x3d) Ram to 32G 6000MIT and the GPU to 5070. I am really happy with the 5070 for 1440P. The performance is really amazing.",Positive
AMD,5090,Neutral
AMD,"Nahh man, he needs the RTX PRO 6000",Neutral
AMD,5080 is pretty overkill for 1440p tbh. Not a bad choice by any means but a 5070ti would be plenty good,Neutral
AMD,"I have a 9070xt, and it cant even do 1080p ultra 60fps in Cyberpunk. (Path tracing on but still) no way bros hitting 1440p 144 with one.",Negative
AMD,"9070xt cant even do 60fps 1080p on cyberpunk for me lol, I def wouldnt go 1440p with one",Negative
AMD,850W,Neutral
AMD,Yall just ignoring the existence of the Ryzen 4070,Neutral
AMD,but thats only 10 grand…,Neutral
AMD,If 5080 is overkill then the 5070ti is already overkill. There's only a 10-15% difference between them,Neutral
AMD,Overkill? Nah it's what I use,Neutral
AMD,Overkill? Do you know who you're talking to?,Negative
AMD,Removed : Misinformation.  ---  I'm an electrician. It's common for a single circuit breaker to have more than one bedroom running off it.  A GPU spiking to 500w should be no where near enough to trip a circuit breaker. You have other issues at play or a circuit that's already under heavy load.,Negative
AMD,"Well I bought a 5080 and it made my power weird too, so I guess you never know",Negative
AMD,Thats either a CPU issue or you got a borked card.,Negative
AMD,"I play at 4k and get way over 60fps on most games. Never played cyberpunk tho but it's a 5 year old game, im sure 9070xt would chew trough it at 1080p. What's your CPU and GPU usage?",Neutral
AMD,"So you have a CPU bottleneck, the person asking for advice has a 9950x3D, you should probably not be giving advice in this situation. The 9070xt is a great card for 1440p.  But as a side note, the crowd density option in Cyberpunk has a pretty large impact on CPU performance, turning that down would probably help you out. And path tracing is absurdly demanding and has a CPU cost as well.",Negative
AMD,"Mhm, what cpu do you have?",Neutral
AMD,That's weird,Negative
AMD,"I just picked up a 9070xt yesterday to replace my 3080. Cyberpunk runs great at 1440p but I don't do path tracing. It's like a 50% fps improvement from the 3080, and double if I use frame gen lol. Frame gen actually feels smooth too. I tested fsr frame gen with my 3080 and it felt terrible before.  Guess I'll test out path tracing now.",Positive
AMD,Taking the performance of the most demanding path traced game and extrapolating it to every other game is definitely a take.,Negative
AMD,"Maybe 5070ti, but you cannot run the best gpu’s on that wattage.",Negative
AMD,the ryzen 4070 is a midrange gaming card. not a datacenter ai card like this guy actually needs,Neutral
AMD,"Mb, he should buy two then. Infact, why not buy four and plug them into the ram slots",Neutral
AMD,"10400f, but its only at 80% usage at 60c, and gpu is at about 41c at 70%",Neutral
AMD,Cpu 80% at 63c and gpu is 70% at 41c,Neutral
AMD,"I5 10400f, wasnt planning upgrade but the 9070xt was on sale for 440",Neutral
AMD,I only get 80 in rdr2 maxed out. It isn't just cyberpunk,Neutral
AMD,"ram slots? plug them directly into the wall, for optimal power delivery",Neutral
AMD,but 4 b200s is even MOAR MONEY,Negative
AMD,"Thats a somewhat old CPU, but there could be a combination of issues with your config, because my 9070 XT crushes Cyberpunk on all ultra settings at 1440x3440 (no path tracing but tbh I dont really notice it looking much better and it just destroys performance even on high end cards for a minimal improvement.)  If you skip to about 14:30 in [this video](https://youtu.be/VqmY5nVdOfw?si=kUGgbVO-O9FxDAC4) you can see what the 9070 XT is capable of at 1440p ultra.",Negative
AMD,"If your GPU isn’t at 95+ percent utilization then you’re CPU bottlenecked, CPU can be limited by a single thread and not be at 100% all core usage.",Negative
AMD,"Something else is the problem then. Is it on an SSD or hard disk? Also, how much RAM do you have?",Neutral
AMD,Clear bottleneck go on 1440p you can even use virtual super resolution,Neutral
AMD,Yeah,Neutral
AMD,"Thats insane. I think its all my cpu, as in rdr2 my single core is at 114% and overall 120% usage, however that happens. I get 80-100fps rdr2 maxed out 1080p",Neutral
AMD,"2tb SATA ssd. The comments led me to think cpu is the issue, as they say usage only shows full core load, not a single core. Its a 10400f",Neutral
AMD,"Disconnect the igpu (the cable from the motherboard) , and connect only the 9070xt to the monitor .    Do you get signal output ?",Neutral
AMD,"No, no signal output from the GPU",Neutral
AMD,"I think 90% the gpu is dead , but doesn't hurt to try :   Go to Bios ->IO Ports -> Initial Display Output -> PCIe 1 Slot  and -> Integrated Graphics -> Set to Auto\*  \*I usually suggest disabling the igpu since it can cause conflicts , but if the 9070 is dead , you will have to clear cmos (reset bios) and apply your previous settings again .",Neutral
AMD,"Tried that, sadly still the same outcome",Negative
AMD,"Well , unless you can find a second working PSU and a gpu card , I guess you start the RMA process . ( it happens on all devices , I know it sucks but most all other stuff are more important :) )",Negative
AMD,"Nvidia usually works great on Linux, the only issue for most people is needing to grab the proprietary closed-source installer and apps like Steam Big Picture being horrible.",Positive
AMD,You could get a rx 7700xt or a 7600xt but they both have higher power draw then the 1660s and the 7600xt isnt that large of a jump in performance but they both have more vram which is nice,Positive
AMD,If you really need to you can get a 6700xt which takes like 100watts more power,Neutral
AMD,Tho power shouldn't really be an issue with 650watts since I have a ryzen 7 5800x and 3070 that's at 270watts working perfectly fine on my 650watt psu,Neutral
AMD,"It's just FSR now, no 4. You can see where it says ML for the new machine learning model.   AMD hates good marketing so they changed it to be even more generic.",Negative
AMD,So they just rebranded the name for confusion?,Neutral
AMD,I think they just wanted to standardise the brand name but it's caused confusion for people that aren't keeping up with the news from AMD.   FSR4 built up a lot of positivity so it's just strange they'd decide to rebrand it to be more generic when the previous iterations of FSR were not so well received.,Negative
AMD,"Welcome to Windows 11 and the reason why I install only W10 Enterprise LTSC IOT on my devices (and friends+family)  Slow cycles, but stable and no issues.",Negative
AMD,Did you connect the extra 8pin cpu power plug? It's on the top-left of the mobo.   I'm suggesting this because it seems it only happens when gaming = increase in powerconsumption.,Neutral
AMD,"Make sure he's not editing the tuning section in the amd software. Overclocking and causing the instability. If he's followed a guide online to boost gaming performance, they often include how to overclock your GPU.",Neutral
AMD,"Hey, thanks for reply!  The MB: ASUS TUF GAMING B550-PLUS has only one 8pin CPU power plug if I am correct:  [https://imgur.com/a/ZWoqR48](https://imgur.com/a/ZWoqR48)  Yes, it is connected :(",Neutral
AMD,Also two seperate ones to the GPU? :D,Neutral
AMD,"Yes, also GPU, like this (error included :D)  [https://imgur.com/a/3C8juh2](https://imgur.com/a/3C8juh2)",Neutral
AMD,"Hm, try setting PCIE to 3.0 in BIOS and see what happens.  Some boards (especially the Primes and VDH for example) have massive signal integrity issues",Negative
AMD,Linux devs doing sidequests again,Neutral
AMD,Very happy to see this ship. This also brings life to the GPUs in the old Mac Pro trashcans as they share the same architecture. So good to see.,Positive
AMD,Old is selling it short.,Neutral
AMD,"I have a R5 240m, worth switching it from windows 10?",Neutral
AMD,ive hd7950 and i feel noticed.,Neutral
AMD,Neat!,Neutral
AMD,It'd be nice if the R9 M370X in the Macbook Pro 2015's also end up being supported. They are GCN 1.0/Southern Islands too.,Positive
AMD,"Yeah, it seems like somewhat of an understatement.",Neutral
AMD,"It's good news all around, especially in this era where hardware shortages and the accompanying price hikes make people instinctively want to hold onto what they have for as long as possible.  Moreover, having viable alternatives to Windows when Microsoft seems dead set on forcing its user base to sunset perfectly good hardware for nebulous benefits is also a good thing.",Positive
AMD,"Definitely, with windows 10 being end of support and if the computer is unsupported on windows 11, you don’t really have a better option",Negative
AMD,"There is essentially a distro for any needs. I wanted something newer and gaming focused, ended up getting Pop!_OS rather than having my PC running Windows 11 when I bought these parts earlier this year… never going back",Neutral
AMD,What if I play counter strike?,Neutral
AMD,"You shouldn’t have any problems, it’s been supported natively on Linux for a while.",Positive
AMD,"Runs natively on Linux a lot better now than at the launch. A lot of it is down to the poor nvidia driver performance at times causing problems for folks with their GPUs, but there are plenty of additional optimisation guides available for this now (because yay for being able to customise your system). One other reason why I like AMD GPUs is the driver stack works so well on Linux.",Positive
AMD,Yeah but I play faceit because I can't deal with cheating infested premier,Negative
AMD,"Congrats my friend, welcome to a life of pain 😉",Neutral
AMD,Wdym? Life of pain was on my laptop with GTX1650,Neutral
AMD,"Ahhh my young padawan, you will learn. Ask me what do you mean in a year or so. Take care my friend 🙏",Positive
AMD,5700x3d is the second best chip on the am4 platform and its BARELY behind the 5800x3d. I think its a fair trade. Good cpu,Positive
AMD,"I think it’s probably the best trade you could have made considering your constraints. And these early 3D processors are proving very resilient, in terms of how well they do even today. You got a GPU and you almost got a free processor in a bundle!",Positive
AMD,"If you couldn't access a good Pre-build or Combo for AM5, you made the right choice with the CPU.",Positive
AMD,"Great trade, these are going for $350 on ebay lol",Positive
AMD,"Finding any am4 x3D component for a fair price is damn near impossible right now and you not only did, but were able to trade for it even - myself and others dream to be in your shoes right now; trust me lol",Positive
AMD,I love my 5700x3d. Was a large upgrade from my previous one,Positive
AMD,I got one for £139 on AliExpress the listing only had 3 sold but it was a gamble so you never know,Neutral
AMD,Perfect!   Remember to link your windows key to your ms account before you swap your cpu. Windows will de-activate when it detects new hardware & unless your ms account is linked you cant fix it with your existing key.  (I usually get automodded for even mentioning this),Positive
AMD,"You have exactly my build, I also upgraded the 3060ti to 5070",Neutral
AMD,"Absolutely fair, would have done the same!  Would love to trade my RTX 2080 Super for an AM4 X3D chip.. Sadly they're barely any on the market here in Germany, and the ones who are trustworthy are usually 350+€",Positive
AMD,good hussle 👍,Positive
AMD,Great trade. AM4 X3D’s are mega value,Positive
AMD,Damn I should have bought alot more 5700x3ds than the 2 i got on aliexpress for 150 each last year.,Negative
AMD,"that's a fair price of 5700x3d on my country. If i have ddr5 lying around under my couch, i rather go to 7800x3d route. It's the same price",Neutral
AMD,"If anyone finds a 5700x3d and they don’t need it, I’ll buy it! I’m stuck in this weird place where I’m still using a Ryzen 7 3700x, while every other part of my system is much newer. I really don’t want to have to completely upgrade to AM5 though given DDR5 price at the moment",Negative
AMD,What cpu did you use with the 3060Ti?  Only then can people form an opinion on whether it was a good trade...,Neutral
AMD,Something is off the 5700x3d was made in 2024.not 2021?,Neutral
AMD,A GPU and a CPU aren't interchangeable parts.,Neutral
AMD,"Yes, I forgot to mention that switching to AM5 is way out of my budget.",Negative
AMD,The price of DDR5 makes that completely a non-starter.,Negative
AMD,5700x3D is still faster than every non x3D AM5 tho.  Even the 5500x3D beats the 9600x and 9900x in most games.  EDIT: [Proof ](https://youtu.be/NdpfV5IkUi0?si=5E3ubGlKgnMzRZ9V&t=819)since some people for some reason think AM4 is 30fps below AM5 and didn't even watch a benchmark about it lol.,Positive
AMD,"Yes, brother, I almost forgot about that.",Neutral
AMD,It will do it for motherboard swap not for CPU swap. If you swap CPU turn of bitlocker if you have it and you will need to re-configure BIOS. Reinstalling windows is also recommended for best performance but not strictly required.,Neutral
AMD,"fuck having an MS account at all, windows keys are very easily solved problems lol",Negative
AMD,"Well, I'm still building it; it's my first time building a PC and I'm struggling.",Positive
AMD,"I'm from Peru and I only found two guys selling the X3D. I bought one, and now there's only one more selling it for a similar price to what I paid, although with customs and everything, it'll end up costing you almost as much as buying it on AliExpress.",Neutral
AMD,"A Ryzen 5 5600G wouldn't bottleneck it, but a 5070 would.",Neutral
AMD,"I don't think the 2021 means it was manufactured in 2021. All 5700X3D say 2021.  edit: Searching online it seems “2021” is simply the copyright year for the chip’s design, not the manufacturing date of your specific unit.",Neutral
AMD,🤣,Neutral
AMD,Good job responding to the title Did you read the rest of the post?,Positive
AMD,It was a good trade and ur PC will be very good for next several years.,Positive
AMD,That’s fucking bullshit mate. Fastest am4 chip yes,Negative
AMD,"I mean, I just swapped out my 5600x for a 5700x3d 2 weeks ago and windows11 got deactivated, but maybe I'm just lucky.",Neutral
AMD,"Read the manuals and have fun with it, but ofc don't forget the basics like leaving the power off xD",Positive
AMD,"Yeah, it’s tough out here. It’s still way cheaper than upgrading to AM5 but I just have to find one lol",Neutral
AMD,So you went from a 5600G to a 5700x3d at the cost of a 3060Ti plus extra $$$ spent on a 5070.  Not a great deal in my opinion.   Hope you can get around a $100 for the 5600G then (which you probably won't).  I would've seen if I could sell the old mobo + 5600G + 3060Ti and gotten a B760 DDR4 mobo + 12700K honestly.  You would've probably found takers with that combo.,Negative
AMD,Thats prob right,Neutral
AMD,[https://youtu.be/NdpfV5IkUi0?si=5E3ubGlKgnMzRZ9V&t=819](https://youtu.be/NdpfV5IkUi0?si=5E3ubGlKgnMzRZ9V&t=819)  Say that again?,Neutral
AMD,Most likely. I swapped CPUs and never had to do it and did have to reactivate when I swapped motherboard. Just as expected.,Neutral
AMD,"And also, YouTube is your friend, plenty of building guides available to watch and learn on.",Positive
AMD,On one game? Are you retarded.,Neutral
AMD,"Are you blind or what? lol.  In all games tested they perform the same.  What's gonna be your next dumb comment? ''Gamers Nexus is not a reliable channel, look at this ''randombenchmarks69'' channel that says AM5 is way better!'' ?",Negative
AMD,"The 5800X3D does NOT “beat all non-X3D AM5 CPUs.” It can beat many of them in gaming, because the 3D V-Cache hard-carries FPS in cache-sensitive titles. That’s it. That’s the context. Ur 5800x3d is also the high end for am4 so that’s no surprise. The 9600x will rock ur cpus shit",Neutral
AMD,">The 9600x will rock ur cpus shit  Dude even the 9900X can't beat the 5800x3D.  **Just watch the video** **with the benchmarks** instead of being butthurt because your CPU is not way better than others (especially with that GPU).  And I don't even have a 5800x3D wtf, are you 10 years old of something?   I don't need to have a 5800x3D in order to watch a benchmark and know how it performs.  >That’s it. That’s the context  Dude please, treat that ADHD.  You can't even read a complete comment before getting butthurt on the Internet?  >Even the 5500x3D beats the 9600x and 9900x **in most games**.  IN MOST GAMES.  I know the context, **I just said the context** 2 comments above.   You had a bad christmas huh? Can't really explain why you would get so salty over the internet just because a benchmark proves you said something wrong.",Negative
AMD,"If you meant the Ryzen 9 9900X (the newer Zen 5 AM5 CPU with 12 cores and strong clocks) — it’s a much newer, more powerful CPU overall than the 5800X3D in most scenarios: sorry mate",Positive
AMD,">in most scenarios  Yeah, in most scenarios except the most important for most people, gaming.  Also, are you really trying to brag about the 9900X beating the AM4 x3Ds in productivity?  A 5500x3D build is like **4 times cheaper** than the 9900X.  How performing equal is a win for the 9900X?  Damn, you are really not the smartest kid in class huh?",Neutral
AMD,"> 6-core Zen2  > Rx 5700xt  > 16GB  The first thing you do is tell him that he did an amazing job for $150. Let him play with it a bit, there's honestly very little that would be denied to him with this machine. See if you run into any issues but there's a good chance he's completely content and should just hold onto the $500.",Positive
AMD,At 150 bucks almost every working PC is a bargain.,Neutral
AMD,Killer deal dude! Hope it doesn't heat up like my ex's temper after running Crysis on max settings,Negative
AMD,Killer deal but if they want to upgrade instead of save that $500 slap in a 5600x(update the mobo bios first) or somethingsimilar and 9060xt 16gb and go to town. Could sell the 5700xt and 3500 and probly get a second 16gb stick of ram also cus looks like only has 1.,Neutral
AMD,That's a good starter PC for 150.,Positive
AMD,150 is really good for sure,Positive
AMD,That’s awesome. That’ll play almost anything. That’s an accomplishment for $150.,Positive
AMD,"if anything I'd put that 500 toward a nice keyboard and mouse, perhaps a monitor, and some games",Positive
AMD,"See if you can get 16 more gigs of the same ram, and upgrade to a 5800x3d chip",Neutral
AMD,"With this GPU you will get the most performance if you install linux.  Not kidding, expect 5-15% more frames.",Neutral
AMD,Shit can be worth 500$,Neutral
AMD,Beautiful deal. I'd tell him to get more RAM but maybe wait another year lol,Positive
AMD,Queria que aqui no Brasil também tivesse estes preços.,Neutral
AMD,"Looks like the RAM is one stick. If you can find another stick of the same, that would help.     The CPU cooler is stock. A 2-fan air cooler or inexpensive AIO would allow for overclock potential and keep temps lower overall.     Make sure the front fans are intake, and the rear fan is exhaust. You could add an intake fan to the bottom and/or an exhaust at the top, with a fan controller if wanted/needed.     That's a great 1080 machine, so a 120Hz+ monitor and quality keyboard/headset/mouse(pad)/chair would it a battlestation.",Neutral
AMD,The GPU alone cost nearly that much. Absolute steal of a pc,Negative
AMD,Good for him thats a steal. I'd throw an extra 16gb of ram in there and slap an AIO on the cpu so he can overclock with peace of mind. Should run about anything at 60fps with no issues. Great for a first build.,Positive
AMD,well your friend is a very lucky person,Positive
AMD,use the 500 and get another stick of ram for dual channel     and yes I know the times are horrible for ddr4 but single channel is abysmal     and anything leftover maybe a 9060xt 16gb,Neutral
AMD,honestly not bad. Let me know if your wanting to flip it for a 10$ profit hahaha. My wife needs a good entry PC.,Positive
AMD,150$ is a good deal for this!  I’ve sold similar builds fast for 250$-350$ depending on extras and people have been very happy getting those deals.   150 is a awesome,Positive
AMD,That's a great score!,Positive
AMD,I'd say add 32gb ram and new ryzen 5 5500,Neutral
AMD,"Hey man, when you need money for crack, you NEED money for crack.",Neutral
AMD,That is an exceptional PC for 150,Negative
AMD,"I have almost the same system, but radeon 5500 xt, smaller ssd and a Ryzen 5000 series. Cost me about 1k in 2020, I think. It has worked great... 150 for that makes me jealous...",Positive
AMD,Isn't that what the RAMs worth??,Neutral
AMD,Wow that’s a great deal,Positive
AMD,I thought it was on fire for a sec with those yellow lights,Neutral
AMD,not bad,Negative
AMD,Your friend did well,Positive
AMD,"In my country, the 5700 XT costs $150",Neutral
AMD,Really good deal. I would suggest wiping the SSD(s) and then performing a clean windows install just to get rid of any virus/malware the previous owner might have downloaded.,Positive
AMD,"Very good PC for $150, still enough for 1080p 60 FPS gaming.",Positive
AMD,"""trust me bro that's what he paid bro it was 150 bro I swear bro""",Neutral
AMD,This is a solid 1080p machine.  Your friend got an amazing score with this PC.,Positive
AMD,That’s very good for 150!,Positive
AMD,"That's really, really solid for 150$. For someone just getting into PC gaming, this will do a damn decent job of running most newer games pretty well.  The only thing I'd consider is the jump to Zen 3 (5600/X, 5700X, 5800X, either or) to beef up the platform for a future GPU upgrade while you can still buy AM4 chips new relatively cheap, and just enjoy it for now.",Positive
AMD,Good score!  Repaste the CPU and reseat all connections.,Positive
AMD,Does it have/support an nvme? If it does and you don't have one get one and make the boot drive,Neutral
AMD,Where's the psu,Neutral
AMD,This isn't a high end gaming rig but ull be able to play most games on decent settings on 1080p monitor.,Positive
AMD,That's decent. Congrats,Positive
AMD,Only thing I would do would add more storage at this point personally.,Neutral
AMD,Very new games will have problems with the lack of hyper-threading on only 6 cores and some UE5 games hate RDNA 1 but everything else will run great.,Negative
AMD,"I thought so myself, as well.  Seeing the graphics card, everything beyond a 10 series card is just gravy, at that point.",Neutral
AMD,How did you run Crysis on your ex?  Asking for a friend.,Neutral
AMD,"I would love to switch to Linux, but Rocket League (my baby) is not available. Sad.",Negative
AMD,Wait another year and a 16gb ram stick will be out of his budget 😭,Negative
AMD,More ram? I know it's just idling for that screenshot but it has 15.9gb available out of 16gb. I wanna know how...,Neutral
AMD,The “steal” part probably being so true. His friend bought this from someone who stole it. Only way to explain the price,Negative
AMD,You should probably do this before you plug it into your home network.,Neutral
AMD,What game? Both consoles drop to 8 threads without SMT to turbo.  I think they'll be fine on just about everything,Neutral
AMD,![gif](giphy|12nFB2c7gK5Xri),Neutral
AMD,You can run anything on your ex,Neutral
AMD,I played Rocket League last week on my CachyOS (Linux).  ProtonDB says Platinum Support for Linux.  https://www.protondb.com/app/252950,Neutral
AMD,I think it's everyone else EXCEPT them that can run anything on their ex...,Negative
AMD,"Wait, can I just install proton and run it on any distro???? Is it really that easy????",Neutral
AMD,"Basically yes.  You would not need to install proton manually, because it comes with steam on most distos by default.  You can login on protondb and check your whole steam library at once.  Epic games can be played through heroic game launcher.  I recommend you r/linux_gaming but you can also ask me, maybe i can help you if there is an issue or something.",Positive
AMD,"I got it figured out, thanks! I got it working with the steam version as I do not want to switch to epic games lmao. I just had to have RL force run a specific proton version. This has been a wonderful, enlightening experience ❤️",Positive
AMD,What is the frame increase?,Neutral
AMD,nice 😊 enjoy your new OS. I'm glad i could help.  Which distro did you choose?,Positive
AMD,"Nothing to write home about, but I went from 400-500 to around 700",Neutral
AMD,"This depends on the hardware and games you play.  In short you can expect on average roughly:  \- modern nvidia -5-15% FPS  \- old nvidia worse than modern  \- modern amd +0-10% FPS  \- old amd +5-15% FPS  Please take this with a grain of salt, because this is strongly simplified. The real performance impact depends on many variables (GPU-Vendor/actuality, RAM amount, Games and other).",Neutral
AMD,"I'm using Linux mint to see if I could get it working, but I am thinking about doing Pop! since it's got better performance with current gen hardware. Just upgraded my PC too, so we're rolling good this Christmas lmao",Positive
AMD,Order the chicken wings and bring out the Capri suns because it's gonna boot like you never seen it boot before. The bestest boot in all of history.  ![gif](giphy|BPJmthQ3YRwD6QqcVD|downsized),Neutral
AMD,So... Don't keep us in suspense? Did it boot?!,Neutral
AMD,![gif](giphy|9SIXFu7bIUYHhFc19G|downsized),Neutral
AMD,![gif](giphy|QBd2kLB5qDmysEXre9),Neutral
AMD,DID IT BOOT THO,Neutral
AMD,"Solid build for a kid for sure. Well done, dad.",Positive
AMD,Nice build!   Finished one with my son earlier this week,Positive
AMD,I just built a very similar system as my first build. It's going to be a killer experience for you and your kid!,Positive
AMD,"Looks like another comment covered it, but I also want to make sure you know that its entirely possible for the first boot to take longer than you expect and not to freak out immediately.",Neutral
AMD,Did it boot? I can imagine the smile on your face and your son.,Neutral
AMD,good cable management in the back! looks great!,Positive
AMD,Celebrate how? Don't leave us hanging,Neutral
AMD,Allmost the same case as me! Nice :) (I got the airflow version),Positive
AMD,That’s a sexy gpu,Neutral
AMD,"No se ve bien, pero por las letras pareciera que la fuente la pusiste con el cooler hacia arriba, si es así la pusistes  al reves, el cooler va abajo para que tome aire fresco desde la rejilla.",Neutral
AMD,That's clean. Great PC.,Positive
AMD,CPU cooler looks skimpy,Neutral
AMD,How you gonna have time to post all this and not boot the thing? What a tease!,Negative
AMD,Ahhh nothing like a wholesome father and son moment like building a pc. Remind me to do the same when I have kids in the future :D,Negative
AMD,"Sleek build! Congrats fellow dad, your son will be happy! I have just finished a similar aesthetic one. Check it out [https://www.reddit.com/r/pcmasterrace/comments/1pxt6zc/first\_desktop\_in\_15\_years/](https://www.reddit.com/r/pcmasterrace/comments/1pxt6zc/first_desktop_in_15_years/)",Positive
AMD,Congrats on the build! But i think something’s wrong with that gpu bracket,Negative
AMD,Odd choice in CPU but sweet build!,Neutral
AMD,TRUMP?,Neutral
AMD,"We’re about to have dinner, and he wants to take a break and continue tomorrow. Damn it. The suspense is killing me. But I will be strong.",Negative
AMD,"I‘m waiting too. It looks like my kid wants to teach me a lesson in patience. We are going to power it on tomorrow. I think it’s outrageous, but it’s not my PC.",Neutral
AMD,"Thanks. My goal was to give him something that performs well, but has also room for upgrades in a few years. I mean you always have to make some compromises. For instance a 2TB SSD would have been nice and would have been something I would put in my own Pc. But I thought it is an upgrade that he can do easily by himself if it bothers him. So I saved there a little bit money and give him an incentive to work on his PC a little bit at a later date.",Positive
AMD,Thanks for reminding me.,Positive
AMD,Sadly you and I have to wait. He wants to continue tomorrow. He needed a break before diving into the bios configuration and windows installation process.,Neutral
AMD,Yeah came here to say this. Anything more than this is just OCD,Negative
AMD,"I’m glad you like it. It took some time to get it right. I still see some stuff that could be improved, but I think it’s good enough in the back. It’s not messy and you can easily replace something.",Positive
AMD,He’s gonna be a brother in nine months,Neutral
AMD,"It’s rated for 190W TDP, the CPU has 65TDP (I think). This should be enough.",Neutral
AMD,First we wanted to take some time to appreciate it. And now I have to wait. He wants to take a break till tomorrow.,Neutral
AMD,Should I remind you in 10 years?,Neutral
AMD,Can you elaborate what you mean? I will check it.,Neutral
AMD,Why is it odd?,Neutral
AMD,"Great CPU choice, what do you mean?",Positive
AMD,What would you choose without going over budget and why?,Neutral
AMD,"I had no idea RAM needed to be ""trained"" (or something like that) at the first boot.  So I am here, in front of my first build, powering it on, and \_nothing happens\_ for several minutes. At the moment I start to think about how much of a moron I am to even try to build myself, instead of buying pre-built, and how annoying troubleshooting that mess is going to be, I see the POST happening and can enter a happy little bios.",Negative
AMD,Reboot the kid,Neutral
AMD,"My son gets by fine with 1TB.  He mostly plays Fortnite, Minecraft, and Roblox. But he’s picked up a few others and he’s never challenged the 1TB since he typically uninstalls the games he’s not currently playing.",Positive
AMD,Yeah that’s plenty didn’t realize it was a 65,Neutral
AMD,"I have the same GPU, your anti sag bracket shouldn't be ""sagging"", you probably need to loose it a bit near the support.",Neutral
AMD,I would also like to know why it’s odd lol,Neutral
AMD,The only argument I could imagine is that it's about €300 opposed to the €200 for a 9600x which delivers almost exactly the same performance.,Neutral
AMD,It's apparently not considered as a a very good price to performance option,Negative
AMD,"Wouldn't a 9600x be more reasonable for this build? Afaik there isn't much of a difference between the 9600x and 9700x, at least with gaming, although tbf I'm assuming it's for gaming.",Neutral
AMD,He thinks just everyone owns a 9800x3d lol,Neutral
AMD,"Ha, thanks for the heads up. Ive heard that’s a thing with ddr5, but forgot about it. So tomorrow when we power it on, I will wait a little bit longer before I get concerned…",Neutral
AMD,Ram does not to be trained at the first boot of a computer lol,Neutral
AMD,I was surprised as well when I saw the specs of the CPU. there’s an option to change it to 105W TDP.   But even with that it will be enough.,Neutral
AMD,Because you either grab a 7800X3D or 9800X3D - if gaming is their main „focus“.,Neutral
AMD,That is valid. The 9600x is a much better choice because it's just as good but 33% cheaper.,Positive
AMD,Its praised as the best price to performance gpu of current generation,Positive
AMD,"> I will wait a little bit longer before I get concerned…  And then wait a little more, ha.",Neutral
AMD,"Some motherboards will throw an error, but you have to wait it out, threw me for a loop when I first booted the the mb light stayed a solid yellow.",Negative
AMD,"It's hilarious how people go online and just like... say things that can be disproven with the easiest google search ever.  Yes. Yes it does. New RAM often needs to be trained. It's not an active process that requires user input, but it does happen.",Negative
AMD,Those are hundreds more expensive for exactly 0 additional fps when paired with a 9060 XT.,Negative
AMD,"9700x is a CPU, not a GPU.",Neutral
AMD,"Yeah, but it’s a thing you shouldn’t even notice and it happens when you launch the pc. What about the pc training itself will not make the pc launch for multiple minutes? I built multiple pcs with both DDR4 and DDR5 and never did I have to wait several minutes for the ram to be trained lol",Negative
AMD,"Yeah, if they had the extra money, it would have been better spent towards a better gpu",Neutral
AMD,"Ah shit, my dyslexic ass saw gpu even though i read the comment twice, my bad, the downvotes also mustve reinforced this lol",Negative
AMD,"I just did a 9800x3d/32gb ddr5 build and it took several minutes to post on first boot. It also did it again when I added another set of ram to it. So yes, it does in fact take time to train.",Neutral
AMD,"It depends on the RAM and the PC, but it can take a while. Crucial's website says up to 15 minutes",Neutral
AMD,"I wasn’t aware of that! I also got an 9800x3d last week and had nothing like that, but i always learn new things!",Positive
AMD,"Alright then ill retract my comment, literally wasn’t aware of this, my bad!",Negative
AMD,"Tbh it scared the hell out of me. I haven’t built a pc in over 5 years and on first boot it didn’t post, luckily I was patient and it eventually posted. Then I saw a comment on here about memory training on first boot and was “oh that’s what that was.” Then I added 32gb more and it did the exact same thing.",Negative
AMD,I respect that. Sorry I kinda came out the gate swinging lol,Neutral
AMD,"Haha no worries, you were absolutely right and I was too lazy to google and just say what I thought, you learn something new everyday",Positive
AMD,"It depends how much you're willing to spend on an upgrade.   Most noticeable would be an upgrade to a dGPU, since even though the 5600GT is a beefed up iGP, it's still an iGP from \~2020.   If you have extra budget left over from that, the best CPU you can do in that socket is a 5800x3d or 5700x3d, but finding something like a 5800 xt is much more likely at a reasonable price.",Neutral
AMD,OP what are you looking to do with the PC?,Neutral
AMD,Can you list the brand/model of what you have? Also what operating system? If it's Windows you can search for a program called system information.,Neutral
AMD,Just for casual gaming. I mostly play ARK and similar games but right now I'm playing on low settings.,Neutral
AMD,Like others said a GPU makes the most sense.   I’d got RX 9060 XT if you have a budget or a 9070 XT if you have the money to spare.   Everything else seems decent. That hardware can game.,Positive
AMD,The 5060 by a fair margin because it’s more recent and has access to newer technology. What price are they though? What about something on sale like this  https://www.walmart.com/ip/iBUYPOWER-Element-SE-Gaming-PC-Desktop-AMD-Ryzen-7-8700F-NVIDIA-GeForce-RTX-5060Ti-8GB-32GB-DDR5-RGB-RAM-1TB-NVMe-SSD-ESA7N56T01/17048165781,Neutral
AMD,"Intel + RTX Combo, i don't trust longetivity of RX Radeon driver update unless it's Radeon 9000 series",Negative
AMD,"I don't like both of them. If the Case is the one pictured then im pretty sure the PCs are overpriced.  On the Ryzen CPU side i dont like the Desktop 8000 Series. So like it doesnt exist for me😂 Oh and i never heard of the RX 7700 16GB, i only know the RX 7700XT 12GB but could be me who missed out on that.  On the Intel side i dont like the RTX 5060. Would be to weak for me. When i buy a PC it should last a bit longer than 2-3 years in terms of upgrading.  And on BOTH PCs i dont like the 16GB RAM. I know it expensive atm but 16GB is nowadays the Office PC recommendation and 32GB+ for everything else.  Would love to see the prices tho cuz you cant recommend without knowing the price.",Negative
AMD,If you have some selfrespect you would choose AMD and your going to start using Linux as operating system.,Neutral
AMD,Have you considered this? its a $899 but you get the 5060 ti and 32gb ram. [https://www.walmart.com/ip/iBUYPOWER-Element-SE-Gaming-PC-Desktop-AMD-Ryzen-7-8700F-NVIDIA-GeForce-RTX-5060Ti-8GB-32GB-DDR5-RGB-RAM-1TB-NVMe-SSD-ESA7N56T01/17048165781?classType=REGULAR&athbdg=L1103&from=/search](https://www.walmart.com/ip/iBUYPOWER-Element-SE-Gaming-PC-Desktop-AMD-Ryzen-7-8700F-NVIDIA-GeForce-RTX-5060Ti-8GB-32GB-DDR5-RGB-RAM-1TB-NVMe-SSD-ESA7N56T01/17048165781?classType=REGULAR&athbdg=L1103&from=/search),Neutral
AMD,The Ryzen was 849.99 and Intel was 779.99,Neutral
AMD,The Ryzen was 849.99 and Intel was 779.99,Neutral
AMD,"Okay for only 70 more i would definetly stick with the Ryzen System. CPU is ~10% faster based on what you play, sometimes the Intel is faster, but overall Ryzen +~10%. The RX 7700 and RTX 5060 should be on the same performance level. But the RX 7700 has double the VRAM of the RTX 5060.  If you want to buy one of the 2 PCs:  -Raw Performance + VRAM Headroom: Ryzen 8700F + RX 7700  -If you care about CUDA, more modern Upscaling (DLSS 4, Multi FrameGen) and better RT: i5 14400F + RTX 5060   Edit: On both PCs i recommend upgrading to 32GB. Both PCs come with only 1x 16GB Stick of RAM. Thats a bottleneck in itself for the CPU.",Positive
AMD,"6750 xt for £208, if you live near bristol and can collect it  https://preview.redd.it/i4v8fl37nx9g1.jpeg?width=1206&format=pjpg&auto=webp&s=4817e3d530466741c966fd7323fbddfb9eeee3af",Neutral
AMD,"6700/6750xt, maybe a 6800.  Great bang for buck GPUs and capable of 1440p, but forget ray tracing. 7000 series wasn't much of an upgrade, 90 series is over your budget.",Positive
AMD,The RX 7600 8GB can be had for that amount new. Second hand you may find up to an RX 6800.,Neutral
AMD,"Looks good, but I want to do my due diligence before committing to a purchase",Positive
AMD,I'll have a look. Not too worried about RT. Tend to turn it off on my main mostly. I'd rather proper raster.,Neutral
AMD,"Interesting, I take it the performance would be best on the 6800 from those?",Positive
AMD,6750 xt 12gb > 7600 8gb,Neutral
AMD,literally the best amd gpu you can buy at that price point… miles better than the 7600,Positive
AMD,"Yeah, about 40% faster than a 7600 8GB.",Positive
AMD,"Yes, by 40% raw performance and 50% extra VRAM.",Neutral
AMD,https://preview.redd.it/zxmhn886cy9g1.jpeg?width=1206&format=pjpg&auto=webp&s=de69472beeb6c24d19f1bdedc37c47fb1aa63ea2,Neutral
AMD,"Yes, I know. Which is why I recommend second hand as they can get up to 6800 class cards in their price range.",Positive
AMD,9060xt 16gb + PSU swap (B tier 750watt)  Could also upgrade to R5 5700 CPU (Better then 5600 for productivity),Positive
AMD,GPU,Neutral
AMD,"Do you think the 9060xt will be good for 1440p mid-high settings down the line? I plan on upgrading my monitors.   Also im trying to future proof a bit, that’s why I was leaning more towards AM5. But honestly a 5700 would be pretty adequate for my needs.",Neutral
AMD,"9060xt can do a stable 90-120FPS at high settings 1440p with FSR Q in battlefield 6 and ~90FPS in arc raiders high graphics.   If you changed to an am5, you would also need to swap motherboard and ram, certainly not a budget option right now.",Neutral
AMD,Powerbutton isnt connected to the board. Bottom right,Neutral
AMD,"Like everyone else is saying, case connectors are not connected.   Also looks like you have two sticks of RAM in slots one and two? Most dual channel RAM setups operate in slots two and four. I could be wrong. Check your motherboard manual.",Negative
AMD,"The cables from the front panel need to be connected, there should be some small cables coming from the front/top panel for the power switch and reset switch and a couple bigger ones for the USB ports and stuff. The smaller ones connect to the front panel area down the bottom right of the motherboard, check the manual or the writing on the motherboard to know eactly what to put where.",Neutral
AMD,"The front panel connectors aren’t connected, which is why your PC isn’t turning on.",Negative
AMD,Your power button isn’t connected,Neutral
AMD,Bottom right corner of the motherboard is your front panel IO. You should have cables attached to the power button. Use mobo manual to hook them up correctly to the pins i mentioned,Neutral
AMD,Read your motherboard manual. They tell you what cables must be connected.,Neutral
AMD,Like everyone else is saying:  Front panel cables go here. Should be in the manuals on how to connect.  https://preview.redd.it/x7zggmsam8ag1.jpeg?width=1080&format=pjpg&auto=webp&s=22e146f33df359b9037529c5ce46baf5ca405d24,Neutral
AMD,Please put ur RAM sticks in the second and fourth slot,Neutral
AMD,Power cable isn't plugged in either... step one on the ole' IT call-center helpline flowchart.,Neutral
AMD,"Attention, you're missing another cable to the CPU power supply. There are four free female connectors at the top left. This motherboard apparently has 8 + 4 lines.",Neutral
AMD,Is your power supply on?,Neutral
AMD,Motherboard manual is your best friend here regarding all of the problem you encountered here.,Positive
AMD,Did you flip the switch on the power supply to turn it on?,Neutral
AMD,You can also check it is going to power on / boot by sticking a screwdriver or similar between the power pins. Better to do this and see if doesn't boot than put everything together and nicely inside a case before realizing there is an issue!,Neutral
AMD,"I have the same case... You see that black flat cable bottom left of your motherboard? That's the power cable, you've connected it to a USB header. Unplug it from that and look in the bottom right corner of your motherboard for the correct header. Also, ignore the comment about your fans being the wrong way round, they are reversed fans and 100% correct as is.",Neutral
AMD,You didn't plug in the front IO,Neutral
AMD,I see no front IO connected to their respective places.,Neutral
AMD,"Front panel connectors not connected bro , also check if you plugged everything into the psu.",Neutral
AMD,What everyone else is saying and your ram sticks are in the wrong seats. Just to confirm check your mobo manual.  Usually you want slots 2 and 4. Looks like youre set in 1 and 2.,Neutral
AMD,"F\_PANEL connector needs to go in the bottom right cluster of connectors, read your case's and motherboard's manual if it's not a single plug and play connector",Neutral
AMD,"Read your Motherboard manual, OP. Many have accurately called out that you're missing the front panel connectors. You should also be using the preferred RAM placement for dual channel. You are also missing a CPU power cable, the 4-pin, which while not technically needed is still preferred. I would say it's worth re-doing the cable runs, if nothing else to work on your cable management.",Neutral
AMD,Besides everything else everyone is saying pls for the love of god route your cpu cable through the nearest hole to where you plug it...   It's not why you PC won't turn on (considering the power button isn't connected and the ram is not correctly installed) but it tilted me a bit :),Negative
AMD,I'm a noob too,Neutral
AMD,"I believe there is some sag there on the Gpu, definitely not the cause but I recommend getting a bracket or something to correct it",Negative
AMD,RTFM 😂 nearly there!,Neutral
AMD,"Ram is also in sequential slots, looks like there are 4 slots, the ram should be in 1 and 3 or 2 and 4",Neutral
AMD,Doesn't look like you have all your case buttons plugged in on the bottom right of your mobo otherwise check all connections,Neutral
AMD,"Just feel around the bottom right pins with a metal screwdriver, it’ll turn on.",Neutral
AMD,Wireless casing,Neutral
AMD,"Power button isn't connected. Also, your two front most fans are the wrong way.  Edit: rechecked comments and zoomed in. Front most fans are correct! My apologies.",Negative
AMD,It ain't got no gas in it,Neutral
AMD,Ain’t got no gas in it,Neutral
AMD,"Start by stripping it down to the basics....1 stick of RAM, no GPU....etc....",Neutral
AMD,you built a PC and don't know how to connect the power button?  lol,Negative
AMD,"this is correct.  those multi pin spots bottom right of the board are for power button, USB etc.  check manual that came with motherboard to connect them.",Neutral
AMD,GREAT EYE!!!,Positive
AMD,"Good Catch.  OP, Fix your ram! Read the motherboard manual!",Positive
AMD,Also slot 2 doesn’t look correctly seated,Neutral
AMD,I'm going to take advantage of this image to suggest OP to reroute the GPU cable through the whole that's circled there. Makes the build look a bit more clean.,Positive
AMD,this is usually not needed though unless doing extreme overclocking,Neutral
AMD,"No need, power button is not connected.",Neutral
AMD,Were you born knowing everything?,Neutral
AMD,Fun fact I thought that too. Then my pc wouldn't start without it. Some Mobos won't allow it start without all of them filled.,Neutral
AMD,When they are going to release the 8 core version of HX3D for mobile?,Neutral
AMD,Why is this being covered in the news as if it's a specific RDNA2 problem? It sounds like it's isolated to specific cards and people who let their GPU sag. It's almost 1:1 tied to heavy cooler cards.   I haven't had as much of an artifact from my 6900xt from xfx with a z support bar all these years later and that thing is massive.,Negative
AMD,well that's good to hear they still honor cracked gpus,Positive
AMD,Looking forward to seeing the Crosshair X870E Hero NEO board (just for curiosity since I already have the non-NEO version),Positive
AMD,"> ROG  I can't wait for them to cost more than the CPUs going into them./s  Seriously though, I feel the chipset offerings for higher-end systems is lacklustre, especially considering the insane prices some of these boards go for.  X870 is just a B650E in fancier terms and X870E is basically X670E but with USB 4. I feel X870E in particular is bad as it takes up 4 PCIe lanes and provides less options to the user (you could always add USB 4 through a PCIe expansion card and basically have an X870E, if needed).  X870 may also be seen as a downgrade on X670 as it offers less M.2 slots due to having only one BIOS chip instead of two, like X670, X670E and X870E. Some X670 boards also received unofficial PCIe 5.0 support on the main x16 slot, which brings it on par with X670E.  I feel AMD should've changed the chipset uplink to PCIe 5.0 x4 instead of PCIe 4.0 x4 for X870 and X870E, as this would bring the bandwidth between the chipset and the CPU to parity with Intel's Z890 (runs at PCIe 4.0 x8). This would also enable the use of an extra PCIe 5.0 NVMe SSD without leaving too much performance on the table as it will no longer be bottlenecked by PCIe 4.0 between the chipset and the CPU.",Negative
AMD,"Well, just like Gigabyte released their (Elite, Pro, Master, Extreme) ""X3D"" refreshed versions... sound like Asus wants to do the same.  I got the Aorus Elite X3D board and im super happy with it. Curious what the NEO boards have to offer.",Positive
AMD,What does neo mean? Did they say anything about that?,Neutral
AMD,Will there be more workstation boards or is it just Pro WS B850M-ACE SE?,Neutral
AMD,"Of course, right after I buy a new X870E board...",Neutral
AMD,"Finally, will be keeping eye on what these revised boards do.  MSI got the Max, Gigabyte has X3D, Asus has NEO (lol) and dunno ASRock yet but I'm guessing people have trust issues. MSI messed up with my godlike I got last year so they replacing it with a godlike x. Was buggy as hell so if the moment my replacement acts up I'm replacing it quick with gigabyte or this asus.",Negative
AMD,"I wonder if there is a new chipset for Z6 next year. Seems awfully unnecessary with a whole new lineup now, mid-generation. The 8xx boards were already an opportunity to improve designs and add slightly newer 3rd party chips like Wifi+BT along with USB4, two years after 6xx and Z4. And the 8xx boards were the *same* chipset hardware. What the hell is this then supposed to offer?",Negative
AMD,Thunderbold 5 (80gpbs USB) or it didn't happen.,Neutral
AMD,"My preinstalled io shield on the B450-F is so loose that I can barely plug in a usb connector anymore. Can I fix it? Nope, a giant plastic shroud is blocking access. I used to swear by Asus, but wow their products are trash these days. I can't even control the LED's anymore because I need something called ""armory crate"" and not just the standalone ""aura sync"". Fuck asus and their crap software, which btw wont even install anymore either.",Negative
AMD,"I really wanted the X870E Crosshair Hero, but the rear I/O was so disappointing at that price.",Neutral
AMD,there are several x870e boards which allow for the USB4 ports to be disabled (or shared with an m.2 or pcie slot). For whatever reason ASUS doesn't allow for this yet. If AMD somehow got their hands on some USB4v2 chips it wouldn't matter at all since those essentially operate at pcie 5.0 output anyways.,Neutral
AMD,At least you get a faulty PCIE slot and a warranty not worth the paper it’s printed on.,Neutral
AMD,what did people expect when it comes to pricing?   Companies are for profit and not defending them only explaining the natural course of whats happened.   AMD took Intel's 2 generations per socket and kicked it to the curve.   Companies are selling 2-4times fewer boards since people upgrading chips aren't upgrading there boards anymore.  The only way to make the same profit was to price boards higher. People though they could have there cake and eat it to.   That isn't even taking into account inflation and all that.,Negative
AMD,"Have you had any problems with the bios? I was thinking about picking up the Elite, but I haven't seen many reviews of the X3D version yet.",Neutral
AMD,It means 20% higher cost.,Neutral
AMD,Neoprice. Basically they are increasing price to account for inflation until 2030,Neutral
AMD,Small improvements/changes just to refresh the lineup so they can ask for MSRP prices again.,Neutral
AMD,"maybe new things like more usb in the io shield, better vrm (maybe +16 phases or more), led debug display, clear cmos button, better pcie button and maybe rgb logo plate  oh! and a +10%-20% price increase .\_.",Positive
AMD,"I don’t think so, workstation motherboards usually takes longer. The Pro WS B850m just launched last month.   Saying this as I just bought the Pro WS B850m last month, but I picked it because of mATX & 10GbE. So unless the new range will have these I won’t be interested.",Neutral
AMD,workstation board with 24pcie lanes?,Neutral
AMD,Me2 :D Last week godlike edition x.,Neutral
AMD,"Well, my board came with bios version F2 and had a couple of random reboots in the first few days and then did some RAM tests. Turns out after updating to F4 it solved my RAM stability issues. Right now its great.  Just forget the base Elite and get the Elite X3D. It has better VRM, metal backplate, more ports, more headers, full bandwidth wifi 7, 5G Lan (instead of 2.5G).",Positive
AMD,The X is MSi’s GodLike refresh. You’re good.,Positive
AMD,"Good to know, yeah the x3D seems just objectively better, and it's PCIE lane sharing split makes way more sense than the original board. I wonder if these new Asus boards will also get better lane sharing",Positive
AMD,"It's identical to the Godlike, even. Only differences are cosmetic",Neutral
AMD,Check [this review](https://youtu.be/DCzTATSHVMM) of the Aorus Pro X3D (which is basically the same as the Elite X3D just with a bit better VRM and a tiny RAM fan included). The X3D turbo mode 2.0 is very good. Gigabyte really did some magic with these X3D cpus.,Positive
AMD,"Interesting but not my fav stock cooler design, so no big loss honestly. The stock coolers on 6000 went pretty hard though",Positive
AMD,"Your post has been removed because the site you submitted has been blacklisted, likely because this site is known for spam (including blog spam), content theft or is otherwise inappropriate, such as containing porn or soliciting sales. If your post contains original content, please message the moderators for approval.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
AMD,"Hey, first time running this benchmark, or any 3DMark stuff, but this is what I got from it.  [https://www.3dmark.com/3dm/148400061](https://www.3dmark.com/3dm/148400061)  >Difference: +14.7%  >RX 9070  >Adrenalin 25.12.1",Neutral
AMD,I've got an old run here on RDNA2 from 2021:  https://www.3dmark.com/sf/19382  >Difference: +4.7%  >6900XT  >Adrenalin 21.10.2,Neutral
AMD,[https://www.3dmark.com/sf/149812](https://www.3dmark.com/sf/149812)  AMD Radeon RX 7900 XTX(1x) and AMD Ryzen 9 9950X3D  Sampler Feedback off 840.74 FPS  Sampler Feedback on 777.98 FPS  Difference-7.5 %  Driver version32.0.22029.9039   Adrenalin 25.12.1,Neutral
AMD,"https://www.3dmark.com/3dm/148655489  Difference +13,5 %  RX 9070 XT  Adrenalin 25.12.1",Neutral
AMD,Thank you! Interesting,Positive
AMD,Thanks for sharing it! Could you share a recent one? It doesn't matter if it's with a different GPU. All setups are welcome for the research.,Positive
AMD,"Thank you! It's curious that, so far, only the shared results from RX 7900 XTX users have notable negative performance differences in this test, indicating significantly worse performance when using the Sampler Feedback feature ...  Mine is from the post example (RX 7900 XTX, Adrenalin 25.11.1):  [https://www.3dmark.com/sf/149548](https://www.3dmark.com/sf/149548)  Difference: **-7.9%**",Neutral
AMD,Just reran it on my 9070XT:  https://www.3dmark.com/3dm/148680595?  >Difference: +13.1%  >9070 XT  >Adrenalin 25.12.1,Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,This will actually run Windows 98 without additional expensive DDR5 ram :),Negative
AMD,I really wish people would benchmark modern use cases like having your game on one monitor and on the second monitor a browser open with 12 tabs one of which a video you are looking at in between stuff ingame. Then possibly also being connected to voice chat in discord the whole time? I can’t be the only one that has this scenario a lot of the time.,Neutral
AMD,Will this be faster and more performant than 9800X3D in gaming at 0.1 & 1% lows?  192MB of L3 cache is MONSTER,Neutral
AMD,If X3D is so good where’s X3D2??,Positive
AMD,Except for very rare workloads I still don't see the benefit. If you only game then it will be zero difference there.   Most production workloads benefits more from the higher clocks.,Negative
AMD,"I am not sure of the purpose of this CPU, Infinity Fabric isn't facts enough to not cause stuttering in latency sensitive operations (gaming being one of them).   Is it meant for other use cases, i would guess?",Negative
AMD,"I don't see how this symmetrical design will improve performance in a meaningful way, given the interCCD latency penalty still exists.",Negative
AMD,I'm really excited about the price in comparison with the regular x3D model.,Neutral
AMD,Of course they waited until we're right in the middle of a RAM apocalypse to release it.,Neutral
AMD,Took them long enough,Neutral
AMD,Well now… let’s see how the 9950X3D2 Electric Boogaloo compares to Core Ultra 5 245K.,Neutral
AMD,"Weird product. Pointless for 99% of games as inter CCD latency will still ruin performance when using more than 8 cores (so you'd still want to make sure games stay on one CCD... just like the 9950X3D now).  The number of games that scale to more than 8c/16t can be counted on one hand, pretty much. For me it's just M&B Bannerlord where I saw small but consistent gains by using both CCDs of a 9950X3D (but since I'm fully GPU bound with my regular settings on 1 CCD already... it hardly matters!).  Might be a nice product for professional apps that need cache, though.",Neutral
AMD,"Finally, something that might be able to run cities skylines ii",Positive
AMD,Would this mean that each CCD has cache directly attached now?,Neutral
AMD,"Of only i got some ram a few months ago... Though about getting the next generation. Then there was a sale on 9800x3d and i almost jumped on it.... Bah just my luck in all things in life, late to the party.",Neutral
AMD,"What's the point of this CPU, isn't Zen 6 around the corner?",Neutral
AMD,Still waiting for more than 24 pcie lanes on desktop,Neutral
AMD,How much better do think X3D2 will be over X3D for MMO’s?,Positive
AMD,Is this worth upgrading from a 9950x3d?,Neutral
AMD,Time to sell my 9950x3d for 1% better performance.  Isn't that what CPU buyers do these days!?,Neutral
AMD,x3d2 is such a stupid name.  anyone else remember that one time a company fucked up and listed a 'ryzen x4d'? use that instead.,Negative
AMD,So the question is - wait for Zen 6 or go with this? Coming from 7950X3D.,Neutral
AMD,"Full gigabyte or bust, DOA  /s",Neutral
AMD,Names are too god damn long,Negative
AMD,I wonder if it needs ram at all.,Neutral
AMD,windows 98 SE was the goat,Neutral
AMD,"Be careful, I was a Beta Tester of MS-DOS 1.0 - I was a part of the original Tiger Team if my memory is right :)",Neutral
AMD,"I'll go the Linux way....Just Crunchbang, TinyCore or Puppy Linux will run entirely on Cache.",Neutral
AMD,"It's a few years old now, but can't see how it would have changed much - hardware unboxed tested this.  https://youtu.be/Nd9-OtzzFxs?si=exfpdvK7cM0GHpdx",Neutral
AMD,"Benchmark has to be reproducible, that kind of load is far too finicky to be a reliable benchmark.",Negative
AMD,"All this stuff isn't really resource intensive, any somewhat modern CPU shouldn't have any issues handling that amount of multi tasking. I constantly have edge open with several tabs active and 100+ suspended, Spotify, discord and I don't notice any slowdown on an 11yo 5960x, which is significantly slower compared to basically any modern CPU.   You don't need a 16 core 9950x if all you do while playing is discord and YouTube, a 6 core 9600x will be perfectly fine.",Neutral
AMD,"I find myself in this exact situation 90% of the time I'm playing a game. Discord open, maybe not in a call but its there, firefox with a bunch of tabs open, steam running in the background, whatever game I'm actively playing and often a YouTube video or streaming platform playing.  I'm curious, do you also get video playback stutters when watching something on one monitor with a game running on the main monitor? I've been trying to figure out whether its inevitable due to the game hogging resources or if there's some configuration I can do to stop or at least reduce it.  I'm running a 9800x3d, rtx 4080 until a couple days ago - now running a 5090 (yay!), 64GB DDR5-6000, Windows 11 all running on nvme ssds. Also MSI MPG X870E Carbon Wifi mobo. Monitors are currently 2x 2560x1440 144hz.",Neutral
AMD,Surely out of all the CPUs out there this will be one of the worst for your use case /s,Negative
AMD,"It's almost impossible to do repeatable benchmarks in that manner, it's why basically no one does it. You would probably have to run every test a dozen times to get a good average result.",Negative
AMD,I do all of that at the same time with a 5900x from 2021 AM4 has been good to me. Makes me consider skipping AM5 all together.,Positive
AMD,"I actually ended up getting a huge ultrawide monitor to replace dual monitors. It helped reduce the adhd task switching a little bit lol. With the Windows game mode, those other tasks go into low priority mode. Discord and chat is light enough to not affect much.",Neutral
AMD,While it's older - I love my 5950X which is heavily OC'd for exactly this. I often have virtual machines running in the background then decide hey - it's time for a game. I'll upgrade some day but my glob RAM prices lmao,Positive
AMD,Those sort of applications have essentially zero CPU cycle hits. Video will be GPU and browsers/electron apps will be RAM.,Neutral
AMD,"Video and discord barely require CPU time, video is decoded by gpu and discord runs on other cores.",Neutral
AMD,So basically watching p*rn on the 2nd screen while leveling your paladin in Classic WoW.,Neutral
AMD,"Most people have one 1080p monitor. That is not true of extremely high-end users, though..  Additional monitors and browsers tend to have negligible effect on performance on modern hardware, unless you are hitting bus or memory bandwidth limits.",Neutral
AMD,"lmao or you could like, not do that.",Negative
AMD,Why does this matter at all? This is not what benchmarks measure - they measure relative hardware performance.,Negative
AMD,"In gaming applications you're probably going to see very comparable results to the 9800X3D, since games will still only use one CCD. This chip is basically just two 9800X3D's stuck together, but actually using both CCD's in gaming is still limited by the Infinity Fabric connection, which right now makes that not worth it (though AMD is rumored to be working on exactly that for Zen6 and beyond)",Neutral
AMD,its 96mb per ccd so performance will be the same,Neutral
AMD,It's 192Mb across 2 CCD so unless your game spills across both CCD and are both somehow used identically it will not affect anything,Neutral
AMD,Purely speculative guess: games that noticeably benefit from more cache see ~5% uplift.    Price will be $200 more than 9800X3D (another guess).,Neutral
AMD,The 2025 fashion: 0.1% and 1% paranoia,Neutral
AMD,"They're currently working on X4D, the tesseract-stacked cache packaging",Neutral
AMD,Could you kick up the 4d3d3d3?  4d3d3d3 Engaged.,Neutral
AMD,"This. This processor is intended for only three types of people:  1. Those running multithreaded scientific workloads (like climate modeling) which want all the fast cache they can get and are sometimes limited by the slower single thread performance (and cost) of Threadripper or Epyc. 2. Those with more money than sense who don't understand how multi-CCD Ryzen CPUs work and are easily manipulated by ""bigger number + X3D = better"" marketing. 3. The ""simulation gamer"" who only plays niche and highly CPU bound games that can scale with 16+ threads.  As soon as a game causes one CCD to reach across the IF to access code assets residing in the 2nd CCD's cache, the majority of the benefit of 3D cache vanishes.  But, maybe the 9950X3D was a testbed with a new hardware thread scheduling workaround that AMD ended up releasing because it was deemed successful. I doubt it, but I'd be thrilled to be wrong.",Neutral
AMD,You can now play two games at the same time.   You're welcome.,Positive
AMD,"Only benefit I see is if you have a use for 16 cores and you want the 2nd half to be more efficient compared to a regular 9950x3d. Also for 9950x3d users, often workloads that would benefit from vcache end up the cores without. Either way I expect the benefit to be very small.",Positive
AMD,"aren't there benchmarks out there where it shows the 3d cache really helps ray&path traced games? maybe what AMD has been cooking lately in that area has got something to do with it. not to mention we don't know the config of the next consoles, maybe it's time for a 16 cores PS6",Neutral
AMD,It will mean that those of us who have use cases that can be niced to run on a single CCD can leverage these CPUs to full effect. There was little reason to purchase them instead of a 9800 X3D because the second CCD was useless. It may be moot for the layperson though.,Neutral
AMD,"Yes, each of the two CCDs has the usual 32MB of L3 cache plus 64MB stacked below (above for Zen 3 and 4, below for Zen 5) for a total of 2x96MB.",Neutral
AMD,"Yes, each CCD has it's own cache",Neutral
AMD,"No, each has its 32 + 64 on a different stack",Neutral
AMD,you wont be missing out on much. theres not many games that are not affected by the ccd to ccd latency,Neutral
AMD,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q4 2025 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",Neutral
AMD,"Will be the same, no MMO uses more than 4 cores. All popular MMOs run on very old game engines.",Negative
AMD,probably not,Neutral
AMD,Hell no,Negative
AMD,Wait for zen 6,Neutral
AMD,Is there anything your 7950X3D can't do that you need it to?,Neutral
AMD,Also wondering. I got a 5700X3D but haven’t pulled the trigger yet on an AM5 build.,Neutral
AMD,Your cpu is already insanely good and the 9950x3d is not a whole lot better.,Positive
AMD,"I hope you realize that AMD releasing this now means Zen 6 X3D CPUs are not coming anytime soon.  You will have to wait for these to sell for months before the first Zen 6 chips arrive, then you will have wait months more for X3D SKUs to be launched.",Neutral
AMD,Wait for 11850x3d2 that will be Zen 5 compatibile,Neutral
AMD,why is he kind of correct though,Neutral
AMD,"Fun fact: during start up, your CPU actually use cache as ram(CAR). Dram is init after SEC phase.",Neutral
AMD,"Been a while since I've studied computer architecture but unless the cache is fully associative, which it isn't, then certain addresses with the same index bits will overlap with each other in the cache, and so certain accesses to cache will end up evicting other cache lines back to main memory, even if the cache has enough space to store all of them. So you would need some amount of DRAM at the very least. That's also not counting the fact that your CPU probably won't even boot without RAM and other technicalities.  On Linux on my 7800x3d I can check the cache details using *dmidecode -t cache* and the L3 is 16-way set-associative. Someone else should probably verify that what I'm saying is correct, but even in theory it isn't possible to run windows 98 entirely off of the L3 cache",Neutral
AMD,I assume most operating systems are written to expect there to be some RAM in the system. It would certainly be possible to create a Linux that could run without RAM. You may need a custom BIOS as well to boot the computer.,Neutral
AMD,This is why I have discord and the browser configured to run in the non-vcache cores on my 7950x3D.,Neutral
AMD,Thanks a lot I wasn’t aware of this video by them. Interesting though 10% is no small difference in 1% lows id say. I wonder what the difference between a 9800x3d and 9950x3d would be in that scenario where its literally a second ccd available,Positive
AMD,We need a test benchmarking running wsl with dev servers alongside games lol,Neutral
AMD,"I have a 5800x3d and I constantly run into issues with a game on one screen and streaming something on the 2nd.  Some streams run fine on the 2nd monitor when not full screen, but if both screens are full screen the entire system lags.  I would very willingly pay for a 16 core x3d processor if I knew it would improve my issues, but I never see anyone running these kinds of tests.",Negative
AMD,Ive found its due to hardware acceleration being on for your browser. Hardware acceleration uses the gpu for video and if you’re maxing out your graphics card it causes the video to stutter. Turing it off uses the cpu so as long as your cpu is not capped out it should be fine. I put my browser on my non v-cache cores and have no issues that way.,Neutral
AMD,"This is often due to differences in refresh rates. Believe it or not, a not so talked about thing is that synchronization and multiples of numbers matters too. 60 to 120 to 180.to.240 to 300 to 360 for example..you will stutter more with 60.and 144hz for example as they are not multiples and do not synch. The same can be said for polling rates too. This is more talked about among esports and competitive gaming and among hardcore audio and more latency based communities. And yes, video playback on a separate monitor still can cause stutters while gaming on another. Its the animations and movement of stuff open on the second monitor and thus another latency conversation too.",Neutral
AMD,I was having a similar stream stuttering issue on Firefox. Its been a while but I believe it was an issue with hardware acceleration and a mismatched resolution or refresh rate.  That second part doesnt apply to you but I had a 1440p 240hz and a 1080p 144hz monitor.   I havent had this issue in over a year before I switched browsers.,Neutral
AMD,You watch Youtube videos 90% of the time you're playing a game?  How do you focus on either? :/  Does the overlapping audio not drive you mad?,Negative
AMD,"For me, I have a 7800x3d and a 7900xtx on 32gb ram. I almost always have YouTube streaming something on my separate 4ktv while gaming at 7680*2160p (both at 120hz) and rarely get stutters. I've never been able to pin the stutters to anything in particular like loading into levels or anything, it just happens on and off but goes away very quickly. But overall pretty smooth. I would have figured it would be vram related if anything but with your 5090 that makes it even less likely. I've never adjusted any configurations for it either.",Neutral
AMD,"I used to get video playback stutters by the Spotofy client (lol) if it was open and visible next to a windowed game. It stopped after my latest upgrade though and I don't know if it was due to going to a X3D, or because I reinstalled Windows (on a new NVMe)",Neutral
AMD,Maybe im a super nerd but I've always had old laptops around that can do secondary stuff on without gumming up the gaming pc. Fixes the stutters right up.,Neutral
AMD,"Yeah. I think that it might something to do with resource allocation and maybe how Windows handles focused/unfocused windows? That's the best guess I have. That being said, I **do** have a solution that did fix it for me: if you have an iGPU (which you do, since you're on a 9800X3D--make sure to set the iGPU to enabled or auto in the BIOS), **plug your secondary monitor into the iGPU.**  I drove myself insane trying every fix under the sun because the issue was *so* goddamn annoying. Yes, hardware acceleration was the *first* thing that I turned off. Wasn't the refresh rate, since both of my monitors at the time were the exact same model, DDU didn't fix it, and I'd had the issue on 2 different builds over the span of like 5 years.  So I plugged secondary display into the motherboard to use the iGPU on my 7800X3D and haven't had the stuttering issue ever since. The thought process was like: ""well, if it's a resource/utilization thing, what if they both used a different GPU?. I only run games on my primary monitor, and I only use my secondary display for my browser, watching videos, Spotify, Discord, etc., so they don't *need* to be running off the same GPU. It's worth a try.""  Anyway, I'll get ahead of some things people might ask me about:  - Thermals: No significant/noticeable increase in temperature in everyday usage that I can't attribute to other causes, like measurement error, ambient temperature in my room, dust, etc. No thermal throttling issues, no issues hitting the advertised boost clock. My disclaimer is that I do have a modest (-20mV) undervolt running *now*, but my system was not undervolted until a while after I tried the iGPU thing.  - Performance: Negligible, if any, difference. I didn't really feel like running experiments after all this troubleshooting, to be honest.  - Power draw: Negligible.   - Jank when switching windows/mouse/etc. between monitors: Nothing noticeable.",Neutral
AMD,Ain't no way a large steam game with a lot of decompression uses 1-2%. Linus even had a video about it utilizing processors with more threads.,Negative
AMD,Yes but look at the video comparing a 6core vs 8core. It makes a difference in 1% lows when multitasking that was not there „just“ running the game and to me that matters. Hence why I wish gaming+some mentioned usual task would be tested as well. It’s not hard to repeat a discord chat and a specific video running for the duration of the benchmark just like they did 3yrs ago I think,Neutral
AMD,"Compared to the 9950x3d, u dont have to worry where u place ur game on (the 3d cores) since all cores have 3d, so thats nice.",Positive
AMD,bit faster but same,Neutral
AMD,"If by paranoia you mean actually useful metrics, then sure",Neutral
AMD,I heard the X4D cache actually sends data forward and backward in time.,Neutral
AMD,I’m waiting for 5D. I can’t legitimize upgrading every D.,Neutral
AMD,Can I see a hat wobble?,Neutral
AMD,"You could before with the non-vcache cores, it just wouldn't perform as well.",Neutral
AMD,"Yeah, i had a suspicion about that to be honest.",Neutral
AMD,"Now that you said, I guess it offers the convenience of not having to manually set core affinity to the 3D V-Cache CCD with Process Lasso, as both CCDs have it, or install the chipset driver that automatically does that.",Neutral
AMD,"Yeah wow fps is garbage in raids, but x3d significantly improves it.",Positive
AMD,Not having to worry about games running on the non-cache cores.,Neutral
AMD,I'd wait,Neutral
AMD,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
AMD,"With the exception of modern Zen CPUs, where the whole upbringing is done by the PSP...  Sadly, it doesn't work without RAM, 2x the cache seems to be the minimum needed, as the PSP also initializes memory encryption and other features before the entry address is loaded.",Neutral
AMD,"> Been a while since I've studied computer architecture but unless the cache is fully associative, which it isn't, then certain addresses with the same index bits will overlap with each other in the cache, and so certain accesses to cache will end up evicting other cache lines back to main memory, even if the cache has enough space to store all of them.  That only happens when RAM is larger than cache. If you don't have RAM and only use the cache then direct-mapped is good enough because there aren't any other addresses that can alias a cacheline!",Neutral
AMD,"I can verify you are correct and the other guy contradicting you probably didn't understand what you said. The cache info doesn't have the full address, just parts of it. The cache architecture would have to be completely redesigned to support a ramless mode. If this was the case it would be fully associative as you rightfully pointed out.  Source: What Every Programner Should Know About Memory (Ulrich Drepper)",Neutral
AMD,this makes me wonder if it's possible to do this while also load up a video game only in VRAM,Neutral
AMD,"Does 9950x3d2 have 2 CCDs?  I'm assuming, but also says `dual 3d-vcache`, so assuming both CCDs have their own dedicated cache, so theoretically shouldn't have to do weird tweaks anymore and everything will just run fast?",Neutral
AMD,Did u use process lasso?,Neutral
AMD,Run the game on one CCD and everything else on the other feelsgood,Positive
AMD,"Sounds like a hardware acceleration issue to me. Tinker around with the hardware acceleration settings in your browser, discord, game clients etc.",Neutral
AMD,Is hardware decoding enabled on your browser?,Neutral
AMD,That is almost certainly not a CPU issue,Negative
AMD,"Sounds more like a software issue since it only happens when putting the stream in full screen, and that doesn't make it heavier on resources. You won't solve the problem by throwing more cores at it.",Negative
AMD,"I run 3x 4k monitors at 144hz. Typically playing BF6 and afking on OSRS, discord open/multiple tabs/etc. Originally had the 7800X3D. I'd see a noticeable drop in 1% lows as I was pretty much maxing it out when doing the above. I bought an 9950X3D and 1% lows are much better/not seeing high peaks on CPU util. Extra 8 cores make a noticeable difference if you are multitasking. Just wish I would have waited for this new variant for the extra cache.",Neutral
AMD,How do u do the manual v vache thing,Neutral
AMD,"That's why I set my main monitor to 120Hz, even though it supports up to 165Hz. It's a multiple of common framerates like 24, 30, 40 and 60 Hz, and I can't notice much difference to 165Hz anyway.",Neutral
AMD,"Well 90% is an exaggeration. Really it depends on what I'm playing. If its something chill that doesn't require much focus I like to play a movie or something. Partly for background noise partly for entertainment if I'm grinding something repetitive. For example, if I was cargo hauling or mining in elite dangerous I'd be watching something but if I was playing Battlefield or maybe a story game like BG3 I wouldn't be watching anything cause I need to focus on the game. Other times I might be watching a tutorial for the game I'm playing.",Neutral
AMD,I'll try this on my 9950x3d and 1000/1000 connection when I get back home. Can't remember how much CPU power it uses. But iirc it's not more than perhaps 3%?  !remindme 5h,Neutral
AMD,You haven't had to worry about scheduling these CPUs for years now if ever.,Neutral
AMD,"For most games you still want all the threads to run on the same CCD, compared to the 9950X3D you just don't care which one.",Neutral
AMD,"But you still have to worry about the game using both CCDs at the same time.  As in, some threads on one CCD, some on the other. Most games will fit into one single CCD (8 cores) but if you have other things running you might accidentally see unwanted thread migration.   This is a really weird product honestly, that makes more sense for productivity apps that need cache rather than gamers.",Negative
AMD,"You are all silly. Just about lows you have ""knowledge""",Negative
AMD,Imagine your computer next year using all its cache to super charge your game this year… then Jan 1 you can’t use your computer because you’re feeding last years games.,Neutral
AMD,"Well, sure. You could do the same on a 1600x if you really wanted to. The experience would be less than stellar, though.",Neutral
AMD,"I wrote my own program in go that nices processes according to what I need and know is optimal for the programs I run. Same concept as process lasso, just much more effective because it does exactly what I need.",Neutral
AMD,"The exact same problem from your current CPU would exist on this hypothetical CPU, because a game running on both CCDs would be getting fucked by latency trying to communicate over the infinity fabric.",Negative
AMD,"Imagine the blazing fast speed the OS would run, if loaded to the L3! I bet something like tinycore would work, give the low ram req.",Neutral
AMD,Challenge accepted,Neutral
AMD,"... And I can verify that you need to reread your source.  Cache as ram is a normal thing that a lot of platforms support. Cache tags need to contain the entire address, or they could not be used to disambiguate accesses.  The way car works is that you turn off writeback, and choose to only use a contiguous linear range of addresses smaller than your largest cache. This cannot cause any conflicts.",Neutral
AMD,In theory it should be possible yes. You may need a custom graphics card driver.,Neutral
AMD,There are two CCDs and each has a V-cache tile,Neutral
AMD,"No because it causes a bug that locks up the entire system after hours, i just use a bat file to start certain programs like a browser and discord, never failed for a year + now, unlike Process Lasso.",Negative
AMD,"I went from a from a 5900x/5700xt without this issue to a 5800x3d/6700xt with it.  The 5800x3d/6700xt is much better overall, but I occasionally run into hiccups that I did not have with more cores and a weaker gpu.  Whatever the cause is, I would still appreciate benchmarks with things happening on a 2nd monitor to get a better idea of what is worth spending money on to maximize performance.  I mainly play strategy/4x games that push the cpu much harder than the gpu.",Neutral
AMD,"Which is why I have as of yet not just thrown money at the problem.  I would still like to know for sure.  Even it is an issue caused by software, does not mean hardware can't fix it.  I have read some people use core parking to keep gaming on the main ccd and everything on the 2nd monitor on the other one.  That could conceivably solve the problem. Or not.",Neutral
AMD,"I have a 7950x3d. 8 of the 16 cores have the 3d v-cache for gaming. The other 8 cores are just regular cores. I use an app called process lasso that I can use to manually force applications onto the gaming cores or the regular cores. I force my games onto the v-cache and all other apps like my browser, discord, and Spotify onto my regular cores. Sorry if its over explained I’m just not sure where your knowledge on this starts or ends.",Neutral
AMD,I will be messaging you in 5 hours on [**2025-12-26 18:56:04 UTC**](http://www.wolframalpha.com/input/?i=2025-12-26%2018:56:04%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Amd/comments/1pw23gu/amd_ryzen_9_9950x3d2_16core_cpu_with_192mb_l3/nw0vcwt/?context=3)  [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FAmd%2Fcomments%2F1pw23gu%2Famd_ryzen_9_9950x3d2_16core_cpu_with_192mb_l3%2Fnw0vcwt%2F%5D%0A%0ARemindMe%21%202025-12-26%2018%3A56%3A04%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201pw23gu)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,Neutral
AMD,No? U need process lasso,Neutral
AMD,"I am wondering if ""running the game on cores all on the same CCD"" might still require parking the cores​ of the other CCD via ""game bar"".  So, even if both CCDs have X3D, it might not help that ""one does not have to care which CCD the game will be running on"". The ​other CCD might still need to be parked via game bar during games to avoid the game running across CCDs.",Neutral
AMD,Ok have fun with a 60 average stutter fest,Neutral
AMD,"Isnt it still two cores X3D cores. I would assume they would still either run on CCD1 or CCD0 most of the time, the time they spill over to both and the impact on performance, well lets wait for reviews if the cpu ever comes to market.",Neutral
AMD,"Tiles aside, it's a chiplet, this. ^^  TomAto tomaato though they are basically the same thing.",Neutral
AMD,"it's not a ""tile"". tiling is what Intel calls their multiple-chip designs, AMD called them ""chiplets"" for their GPUs. the 3d cache is stacked on top of the same die, it's not a separate tile like intel's CPU tiles or AMDs gpu chiplets",Neutral
AMD,Interesting. I have never heard about this bug. Could you please tell me more about the bug?,Positive
AMD,"start """" /affinity 1 /LOW ""C:\Games\something.exe""  Something like that?",Neutral
AMD,It doesn't cause that bug by default because I use it in background for years to launch games on 3d cores and it works. What's that bug again? You can probably fix whatever is the real problem,Negative
AMD,"Never had process Lasso fail me, been using it for 7 years now.",Neutral
AMD,"You could try splitting 4 cores on the game and 4 cores on the background stuff to see if anything changes, but ofc try a lighter game that can run fine on just 4 cores. Or split 6 - 2 cores.  Might also be a GPU related issue, maybe the driver doesn't handle multiple full screen apps correctly. Are you using gsync/freesync?",Neutral
AMD,Do you even own the hardware or are you just repeating stuff you read on reddit 2+ years ago?,Negative
AMD,No you don’t. That hasn’t been necessary for years now.,Negative
AMD,"As long as you install AMD's chipset drivers, and have an updated version of Windows, you don't need to do anything regarding the cores being used for games. The Windows scheduler handles that all properly, and has for a while.   Source: Literally running a 9800X3D rn",Neutral
AMD,You poor soul...,Neutral
AMD,"Stacked under the cores on the 9000 series, that's how they got higher clock speeds out of the x3d chips",Neutral
AMD,"You can give the information you wanted to provide without having to pull a ""Um akshually"". Just because a multi-billion dollar tech company has a process called 'tiling' doesn't change what the word means in the broader societal lexicon. It's a thin rectangular slab of L3. Thus it's a tile.",Neutral
AMD,"After closing games (like a 2-3 games closed later, there was no consistency/pattern to be found), with delayed CPU affinity assignment (usually 15 seconds, because some games have an issue with instantly assigning core affinities at launch, example, Baldur's Gate 3, if you start it with a specific CCD affinity manually, it will not be able to have produce any sound for some reason), the PC would lock up completely when that happened, and it only happened with if Process Lasso tried to do the core assignment (the amount of delay i set didn't seem to matter, i tried 5 minutes tops), the PC is perfectly stable otherwise for years now (i build it, i tested it, with so many tools and configured for 24/7 stability). Haven't used Process Lasso since then, due to afformentioned issue.  Edit: I forgot to mention that, i always had and updated the AMD cache driver and Driver set to BIOS (Auto produced the same exact behavior).",Negative
AMD,"This places Discord on the second CCD (frequency CCD if you want it on the other CCD, just change the FFFF0000 to 0000FFFF), there are better ways to do this affinity handling but it's a decent one if you want something simple.  cmd.exe /c start ""Discord"" /affinity FFFF0000 ""C:\Users\User\AppData\Local\Discord\app-1.0.9218\Discord.exe""  The only issue with this is due to Discord changing the versions on the file name and i have to change the number in the end (haven't bothered finding a workaround, probably exists though), but for other programs i never had to do this.  Edit: Well, a better workaround  @echo off set ""discordPath=%LocalAppData%\Discord"" set ""latestApp=""  for /f ""delims="" %%i in ('dir /b /ad /on ""%discordPath%\app-*"" 2^>nul') do (     set ""latestApp=%%i"" )  start ""Discord"" /affinity FFFF0000 ""%discordPath%\%latestApp%\Discord.exe""",Neutral
AMD,"Added a workaround in my other reply, cheers.",Neutral
AMD,U sound frustrated,Neutral
AMD,"9800X3D doesn't have two CCDs, 7950X3D user here and when processes jump between CCDs it creates latency issues which is why project lasso is a thing. I have learned about this recently, I wasn't using my CPU to it's full potential.",Negative
AMD,"I was talking about the dual ccd variants, im pretty sure i read that u have to uninstall game bar because otherwise amd's software that decides where to put the load on the cores (i forgot its name) doesnt work properly, but that means u lose game mode which is pretty bad. So yeah, u have to use process lasso. I dont own the cpu but either way its a good practice to do manually than relying on some software that god knows what its doing.",Negative
AMD,Nope. They're chiplets.,Neutral
AMD,Ohh I see. Is it the same case with 9950x3d?,Neutral
AMD,Thank you kindly,Positive
AMD,"batch scripts are a scourge, a blight on humanity",Neutral
AMD,I hate it when people spread misinformation online.,Negative
AMD,"I have a 7950x3d… I can’t remember the last time I had trouble gaming due to the unbalanced CCDs. Everything just works now without much thought. When it first came out yeah, there were issues where games tried to use both CCDs with threads hoooing between CCdS and had wild stuttering and performance issues.",Negative
AMD,I'm just going to call it rectangular thinking rock,Neutral
AMD,"Are we talking about CCDs or the L3 layer? One would constitute a chiplet the other doesn't have a ""chip"" it's just a slab of storage medium. Unless we're just calling anything square shaped that ends up on the die chiplets now",Neutral
AMD,"I don't own that CPU so i can't test it for that specific issue to be honest, someone with it might be interested to do it though, maybe ask around (i would like to know too by the way), cheers!",Neutral
AMD,They can be but it depends on a lot of things.,Neutral
AMD,blight.bat,Neutral
AMD,Me too,Neutral
AMD,"No im talking about amd software, the 3d v cache optimizer or smth like that, the  same one that decides to place ur game on the 3d ccd and the other stuff on the non-3d ccd. Last i read it wasnt very good and it had big conflicts with game bar / game mode. So its best to not use it and just use process lasso. No idea how it's on the 9950x3d, but thats why people recommended avoiding the 7950x3d for gaming and just getting the 7800x3d. Now with the 9th gen it seems like most of these issues are fixed from what ive ""heard"".",Negative
AMD,"As long as you don't call it a tile, we're good.",Neutral
AMD,Lol,Neutral
AMD,"Yes, that’s exactly what I’m talking about. For x3d chips you rarely have to worry about it anymore. The chipset driver takes care of it, including the older generations. It basically modifies the windows scheduler which is what lasso did.",Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,"I don’t understand why they won’t make one with 6 cores and 16cus with no npu for gaming handhelds. We don’t need 12 freaking cores in our handhelds, it’s actually detrimental considering it pulls power from the tdp starved gpu.",Negative
AMD,Rebadged ryzen 5 340.,Neutral
AMD,"This doesn't deserve the AI7 naming with 2+4 cpu cores and 4 cu graphics.  I think this is the uncut Krackan Point 2 die. If it's Gorgon Point, then they could have given it 3+3 cores, just like the 340 predecessor, and not make it worse than it.",Negative
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,Soo rdna3.5? How much the improvement from 740m?,Neutral
AMD,a 6/16 with RDNA 4 (for the FSR4) would make a fantastic successor Steam Deck 2,Neutral
AMD,"Isn't this the Z2E? It's an 8 core IIRC, but 5/8 are Zen 5c cores (granted, it's cut down from Strix Point).",Neutral
AMD,"Not a ton, but measurable. Per-CU rdna3.5 isn't much better in typical gaming, but it is better. Check out 880M vs 780M benchmarks to see the top end of each.",Positive
AMD,They will def skip rdna4 for some stupid reason.  I get it’s a stop gap before rdna 5…but it’s also a 2 year gap.  Maybe longer now with memory prices.  I think steam deck 2 will be 6 zen 6 cores and 12-16 rdna 5 CUs in 2027.  But again…memory :/,Negative
AMD,In 2028.,Neutral
AMD,"Z2E is rdna 3.5, which is just a refined rdna 3.  Rdna 4 is significantly more efficient than rdna 3.  Example: the 9070xt is a 64CU chip that outperforms the 84cu 7900xt at similar TDP (~300w)  If that holds true at lower tdp’s, then an RDNA 4 16cu chip would perform around 25% better than the Z2E, which is already basically 80% better than the steamdecks apu.  So a hypothetical rdna4+ 6core 16cu handheld would have >2x performance to the OG steamdeck.",Positive
AMD,"AFAIK Valve said they're not going to make Steam Deck 2 until there's a very noticeable advance in APU technology. IIRC even +50% performance is not enough for them which is a shame on one hand, but on the other, I totally get it.",Negative
AMD,My comment was not about graphics.,Neutral
AMD,Rdna 5 would be well beyond 50% considering rdna 4 would deliver almost 50% uplift.,Neutral
AMD,"You can't push for devs to make console-like optimization for your device if you keep releasing new devices everytime there is +10% power available !  SD2 should be like Switch 2 vs Switch 1, something that really feels like a different generation, that will run games that the first one can't even hope of launching.",Neutral
AMD,"Isn't it like there would be no RDNA 5? AMD is planning to merge RDNA (consumer branch) with CDNA (professional branch) into UDNA. Nonetheless, the rumors suggest that APU based on Zen 6 (which would be the next generation) could power the Steam Deck 2 with its launch in 2028 ([article](https://www.linuxjournal.com/content/steam-deck-2-rumors-ignite-new-era-linux-gaming)). Fingers crossed for Valve",Neutral
AMD,">You can't push for devs to make console-like optimization for your device if you keep releasing new devices everytime there is +10% power available !  The problem with Steam Deck is that it's just a PC packed into a handheld. It uses the same games library as your ""normal"" PC, someone's else ""normal"" PC, my ""normal"" PC, so there's no chance we'll get the console-like optimization since it's impossible. PCs are just too different.   Although I totally agree that there's no point in releasing a new handheld every year or two because there's a new APU with +15% more performance.",Negative
AMD,I read that as zen6 being 2028 and got a little concerned. I’m planning to upgrade to zen6 when “AM6” drops and that timeline was putting it at 2032 lol.,Neutral
AMD,"Yes it will likely not be called rdna 5, rumors have said udna for a while.  I think 2028 is way too late.  Next gen consoles and PlayStations portable will be 2027 which will all be zen 6 and udna.  Unless memory pricing pushes that back.",Neutral
AMD,"Devs can create specific settings to target 30/40/60 fps, just like on console, by tweaking all graphic parameters, NPC density and various effects.  It works on Steam Deck because everyone using one have the same hardware and same small screen so devs can adjust based on that.",Neutral
AMD,"I think AM6 will be revealed in late 2027 or in 2028 as it'll be around 5 years since AM5 release, so it's at least 2 years of waiting.  I also wait for AM6 to upgrade my PC, but if the current situation will stay with us for longer (or, hopefully not, become even worse) mg rig will have to stay with me for a while.",Neutral
AMD,I’m rocking an AM5 platform now and my plan is to upgrade when they EoL the socket to an end stage processor.,Positive
AMD,GCN 1.0/1.1 - HD7000 and R7/R9 200,Neutral
AMD,"Wow, really old huh...",Positive
AMD,Alternative title: Linux's degraded performance on old AMD Radeon GPUs for years.,Neutral
AMD,"So is this bringing it in line with Windows performance or improving beyond?  EDIT: Feel like this is a legitimate question, no? Curious if this is the culmination of a bunch of improvements that were just never applied to these GPUs on Linux (since it was using the old driver) that Windows already saw or if this is just a straight up performance boost - I have an old R7 260X that, granted, I still probably don't have a use for besides it being a backup, but it's an interesting development.",Neutral
AMD,Time to dig out my HD 7970 GHz Edition for testing! (Card was re-released as the slower 280X),Neutral
AMD,Thx Gabe?,Neutral
AMD,Good example how Linux support for older hardware is way longer term than Windows.,Positive
AMD,The biggest benefit here is better compatibility with wayland.  The legacy radeon driver was buggy with newer software stacks.,Positive
AMD,"Have you comment how much of a troll that thumbnail - let's put the highly sensitive, expensive electronic equipment in a Christmas tree.",Negative
AMD,"Considering ti was tested on modern cpus it's still slower than Windows, sadly.",Negative
AMD,I just want kernel level anticheat as an option for Linux.  When that happens I’ll switch.,Neutral
AMD,"And in the usual news, my 30% decrease of caring for linux has happened again. Still 0%.",Negative
AMD,Cannot play anti cheat kernel games such as EA FC or any competitive games = useless.,Negative
AMD,"I had an R9 270X back in circa 2013. Good ol’ days man! Hearing these words like GCN, R9 brings back memories :’)",Positive
AMD,There is a larger gap between today and when GCN1.0 launched than between when GCN 1.0 launched and the first ever Radeon launched.,Neutral
AMD,"Yes, it could be argued amdgpu shoud've been the default for a *long* time. It was default on gaming distros for a reason.",Neutral
AMD,"I've always been using AMD GPU with my older cards, as the Arch Wiki recommended...",Neutral
AMD,"Since the boost is just from switching to the very same driver that all the newer Radeon GPUs have already been using I’d expect the Windows comparison to be similar to what you’d see with those other GPUs.  In fact depending on the distro you might never see the “improvement” here if said distro has already force enabled the new driver (plus anyone savvy enough probably did already do so if the distro hasn’t done it), for pure gaming purposes the new driver has already been working well for quite some time, the missing features stopping it from being made default for everyone was generally more niche stuff like analog video output support which the old driver did have.",Neutral
AMD,Windows has no support for older GPUs to begin with. This is improving Linux support long past what Windows support provides.,Negative
AMD,I had look at benchmarks of the same games and it feels like is still up to half as slow,Negative
AMD,"Does the last windows driver for these cards support vulkan? if not, that's a nice benefit of this over the windows driver.",Positive
AMD,check windows sub and look for updates fpa drops almost every year🤣,Neutral
AMD,"Only 3 and a half years after the last Windows driver version for these cards and Linux has *almost* caught up to the same performance. Yeah, great example.",Positive
AMD,If that happens companies will make use of it. Without it those who release on Linux will have to bend the knee and release their games without kernel level access.  I'd rather not play a game than let it touch my kernel level.,Negative
AMD,"No, that shit doesn’t belong in either Linux or Windows",Negative
AMD,Kernel level anticheat is something the game companies choose to do not something the Linux ecosystem needs to implement. Eac for example works on Linux as an anticheat officially . I have over 4k games in my steam account and none of them need kernel level anticheat. I can play all of them under linux. That is more than enough without giving kernel level access to a company like riot(for example) to have literally more privileges than me on my own pc at all times. People have accepted some extremely invasive measures from companies to play their multiplayer games. Cheating sucks I get it but the way they have implemented the solution is unacceptable imho,Neutral
AMD,"And when that happens something else will be a hard stop for you that will be ""and then I'll switch""  let's be serious for a moment, you can just say you won't switch. It's not bad inherently bad, it's only bad if you don't have a valid reason which this excuse gives you until the next one",Negative
AMD,"You just want to run rootkits on your machine, huh? Better you never use Linux than we get that garbage implemented.",Neutral
AMD,"I don’t know why you get downvoted, you said OPTION. This is keeping a lot of gamers from switching to Linux.",Negative
AMD,this will not happening   and please dont switch over something like this,Negative
AMD,Don't you have a Battlefield 6 match to lose or something?,Neutral
AMD,Why  don't you care about Linux?,Negative
AMD,It's your fault for playing trash games to begin with.,Neutral
AMD,Ive been trying to remember what was the feature they had for battlefield 4 that improved the fps by like 40%. Could only do it on amd cards though,Neutral
AMD,Stop,Neutral
AMD,"12 vs. 13 years, checks out. Why do you mention this though?",Neutral
AMD,Yeah but the true insanity is that the 7970ghz has the same speed as the arc380,Neutral
AMD,It didn't support VGA output until recently,Neutral
AMD,Don’t know about Windows 11 but on 10 you can run 20 year old GPU e.g. 8800 GTX: https://m.youtube.com/watch?v=Gtikrie4TKY&t=275  That’s old enough for me.,Neutral
AMD,"This is my biggest irritation with the Linux gaming enthusiasts whenever they talk about benchmarks being so good on Linux.  They never compare them to Windows unless it's close or in Linux's favor, nor do they ever address the GPUs that 75% of steam users have, nvidia. Because the 1% lows there are so bad compared to Windows and nvidia doesn't give a shit to fix the problem they've known about for over a year now.",Negative
AMD,"You can't use it on Windows at all, so your performance there is 0% and forever will be, while these cards will be usable on Linux for a long time still.  It's amazing such old cards are even being worked on on Linux. New AMD cards don't have any performance gaps in fact Linux graphics stack is way higher quality than Windows one since not only AMD is working on it.",Positive
AMD,"> I'd rather not play a game than let it touch my kernel level.  Unfortunately, as someone who's played league for 15 years, that really isnt an option. I took a 5 year break, checked it out again last year and now i play regularly again. Not as much as i used to, i play a few games a day now as opposed to like 5-6 fron S1 to S9, so roughly half the playtime, but i have easily over 10k hrs in this game, likely closer to 15k. And i play ranked only, so since im not a casual i cant just ditch the game like that. Over the 5 yrs i played many other competitive games, i played cs go, fortnite, wow, rocket league, but none of em could scratch the itch. After coming back to league, i realized what i was missing. I was missing league. Theres just no other game out there that feels this good to play. Especially since im very good at it (I was Master in S8 which is like GM now due to inflation and cuz GM didnt exist back then). Turns out, being REALLY good at a game makes it 10x more fun when u're playing it competitively, who would have thought! Ofc, im not gonna be a pro player or anything like that, but it doesnt mean i cant feel satisfaction from my 3 ranked games a day and feel the pleasure of improving and watching my rank grow. None of the other games i played felt anywhere near as good. In cs go i got master guardian but thats kinda mid. In rocket league i saw the dudes flying and i was like, nah no way i can learn that without another 10k hours investment, same for dota. Fortnite no build was cool, i employed the hide in a cellar afk strategy and was able to reach Unreal, the highest rank, averaging 0.11 kills per game. I quit cuz it was boring and after watching pros like Prospering i realized this guy can do 40 kill games, aint no way i will ever be as good as him at this game. Wow was cool and i nearly got the 0.1% title in m+, fell barely short, was doin 14s and 15s as a tank. However, these 3 months were miserable, everything was a chore, its like u gotta wash the dishes and clean the toilet in a video game, dailies weeklies rep farm, alt chars grinding, raids and vault slots to rng the bis trinkets etc. And in the end it didnt even feel that good, honestly wow is such a slow paced game its kinda boring. And yeah, i realized there doesnt exist another game like league that makes me happy. That, and nvidia drivers being complete poop means linux is not an option at all for me, and most other competitive gamers out there.",Negative
AMD,"The problem is that it does exist in current reality, and I can't play with my friends unless I use windows.",Negative
AMD,"Yeah it does, otherwise ur games will be full of cheaters, as opposed to one occasionally here or there.",Negative
AMD,yup unfortunately i still have a dual boot system with windows so i can play counterstrike on faceit (with their anticheat) its the only reason i still have windows on my PC honestly.... like there is literally no other reason to have windows now.,Negative
AMD,"Sorry to break it to u but EAC is worthless as an anticheat. The only anticheat that successfully defeats external DMA devices is Vanguard, because it runs since boot and is incredibly strict, requiring strict security features in ur bios to be on and working. Ive successfully cheated in eac games many times just to test how easy it is, and oh boy, is it easy, as long as u got money to pay. Ofc, u do get banned eventually, but in the short term EAC doesnt stop u from even INJECTING into the game in some cases (like dead by deadlight), it is actually so bad lol. Vanguard is the only good anticheat on the planet.",Negative
AMD,"No, it’s literally the fact I can’t play Battlefield that makes me not install Linux right now.  I’d just want the option to play it on a Linux distro, haven’t had Linux installed for ~15 years; but Windows right now is horrible.",Negative
AMD,"I don’t see why it would matter. I simply want the option to install it, doesn’t mean anyone has to.",Neutral
AMD,we don't want anything closed source by companies inside the kernel,Negative
AMD,"It is what it is, not like I care about those Reddit votes!  I’d just want it to be an option, doesn’t mean anyone is forced to install it; but it clearly hit a nerve with some.",Negative
AMD,"I disagree - this will happen sooner or later.  SteamOS is trying to make Linux mainstream, and mainstream pc gaming includes triple A games.   Also I don’t understand this mentality, please everyone try out Linux except if you want something I don’t want. I mean. What?  You did read AS AN OPTION right?",Negative
AMD,"Lol, I mean every game that requires kernel AC isn't a trash game. It's unfortunate but the solution to this problem shouldn't be ""just don't enjoy those games lul""",Negative
AMD,one gap is larger,Neutral
AMD,8800 isn't 20 years old  *Yet*,Neutral
AMD,The instrumentation software is different between both OS so there is a need to someone like GamersNexus to validate both sets of capture tools to a deterministic statistical range;,Neutral
AMD,"> You can't use it on Windows at all  TIL my 7970 hasn’t been working in Windows all these years, who knew!",Negative
AMD,"Hey, I have a fun fact: You don't have to be an obnoxious prick.  Is it amazing that these old cards have active support? Hell yea.  Can you run em under modern windows? Also yes. Sure, drivers work just fine. Just because it's not updated, doesn't mean it doesn't work.  Hell, even my NVS5400M is working on Windows Server 2025. And you *can* play games. As long as it supports the required API, and Shader Model and such.    It's just that.. also yes, if windows does change something and old drivers stop working, then yeah. Pain. But it hasn't happened yet.",Positive
AMD,That's fine. Perhaps Linux isn't for you. Allowing additional third party software access to the kernel level is a security risk. It's silly to introduce such risks due to a couple of gaming companies demanding it for the sole purpose of anti-cheat.,Negative
AMD,"I have a strict rule: Never judge other people's game choices, respect everyone's right to play what they want and spend their time on the activities that make them happy...  ... it's a very strict rule that I will still **absolutely** break when it comes to the gross cesspool that is LoL. It's like the smoking of video games. You should quit or seek help or something.",Neutral
AMD,"I would rather have cheaters, than give them access to my kernel",Negative
AMD,"Yeah ive thought about that but unfortunately im too much of a mixed usage guy, i launch game, alt tab to discord, check smth in the browser, watch some twitch, back to the game etc. So when would i ever boot into linux when im fiddling with a game the whole time im on the pc?",Negative
AMD,It’s your choice to accept their rootkit on your system. As I said I prefer my privacy over their virus. You do what you prefer I am not going to tell you otherwise everyone has their own priorities. I just don’t trust them simple as that. Neither riot or ea,Negative
AMD,Then someone should maintain a fork of the kernel with the garbage implemented. It should never exist on mainline Linux.,Neutral
AMD,"You are right it’s ONLY 19 years old, my bad.   That indeed makes it fairly modern graphic card ;)",Negative
AMD,"At least for several years, yeah. Good luck running it on any recent Windows today, let alone playing games on it. While Linux users can.  Figure out a basic fact - Linux has way longer support for hardware compared to Windows.",Negative
AMD,"> if windows does change something and old drivers stop working, then yeah  Exactly the point. If you are a fan of using drivers as a roulette - have fun. I prefer normal support which Linux provides.",Neutral
AMD,"I think league is not different than the other games like Marvel Rivals. U feel amazing when u win (dopamine rush), and u feel terrible when u lose. Altho, in league it feels extra miserable. Like in cs or rivals, u can be 1v3 and u might just clutch it, even with a deagle u can get some good headshots and win the round. But in league, if ur teammates were feeding, well now that 1v3 is the enemies also having 2 items on top of u, meaning they are like a tank and u're like a kitten to them, making it impossible to win. So u lose and ur whole game experience is getting trashed around because someone else fed the enemies. Even when i came back to league and i initially didnt care about my rank cuz i was just relearning the game after 5 yrs of not playing, it felt sooo miserable to lose. I didnt care about my LP at all cuz i knew i was rusty as fuck, and it still felt so bad to lose cuz the entire experience that u spend there feels like u're locked in like a hostage or smth. And yeah i main jungle and get told to kill myself and flamed every game (with creative ways like talon E off a building cuz the chat punishments are unreal in 2025), but ive grown such thick skin over the years that it doesnt bother me at all anymore, but yeah definitely the most toxic of all the mainstream games, except maybe for dbd, tho in the last few yrs bhvr introduced anti camping and anti tunneling mechanics so its not that bad. But yeah asides from that league is really not as bad as people make it seem to be, my only big complaint is that people dont surrender early enough. Im in diamond so unlike low elo they do surrender a lot, but still not quite as much as they should, but i also mainly play late game scaling champs so it doesnt bother me as much cuz my teammates wanna keep me hostage by not surrendering (cuz they still think we can win, somehow) then good for me cuz if any1s gonna carry its gonna be me on my monster scaling champ, tho realistically once ure far behind its prob like 10% chance to win, not worth dragging a 20 min game out for another 10 mins to slowly suffocate to death. Enemies in diamond are good at ending games but not amazing like master+ are, so they generally manage to end but it takes a bit more time which makes it more frustrating cuz like they have a 12k gold lead, mess around a lil bit, give u 4k gold back but they still have 8k lead and and 3 drakes so they end anyway with baron/soul at like 30 mins. Basically the enemies throw just enough to give 2 of ur 4 teammates enough hope to press No in the surrender vote, but not enough to actually come back (usually). But yeah i wouldnt say league is a bad game, it teaches u patience and perseverence ESPECIALLY if u main jungle cuz every1 always blames u all the time. U learn quickly whose opinion and words to ignore and how to read people's emotional wellbeing. The jungler is the leader of the team and he's the one who makes the calls on the map, like calling ur support to roam to voidgrubs, telling ur mid laner to push and recall 30 secs before the spawn timer so he can arrive full hp to the fight and so on. If u play mechanically easy champs (so not leesin or nidalee) u can carry pretty much exclusively by using ur brain even if u dont have great hands (aka mechanics), u cant really do that in most other games, like i was great with nades and setups in cs go but my aim sucks so much i always lose a 1v1 fight even in gold, honestly amazing i got to master guardian like that. So yeah, u should try league too, it can be fun if u know how to play around the game's strengths and weaknesses.",Negative
AMD,"Multiplayer competitive games are unplayable when every game is filled with cheaters, the competitive integrity is completely gone. I hate it too btw, but i understand it's a necessity, which is why im stuck with windows, that and nvidia drivers being absolute ass on linux.",Negative
AMD,"I don’t understand the intense dislike for something that would be completely optional and chosen by the user.  I guess Linux isn’t about personal choice. From what I can remember you have to actually agree to let shit install on UNIX, so for those who wouldn’t want to give kernel access to anything you simply don’t let it.",Negative
AMD,Been working perfectly fine in Win11 ever since I moved on from 10.,Positive
AMD,"Dotes is part of what you said about liking league but even more. On the simpler heroes, the game is even more macro focused with less mechanics due to the lack of skillshots and not needing to master kiting due to turnrate. Dota also has more dynamic itemisation and heroes are genuinely alot more flexible and less pigeon-holed into a playstyle.   If you don't care too much about mechanical skill like you said, dota has so many ways to strategise and outsmart with mechanics like smoke, the waygate and the insane abilities of some heroes allowing for crazy maneuvers like underlord ult being able to create a portal. No ff button is the main downer compared to league. I get why you dropped it though because you said you enjoyed league 10× more because you were good at it, whereas you would have to grind to get good in dota, which is thousands of hours.",Positive
AMD,"Someone can write it and you can install it, I guess. Keeping up with internal kernel changes will be a tall order though.",Neutral
AMD,Not sure what you call fine if [AMD doesn't provide support for it](https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-hd/radeon-hd-7000-series/amd-radeon-hd-7970.html). Which is what I'm talking about. Unless you can show a source that they still support it and provide drivers?,Negative
AMD,"Yeah exactly, its not that dota 2 is a bad game, i just realized it would take forever to be even half decent at it. In my 20 hrs playing i also noticed theres no role selector when u queue (i dont wanna play roles i dont like) and most importantly, there is no jungle role. I mean i could go jungle but id prob be flamed from the get go by the top laner (pos x, i forgot the number) since hed be 1v2, and from what i read jungling in dota 2 isnt even good at high level. I remember the patch, smth like 7.34? Or 7.14? Basically right after they nerfed that medusa character, they nerfed her ability so she cant jungle well. Its like every time a hero is good in the jungle at high level valve nerfs him so u have to play 2v2 in the top lane (or was it bot lane). Anyway, im too far down the rabit hole with league and i like playing it cuz i feel like i have a lot of impact as a jungler, it is by far the most impactful role in soloq so very often when i lose a game i know its my fault cuz i didnt play better or cuz i fucked up. Like yeah top can go 0/5 but if i had ganked bot better they would be 5/0 and it would all equal out, but maybe my pathing was bad or my gank timing was a bit off so we didnt get kills bot and then they had a fed top and we had fuck all. Also instant queue times and if u queue as jungle/mid u always get jungle. Plus as the jungler being the leader od the team, most people listen to u. Some dont but if they dont listen to u, they certainly wouldnt if u were playing any other role either.",Negative
AMD,"Which is why I’m eyeing SteamOS.   I wouldn’t mind a swap from Windows, but I want to be able to play the games that I play and not dual boot.  Haven’t bothered with dual booting in 15 years, and then it was MacOS and Windows.",Positive
AMD,The Windows 10 driver installs fine under 11.,Positive
AMD,"If AMD doesn't list it, I wouldn't consider it fine. May be you can gamble and it would work, but that's not called supported, especially since driver isn't even open source which means that no one but AMD can bring it up to date. Want to try it with Windows 12 when it will come out?  Which goes back to my point. Linux hardware support is way longer term than Windows.",Negative
AMD,"By your own source, AMD hasn’t provided a Linux driver since 2015. Guess they don’t work for you either, and the article in the OP is a total fabrication.",Negative
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,"Inb4 Pcmasterrace’d comments here, it’s extremely important for volume and mass adoption to be offered by mainstream (dell hp Lenovo) prebuilts.",Neutral
AMD,"The irony of putting a factory overclocked 9800X3D into an Alienware PC (which will end up either underclocking it or hit a temp limit, dropping clocks).",Negative
AMD,"Honestly, when it comes to gaming, AMD doesn't need to pump out new CPUs, they need a new king GPU.",Neutral
AMD,So what we have now is Dell stating they are going to offer the 9850X3D CPU in their prebuilts. That plus the page for the 9850X3D being on AMD's website for a short time means that it is almost certain that the 9850X3D is real.  The article also states that Dell has no news on the 9950X3D2. This reinforces my belief that I do not think that the 9950X3D2 is real. The benchmarks that were seen and reported on could have easily been faked just like the fake 9700X3D that some Redditors made. There has also been no 9950X3D2 AMD website page leaked like what happened to the 9850X3D.,Neutral
AMD,"Dell, ewwwww",Neutral
AMD,"> it’s extremely important for volume and mass adoption to be offered by mainstream  Yeah for regular consumer tier chips but this is literally a binned 9800x3d, you don't need ""mass adoption"" for that",Neutral
AMD,I'm going to get flamed for this but it does seem they've solved many of their cooling issues with their new cases.,Negative
AMD,"Just one of my gripes about prebuilt desktop PCs from big OEMs like Dell, HP and Lenovo (including their gaming brands), they more often than not gimp some component.",Neutral
AMD,The customers buying these kind of pcs wouldnt notice anyway,Neutral
AMD,Old school overclocker. Good fuckin reply. I'm tired of how OEMs do this crap. Vote this shit up.,Negative
AMD,Bumping the frequency of an existent cpu is obviously vastly easier.,Neutral
AMD,![gif](giphy|zbzNUbpFnlw8E),Neutral
AMD,"They have one, it’s called rx580",Neutral
AMD,Not just that specific sku but the whole platform,Neutral
AMD,"I do recall GN's last Alienware PC video, giving the Alienware PC the ""it's better than Dell"" award. So, maybe.",Positive
AMD,It’s pretty awesome to hear they solved such a difficult problem. I hope other case makers can follow in their footsteps,Positive
AMD,"Top components (IMO) these companies tend to cut corners with:  - Motherboards  - Power Supply Units  - CPU cooler  - RAM  Cheap motherboards tend to have cheap power delivery, resulting in overheating VRM, which results in CPU throttling.  Cheap PSUs just mean that they typically cannot handle higher power demanding components (like a future GPU upgrade).  Cheap CPU coolers mean CPUs run at higher temps, resulting in lower clocks, resulting in worse performance.  Cheap RAM results in lower speed or higher latency RAM, resulting in worse RAM and CPU performance.",Negative
AMD,I always hated how some use custom components like psu making upgrading difficult or not possible.,Neutral
AMD,"Just because a customer might not notice doesn't make it right.  Maybe companies should start pulling cellulose in flour; customers won't notice.  Maybe we should start disabling 1/4 cylinders on brand new cars; customers won't notice.  Or maybe, we shouldn't allow for OEMs to fuck over consumers by cutting corners so executives can pocket larger bonuses.",Negative
AMD,:),Neutral
AMD,yes but this is specifically about the 9850x3d,Neutral
AMD,I watched a video where they toured the engineering department and described all the changes they made to the new case and it seemed like they had a pretty solid grasp of it and made some good choices.  I'd never buy one but I'm less likely to recommend against one for somebody else.,Positive
AMD,Which is hilarious bc Alienware is owned by Dell.,Neutral
AMD,It was never a difficult problem to solve. It was just them being cheap and not caring.,Negative
AMD,Well now it will only be low quality ram as cheap doesn’t exist!,Negative
AMD,"Mainstream pre-built are always going to focus on reliability and profitability and ease of manufacturing compared to PCs people build themselves or have built on spec.   Computers built on an assembly line are always going to have to account for that.   Compared to the old proprietary pre-built systems of my youth, people have no idea how good they have it rn.   Margins are relatively small on PCs. It’s not as easy a business to do well at-scape as people think.   There’s a reason gaming PC’s back in the day from boutique manufacturers like Falcon Northwest were prohibitively expensive.",Neutral
AMD,"> Or maybe, we shouldn't allow for OEMs to fuck over consumers by cutting corners so executives can pocket larger bonuses.  Yes, I fully agree with your statement but also, using a binned chip in a box that will be for people who buy prebuilts is still silly",Negative
AMD,That's.... That's the point of the joke.,Neutral
AMD,How much more obvious does sarcasm have to be for redditors,Neutral
AMD,"Lol, I guess I am fortunate enough to have forgotten about that.",Neutral
AMD,"There are still boutique manufacturers today. Though, unless they are doing hardline water cooling and lots of customization, their prices are still difficult to justify.  Otherwise, OEMs are still utilizing proprietary parts. Though, in the last couple years, this has become less of a problem. IMO, no proprietary parts should be used in OEM PCs, unless they're customized to be SFF or something unique. There is good reason almost everything is standardized with PCs.",Neutral
AMD,Perhaps you're not as obvious as you think you are?,Neutral
AMD,lol. Their prices are not “hard to justify.”  The margins for smaller manufacturers are paper thin on components. Then you have overhead with rent and employees and utilities. You need customer service and support. You have warranties you’re supplying your customers.   People don’t realize warranties on OEM components are not the same as retail components.   Anyone who’s actually built computers for clients and tried to start a business doing so knows how much of a shit show it is to deal with and how little profit there is and how much work it takes to build computers to spec for clients. A build that should take a couple hours can balloon easily when you’re not using the same components throughout multiple builds and run into an issue you have to troubleshoot.   Then because most people are dumb with technology you’ll end up getting your phone blown up everytime they have an issue.,Negative
AMD,"The proprietary parts have reasons, youtubers like gamernexus are just willfully ignorant or driving narratives about it.   For example alienwares historically have shared a case layout and design with precision workstations which addresses most of his ""complaints"" and comments about some of the previous chassis and why they do noticeably better with blower cards. The case designs are also why the computers get to you in one piece instead of several like lets say Cyberpower.   Then theres the fact that many dell systems are fully 12v, another thing youtubers dont like but is much better in general than the rather ancient setup you see on ""standard"" computers.  Then theres also the warranty from oem and boutique. If you rma a gigabyte/asus/whatever anything youre looking at 2-4 weeks of downtime. Ya know what that downtime is for an alienware desktop? 1-3 days, same day if you hate your wallet.   Youve said a lot of wrong things in this thread, but perhaps you should actually look at systems youre talking about directly or maybe use one.",Neutral
AMD,"It wasn’t me making that comment, but judging from the upvotes I think other people managed to understand their comment just fine.",Neutral
AMD,The whole other casemakers learning from them thing really put it into obvious sarcasm territory,Neutral
AMD,Or they were simply agreeing that it's good alienware have finally fixed their case.    Honestly you cannot take anything people say for granted these days.,Negative
AMD,"If you tell a joke and the person doesn't laugh, is it a bad joke?",Negative
AMD,This isn't a joke it's sarcasm. Also when your audience is 10 million people do you really think everyone is going to laugh?,Negative
AMD,So are you saying some people just wouldn't find the joke funny?,Negative
AMD,Its great to see more white options.,Positive
AMD,Is this to fix the issue their Nitro cards have with the 12v6x2 cable?  Clearly not a BTF competitor since the board isn't Back-Connect.  I guess I just don't understand where this setup is supposed to slide into the market.,Negative
AMD,Can we just get an industry standard for these GPU power connectors on the motherboard?   I rarely buy from the same GPU as motherboard manufacturers,Neutral
AMD,Who's buying them at this time anyway,Neutral
AMD,they will need to reinforce that power slot -.-,Neutral
AMD,![gif](giphy|TqdbhSWUmWLC0),Neutral
AMD,Looks like China only release,Neutral
AMD,"Yeah, shame it's not btf nor a smaller board. With coming rise in price their standout feature just be.. Their colors?",Negative
AMD,"Also, like Asus, no load balancing either.  So f that bullshit board connection.",Negative
AMD,"It’s just for people who want a clean wiring set up - so yes, it’s for people who would buy the Nitro otherwise.",Neutral
AMD,"Yeah, the timing is poor for sure.",Negative
AMD,"Strange, seems counter intuitive. 😏",Neutral
AMD,Strix halo doesn’t support FP8 since it’s on rdna 3.5. That’s a tough sell. If these machines used rdna4 they’d be absolute monsters. It’s unfortunate that only the 9070 xt and 9060 xt use rdna4. All of these awesome APUs being released would do so much better with rdna4 imo. Hopefully rdna5/udna1 is implemented in all products when it’s released.,Negative
AMD,They are different things. Nvidias is a dev box with the out of the box nvidia dev ecosystem. AMDs is a PC,Neutral
AMD,"Don't worry, AMD will continue releasing old RDNA iGPUs until Intel miraculously survive Intel's mismanagement crisis.  This is AMD's spin of Intel's eternal quad core.",Neutral
AMD,"TLDR: the slop boxes perform basically the same, so AMD with the much cheaper price point and x86 compatibility is the way to go.",Neutral
AMD,I saw Samsung is using rDNA 4 GPU in upcoming phones,Neutral
AMD,"Yeah, so stupid not even the refreshes get an RDNA4 upgrade. It is like AMD wants them to fail. Who in their right mind pays that much money for an outdated uArch?",Negative
AMD,"Most next gen will be 3.5, but halo will be rdna 5",Neutral
AMD,"At least Intel had an excuse in their stagnation, they were having a hell of a time with their newer process node yields.",Negative
AMD,Huh?  Infernence  Token/s- almost same due to similar bandwidth   Time to first tokem - 2-3x faster on GB10  Multi Batch Token/s - significantly faster on GB10  Fine Tuning - significantly faster on GB10  Image Generation - significantly faster on GB10,Neutral
AMD,"Slop box, lmao. I love the name.",Positive
AMD,At this point Lisa Su should revamp Radeon execs as a whole tbh. Why the hell they keep doing weird business decision all the time while the other division doing fine.,Negative
AMD,"Not stupid, product roadmaps are long and once things are in motion they can't be changed. The it could be RDNA 4 didn't exist or wasn't ready to be implemented when they started on strix halo.",Negative
AMD,Hiring a CEO that doesn't know shit about tech for a long time and then demand a CEO that knows about tech to deliver asap will kill any tech company.,Negative
AMD,Time to first token doesn't matter when we're talking about less than a second difference.,Neutral
AMD,"Not stupid, their different teams just lack coordination.",Negative
AMD,That too. Though honestly the current bubble and planarian-minded chasing of short term stock values.... is basically decimating everything anymore.,Negative
AMD,"If you are using a coding assistant, thousands of tokens per turn as the assistant reads files leads to a huge difference.",Neutral
AMD,"That's cope. If you have more input tokens than output tokens (like a typical coding agent), then prefill speed is more important than decode.  TTFT is just the same than as prefill tokens/s.",Neutral
AMD,Can these even run anything decent enough to be useful as a coding assistant? I feel like you'd be sacrificing too much vs external solutions.,Negative
AMD,"I've yet to find any use case, other than documentation reading, for AI in coding. Then again, I barely find LSPs helpful.",Negative
AMD,"In my brief experience, `codex` cli using `gpt-oss-120b` can actually be surprisingly decent, but it was too painful to use on an RTX 3090 with so much of it offloaded to system RAM. I just picked up a Spark literally yesterday. Planning to see what can be done locally.  It all depends on how much you value having a local option.",Negative
AMD,"Do you think you need the enormous cloud models for just coding? Models small enough to fit into 12GB of vram are competent, especially if given internet search access.",Neutral
AMD,Let us know how it goes.,Neutral
AMD,"I haven't tried any local models, just seen other people's results which haven't been too positive, though I haven't done much research into it which is why I was asking the question in the first place.  I'd really like high accuracy when it comes to coding, dealing with errors is extremely frustrating.",Negative
AMD,"Even huge models will be riddled with bugs, especially in lower level languages like C++ and C. Expecting any of them to be error free isn't realistic. You have to be prepared to do some software engineering, even with AI assistance.",Negative
AMD,"Yet another ""gaming"" laptop with 16 core 9955hx with 9955hx3d option, but the max gpu is 5070ti. Because making a 6-8 core x3d with laptop 5080 or 5090 would make too much sense.",Neutral
AMD,Yeah it happens. Its why I went with the legion pro for the x3d CPU at 100w allowance and 5080 at 175w tgp.,Neutral
AMD,Amd has refused for two gens in a row to make a mobile x3d chip with less than 16 cores.,Neutral
AMD,"I knew I should've done this, but without at the x3d at $1100 USD after credit card and discount incentives.",Neutral
AMD,prolly because intel cant compete,Neutral
AMD,Amd should try to compete to get their x3d cpus into more than 4 laptops first,Neutral
AMD,"Thats the point, intel is so behind that they simply dont need to.",Negative
AMD,They are vastly ahead in laptop presence.,Neutral
AMD,Plan to rock mine until AM6,Neutral
AMD,got my 5800x3d years ago best purchase ever. I was ever so hesitant from coming from the 3700x cpu but the magic that was 3d cache can not be underestimated. now if we can just get a big amd gpu i wouldn't have to worry for a while.  sad fact that pc parts are getting artificial scarcity cause ram makers well we don't want to scale growing demand we just want to price gouge.   Just hope this ai bubble pops in all of these companies faces,Positive
AMD,"I feel like they won’t considering production has been shut down for over a year. The 5700x3D, 5600x3D and 5500x3D are made from already existing 5800x3D chips that failed QA. I don’t think any AM4 x3D silicon has rolled off the assembly line since last September.   Maybe they could do a Rocket Lake style back port and get a newer Zen running on AM4. Again it’s probably not worth the effort unless next year’s market is catastrophic. Intel allegedly still has Bartlett Lake in the pipeline which would be a 12 P-Core DDR4 CPU with no E-Cores.",Negative
AMD,Long live AM4. AMD is slowly turning into Intel.,Neutral
AMD,Can AMD make a 5950X3d while they're at it too?,Neutral
AMD,I've been sitting on a 5800X for a while since I upgraded my old rig to the 3d version. Guess I should pick up an AM4 board and some DDR4 for some home server needs.  ...or maybe I should just sell the 5800x3d because apparently these things are still going for over $400 on ebay used? wtf?,Neutral
AMD,The PC market is trash—and will stay trash for a while yet.,Negative
AMD,"Even if AMD wanted to, this would have a lead time of easily 6 months, and nobody knows if the RAM price problems will persist until then – or if DDR4 will stay affordable until then.",Negative
AMD,Guna ride my 57003dx and 4090 all the to AM6. I’m gpu bound at 4K anyways,Neutral
AMD,My 5800x3d still isn't the limiting factor and I've had it ages.,Neutral
AMD,I'm glad I got a 5700x3d while they were still affordable. I'm gonna be set on my x370 launch day board for the next 4 years lol.,Positive
AMD,"I want to see 5950X3D with two 3D cache or nothing.      the true AM4 behemoth, the beast that was never born",Neutral
AMD,I'd rather see the 5700x3d. Like 2% less performance for a notably lower cost.,Neutral
AMD,"It’s the 1080Ti of CPUs, no chance AMD brings it back",Neutral
AMD,"I got mine when they came out, its a champ. The 3dcache makes it perform like its a gen newer in most games. I plan to ride it out until am6 at least lol",Positive
AMD,Honestly? Im for it.  Rocking the 5800X3D/507012gb/32gb DDR4 and I can confidently say this is a system that'll last me until 2030 and maybe even beyond.,Positive
AMD,"I've got a 5800X3D, 4090 and 64GB ram. I think I'll be holding onto this setup for a LONG time.",Positive
AMD,"Zen3 X3D Production has been closed for over a year, everything sold now is just parts that failed QA and are binned down.",Negative
AMD,With the price of RAM these days this would be a smart move. 5800X3D DDR4 totally still viable these days.,Positive
AMD,Going to rock the AM4 and 5900x until AM6.  There’s no need to upgrade a socket change when you don’t really need it,Neutral
AMD,So thankful I got my hands on one of these when I did,Positive
AMD,Mine's still going strong along with a 7900 GRE. The GOAT.,Positive
AMD,"Ever since i got mine and i forgot what bottleneck is. seriously lol, pairing it with a 9070 xt and they both fight like champs. love it!",Positive
AMD,It’s crazy that these things are talked about like they’re an ancient relic. Grabbed mine at the initial release and it feels like I just got it the other day.,Neutral
AMD,"5800X3D , 32GB ddr4-3600  and 7900XTX  This will need to last me till AM6 arrives.   Might grab a 5800x3d for my sons rig if I can ever find one  .",Neutral
AMD,my fear is that the industry aims to remove the consumer (us) of having his own HW and to force subscriptions on us,Negative
AMD,I miss the good old days of $120 5700x3d on AliExpress,Positive
AMD,"I've seen the 5800XT still being sold. I have a 7950X3D, and I barely use the cache specific cores for anything but gaming. Is it worse to just install the XT as an upgrade in an old build? A friend of mine installed it last week on his PC because he couldn't find the 3d version and he doesn't want to upgrade everything yet (too expensive for him). I think he was quite happy with it even though it wasn't the 3d version",Neutral
AMD,Gamers and reviewers must not really understand business operations then. I cannot think of a single time anyone has spun up foundries(which are already very competitive themselves) because customers were facing a hard time.,Negative
AMD,Dies in my 5950x,Neutral
AMD,If they did return it to production I would snatch one so fast. Looking to upgrade and with RAM prices jumping to AM5 doesn’t exactly look appealing,Negative
AMD,If it _really_ came to it I don't think there's any actual technical reason they couldn't slap a Zen5X3D chiplet on a package with the AM4/DDR4 IO die.  That's probably a product they tested internally and found it just didn't make sense (Zen5's are kinda bandwidth starved with DDR5-6000... there'd be a lot of wheel spinning with DDR4-3200) but that'd have been before ram costs went to Narnia.,Negative
AMD,"I miss the days of legit £125 5700x3d's off sellers on AliExpress, now you gotta drop around £250 for a 5700X3D  I don't see AMD reviving the line now tho",Negative
AMD,That will help only those who already are on AM4 and want to upgrade CPU and already own DDR4 ram. But prices of DDR4 have already been increasing in fast pace and are already almost 300% what they have been just in October here in Norway and I believe similar situation is in other places too. Anyone who thought that stable DDR 4 prices will stay down or not get extremely expensive just wait. Everyone is now trying to make money on desperate PC people. Look just the prices of standard HDD. Everyone who thought of going over from SSDs or M.2 to HDD for mass storage should know HDD prices are also going up in incredible pace. Yeah PC users are FUCKED big time.,Neutral
AMD,"I have had this same thought the last few days after seeing X3D prices skyrocket. There is obviously demand, but is it worth it enough for them to restart production, even if they have the capability. Not to mention the risk of said chip instantly selling out if the batch is too small.  I personally would not be willing to pay more than $200 for an AM4 X3D upgrade, especially since I have a 5600 so it feels like a 5700X3D restart or refresh would be most likely.",Negative
AMD,"i gave my workstation 3700x/x570 to my niece. tbh it was pretty power-thirsty and it didn't age as well as i thought '8 cores' would outside productivity, but it's a nice rig.  currently using a 12700k ddr4 setup, which is fine for me but... eh.  it's not exciting at all.  it wasn't interesting when i built it.  it'll carry me through the AI horror show though.",Negative
AMD,I will buy it 200% if it becomes available.,Positive
AMD,"I did make the step to upgrade from a 5600X to 5800X3D, it was the best decision years later , combined with PBO and a peerless assassin, I have a beast running better and cooler , as I play with a 4070Ti @1440p I feel like I'm in the best sweet spot",Positive
AMD,"Buying myself a 5800x3d a couple of years ago, even at Corona prices, might have been one of the best decisions i made, tech wise  This CPU just...goes. I play games, i work from home, i dabble with AI stuff. I've changed out a total of 4 GPUs since then, a 1660 super, an Rx6700xt, an Rx 6900xt and now rocking an RX 7900XTX. Still the same CPU, never saying no to me, no matter what i throw at it.",Positive
AMD,still rocking with R7 5700X3D,Positive
AMD,"I have 5600, 9070XT with 1440p display. Is 5800X3D is good upgrade? I won't upgrade to AM5",Positive
AMD,"5600 with 4080s, x3d tempts me",Positive
AMD,Had a 5700x3d in my hands returned it for a 7600x    Worth it yea but at the time ram prices where good    Kinda wish I keep both I was expecting to upgrade my kids PC with a 5700x3d I doubt that will happen now,Positive
AMD,"I kinda wish I had gotten a 5700X3D or 5800X3D. They are the best AM4 has to offer, and would have been an affordable upgrade to my old 2700X. That being said, I have recently upgraded to the 7800X3D and it has been amazing.",Positive
AMD,"I would unironically buy on day one. I'd love to buy a used one too, but as far as I can tell, nobody is selling their Zen 3 x3D CPUs on the used market yet (because why would you).",Neutral
AMD,Went for a 5700x3d from a 3600 about 9 months ago. I'm happy I made that decision. The x3d chips are just so impressive in terms of gaming performance. It was almost never what was limiting my system in games.,Positive
AMD,Is my 5600x new again?,Neutral
AMD,I would love it if they made a new higher tier AM4 x3d chip. That would be amazing.,Positive
AMD,I almost bought one of these a year ago to upgrade my old system that could take it.     regretting that.,Negative
AMD,Wait until people learn you can game on non x3d CPUs,Neutral
AMD,"if AMD doesn't capitalize on AM4's resurgence in popularity and the general reliance on DDR4, then Intel will, because LGA1700.  While it wasn't looking very impressive against the 5800X3D, the i9-12900K was still a contender and right now you can get an i9-12900KF for slightly less than an i5-14600KF. Obviously doesn't work for people who already have AM4, but people who are in the market for a cheaper platform than AM5 can get somewhere with LGA1700 and the performance ceiling is still higher than AM4 as long as AMD doesn't raise it.",Neutral
AMD,"Please do this, AMD.",Neutral
AMD,"this is stupid af, i'm for progress and more performance my 9800x3d is light years better than my old 5800x3d",Negative
AMD,God I hope so,Positive
AMD,Plan on upgrading mine sometime in 2027. Don’t really see the need. Especially with all future games being made with handhelds in mind.,Neutral
AMD,"I check eBay and Facebook daily, it’s a miserable time",Neutral
AMD,If they did return it to production I would snatch one so fast. Looking to upgrade and with RAM prices jumping to AM5 doesn’t exactly look appealing,Negative
AMD,"I’ve been using that chip for a while now, it’s a great work horse",Positive
AMD,Please let this happen 🙏 when I finally went to get one the price had skyrocketed so I ended up buying a used 5950x because it was far cheaper.,Positive
AMD,Best purchase I ever made back in 2023,Positive
AMD,"YES PLEASE. I’d upgrade in a heartbeat. Hell, try and revive that 5950X3D",Positive
AMD,My 3600 will have to truck on until well into AM6 probably 🫠,Neutral
AMD,The GOAT,Neutral
AMD,"I am kicking myself for not picking one up when they were readily available.   Would be the biggest boon for my rig, going from a 5600x to a 5800x3d",Positive
AMD,Here I am still rocking my amd 3950x happy as a clam with no plans to upgrade any time soon!,Positive
AMD,I remember knocking this chip when it came out because it wasn’t unlocked for overclocking and now I look like a doofus,Negative
AMD,Sad day when tech enthusiasts ask for almost obsolete tech due to current tech prices.,Negative
AMD,"Best CPU I've ever purchased by far. Paid 300, 3 years ago and there's still nothing it struggles with, not even a bit.",Positive
AMD,Would the effort to improve upon the Zen 3 chips (Zen 3+/Zen 4) be worth it over simply restarting the production of already existing designs like the 5800X3D? Or using a more advanced node like 6nm or 5nm?,Neutral
AMD,Yep would really like to buy one atm.,Positive
AMD,Got 5700x3d this year and it’s awesome. It’s essentially same speed as modern CPUs gaming at 4k,Positive
AMD,Was thinking of am5. But with all these BS. Ill rather stay am4 and get a 5800x3d,Negative
AMD,"I upgraded from a 3600 to a 5800X3D that I got on sale, best upgrade ever. I then upgraded to the 9070XT, which I got for $30 over MSRP, so I will be skipping AM5 altogether, especially now since RAM pricing went insane. Fuck..., maybe I will die using this setup if the tech bros just keep fucking over consumers.",Positive
AMD,I fully expect the RAM market to return to normalcy by August,Neutral
AMD,"Very happy to have grabbed my 5700X3D when I did, upgraded my GPU to 5070Ti shortly after. Yesterday my APC Smart-UPS made a very loud pop sound and threw up an error code as my system immediately went dark. Smelling burning I scrambled for a power bar and all I could do was pray my computer didn't get fried because hitting PC Part Picker is NOT very fun these days!",Positive
AMD,Please just 5850X3d or maybe 5955X3d for us. I will even buy it original price,Positive
AMD,5950x3d would be fucking sick,Negative
AMD,*revive ddr4,Neutral
AMD,Hail the king baby.,Neutral
AMD,I'd buy two for my kids.,Positive
AMD,I had to settle for the 5700x3d…. I actually swapped from a 5800x because I mostly do 90% gaming 10% everything else /sad,Negative
AMD,I would absolutely get one. I'd give my kid the 5600x3d,Positive
AMD,"I would get one. And if there are bundle deals with an AM4 mobo, I would too. Not that my current mobo can't handle it but I broke the USB 3.0 header on it and I can't use the front panel USB ports on my case. Stupid reason I know but well.",Negative
AMD,"5800X3D the GOAT, right after it is 5700X3D",Positive
AMD,My old 5800xed and my old R7 make a good steam machine.  In a small ITX box.,Positive
AMD,"I actually upgraded at the beginning of the year, from a 3700x to the 5800x3d, 16 to 32gb at around 65€ and got the 9070xt, don't regret it at all seeing what again happened, next step will be a monitor upgrade :)",Positive
AMD,"I have a 5900X, I would ‘downgrade’ my cpu to one of these and move my current chip to my server (currently an i5 6500)",Neutral
AMD,Or make somehow make the new processors/mbs work with DDR4 lol. That's the only way I'd upgrade. I'm not an engineer so I'm not sure if that's even possible.,Neutral
AMD,5**9**00X3D.  Dr. Su literally had one in her hands when she unveiled 3D V-Cache,Neutral
AMD,OMG please!,Neutral
AMD,seems gamers are not paying attention to market forces   AI makes more money to sell to,Negative
AMD,Looks like we're going the distance buddy.,Neutral
AMD,But I just got a 5700x...,Neutral
AMD,"went from 3700x to 5800x3D 2 years ago, best investment ever  I am planning to keep this and skip AM5+DDR5 going directly to AM6+DDR6 :P",Positive
AMD,5700x3d would be cheaper for 10% less perf max. But yeah the idea is good!,Positive
AMD,I'd love to see a 5900X3D or 5950X3D.,Neutral
AMD,"Release a 5950X3D with 3D cache on both dies, and I'd buy it.",Neutral
AMD,Be careful what you ask for. Seems like a perfect opportunity for AMD to come out scalping with it costing $600 or more.,Neutral
AMD,"Rocking 5800x non 3D, i won't replace it anytime took tbh",Neutral
AMD,"I just sold a 5800x3D for more than I bought it 3 years ago, which was also more than it cost me for the 9800x3D lol",Neutral
AMD,I bought a 5700x3d off aliexpress years ago for $120. Feelsgoodman,Neutral
AMD,On paper it sounds like a good idea for them to spend some time backporting new archs to AM4 if this memory shortage will last a while. But I still doubt they have enough time to do that if they were to start today.,Neutral
AMD,"I regret not getting a 5800 or 5700x3d because I thought I’d have enough saved up to do a whole mobo, RAM, and CPU upgrade for my rig.  Waited too long then the RAM shortage hit and now I’m stuck with my 3700x for the next few years.",Negative
AMD,"I upgraded mine a while back, keeping it until AM6 at least",Neutral
AMD,"Been trying to find one, but $400+ for a used, 5 year old CPU from a random source on eBay is just a terrible deal. Fuck this bullshit.",Negative
AMD,"I don't see this happening, because the current memory shortages will not be fixed by switching back to DDR4. DDR4 production will be ending very soon and memory manufacturers will be more than happy to shift all production to DDR5.   The current memory shortage problem is a result of an artificial surge in demand while manufacturers remain cautious about ramping up production too hard, lest the bubble bursts and they have to drop prices through the floor. It's going to be a while before either side gives in.",Negative
AMD,"I would love that. By the time I got my savings in a better state, I only found a store selling the last of the stock at premium price. And switching to AM5 is too expensive for my taste. So yeah, it would get in line to buy one.",Positive
AMD,No 5800x better more ram performance gain if the produce them with higher quality they could all run 5ghz allcore and 2x16gb 4000 1:1 mode,Positive
AMD,"AM4 changed the game but none of us has predicted for how long yet! Went with a full custom watercooled AM4 platform since 2017 on 8xcores CPUs on a Crosshair VI Extreme paired with a GTX1080Ti and a 2x8GB kit of B-Die DDR4 (@3800C16currently)  : 1700X 4.1GHz / 2700X 4.2Ghz / 3800X 4.3 Ghz / 5800X3D Undervolted 4.45 Ghz... And with the addition of an RTX3080Ti recently, that 7 years-old AM4 platform feels like it's gonna keep taking it for years to come like a champ!",Positive
AMD,I hope they make more 5800x3d. They stopped by choice.,Neutral
AMD,Where can I get one nowdays? Seems sold out everywhere.,Neutral
AMD,"Yes please!!... Currently I'm still rocking a 5800x3d and have 2 other AM4 pc's waiting for an upgrade. At that time the original 5800x3d had a restriction on clocks and temps due to the 3d v-cache being on top... With the current 9800x3d using the v-cache at the bottom to take advantage of better coolers and temps, I would definitely buy at least 2 more and know some partners that would make the final upgrade for their long forgotten AM4 still rocking ryzen 3000 cpu's.",Positive
AMD,"5800x3d is starting to show it's age, but I'll run it till AM6 I reckon. Same with my 7900XTX. CPU bound on BF6 is kinda annoying. 5800x3d can't really deliver more than 120-130FPS in BF6, and makes me CPU bound which is annoying.",Negative
AMD,"5700x3D (on my Ryzen 1 cheap motherboard) and RX 9070, I'm ready for the years to come.",Neutral
AMD,Which gamers and which reviewers exactly? 🤔,Neutral
AMD,yes please,Neutral
AMD,Releasing 5850X3D (with the compute die on top of 3D cache just like 9800X3D) would be better.,Neutral
AMD,One of the best buys ever for me,Positive
AMD,It's definitely the 1080 Ti of CPUs.,Neutral
AMD,300 well spend bucks 3 years ago...,Neutral
AMD,Yeah Id be willing to pay a pretty penny if they would be willing to sell it for a slight mark up.,Neutral
AMD,"Same, i'm skipping AM5 all together",Neutral
AMD,"Bring back the 5800X3D, please!",Neutral
AMD,"Made the jump from 1800x to 5800x3d, was incredible. Not sure how much the older x370 motherboard chipset is a limiter at this point.",Positive
AMD,Picked up a brand new 5800X3d for $280.00 two years ago and sold my 5800X for $160.  Social media haters said it was a side grade and not an upgrade.  Look who is laughing now.  Ha ha ha.  My 5800X3d paired with my rtx 5070 ti is playing everything on Ultra and high with no bottlenecking.  Will probably be still doing the same for the next 3 years.  And I was also able to pick up 64Gb of G.Skill tridentZ (4X16) 16 18 18 39 for $124 in June this year.  So happy I purchased before this rampocalypse,Positive
AMD,"If they would now come with a 5850x3d with slightly higher clocks, everyone and their mom would buy it.  Companies often underestimate the demand for the ""last best option"" on last gen platforms that are still being used.",Neutral
AMD,Bummed to not have got one of these. My 5600x is fine but I know I’m missing out,Negative
AMD,"Yes! I really hope they do! I'd buy a 5800x3D immediately!   I started to upgrade my AM4 system with the goal of transitioning over to an AM5 build, but the RAM issue has me holding off on things.   I currently am running a 5600x, 32GB RAM(3600), 2x 2TB NVMe, and a 5070ti on a 27"" curved 1440p Asus monitor.   The 5070ti was an upgrade from the 3060ti I was running. And I upgraded to a Corsair 850w psu at the same time.   All I need to complete the new build is a mobo,cpu,and RAM. Possibly a new cpu cooler.   But, if I could get a 5800x3D for a good price, I'd probably buy that and stick with AM4 for awhile longer.",Positive
AMD,Upgraded my R5 3600 to R7 5800X3D like a year ago. Best decision ever. Think i can survive 3-4 more years with it.,Positive
AMD,"Yeah, I won't be changing mine out any time soon.",Neutral
AMD,I got that cpu 3 days ago. Decided to upgrade laterally and keep ddr4. It's really a good cpu.,Positive
AMD,Intel is looking pretty good these days if you're on a budget and want to do ddr4.  I just grabbed a 14600k for my dad's build for only $210 CAD and a 32gb kit of ddr4 was only $200 CAD.I would've built him an AM5 system for sure if DDR5 prices weren't insane.,Positive
AMD,I feel dumb but why exactly? Seems like linked to ram price but ddr4 is also getting more expensive is it?,Negative
AMD,"Hopefully amd bean counters are looking in to it...  If they figure revenue is greater than cost then they would go for it.  The big factor there is estimating how big the market is for a 5850x3d or maybe a 6800x3d or something where they find another foundry to kinda make what they had or jank a zen4 into am4.  The likelihood of the same sku would be very low because of these factors.  So how big is the market/potential revenue and is it more than the cost of janking something together.  That market size question falls each day we get closer to the end of the ram shortage.  I gotta think it is at least being kicked around.  Maybe it will die, but it has to be a thought.",Neutral
AMD,"AM4 X3D chips only existed and made sense because they were derived of Milan-X EPYC CPU's, which is no longer in production. AMD isn't going to spin these back up again just for a small amount of consumer demand without the ocean of datacenter demand backing it up..",Neutral
AMD,Make AM4 Great Again,Positive
AMD,Still Have my old school one [https://imgur.com/a/ryzen-7-5800x3d-PvN61H3](https://imgur.com/a/ryzen-7-5800x3d-PvN61H3) .,Neutral
AMD,Still have mine [https://imgur.com/a/ryzen-7-5800x3d-PvN61H3](https://imgur.com/a/ryzen-7-5800x3d-PvN61H3),Neutral
AMD,Glad I kept it; it was made too good.,Positive
AMD,Would be a good move since newer platforms use DDR5 which is being snatched up by datacenters. DDR4 is probably cheaper.,Positive
AMD,"Upgraded my 3600X to 5800X3D kind of on a ($600) whim. I didn't really have any good reason to. I guess my logic at the time was to get the 'last, and best version' of AM4 and ride out this build. Other than paying full price for it, I've been happy with it. Can't say the same about the buggy Radeon driver experience however.",Neutral
AMD,"I've had my 5800X3D for just over 3 years now. It's going to my wife and I'm upgrading to a 9800X3D. I bought 64gb of DDR5-6000 for $500 CDN less than it is today. Still more than at its lowest, but I knew I had to pull the trigger while I had the money.  Edit: Canadian dollars not US dollars",Neutral
AMD,I sold my used 5800X3D a few weeks ago for more than what I paid for my 9800X3D 😅,Neutral
AMD,"Well, instead of reviving the 5800X3D, isn't it just easier to make a DDR4 AM5 motherboard?",Neutral
AMD,Ditto.,Neutral
AMD,I will rock it forever  if they bring it back,Neutral
AMD,"Mine died under warranty, after waiting 6 months for a replacement I gave up and had to settle with a 5700x. Pretty sad and annoyed by it as this was my plan too.",Negative
AMD,Same. Still going strong.,Neutral
AMD,Plan to rock mine until it can no longer play MMO FFXIV. I'll leave to the Japanese to support the game on a GTX 970.,Neutral
AMD,5700x3d til ATLEAST Second gen AM6.,Neutral
AMD,"Yep, not gonna start considering an upgrade until long enough into AM6 to not be an early adopter.",Neutral
AMD,Same here. Getting in on the end of AM5 isn’t anywhere as appealing as the start of AM6. My next motherboard is gonna get a long time of use…,Negative
AMD,Same brother !,Neutral
AMD,Same!,Neutral
AMD,Yeppers,Neutral
AMD,When is AM6 expected?,Neutral
AMD,"I was thinking about getting a 9800x3d at some point but it's just not even worth it. At this point I have a 9070xt, 5800x3d, and 32gb of 3600 ddr4 ram. I mainly game at 4k, so I'll rock the 5800x3d for a while and just upgrade my GPU in like 2-3 years.",Negative
AMD,"I have a 5700x3d, 32 GB DDR4 4000, a 2 TB SSD, and a 4080 Super.  I’m well situated to outlast the AI boom, figure I should be able to get playable framerates on new games for the next 5 years.",Positive
AMD,"Same. I was debating an upgrade to AM5 this year until I got the option to buy an 5800X3D / RX7800XT upgrade for my aging AM4-System for 500 Euros from a work colleague. Considering that the individual components alone still sell for more than 400 Euros on Ebay, I happily accepted the offer of course and now plan to survive with that system until around 2028 when i will build the PC of my dreams (an OptimumTech style AMD-only SFF-PC) on the AM6 platform. My aM4-system will then have served me well for a decade.  Thank you AMD for not copying Intel's ""buy a new mainboard to your CPU-upgrade because we have switched a few pins around""-nonsense.",Positive
AMD,Rocking mine til AM7,Neutral
AMD,The chip makers are making so much money off of AI that they don't even want to serve the consumer market at all anymore because the margins are lower.,Negative
AMD,"Bought a 5700X3D for $120 USD brand new and sold my 3700x for $75. Such a solid upgrade. Paired with my 5080, I should get another few years on my system.",Positive
AMD,Likewise mate! Also switched out the 3700x for the 5800x3d. Also switched a 2080s out for the. 6800XT at the time.   The 9070XT has been cheaper here as compared to what I paid for the 6800xt. Still can't decide if it's worth it now or worth waiting it all out,Neutral
AMD,"Same!  I got it in a discount bundle from microcenter.  Like 350 for the chip, a new mobo and 32 gigs of ram.  Crazy amazing deal",Positive
AMD,"I don't even blame the RAM manufacturers **this** time.   I think the RAM manufacturers know AI is a bubble. It takes multiple years to build new manufacturing plants to expand production. Meanwhile, the AI bubble could pop in like 6 months.  I blame Sam Altman buying up all the RAM.",Negative
AMD,"I rotationally upgraded my current PC from the 1800x on a 370 platform, up to a 3800x & finally a 5800X 3D & 570, glad I got 32GB of RAM when I did, because the G.Skill modules and latency haven't been available for quite a long while.",Positive
AMD,Same. Got my 5800x3D when rumors were that it was a very limited run so paid £337 in January 2023. It’s nearly 3 years old and it’s been a beast. Was an upgrade from a 3600,Positive
AMD,"Likewise, still feel no need to upgrade from it.",Neutral
AMD,I made the same upgrade last year. Amazing performance along with a 4070 Ti.,Positive
AMD,">got my 5800x3d years ago best purchase ever. I was ever so hesitant from coming from the 3700x cpu but the magic that was 3d cache can not be underestimated.  I made the same upgrade and I totally concur, going from a 3700X to a 5800X3D definitely felt like a bigger upgrade than going from an i5 4670k to a 3700X.  >now if we can just get a big amd gpu i wouldn't have to worry for a while.  As far as I'm concerned my 6900XT still delivers on 1440p on most games, but 16GB of VRAM is beginning to feel a little cramped for the newer games.",Positive
AMD,Exact same 3700x -> 5800x3d for me,Neutral
AMD,How much is the difference now ? And what gpu you have?,Neutral
AMD,"If anything prices on CPUs will drop thanks to lower sales due to increased memory prices, people will keep from updating for a while.  DDR4 production has been completely shut down since 6 months so AM4 parts availability will drop anyway and production plans has already been phased down in favor of other platforms.",Negative
AMD,"Switched to the x3d while using a 3700x as well. 330 at microcenter on the first black friday after its release and I was floored at how much more performance I got. Double the fps in wow was completely unexpected, spikes in fortnite were nowhere to be found.",Positive
AMD,Why would ram manufacturers scale for artificial demand tied to an industry everyone knows is a bubble,Negative
AMD,"We just need to wait for the fabs to catchup on production and eventually there will be some available for consumers and bring prices back to more reasonable levels. This ai bubble popping will just hurt gamers and consumers because it's basically fueling future technology development and advancement. If we hold through the expensive price hikes, we'll benefit from rapid growth in the end.",Neutral
AMD,"I kinda regret my purchase of the 5800X3D. The 5700X3D got announced within literal weeks for hundreds of $ less for like \~3-5% less performance. Couldn't even return it as I was not in the country at the time, and by the time I came back I was well beyond the return window.",Negative
AMD,Also went from 3700X to 5800x3d early on. The difference was significant even with a 1080Ti at 1440p.  Eventually I swapped it for a 6800XT and now an 9070XT since release. Works great.,Positive
AMD,"Managed to buy a new 5700x3d , the last one , few months ago.",Neutral
AMD,it's sadly not going to. It's going to pop in all of our faces,Neutral
AMD,"It takes time to scale up, guaranteed they're already in the process of doing so",Neutral
AMD,The scarcity isn’t artificial at all.  The memory makers found buyers willing to spend far more than those that make consumer memory.,Neutral
AMD,"> I don’t think any AM4 x3D silicon has rolled off the assembly line since last September.  SKUs haven't, but AMD extended Milan availability to 2026, which means the silicon is there.",Neutral
AMD,"The beauty of it is they wouldn't need to do a Rocketlake style backport, just put newer chiplets alongside the older IO die on the package.",Neutral
AMD,Almost definitely easier to release an AM5 motherboard that supports DDR4 than a newer Zen on AM4.,Positive
AMD,"Bartlett Lake is allegedly for the embedded market only, no?",Neutral
AMD,"Even if next years market is catastrophic, I just dont see how spinning up an entirely new hybrid SKU (So eg Zen 4 or 5 but with DDR4 Support) would make any economical sense when looking at the cost and also the fact theyre already 2 Gens in on AM5.",Negative
AMD,"I'm up to 4 AM4 machines in my house now.   3600x, 3700x, 3900x, and my 5800x3d.  I'll be running AM4 for probably the next 10 years until I retire the last of them   Edit:  also have a laptop with a 5600h, but that's not Am4",Neutral
AMD,"Oh man, I fucking wish. I’d be all over it.",Negative
AMD,THIS!!!  I really enjoy my 5950x but it could use a little more oomph in the gaming department.,Positive
AMD,Pfft just have them make a 5950X3DX edition with a 32GB HBM MALL cache with DDR4.     Problem solved!,Neutral
AMD,"It was in the labs, Lisa Su even showed a prototype at Computex 2021 lol.",Neutral
AMD,"They actually did...but it was just an engineering sample that never made it to market 😞. They maxed it out with 3DV-cache for each CCD, too.  https://youtu.be/RTA3Ls-WAcw?si=\_6NcQQG798fTH8aR&t=1072]  How awesome would that be tho, if they finally crowned the AM4 platform with a 5950X3D (with the cache on both CCDs, flipped in the right position this time for higher clocks, and windows cache optimizer chipset drivers).  Do it AMD!!!",Negative
AMD,Yes  And maybe give us 2 x3d dies,Neutral
AMD,If they did that it might straight up cannibalize themselves for how too good it would be,Positive
AMD,I’ve been waiting for this since X3D has been a thing.  I will unironically pay $400 or more for that chip so I don’t have to upgrade my entire PC when my 5900X is no longer cutting it. Getting an extra 3 years or so would be worth it.,Positive
AMD,Gimmi.,Neutral
AMD,Or bring back dual CPU motherboards to the DIY market,Neutral
AMD,"Did you check the new 9950x3d-2?... It has v-cache over each CCD, instead of only one of them... A 5950x3d-2 would be the ultimate and final upgrade for an AM4 setup... I will definitely do the jump to AM5 when RAM prices get closer to normal next year and with ryzen 10000 jump in performance.",Positive
AMD,They don't make them anymore and they're one of the best DDR4 cpu's available. I bought a 5700x3d at the beginning of the year and it had already risen to $250. I imagine it's higher than that now.,Positive
AMD,Man i would sell my 5800x3d now if it's fucking $400 so I can upgrade but ram prices are ridiculous. Maybe I'll wait til AM7 now looking at hardware prices... Was previously gonna do AM6 upgrade.,Negative
AMD,RAM price is expected to come down in 2028.,Neutral
AMD,"> stay affordable  lmao, even bottom of the barrel ddr4 is like triple the price it was a few months ago.",Neutral
AMD,Got a 5700x3d on cyber Monday from AliExpress and they’ve shot up since. Can’t wait to slot it in to replace my 2600. My 1080ti will be happy,Positive
AMD,AM4 Boards a true troopers.,Neutral
AMD,"The reason the 5700x3d released so late, compared to the 5800x3d, is that it's just 5800x3ds that didn't make the cut in clockspeed.  I got one towards the end of last year for a great price, and the performance difference is usually a little bit more than 2%, but it was literally less than half the cost of a 5800x3d.",Neutral
AMD,Same as me. I bought 128GB RAM but two of the sticks stopped working. 64 is enough though.,Negative
AMD,Thankful about being two generations behind?,Neutral
AMD,They will or they will starve for sales and their Mobo partners will as well.,Neutral
AMD,I noticed a difference going from a 5600x to 5800x3d. Mostly in the 1% lows.,Neutral
AMD,"Yes, if you can find it for a reasonable price. I went from a 5600x to a 5700x3d and it was worth it to me, but I paid like $130 on AliExpress a bit over a year ago.",Positive
AMD,Unless you're playing cpu bound games it's probably not worth it. If you can get it for cheap then sure.,Negative
AMD,DDR4 production is over. This product would only be for people already on the AM4 platform.,Negative
AMD,"The AM5 Chips only have ram controllers for DDR5. You could make a motherboard with DDR4 slots but I seriously doubt it would work. Before, the ram controller was on the motherboard but now it's inside the CPU itself so we'd just be back to square one",Negative
AMD,"Not going to happen, Zen 3 X3D Production ceased like a Year ago and every x3d product based on Zen3 sold are just downbinned 5800X3Ds that failed QA Testing. Even if they now decided to buy both 7nm production capacity and Front side Hybrid Bonding it would take at least a year until you would be able to buy new 5800X3Ds, and all of which would be pretty expensive for what to them is a last last gen niche product. I mean yeah it would be pretty nice but economically this wouldnt make any sense even if they tried to be nice.",Negative
AMD,How’d it die?,Neutral
AMD,If this happens to me I'll probably just sell the rest of my PC at this point lol,Neutral
AMD,I just got my hands on GTX 970 to replace 660Ti on my second scrap PC and it's surprisingly capable card running Valheim at 1080p max settings,Positive
AMD,"Same here, upgraded my 5600x to a 5700x3D before they became expensive AF. Upgraded my 6800 to a 9070XT a few weeks ago, initially wanted to do a full upgrade somewhere in early '26, but at the moment I can play everything on ultra with 150+ and beyond fps on 1440p, so I'm good for at least 3-4 years, I reckon.",Neutral
AMD,"Same, with my 5700x3D and 4070 Super, I think I’m good for some years",Positive
AMD,"I bought in 2020, at what I thought was the end of AM4–but I had the money (and time, lol) to upgrade. I got a 5600x figuring maybe down the road I could get a 5900x or 5950x if I wanted something faster. Instead, I got a 5700x3D cheap last year which has been a nice upgrade!  I’m hoping AM6 has more cores per CCD, I’ll probably start with an x600 and then upgrade to x3d on the 2nd or 3rd gen.",Positive
AMD,Not anytime soon. 28 or 29 or something .,Neutral
AMD,It would use fabs that aren't currently being used to make AI chips (for the most part I would assume) so it would be more additional revenue than one eating into the other.,Neutral
AMD,greedy bas\*\*\*\*s. But nothing new,Negative
AMD,"For AMD their AI gross margins are actually below their corporate average. Meanwhile, gaming CPUs for AMD have always been above their corporate average margin.   There's also nothing that stops AMD from doing this. There's no conflict of wafer supply or other packaging shortages that overlap with AI. The hardest part might just be trying to start back up those old packaging lines and old wafer starts. It could be pretty difficult to do it. And as soon as they could have any products out would be in 6 months which would probably still be worth it. But who knows.",Neutral
AMD,yeah i came off a 5600x for a 5700x3d and paired with a 9070xt its really a solid combo. I got so lucky doing a minor refresh on my pc earlier this year.,Positive
AMD,"I swapped my 2700x for 5700X3D and Vega 56 for RX 6800 XT, works wonders :3",Positive
AMD,I bought my 5700X a year ago for 140€ new. I hate the market.,Negative
AMD,Same cpu paired with my 5070 ti. How much longer will this config last?,Neutral
AMD,"If you can afford the 9070xt now, do it and sell your 6800.  Shits only going to get more expensive the more you wait and it’s a solid card.",Negative
AMD,"I upgraded my CPU 3 years ago and my GPU 2 years ago. Went from 3700X to 5800X3D and RX 5700XT to RX 7900XTX. Managed to sell my old CPU, GPU and some other stuff and got enough to upgrade my RAM few months ago. I started doing a lot of 3D work and game dev stuff, so 16GB wasn't cutting it, got an upgrade to 64GB for €250. Seeing the current shit show, im so glad I made the jump to 64GB",Neutral
AMD,Supposed leaks put it at 2028 before prices come down due to backlog demand even with increased production.,Neutral
AMD,Why scale up? If the bubble pops. They would be stuck holding the bag and ram would drop even more than if it just pops with less equipment,Negative
AMD,"Except it is artificial.  Both Samsung and SK Hynix have just come out and said it lol  https://wccftech.com/two-of-the-biggest-dram-suppliers-are-skeptical-about-increasing-production/  This is all due to OpenAI buying insane amounts of ram production to keep others from getting it, artificially inflating prices.",Negative
AMD,"The extension was over 2 years ago already with availability up until 2026 theoretically only for certain CPUs. AMD likely ended manufacturing this year.  The peak run for TSMC N7 was 2019 - 2021, with most capacity dropping off by 2023. This is a 9 year old node now.  By now TSMC N7 is a small shadow of what it was with most remaining capacity bought by 2nd tier chip designers like Huawei.",Neutral
AMD,It’s a bit of a mystery. Probably should have been released by now but they haven’t officially cancelled it,Neutral
AMD,"Kinda same, as I'm now on AM5 but my bf inherited my 3900X and my server runs a 1600X lol",Neutral
AMD,"Got 5950X, 3800XT, 2700X and 3200G systems in our home. And my old 1700X laying in a box.   I just today was looking for 5800X3Ds to replace my sister's 2700X and living room 3800XT. Guess I will just get some 5800Xs instead, as the X3Ds on eBay are over 400€ while the 5800X is everywhere for around 160€ new... The 3200G will also be replaced with a 5700G soon.",Neutral
AMD,"Right there with you. I kept buying all of my co-workers ""old"" PC parts for my home servers and wife and kids machines. Up to 6 AM4 machines including mine.",Neutral
AMD,"3 here with 5800x, 5700g, 1600x in systems and a 2400g laying around.",Neutral
AMD,"I tried switching out my 5950x with a 5800X3D for a little bit. The difference in gaming wasn't all that noticable for me and the games I play. I ended up giving the 5800X3D to my nephew and going back to the 5950X. I need all the core for blender, code compilation and other multi-threaded workstation stuff, so I found the 5800X3D too slow for embarassingly parallel workloads.      Guess I'm waiting out the next few years before it makes any sense to upgrade. If I were to buy anything it would be the 395+ AI MAX.",Neutral
AMD,Process lasoo,Neutral
AMD,BLACK EDITION,Neutral
AMD,I wonder if anyone watched that video... He said they did the 7950x on AM4 before they made the 5800x3d... https://youtu.be/RTA3Ls-WAcw?t=1101... I would guess that is the 7950x3d... Which means they can just duplicate the 7950X3D on AM4 using the 7000 architecture.,Neutral
AMD,That would hurt productivity while gaining nothing for gaming.,Negative
AMD,"If it is too expensive to upgrade AMD will starve... They can support newer features on new MOBOs for the AM4 line while pushing newer CPUs for the AM4 line. Let people reuse the DDR4. Helps customers, helps mobo manu and helps customers. Win/Win/Win in an otherwise screwed situation that is expected to last until 2028 at the earliest.",Neutral
AMD,That doesn't make any sense when there are 16 core consumer CPUs. Most people don't need that much and anyone who needs more can get threadripper or epyc.,Negative
AMD,For gaming latency would get worse. Trust me we've already had problems with cross CPU memory talk in big data apps. Gaming would be much worse.,Negative
AMD,I definitely am not used to my hardware appreciating in value.,Negative
AMD,Yea they’re like $330 now. I’m just rocking a 5600x. Not much point upgrading to non-x3d chip and the x3ds are too expensive =/,Negative
AMD,Check ebay. Nothing goes for under $400 without damage like bent pins.,Neutral
AMD,By whose estimate?,Neutral
AMD,"Nobody knows when the prices will come down. They are expected to NOT come down in 2026 because that DRAM manufacturing capacity is already sold out.  When prices come down depends on when demand growth from AI slows down. Some people think it's a bubble and will soon pop. I don't think it's a bubble. RAM, electricity and permits seem to be the big bottlenecks for data centers. Financing could become an issue if the markets turn bearish on AI.",Negative
AMD,"I fully expect it to come down by August next year, as the AI bubble starts bursting",Neutral
AMD,You upgraded your CPU before that old ass GPU?,Neutral
AMD,"You should check the warranty on your busted sticks--a lot of manufacturers give a lifetime warranty.  I had two sticks die, and they were replaced under warranty...but I also only had 2 RAM sticks, so I had to shell out for replacement RAM while I waited for them to come back, which was annoying--but it beat just not using my PC, lol.",Negative
AMD,Yeah because I would have needed to build an entirely new system to upgrade past a 5800x3d dipshit,Negative
AMD,Couldn't they just create a conversion chip within the motherboard to make the CPU think it's DDR5 ram?  There might be a minute drop in speed from the conversion but it's better than paying half a grand in Ram.,Neutral
AMD,It would make plenty of sense if AM5 sales are way down due to the RAM situation.,Neutral
AMD,I had two 9950x die on me for no reason. Just happened I guess,Negative
AMD,Just went to turn on the pc one morning and it wouldn’t boot,Negative
AMD,I had two 9950x die on me for no reason. Just happened I guess,Negative
AMD,Same here with the 5600x to 5700x3d upgrade when it was cheap on alibaba last year. So glad that I made that upgrade.,Positive
AMD,"That's what I was worried about if I did make the jump, glad to hear I'm not super crazy",Neutral
AMD,"You can't easily refit a fab for an entirely new and very different process node. You basically would prefer to build a whole new one. In particular, the most advanced chips have very specialized processes / devices, EUV litho and Gate all around FETs.",Neutral
AMD,It’s the same fabs tho.,Neutral
AMD,What kind of increases did you see from the 5700x3d?    Ive been scouring marketplace trying to find one of the x3d chips and ive begun offering to swap my 5600x plus cash so they can put it in their build and still sell a complete system. No dice yet.,Neutral
AMD,"Did you see quite the performance uplift gaming wise? I'm currently running a 5600x+9070xt, benchmarks I've seen are around 10%.",Positive
AMD,Im trying to upgrade from a 2700x. Upgraded to a rtx 5070 earlier this year. Am4 x3d is through the roof and same with am5 kits,Neutral
AMD,When I saw it at that cheap of a price I bought 3 of them and gifted them to people that were on am4 still in my friendsgroup. I knew it was too good to pass up,Positive
AMD,At 1440p? 4-5 years at decent settings probably!,Neutral
AMD,Or keep the 6800 as a backup. Who knows what availability looks like for the next 12 months?,Neutral
AMD,"Money.  Just because the bubble pops doesn't mean the technology will be going away. Homes and the internet still exist.  Samsung, micron, sk hynix have all already reportedly begun to scale up and it's going to take years. Probably be 2029-2030 before these new production lines are ready.",Neutral
AMD,I can see industry wanting to use their existing DDR4 and upgrade to high core count EPYC 7003 high core count SKUs. That might be incentive enough for AMD to restart production.,Neutral
AMD,My server is the 3900x.   I picked it up from Facebook marketplace.  Came with a x570 motherboard and 64gb of ram for $250.   Couldn't pass it up.     It replaced my old Fm2+ motherboard with an athlon X4 860k,Positive
AMD,Thanks for that. I got the 5950x when I thought it was gonna be the halo chip. This helps allay my fomo.,Positive
AMD,I went 9800x3d and threw my 5950x in another PC and the difference in gaming can be massive but it depends on the game and how CPU dependent it is. The heavier ones are going to see the biggest differences. 5800x3d was basically on par with 1st gen AM5 non x3d stuff.,Neutral
AMD,I use that when I game and stream simultaneously.  It's why I bought the 5950x.  Like two PCs in one.  :D,Negative
AMD,Depends on the game and depends on the productivity  There are reasons to want a 16 core 3d vcache CPU,Neutral
AMD,"Youre ignoring the fact that backporting newer Zen Architectures to work an AM4 and support DDR4 would require completely redesigned silicon for a product that would cannibalize their newer chips, for an publicly traded company, this just doesnt make any economical sense.",Negative
AMD,Yea its a weird feeling. My ram went from 200 to 700,Negative
AMD,"If you haven't been paying attention, EVERYONE in the industry.  https://tech4gamers.com/memory-shortage-till-2028/  https://www.tomshardware.com/pc-components/dram/the-ram-pricing-crisis-has-only-just-started-team-group-gm-warns-says-problem-will-get-worse-in-2026-as-dram-and-nand-prices-double-in-one-month  Mircon wouldn't leave the consumer market if it was just a flash in the pan..  It's just price fixing at it's finest..  https://www.tomshardware.com/pc-components/dram/memory-makers-have-no-plans-to-increase-production-despite-crushing-ram-shortages-modest-2026-increase-predicted-as-dram-makers-hedge-their-ai-bets",Neutral
AMD,Mine!,Neutral
AMD,Yes it is old ass but it still kicks ass. The cpu and 32gigs ram will keep it running. Next upgrade will be gpu when i see the need,Positive
AMD,"Thanks. Maybe I can get them replaced on warranty, sell them and retire early :)",Positive
AMD,chill down pal lmao,Neutral
AMD,I really don't think that's how it works. Ram is electrically incompatible between generations. The fastest DDR4 is also slower than base DDR5 meaning that it probably couldn't run at all (3600MT/s for DDR4 highest VS 4800MT/s for DDR5 lowest),Negative
AMD,Which currently they aren't and even then I think it would be highly unlikely for that to happen on a scale which would be noticeable enough to show up on AMDs Radar since Intel's current Plattform also is ddr5 exclusive.,Neutral
AMD,"I had my first 5800x3d arrive DOA, Amazon thankfully replaced it for me after a 5 min chat",Positive
AMD,Did you upgrade the Bios? Apparently MOBOs are overvolting.,Neutral
AMD,"At first I wasn't sure if it was a big enough upgrade, but it certainly was. Again, good for at least 4 more years.",Positive
AMD,"Yeah I made a machine to play PoE in 4K in 2024 summer with a 7900XT and an 5700X3D. It delivers. And it'll deliver for quite some time. Although I am upgrading the video card in a roundabout way: I will move from Europe to Canada next summer, I bought a 9070XT in Canada during Black Friday and sent it to a friend, will sell the 7900XT locally next summer and that's how I will get a practically free upgrade. I will take my CPU, RAM and SSD with me -- small, lightweight and by now super valuable.",Positive
AMD,"5800x3d are 7nm, those fabs aren't pumping out bleeding edge AI chips.   There is still use of those fabs for other purposes, but I hardly see them at capacity",Neutral
AMD,i dont recall exact numbers but it was worth the effort for me. I noticed better performance and i'm hoping this will get me till am6.,Positive
AMD,Same here,Neutral
AMD,"I wish I had friends like you 😭  That CPU here was like double the price I spent on the normal one, and I had to save for RX9060XT so...  It is what it is.",Negative
AMD,Much like how I kept the 2080s as a backup.   If the 9070XT is going to fail within a single year it's not a good buy though.,Negative
AMD,Maybe vene hopefully update them as well while they are at it?,Neutral
AMD,Nice! Daddy has the 9950x3d and it is nice having so many threads for sure,Positive
AMD,"The ccd to ccd latency kills any benefit for gaming.   Some games doesnt even benefit from the extra cache and would rather run on the non 3d cache ccd.  And because zen3 has the cache on top of the ccd, there is a frequency penalty which hurts alot of productivity. There are ofc some that can use the extra cache, and in that case it would benefit. But that starts to get very niche on a desktop cpu, and gives up performance in most other use cases.",Neutral
AMD,"You are ignoring the fact they already mated the 7950X to am AM4 platform before they released the 5800x3d. It was an engineering sample.;)  [https://youtu.be/RTA3Ls-WAcw?t=1101](https://youtu.be/RTA3Ls-WAcw?t=1101)  At least the AMD Engineers claimed they did it. But what do I know, I am ignoring facts. I am also ignoring the fact that the 7000 series chips are still being produced.  ""Key Details from AMD Engineers  During a visit by Gamers Nexus to AMD’s testing labs, engineers clarified several points about these ""hybrid"" samples:  * **Internal Proof of Concept:** AMD engineers explicitly stated they had the  **Ryzen 9 7950X**  (Zen 4) running on **AM4 boards** internally. This was done primarily to test the Zen 4 compute dies (CCDs) and architecture using the existing, mature AM4 infrastructure before the AM5 platform was fully ready. * **The ""Hybrid"" Design:** To make this work, engineers likely paired the **Zen 4 CCDs** (5nm) with a compatible **Zen 3-era I/O die** (12nm) that supported DDR4 memory and the AM4 socket. * **Decision to Cancel:** Despite having functional 16-core Zen 4 samples on AM4, AMD chose not to bring them to market. They instead focused on the  **Ryzen 7 5800X3D**  as the final high-performance gaming upgrade for the AM4 platform because the 3D V-Cache provided a more significant gaming benefit on the older socket than a core-count increase."" * [https://www.youtube.com/watch?v=iM1NXHQ8YTA&t=2s](https://www.youtube.com/watch?v=iM1NXHQ8YTA&t=2s)",Neutral
AMD,"The shortage lasting until 2028 is probably the best case scenario here, unless the bottom falls out of the AI market entirely. Chip manufacturers might be able to catch up with current demand by 2028, but as long as the AI companies have effectively infinite money to throw around they can just keep buying up the new production capacity",Neutral
AMD,"And that's assuming OpenAI doesnt buy even more, or their competitors don't scoop up more to make sure they have enough.  The only thing that's fixes this is the AI bubble popping.",Negative
AMD,There are people who went to jail for the last price fixing scandal. I doubt anyone in power has forgotten. It was a pretty big deal,Negative
AMD,"Another ""fact"" you know nothing about. [https://www.pcgamer.com/hardware/motherboards/the-dominoes-are-falling-motherboard-sales-down-50-percent-as-pc-enthusiasts-are-put-off-by-stinking-memory-prices/](https://www.pcgamer.com/hardware/motherboards/the-dominoes-are-falling-motherboard-sales-down-50-percent-as-pc-enthusiasts-are-put-off-by-stinking-memory-prices/)  [https://www.tomshardware.com/pc-components/ram/bewildered-enthusiasts-decry-memory-price-increases-of-100-percent-or-more-the-ai-ram-squeeze-is-finally-starting-to-hit-pc-builders-where-it-hurts](https://www.tomshardware.com/pc-components/ram/bewildered-enthusiasts-decry-memory-price-increases-of-100-percent-or-more-the-ai-ram-squeeze-is-finally-starting-to-hit-pc-builders-where-it-hurts)     You point to the year overall and claim all is well. You do not bother reading the whole situation and ignore that there is a time lag between prices of ram going up and its effect being reported. It will get worse for Am5 mobo and CPU sales.",Negative
AMD,"TSMC upgraded several of the 7nm fabs to 4nm tho, so those production lines don't exist anymore.  Edit: and they are converting more currently: https://www.tomshardware.com/tech-industry/semiconductors/tmsc-ponders-upgrading-2nd-japan-fab-to-4nm-could-pave-the-way-for-more-advanced-chips-for-japanese-customers",Neutral
AMD,x3D production is not limited by lithography process but packaging which is a bottleneck right now and will be for at least several years.,Neutral
AMD,Anything capable of 7nm is probably capable of manufacturing ddr5. Ddr5 is 10-12nm lithography  I imagine it's backwards compatible and lower nm manufacturing is capable of producing higher nm components still.,Neutral
AMD,"I'm just paranoid about my tech failing when replacements aren't readily available. Also, any random piece of tech can fail regardless of the overall quality of the product line.",Negative
AMD,"Yes. We all are aware of that  Keep in mind, there are reasons why some epics have 3d vcache. Simulations and AI aren't ""very niche"" on desktop. They're not common, but it would be nice to have the option.  I don't know why anything optimized outside of gaming and streaming gets so many of you guys bothered when people say they want it.  Also, there are even games that want the cores AND the cache",Neutral
AMD,"First of all, please spare me your AI written SumUps, second of all there is still a difference between prrof of concept and production silicon. You can bet this combination was probably full of architecture bugs. Like they literally said in the snippet you send that they didnt go through with that for a variety of reasons, not only the narrow statement of the 5800x3d being better like you said.",Negative
AMD,There is a possibility of the bubble popping on AI.,Neutral
AMD,"We know it's nothing but bullshit. they knew about these contracts for quite some time, but still slowed production because of ""oversupply"".   Anyone thinking otherwise doesn't have a functioning brain.",Negative
AMD,"Doesn't matter as the current administration in the USA is letting them do whatever they want. If we want any action, the EU will need to bring them to justice.",Negative
AMD,"thats not how it works. the companies making DDR5 are SK Hynix, Samsung, and Micron (and a bunch of smaller players). TSMC's fabs aren't designed to produce DRAM or NAND, just more specialized ones like MRAM or RRAM",Neutral
AMD,> I imagine it's backwards compatible   Might want to look into things instead of using your imagination when facts are involved. Too much nonsense being spread online as if it's reality because people assume. Imagination should be left for creating things.,Negative
AMD,"No it isn't. TSMC 7nm afaik is an entirely new node and not a refresh thus it will need a redesign instead of just use the same design that they use for 10nm.  Also you typically don't use 7nm or lower for RAM since there is a lot less benefit vs making it on the bigger nodes. Basically RAM doesn't scale as well as logic (the density improvement for making RAM on smaller node is not great) thus why you don't see companies making DRAM on 7nm.  So when if someone want to make RAM on 7nm, they need to design it first and do all of the testing and validation process and then potentially have a product that is a lot more expensive. I will say that with the current price and assuming this kind of pricing last long, it might be viable (as in not losing money) to sell RAM that is made in 7nm process but still doing that would only add very little available RAM to the pool and the product itself is not going to be better.",Negative
AMD,"You’re imagining wrong, ram manufacturing uses completely different processes and TSMC doesn’t produce ram at all, it’s made in specialized factories by different manufacturers.  Samsung is the only manufacturer who has both logic foundry processes and memory manufacturing but in different fabs.",Neutral
AMD,Confidently incorrect,Neutral
AMD,Wat,Neutral
AMD,Cores AND the cache is exactly what you get with 5950x3d...  Which games btw?,Neutral
AMD,"I knew odds were high you would be too lazy to watch the video and the AI summarizing the video would prove I was not pulling it from my arse. But I think the real problem is it pissed on your oversized ego.   ""(W)ould require completely redesigned silicon"" is the problem with your claim. I was proving you were wrong. It does not need to be ""completely"" redesigned, but repurposed and bug stomped. Maybe figure out the meaning of the words you are using?  You also did not bother addressing the fact in my original statement that there is nothing to cannibalize if people cannot afford Ram for it. ""If it is too expensive to upgrade AMD will starve...""  The people with money that can afford the expensive DDR5 setups will benefit from those and the performance they offer while the potential customers will be able to afford AM4 setups that cannibalize old DDR4 builds. Think Ryzen 1000 series, Ryzen 2000 series and Ryzen 3000 and even Ryzen 5000 series that wanted upgrades but were waiting for a little longer and now cannot afford them because of DDR5 shooting up over 300%. Well they cannot afford to upgrade anymore, but they want to. This would give them a reasonable path. Think of the $100 steak vs the $500 cheeseburger. The $100 steak is reasonable and logical  when compared to the $500 cheeseburger.  It also builds customer loyalty and customer good will.  I really hate explaining logic to arrogant people that have proven there is nothing to be proud of. AS it stands I am probably not breaking it down small enough and I am sorry for that, but I am only willing to do so much. I hope you have a good night and please ponder on what I am saying without flying off the handle or accusing others of ignoring facts when they are things that are not even facts but illogical opinions you have.",Negative
AMD,This.  If the bubble pops before 2028 prices will normalize faster.,Neutral
AMD,If the AI bubble pops the US economy is screwed seeing how it's driven by tech companies as the majority of it's valuation,Negative
AMD,If the ai bubble pops we’re going to have bigger problems then buying ram. GPD growth is like .2% when you don’t count AI investments.,Negative
AMD,"In wich scenario AI bubble ""bursts""? all of a sudden millions and millions of people simply stop using it?  AI is here to stay, prices will level down, but it is going to be gradual.",Neutral
AMD,"> TSMC's fabs aren't designed to produce DRAM or NAND  DRAM is much simpler than what TSMC's fabs are designed for.    I think the gotcha would be the economics. DRAM might not necessarily benefit from TSMC's 7nm process very much, and it could be significantly more expensive to manufacture.   Those 10-12nm manufacturing processes used for DRAM don't need to be as advanced, but probably do need higher volume, and would therefore be more optimised for the cost of production over having the smallest features, or the highest performance.",Neutral
AMD,"I'm no expert, but I'm not spewing nonsense.  Perhaps you should take your own advice.  [This is basically confirming what I've said. New nodes have some amount of backwards compatibility built in to enable old designs to be migrated to new nodes more easily.](https://i.imgur.com/Q5zXzXp.png)",Neutral
AMD,"SRAM (used for cache on logic processes) is the least scaling part of a logic focused process that’s correct, but Dram doesn’t use SRAM its much simpler but slower and still scales down rather well.  DRAM manufacturing however is quite different from logic manufacturing and focuses on another set of characteristics.",Neutral
AMD,maybe ram prices will crash then :D,Negative
AMD,"""US economy"" is a bit more complex than market indexes. The money will just distribute to other industries",Neutral
AMD,"oh no the rich people number that doesn't actually go to anyone not in the top 5% will go down, how horrible",Negative
AMD,"It's worse than that, it's 0.2% when you exclude only data centers, not even the full AI bubble.  So even just stopping the production of data centers would send the US in an atrocious recession.",Negative
AMD,the money will just move from AI to gold/bonds to other stocks allowing better industry/market development outside AI,Neutral
AMD,"Which is still not how this works, producing dram is a completely different beast from producing CPU/GPU/whatever chips that actively compute.",Negative
AMD,Using AI summaries isnt reliable. It told me that Amari Cooper didn't retire and pulled Facebook articles as reference while linking an ESPN article where Amari Cooper stated he was retiring. It said there was no reliable source. Those AI summaries pull bad information and make shit up all of the time,Negative
AMD,"Holy crap, I'm so embarrassed for you right now.",Neutral
AMD,Your source is AI generated text? For real? WTF,Negative
AMD,"Yes, because the crash of a critical sector has never led to a catastrophic recession before. Oh wait, that’s exactly what happened in 2008 with the housing market crash genius.",Negative
AMD,"Except those rich poeple/companies employ everyone else. And what do yoit think all those mutual funds, pension funds, 401ks are investing in right now?  It will be worse than 2001 and 2008.",Negative
AMD,"Google's AI is also so insanely bad, feels like all it does is hallucinate.",Negative
AMD,"AI is not critical to anything, housing and (""too big to fail"") banking is",Neutral
AMD,"I'm OK with it being worse than 2001 or 2008. It's not like the boom economy right now is helping anyone - rents are at all time highs, food costs too much, people are still getting laid off regardless anyway, and we can't even afford distractions like PC gaming because they're stealing all the wafers for Sam Altman's ego",Negative
AMD,sure we'd all like to see amd pay lots of devs to improve / fix rocm but    I think they also need to do stuff like this to get people interested.    After all you'll always need people to tinker with it because they want to and    not because they are forced to in the end,Neutral
AMD,Smart move,Neutral
AMD,"AMD will do anything but hire more software developers to fix their stuff. Running a goddamn lottery here, this is beyond embarrassing.",Negative
AMD,Its cheaper than hiring actual developers.,Neutral
AMD,You do realize Nvidia did something quite similar to this right? It's the reason they're so crucial in all areas of AI now.,Positive
AMD,"but they have?  [https://github.com/ROCm/ROCm/graphs/contributors](https://github.com/ROCm/ROCm/graphs/contributors) there's a clear uplift in work being done  they've also been working on the build system to get better, quicker, smaller builds of ROCm. This isn't ROCm itself, but does enhance the user experience. An entirely different, parallel project. Sounds likee ""more software develpers to fix their stuff"" to me  [https://github.com/ROCm/TheRock?tab=readme-ov-file](https://github.com/ROCm/TheRock?tab=readme-ov-file)  also, you need proper data to know what's wrong  have you already complained about tesla cars providing training data for the self driving features? I'll write it for you  >Tesla will do anyhing but drive the miles themselves and create the training data to fix their AI model. Instead they're stealing MY footage so the car will learn to drive in MY area. Can't believe they're not just driving here themselves, this is beyond embarrassing.",Neutral
AMD,"This has been their MO for awhile. They release something way after Nvidia that isn’t as good, then hope the community does free work to make it useful.",Negative
AMD,Also gets people to use ROCm and see if it's a viable alternative for their use case. Might get some people to actually write code in other areas for Rocm.,Neutral
AMD,They're crucial in all compute areas because they bet on cuda two decades ago and started including it in every GPU. Not because they ran a lottery to fix bugs in cuda.,Neutral
AMD,"\> there's a clear uplift in work being done  An uplift that's 20 years too late.  \> have you already complained about tesla cars providing training data for the self driving features? I'll write it for you  No, why would I? That's a completely different issue. Tesla isn't dangling FSD above people's heads to entice them to improve it by working for Tesla for free. Your analogy makes no sense.  I am glad that ROCm is improving, but running a lottery is a shitty way to attract developers. As you stated in a different commit: run programs with unis and companies, give away stuff periodically, provide uni courses and pre-uni programs, etc.",Negative
AMD,"I hate to break it to you but most free software was done for free, at least they are offering quality barter.",Neutral
AMD,But why would you waste time and resources on rocm when cuda exists.,Negative
AMD,The only reason CUDA gained such prominence was because Nvidia literally ran programs giving away free GPUs to colleges and universities to get people familiar with the software and write extensive libraries for it for many use cases. The whole CUDA ecosystem relies on the work the regular Devs have done. Something similar to what AMD is doing now. You think this program is just to fix some bugs? It's to get people to actually use Rocm and learn it as well. The laptops are an incentive.  CUDA like you say has been around for 2 decades. And it still wasn't until the past decade that it's been so prominent. Part of that is because of the initiatives that Nvidia took.,Neutral
AMD,"\>Tesla isn't dangling FSD above people's heads to entice them to improve it by working for Tesla **for free**.  \>I am glad that ROCm is improving, but **running a lottery** is a shitty way to attract developers.  Nvidia really is cheapening on bots nowadays",Negative
AMD,That's a stupid question. You do it so you aren't dependent on one company with a black box technology.  ROCm is open source. It's got far more potential because it's customizable beyond CUDA. It's already very usable for large companies.  On the other hand nobody outside of Nvidia knows what's going on with CUDA. If they decide to pull the plug you're done.,Neutral
AMD,Because a single source of coding infrastructure for the fastest growing and largest industry in the world is a recipe for disaster. Especially when Nvidia is at the helm.,Negative
AMD,"There's a bit of a difference between a lottery with 20 laptops for fixing bugs and giving away free GPUs to familiarize people with the software. And by ""a bit"" I mean a fucking world of difference.",Negative
AMD,"They’d never pull the plug, they absolutely will charge a royalty on every product that is monetized on a CUDA foundation. They have no revenue streams left to conquer and that’s a big problem in a culture of constant unlimited growth.",Negative
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,80% faster but top end card has 2gb memory due to shortages.,Neutral
AMD,Can’t wait for 10060xt 8gb 550$,Positive
AMD,"Looks like I won't be buying any computer parts next. With new GPUs coming in 2027 (i have a rx7800xt current gen doesn't offer enough of an upgrade). Memory is too expansive to justify upgrading to AM5 or Intel. SSDs are expansive, monitors are atleast somewhat interesting though i got a 4k monitor 165hz monitor last year. Well I guess AI has killed cosumerism in the PC space. The only thing affordable is peripheals.",Negative
AMD,I thought the next thing was UDNA?,Neutral
AMD,"Ok so mid 2027 ram prices will be ""down"" to the new ""affordable"" normal.  So next 18 months will be dead period for pc building.",Neutral
AMD,RDNA 4 EoL by 2028 then.,Neutral
AMD,"No point launching it in 2026, that's for sure.",Neutral
AMD,I thought RDNA was dead and the next architecture is going to be UDNA?,Neutral
AMD,"That's a long time without a flagship card. The fastest they have now is the 9070XT. I have one and I'm happy with it, but I can't deny that it's like a 5070Ti at best and in pure raster, the 7900XTX still outruns it. Nvidia has two whole tiers above it and who knows what they will come out with before RDNA5 arrives.  Before the memory apocalypse hit, they could have allowed AIB's to slap 3GB chips on the 9070XT or let them clamshell it for either 24 or 32GB cards. Instead, they decided to make that a workstation-only option.",Positive
AMD,"If it's true, please AMD do not fuck this up.  You have one chance to not fuck this up and take Nvidia off the top, please, please, please don't fuck it up again.",Negative
AMD,It looks like I'm in no rush got the RX 7800 XT during the beginning of the year and prices going up on NAND chips are going to say a lot on the price of the gpu's in 2026.,Neutral
AMD,Thank God I bought a 9070 xt with how these GPU are being produced etc and the scrapers in between more like 2028 for half the people in the world,Positive
AMD,"I’m excited for future advancements, and i am curious what leaps and bounds are made for ARM in the next 5 years. For now, I’ll just stick with my 6950xt, but here’s to hoping nvidia drops the ball and we get good competition in 2027/8",Positive
AMD,Whatever happened to rdna4 being a “stopgap” (just like rdna3 was supposed to be) and the follow up will be fast and with a proper high end?,Neutral
AMD,My 6800XT will be fine until mid 2027 then.,Neutral
AMD,What happened to UDNA? There was lots of stuff like year ago that RDNA4 is last one and they make UDNA instead and was slated for 2026 or so?,Neutral
AMD,"With how long generations have become, I really think it was a mistake to skip the high-end this time. The 7900 XTX released in 2022, and it's basically going to sit without a successor for 5 years. We've really only gotten a full AMD product stack in the GPU market twice in the last 10 years (RX 400-500 didn't go high enough, while Vega had just 3 products).  I'm happy with my 9070 XT, but it's kinda hard to argue in AMD's favor when we're talking a 5-year gap where high-end buyers have no upgrade path with AMD.",Neutral
AMD,End of driver support for RDNA3 24 hours before.,Neutral
AMD,I have 7900xtx and 7950x3d and it sliced through any games at 1440p. My plan was to upgrade when AMD releases their next kind GPU. Was hoping that would be in 2027?,Neutral
AMD,we having 9070 xt 2 more years really :S,Neutral
AMD,Hopefully this will work on the 9000 series or prob not cause you know how AMD is,Neutral
AMD,praying for my 3060ti to hold out till then,Neutral
AMD,You mean UDNA,Neutral
AMD,wow that like 100 years from now. Gonna use this card forever,Positive
AMD,"Let's hope AMD makes the right decision to actually make their new APUs which this architecture instead of using RDNA4 or god forbid, RDNA3.5 again.",Neutral
AMD,"I need a halo product, with a die size of at least 600 mm\^2",Neutral
AMD,"Good god.  A 2 year wait between releases.  If that ends up being true, this might be dead when it gets here.  Still don't know why they didn't make a flagship with the 9k series/RDNA4.  Maybe chiplet tech flopped out on GPUs, because I'm pretty certain all of the 9k series are monolithic.",Negative
AMD,"Another RDNA can gtfo, I want UDNA to go head to head with Nvidia stuff.",Neutral
AMD,By mid-2027 game will be rendered in real-time by AI. No raster or RT. Those who think this is pie-in-the-sky hasn't seen what can already be done. In 18 months this will be extremely apparent and a waste of time getting a  traditional GPU,Neutral
AMD,Time to bring back the ole Nvidia 6200 “Turbo Cache” tech and push the ram shortage back into your system ram. /s,Neutral
AMD,"probably 32gb because the memory bubble will've popped and they will get it for half the price it cost 6 months ago since the memory makers will have to get rid of it, then they can charge the consumer more money for the card.",Neutral
AMD,They must be using Apple memory then since Apple memory is larger for the same amount. Apple said so.,Neutral
AMD,"2GB. No problem, I know how to use it.  Please release it in 26Q3.",Positive
AMD,Assuming they don't change the naming scheme again.   Radeon AI RX 395XTX Pro AI Plus,Neutral
AMD,No way they use GDDR6. If they're going to cheap out 9GB makes more sense. That would allow them to use one less module and have one less PHY. That would be super scummy though. If they drop the 60 series to 3 PHY I'll have given up.,Neutral
AMD,I feel like monitors coming down is the only positive thing in PC these days. Soon a 4k OLED 240hz panel will cost less than 64GB ram!,Positive
AMD,You can spend all of 2026 working 2 jobs to save up for new parts for 2027!,Neutral
AMD,"I feel like there is a definite plateau for gaming we're in now. I think PS5 will last for a LONG time as sort of a ""Series S"" entry level for Sony and games will be at least somewhat optimized for that.   So anything that's around your 7800XT or 4070TI level raster right now will last well into 2030s as a viable gaming machine.   Back in my day a 2 year old GPU might not even launch your game, let alone give you good performance. Stalker, doom 3, crysis. Even current Gen for the era couldn't get you solid 60 FPS in those.   Nowadays? I modest 6700xt or a 3080 from 5 (!) years ago paired with something like a 5700x or a 5600x3d will give you great gaming experience if you set graphics to medium/optimized. Even UE5 games are getting better. Stalker 2 patch 1.7 is WAY more perfromant than on launch and they teased an engine upgrade. Everything is just so scalable and flexible now.",Positive
AMD,With a 7800XT and int8 fsr you're set for a while,Neutral
AMD,So happy I pretty much fully upgraded my PC this year. However I sadly didnt do my ram so thats a bit cooked.,Positive
AMD,"You know what?  A 7800XT is plenty of card to have a fine old time playing video games.  There isn't a single game out there it won't run well enough to have fun playing, and many thousands in the back catalogue that it can run absolutely maxed out.  The RAM famine and AI and all that is a bullshit situation, but if there's one benefit we can take from it, it's to stop worrying about hardware we don't have, and just enjoy the hardware we do have.",Positive
AMD,"I have a 7800xt to and the performance still surprises me everyday, I’m big chillen till rdna 5 are at a good price.",Positive
AMD,"While i agree with most of your points, the jump from a 7800xt to a 9070xt was extremely noticeable for me. Gained up to 50fps in some titles in 1440p.",Neutral
AMD,"yeah 5800x3d/7900xtx here, only thing I wanna upgrade is the CPU. New Mobo, Ram, CPU, maybe case depending on AIO needs; just too expensive to realistically upgrade   edit: 5800x3d cant sustain 240fps or higher 1% lows in CS2",Negative
AMD,"I’m also rocking a high refresh rate 4K monitor on a 7800xt. I’m very curious about your experience playing games at native or upscaled 4K on your card. What games do you play native? Which upscaler do you use when the need arises? I’ve been experimenting with fsr 4 int8 in performance mode, I’m still salty that Amd hasn’t officially released a version of fsr 4 for rdna 3",Positive
AMD,"Yup. I have a system that I built in 2023. It's a 7800x3d, 32GB ram, 2TB Nvme and a 7900xtx. Looks like it will continue to be unchanged for another 2 years",Neutral
AMD,Rumour mill has been flip flopping on the name for a while. RDNA5 and UDNA are the same thing.,Neutral
AMD,UDNA and RDNA5 are interchangeable for almost all media reporting.,Neutral
AMD,"Just be mindful that a lot of rumors don't become true. Especially this is videocardz we're talking about lmao. If you check the article there isn't any branding even, just a rumored release date. It was videocardz' prerogative to label it RDNA5. This might literally be UDNA but people who only read titles will think it isn't because for some reason they attached RDNA5 to the title.",Neutral
AMD,The last I heard it was UDNA and it was going to mass production in Q2 2026.,Neutral
AMD,"CDNA-Next and RDNA5 are the first iteration of UDNA where both architectures share the same design and features.  So, expect a wider CU with equal INT/FP processing capability. Most likely 4xSIMD32 or full 128SPs or what a previous RDNA WGP currently is. I think the extra FP32 ALU has spawned a full SIMD32 with INT support to eliminate the restrictive implementation of dual-issue FP32. 2xSIMD32s (essentially 1xSIMD64, but not really) might be paired and issued instructions simultaneously to maintain compiler compatibility for dual-issue or to execute 1-cycle wave64; otherwise, each SIMD32 can be issued instructions every cycle in wave32 like any RDNA GPU. RDNA's GCN-compatible CU mode (1xCU or 64 threads) will be discontinued and RDNA's WGP mode will become new CU mode (128 threads).  CDNA-Next will likely have 4xSIMD64 with virtual 8xSIMD32s because it lacks graphics engines and has more transistor budget. This will support 1-cycle wave64 along with full-rate FP64, 2xFP32/INT32 (packed and independent via virtual SIMD32), 4xFP16 and so on. CDNA compilers use GCN's 64-threads, so 1-cycle wave64 can be executed 4 times, not unlike 4xSIMD16 where 4 cycles were needed to accumulate and execute 64 threads. Work can be up to 4x faster. The real throughput increase is in the matrix cores, likely supporting a full 16x16 matrix or 8192 ops/cycle for 16-bit and 16384 ops/cycle for 8-bit. 32768 ops/cycle for 4/6-bit.   Consumer hardware throughput will be cut in half. FP4/6-bit throughput could be locked to 8-bit to save transistors in consumer hardware too, while INT4/6 is full rate, but if AMD moves FSR to transformer model, it'll need full FP4/6 output.",Neutral
AMD,This site and a number of other rumor sites have been mis-naming it everytime I have seen them write about it for months,Negative
AMD,I think UDNA is AMD’s version of CUDA. So it would be used for AI upscaling and whatnot.,Neutral
AMD,It most certainly is. Videocardz just called it RDNA5 for some reason,Neutral
AMD,Keyword: “I thought”,Neutral
AMD,very likely,Neutral
AMD,It is extremely rare use case that a game needs more than 16GB VRAM.,Neutral
AMD,"At this point the only way Nvidia gets removed is if it removes itself. Whatever advancements are made with RDNA5 are going to be the same as current Nvidia tech, they're that far behind. But Nvidia might be intending to retire, seeing that rumor that they're cutting down on GPUs production by 40%.  They are literally leaving slowly the scene. AMD is gonna be at the top by default unless they massively fuck up",Negative
AMD,"Even if they do, they still make a lot of money selling this tech to consoles",Neutral
AMD,this is said every gen,Neutral
AMD,"What chance? For example atm, 9060 xt vs 5060 Ti and 9070 xt vs 5070 Ti, you choose based on your needs, gaming is your focus? then pick up AMD as it offers good price to performance, need CUDA too, well go NVIDIA.  I don't understand this argument of ""do not fuck this up"" or ""never miss a opportunity"", sure launch prices were all over the place, but after a couple of months, most GPUs just got to their MSRPs and it was up to each person to decide what was better...then AI popped up and we're screwed up now.  This is all very simple, as i mentioned, gaming go for AMD and on a ""budget"", if you're able to pay premium or need CUDA, go NVIDIA.",Neutral
AMD,"ain't no way homie, AMD only just now catching up to a compute focused architecture. I \*highly\* doubt the first gen is going to compete with NVIDIA at the top. If anything it'll be akin to Intel's 285K processor. Good gains in compute maybe even ray tracing but raster will probably suffer. Which is fine, people who game at 1080p will be fine. People playing on 4k might be miffed...  Anyway, tl;dr: first gen architecture always seems to suck =/  Is there a first gen AMD architecture that didn't suck? With the first AMD dual core CPUs as an exception, probably?",Negative
AMD,"What do you mean?  That's already what happened.  The RDNA4 lineup was clearly pretty quickly put together, with Navi 44 almost literally being a cut in half Navi 48.  And all with monolithic dies that are easier to design/make.    RDNA3 was never supposed to be a stopgap, as it was a pretty extensive architectural overhaul.  It just wasn't a good one. lol RDNA3 also had a full top to bottom range of GPU's and products.  RDNA4 ironically is a pretty decent shakeup in architecture as well, but this time it was successful, except AMD perhaps didn't expect it to be as good as it is, so they didn't make plans to take advantage of it with a full range lineup.    They seem to have put more eggs into the RDNA5/UDNA basket in terms of product plans.",Neutral
AMD,"Yeah, I'm about to say the same for my RX 6700 XT.",Neutral
AMD,It's entirely possible that people are just using RDNA5 as a placeholder name as the obvious next in line from RDNA4.    It's really not important what it's called at the end of the day.,Negative
AMD,I dont think AMD expected RDNA4 to be as good as it was.,Negative
AMD,"Why would it be DOA?  What's the competition in that timeframe?  As for your second question, I'm convinced it was to free up advanced packaging throughput for the MI300 series.  A big RDNA 4 card would have had a very complex design, requiring the same kind of advanced packaging that the MI300 and later chips have, meaning each big RDNA 4 package AMD made would be a large fraction of an MI300 package that they *couldn't* make.  The difference in profit margin between the two is immense.  That's the real reason they scrapped chiplet RDNA 4 and went monolithic.    They didn't go larger than Navi 48 because that would have taken a lot more time.  They already had Navi 44 designed, which would have been the only monolithic RDNA 4 chip originally.  They basically mirrored that design allowing them to double the CU count fairly easily.  Going bigger would require a lot more design work, all for a chip which would be in a price class that less than 5% of gamers buy into.  So their claims about targeting the more populous part of the market were half true - that explains why they didn't try to make a larger monolithic part - but they concealed the reason for moving away from chiplets, which I contend was packaging pipeline contention with the MI300 series.",Neutral
AMD,I think it's just naming differences at this point.,Neutral
AMD,RDNA5 and UDNA is the same thing.,Neutral
AMD,"What exclusive Nvidia stuff are you referring to? As far as gaming goes, there's not much you're gonna miss by going RDNA4.",Neutral
AMD,"I've seen what can be done.  We're not even remotely close to being able to do what you're saying.  You're buying into delusional claims by AI companies, but they are only talking to shareholders.",Negative
AMD,"I don't think pure ai graphics will ever take off, it halluciantes way too much for text alone lmao",Negative
AMD,"By what technology? Games are already using statistical modelling to render games to significantly increase performance.  If you are talking about GPTs, these require mid-range levels of performance and much higher amounts of VRAM to render video and they aren't able to maintain concurrency for more than a few seconds.  To do what you're saying will require everyone to have 32 GB of VRAM and 64 GB of system RAM and your video game won't be able to follow any kind of internal consistency after 15 seconds (and maybe slowly increasing that length at the cost of performance) Input delay will also be very very high.  Sure things could change in the future, but that will require a novel ""AI"" technology that does not exist at all.",Negative
AMD,The computer cost of real time AI rendering is far too high for that to be realistic that soon.,Negative
AMD,I think your vision is correct but the timeline is off. That’s a 2029 situation imo,Neutral
AMD,"Vega had HBCC more recently, but because it fragmented data into 64KB pieces to saturate (then) PCI 3.0, it's incompatible with many modern engines' texture streaming behaviors. Failure to launch or hard crash during gaming could occur. I tested it out when I had Vega64, and when it worked, it was pretty brilliant.  I'm all for unifying memory because RAM is still faster than NVMe drives. System RAM can just be a large LLC for a GPU. We'd need to move away from chipset muxing though, as PCIe bus will be saturated as data is moved between RAM and GPU VRAM. Or we could move away from PCIe entirely ...",Neutral
AMD,"ATI also had ""Hypermemory""",Neutral
AMD,"I don't think you realize part of the reason RAM prices are skyrocketing like they are. Part of the reason is because the manufacturers are not drastically ramping up their production. There will be no ""crash"" with a flood of supply. RAM makers had this happen to them before and they aren't allowing it to happen again. Production will remain steady, with a slow increase in supply. But not enough to cause a ""flood"" *if* datacenters decide to stop buying RAM for some reason.",Negative
AMD,Too soon for that. nvidia already has a lion's share of tsmc booked out for 2026. I'm willing to bet the steam won't run out until 2027 or even later. Odds are the prices won't deflate much for another year or so after that.,Neutral
AMD,"You joke, and I disagree about them charging out the ass for desktop/laptop RAM, but my iPhone 11 performed way better than my S20FE, with much less RAM.  As someone who developed for both Android and iOS before, it’s purely because Apple’s kernel is much better at managing app memory, at the cost of having a more restrictive API to work with. Same reason why Apple devices are able to have better battery life.",Positive
AMD,"You know it’s bad when that ^ doesn’t sound too unrealistic, based on how they’ve been naming things lately",Negative
AMD,It's freaking crazy and freaking exhausting with all those non-sensical naming schemes. I really have no clue what kind of data they're following that shows them that these abominations are supposed to be good for sales.,Negative
AMD,"You know they're gonna be like ""6070 XT"" going backwards to match NVIDIA's 60 series launch LMAO.",Neutral
AMD,Soon? Soon is now. Mid tier 64gb ddr5 kits are a bit more than the more stable 32” 4k oled 240hz monitors.,Neutral
AMD,"Also demand might be lower since fewer will be able to afford a system capable of running 4K, further reducing prices.",Neutral
AMD,That is now. I literally bought a 32 in 4K 240 hertz MSI OLED for less than my 64 gigs of RAM I have.,Neutral
AMD,"With how turbofucked the prices are looking, we might have to pick up three jobs and a side hustle.",Negative
AMD,If recent next gen PlayStation rumors have any merit then the PS6 handheld is suppose to be around a PS5. Sony already has been make moves in the developer side to make version of their games that run in a supposed lower power mode.,Neutral
AMD,"Same, I jumped on 64Gb DDR5 last year and have never felt better about a purchase",Positive
AMD,How’d you fully upgrade your PC without a ram bump? Still DDR4?,Neutral
AMD,"There are some RT titles where you're 100% correct.  However, there are also some RT titles where the 9070XT loses to a 4070. I'll wait until AMD has a generation that performs consistently across a range of RT titles before I spend money on upgrading specifically for RT performance. I'll be surprised if that card ages any better than any of the RDNA cards have so far.",Neutral
AMD,Doesn't mean much if it is an esport title. Personally I don't do upgrades unless I get about twice fps. Coming from 6800xt 9070xt is mostly about 30% faster which is not enough.,Negative
AMD,I agree but there isn't an AAA game which has Ray tracing that is worth my time for the last 2 years.  The last time a game put my 7800xt on its knee is Cyberpunk 2077.  The rest of the games I play are plain o raster games.,Negative
AMD,"Almost the same setup, just a 7900xt. Was weighing the 9070XT, so my son can get the 7900XT and I the new one, similar performance at least.  And I have the 6950xt as backup.  But I already decided to wait for AM6. AM5 is all good, but with the current price and AI shenanigans..  it makes no real sense to plan for anything really.  I mean, I also don't buy at first day and wait for a few months to get my hardware.  That way at least the first issues are gone.  But really - the 5800X3D is so good, it will give me some more years im sure.",Neutral
AMD,I had that setup and went AM5 with 9800X3D + my XTX now and its a nice gain. The 5800X3D did produce a small bottleneck on the XTX.,Positive
AMD,>It was videocardz' prerogative to label it RDNA5.  Mark Cerny is the one who said RDNA 5 in an interview about Amethyst most recently. Nobody from AMD or Sony has called it UDNA publically for months/years AFAIK.,Neutral
AMD,amd themselves have called it RDNA5 and UDNA  im not sure the name is set in stone yet.,Neutral
AMD,"Rdna 5 is Udna, just renamed for their gaming brand.",Neutral
AMD,Videocardz will literally post three different stories in a given day with three different conflicting rumors.  I wonder sometimes why they're even allowed here. I suppose because a broken clock is still correct twice a day...,Negative
AMD,If the architecture's focus is on compute maybe they'll be making the pro cards first and consumer Radeons later? Seems to be the pattern these days.,Neutral
AMD,"It is not.  It's just a label for the decision to unsplit design between data center and gaming.  Rather than RDNA for gaming and CDNA for data center, it's UDNA for both.  RDNA 5 is what the gaming part of UDNA 1 is being called, even if for no other reason than the fact that AMD hasn't made any official statements about it.",Neutral
AMD,"Mark Cerny is calling it RDNA 5 in interviews, and Sony is heavily involved in this generation. It's a ""he said, she said"" situation at its core, and there's no reason to believe Cerny right now. I don't think ""UDNA"" has been said by anybody publically for years.",Negative
AMD,"True but It's though to ""downgrade"" back to 16GB if you are coming from a 7900XT or XTX with 20 or 24GB.",Neutral
AMD,"This is AMD.  They are famous for fucking up their GPUs in both launch, features and pricing.    This is one of those rare moments that AMD can actually take a percentage away from Nvidia.",Negative
AMD,"Imo, i need them for gaming. Nvidia can have AI. Question is if AMD wants gaming...this generation is the best AMD had in years.  Better RT, MASSIVE UPSCALE IMPROVEMENT, efficient cards with lots of memory and reasonable prices. AMD did not have cards this good for a loooong time, definitely their best showing in the last 10 years",Positive
AMD,...and they will. When has AMD ever failed to fail?,Neutral
AMD,No they're already envolving ahead of NVIDIA with RDNA4 on some fronts (Dynamic VGPR and OBBs fx)  just overall behind. RDNA5 will prob be forward looking like GCN without all the architectural flaws.   NVIDIA will never abandon gaming while Jensen Huang is CEO.,Neutral
AMD,"If you are ecosystem locked, you are ecosystem locked, if you are not, you should always go for the better value.",Neutral
AMD,Rdna4 was supposed to be a stopgap and a short generation and the successor was supposedly to come sooner than normal. But now this rdna4 generation will last over 24 months with only 2 dies zero mobile chip zero high end sku.,Neutral
AMD,"> except AMD perhaps didn't expect it to be as good as it is, so they didn't make plans to take advantage of it with a full range lineup  If the leaks are to be believed, it was actually the opposite. RDNA 4 was a big improvement over RDNA 3 in efficiency and it was great for mid-range performance, but it simply didn't scale. They never got Navi 48 to produce performance to rival the 5080 at acceptable power levels, so they just gave up and used it to compete with the 5070 and 5070 Ti.",Negative
AMD,"fuck man u still on am4?  i feel like jumping from the 6700xt to a 9070 - its a pretty big jump in performance and i play to move to a 27"" display... ugh i feel like i'm in no mans land LOL",Negative
AMD,"I really don't think that's an issue, unless they really thought it would suck.  It's about 50% faster than a 7700 XT, but RDNA 4 released 2.5 years after RDNA 3, so that's not a crazy level of advancement. It's sold at $600+ and compares most closely to the 7900 XT, which has been in the $700-800 range since its disastrous launch at $900 in 2022. Similar performance (2 FPS faster in Hardware Unboxed's launch review, at 1440p) and less VRAM for $200 less is a pretty tame improvement after such a long generation.",Neutral
AMD,"The Nvidia 5000 series released early in 2025.  There isn't anything in the AMD 9000 series that really brings a big fight to the 5080 or the 5090, so they are that much further ahead for flagship cards.      I find it highly unlikely that Nvidia engineers are going to sit around and patiently wait on where the next generation of AMD cards will land in terms of performance, meaning they will probably continue iterating the Blackwell architecture and have something ready for release or at least leaked or advertised by the beginning of 2027, when AMD is still 6 months away from a release.      The 9070 XT still only comes in at like 5070 TI performance.  So with this next AMD GPU coming in and hopefully entering the flagship fight, I just see trouble for AMD.  I mean, hopefully I'm wrong, but essentially Nvidia has all of this extra time to iterate on an already faster product to retain its current and past positioning, if the new gpus and architecture isn't going to hit the market until the middle of 2027.",Neutral
AMD,A full fat tensor core equivalent. Accelerated bvh traversal. Im probably using the wrong terms/words but I know RDNA4 doesn't have the full equivalent of a tensor core.,Neutral
AMD,"I mean current Nvidia GPU's can already perform 91.75% of a game's rendering using AI alone, with Ultra Performance DLSS 2/3rds of the pixels are dreamed up and with 4x FG, 75% of frames are dreamed up. Quality aside,  between the two of them the conventional graphics hardware is only doing 8.25% of the work required to draw the content",Neutral
AMD,"I don’t think HBCC ever got enabled, did it?",Neutral
AMD,Well yes. But Apple claiming that their 8gb is somehow worth 16gb on a normal PC is complete horseshit.,Negative
AMD,And 1440p OLED is cheaper than 32 GB. World’s gone mad!,Neutral
AMD,Now it makes more sense: a handheld! I kept wondering why Sony was funding a WMMA INT8 PSSR 2.0 API and SDK when future AMD hardware will be WMMA FP8 and WMMA FP4/FP6.  I suppose it also serves as a drop-in AI/ML upscaler for all hardware from PS5 Pro and newer.,Neutral
AMD,"Same went from a 5800X3D sys with 32GB to a 9800X3D 64GB last year since DDR5 was down. I essentially wanted to have a PC that was ready locally to run OpenSource AI models, Heavy Sim games, and run a WorkVM while playing games at the same time with minimal slowdowns and so far it’s working well for me. The one thing I’m really kinda hoping for is that maybe with RAM so high the price will drop on a 9900x3D or the 9950X3D won’t be so expensive.  Was excited jumping from 4 cores to 8 with a 8350 to 3700x and I kinda want to jump from 8 to 16.",Positive
AMD,Nah I hopped on ddr5 very early and upgrade relatively frequently,Positive
AMD,It’s also worth seeing if fsr 4 actually becomes a thing or if it fades away.  Because that either takes away or adds to the dlss benefit.,Neutral
AMD,I feel like those titles heavily favor Nvidia RT/Path Tracing anyways. I made the jump from a 7800XT and the 9070XT  is a night and day difference especially in RT. Given a more hardware agnostic RT solution the 9070XT is a bit better than the 4079 though you can always prove me wrong.,Positive
AMD,"Yep, the 9070 xt is amazingly priced for what it is. I just wish there was a 9080 or 9090 from AMD. I paid right under $800 for my 6800 xt during the price mess before. Now that same $800 only gets me a 5070ti which is barely faster than the $5-600 9070xt. It's just not very economical for me to upgrade at this point.",Positive
AMD,Nope. I get avg 35% fps boost from 6800xt to 9070 non xt. 9070xt is about 10-12% stronger.,Neutral
AMD,"My 6800 xt will have to last me until rdna5 releases,  no point upgrading now to a 9070xt when everything i play runs well with my rig (next upgrade will be rdna5 gpu if good and a X3D cpu)",Neutral
AMD,"Cyperpunk is good, control is also pretty crazy but it doesn’t help that there is no FSR.",Negative
AMD,"Just an FYI, you'll be waiting another 4-5 years for AM6",Neutral
AMD,I mean that's sort of what Nvidia does already no?,Neutral
AMD,"More so if AMD like money they will be focused on the pro cards. NVidia isn't doing the same because they hate consumers, they just love money.",Neutral
AMD,XTX here...the microcenter $579 deal for a 9070xt has me flip flopping on the purchase. I know 16GB is fine for everything I do but it still feels bad to downgrade the VRAM 😆,Negative
AMD,How did AMD tuck up 9070XT launch?,Neutral
AMD,Please AMD marketing don't make RDNA5 into a steaming turd.,Neutral
AMD,"Navi 48 was a rush job (as the name implies), where they just doubled the 44, Navi 41 was a chiplet design that failed",Neutral
AMD,also it should be noted that these chips are designed years in advance. they might or might have not expected rtx 50 series to be such a small upgrade.,Neutral
AMD,"Yes sir! I'm on AM4 with a Ryzen 5 5600, 32 GB (2x16) DDR4 3200 MHz CL16 playing games on 27"" 1440p.  I also would like to make the jump to the RX 9070 XT, but to squeeze  the most out of that upgrade I would need to also upgrade my CPU to e.g. a Ryzen 9 5900 XT. The X3D CPUs are not sold anymore.  And the nail in the coffin is that I do not have any upcoming games I need the before mentioned upgrades for. Potential games I could need an upgrade for is DMZ 2 (COD/MW) and GTA 6, but DMZ 2 is not confirmed to be released and GTA 6 is released in 2027 for PC at the earliest.",Neutral
AMD,My pc runs every game and app perfectly fine. My last pc lasted 10 years. I didn't think this one would but it's trending that way. I'm probably going to wait for zen7 at this point.,Positive
AMD,Perhaps you should poke your head up and look at the DRAM situation.,Neutral
AMD,"Afaik the tensor cores handle the (1) DLAA, (2) DLSS, (3) ray reconstruction and (4) frame generation for RTX 40 and 50 series.  RX9000 (RDNA4) now also have the necessary hardware inside to do all of these 4 computations under FSR Redstone launch. There might be slight performance difference with Nvidia being faster, but for the most part it is decent.",Neutral
AMD,I think you’re confusing interpolation versus true generation. Interpolation isn’t nearly as taxing as true generation.,Neutral
AMD,"There's a WHOLE lot more to graphics than just pixel output and framerate. :/  And if we still need games to render like 50-60fps to get any kind of decent base for frame generation, that's still a lot of performance required to get there in the first place.   Very different to talking about starting from 1 frame and then extending that out to 60-120fps+ frames.",Negative
AMD,"HBCC was enabled as a driver toggle - I remember seeing it and having it turned on on my Vega56. I dont think I ever used enough VRAM for it to matter though. Games didn't use that much VRAM back then and the compute loads I ran (most SETI@HOME) were designed around 2-3gb cards, so never pushed the memory much either.  The feature they never enabled was the primitive shader IIRC.",Neutral
AMD,"HBCC was a driver-level option with adjustable memory amounts capped at 1/2 of system RAM, and it was available in any standard driver.  Primitive shaders were never enabled in Vega. Microsoft ended up choosing Nvidia's mesh shaders implementation for DX12 anyway. Not many games are using it, unfortunately, because devs are having performance issues or are not getting expected performance improvements that were claimed vs traditional pipeline.  Fun fact: PS5/Pro use AMD's primitive shaders as Sony developed their PS5 SDK around AMD's solution before Microsoft standardized Nvidia's mesh shaders.",Neutral
AMD,That’s what I said in my first paragraph.,Neutral
AMD,"I also own a 7800XT, so the DLSS argument is already written off for me",Neutral
AMD,"The tech itself is  great, but the adaption might be lacking. I've been using optiscaler to run FSR4 in dlss/xess supported games, since so few games support FS4, and it's been working great.",Positive
AMD,">Given a more hardware agnostic RT solution the 9070XT is a bit better than the 4079 though you can always prove me wrong.  Is your solution not to play games that favor Nvidia GPUs then? Because that's already what I'm doing with my 7800XT, and I don't need to drop 850CAD on another GPU with the same handicap (even if it's improved a lot). Rumors suggest that RDNA 5 will be **way** better at RT than any previous AMD generation, and even if they don't pan out, I'll pick up a used 9070XT once I can get one for <500CAD, like I did with my 7800XT this year.",Positive
AMD,"Agreed, I can go to microcenter and scoop up 9070xt for $579 +tax but it doesn't seem like a *big* enough performance leap from my rdna2 card in ultrawide. Be nice if there was a XTX binned variants or a teir above.",Neutral
AMD,AMD missed out on having a high end competitor this generation. An $800 9080 that competed with 5080 would have sold very well. That probably would have been my upgrade instead of a 5080 tbh.,Neutral
AMD,You are right. Still not enough of a jump for me. RDNA5/UDNA will maybe do it if they make high end tier.,Neutral
AMD,I played control at 1440p with low or med rt and still get ard 70 to 80 fps  Still less heavy than cyberpunk.   Cyberpunk with medium rt I only got 50 to 60 fps. I needed Xess quality to bring it up 60 which occasional 50+ dips,Neutral
AMD,"Sure? Wasn't the next gen supposed to be the last AM5 and the new AM6 with DDR6 is planned for 2027/28?   But generally, yeah, I know it's still some time. But with the next GPU gen coming in 2027 ... I doubt there will be much reason for a CPU upgrade anyway. With my 7900XT I doubt the 9800X3D would be much of a difference, as I play in 3440x1440 :)",Neutral
AMD,"The only way I could think of is that it's only a mid-high end card. They didn't release a 9080/9080xtx. But, there's probably reasons outside of my current knowledge as to why they made the choice.",Neutral
AMD,damn i got lucky.. i went 1600x 3600x 5600x  but i sold my 5600x when i saw how insane the 5800x3d was lol...  i could get the juice out of the 9070xt or a higher end nvidia card but i dont want to upgrade my power supply lol....    honestly im looking at used 5070's right now... i usually dont buy nvidia but this card poops on my 6700xt and its getting cheaper...i hope i don't regret not getting  more vram... i guess nvidia will release 50xx supers next year...im hoping to skip am5 all together before my next big build,Neutral
AMD,word... i mean my mother board is almost 8 years old lol.  i've updated my cpu 4 times and my gpu twice... this will be my last update if i do it.  pretty happy with my 5800x3d still.   i just hate hate how much i need to spend to make a gpu update worth it.  might just go the used gpu route,Negative
AMD,I get that DRAM makers are leaving the consumer space to jump on the machine-learning cashcow.,Neutral
AMD,"It's AI powered interpolation. One way or another, the conventional graphics hardware is only providing less than 1/10th of the data presented on the screen, even if that 1/10th requires a massive amount of effort to produce",Neutral
AMD,have you tried FSR4 INT8? I have been using that on my 7900 XT for several months and I am very happy with it,Positive
AMD,"Nah, I'll just settle and lower RT and/or use quality settings. I'm honestly good enough if a game hits 60@1440p if everything is maxed and even in some Nvidia RT titles I can set RT to medium and still be above that.",Neutral
AMD,"I understand. For me, the 9070's power consumption was a big change. 230W on stock PL on my Asus. Add to that UV at the same PL, which works like OC because it increases the clocks - giving another 5-6% performance. On the 6800xt, in order to achieve a reasonable power consumption of around 220W, it was necessary to significantly reduce the clocks (fixed at a maximum boost of around 2100 MHz on the core), which cut performance by another 5-8%. I tested this thoroughly. So in this scenario, with both cards operating at 220-230W, the 9070 is a card that is well over 40% more efficient for me. Sometimes 50% (e.g., GoW Ragnarok). And that makes a difference. There is also a new manufacturer's warranty, because the previous one expired on my old card (I now have an Asus Prime 9070, previously a TUF 6800XT). After selling my 6800xt, the cost of the new card is $250. So, nothing but advantages.",Neutral
AMD,Zen 6 comes out Q4 2026 or possibly delayed to Q1 2027. Zen 7 is also going to be on AM5. So at the earliest you're looking at Zen 7 Q4 2028. Meaning AM6 2030. And have fun with a new platform adoption.  DDR6 isn't even going to be pushed to server until sometime late 2027 (if we're hopeful). And it's usually 18 months before you start seeing it on the consumer side anyway.,Positive
AMD,That's whyAMDis king. Intel would never let you get so many upgrades in. I bought at the tail end of am4 but am overall very happy with my system still,Positive
AMD,"Then you should realize that also affects graphics cards, which rely just as much on that DRAM manufacturing capacity.  nVidia likely isn't going to be making any new gaming cards anytime soon as well, so it's not like AMD will be leaving the market without competition in the current generation.  The only cards they don't have an answer for are purchased by a vanishingly small percentage of the PC gaming market.",Neutral
AMD,Yes but even AI interpolation does most of the work with classical deterministic algorithms so it’s mostly just classical algorithms doing most of the work which is why it’s not generative AI and why it is performant. True AI image generation requires a ton of processing,Neutral
AMD,"Eh, it's not worth the trouble of flipping my card and adding another 350, and it'd depreciate faster than my 7800XT will now as well. 9070XT is not the upgrade I'm waiting for.",Negative
AMD,"Oh, thanks. I might have gotten the timelines wrong then.  But even so, without a new GPU a CPU upgrade (based on benchmarks) won't make much sense. With ram prices even less so.  Right now I'm going to save up some money, so I can upgrade CPU + GPU. Guess the next upgrade won't happen before 2027 or 28 then. And when nothing changes much, won't be needed anyway.  Really, every game I play right now runs fine. Can't do heavy path tracing and other stuff, but I'm fine with that. Most games that need high settings of it to look great need a 5090 at least to get playable frames anyway.  I just hope nothings breaks till then :D",Neutral
AMD,"I'm not talking about DRAM, and would wager that though the current year situation translates to an increase in card prices, it won't slow down card development or hell, even their release.  Why would Nvidia ignore another opportunity to reposition their prices?     When will the next Nvidia generation come?  I'm not sure.  But the head start they are getting on designing the next generation on top of the performance lead of the current Gen VS AMD, seems like cause for concern to me.      But hell, I might be wrong.  Hopefully I am for that matter!  The display port on my 6900xt just died last night.",Neutral
AMD,DLSS4 is generative. The transformer model used for DLSS is definitely generative and they claim that MFG also uses a generative transformer model.,Neutral
AMD,Resolution plays a large factor too. If you're playing 4K...then CPU matters much less.,Neutral
AMD,">  it won't slow down card development or hell, even their release.  It already has affected graphics card releases and will affect more. According to rumors Nvidia's refresh (super line) of the current 5000 series has been canceled or postponed to late '26. The usual cadence for AMD GPUs is Q4, which for the 9000 series was postponed to March this year due to FSR4 development problems. So the usual cadence for RDNA5/UDNA would be Q4 '26, but, highly likely due to the DRAM shortage, has been postponed to mid '27.  There's a good chance there will be not a single new GPU release next year, apart from the Intel B770 which is expected to be announced during CES in the coming weeks. Will be interesting how the B770 will be priced in the current situation.",Neutral
AMD,"I just looked it up, It’s a probabilistic algorithm but it’s implemented deterministically. It’s not creating something new it’s combining existing things and predicting what’s in between them but it does so predictably. It’s more straightforward than generative AI",Neutral
AMD,"The development of new gpus is not slowing down.  Blackwell is already released, and yeah the super series is cancelled but you can already get the card.  We can be honest, the 5k series isn't like the 3k series with a 10gb 8-tiet card, so the super series was just for appeasement and isn't going to offer anything other than a bullet point on why to buy them over AMD (the extra RAM isn't likely to stretch performance much, of at all, if the speed is the same).     They can't hit the same price point, so I concede that a refresh of a currently released card won't release due to the shortage.     And AMD leakers are already saying udna is slated for mid '27, per the OP's article.  But yeah, Intel and big Battlemage?  They need to get in the game and get that card out.",Neutral
AMD,that is the most non-descript render of a laptop possible,Neutral
AMD,So light it visibly doesn't have any ports?,Negative
AMD,"Does intel 10A still come out as scheduled in 2027? I googled it and found out intel said the 10A will come out in 2027, but this was old news in 2024.",Neutral
AMD,I wonder how intel and other companies are going to manage for next year? Prices for memory and SSD’s are predicted to go even higher putting off many buyers from getting a new PC build or laptop.   This makes me concerned Nova Lake won’t sell as well because of this.,Negative
AMD,It's shameful to see LBT posing with 14A wafers when all the groundwork for this was setup by Pat Gelsinger. The entire Intel board should have been sacked instead of Pat.,Negative
AMD,"GFHK also has 14a for Razor and Coral Rapids in 2H 2027, so I'm taking what they are saying with very little credibility.   Plus, we had very similar rumors during 18A, and that went nowhere. Fool me once...",Negative
AMD,Unbelievable till official announcement,Neutral
AMD,can't they use it to make more ram ?,Neutral
AMD,good news,Positive
AMD,They can't even sell 18A to NVDA what are they doing on 14A really ?,Negative
AMD,"Lisa So Sue Me wants a taste of the Lip? Am I living in a different dimension? I callled out So Sue Me on X, is she jumping on Big Blue’s Back?  Is anyone Dollar Cost Averaging INTC? It will still be awhile before IFS is firing on all cylinders. The Lip said he would stop high end chip production for external customers (If No One Took A Byte) in order to get $$$ to build out Ohio Fab.   Let’s get it done. I’m driving distance from the Ohio Fab, any chance Intel will give me a tour?",Neutral
AMD,"If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  [https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots](https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots)  *""Intel's previously-unannounced Intel 10A (analogous to 1nm) will enter production/development in late 2027, marking the arrival of the company's first 1nm node, and its 14A (1.4nm) node will enter production in 2026.*  ***\[Edit: to be clear, this means 10A is beginning development, not entering high volume manufacturing, in 2027\]*** *The company is also working to create fully autonomous AI-powered fabs in the future.""*",Neutral
AMD,"14A probably won't be ready for 2027, much less 10A.",Neutral
AMD,10A & 7A are in R&D phase,Neutral
AMD,It's gonna be 2026 soon and 18A is launching at the very start of 2026. A double node shrink in like 2 years doesn't exactly sound very possible.,Neutral
AMD,"Remember, these are just names/nicknames. 10A? The difference between 14A and 10A is probably equivalent to the difference between 14nm and 14nm+",Neutral
AMD,And yet here you are.,Neutral
AMD,"Brother, don't hint at your place of employment when you have your full face in your profile as well as you commenting in NSFW subs.",Neutral
AMD,There will probably still be another of layoffs next month 😂,Negative
AMD,"Yes, perhaps it’s better if you post it on the r/intelstock subreddit instead 🤪",Neutral
AMD,"Ram should be at a more reasonable price in 2027 according to Moores Law is Dead. Maybe not $100 for 32GBs, but maybe below $200 🤞",Neutral
AMD,They have contract.,Neutral
AMD,"The entire Intel board probably should have been sacked, but Gelsinger as well. He failed at his main mission and drove the company into a crisis. That kind of thing should have consequences.",Negative
AMD,"TBH I feel LBT is doing a good job. I was hesitant at first, but he's making a lot more sense than Pat's crazy descent into spending crazy amount of cash with no business in sight.  Speaking as a shareholder.",Positive
AMD,Who was it that decided to exit the SSD business.  They sold off a cash cow for pennies on the dollar.,Negative
AMD,Nvidia is at least some what believable. AMD though?,Neutral
AMD,"I thought that too. At least they'd have some money coming in. But apparently it takes years to rejig the plants to churn out RAM instead of CPUs. And they're heavily invested in getting the next gen CPU fabs working.   Pivoting to RAM just doesn't make sense, unless they magic'd up a new type of RAM that's cheap to make and has super low latency - which is one thing I've always thought they ought to do.   Imagine if external RAM ran with super low latencies like CL1 or CL2 or something. You wouldn't even need branch prediction and prefetch and massive caches in the CPUs.",Neutral
AMD,"""news"" needs a lot of quotes around it...",Neutral
AMD,This isn't wallstreetbets. We don't talk like that here.,Negative
AMD,">If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  Yup, and to make it even more obvious, the same graph also has Intel 14A showing up early 2026, and 20/18A showing up at the start of 2023*,* so clearly it's not the date of when the node is going to come out (or even start HVM).",Neutral
AMD,"Dunno why this is being downvoted, the CEO of Intel himself said that 14A is a 28-29 node in the Q2 2025 earnings call.",Neutral
AMD,enough info about how intel names products exists to know. if it didn't increase in transistor density per mm it would not be called 10A.,Neutral
AMD,"I think everyone knows there will be continued Q1 and possibly Q2 layoffs.   Return to office didn't lead to enough voluntary attrition. Leadership wants to hit a magic number which sounds good for financial reports, not what is actually viable to run things.",Negative
AMD,The thing intel is doing rn is literally pat's groundwork isn't it?,Neutral
AMD,That crazy amount of cash being spent by Pat is what enabled 18A and 14A. They HAD to buy multuple $250 Litho machines from ASML in order to make that possible. Pat was playing catch up after years of under-investment by Swan and Krzanich. It was necessary and LBT is getting the credit. You don't appear to understand the lead times required in the semi industry. Pat understood that. The mistakes Pat made were trying to build a fab in Ohio and not cutting headcount and getting rid of dead weight sooner.,Neutral
AMD,Still a tall order imo unless it's some defense chip for RAMP-C,Neutral
AMD,"If they're following industry standards I'd say it depends on how good AMD's next gen is. Intel doesn't need direct access to AMD designs to etch chips for them, and designers make way more than per wafer than foundries do.  If AMD has superior designs to intel again they could finally ship out some damn chips for laptop OEMs. It would hurt intel more than the revenue would benefit them imo since client has really been carrying intel for the last six years and demand for AMD chips has been high despite the drip feed of strix chips. honestly I'm considering an AIO/NUC/whatever the new name is with strix halo and unified LPDDR5 to upscale old footage without having to use my daily desktop. imagine if it was available at scale.",Neutral
AMD,"they don't have to make faster ram, just make it, right now, some ppl don't really care about speed",Negative
AMD,"So, risk production in late 27/early 28 and HVM in 2029 I suppose?",Neutral
AMD,YEs it is. He did make mistakes. He was hiring like crazy at the beginning of his term. And he should have started cutting sooner. But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.,Negative
AMD,"Nothing they're doing *right now* is a success story. Remember that they don't actually have customers, and that is first and foremost what got Gelsinger fired. As things stand, the foundry as a whole is a failure. If things turn around, that will have to be under Lip Bu.",Negative
AMD,I wasn’t aware 14A is part of the RAMP-C initiative. I thought it was only Intel 16 & 18A that are currently covered by RAMP-C?,Neutral
AMD,I think so. Maybe optimistically we see a 14A product in late 28'.,Neutral
AMD,"> But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.  No, that was just more wasted money. 18A doesn't even use the high-NA machines Intel bragged so much about. It seems they tried blaming their struggles in foundry on the equipment instead of the broader org culture and talent.",Negative
AMD,"As much as I hate to say it, Intel arc was also a mistake.",Negative
AMD,get out of here with your sensable comments. we only circle jerk on this sub,Negative
AMD,It can expand in future ? My point is how can we believe such stuff at face value without actual proof.,Neutral
AMD,14A does use the High-NA machines. They didn't buy them with no plan to use them That would be stupid.,Negative
AMD,"no it wasnt. GPU's are surpassing cpu's eventually if not now.  a major part of amds success  was buying radeon all those years ago. when intel realized how utterly shortsighted they had been, they pushed arc heavy even though it wasnt going to succeed that well.  this was the right choice, as otherwise they would look like a dinosaur.",Neutral
AMD,"It can expand in the future but this is a trial, it’s not yet a long term commitment until the outcome of the project is known (final evaluation won’t be until 2026/2027). 14A is not part of RAMP-C, it’s still in phase III trial with 18A. There’s been no additional RAMP-C design calls via NSTXL that I’m aware of",Neutral
AMD,"> 14A does use the High-NA machines. They didn't buy them with no plan to use them  They bought the very first high-NA machines, claiming it was for 18A. Now they won't be used until a node that hits volume in '28/'29, by which point TSMC will have (or rather, already has) much better machines. So what exactly was the point?  > That would be stupid.  Is that not a perfectly apt description for Intel's foundry strategy in recent years? It sounds like they really drunk the coolaid with their attempts to blame the 10nm failures on the lack of EUV.",Negative
AMD,Yeah. The real mistake was LBT and the other Intel board members nerfing the r&d budget.,Negative
AMD,14A will have volume production in 2027.,Neutral
AMD,Didn't Intel say in a presentation that 2027 is risk production for 14A? https://www.techspot.com/news/107736-intel-doubles-down-foundry-ambitions-unveils-18a-14a.html  https://www.youtube.com/watch?v=5Jbj4RQBXbo&t=818s,Neutral
AMD,"Lip Bu himself is saying '28-'29. At this point, there isn't a chance in hell it's ready for volume in '27.",Negative
AMD,I just wanted arc to succeed 😔,Neutral
AMD,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Negative
AMD,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Neutral
AMD,I hope it comes to desktop CPUs,Positive
AMD,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Positive
AMD,Yeah this headline doesn't add up based on my own testing,Negative
AMD,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Negative
AMD,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Negative
AMD,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Positive
AMD,concur  some benchmarks are biased,Negative
AMD,lateral,Neutral
AMD,If it’s just “16% faster than 890m” it’s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Negative
AMD,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Neutral
AMD,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Positive
AMD,Answer to strix halo was the partnership with nvidia,Neutral
AMD,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Neutral
AMD,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Neutral
AMD,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Neutral
AMD,"I mean no offense, but Passmark is irrelevant.  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Neutral
AMD,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Neutral
AMD,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Neutral
AMD,"I guess we'll see more when we get actual info about the potential devices.  Right now, I haven't read about a device coming to market.",Neutral
AMD,Back in the day you could overclock a 2600k from 3.4Ghz to 4.5Ghz on a $25 Hyper212 cooler. The performance gains were incredible as Sandy Bridge scaled very well at higher clocks.   Now days CPUs come overclocked already.,Positive
AMD,"""It's crazy to think that a cpu from 2009 can be easily overclocked.. 2.9Ghz to 4.1Ghz is crazy !""  You could overclock huge amounts on earlier generations - I used to run Pentium 4 1.6GHz chips at 3.2GHz on air-cooling, more on phase-change cooling.",Neutral
AMD,"I ran my i5 750 2.67Ghz for years at 4Ghz without any issues. I benched it some at 4.2Ghz even, but it was not fully stable.  The X58 CPU are even better tho. And even if you had insane OC potential back in the days it was not as good as it sounds, since the turboboost was higher than the stock frequency that is listed.",Negative
AMD,"Lol a 15 year old computer running Windows 11, meanwhile Microsoft telling people to upgrade 5 year old laptops for win10 being EOL.",Neutral
AMD,X5690@4.6GHz on Rampage III Extreme 😘,Neutral
AMD,it is crazy that intel sold you same technology at downclocked speeds to make a nice model range with different prices.,Negative
AMD,Sick stuff. I still got my i7 930 at 4.2Ghz running just fine. These types of chips overclock like crazy.,Negative
AMD,Be nice. Give it another stick of ram!,Positive
AMD,Q6600 G0,Neutral
AMD,"Cool. Glad it worked for you. I have dual xeon server, maybe i should try it. But its production server dont wana break my apps. Lol",Positive
AMD,My 2500k did ~4.8 ghz and my 6950x did 5.2 ghz. Its base clock was like 3.2ghz and this was using 128GB of quad channel DDR4.  It was “stable”,Neutral
AMD,45nm is crazy in 2025,Neutral
AMD,500W power draw when,Neutral
AMD,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Neutral
AMD,"I used to run my i3-540 at 4.2GHz, air cooled on what is effectively worse than a Hyper 212 Evo. I miss the old days when I could overclock the snot out of them. These days I guess they're binned to almost their max potential out of the factory so most of the time I'm undervolting them.",Negative
AMD,Well done. Still using two H55m machines with OC (x3450 and i5 661).  They also OC decently at stock voltage keeping turbo and all power savings. My X3450 does 2.6 -> 3.3Ghz(3.8 turbo). The advantage is that it idles quite low at 50-60W.   But for gaming and rendering it's better to go all in as you did. Most chips can do anywhere from 3.8 to 4.2 all cores IME.,Positive
AMD,nah my 40 logical processors would smash through it all  x2 xeon e5-2680 v2,Neutral
AMD,"is that better? I dont need to dive into setting anymore, the CPU maker do it for me with warranty.",Neutral
AMD,"There is still more to work with, especially if one does not fossilize on static all core OC, but does 2-step TVB fueled dynamic OC, Ecores are Aldo the source of much happiness on arrow",Neutral
AMD,I miss overclocking. Felt like you were getting a bargain. Now I don’t even try.,Negative
AMD,"Not as big an OC as yours, but I had a pre-built from FutureShop.  It was their home brand name.  Found a BIOS for the board that wasn’t theirs.  Managed to get 3.2GHz out of a 2.4GHz Pentium 4 on pre-built from FutureShop cooling.",Neutral
AMD,"Wow, soo cool",Positive
AMD,The motherboard doesn't accept other stick of ram. Only my corsair ram work,Negative
AMD,How did you even get a 6950x to boot at 5.2ghz? Most of them hit a wall around 4.3ghz,Negative
AMD,"Has its ups and downs. Now that I'm older and have less time to tweak things and mostly just want shit to be stable, I see ""pre-overclocked with maybe 5% performance left on the table"" as a pro. The con is that chipmakers just jam a ton of power through it to make it happen, and the option of buying a half-price chip and spending an entire sleepless weekend tweaking it yourself to get 95% of the more expensive chip's performance is gone.",Negative
AMD,"Same - the complexity and heat rose a lot and the gains because less significant - with multi-core chips and turbo frequencies there just isn't much headroom in them.  That and I work fixing issues with computers all day, I just want my own PC to work.",Negative
AMD,"Thats because these older CPUs were surprisingly energy efficient. Also mostly because now modern CPUs are powerful enough where overclocking is pointless. Even my i3-12100 being overclocked would be pointless, even if its only a 80 watt CPU",Negative
AMD,He couldn't without LN2.,Neutral
AMD,"It was short lived, over ~7 years I had to pull back the multiplier from 52 to 44 to keep it stable.  I retired the system this year.  It was a full open loop from EK.  2x Pascal Titan X in SLi",Neutral
AMD,"I used to overclock everything, now I undervolt everything lol",Neutral
AMD,"The most important characteristics of a laptop are battery life ( power efficiency) and screen quality. That is what sells ( non apple ) clamshells.    All the other crap they test in various reviews are mostly meaningless to the actual user. There is a very small %age of the market for high power/perf laptops and even smaller market for gaming.   Most of the reason a laptop is “slow” is bloatware and has nothing to do with the cpu choice anyway.  And Intel is already “beating” AMD in laptop cpu sales, by a substantial margin. People incorrectly assume AMD has most of the market share in all segments because of the very noisy and super-biased gaming reviewers, who mostly focus on $3000+ gaming desktop builds. Yes AMD is handily winning there.",Neutral
AMD,"I have a T14s Gen 5 Core Ultra 5 135U and recently I saw \~9h of battery life, browsing websites. It's quite nice piece of hardware so with Lunar Lake it would be just perfect. Of course not for heavy workload because for this we will have Panther Lake. I work in tech for years, not an expert in laptops area but I can assure you that new CPUs from Intel that I mentioned earlier are way, way, way better than previous generations.",Positive
AMD,"I've gotten one and honestly it's amazing, easily the best laptop I've ever used so far.   I was skeptical about the battery life claims but I've genuinely found that using it for about 8 hours straight for coding, only drains the battery maybe 50%.  I've set it to only charge up to 80% max for battery health conservation, and I've regularly coded for 12 hours straight on the medium performance profile and haven't needed to charge until I got back home.  (This is for the Ultra 7 258v cpu variant btw)  Also this is while running Fedora with KDE Plasma which makes the battery life even more impressive as it's one of the heavier distros running cutting edge hardware and I've heard that Linux has less battery optimization compared to windows.    Screen isn't anything to write home about but the 100% srgb one looks good enough and is bright, 60hz looks kind of bad but I know that it saves a lot on battery.   Keyboard feels very nice as far as laptop keyboards go, having it be easily swappable is lovely as I wore out the keys on my old laptop, and I want this thing to last.   Linux hardware compatibility is perfect so far, even the fingerprint sensor works out of the box on fedora.   My only real complaint is that the plastic it is made out of is a major grease magnet and if I touch it without having immediately washed my hands, even if my hands weren't dirty, it'll leave dark patches from oils. Also it would be nice to have swappable RAM but I think 32gb ought to last a very long time anyway.   Genuinely seems like arguably one of the, if not the, best laptops for actually getting work done. Maybe it's not as fancy or sleek, but it just works. It's like the 2001 Toyota of the laptop world, it's not winning prizes for looks, but it'll never die, gets good mileage (battery life), and is easily repairable. Maybe not the laptop you want, but definitely the one you need (excluding people who need something like a dedicated GPU or really need super high CPU performance).",Positive
AMD,"Intel beats AMD in software (drivers, firmware) … I got think pad 780M laptop by company I work for. Randomly display won’t get detected. Randomly audio device goes missing. Not fun thing to reboot your laptop and miss 10 minutes of meeting.",Negative
AMD,"Soldered RAM sucks.  Nothing beats popping out the standard 8GB stick(s) a notebook may come with, installing a couple 2x32GB sticks yourself and having it actually work.",Negative
AMD,lol. Even in the cons it says weaker multicore than AMD….?   This article seems like AI wrote it,Negative
AMD,"Unfortunately Intel abandoned the on-package RAM after Lunar Lake again, which is the primary reason for the great efficiency and low power usage. I kind of understand why, it's expensive and not very flexible, plus apparently the market doesn't actually care that much about long battery runtimes. Only a small minority of people are ready to pay premium for this.",Negative
AMD,At work we are a Lenovo shop and recently swapped from Intel to AMD T14 laptops. Too many issues with Intel and the AMD models offer the same performance for less money.,Negative
AMD,Suck at gaming.,Neutral
AMD,Try a modern mac and tell me its not the CPU holding the UX back. It's all about the cpu's.,Neutral
AMD,"That would be nice. We have a bunch of laptops with Intel's i5, 13th gen I believe it was. 2 p-cores, some e-cores. They are all slow as fuck. I mean it. The CPU is so extremely slow and goes into tdp limit right away. Most users hate them.  Battery life is ok, but they are really bad in terms of performance.  So - I wouldn't say CPU doesn't matter.",Negative
AMD,"AMD is held back in laptops by some shady deals of laptop manufacturers with Intel. It's impossible to get a 4k AMD laptop with 5090 for example, all of those are Intel (I found literally one AMD laptop like that compared to 25 from Intel). That's utterly ridiculous.",Negative
AMD,Isn’t the keyboard one of the most important characteristics?,Neutral
AMD,"Yeah, so much of laptop performance is dictated by things other than the cpu. Its kinda wild.   Intel does a way better job getting good laptop designs. Amd historically has just been a cpu seller, telling oems to go wild and do whatever . . . And it always ends very badly.   The biggest sign amd is taking share is not cpu benchmarks, but will be things like having premium screens, good thermals, lack of bloatware, dual channel memory, good SSDs, good colors, etc etc. and . . . ACTUAL AVAILABILITY. I dunno how many reviews i see where they review and intel and amd parts. Usually there is some way intel has a better premium finish. And then amd just has zero ways to actually buy their model. Its the weirdest thing.",Negative
AMD,"Hey I’m looking at the exact same laptop that you have. Can you tell me about the build quality and if there’s any keyboard flex when pressing down on it? Please tell me. I’m going to use it for word, excel, reading lots of pdf files and ebooks and watch movies. Will it be enough for that?",Neutral
AMD,"But haven't you heard, AMD beats Nvidia slightly at linux gaming benchmarks.  That means AMD has the best software support.",Positive
AMD,"I miss the times when laptops were far more upgradeable. I got a budget laptop for college with a low-end dual core, a spinning HDD, and 1 stick of 2GB RAM. By the time I retired it ~6 years later I've upgraded the CPU, replaced the HDD with a SSD, and added 2x4GB sticks of RAM. I also could've swapped out the network card and even the DVD drive for another SATA drive, but never got around to those.",Neutral
AMD,Soldered ram is a lot faster. So no.,Neutral
AMD,Yes but now RAM costs a ton of money,Neutral
AMD,Is multicore performance the only consideration when buying a laptop?,Neutral
AMD,What kind of issues?,Neutral
AMD,What kind of issues with Intel? I thought it was the AMD that had tons of issues,Negative
AMD,It is not a gaming laptop,Neutral
AMD,"Yep. More specifically, it's mostly the single thread performance and efficiency.  It's how a MacBook Air can be fanless, run super fast, and stay cool at the same time while you're doing work with it.",Positive
AMD,It's an enterprise grade product you buffoon.,Negative
AMD,Exactly. Thinkpads are not targeting average Joes. They are targeting business and enterprise customers. Their Yoga and Ideapads are targeting the regular Joes.,Neutral
AMD,Build quality.    Thinkpads are solid machines that are easy to fix.    It's one of the few laptops that comes close to MacBook quality and everything judt working.,Positive
AMD,"So build quality will be subjective, from what I can tell, it's got very good build quality in terms of ""real"" factors such as durability. But it definitely feels less ""premium"" than similarly priced consumer grade laptops. The plastic is plastic so it will flex a little bit, but the parts all seem very well put together and it does feel ""solid"" overall.   I haven't really noticed keyboard flex, but I have noticed a slight amount of flex where my palms rest, particularly on the right side, where the smart card reader is, which makes sense as it is just a big hole in the side of the laptop. I plan on getting a dummy smart card to fill the gap and hopefully that should reduce it.   Overall whilst the internal chassis is metal, the outside is just plastic. I imagine that is good for durability, as it ought to be able to absorb shocks, but, as I said, it definitely makes it feel less ""premium"". They key press feel of the keyboard definitely does feel very nice as far as laptops go though. Obviously it's still nothing compared to a good mechanical keyboard but for a laptop it's very nice.   I bought this laptop for longevity and durability, so given that It's only just come out, I can't really say much about that, but the prestige of thinkpads of previous generations kind of speaks to their reliability. Plus it's apparent that they are still quite easy to repair and Lenovo has video guides on replacing loads of the parts.   And for your use case the battery life should be very good. It seems the Intel chip was designed to be very efficient during periods of downtime and something like viewing a PDF or editing a document has a LOT of downtime for the CPU",Positive
AMD,"Because only on Linux, Valve heavily funds AMD driver development and they also get other community contributions. The commonly used RADV vulkan driver was started as a community effort without AMD involvement.",Neutral
AMD,"I hope you're joking(sorry if you are), cos Nvidia isn't actually a good benchmark for software support on Linux. Intel is so much better at Linux software support than Nvidia",Negative
AMD,"The question is, is the soldered ram you're buying in a laptop faster than the ram you can buy and install yourself?",Neutral
AMD,All the more reason to make it upgradable,Positive
AMD,Lunar Lake isn’t how Intel beats Amd lol. Panther lake will stop a lot of the bleeding for sure. AMD is so far making mostly good moves and Intel is as well with LBT. The goal for Intel over the next 3 years is stop losing customers base. I will point out Intel still has about 75% of all x86 customers.,Neutral
AMD,"I mean, the only other pro is, that you cannot get the 2.8K panel on the AMD version for some reason..... sooo",Negative
AMD,This was back when the 14th generation were having issues.,Negative
AMD,"It's $2,000 so no excuse.",Neutral
AMD,Thank you so much for this valuable and comprehensive information! I really appreciate it:),Positive
AMD,"Usually yes, soldered ram will be faster. And in case of lunar lake, it is faster and more efficient due to it being packaged with the cpu. Like Apple's unified memory.",Positive
AMD,"Lunar Lake already beat AMD, nobody buys AMD laptops",Neutral
AMD,i stopped at $2100 for a Thinkpad T14,Neutral
AMD,"Ah okay, got it thanks.",Positive
AMD,wrong  Nobody Supply AMD laptop     There fixed for u,Negative
AMD,"I do, and many of the people I know do.",Neutral
AMD,Nobody pays that much.,Negative
AMD,"Not anymore, there are plenty of AMD laptops on the market, of course - depending on region.",Neutral
AMD,Try the shunt mod,Neutral
AMD,"Cool, errr...  icy",Neutral
AMD,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Neutral
AMD,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Positive
AMD,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Neutral
AMD,did you use dry ice? how did you hit sub-ambient?,Neutral
AMD,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Neutral
AMD,Are you in the US? If so how were you able to get Maxsun?,Neutral
AMD,Oh... for sure 😁,Positive
AMD,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Positive
AMD,Great work dude! Only 200MHz to go 😉,Positive
AMD,Car coolant in the freezer 😁,Neutral
AMD,That's the way! Let us all know the results.,Positive
AMD,I am in Australia.,Neutral
AMD,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Neutral
AMD,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Neutral
AMD,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Positive
AMD,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Neutral
AMD,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Neutral
AMD,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Negative
AMD,Oh! You put the car coolant to run through a freezer? Wow! Nice,Positive
AMD,And largely against the non-x3d lmfao.,Neutral
AMD,Aren't they just showing that AMDs CPUs are better for gaming?,Neutral
AMD,"Whoever downvoted my comment, why not run your own benchmarks and compare your results with mine? You could make posts asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming. Or how about go to tech sites and tech tubers asking them to provide their benchmark scores. its disappointing  to know people prefer to stay in ignorance and prefer people getting scammed over knowing the truth, just face it reviews adjusted to satisfy the sponsor how do you think they make money.",Neutral
AMD,"I haven’t tried ARL processors yet, but based on my own tests, 14700K with DDR5-7200 is 30–40% faster in gaming compared to Zen 4/5 non-3D chips. In the five games I tested, 14700k matched 9800X3D in three, lost in one, and won in one. I have no idea how tech sites and tech YouTubers show non 3ds as being just as fast or only slightly behind when they are actually far behind.  14700k vs 9700x/7700x  [https://ibb.co/cSCZ6fdX](https://ibb.co/cSCZ6fdX)  [https://ibb.co/999WzW4T](https://ibb.co/999WzW4T)  [https://ibb.co/Gjw1nXX](https://ibb.co/Gjw1nXX)  14700k vs 9800x3d  [https://ibb.co/FbMNL40q](https://ibb.co/FbMNL40q)  [https://ibb.co/8nQXpYZ5](https://ibb.co/8nQXpYZ5)  [https://ibb.co/vvcpKsGY](https://ibb.co/vvcpKsGY)  [https://ibb.co/67Jn8tKr](https://ibb.co/67Jn8tKr)  RE4  9800x3d 179FPS  14700k 159FPS",Neutral
AMD,"i dont think proving you can do more work faster is going to turn around sells. more people exclusively game or game and do light work, than those who do heavy work at home on computers.  i dont relive there are enough youtubers and streamers out there to capture as there are general gamers",Negative
AMD,"I mean, yeah. A 9800X3D costs way more than 265K, while a 9700X is within 10 bucks of it.",Neutral
AMD,I assume they compared with CPUs in a similar price range,Neutral
AMD,I don't think these slides are as good as intel thinks they are. They are just an advertisement to buy x3d ryzen cpus lol.,Negative
AMD,I guess the point Intel is trying to make is that it's great for gaming and much better for content creation (aka work). It's also cheaper.  X3d CPU's are only better for gaming (and not in all games). Doesn't mean they are crap but you pay a premium for x3d.,Positive
AMD,"if you think you're 100% correct - go out and buy an x3d and run the tests, post a video with the results, and put an amazon affiliate link in the comments for a 14700k build that overall is better than an x3d.  there's nothing wrong with the 14700k, it's a beast. i think there's a few select games where it even beats the 7800x3d. but there's a reason that you can find hundreds of youtube channels with sub counts up and down the spectrum from 0 subs to 10M subs showing hardware side by sides with the x3d's beating the 14700k - and most of them don't have advertisers to please.  in my opinion, you're getting downvoted because you don't want to provide proof, you're out here saying it's tech tubers fault that you have no proof. you can also find a good amount of open source benchmark data at openbenchmarking.org, it's not pretty - but there's useful data there as well.  but hey - you're only hurting yourself by not breaking open the youtube techfluencer conspiracy around AMD processors. go buy an x3d, do all the testing, and use it as a springboard to free us from the lies of big tech. i'd love for you to be correct, but until you can provide proof, it honestly just looks like you have buyers remorse with your 14700k.",Neutral
AMD,">asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming.  [but it's not true?](https://youtu.be/IeBruhhigPI?t=13m3s) 1%low on 9700X is identical to 14700k/14900k, and better than on 14600k, AVG. FPS is slightly higher on 14700k/14900k, or is 9.7% higher average FPS on 14900k considered ""far ahead""? Plus, 14900K is not a CPU for gaming, it's a CPU for production workloads plus gaming, which makes it noticeably more expensive than 9700X, so realistically we should compare 9700X to 14600K/14700K.",Neutral
AMD,"Cherry picked results or just outright fake.  A 14700k isn't 30-40% faster compared to zen 4/5 non-3d chips.  Come up with a decent game sample first first before coming up with stupid conclusions.  Every reviewer I've found just happens to have different results than yours so everyone must be wrong except ""me"" right?  Even if the 14700k is slightly faster it still consumes a lot more power so theres that.  The numbers you can find from reviewers point to the 9700x being around 5 % slower, and you here are talking about 30-40% lul, if it was 30-40% faster it would be faster than a fking 9800x3d lmao.",Negative
AMD,I just built a 265K machine with 64GB 6800 RAM and it is no slouch. I haven't really played much on it yet as I've been working out of town a lot lately but my 4080S is now the limiting factor rather than the 10850K I had before.,Positive
AMD,RE4  9700x vs 14700k   [https://www.youtube.com/watch?v=FcDM07sgohY](https://www.youtube.com/watch?v=FcDM07sgohY)  [https://www.youtube.com/watch?v=-WO0HqajShY](https://www.youtube.com/watch?v=-WO0HqajShY),Neutral
AMD,"I did the same comparison, but it was 13900k vs 7800x3d and the Intel was much smoother. It's sad that you are getting down voted by bots who have never tried it and repeat tech tuber numbers. It's sad because ultimately the consumer is the loser when they are believing in lies.",Negative
AMD,"Now install windows 11, lol",Neutral
AMD,"> I haven’t tried ARL processors yet, but based on my own tests  Then whatever you're about to say has 0 relevance to the topic at hand.",Neutral
AMD,"Clearly not as sometimes they use the 9950x3d, and sometimes the 9900x to compare to the ultra 9 285k.   One of those is the half the price of the other.  (And guess which one they used for 1080p gaming comparison)",Neutral
AMD,"And to add to that, which often gets omitted. Is that they are only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, intel in my mind actually makes more sense.",Neutral
AMD,"Why not tech sites provide built in benchmarks scores and let everyone veryfy them, and let us decide which processors better, why should we believe in charts blindly, why should I stay silent about what i found.",Neutral
AMD,"I sell PC hardware and get to test a wide range of components. I’ve built 7950X3D and 9800X3D systems for my customers, and when I tested five games back then, their performance was very close to the results I got with the 14700K. I’ll be able to test more games in the future if another customer asks for an X3D build. I never claimed the 14700K was better than the X3D in general I only said it scored higher in one game, Horizon Zero Dawn Remastered, and matched it in others. At first, I thought something was wrong with the build, so I made a post asking 4090 owners with X3D processors to benchmark HZD, and they confirmed my result. If I were to test 10 or 20 games, the 9800X3D might turn out to be about 5% faster, but I’m still not sure. Since I’m testing on 4090s, I can buy whatever processor I want, but based on the five games I tested, it doesn’t seem worth it. Also, the minimum frame rate was always a bit higher on the 14700K. X3Ds are generally good, but my main concern is that non-3D chips are 30–40% slower than the 14700K. Why does it feel like I’m the only one who knows this?  As for proof, I’ve provided built-in benchmark screenshots, phone-recorded videos (since OBS uses 15% of GPU resources), and OBS captures as well, which should give very close idea of processors performance. I’ve shown full system specs and game settings every time, and provided more evidence than any tech site or tech YouTuber ever has, yet some people are still in denial. I want just one tech site or YouTuber to show benchmark screenshots or disclose full system specs before testing. None of them do, and people keep trusting charts with zero evidence.",Positive
AMD,"Ive given my evidence by videos recorded by phone and obs,  scores screenshots prove 14700k is 30% faster than 9700x where is your evidence, talk is cheap, you either provide hard undeniable evidence or stay silent.",Neutral
AMD,"14700k vs 9700x  CP 2077 171FPS vs 134FPS 30%  spider man2 200FPS vs 145FPS 35%  SoTR 20%  RE4 160FPS vs 130FPS 25%  horizon zero dawn 172FPS vs 136FPS 30%  30% on average vs 9700x, 35%-40% vs 7700x  here is video recorded testing 5 games by OBS for 14700k vs 7700k  RE4 160FPS vs 110FPS  SM2 200FPS vs 145FPS  [https://www.youtube.com/watch?v=oeRGYwcqaDU](https://www.youtube.com/watch?v=oeRGYwcqaDU)  [https://www.youtube.com/watch?v=u6EPzOGR2vo](https://www.youtube.com/watch?v=u6EPzOGR2vo)",Neutral
AMD,"can you do a test for horizon zero dawn and CP built in benchmarks, also if you have RE4 can you go to the same place of the game I tested on and take screen shot showing what frames you are getting, I want all tests 1080P very high, if its not much trouble for you.",Neutral
AMD,"I only tested 5 games, and the two processors were very close. I’d probably need to test 10 or more titles to know for sure if the 9800X3D is actually faster. The fact that the 14700K came out ahead in Horizon Zero Dawn shows that there are games where Intel performs better.",Neutral
AMD,I did same performance on all processors.,Neutral
AMD,"It does, just like RPL being shown on corrupt tech sites as being as fast as Zen 5 in gaming, in the real world ARL could be much faster than they make it out to be.",Neutral
AMD,That sounds like an AMD Stan argument circa 2020,Neutral
AMD,Lmao   https://youtu.be/KjDe-l4-QQo?si=GsvzFon8HYcrzrMc  theres no conspiracy mate,Neutral
AMD,"You need to grow up, evidence from some rude slime on Reddit is irrelevant for me, I value data from techpowerup, hardware unboxed or gamers nexus x100000 times more than anything coming from you.  I tried making this conversation constructive, but it seems that you're not adequate enough to even try. Good luck.",Negative
AMD,Can you explain me why in the video you are running a 7700x on 6000 cl40 RAM?,Neutral
AMD,"I challenge all tech sites and tech tubers to publicly share their built-in benchmark scores so everyone can verify them. By the way, I posted these scores on all the popular tech tubers pages on X (twitter), but none of them replied. I wish at least one of them would debate these results and prove me wrong.",Neutral
AMD,"Are your results with or without APO? APO supports SOTR and Cyberpunk, so give it a try if you haven't yet to see if it makes any difference.   https://www.intel.com/content/www/us/en/support/articles/000095419/processors.html",Neutral
AMD,"I can do CP 2077 later. I have a 265k with a 9070, but only 6400 ram  (Mostly replying so I remember to do it)",Neutral
AMD,I only have Cyberpunk of the three.  [https://imgur.com/a/L3qMGva](https://imgur.com/a/L3qMGva)     I think the 4080S is holding me back too much in this benchmark.,Neutral
AMD,"Your tests on win10, win 11 24H2 improved performance. All of your tests are complete nonsense anyway",Negative
AMD,I was just thinking the same. We have come full circle.  Time to buy some more intel stock,Neutral
AMD,It worked for them didn't it.  Why is it loads of people think AMD has dominated since original Zen CPU?,Neutral
AMD,Expand ?,Neutral
AMD,"I want elegato unedited video from start to finish i did that and on cp 2077 9800x3d matched 14700k, and it managed to beat it on horizon zero dawn, i want built in benchmark scores so everyone can easily double check the numbers, forget everything online buy the hardware and run built in benchmarks RPL i7/i9s on ddr5 7200 are equivalent to zen 4/5 x3ds much faster than non 3ds, if you can't afford it make posts asking people for thier scores in games.",Positive
AMD,"It is a common knowledge at this point, that HUB, LTT and GN are working for YouTube algo, not the costumers. YT algo promotes saying AMD is king, so their reviews are exactly that (Every single thumbnail or tittle with Intel being dead or destroyed). Let's pair Intel with mediocre ram and loose timings, let's power throttle them, put on some weird power profile, turn off whatever bios setting, focus on specific benchmarks that have very little to do with real usage etc. In worst case scenario they can always say that Intel won, but it's not enough or that Amd will crush them in 6 months or so. They've been doing it for years now.   Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong. It's really not about being right or win some CPU wars, it's more about not being screwed by that giant YT racket that once again works for sponsors and YT bucks, not the costumers.",Negative
AMD,"I spammed my score screenshots on every popular tech tuber’s page on X (Twitter), and none of them had the courage to face me. I wish one of them would come out of hiding and tell me what I did wrong in my tests. Show me the scores they’re getting. Reviews are sold to the highest bidder deal with it. Also, PCGH showed the 14600K beating the 9700X, so why isn’t that the case in other tech site charts?",Negative
AMD,"at first I tested 4080 1080P ultra on these processors on the same DDR5 7200 im using on the 14700k but it didn't make a difference tried the 2 profiles 7200 xmp and expo CL38, so when I got the 4090 I decided to use whatever available but that wouldn't make a difference, I could use DDR4 on 14700k and still win in every game against non 3ds,   here is the test on 14700k 5600 CL46 which got me around 185FPS vs 145FPS for zen4/5 non 3ds.  [https://www.youtube.com/watch?v=OjEB7igphdM](https://www.youtube.com/watch?v=OjEB7igphdM)  also here is 14700k +DDR4 3866 +4080 for RE4 still faster than 9700x+4090+DDR5 6000  143FPS vs 130FPS  [https://www.youtube.com/watch?v=lxZSMJnkYNA](https://www.youtube.com/watch?v=lxZSMJnkYNA)",Neutral
AMD,"Did you get any more FPS when you tried it? I’m getting good frames without it, so I don’t think it’s worth it. Also, in SoTR, even though the 9800X3D scored higher in the benchmark, there weren’t any extra frames compared to the 14700K during actual gameplay. I tested multiple locations, and the performance was the same, the higher score on the 9800X3D came mainly from the sky section of the benchmark.",Negative
AMD,did you do it,Neutral
AMD,I was at:  RT Ultra: 118.78 Ultra: 182.42 High: 203.76  https://imgur.com/a/edaNmHQ,Neutral
AMD,can you reset settings then choose ray tracing ultra preset.,Neutral
AMD,here is w10 vs w11 same scores in every single game.  [https://www.youtube.com/watch?v=vQB4RrNDzyM](https://www.youtube.com/watch?v=vQB4RrNDzyM)  [https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s](https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s),Neutral
AMD,because they exclusively exist in DIY build your pc enthusiast bubble,Neutral
AMD,Pricing was aggressive. A 12 core 3900x was 400 usd.,Neutral
AMD,">And to add to that, which often gets omitted. Is that Comet Lake is only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, Zen 2 in my mind actually makes more sense.",Neutral
AMD,literally multiple videos from him even with a 7800x3d and still 14900ks can only eek out wins in like 1-2 games like congrats you dropped 500$+ on the RAM alone and 720$ on hte motherboard to run that RAM just to lsoe to a 7800X3D,Neutral
AMD,">It is a common knowledge at this point,  It's not an argument.  Only objective way that you have to prove that those sources are spreading misinformation, is to prove it and share it with others - do it, make your own research and share it on Reddit and YouTube, if you're really correct and your data is accurate, it will hurt public reception of these sources(HUB, LTT, GN) and will damage their credibility.  >Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong.   [No true Scotsman - ](https://en.wikipedia.org/wiki/No_true_Scotsman)sorry dude, but your argument is logically incorrect, for pretty obvious reasons - it's unfalsifiable, it lacks evidence and it assumes the conclusion **- ""every dude"" argument is a way to sound like you have the backing of a consensus without having to provide a single piece of evidence for it -** as I said, real evidence in your case would be substantial evidence that proves that sources that you named in your comment are spreading misinformation, which shouldn't be hard to test and release in public.  I don't like to be a part of emotional conversations, I honestly hate those - if we want to speak facts, share facts with me, if you want to speak emotions - find another person to discuss them, I don't like emotions when it comes to constructive discussions.",Negative
AMD,"Okay, I did it",Neutral
AMD,"No, I didn’t remember good",Positive
AMD,"I did. I ran the benchmark a couple more times and the results were similar. APO on, 200S Boost enabled, no background tasks running except Steam. I think it's a GPU bottleneck.",Neutral
AMD,Thanks for solidifying opinion that your benchmarks are fake,Positive
AMD,"Ddr5 7200 cost as much as 6000, i bought mine for 110$ from newegg, I should have got the 7600 for 135$ but I was a bit worried about compatibility, my z790 mb cost me 200$, and on my 14700k on a 4090 it matched 9800x3d in 3 games lost in re4 and won in HZD remastered, ill test more games in the future but since i won in one game to me 14700k+D5 7200=9800x3d, 14900k+ddr5 7600 could be faster, i dont care about any video online or chart, i only believe in built in benchmarks scores and what my eyes see.",Neutral
AMD,"if there isnt cpu bottleneck your score should be around 135fps, can you give it one last try with sharpness set to 0.",Neutral
AMD,"What’s fake about it? What scores are you getting? Prove me wrong with score screenshots, not with fanboyism false accusations. If you’re comparing score screenshots to OBS-recorded videos, you shouldn’t, because OBS uses about 15% of GPU resources, which reduces the scores slightly. I’ve been talking about this and calling out tech sites and tech tubers on Twitter and Reddit for over a month. Do you really think they’d stay silent all this time if I were wrong? If I were faking benchmark scores, they would have publicly shown their results and shut me up for good, especially since I’ve been accusing them of corruption on a daily basis. Just yesterday Tech power up is the first techsite to block me on X for sharing my benchmark screenshot on their page, cowards, shouldn't they discuss what did i do wrong in my benchmark, then again what are they gonna say AMD paid us too much money that we need 10 years in hard work to make that much.",Negative
AMD,Cam someone confirm or is this gas lighting?,Neutral
AMD,This is nice honestly. Points out some nice stuff about ARL besides the usual hate on the internet,Positive
AMD,"The Ultra 7 265K is absolutely a better deal than the non-X3D 9700X or 9900X. The Ultra 9 285K is competitive with non-X3D 9950X. If 3D-Vcache offerings from AMD didn't exist, Intel would likely be considered the go-to for this gen.",Positive
AMD,Intel comeback real?,Neutral
AMD,"How come the non-k variants of ultra 9,7, and 5 have a base power of 65 watt, but the K variants have a base power of 125 watts? The only difference I see between both variants, besides base power, is the 0.1-0.2MHz clock speed increase. Am I missing something here?",Neutral
AMD,3D v-cache has entered the chat.,Neutral
AMD,Take it as a grain of salt. Intel marketing LOL,Neutral
AMD,Does anyone have the Intel Ultra 9 285k and can test all of this ?,Neutral
AMD,Thats cool ...but lets talk about better pricing.,Positive
AMD,"I got the 7800x3d and the 265K, the 265K got higher fps. Also when i watch twitch streamer i Look at the fps if the streamer is useing a 9800X3D. In CS2 the 265k got 50-80 fps less then the 9800x3d  (500 vs 580 fps) in cs2 Benchmark the 265K is consuming about 100 watts",Neutral
AMD,Tech Jesus has entered chat :).,Neutral
AMD,"Intel still trying to sell their already abandoned platform, they should learn something good from AMD and provide support for three generations per socket (and yes, Zen 6 will use AM5, already confirmed by AMD).",Neutral
AMD,"Two things can be true at the same time. Arrow lake has legitimately improved since it launched last year, and Intel is portraying it in the best, most optimal light they can that is probably not representative of a broad spectrum of games reviewed by a third party.   It would be interesting to get a retest of arrow lake now, but I dunno if it is worth the time investment for some reviewer like TPU or HWUB to re-review a relatively poor platform that’s already halfway out the door.",Positive
AMD,"Arrow Lake performs quite well at general compute and power efficiency compared to prior generatons ans even against AMD.  Where they have had trouble is in specifically gaming apps, particularly against the X3D variants (which it should be said drastically perform non-X3D AMD parts in gaming). And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  This led to Intel being dismissed out of hand by gaming focused YouTube reviewers like HUB, GN and LTT despite them being perfectly capable parts outside of gaming, simply because they did not place well in gaming benchmarks in CPU limited scenarios (e. g. with a literal 5090).  HUBs own benchmarks, btw, show an utterly inconsequential difference between a 9700X part versus a 9800X3D part when paired with anything less than a 5080/4090 in gaming. Most people with most graphics cards wouldnt see the difference with their card between the various high end CPU parts unless theyre literally using a $1000+ GPU. Ironically the 9070 performance was identical with a 9700X and a 9800X3D.  So it's actually kind of the reverse, gaming reviewers painted Arrow Lake in the worst possible light.",Neutral
AMD,What do you mean by gaslighting in this case?,Neutral
AMD,"The answer depends on whether or not you're a ""next-gen AI gamer"" (apparently that means 1080p avg fps with a RTX 5090).",Neutral
AMD,I doubt they would give false numbers. Those are probably what you get with the setup they list in fine print. Also they pretty clearly show the 3dx parts are faster in gaming. But they have probably picked examples that show them in a good light.   There is nothing wrong per se in the arrow lake lineup. In a vacuum if AMD and intel launched their current lineup now they would both look pretty competitive. AMD zen5 was also a bit of a disappointment after all. The issue that destroyed the image of arrow lake is that the previous generation was often faster in gaming. That made every reviewer give it bad overall reviews.,Neutral
AMD,Wait till refresh comes out in December... Memory latency lower and performance boosted. Price also lower... Wish it could come to the old chips but it architecturally cant. :(,Negative
AMD,"generally they cherry pick, so maybe the demo they use is process heavy with minimal I/O (arrow's biggest performance hit is memory movement). if you manage to keep memory movement low core performance is actually really good on arrow lake. it just gets massacred by d2d and NGU latency, which they don't get a pass for but its unfortunate that decent core designs are held back severely by packaging.",Negative
AMD,"there is just no way dude lol  even their comparisons make no sense, 265K = 9800X3D / 265KF = 9700X? lol? how would that even be possible",Negative
AMD,Honestly it's hard to get real conversation with people who fanboying Amd so hard. Why people can't be normal when we talk about Intel and Nvidia? *smh,Negative
AMD,"The base clock rates, my assumption how it boosts during heavily multi-threaded tasks, is like 60%-80% higher on both core types.",Neutral
AMD,Nova Lake bLLC about to ruin Amd X3D party.,Neutral
AMD,Look at the last slide. Intel's source is TPU's review data which if anything is a less than ideal config for arrow considering it's only DDR5-6000. I'm honestly surprised they didn't pull from one of the outlets that did 8000 1.4v XMP+200S boost coverage.,Negative
AMD,I always wondered if Intel marketing budget is higher than the R&D budget,Neutral
AMD,Intel Arrow Lake is much cheaper than Amd Zen 5.,Neutral
AMD,Genuine question how often do you or even the average person keep their motherboards when upgrading the CPU? IMO the platform longevity argument is stupid,Negative
AMD,only an AMD fan would worry about replacing their shit CPUs under 3 years,Negative
AMD,Hardware unboxed isn't a reliable source.,Neutral
AMD,"As a real world example, I upgraded my primary system from a 14700k to a 265k recently. My other desktop is a ryzen 7900 (non x, 65 watt tdp)  Compiling my os takes 6 minutes on the 7900, 10 minutes on the 265k, 10 minutes on a 3950x, and 16 minutes on the 14700k.   That is not in windows, but my os which does not have thread director or custom scheduling for e cores. (So going to be worse than windows or Linux)   All the numbers except the 7900 are with the same ssd, psu, custom water loop.  The motherboard and cpu changed between Intel builds and the 3950x was the previous build before the 14700k.   The e core performance significantly improved in arrow lake.  It’s now keeping up with 16 core am4 parts in the worst possible scenario for it.  If the scheduler was smarter, I think it would be very close to the 7900.   In windows, it’s pretty close in games I play on frame rate to the old chip. A few are actually faster.",Neutral
AMD,"> And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  Yeah, 10% less performance when frequency matched is quite a lot.",Negative
AMD,Telling people that its performance is better than it actually is?,Negative
AMD,The ones with similar pricing not performance,Neutral
AMD,"True, except AM5 supported all cpus 7000, 9000 and will support the newest one. So not exactly true per se, because you need a new motherboard every time and those on AM5 can upgrade cpu without buying a board. Imagine if someone was on 13400F and wanted to buy lets say 285K? Well, you need a new board. So it depends what “cheaper” is seen as.",Neutral
AMD,"Yep. We all see the patterns here. Gamers nexus, HUB, LTT and most big name they all keep mocking Intel, even when Intel did good job to bring improvements to their products like fixing Arrow Lake gaming performance but those ""tech reviewer"" decided to be blind, they don't even want to revisit Intel chip and benchmark it, they don't even want to educate their viewers.    At the same time they keep overhyping Amd products to the moon like they own the stock, it's obvious now who is the real reviewer and who is fraud reviewer using their own popularity and big headline to spread their propaganda for their own benefits. They are just as worse as shady company!",Negative
AMD,"Platform longevity isn't stupid lol. There's a reason why am4 is one of the best selling and one of the best platforms of all time. The fact that you can go from a 1700x to a 5800x3d on the same mobo for 85% more performance is crazy good value. You can say that all you want but there's a reason why in the diy market everyone is buying amd over intel. People know arrow lake is doa where as am5 actually has a future, which is why everyone is flocking to the 9800x3d in droves.  Zen 7 is rumored to be on am5 as well. Meaning that am6 would come out in 2030/2031. If that's true am5 lived for around 8/9 years.  That's insane value to get out of one socket.",Positive
AMD,Quite common for AM4 in my experience.,Neutral
AMD,"My brother went from 1700X to 3700X to 5800X3D on AM4, so 10 years next year? I plan on upgrading to 3D cpu probably 9600X3D once its out or maybe 7800X3D (something more affordable but still somewhat better). Kinda want better 1% lows tbh, but maybe I’ll just wait for next ones unsure.",Neutral
AMD,"How is it stupid? On one hand, intel sockets used only 3 gens from 12th-14th while on the other hand amd has not only used 7000-9000 but will also be using zen 6. I don't think you realize just how big of a hassle changing motherboards are",Negative
AMD,"I upgrade my motherboard usually because of cpu socket. Thinking of going AMD. I build my PCs for myself, my wife and my kid. Lots of floating parts in this house could use the platform longevity.",Positive
AMD,I originally had a Ryzen 5 3600 and am now on a 5700x3d so it’s pretty relevant I’d say. Of course it’s anecdotal but I don’t think it’s too uncommon,Neutral
AMD,"I agree with you, it’s daft, no doubt about it. But you’ve got to factor in that people can be daft too. Most tend to skimp, only to end up buying another middling CPU the following year, and then again after that. And so the cycle trundles on.  When I buy PC parts, I usually go for the best available. Provided the price is reasonably sane. I picked up a 9900K seven years ago and it’s still holding its own. Is it on par with the 9800X3D? Of course not. But it’s still a very capable bit of kit. Next time I upgrade, I’ll go top shelf again.",Neutral
AMD,"They frequently revisit products and test old hardware, but naturally that's not going to be their focus. Channels with that focus like RandomGaminginHD fill that niche.",Neutral
AMD,Sooo they are in the YouTube space for the money not for the love of tech,Neutral
AMD,"So true. After all they are reviewer, it's their job to revisit something if it's worth to benchmark due to improvements.   Didn't they said they want to educate their viewers? If so then why being lazy? This is obvious they just care about making big headline which makes more money for them. It's BS!",Negative
AMD,"If only the Arc B770 can beat the RX 9070XT with performance and features, then I definitely stay on Intel's side, but RN I doubt it",Neutral
AMD,"Even accepting that as true, I don’t think it changes the main point I was making. Arrow Lake has probably improved versus a year ago. It’s probably not as good as intel is making out. A good third party reviewer would be needed to determine by how much, yet it’s probably not worth their money and time to test it.   So the truth is probably in between what Intel is saying here and what the state of things was 6 months ago.",Negative
AMD,Isn’t that just either lying or exaggerating?   I know those words don’t “go hard” and are “low key” boring.   But I think some internet buzzwords are just overused or badly applied.,Negative
AMD,"Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Negative
AMD,The 5800x 3d costs anything between 450 (time of launch) and 300. With that kind of money - you can literally buy a cpu + a mobo bundle. There is no value in the platform upgradability argument. You can literally buy a 13600k and a brand new mobo for cheaper than the cost of the 5800x 3d alone.,Negative
AMD,YouTube has created an entire industry not just for gaming but for everything else where all these people rely on the YouTube algorithm and have figured out how to exploit it and bitch and complain whenever YouTube tries to change things up.  We need a new platform based on YouTube but built by technology like ipfs and run by the internet community.,Negative
AMD,"No other card in recent memory has been more overhyped than the 9070 XT , we should be getting a B770 before 2025 ends. Make sacrifices to the Silicon Gods and perhaps your wishes will come true.",Neutral
AMD,"The only things that have changed since launch is the bugged ppm driver on windows causing extremely low clocks under moderate load (games being the main example), and 200S boost making d2d/NGU OC a single click for people not willing to type in 32 in the NGU/D2D ratio boxes. The current performance on arrow is achievable on the launch microcode.",Neutral
AMD,Sure but charts seem about right to me,Positive
AMD,APO is game specific. I'm referring to what has changed overall.,Neutral
AMD,"I'm unable to claim my copy of Battlefield 6 from the recent Intel promotion even after redeeming the code.  I redeemed my promotional code last month which added the ""Intel Gamer Days"" bundle to my account that includes Battlefield 6. The instructions say to return October 3rd to claim the key. I've been attempting daily to claim the key but just keep getting the message ""We are temporarily out of codes and are working to reload our system. Please check back soon.""  The game launches tomorrow so this is pretty frustrating. Is anyone else having an issue claiming their BF6 key?  I put a ticket in with Intel support but haven't heard anything yet.",Negative
AMD,"bought the newegg bundle. I got a master key but my PC is running linux. The linux HST tool doesn't even have an extension, no .exe no nothing. I don't know how to run it.",Negative
AMD,"Hello there, it seems that after updating my Windows 11 I can't scan for new intel drivers by using the Intel Driver & Support Assistant. It scans for a while but then it gives me the message: ""Sorry, something went wrong while trying to scan""; it seems to affect the Intel Graphics Software as well as it keeps looking for updates without success. I've already tried to reinstall DSA and deleting any folders associated with the program, rebooting Windows and also uninstalling the Windows Update. Any thoughts?",Negative
AMD,"After updating to W11, these awful KILLER apps and processes have returned and reduced my upload speed to less than 1mb!!! Normally I get 30mbs up.   Dell PC, 32 gbs RAM, Intel i9. x64 W11 Home 24H2, OS 26100.6584, WIRED Ethernet direct to router/modem.  The Killer thread is closed and directs users to post in this thread.   The same thing happened in W10 on same machine but problem was fully solved by the KillerSoftwareUninstaller tool, which also disabled Killer permanently from coming back on every single update.  However, I tried the same tool after the update to W11 and it removes Killer, and everything works better for a day or so, then suddenly everything slows down and Killer is back.  PLEASE help me get this awful suite of useless Killer tools off of my machine forever!   Thank you!",Negative
AMD,"I have a i7-13700 that may be the root of my issue. Long story short, MATLAB is getting an Access Denied error when trying to link to an Imperx framegrabber using videoinput. Imperx FrameLink works fine. The function that it crashes at works fine when called from a different way (getAvailHW), and only crashes when trying to call it through videoinput. The exact same setup works just fine on another computer. Even when no cameralink cable is attached, it crashes on this one. BIOS is fully updated. The CPU passed the checking program Intel provided.   Looking up this issue, people had similar problems, and they linked it to the CPU issues with 13th and 14th gen. Intel released a chart for what settings you should have in BIOS, but it does not cover the 13700, only the K variant. What settings should I have in BIOS to rule out Intel?  Chart: https://community.intel.com/t5/image/serverpage/image-id/56057i81282C3BCB9162A9",Negative
AMD,"Is this normal activity for a 265K at Idle?  https://imgur.com/SSSOB9F      The multiplier and clock speed keeps jumping around all over.  There are no background processes running, nothing in task tray, closed all applications except HWINFO to record this.      I am running into an issue where I feel like either my CPU or my AIO is bad.  The build is new, completed yesterday.  Idle temps hang around 30-40C; which seem fine.  But as soon as tasks start happening, it jumps all over up to 80C.  I have a 360 AIO on it.  Gaming even, it hit 83C earlier.  It doesn't make sense to me.  I have built a few 265K rigs and not had this kind of fluctuation in temperature before.  Even running cinebench on previous builds, the CPU never went above mid 70s.  But this newer machine is hitting 80+ in games.  Previous 265K PCs have ran mid 50s in games.  Sometimes 65 depending on the load, but never 80+.",Negative
AMD,"Dear Intel team,  according to **desktop** [**Core 14th datasheet**](https://edc.intel.com/content/www/us/en/design/products/platforms/details/raptor-lake-s/13th-generation-core-processors-datasheet-volume-1-of-2/system-memory-timing-support/) \-> memory timings support:  \- only **CL50 DDR5 5600MT/s** **is supported**  \- only **16Gbit / 24Gbit die density is supported**    **Can I safely use following DDR5-5600 CL36 kit from Kingston KF556C36BBE2K2-64 with 32Gbit die density?**  Will it cause some memory troubles / issues on memory controller ?  System is not overclocked (and never was/will be), everything set to Intel defaults, bios has latest in 0x12b microcode from ASUS (board Q670M-C CSM).",Neutral
AMD,"Hi, I’m at my wit's end with my build and would really appreciate some advice.  My PC has been plagued by random crashes, CRC errors, and installation failures for months:  - Random application crashes, often citing `KERNELBASE.dll`, `ntdll.dll`, or `ucrtbase.dll`. - Frequent CRC errors when extracting large ZIP or RAR files. Retrying usually works. - Software installations fail with data corruption or unpacking errors, only to succeed when I try again. - Games crash or stutter randomly. - Very rare BSODs.  Specs:  - **CPU**: Intel Core i9-13900K - **Motherboard**: ASUS ROG STRIX Z790-H GAMING WIFI - **RAM**: 32GB (2x16GB) G.Skill Trident Z5 RGB (DDR5-6400 CL32, Model: F5-6400J3239G16GX) - **GPU**: MSI RTX 4090 Gaming X Trio - **Storage (OS Drive)**: Crucial P5 Plus 2TB NVMe SSD - **OS**: Windows 11   This is what I've already tried (everything passed):  *  **Memtest86:** Completed multiple passes with no errors. *  **Prime95 & Intel XTU:** Seems to be stable. *  **FurMark:** GPU stress test is stable. *  **Storage Health:** All drives pass SMART and manufacturer-specific self-tests. *  **System Integrity:** `sfc /scannow` and `DISM /RestoreHealth` complete successfully. *  **Updates:** All drivers, firmware, BIOS, and Windows are fully up-to-date. *  **Physical:** Cleaned the case, re-seated components, and replaced thermal paste. * **XMP**: I've disabled/re-enabled XMP multiple times, doesn't make a difference.  I have trouble finding a consistent way of reproducing the issue. Today I tried a 7-Zip benchmark which failed once with a ""decoding error"", but I wasn't able to reproduce it afterwards. I couldn't get Intel Processor Diagnostic Tool to fail after multiple hours. So...  *   **Is this a memory instability issue?** Could the RAM be faulty?  *   **Or, could this be a faulty CPU core?** I found [this post](https://www.reddit.com/r/intel/comments/15mflva/tech_support13900k_problems_when_using_multiple/) where a user had identical symptoms (CRC/7-Zip errors) that were only resolved by replacing a faulty 13900K, even with XMP off.  Thanks in advance for any help.",Negative
AMD,"# Alienware 34 - AW3425DWM resolution issues  [](https://www.reddit.com/r/ultrawidemasterrace/?f=flair_name%3A%22Tech%20Support%22)  I just got the AW3425DWM, and my laptop is a Dell Inspiron 15 5510, which is not a gaming laptop. I'm not a gamer.  When connecting through the HDMI port on the laptop and monitor, I can't set the resolution to 3440x1440; I can go up to 3840x2160, but not the monitor's native resolution. However, when I connect through the USB-C port on the laptop to the DisplayPort on the monitor, I can set the resolution to 3440x1440 without any issues. The downside is that I've lost the only Thunderbolt port I have available.  Is there a workaround for this issue? If I use an HDMI to DisplayPort adapter, will I be able to set the resolution to 3440x1440?  I understand that the HDMI 1.4 port usually can handle 21:9 resolutions, but with the latest Intel drivers, it isn't giving me the option for 3440x1440",Negative
AMD,"Hi, can someone help me please? I'm trying to generate a video on AI playground after installing it but it just keeps loading for videos and images don't show up. That's my it it'll specs:   Processor: Intel(R) N95 (1.70 GHz) Installed RAM: 32.0 GB (31.7 GB usable) System Type: 64-bit operating system, x64-based processor Graphics Card: Intel(R) UHD Graphics   -That's a part of what's in the console:    No key found for setting negativePrompt. Stopping generation oa @ index-fOc02QH8.js:61 w @ index-fOc02QH8.js:61 await in w V @ index-fOc02QH8.js:22 pt @ index-fOc02QH8.js:61 await in pt V @ index-fOc02QH8.js:22 c @ index-fOc02QH8.js:61 await in c Il @ index-fOc02QH8.js:14 Sr @ index-fOc02QH8.js:14 n @ index-fOc02QH8.js:18 index-fOc02QH8.js:61 uploadImageName b99fb28ea4440a2f51ce53cd5c529554e5965b66f5f5a8506b9b973b66e754bd.png index-fOc02QH8.js:251 [comfyui-backend] got prompt (anonymous) @ index-fOc02QH8.js:251 (anonymous) @ VM5:2 emit @ VM4 sandbox_bundle:2 onMessage @ VM4 sandbox_bundle:2 index-fOc02QH8.js:61 updating image {id: '6eb88f1b-f433-4541-a421-40619ac9fdc2', imageUrl: 'data:image/svg+xml,%3C%2Fpath%3E%3C%2Fsvg%3E', state: 'generating', settings: {…}, dynamicSettings: Array(3)}       With: RuntimeError: UR error     (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:251 [comfyui-backend] Prompt executed in 116.86 seconds   (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:61 executing Object",Neutral
AMD,Will XeSS 3 and Intel multi framegen be available for Iris xe graphics igpus?,Neutral
AMD,"Putting together a 4k gaming 5090 machine, deciding between the 285k or 265k. Does the extra L3 cache of the 285k make any difference, or does the ~5ns less latency of the 265k make more of a difference?   I plan on a 2dimm board to push memory and OC slightly, but nothing crazy.",Neutral
AMD,"Hi all,  I ordered a contact frame from Thermal Grizzly because the temperature of my CPU wasn’t great, and I was tired of the fan noise.   I installed the contact frame in August, and everything was fine until two weeks ago, when the desktop suddenly stopped booting. The fans were spinning, but there was no POST, no debug LED — nothing. I thought the motherboard was dead.   However, when I removed the AIO head and tried to boot again, it worked!  Since then, I’ve been experiencing intermittent no-POST issues, especially after gaming sessions.   To get it to boot, I always have to adjust the screws on the AIO head. If it’s too tight: no POST. If it’s too loose: CPU temperatures are high. For example, in Hogwarts Legacy, I get around 60 FPS with an RTX 4090, while the CPU runs at around 18% usage @ 85°C. Which is not normal at all, as I used to run the game at around 100–110 FPS.  I tightened the screws on the contact frame using my thumb and index fingers.   I’m looking for advice from anyone who has encountered a similar issue, because at this point my only idea is to remove the contact frame and reinstall the original one.  For context, my setup is from November 2022, and I installed the contact frame in August 2025. I have never removed the CPU from the socket since the first installation.  Thank you all.",Negative
AMD,Anyone know why the performance of my 13700k is so much better when using a pre micro code bios on a z790 board? This even after applying the latest micro code update through windows,Positive
AMD,"My hp elitebook 830 g8 notebook, had some problems booting up and it shows blinking lights (caps key) and it keeps trying to boot up without success, but i after getting it repaired now it boots up but sometimes it still does the same thing and boots up after awhile  I was already using the latest versions of the BIOS and ME firmware, and trying to download and update them again did nothing. However, avter updating the BIOS (to the same version), it showed a message saying ""HP Sure Start detected that the Intel Management Engine Firmware is corrupted"", but it only did so once.  What it does consistently is it restarts once or twice when i boot it. It shows the logo, then turns off, and then boots normally.  In the BIOS, it shows in system information that ""ME Firmware Mode: Recovery Mode"".  Windows also takes much longer to boot than usual, can take up to a full minute.  I can recall that the whole system was super slow at some point, but that was before I started diagnosing any of these details. It works fine now after boot.  Tried all sorts of things, installing ME drivers doesnt seem to do anything, and IDK what to do now.  help",Negative
AMD,"My processor with integrated 11th Generation Intel graphics (Intel Core i3-1115G4) completely lost Vulkan support after installing the latest version, 7080 (coming directly from the previously installed version, 6987). I'm testing the drivers one by one, but it looks like Vulkan support has been completely removed. Did this only happen with my processor, or with all 11th Generation processors?",Negative
AMD,"I'm on Win11 with Killer WiFi 7 BE1750x 320MHz Wireless Network Adapter. PC keeps crashing/restarting. Here is an error:  The computer has rebooted from a bugcheck.  The bugcheck was: 0x00020001 (0x0000000000000011, 0x0000000000210720, 0x0000000000001005, 0xffffe700010059a0). A dump was saved in: C:\\WINDOWS\\Minidump\\121225-18625-01.dmp. Report Id: 189a9fba-5969-4b6c-8199-d8b6a03a1a34.  Copilot had me disable all Killer services except Killer Network Service, but the issue persists. Now it tells me to uninstall the drivers and block them from reinstalling and just use the generic drivers.",Negative
AMD,"I recently bought a 2025 LG Gram 17 (Intel 258V + 140V). Great laptop for productivity tasks which is its primary use for me. I might occasionally game on it if I'm traveling. I only really play Final Fantasy XI these days which was released 2003 so this processor has no problem running it at max settings and \~60 FPS even in Silent mode.  That's when it is plugged in though.  As soon as it is unplugged the performance drops like 80%. I don't mind the performance hit when I'm doing productivity tasks, but would like control over it when I want.  I've tried various things to improve this while on battery:  * Making sure it is designated as ""High Performance"" in Windows. * Making sure Advanced Power settings are the same between plugged-in and battery. * Making sure in Intel Graphics Command Center that Display Power Savings are off. * Making sure in My gram that performance is set to high. * Changing anything anywhere I can find that might be limiting performance on battery.  I haven't started digging through the BIOS yet and don't know that I will. At that point it is too much of a hassle as opposed to some quick toggle(s) in the OS.  Any other places I should be checking or suggestions to address this related to the CPU itself?",Positive
AMD,"Hey everyone, looking for second opinions because this behavior doesn’t seem normal.  Specs:  CPU: Intel i5-14600KF  Cooler: Scythe Fuma 2 (dual tower, dual fan) with oficial LGA 1700 mounting kit.  Motherboard: ASUS TUF Gaming B760M-PLUS WiFi II  Case airflow is fine.  Darkflash DLX case with 9 fans.  Ambient temp \~25–30°C (Brazil)  Before this build I had a Ryzen 7 5800X on an ASUS TUF board with the same case and airflow, and temps were completely normal. No overheating issues like this.  but now, I'm facing the following:  Idle temps sit around 60–75°C  Any moderate load (opening anything, even the bios) causes instant spikes to 95–100°C. When acessing the Bios, the temp shown is 78-88°C  CPU thermal throttles immediately  Heatsink stays barely warm, even when CPU reports 95–100°C  Fans ramp up correctly  I’ve tried:  Re-mounted the Scythe Fuma 2  3 times (check [https://imgur.com/a/ehBnxLn](https://imgur.com/a/ehBnxLn) for how the termal paste was when I remounted  the last time)  Re-mounted the CPU.  Checked power limits  Limited PL1/PL2 to 65W → temps drop but CPU becomes extremely slow, 1˜3GHZ, but still hitting 50-60ºC on idle and 80-90ºC on the rest, 88 on the bios as well.  Undervolted –0.06V, didn't solve.  Has anyone else seen this behavior with 14600KF + ASUS B760 TUF boards?  My friend (chatgpt) recommended a contact frame and said that it would fix it. What do you think about this?  Any input appreciated. Thanks!",Neutral
AMD,"This isn't tech support, but my thread got locked and the mods said to post it here.  My 14700k is starting to give instability symptoms, so I opened a ticket with Intel.  The next day, I got an email asking for more information and was told they would call yesterday if they hadn’t received it. I replied to the email with information and my order confirmation, but as of 4:00 yesterday, I didn’t get a reply.  So, I went into the ticketing system and didn’t see my reply. I added a new comment with the information from my email and received almost an immediate email from the support person granting the RMA.  He asked for shipping information and if I’d choose option 1 or 2. For option 2, he asked for my agreement to the process and my billing address, name on the credit card, and expiration. He said he’d call to get the credit card number.  I replied to the email and pasted the information in my ticket. Oddly enough, I didn’t see his second response in the ticket.  Is that really how this goes down? There’s not an order system where I can input my credit card information? I have to wait for a call from some random support person and give him my credit card number over the phone?",Negative
AMD,"Hi,  I’m trying to register my account for claiming master game key card, but when i put my phone number to complete registration it always “phone number unreachable”.   I already submitted the ticket for support, i worry about not get any replies because of xmast and new year holiday.   Can you help me to complete my registration, i just want to claim the game and play in peace.   I tried to look for solution but nothing have same problem with me ? Is there any possibilities that on weekend the system can’t use A2P SMS for Indonesia Region",Negative
AMD,"Intel ARC b580 apparently unable to run games in the snowdrop engine, but in such a way that it's almost impossible for benchmarkers/testers to catch.  I've been trying to run Avatar Frontiers of Pandora on my arc b580 for over a year now, and I keep having the same extremely unusual crash.  If I freshly reinstall the game, or change my operating system, or switch arc driver version, I can play the game normally, at regular framerates, with no issues, for around an hour. MAYBE two if I'm lucky.   The game then crashes, with seemingly no trigger. No thermal issues, no unusual resource usage, no memory leak, no framerate issues, no stuttering, not even a crash report. The game will just close as if I had closed it manually.   I don't crash in the same location every time, it crashes with different game settings, xess enabled/disabled, different monitors, different ram configurations. I can have the framerate capped or uncapped. GPU maxed out or not, power draw high or low. I can't consistently recreate the crash in the same way every time. The only certainty is that it WILL crash eventually.  After crashing once, It will then crash on game launch, every single time. The game opens, the epilepsy warning flashes up for a second or two, and then the game crashes in the same way, with no freeze, no crash report, just the game closing.  Since the game runs fine for an hour or so after I first install it, any benchmarker testing the gpu will likely never see the crash, because they install the game, run benchmarks, do their tests, and then close it before seeing anything wrong.  The following are fixes that I have tried, and have NOT worked.  * Updating graphics drivers (to both the latest absolute driver, and latest WHQL certified driver). * Downgrading graphics drivers (going all the way back to the first game ready drivers for the b580). * Updating windows (latest windows 10 and latest windows 11 versions). * Running a memory diagnostic * Enabling/disabling XMP. * Verifying integrity of game files. * Forcing the game to run in the Vulkan engine instead of snowdrop. * Performing a complete reinstall of the game. * Replacing one of the .dll files with a ""repaired"" one. (this apparently fixed a crash for some people). * Forcing the game to boot in dx11 instead of dx12.  The following are fixes that I have tried, and HAVE worked.  * Running the game on my AMD integrated graphics.  Obviously running the game on an igpu doesn't offer playable performance, but the game will run, and I've yet to find any crash, even leaving the game open for several hours.  Due to this issue, I'm on the verge of selling my b580 and replacing it with a different card from another manufacturer that hopefully won't have the same issue.  Has anyone had a similar issue and knows the fix? Although I suspect the issue has to be resolved with a driver update.",Negative
AMD,VR Support for ARC GPU? :),Neutral
AMD,"Hello, I'm experiencing an intermittent but severe system hard freezes on a prebuilt PowerSpec G441 desktop purchased from Micro Center 2 years ago. The freezes occur during gaming, mixed workloads (gaming + video playback), remote desktop usage (TeamViewer), and occasionally shortly after logging into Windows with no workload running.  When my PC hard freezes:  \- Mouse is unresponsive  \- Keyboard is unresponsive  \- No audio sound  \- No BSOD or error message     My PC does not recover and requires a manual power button reboot.  After a forced reboot, Windows loads normally and allows me to log in but the system may freeze again within 1–2 minutes, even at idle. Other times, the system may run normally for days before the issue reappears.  No relevant error logs are generated at the time of the freezes. Event Viewer only shows Kernel Power events related to the forced shutdown.  So far I've tried all possible solutions to resolve my issue:  \- Updated motherboard BIOS to the latest available version  \- Updated Windows 11 fully and also reinstalled Windows 11 through USB  \- Performed clean GPU driver reinstallations  \- Verified CPU and GPU temperatures (normal under load)  \- Ran hardware stress tests and diagnostics (CPU, GPU, memory, storage) and no failures detected  \- Tested each monitor individually (dual monitor setup)  \- Swapped HDMI/DisplayPort cables and ports  \- Tested my PC without background applications  \- Returned my PC to stock settings (no overclocks, no undervolts, no XMP changes beyond default)  I've also took my pc to a local repair shop, which they could not reproduce the issue but suggested it may be CPU or PSU related issue. However, when I took my PC to Micro Center, technicians ran similar diagnostics and stress tests but were also unable to reproduce the freezes. They have also tested the PSU voltages and it passed. Despite extensive testing, the issue remains unresolved and continues to occur consistently now.  Specs:  Operating System: Windows 11 Pro 25H2 (Build 26200.7462)  CPU: Intel Core i7-13700KF (stock settings)  GPU: NVIDIA GeForce RTX 3070 Ti 8GB (stock)  Motherboard: MSI PRO Z690-A WIFI (MS-7D25), BIOS v5.32  RAM: 32GB (2×16GB) DDR5-5600 (G.Skill)  Storage: WD Blue SN570 1TB NVMe SSD  Cooling: NZXT Kraken 240mm AIO liquid CPU cooler  PSU: 750W 80+ Gold (PowerSpec OEM, included with prebuilt)  Display: Dual monitor setup, tested individually  I gave a call to Intel and now waiting for a call back. Thank you and I appreciate any guidance!",Neutral
AMD,"Does my 12700f support TME?  In specs, there's no single line about TME. 12700's specs declare TME support. Processor Identification Utility does not mention TME, `hwinfo64` shows TME in gray, and `cpuid -1 | rg TME` prints `TME: Total Memory Encryption = false`. There can i check whether my cpu support TME or not?",Neutral
AMD,"I have an i5-13600kf and a asus strix b760-i motherboard and mostly it runs fine but in specific games I get random shutdowns. Most commonly hell divers and now expedition 33.  I found the vmin instability issue and patched my bios to the latest version now but I still get it happening. Is there any suggested settings changes that have been found to help or is the only course to look at an RMA? I’d rather not be out of a PC for a while so trying to seek help to avoid that if possible.  What’s I find odd is I never get any errors or BSOD. Just the monitor goes black and the system locks up, the fans are still running but it has to be shutdown by holding the power button.",Negative
AMD,"Hi u/Mydst For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues. They'll be able to assist with your Battlefield 6 code availability and on how to resolve this issue.   You may also check these links:  [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Neutral
AMD,"Hi u/afyaff For this kind of inquiry, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry and they may help you on how to input the master key in your PC.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Neutral
AMD,"u/Adrian-The-Great  For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues .They'll be able to assist with your non-working BF6 code from the Newegg bundle promotion.     Additional Information:   [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Neutral
AMD,"u/Broad_Remote3101 This could be a compatibility issue or something with the installation process. Just to help figure this out - are you using a laptop or desktop? If it's a laptop, could you let me know the exact model? Also, what graphics card and processor do you have?  You might want to try following this article too and see if that does the trick. Let me know how it goes!  [Intel® Driver & Support Assistant (Intel® DSA) Results in “Sorry,...](https://www.intel.com/content/www/us/en/support/articles/000026895/software/software-applications.html)",Neutral
AMD,"u/musicmafia77 Based on what you're describing, this sounds like a classic case of Windows 11 being overly ""helpful"" with driver management. To get to the bottom of this and find a lasting solution, I'll need some specific details about your setup.   First, could you check Device Manager and tell me the exact model of your network adapter and current driver version? Since you mentioned you're using wired Ethernet, I'm curious if this is actually a Killer-branded Ethernet adapter. Also, when Killer inevitably returns, which specific processes are you seeing in Task Manager - things like ""Killer Control Center"" or ""Killer Network Service""?  Here's what's likely happening behind the scenes: Windows 11 maintains a driver store cache that can automatically reinstall what it considers ""preferred"" drivers, even after you've removed them. The system recognizes your Killer hardware ID and keeps pulling that branded driver through Windows Update, completely overriding your manual removal efforts. This explains why the KillerSoftwareUninstaller works temporarily but then everything reverts after a day or so.   To truly fix this, we'll probably need to perform a clean driver installation process that involves purging the entire driver store of Killer-related files, blocking Windows Update from auto-reinstalling them, and potentially replacing them with Dell provided drivers rather than Intel generic drivers (since Killer hardware is actually based on Intel chipsets). It's more comprehensive than just running the uninstaller, but it should give you that permanent solution you're looking for.",Neutral
AMD,"Update your bios to the latest version and set it to intel default settings. If the issue is the same, request a replacement",Neutral
AMD,"PL1 should be  65W , PL2 is 219 W you can cross reference this value on this specs https://www.intel.com/content/www/us/en/products/sku/230490/intel-core-i713700-processor-30m-cache-up-to-5-20-ghz/specifications.html   However following intel recommended settings once you load BIOS with microcode patch it should follow the default, you may check also with your motherboard manufacturer",Neutral
AMD,"May I kindly ask you what components are you using (especially MoBo)?      I have HP Z2 Tower G1i with 265K too and what is interesting is that placement of performance and efficient cores is not contiguous (e.g. 8P and then 12E), but somehow ""alternating"".   I observed this on my HP Z2 G1i and thought that it is something special defined by HP Z engineers in ACPI tables (maybe DSDT..), but looks like that it's how things works.",Neutral
AMD,"u/vincococka ,Yes you can use that memory kit safely as long as you don’t enable XMP and keep everything at Intel default settings. Intel officially supports 16Gbit and 24Gbit dies, but many users have successfully used 32Gbit dies without issues. However your memory will likely run at (JEDEC default), which is fully supported. Just make sure, You monitor system stability after installation. Avoid enabling XMP unless you confirm stability with the 32Gbit modules.     [Compatibility of Intel® Core™ Desktop Processors (14th gen)](https://www.intel.com/content/www/us/en/support/articles/000096847/processors.html)",Neutral
AMD,"u/SuperV1234 If the motherboard BIOS allows, disable Turbo and run the system to see if the instability continues.  If the instability ceases with Turbo disabled, it is likely that the processor  need a replacement.",Neutral
AMD,"u/triptoasturias Before I share any recommendations, could you confirm if this is your exact system-[Inspiron 15 5510 Setup and Specifications | Dell](https://www.dell.com/support/manuals/en-my/inspiron-15-5510-laptop/inspiron-5510-setup-and-specifications/specifications-of-inspiron-15-5510?guid=guid-7c9f07ce-626e-44ca-be3a-a1fb036413f9&lang=en-us)? Also, may I know which driver version you’re using and where you downloaded it from—was it from Dell or the Intel Download Center?",Neutral
AMD,"u/triptoasturias Your concern is related to **port and bandwidth limitations**. Most Inspiron models only support **HDMI 1.4**, which is limited to 4K at 30Hz or 2560×1440 at 60Hz. Ultra-wide resolutions like 3440×1440 often aren’t exposed because they’re outside the standard HDMI 1.4 spec. **USB-C to DisplayPort**: This works because DisplayPort has much higher bandwidth and supports ultra-wide resolutions natively. HDMI-to-DisplayPort Adapter, Unfortunately, **passive adapters won’t work** because HDMI and DisplayPort use different signaling. You’d need an **active HDMI-to-DisplayPort converter**, but even then It will still be limited by the HDMI 1.4 bandwidth from your laptop. So, you likely **won’t get 3440×1440 at 144Hz,** maybe 3440×1440 at 30Hz or 50Hz at best.",Neutral
AMD,"u/mercurianbrat This spec can run **basic image generation workflows** (CPU mode or lightweight models), but **video generation and heavy diffusion models will struggle or fail** on this setup. AI Playground’s minimum requirements are currently tied to Intel Arc GPUs with 8GB or more of allocated VRAM.  Currently you can download the installer for discrete GPUs.  We will also publish an installer that will run on Intel Core Ultra-H with built-in Intel Arc GPU (please keep in mind that [Windows allocates half of the system RAM as VRAM](https://www.intel.com/content/www/us/en/support/articles/000020962/graphics.html) for integrated GPUs, so 16GB or more of system RAM are required)  and Intel Arc GPU discrete add in cards with 8GB or more of memory. AI Playground takes up 8GBs of  HDD/SDD requirements: 8GB w/o models,  \~50GB with all models installed.",Neutral
AMD,"u/mano109 As a general corporate policy, Intel Support does not comment on information about products that have not been released yet.  **Visit** our [Newsroom](https://newsroom.intel.com/) for the most recent announcements and news releases.",Neutral
AMD,"**265K all the way.** At 4K with a 5090, you're GPU-bound anyway. The 265K's lower memory latency beats the 285K's extra cache at that resolution, plus you save money for better RAM or cooling.",Neutral
AMD,"u/hus1030  The mounting pressure from your AIO cooler can directly affect whether the system successfully completes POST. When the cooler is tightened too much, it can cause the CPU or motherboard to bend slightly, which may lead to poor or lost contact between the CPU and the socket pins. This prevents the processor from initializing properly, resulting in a no-POST condition. Installing a contact frame changes the pressure distribution compared to the stock retention mechanism, so overtightening the AIO screws can amplify this issue. On the other hand, if the screws are too loose, the CPU temperatures will rise because the cooler is not making proper thermal contact. To avoid these problems, ensure the AIO screws are tightened evenly in a cross pattern and do not exceed the manufacturer’s torque specifications. If the issue persists, you may need to verify that the contact frame is correctly installed or temporarily revert to the original retention bracket to rule out pressure-related problems.",Neutral
AMD,"u/BudgetPractical8748   Intel Default Settings may impact system performance in certain workloads as compared to unlocked or overclocked settings.  As always, system performance is dependent on configuration and several other factors.",Neutral
AMD,Nope got cash back,Neutral
AMD,"u/Any_Information429 Your HP EliteBook 830 G8 is experiencing boot issues due to corrupted Intel Management Engine (ME) firmware, which is a critical low-level system component that manages hardware initialization. This corruption is causing the blinking caps lock light, multiple restart attempts before successful boot, and the extended Windows startup times you've been experiencing. The BIOS showing ""ME Firmware Mode: Recovery Mode"" confirms this diagnosis. Since these issues began after your recent repair, it's likely that the Management Engine chip connections were disturbed or the firmware became corrupted during the service process.  To resolve this, you need to perform a forced recovery of the ME firmware by downloading the specific firmware version for your EliteBook model from HP's support website-[HP EliteBook 830 G8 Notebook PC Software and Driver Downloads | HP® Support](https://support.hp.com/us-en/drivers/hp-elitebook-830-g8-notebook-pc/38216726) and using specialized recovery tools to reflash the Management Engine. You also have check BIOS settings to ensure proper ME configuration and temporarily disable fast boot to allow complete initialization. If the firmware recovery doesn't resolve the issue, this may indicate hardware-level damage to the ME controller that occurred during the previous repair, which would require professional chip-level service or potentially warranty coverage since the problem originated after authorized service work. The good news is that once the ME firmware is properly restored, your system should return to normal boot times and eliminate the restart cycles you're currently experiencing.  USB flash recovery method is definitely worth trying first - it's designed specifically for these types of firmware corruption issues and should get your laptop back to normal boot times without all those frustrating restarts. Check here: [Support Search Results | HP®️ Support](https://support.hp.com/us-en/search/videos?q=BIOS) BIOS Videos",Negative
AMD,"Hi @[Content\_Magician51](https://www.reddit.com/user/Content_Magician51/) Upon checking, there is a new driver version available which is 32.0.101.7082. You may try this and use DDU method to make sure that you performed a clean driver installation. Here are the links of the latest driver and the steps on how to perform DDU.  [Intel® 11th – 14th Gen Processor Graphics - Windows\*](https://www.intel.com/content/www/us/en/download/864990/intel-11th-14th-gen-processor-graphics-windows.html)  [How to Use the Display Driver Uninstaller (DDU) to Uninstall a...](https://www.intel.com/content/www/us/en/support/articles/000091878/graphics.html)",Neutral
AMD,"u/MISINFORMEDDNA  I took a look at your crash error, and here's what's going on. That error code you're seeing (0x00020001) is actually what's called a ""hypervisor error,"" which basically means it's a problem with Windows' virtualization stuff rather than directly being caused by your WiFi drivers.  The real culprits are more likely things like memory issues, BIOS problems, or conflicts with virtualization features like Hyper-V. I'd suggest running a memory test first (just search for ""Windows Memory Diagnostic"" in your start menu), and if you have any virtual machines or Docker running, try shutting those down temporarily, we'll probably need to dig deeper into hardware or system-level issues to really fix this one.",Negative
AMD,"u/strumpystrudel So what you're experiencing is actually pretty normal behavior for your laptop when it's unplugged - that 80% performance drop is totally expected and here's why. When your laptop is plugged into the wall, your CPU can run at much higher power levels (probably around 28W or more), but when you switch to battery, it gets severely limited to maybe 8-15W to preserve battery life. This is especially true for ultrabooks like the Gram that prioritize being thin and light over raw performance. The thing is, a lot of this power management happens at the hardware level with Intel's built-in systems, which is why all those Windows power settings you tweaked aren't really making a difference - the CPU is basically ignoring them and doing its own thing to save battery.  Now, for a game like Final Fantasy XI, you should still be able to get it running decently on battery with some tweaks, but expecting that same smooth 60 FPS at max settings is probably unrealistic given the fundamental power constraints of ultrabook design. Most ultrabooks see this kind of 60-80% performance hit on battery for any sustained workload, so you're definitely not alone in this.   But honestly, this is just how these thin and light laptops are designed to work - they're amazing when plugged in, but they have to make compromises when running on battery to actually give you decent battery life.",Neutral
AMD,"u/Aggravating_Gap_203 I'd recommend running Intel's Processor Diagnostic Tool first to rule out any hardware defects with the CPU itself. Just download it from Intel's website, run the test, and let us know if it passes or fails. While you're at it, try loading your BIOS defaults and make sure your power settings are at Intel's recommended specs - PL1 should be around 125W and PL2 around 181W for your 14600KF-[Intel® Core™ i5 processor 14600KF](https://www.intel.com/content/www/us/en/products/sku/236778/intel-core-i5-processor-14600kf-24m-cache-up-to-5-30-ghz/specifications.html)  [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html)  Your friend is actually spot on about the contact frame recommendation. Before you buy one though, try remounting your cooler one more time - make sure you're tightening the screws in an X-pattern and that everything is perfectly aligned. Sometimes it just takes that perfect mount to get things working right. Let us know what the Intel diagnostic shows and we can go from there!",Neutral
AMD,Hi [RadioFr33Europe](https://www.reddit.com/user/RadioFr33Europe/) I sent a direct message to gather more details for me to review the case and check the status of your replacement request.,Neutral
AMD,"Hi [Designer-Let-7867](https://www.reddit.com/user/Designer-Let-7867/) For issues related to game bundles and how to claim it, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry/issues/error message during claiming.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Neutral
AMD,"u/earwig2000 Let me check this internally. From what I see, you’ve already tried a lot of steps to address the game crash issue. I’ll share an update here as soon as I have more details, and I might need to collect some info from you for further analysis.",Neutral
AMD,"u/earwig2000 Have you tried doing a clean installation of the graphics driver using [DDU ](https://www.intel.com/content/www/us/en/support/articles/000091878/graphics.html)and then installing the latest version from our [download center](https://www.intel.com/content/www/us/en/download/785597/intel-arc-graphics-windows.html)? After that, please retest the game. If the issue still persists, could you share your PSU make, model, and wattage? Also, check if Resizable BAR (ReBAR) is enabled in your BIOS. Finally, review the Event Viewer for any error messages or crash-related events, this will help us determine whether the problem is driver-level or application-related.",Neutral
AMD,u/outlander94 Kindly check this article: [Is Virtual Reality (VR) supported on Intel® Arc™ A-Series and...](https://www.intel.com/content/www/us/en/support/articles/000093024/graphics.html),Neutral
AMD,"u/Kai-juu We trust the technician’s diagnosis of the system. However, since the unit is a prebuilt, it is likely to have a tray processor. Based on our warranty terms and conditions, we can only replace boxed processors. For a faster turnaround time, please first verify whether the processor is tray or boxed using our website. Once you confirm the type, I can guide you through the next steps.  [Warranty Information](https://supporttickets.intel.com/s/warrantyinfo?language=en_US)  [Where to Find Intel® Boxed Processor Serial Numbers (FPO and ATPO)...](https://www.intel.com/content/www/us/en/support/articles/000005609/processors.html)  [Warranty Policy for Intel® Boxed and Tray Processors](https://www.intel.com/content/www/us/en/support/articles/000024255/processors.html)",Neutral
AMD,"u/Sk7Str1p3 I can see you've already tried several tools and are getting mixed results, which is definitely frustrating.  To help you out better, could you share what's driving this inquiry? Are you working on a security project, dealing with compliance requirements, or troubleshooting a specific feature? Understanding your use case will help me point you toward the right verification methods or suggest alternatives if needed. The more context you can provide, the better I can assist you!",Negative
AMD,"Thank you so much for your help!  The ethernet adapter is Killer E2400 Gigabit Ethernet Controller.  I am not super tech savvy, but what you are describing makes sense and is consistent with what I've read about this problem. I will need simple step by step instructions to fix as you suggest. Thanks again!",Positive
AMD,"Intel 265K.  Motherboard was a Gigabye Z890 Aorus Elite X Ice.  32GB Corsair DDR5-6000 CL30.  Gigabye AORUS RTX 5080 MASTER.  Corsair 1000W PSU.      I had ordered another AIO cooler, and when I took the current one apart to repaste and install the new cooler, to test if the cooler was bad, I found that the pin contacts on the CPU had several dark spots.  Either the CPU or motherboard was bad from the get go.  I took it all back and exchanged it for an AMD 9800x3d and X870E motherboard, and it runs flawlessly.  Good stable temps.  35C idle, no jumpiung around, and gaming averages 57C, never passing over 65C.",Neutral
AMD,"Hi, I've tested disabling turbo and I still experienced issues. I've created a support request, case number 06728608, please take a look ASAP.",Negative
AMD,"Actually, this is not the case. After spending hours on this issue, I've finally found the 31.0.101.4502\_A14 drive at Dell's website that is working without any problems with the 3440x1440. So your claim that HDMI 1.4 is limited to 2560x1440 is false. HDMI 1.4 can go further to 3840 x 2160 without any problems. It's unacceptable the lack of support for UWM from Intel graphics cards and drives. Many previous drivers let you choose 3440X1440 resolution. Why did Intel stop supporting this resolution in the last graphics drivers?",Negative
AMD,I was going with the 265K over the 9800X3D since the Intel stuff seems to get better 1% lows and smoother experience at 4k and above. But does DLSS change that? Does DLSS lowering the render resolution push the 9800X3D back into the lead?,Neutral
AMD,"Tried running a clean driver reinstall using DDU. (I'm pretty sure I did this last install too but did it again to double check) and that didn't fix the issue.  ReBAR is enabled, it was on by default.  My PSU is the [Thermaltake Toughpower 650W Gold](https://www.thermaltake.com/toughpower-650w-gold-modular.html)  Windows event viewer did pick up the crash [(imgur link)](https://imgur.com/NhS5e6l), not sure what to make of it though.",Neutral
AMD,"u/Intel_Support Thank you for your guidance. My system is a prebuilt, so the processor never came in a box and is likely a tray CPU. Micro Center told me to reach out to Intel for warranty support, so I just want to confirm, should I be working with Intel directly or do you recommend contacting PowerSpec/Micro Center for the RMA? Thank you again, I look forward for the next steps!",Positive
AMD,"u/musicmafia77 After checking our records, I found that the unit you're inquiring about has been discontinued-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html). While direct support is limited to self-help resources and community insights from other users, I'd be happy to extend some steps that should help resolve your issue.     Based on your initial description, here's what I recommend:     First, please review this article-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html) and follow the troubleshooting steps provided here-[Clean Installation of Wireless Drivers](https://www.intel.com/content/www/us/en/support/articles/000022173/wireless.html). Once you've updated to a newer wireless driver, I'd suggest reaching out to Dell Technologies for assistance with downloading the appropriate OEM driver for your specific system.     If you prefer a more direct approach, you can:  * Visit Dell's website and download the wireless driver for your adapter by entering your service tag in their support portal * Alternatively, run Dell SupportAssist after completing a clean installation of the wireless driver     For additional troubleshooting, here's another helpful article that covers complete removal of Killer software issues, which may be relevant to your situation.-[How to Perform a Clean Installation to Solve Most Intel® Killer™...](https://www.intel.com/content/www/us/en/support/articles/000058906/wireless/wireless-software.html)     If you've tried these steps and the issue persists, I'd recommend coordinating directly with [Dell's support team](https://www.dell.com/support/home/en-us), as they're best equipped to assist with OEM-specific products and can provide more specialized guidance for your particular system configuration.",Neutral
AMD,"u/SuperV1234 Hi, thanks for the update. I’ve reviewed case number and confirmed that it’s currently being handled by our **warranty team** for replacement. You should be receiving further instructions from them shortly.     [Guide to pack your faulty CPU](https://www.intel.com/content/www/us/en/content-details/841997/guide-to-pack-your-faulty-cpu.html)",Neutral
AMD,"u/triptoasturias this explains, The generic Intel® driver provides users with the latest and greatest feature enhancements and bug fixes that computer manufacturers (OEMs) might not have customized yet. OEM drivers are handpicked and include customized features and solutions to platform-specific needs. Installing Intel generic graphics driver will overwrite your handpicked OEM graphics driver (in your case Dell driver). Users can check for matching OEM driver versions at OEM websites. For more information on how the installation of this driver may impact your OEM customizations, see this [article.](https://www.intel.com/content/www/us/en/support/articles/000096252/graphics.html)",Neutral
AMD,"u/earwig2000 ﻿Thank you for sharing this information. I will begin investigating the issue and attempt to replicate it on our end. I'll post an update here or notify you directly once there are any developments. If I need further details, I'll reach out to you here. I appreciate your patience as I work on this matter.   [](javascript:void(0);)",Neutral
AMD,"u/Kai-juu According to our warranty policy, RMAs for tray processors must be handled by the original place of purchase, as clearly stated in the article I referenced.",Neutral
AMD,Unfortunately my Dell warranty support has ended and so far the forum there has not been able to help either. The removal tool works but Killer just keeps coming back.,Negative
AMD,"I also went to Dell, typed in my service tag # and cant find any other ethernet drivers.",Negative
AMD,"If it helps, I'm also using   CPU - Ryzen 5 7600   RAM - Corsair Vengeance 32gb DDR5 6000mhz cl36  SSD - Crucial p3 plus 1tb  Motherboard - MSI b650m-a",Neutral
AMD,u/earwig2000 Please check your inbox; I’ve sent you a message.,Neutral
AMD,"Most commenters are wrong or didn’t read all the articles. Nvidia wants some custom SKUs of server chips to pair with its data center GPUs, and Intel wanted a better solution for pc parts for the GPU tiles.   NVIDIA is NOT getting an x86 license, nor do they want one. In case you haven’t noticed, Nvidia is a $4T company largely on sales of data center GPUs, sales of which were 10x the gaming segment. Profit/earnings ratio of data center is even higher than the 10x.   In my opinion, the only reason for Intel to use RTX chiplets on pc parts is so they can have a few skus for mobile gaming. Lower end laptop chips would still use Xe cores.  This solves NONE of intel’s structural issues - they need, in this order  1. Major foundry customers  2. Regain server market share 3. Data center GPU offerings 4. Move all pc chiplets back to Intel nodes and start to fill up the fabs again",Neutral
AMD,An Intel + nVidia handheld would be something I would welcome just for DLSS4 alone.,Neutral
AMD,"I don’t think it will change that much overall.  It’s no secret that NVIDIA wanted a x86 license for decades now and now they basically got it. This will ""hurt"" AMD by offering competition to markets where AMD currently holds a monopoly like with Strix Halo, most handhelds and consoles.  I don’t think we’ll see big changes on the Intel side.  I don’t think this will affect neither the iGPUs nor ARC because why would it.",Negative
AMD,"this jointventure is NOT about consumer gpu and does not change anythig about gaming gpu of intel, amd or nvidia.  this is instantly obvioiusly from just reading the headlines of the news announcing this.   this tumbnail is just hub using this news to suggest jet another time that maybe amd might just be screwed. they do this because they are neutral in every way.",Negative
AMD,"i think it's bad for us, consumers",Negative
AMD,Was the team up really to crush AMD or Nvidia's answer to enter China?,Neutral
AMD,AMDware unboxed only cares about AMD anyway,Neutral
AMD,This hurts the arc division way more than this could ever hurt amd.,Negative
AMD,They will crush user's wallet,Neutral
AMD,you guys realize Nvidia buys all their stuff from TSMC the same company that makes all of AMD's stuff...So now intel will buy from them too since they cant compete? Because no amount of Intel+Nvidia humping is going to make a TSMC baby...you got to pay to play.,Neutral
AMD,"Even assuming they made monster co-op AMD still has own ecosystem of GPU and CPU, along with consumer support. It's Radeon and Ryzen are still very strong consumer wise. So no, AMD is not screwed",Positive
AMD,Remember Kaby Lake G? No? This will also be forgotten soon.,Neutral
AMD,Yes.,Neutral
AMD,Intel partners with UMC of Taiwan the third largest foundry in the world.    What does it mean for UMC and Intel?   How will it affect transfer / exchange of technology between Taiwan and Intel?   How does this affect TSMC position with USAG?,Neutral
AMD,Foveros baby!,Neutral
AMD,AMDUnboxed on suicide watch.,Neutral
AMD,"For mobile/laptop performance, yes AMD is in the dust, maybe? Makes sense since AMD GPU division is mainly focused on the console partnership with Sony/Microsoft--leaves the mobile platform wide open.",Neutral
AMD,"Who knows, time will tell. But if AMD does find its back against the wall, this is when it does its best. This should be interesting to say the least.",Positive
AMD,"The last time NVIDIA made onboard graphics for a PC, they could hardly be considered graphics. The 3D performance was better than Intel, but features like CUDA, and a good handful of video decoding capabilities were missing. You were much better off getting ATi onboard graphics back then inside of an FX or AMD Bullerdozer chip.  In more recent times, this also applies to laptops with the low end MX chips. Many lack encode/proper decode support, and they are effectively just 3D accelerators. The Intel Onboard Graphics would be a much better choice overall. Whereas an AMD APU would give you the full suite of features.   I'm not entirely worried for AMD. It just means if Intel and NVIDIA manage to not screw up onboard video, AMD will just need to make sure Radeon can remain competitive. I know they have a lot of tricks up their sleeves still, like designing HBM memory onto the die (how Vega was supposed to be originally) and unified memory (soldered down, see the new AI branded chips).",Negative
AMD,Ohh noooerrrrrrrrr,Neutral
AMD,"Actually this may be good for AMD and I will tell you why: Open source drivers. Nvidia if they had a say in intels graphics division would make the intel GPU stuff proprietary and locked down and when support ends its gonna hurt. Meanwhile AMD has open source drivers and in the age that linux is gaining popularity will make it the preferred platform for anyone interested in the growing linux ecosystem. Nvidia will NEED to change how they handle things or intel may not be as attractive as they once were, intel has already suffered a massive knock on thier reputation and this may be the mortal wound that can work in AMD's favor.",Neutral
AMD,"I don't see how this means anything at all for AMD in desktop. This is borderline irrelevant for desktop gaming. Most users pick up an Intel or AMD CPU and an NVIDIA dGPU.  APUs don't really exist in desktop gaming.  In mobile? I think most people won't care much. Intel and AMD APUs both offer compelling and reasonable alternatives for everyday usage. Anyone hoping to game on a laptop as their primary device will choose a model with an NVIDIA dGPU.   On handheld/console, AMD already has the largest market saturation, except for Switch, where Nintendo/NVIDIA opted for an ARM solution.  I don't see this APU being much more than a side choice that offers at best a reasonable alternative to a high end AMD APU, perhaps in the relatively small handheld market, if the performance is significantly better than existing AMD and Intel APUs, due to the DLSS support in a mobile form factor where a dGPU is not a realistic option, and upscaling is necessary for reasonable performance in a limited power package.  In all other cases I dont see this being a compelling option.",Neutral
AMD,So either a 200w igpu with a 300w intel cpu or an intel cpu with a 5030 slapped on it?  I think amd will be fine,Neutral
AMD,welcome to the Nvidia and amd duopoly,Neutral
AMD,"I’m really curious what the power efficiency ends up looking like here - mobile performance/W has continued to be very good in recent Intel platforms, and Nvidia is the market leader in that regard among gpu IHVs. But until now, Nvidia chips being intrinsically dGPUs has completely neutered any potential mobile power efficiency or handheld capability - but not anymore if such a thing can be a chiplet/tile! Interesting days ahead…",Positive
AMD,"By that time amd will have already made an FSR4 capbable handheld, so no significant change",Neutral
AMD,"It won’t affect either cause on parts that don’t need a nvidia igpu why raise the price with it when arc igpu is really good as is, maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive",Neutral
AMD,a partnership doesnt mean they get free reign over license lol,Neutral
AMD,"AMD already has the next gen of consoles. Except for the Switch, which NVIDIA already has (with ARM, not with Intel).",Neutral
AMD,"None, It's a way for NVidia to have another product line and stop Intel from having a competitive product line.",Neutral
AMD,"Not to crush amd, but this essentially just kills arc and hurts the consumer market more. You won't be getting any of those 250 dollar 12gb gpus anymore.",Negative
AMD,Why would it?,Neutral
AMD,"It has nothing to do with the dgpu product stack. Nvidia makes very little from the diypc market. Way more comes from prebuilts, laptops and 90+% from data centers and other enterprise use. In a very real way, the arc cards are not a competitor to Nvidia.",Neutral
AMD,"Unfortunately no matter what given Intel's current state the division is going to suffer. Intel cannot afford to invest in products that will take a very long time to generate profit or may never generate profit. If even AMD cannot compete with Nvidia, then Intel will likely never be able too either",Negative
AMD,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Neutral
AMD,Past != Future,Neutral
AMD,"nvidia also used to make motherboard chipset, with mixed success.",Neutral
AMD,FSR 4 looks like the later versions of Dlss 2 did,Neutral
AMD,"This ""insignificant change"" we definitely want though.  Competition is good in a stagnant market...",Neutral
AMD,"Other than Halo SKU which we are yet to establish any trend of, AMD notoriously drags their feet. RDNA3 will be with us for a while",Negative
AMD,Wont this be targetting the ML workstation space where you have the iGPU with access to >128GB of RAM like Strix Halo with DMA and/or NVLink banswidth?,Neutral
AMD,">maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive  Possibly an overlap like this would be a waste of money developing, and tbh I highly doubt customers would go for the Intel option rather than a Nvidia one, *even if* the Intel option is better performing, because of Nvidia's insane mind share.",Neutral
AMD,Never said that.,Neutral
AMD,Also a way for Nvidia to invest some of their funny money (which is essentially infinite) into a cheap company. Intel under $20 was ludicrous. Intel is still at book value at $29.,Neutral
AMD,Why do so many people think that this will kill ARC?,Negative
AMD,The market for Arc is the same as for Nvidia.,Neutral
AMD,Was that back in the MacBooks having the NVIDIA 9400m paired with a Core2Duo? Those were solid.,Positive
AMD,"No, but it is a reminder on shortcomings to watch out for. The Intel + AMD Partnership with the Intel NUCs several years ago made for some pretty compelling hardware. I still have a NUC8i7HVK in service which works like a champ.  If we are talking about current day NVIDIA with their Discrete Graphics (not the MX budget crap) then their GPUs are obviously smoking anything AMD and Intel. That doesn't resolve some of the Linux problems that still remain with NVIDIA, and that's not going to stop some of the BS NVIDIA does with their hardware around NVENC/NVDEC stream limits. But they have raised those limits, and the open NVIDIA drivers are getting better in time.",Positive
AMD,"Yup. Their nForce boards were great for overclocking, but rather buggy too.",Positive
AMD,"It's not a later version of dlss 2, it's between dlss 3 and 4. Just go watch some videos comparing both.   And btw redstone is coming probably at the end of this year which will improve ray tracing quality and frame gen performance.",Neutral
AMD,"No reputable reviewer thinks that. Most I've seen say it's better than DLSS 3, and very slightly behind DLSS 4.  Why lie about this?",Negative
AMD,They literally haven't said a word about it everyone's just speculating off of Moore's laws anti Intel videos lol consumer is the last thing they care for atm and might be server first cause that makes the most financial sense,Negative
AMD,If they do that in the case the two companies deal fall apart Intel is screwed with no graphics to compete with even the basic amd ones,Negative
AMD,"Well phrased, They now have a seat at the table of Intel to sway votes enough to dissuade them from making competitive products",Neutral
AMD,"Too many people are peeping that MLID podcast. Dude there is very confrontative about his theory that ARC is being ""effectively"" canceled because of some very early roadmap not paying out. Everything is a confirmation bias at this point.   I on the other hand don't get why would Intel forfeit it's competitive advantage with GPU accelerated AI, XeSS or QuickSync and made itself dependant on nVidia. Doesn't make sense especially now, when they have significant influx of cash from both Nvidia and government.",Negative
AMD,"Arc was already not doing well long before. Pouring millions into their gpu division every year with no returns. Their own road maps had their gpus coming out a year sooner but everything got delayed with them resizing the company. Like this deal is not exactly what killed arc, but arc was already struggling beforehand so this was most likely the nail in the coffin.",Negative
AMD,Nvidia does not have an A310 competitor.,Neutral
AMD,But I still remember how many motherboards with Nvidia chipsets I replaced way back.  They used to reliably overheat and need a reflow.  I think it was the HP DV6000 and DV9000 series.,Neutral
AMD,I have not lied,Neutral
AMD,"While workstations dont directly contribute a big chunk of the sales, I suspect they dont want to yield their foothold on the ML/Data Science guys.  If they start getting familiar with non-nVIDIA ecosystems, it could mean indirect losses when these guys start procuring non-nVIDIA dGPUs in servers when they do port their workstation experiments to ""larger"" hardware now that AI is all the rage.  B60 is gaining momentum on homelabs, and Strix Halo to some extent getting popular for those needing more memory for their larger datasets.",Neutral
AMD,"MLID has used clickbait trash thumbnails/titles way too many times, no one should watch that garbage channel to begin with.",Negative
AMD,"I think Intel priced in multiple generations of taking losses when they decided to enter the GPU market. Essentially impossible to walk in to a mature duopoly market and immediately start turning profit as a new player, even with Intel's resources.  Of course I think they also had much rosier projections for their other ventures when they started this journey, so who knows if the GPU losses are still tolerable.",Negative
AMD,Intel doesn't have a current gen A310 competitor either.,Neutral
AMD,Theres no way that nvidia is gonna let intel keep producing entry level gpus that directly compete with their own. I know their deals are in regards to apus and to compete with strix halo but nvidia is not gonna invest into a company with a competitor to their gpus. Like a b580 destroys the 5060 and 5050 while costing less or the same amount. There's no way nvidia invests 5 billion without some agreement that kills off arc as long as they are together.,Negative
AMD,Their nForce boards were excellent for overclocking and tinkering! That's for sure. I also remember them being a bit buggy lol.,Neutral
AMD,IIRC that wss the AMD Turion X2 + NVIDIA 7300Go laptops. Those were notorious for dying.   I also remember some budget laptops with the Turion X2 + ATi 3200 chopsrt graphics would die due to graphics desoldering off the board. The GPU would sit at the end of the heatsink and burn up.   The 9400M MacBooks I remember would get hot enough to start leaving powder from the chassis on my desk lol.,Neutral
AMD,"You did tho. It doesn't look like ""later versions of DLSS 2"". Every reputable source has said it's better than DLSS 3 and close to DLSS 4",Positive
AMD,"They're already at 2, going on 3, generations of losses though (Intel discrete GPU's basically have 0 marketshare, even worse than AMD who are also doing really badly) so even if that were true that isn't a reason to be optimistic.  As a company they also seem to be desperate to get income as well and so there is no reason to assume they're going to tolerate that situation for all that much longer.    I doubt they'll entirely kill off their GPU division, they can still do iGPU's which will be good enough and cheaper than paying NV a royalty, but the discrete GPU line is clearly in trouble.",Negative
AMD,A310 is cheap enough and good enough for it's purpose. If it ain't broke why fix it,Positive
AMD,"Wrong man. You just don’t understand how small the diypc market is compared to prebuilts, laptops, and enterprise- it’s a fraction of one percent.",Negative
AMD,The later versions of Dlss 2 look like Dlss 3,Neutral
AMD,Nvidia probably feels the same about their low end SKUs.,Neutral
AMD,"The DIYPC market largely drives the “prebuilt” market, with a 1-3 generation lag.  Many people who buy “prebuilts” have people they go to for advice (i.e. they talk to their friends/family members who DIY). When there’s a big swing in the DIY market, it typically ripples out to the prebuilt market over a few years/generations.",Neutral
AMD,"Lol, what an explanation hahahaha   And that's still not ""better than DLSS 3"" as most say",Negative
AMD,Yeah lol,Neutral
AMD,AMD Unboxed say anything about that? What about Linus Cheap Tips!,Neutral
AMD,"""Everyone I don't like is biased""-ass answer",Negative
AMD,"can someone explain how this might effect me, as a long term holder, as if I am a golden retriever?",Neutral
AMD,The MTL/ARL/LNL Dates are all wrong for CPU launch it's an OEM roadmap not Intel.,Negative
AMD,Medusa Ryzen... Sounds like something I'd rather avoid in real life.,Negative
AMD,Panther Lake is slipping to Q2 2026 now?    It was originally supposed to launch in Q4 2025,Neutral
AMD,Just hodl until you get the biscuits,Neutral
AMD,"If leak is correct it's bad news.   Panther lake q2 2026, Nova lake q2 2027.    It means 18A will arrive later than TSMC 2N.    Nova lake will likely arrive after Ryzen successors.    Combine that with other performance rumors where there isn't a huge performance uplift. It's bad news.",Negative
AMD,"The dates aren't wrong. As you said, It's the OEM roadmap, and they follow a Q2 release cadence.",Neutral
AMD,Gate All around and Backside power was supposed to be in 2024...  Now it's q2 2026. At this point it's 4N7Y... Which is what we call TSMC cadence.,Neutral
AMD,"This isn't Intel and AMD's road map, this is an ""undisclosed OEM's"" road map.   This doesnt tell us anything about PTL's launch. Just when this OEM will refresh their laptops to have PTL, and its normal for OEMs to launch refreshed models at certain times.   Dell, for example, will usually do Spring, so this may be them. Surface is notorious for being 3 or so quarters after CPU launch.  Look at where MTL is on this schedule, for example. Also around Q2, even though it had been on store shelves technically since the previous year's Q4.  Edit: Looking at the roadmap a second time, this OEM seems to always target a Q2 launch cycle: ADL, RPL, MTL, ARL, PTL, and NVL all follow it. LNL is the only outlier.",Neutral
AMD,I thought Arrow Lake refresh was in the cards for 2025.,Neutral
AMD,Wasn’t Panther Lake going to use TSMC for a portion of the volume?,Neutral
AMD,Yeah I forget to to mention wrong In the sense that it is not Intel CPU Launch OEM roadmap.,Negative
AMD,This entire thing is a mobile roadmap so why are you here?,Negative
AMD,"This ""undisclosed OEM"" is Lenovo by the looks of the font used. Also the timeline makes sense. For example Lunar Lake is shown to be in very late 2024. Lenovo had only one model of Lunar Lake available at that time - the Yoga Aura Edition.",Neutral
AMD,"Yeah if it's Surface roadmap, it's a nothing burger.",Negative
AMD,Yeah some OEMs don't refresh their lineups until the new generation is already launched,Neutral
AMD,I think it was for the iGPU but I don’t remember. I just know the majority of PTL is 18A. I saw a rumor that they were using Intel 3 for the iGPU also so idk.,Neutral
AMD,I also think it's Lenovo. Q2 is when they refresh their ThinkPad line,Neutral
AMD,The 4 Xe3 core iGPU will use Intel 3. The 12 Xe3 core iGPU will use N3E.,Neutral
AMD,"Wonder if Intel will do stacking two bLLCs on a single CPU tile like rumoured as an option with AMD's Zen 6. I iknow this is basically both Zen 5 CCDs having each an 3D v-Cache and NVL seemingly going to have that option now. But two bLLCs meaning 288MBs of stacked cache + cache in compute die would be insane and crazy for gaming.  Regardless I'd love a P Core only RZL SKU with much stacked cache as possible, or heck just 48MB 12P Griffin Cove + 144MB L3 bLLC would be sweet.",Neutral
AMD,"I like how they just throw random words around to pad their ""article"".",Neutral
AMD,The only reason I don't have a dual CCD ryzen is the lack of X3D on the 2nd CCD.  I would 100% be in for a 16 or 24 (when they go 12 core CCD) core dual X3D.,Neutral
AMD,"""leaks""",Neutral
AMD,"When are people going to realize ""Leaks"" are marketing ploys to get eyes on product. Come on people.",Negative
AMD,"Considering the latency is not even expected to be any good with BLLC, AMD is seemingly quite a careful opponent for Intel indeed",Negative
AMD,Scared of their rumor?  Lets release our rumor!,Neutral
AMD,I don't think 2x bLLC could fit on the package. Sounds like nonsense to me.,Negative
AMD,it seems as if right now the best option for consumers is to just build a cheap 1700 or AM4 system and call it a day for the next 4-5 years or so. but that's just me,Neutral
AMD,"My god.  The only reason huge caches benefit gaming, is to benefit rushes to shipment by the publisher, so that game engineers either don't have to optimize, or they can use garbage like Unity, or both.  This is going to make so many mediocre developers better... except no it won't.  It'll just get the market flooded with more garbage.  Also... diminishing returns, and I doubt AMD will put this in Zen 5 -- seems like a trash rumor, to me.",Negative
AMD,"3D cache will eventually become full of assets and dip, if and onlyif IMC is bottlenecked by random memory access.  - the ability for cache to transfer data from memory, to IMC, to cache has a FPS hit (cache hit-rate). This is due to the bottleneck of a chiplet-based IOD: compromising memory latency with cache hit-rate. Eventually the cache fills and relying on IMC to feed the cores anyway.  You can easily test this by monitoring 1% lows and increasing memory latency (lowering DDR4 B-die clocks/DDR5 hynix and increasing timings) to see how Cache holds up when full of assets while memory is being bottlenecked-   What games need is 10-12 P-Cores and an IMC capable of high speeds. Within a monolithic die to reduce relying on cache hit-rate and IOD chiplet bottlenecks.",Neutral
AMD,This is all they needed to sell for gaming again and yet they gave us all the e cores no one asked for jfc.,Negative
AMD,bLLC is not stacked cache,Neutral
AMD,What do you consider random? The article was perfectly clear.,Neutral
AMD,Do you have a specific application in mind? I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.,Neutral
AMD,Some rumors of 16-core dual X3D chip actually surfaced yesterday...,Neutral
AMD,"Isn't the latency similar? The advantage of the 3d cache is an even larger cache size, while the advantage of what intel is going to do with bllc is that there is less of a thermal issue so the clocks can be higher.",Neutral
AMD,Probably only on the skus with less cores.,Neutral
AMD,"They would have trouble fitting just 1 bLLC onto a package, 2 bLLC might only be viable for something like a server chip with a much larger socket.   It may be possible once Intel starts 3D stacking they're CPUs though, but we have to wait and see.",Neutral
AMD,Yeah. Definitely just you,Neutral
AMD,You could literally make that claim with any CPU performance increase.,Neutral
AMD,Speak for yourself. Skymont ecores have raptor lake IPC. We need more of those,Neutral
AMD,"What CPU do you have? If its Zen 4 or slower, your cores are same or weaker than Arrowlake E cores in IPC",Neutral
AMD,This is correct. Stacked cashe comes later. I'd say around 1 to 2 years after. And that comes with stacked cores as well if they go with that path.,Neutral
AMD,"Correlating asset pop in with memory latency is utterly nonsensical.   Edit: Lmao, blocked.  Also, to the comment below, the creator of that video clearly has no clue what there're talking about. There's no ""bottleneck between cache and memory"" with v-cache. The opposite, if anything. Just another ragebait youtuber who doesn't know how to run a test.",Negative
AMD,"> I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.  It does, but i'd rather every core be equal in terms of per-core cache in a local sense. Thread scheduler can handle the CCD split (usually)  There are definitely workloads that benefit from X3D but either scale to many threads or are let down by poor OS scheduling.  It could even be a boon to gaming with well thought out core pinning of game threads.",Neutral
AMD,"No, because it would be the same die. Just won't fit.",Negative
AMD,He has a fair argument with the horrible optimization of modern video games but any innovation in the CPU space is a good innovation at this fucking point,Negative
AMD,Id love a 200 e core cpu with lots of pcie lanes for my home server. But I do not want mixed cores.,Neutral
AMD,Yea but I’m not paying for e cores that are barely faster than my current skylake cores. I just want p cores only.,Negative
AMD,"IPC is just 1 factor  Zen 4 clocks much higher, and got a far superior low latency caching subsystem   No one sane thinks that Skymont can actually beat Zen 4 in most workloads Only if you downclock Zen 4... and guve it slow ram to increase latency",Neutral
AMD,"Now pay attention to the memory bottleneck of chiplet IMC architectures on x3d chips:P https://youtu.be/Q-1W-VxWgsw?si=JVokm7iLScb7xynU&t=700  This simple 1% lows test indicates how texture-pop can impact frametime, directly. Since assets are first stored in memory, then fed to cache, any bottleneck between Cache - to memory will cause delays when loading asset thus 1% FPS... measured temporally(in this case).",Neutral
AMD,The bad optimisation critics aremostly people trying to sound smart by rambling things they dont understand.  Game engines are very optimized these days.   Its funny they mostly pick on Unreal Engine 5 as the prime example... That is probably the most complete game engine out there. So naturally its hard to make all that stuff work well togetter... They have litterally some of the best coders and researchers in the business ... Id love to see you do a better job.,Negative
AMD,Mixed cores are awesome if the scheduler would do what i want and there lies the problem and i dont think even AI can solve that yet.,Negative
AMD,"I said IPC of Zen 4 is the SAME and weaker than Zen 4 is weaker than Skymont in IPC. No more no less. As for clocks, the core scales to max 5.2ghz (default 4.6ghz), certainly overall slower than Zen 4. IPC is a comparison about throughput at a given clockspeed. Its a give to compare you will need to match the clocks.  Funny you you should mention latencies, considering that core to core (even accross clusters), latencies of Skymont are better than Lion Cove? So if latency is the issue, it has nothing to do with Skymont and everything to do with the trash uncore they have based on the Meteorlake SOC. Even workstation Crestmont on a different package performs better. Furthermore skymont IPC is measured based on Arrowlake. Arrowlake is worse latency than Raptorlake and Zen already. Latencies have been accounted for.",Negative
AMD,"I play Warthunder almost daily. It’s ridiculously fucking optimized.   I got cyberpunk when it came out, it was a shit show. Elden ring? Mediocre.  Escape from tarkov? Fucking laughable.  Call of duty? 500 terabytes. Besides this small list,  I’ll wait for battlefield 6 (which by the way also has a history of releasing unoptimized)  The one thing I will say is that developers do work hard after releases to optimize their games. I’m just so use to playing Warthunder which almost always works, everyday, perfectly, with no complaints. lol",Negative
AMD,"Do tell how a bigger cache makes games faster, if you are aware of some details.  And, of course, how doubling the already tripled cache will yield perf improvements worth the huge amount of extra money to make such chips.",Neutral
AMD,Any efforts of Intel to improve efficiency and even performance on old products is a pro-consumer move. Good to see.,Positive
AMD,It's not sorcery. Its just Intel doing the game developers work.,Neutral
AMD,"Is it updated? Last time I tried it, it didn't have an uninstaller and not many games were supported. In the ones I tried, there was no difference at all.",Negative
AMD,"It's very specific scenarios where this can unlock new performance, most games it won't do anything or even lower performance.     So they can't *really expand it* since it's an uncommon scenario. That's why we have so few games that benefit from it.",Negative
AMD,Never count out Intel. They have some very talented people over there.,Positive
AMD,"Regarding the cache optimizations you pointed out, I have a question. Can you test all APO supported games you own and capture the utilization of the E-cores ?  My understanding is that Intel is keeping 1 E-core per cluster active and parking the remaining 3 E-cores. So the lone E-core of each cluster has full access to the 4mb shared L2. For the i9 they can have 4 such solitary E-cores that have their own cache and hence do not touch the L3 cache. The 4 E-cores handle background task while the 8 P-cores with the full 36mb L3 cache deal with the games.  Please correct me if I am wrong.  Also, what cpu cooler do you use ?",Neutral
AMD,What about this post saying APO havent gotten anything in over a year.. https://www.reddit.com/r/intel/s/tTKCn1CnDZ,Neutral
AMD,Does anyone know if changing the Windows power plan affects performance? What Windows power plan do you recommend for the i7 14700kf? Balanced? High performance?,Neutral
AMD,I wonder why APO isn't just tied to game mode. That way it would work for all games from ADL to ARL.,Neutral
AMD,"or just get rid of E cores. I know all the non engineers in here think they are great, but real world they are crap, this is just another in a long line of examples of why intel needs to ditch them.   anyway go ahead and downvoted me becuse being correct in this sub is looked down upon.",Negative
AMD,A few years ago you'd have to tweak the hell out of AMD CPUs to get comparable performance to Intel equivalents. Now the tables have turned.  In either case those who know how to tweak will get top performance not matter the platform.     Kudos,Positive
AMD,Let me teach you a trick.... there is a hack to disable e cores for every game using Intel compatibility tool.... you can try yourself to see if it works,Neutral
AMD,I will follow all!,Positive
AMD,Congratulations on getting a CPU that didn’t fry itself.  Consider yourself one of the few lucky ones.,Positive
AMD,"It looks like Intel APO almost completely disables the efficiency cores for the game when you look at the screenshots I posted below with per core activity.  But this performance boost likely cannot just be gained by turning off the efficiency cores in the BIOS yourself I wager, because it would be too simple.",Negative
AMD,How is this doing the game developers work?,Neutral
AMD,"It's updated as far as I know, but not for every platform.  If you're on Arrow Lake you have the new games that were added, but if you're still on Raptor Lake then you only have the original titles.     This sucks because I own BG3, which is one of the new titles they added recently along with Cyberpunk 2077.",Negative
AMD,"They have added a few new games, but it appears you need to have Arrow Lake to be able to apply APO on them which is BS!  They need to have a better system than this.       The DTT driver should be downloadable from Intel, and not from the motherboard manufacturers who cannot be trusted to make the latest version available.",Negative
AMD,The intel software team is pure black magic when they allowed to work on crack.,Neutral
AMD,"Here is a screenshot showing CPU activity across all cores.  This is with HT disabled BTW, and the cooler is an Id FROZN A720.  It looks like the efficiency cores are not engaged in this particular game when APO is enabled.  And Metro Exodus is the only game that I own in the current lineup for the 14900K.  I know they've added some games for Arrow Lake (like Cyberpunk, Baldur's Gate 3) but those games still aren't available yet for the 14900K     [https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png](https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png)",Neutral
AMD,"Raptor Lake isn't getting new games for APO.  Which strikes me as greedy, since I'm sure it would work fine. Intel originally said that APO wouldn't work at all on 13th gen, which turned out to be completely false.",Negative
AMD,"When combined with Nova Lake's bLLC, it could give Intel a major edge over AMD's X3D.       And yeah, limiting it to certain games and certain CPUs is a real downer for the technology, so hopefully Intel can find a way to streamline and expand it when Nova Lake launches.",Neutral
AMD,"AFAIK, it's recommended to use Balanced mode for proper use of the efficiency cores if you have a Raptor Lake or Arrow Lake CPU",Neutral
AMD,Balanced in full load will just do the same thing as High Performance.,Neutral
AMD,"It's capable of a few things, but the main mechanism from what I've seen is that it temporarily disables three out of four cores in each E-core cluster by forcing them into the lowest power state. The result is that the remaining active core has access to each cluster's full L2 cache (2MB on 12th-gen, 4MB on 13th/14th-gen). L2 cache isn't shared with P-cores (L3 is global), so this can really minimize E-core cache evictions before they're forced into slower memory, and games do love themselves some cache.  It's a genuinely clever way of maximizing available resources and I really wish they'd allow user control over its features, but it seems to be pretty tightly leashed to/by the team that developed it. It obviously wouldn't benefit every situation, such as particularly low/high thread occupancy situations, but it's pretty rough to have the option tied to quarterly updates for one or two specific games.",Neutral
AMD,Or... Just process lasso.,Neutral
AMD,Because it’s optimizations on how it can efficiently use the cpu.,Neutral
AMD,Must the App be downloaded? Or it's automatic after installing the PPM and DTT drivers?,Neutral
AMD,"All of this stuff is super situational. For when it’s supported for my 14th G, I love it.  To call it better than X3D though? That’s a bit much.",Positive
AMD,They are they came up with ML for their upscaller before the red team did and even better than FSR as a dump upscaler and they also did a good RT implementation. But the hardware still has raw horse power issues and they already use a big die for what it can do.  In the 80's or they shammed The DEC Alpha team on subnormal floating point operations with the 8087 FPU. Intel was doing way more precession without having to round so earlier on a consumer FPU while the Alpha CPU was a Mainframe one. Intel also gave it up to help estandardize the way CPUs handle floating point types.  Intel compilers were also black magic becuase they optimized so much C and C++ for Intel specifically but there was always controversy causeless AMD x86 couldn't take part of those optimizations,Neutral
AMD,Were there any background tasks running when the screenshot was captured ? I want to see how the E-cores are assigned for the background tasks under APO.,Neutral
AMD,In certain games. It's not universal at all. There would be scenarios where it gets an edge but if AMD is pushing as hard as rumors state they may still have the better CPUs overall.,Neutral
AMD,"Great explanation that makes sense!  It definitely has to do with the cache, that much is true.   A Raptor Lake CPU with 8 P cores and no efficiency cores, with 36MB of L3 would perform exceptionally well in games.   But the efficiency cores have their uses as well in highly parallel tasks.",Positive
AMD,"Yeah it's a pretty cool, yet simple idea. It would be really nice to give users more granular control natively.",Positive
AMD,Are the e cores clustered in order so we would be able to manually achieve this with process lasso?,Neutral
AMD,Why should game devs be automatically expected to specifically optimize their game for specific architectures of Intel's CPUs? It's not on them to do so.,Negative
AMD,You need to download the Intel Application Optimization app from the Windows store,Neutral
AMD,"I never said it was better than X3D, only that it is useful as a foil.       Remember that Nova Lake will have bLLC, which is the answer to X3D.  But in conjunction with APO, it may give Intel the edge and help them regain the gaming crown in the next cycle provided they can streamline and expand the technology.",Positive
AMD,"Actually, it is better than X3D in almost every GPU/CPU combo where the GPU throttles. Intel 14th gen almost universally wins 1% lows and frequently FPS - almost universally in 4k gaming, but as I said, when the GPU throttles at any resolution, which is almost always, Intel wins.",Positive
AMD,"Eh, they're very good but they benefited vastly from AMDs mistakes and the lessons learned from it. Remember AMD didn't have any ai hardware for their first 2 gens on RDNA and RDNA 3 didn't have enough for FSR4 either. It's not that intel were better at producing a ML upscaler. They just had the hardware for it from the get go so they could implement it quicker.  They did make Xess though which is miles ahead of FSR 3 even on AMD systems (on average. FSR3 is pretty good when modded and can be competitive) so they do have some extremely talented people there. I hope they fund them properly and don't go the AMD route of having Ryzen and Radeon not work together due to pride.",Positive
AMD,"Just  Steam, MSI Afterburner, Windows Security, Nvidia App and my sound card command center",Neutral
AMD,Where to download this APO,Neutral
AMD,Except its THE game devs job to optimize games for multiple cpus and gpus.,Neutral
AMD,It’s literally their job to do so? wtf you talking about?,Negative
AMD,AMD also has Heterogeneous CPUs and even more so ARM cpus.  Devs should start to do this optimizations to schedule the stronger cores.,Neutral
AMD,"Will do, I got a 265K. Performance is already great tbh.",Positive
AMD,I love Intel but with how the company is doing right now I’m in a “I’ll believe it when I see it” type of skepticism.,Neutral
AMD,"Sorry I am making unreasonable requests. Can you test the game with APO on, and a few background tasks ? Like a few browser tabs, a youtube video playing, discord etc ? I am interested in the E-core usage, particularly the spread of E-core usage.   Thanks.",Neutral
AMD,"You need to enable it in the BIOS, me ale sure the DTT driver is installed and then download the app from the Windows 11 store",Neutral
AMD,"Which they already do, and are often overworked and underappreciated for.   But sure, it's their fault they didn't optimize the game even more for a subset of already relatively high end CPUs....",Negative
AMD,"You are massively oversimplifying. A lot of games that support APO, like Metro Exodus EE OP used as an example, were released before Intel released CPUs with P and E cores. Expecting studios to rework and optimize old games for new CPUs is very naive.",Negative
AMD,"They should be focusing on optimizations that help the vast majority of all CPUs, especially older/lower end ones.   It's not their job to specifically help one companies latest CPU architectures because they couldn't figure out how to create a core that has both good area and performance.",Negative
AMD,That's not simply what APO does.,Neutral
AMD,"You should have the full list of current APO optimized titles, unlike myself which only has the original list LOL!",Neutral
AMD,"Everyone should be skeptical, sure, but I choose to be hopefully optimistic also.",Neutral
AMD,"I booted it up again, this time with three Chrome tabs open including one that had an active YouTube video playing.  The efficiency cores were definitely being utilized for the background tasks I believe as I could see a small bit of load going from core to core.     [https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png](https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png)",Positive
AMD,i am not talking about older games. i am talking about newer games.... really dude?,Negative
AMD,Ok…all CPUs include Intel CPUs…I don’t get your thought process,Neutral
AMD,"According to Intel, the app is entirely optional. The DTT driver already includes APO, the app is just an interface to control it and disable APO, if needed.",Neutral
AMD,Thanks a lot.,Positive
AMD,> i am not talking about older games. i am talking about newer games.... really dude?  New games seldom have problems with P and E cores. That is why if you took at minute to look at games that APO support you would notice almost all were released before 2022 before release of Alder lake CPUs. Because older P and E unaware games struggle. Don't really dude me if you are this clueless.,Negative
AMD,"APO includes specific optimizations made for specific Intel skus, not general optimizations that help pretty much all processors.",Neutral
AMD,That is not true whatsoever. Games do not properly utilize the p and e cores and unreal 5 is very known for this.,Negative
AMD,Ok…? If there is a specific problem then they need to optimize it for that hardware. Literally a devs job. Still don’t get your thought process,Negative
AMD,"Dude, Unreal 5 runs like crap on pretty much all hardware. It stutters on AMD and nVidia graphics. Unreal 5 doesn't count.",Negative
AMD,"There isn't a specific problem, it still works lol, just not as well as it could on a specific architecture.   They shouldn't be wasting their finite resources/time on optimizing specifically for an already relatively high end and well performing architecture, but rather attempting more generalized optimizations that help all users.   It really shouldn't be too hard to understand.",Neutral
AMD,"Very cool. Cheers Intel! As a 6900XT owner, I use Xess whenever it's available. The DP4A version got very solid once V1.3 hit.  Hopefully more devs implement it.",Positive
AMD,"I like how Intel still support DP4A model. My meteor lake chip gonna love this XeSS2 with FG and LL.  Meanwhile Amd give middle finger to their consumer who don't have radeon 9000 series, FSR4 doesn't works at all on their previous GPU including the flagship rx 7000 series.",Positive
AMD,Did the DP4A version also improve from 1.3 to 2.0?,Neutral
AMD,"I thought it was a mix of both shader model 6.4+ and DP4a, not either or",Neutral
AMD,"i don't get the performance i got with FSR, but damn, FSR looks like crap in every aspect, i got less performance with xess in comparison, but at least got better framerats than native and get a better image quality",Negative
AMD,Okay but why would I want to use that instead of NVIDIA DLSS?,Neutral
AMD,It’s the least you should get after not getting FSR4.,Negative
AMD,You'd use it over FSR if that's available too?,Neutral
AMD,Except you NEVER got acces to the DP4a version tho. Intel cross vendor locked the DP4a path to their igpus and only allowed Shader Model 6.4 version for other gpus.,Neutral
AMD,"I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4.   On Linux they managed to emulate it using the FP16 instruction, which is available on RDNA3, but it seriously impacts performance and introduces 1,5ms of input latency, compared to the 0,5/0,8ms of latency on RDNA4 with hardware FP8.",Negative
AMD,And they expect people who bought previous RDNA to buy more RDNA,Neutral
AMD,Not by a significant amount.,Neutral
AMD,DP4a only relevant for intel igpus as its crpss vendor locked by intel. Other gpus only get the Shader Model 6.4 version.,Neutral
AMD,for the games that dont support DLSS,Neutral
AMD,"If you own a card that doesn't support dlss.   This is an extra option, it doesn't affect your choice to run dlss or not.",Neutral
AMD,10 series cards will benefit from this,Positive
AMD,It's great news for gpu that can't use dlss because fsr3 is a blurry shitfest.,Positive
AMD,"I mean it gives better image quality than FSR 3.1, so I know I definitely do.",Positive
AMD,Sure. The image quality is generally better. I'd only use FSR if I was trying to squeeze every last drop of performance out of my GPU since it's faster than Xess DP4a. But Xess will be tried first.,Positive
AMD,Do you have a link? Anything I have seen points to the GPU using Xess DP4A unless it's an ARC GPU. So it supports RDNA2 and newer on AMD and Turing and newer on Nvidia.  https://www.pcgamer.com/intel-xess-compatibility-nvidia-amd/  And I do literally run Xess on my 6900XT (currently using it for F7 Rebirth through Optiscaler). So if I'm not running Xess DP4a then what am I running?,Neutral
AMD,">I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4   Sure, but they can make FSR4 for non hardware accelerated like Intel right? FSR 3.1 is so bad even when compared to Intel XeSS 1.3 DP4A, but Amd refuse to improve it because they want to force people to buy radeon 9000 series.",Negative
AMD,"Amd really all in when it comes to copying Nvidia. Guess what? The more you buy, the more you save!",Neutral
AMD,DP4a is cross vendor.   XMX is Arc only.,Neutral
AMD,DLSS is the widely adopted upscaler in the current market. I'd be shocked if a major game in 2025 releases without DLSS at the very least.,Neutral
AMD,1080ti heard no bell,Neutral
AMD,"You run the Shader Model 6.4 version which i stated. There was a discussiona bout DP4a being cross vendor locked but for the life of me i cannot find it again. The article you linked is from 2021, in early 2022 intel cross vendor locked DP4a. The main article of the post clearly states Shader Model 6.4.",Neutral
AMD,where did amd touch you bud?,Neutral
AMD,"Yeah more and more games are adding DLSS support, so I'm not sure when I would need Xess",Neutral
AMD,"That's true but in edge cases where even the lightest Xess option is still too heavy, FSR may still deliver a better overall experience.   E.g. CP2077 on the steam Deck in Dog Town. When I was testing some stuff out (trying to see how close I could get my Deck to the Switch 2 build) I found Xess to be a unsuitable. Even on the lightest Xess mode, the performance hit was a bit too noticeable. FSR doesn't look as good as Xess but it runs better and you can get away with a bit more upscaling artifacting on the Decks relatively small screen. Just rendering at a lower res and letting the screen upscale was also a solid option in that scenario.  As I say though, that's an edge case where you really just need outright speed. If your GPU doesn't support DLSS or FSR4 then using Xess makes sense in the vast majority of cases.",Neutral
AMD,Are you sure you're not getting the DP4A/Shader Model 6.4 and Arc exclusive version mixed up? DP4a was added to Shader Model 6.4. https://github.com/microsoft/DirectXShaderCompiler/wiki/Shader-Model-6.4.  There is a separate path of Xess that is better than DP4A but that only works on Arc Cards.,Neutral
AMD,"It's good to see finally Intel working with game developer to bring optimization to Intel platform. This time not only optimization for Arc GPU, but also for Intel Core CPU like proper CPU scheduling for P and E cores to make sure the game runs on the right thread.  I think this also the reason why they never update APO game support again. Obviously optimizing their platform by working directly with game developer is better than doing it by themself.  I really hope we got optimization like this for more titles.",Positive
AMD,News like this will never been posted on Yahoo Finance or Stocktwits... Instead the news from Short-Sellers and Paid Bashers ...,Negative
AMD,"Im not impressed, my i9 12900k is bottlenecking my 4080 like crazy in bf6 beta :s  Im only getting 100-140 fps on my cpu while my gpu wants me to have 200. Any kind soul who can explain how i increase this?  My cpu is right out the box, i have not done any OC what so ever",Negative
AMD,13700k vs 9800x3d BF6 Beta menu cpu fps comparison (105 vs 370 fps) [https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8](https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8),Neutral
AMD,You sound like an intel stock holder lmao The markets don't care about gaming at all.  They only care about AI slop,Negative
AMD,"the game is just really latency sensitive, i dont think it will even change in the full release sadly  i kinda wonder what makes the game so demanding on CPUs, the mechanics and physics arent much better compared to older BF (like BF4) that barely use modern CPUs beyond 20 - 30%",Negative
AMD,"Dealing with same issue on a 285k + 5090. dipping to 125 fps curing certain scenarios specifically on the cairo map at point C. MY system should not struggle at all at 1440p to reach 144 fps, but the 285k is getting absolutely destroyed by bf6 and I have no idea what the issue is,",Negative
AMD,"It's not like game runs bad on Intel CPU which has P and E cores, i will be lying if i say that. It's about maximizing CPU performance. CPU demanding game will runs with much higher FPS when it knows which thread should be utilized.    This is why game with APO enable like R6S on i9-14900K at 1080p low settings, it got performance increase up to 100FPS which is massive.",Neutral
AMD,"Most games are optimized for consoles which are natively eight cores, and a lot are ported over to PC and so you basically need eight fast cores in whatever CPU you happen to have, and the rest don't really do much. That's why Intel HEDT and AMD's Threadripper aren't of much use in this regard.",Neutral
AMD,"No, that's because and isn't really better for gaming unless you get their x3d variants. Those have a different style of memory that is better for gaming in particular but can underperform when it comes to workload tasks. Idk why 🤷",Negative
AMD,"Yeah and in a game like bf, I only care about the 1% low staying at minimum my monitor refresh rate 🫠 But I have not spend that much time optimizing my settings so I think I can work around it",Neutral
AMD,"I found a ""fix""   Dynamic render scale, I set it to 75, so when shit really hits the fan, it just lowers the render scale for just enough time for me to not drop below my monitor refreshrate, and it happens so fast I don't see the change in gfx",Neutral
AMD,"From what I understand, many games are generally fairly latency sensitive. The fast cache on the X3D chips basically helps with that.",Positive
AMD,Thank you :),Positive
