brand,text,sentiment
Intel,"so the A, B, C, (etc) Series is just branding? and they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name? … but Xe3 plus will be enough to call it C series.",Neutral
Intel,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",Positive
Intel,Everything about Celestial I've heard seems to be regarding mobile. Is there actually going to be dedicated discrete graphics?,Neutral
Intel,The Intel driver downloading a shader cache for games from Intel sounds very noteworthy.   I'm really skeptical tying GPU drivers to a manufacturer's online service.,Neutral
Intel,"They don't care. These will be mid tier chips at best. The RTX IGPU ones will be their flagships. These are still damn impressive though, just have to make that clear.",Negative
Intel,"The mega brains of this community really struggle with product naming lol, its so funny to watch supposed intelligent people straight faced say they can't work out what product out 6 to buy because of their product names, most of them are in different market segments so you can't even accidentally buy the wrong one.",Negative
Intel,"They may divide ABC depending on ISA/hardware capabilities used for example. Like AMD used ""RDNA 3.5"" on mobile as full RDNA 4 would pull more power due to more silicon dedicated to AI (including FSR 4 -\_-). RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4. So Xe 3B will be better than current but Xe 3C will do better at DXR and/or AI frame gen/whatever new tech they added.",Neutral
Intel,"This will help you to understand: https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake  This is not brand new architecture, it's refinement on battlemage to work more efficiently. Thinks of it as Xe2.5 but for some reason marketing is calling it Xe3.",Positive
Intel,">they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name?      No what he said is that the Arc B Series hasn't been around for all that long and the B580 has garnered a very positive reception so they felt in terms of branding and marketing moving in Panther Lakes Xe³ into the B series brand is a better marketing move than throwing away the B series branding and starting the C series branding now. In others words they want Panther Lake to ride on the momentum and positive vibes the B series branding has garnered.   Xe³ is a Xe³ based architecture like Xe³P is architecturally but there is more to selling a product than the engineering, keep in mind there's strong rumors that there will be a B770 launch, Intel also just released the B60 and later this year will provide a big software expansion to Bxx cards so the B series brand is still full of life. Xe³P will launch much later and at that point Intel feels there will have passed enough time to leave the B series branding behind.",Neutral
Intel,An alternative explanation could also be that Celestial dGPUs *are* launching as Xe3P and they would want the launch of C series iGPU and dGPU to be around the same time and the same architecture.,Neutral
Intel,">so the A, B, C, (etc) Series is just branding?  Wasnt it always so? Xe is the architectural names, ABC was just branding for customers.",Neutral
Intel,"No public announcements yet, but logically it would make all the sense for Intel to launch them, and do so in larger quantities so people in the market can get them at MSRP. Battlemage proved there is a lot more demand than there was supply, and it'd take too long to ramp it up once it became clear they were received well. With proof of a market test pass, they've now got a chance to do it right.  They are excellent value products that would allow Intel to get a chunk of that market, given they are available in sufficient quantities at intended price points.",Positive
Intel,Is shader caching on first use a real problem? Sounds like bored people needing something to cry about again.,Negative
Intel,>RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4  Do you have a source for this? I vaguely remember 3.5 wasn't really bringing particular changes,Neutral
Intel,That's exactly what everyone's new GPU generation is.  An entirely new architecture is extremely rare for any company.,Neutral
Intel,That's not what Peterson was talking about context wise when he addressed this in the video.,Negative
Intel,"They would be extremely stupid not to continue to release discrete graphics cards, even if they decide to release 2-3 gaming SKUs per new generation; entry/budget, mid-range, high-end, etc.  - they win. until they keep iterating up, maturing driver, increasing install user base, improving software, until the point they can target higher segments (enthusiast tier)  Gaming GPUs remain the flagship component that pushes the limits of silicon design, features like ray tracing, AI upscaling, and real time path racing all debut in gaming before trickling into pro and AI workloads. Every research report shows that demand for high-end GPUs is still strong. Steam HW survey shows the most popular GPUs are still mid-range gaming cards, showing that discrete GPUs remain central to the PC ecosystem.  And yes, gaming GPUs are now marketed as AI accelerators too, Nvidia and AMDs GPUs both highlight AI compute as much as gaming.  Furthermore, if you look at the Steam HW survey, nobody gives a CRAP about 4k/8k gaming, meanwhile 1440p is growing! Nvidia, the leader in GPUs has been marketing for 4k gaming since 2013 with the 780 Ti/GTX 980 era, then 2016 with 1080/1080 Ti, 2018 with 2080/2080 Ti, 2020 with 3080/3090, and marketing for 8k gaming since 2020 with 3090, 2022 with 4090, and this year with 5090. So from 2013 to 2025 Nvidia has been marketing their GPUs as ""4k ready"" and ""8k ready"" but reality is that majority of gamers don't blame games at those resolutions. 4k adoption is only ~2-3% of users, and 8k is virtually nonexistent. So all Intel has to do is really focus on 1080p Ultra and 1440p High/Ultra gaming, above entry‑level (B570, A580) and below enthusiast (future B770/B780?) and keep optimizing and improving ray tracing and XeSS.",Neutral
Intel,[AMD RDNA 3.5’s LLVM Changes - by Chester Lam](https://chipsandcheese.com/p/amd-rdna-3-5s-llvm-changes),Neutral
Intel,"There are more to that, if you look at RDNA 4 and blackwell for example they had changes to the SMs, enc/dec, etc.   Here it using basically the same cores, but changes are made to better utilize those cores. I think this sentence summarize it the best:  ""Intel classifies Xe3 GPUs as part of the Battlemage family because the capabilities the chip presents to software are similar to those of existing Xe2 products"".  Now I'm not saying it's a bad thing, Intel has a lot of inefficiencies with Battlemage, a lot of time you expect it perform better judging by the amount of hardware. By going this route the basically giving pretty good improvement only year after initial release with basically almost the same drivers. It's just not a new generation but something in the middle between a refresh and a new generation.",Neutral
Intel,"Not rare, but more like it only happens when there's a paradigm shift in graphics that necessitates it.   GCN -> RDNA or Pascal -> Ampere are the most recent ones.",Neutral
Intel,"its not extremely rare. It happens once every 7-10 years or so when they redesign it for significant changes. but yeah, most generations will be refinement.",Neutral
Intel,"In the end it is all matter of marketing, filter it for a moment and understand what it's. As he said ""leverage all the good work from battlemage"". As I read it and looking the architecture it's battlemage with some enhancement that will come to future product celestial. Something like RDNA 3.5. I think they messed up the naming - they want to present it as something new and different (it's in some aspects) but it's not the real next gen in their roadmap.",Neutral
Intel,By discrete iam assuming you mean client graphics SoCs? The margins are not enough on those - you make much more money on Pro cards/ HPC.,Neutral
Intel,"No this is not the case. The decision is purely branding. Xe3 and Xe3P are part of the same generation architecture wise unlike Xe2 and Battlemage, he addressed this.",Neutral
Intel,It's this. IGPU's will be for gamers and DGPU for data center. They basically both have said this :) People just don't notice anything. What do folks think RTX IGPU's on intel chips are gonna be for. LIKE LOL!,Neutral
Intel,"Then why he said ""when we move to our next architecture...""? Meaning they are not the same (Xe3 and Xe3P), also look at the slide they're grouping Xe2 with Xe3 under battlemage.  What you say contradicts their statements, and the article. The decision is of course marketing but nowhere they say Xe3 and Xe3P share the same architecture. This is why I said before it should've been called Xe2.5 or something of that sort like with RDNA 3.5.",Neutral
Intel,Next architecture is just rearrangement of existing cores into more effective set up. No contradictions.,Neutral
Intel,"When we will see deep dive into X3P celestial we will see what have changed. Until then it's just speculation of the differences between Xe3 and Xe3P. My bet there will be more changes than mere ""rearrangement of existing cores"".",Neutral
Intel,"Love a good TAP interview, hopefully he'll do some podcasts too once the embargo lifts.",Positive
Intel,"I like how TAP said ""expect even bigger investments in graphics in the future"". I hope this means what I think it means, because Intel \*must\* continue heavy investment in their own graphics. Accelerated compute is the future.",Positive
Intel,MLID must have an aneurysm seeing the guy still employed at Intel,Neutral
Intel,"Tom's an absolute gem. It's so frustrating to see the most passionate and effective communicators locked up behind embargoes and NDAs, so fingers crossed that this marks the next wave of tech exploration and deep dives. Always nice to see him pop up in a random GN or PC World video.  Credit where it's due, Lex (Alejandro) has also always been attentive and forthcoming in his Reddit interactions, even factoring in the usual embargo/NDA constraints. Hopefully he'll dust off his u/LexHoyos42 account; I'd love to see some mini-AMAs as Panther Lake trickles out.",Positive
Intel,Igpus not discrete gpus,Neutral
Intel,"I would not attempt to read much into claims like that. This is marketing, not an investment roadmap. I'm sure if you pressed them on it, the lawyers would have some words.   Remember, they've yet to even *talk* about dGPUs past BMG. And the big-iGPU NVL is reportedly cancelled (likely in favor of the Nvidia partnership).",Negative
Intel,Hahaha I can hear Tom saying “fucking Tom Peterson has been *lying* to you” as you say that,Negative
Intel,Problem is we don't know the real roadmap is for Intel. I really want them to put out a roadmap like they used to. But things in flux probably :s.,Negative
Intel,I think only way for intel to enter consumer GPU space is to start via iGPU at this point.  They just need to make bigger faster iGPU to make Nvidia RTX xx50/RTX xx60 redundant.,Neutral
Intel,I don't think partnership Nvidia iGPUs will be ready until TTL/HML circa 2029/2030.,Neutral
Intel,"Agreed, but then similarly I would also not attempt to conjecture on their roadmap outside of an investment roadmap... lack of a public roadmap update on graphics in a while is certainly saying something by not saying anything, but it's also not saying cancellations, etc...",Negative
Intel,There are just too many Toms in tech media. Tom of Moore's Law is Dead needs to thin out the competition.,Negative
Intel,I don't think even Intel knows what the real roadmap is anymore. They're so far behind on the previous ones that it's not surprising that they're not releasing new ones.,Negative
Intel,We are not competing with higher end GPUs. It seems they want to capture the budget segment and then move up the ladder.  But I don't like that iGPUs has more priority than dGPUs. Hope celestial comes into life in future,Negative
Intel,"Probably, but no sense offering an in-house big iGPU for just one gen. It needs to be a long-term commitment.",Neutral
Intel,"I mean, the layoffs tell a story. The dGPU group, and client graphics SoC in general, was gutted even before Lip Bu came on. To say it's likely a skeleton crew is probably doing it a favor.",Negative
Intel,You stole what I was going to say... take my upvote.,Negative
Intel,"thats how they are doing it.  iGPU is the one that killed Nvidia MX, or GT xx30 series. So their next target is the RTX xx50.",Negative
Intel,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  One thing seems clear, this solution will be cheaper than standard AI data center products.  Hopefully, Intel can get and maintain footholds in medium-sized AI research market",Neutral
Intel,No way that power usage is at idle. Even if there was only one performance state like Tesla GPUs they wouldn't consume that much power.,Negative
Intel,that's insane vram density,Neutral
Intel,"This thing only has a 1Gb Ethernet port? I don't know much about the use case for this thing, but that seems surprisingly low. Simply from the use case of uploading training data to this I feel like something faster is necessary.",Negative
Intel,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Is there a fork of chrome that runs on gpus,Neutral
Intel,"Now do it with pro dual version of b770s, 64gb each. Could be a far more economical inference solution than what AMD is providing",Neutral
Intel,but is that faster than a single 5090?,Positive
Intel,Is this enough VRAM for modern gaming?,Neutral
Intel,Nvidia: ill commit s------e,Neutral
Intel,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  If they rely on PCI-E for interconnect, bandwidth will be anemic between cards for training purposes. Might be ok for inferencing, but for training, you need the bandwidth.  That's not unclear at all.  This thing is probably not useful for anything, but serving many users with small models for inferencing.",Neutral
Intel,"You think Intel is going to maintain these footholds any longer than they would need to?  This push of their dedicated GPUs into the professional space, is just a mere attempt to get rid of the unsold inventory of ARC-GPUs at the highest possible price-tag right from the beginning – Dumping those into the enterprise- and datacenter-space, in noble hope that someone clueless might bite upon the Intel-brand, and then dip into it at high costs *at non-existing support software-support* Intel pretends to deliver some time in the future.  Since as soon as the bulk of it was sold, Intel will cut the whole division and call it a day, leave anyone hanging dry.  That's what's planned with the whole professional-line of them – Buying this, is a lost cause as a business.",Negative
Intel,"I think they've just taken the 400W per card TDP and multiplied it by 16 to get the 6400W figure, so I think they're trying to say minimum power draw at full load, not at idle. ""Minimum power draw"" is definitely really odd phrasing for that number. Surely you'd think they'd say ""Total GPU TDP"" or something less confusing",Neutral
Intel,I don't think servers are supposed to stay idle for long.,Negative
Intel,"> the 768 GB model uses five 2,700 W units (totaling 10,800 W), while the 384 GB version includes four 2,400 W units (7,200 W)    And an insane power consumption.",Neutral
Intel,You basically always put in dedicated NICs in those kind of servers. The onboard ethernet is rarely used.,Neutral
Intel,Could that make it very cost effective for any particular use cases?,Neutral
Intel,"Even the fancy NVIDIA interconnects are a bottleneck for training performance, as are the memory bandwidth and even L2 bandwidth. One architects the algorithm around that. A tighter bottleneck is obviously not ideal, but it might still make sense depending on the price of the hardware.  As long as a copy of the full model and optimization state fits on the 24GB bank the PCI-E interconnect isn't really too limiting for training. Small models, like a 1B parameters voice model, will naturally fit. The larger the model you want to train, the more complex the architecture might be, but there are ways to train even models over 100B parameters on such hardware, like routing distributed fine grained experts. Not that I think it's a good idea at this point.",Neutral
Intel,It's VideoCardZ so they probably got information from a tweet or Reddit post and copy/pasted without doing any thinking whatsoever.,Neutral
Intel,"I assumed that would be the case, I just didn't see any room for expansion. It's pretty crowded with 16 full sized cards.",Neutral
Intel,"I don't really see any, unless you can find a workload that will fit the card and multiply that workload for 32 users, but as each chip is performance wise less than a 3090, it has to be a fairly light workload.",Neutral
Intel,"1B might work for a voice model, but useful LLM models ARE 100GB+. Smaller models are fun little toys.",Positive
Intel,At least its a human hallucination and not AI hallucination.,Neutral
Intel,Can also be bad translation.,Negative
Intel,"according to the datasheet theres two free pcie 4.0 x16 slots on one version and two pcie 5.0 x8 slots on the other one. Both say ""support high speed NIC"" so I assume there is at least enough room for that",Neutral
Intel,"I wonder if these might make good controllers for ai powered robots. Something highly specialised, like a fruit sorter or something. I don’t know. I’m just hoping Intel can find a buyer because it’s good for the market to have more competition",Positive
Intel,"It's a thing where the next generation of this card would be a viable competitor for the generative AI crowd, but not this one, where it can't compete with a 5 year old 3090.  For robotics, you just want inference. There are much better options focused on low power, small form factor NPUs custom made exactly for that, and they can be paired with small SBCs like a Raspberry Pi.",Neutral
Intel,No word on OMM so I guess that's reserved for Xe3-P. Wonder what other changes that architecture XE3-P will have.  The new Asynchronous RT Unit functionality in Xe3 is very cool and now Intel also has Dynamic VGPR. Neat.,Positive
Intel,"It's interesting they lump it in with B-series, given the architecture is pretty distinctly different from Xe2 used in Battlemage. Note that BMG/Celestial names have been explicitly used for dGPUs, however. They're not synonymous with Xe2/Xe3.",Neutral
Intel,"It would be nice if Intel released a small, budget friendly, passive cooled, PCIe powered dGPU with multiple QuickSync units, made specifically for AV1 encode and H.265 4:2:2 chroma with 10-bit depth.   It's unfortunate that one has to cough up $2,000+ (at least around here) for an RTX5090 to have three NVENC units at their disposal, and the 5090 still can't compete with a passive-cooled Apple Macbook that draws a couple of watts at peak.   You know things are crazy when Apple is the only company that caters to your specific needs!",Neutral
Intel,"Hello Dangerman1337! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Very bleak chance that Intel will continue with discrete gpu line up. Pretty shit decision tho,Negative
Intel,"So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In&nbsp;card under this precisely defined term.  Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – *In favour of Nvidia-GPUs?*",Neutral
Intel,Xe3P needs that feature parity with RDNA 5 and RTX 60 overall since its a 2027 product.,Neutral
Intel,"hopefully that means NVL-AX is not getting axed. at least, the 24 core die. heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch. super weird, not believing too much into it. curious about Druid, people seem optimistic, idk why lol.",Neutral
Intel,yeah super weird -- but that does bode well for the next dgpu being based on Xe3,Neutral
Intel,"Arc Pro B50 was pretty interesting to me, it was a modern 75W card that isn’t gimped.",Positive
Intel,With a board with lots of PCIe x16 slots you can stack A310's or A380s and get along fine. I do 3 transcodes of av1 great on a single one right now.,Positive
Intel,The reason they'll never make this is the same reason they don't put SR-IOV on consumer cards.,Negative
Intel,Can you do this work on a $600 Mac mini? Or do you need something more expensive.,Neutral
Intel,"Yeah prob, but by how much. Looking forward to the post CES reviews and Xe3 better be a major leap over Xe2.",Positive
Intel,They will if they have xe cores that gen. They'll only fully kill it if they abandon that core development entirely to exclusively use Nvidia graphics tiles,Neutral
Intel,">Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  What in the world are you talking about. This roadmap says nothing of the sort.",Negative
Intel,"No, they are saying the GPU in Panther Lake is Xe3, but instead of classifying it as a new-gen architecture, they are saying it's just an extension of Xe2, so they've put it under the B series product family.",Neutral
Intel,100% but I fear this is another botched gen so it will at best be on par with current gen offerings.,Negative
Intel,"Intel buying chips from Nvidia won't be as cost efficient as them developing their own chips. Intel can't abandon GPU development in the short term or it'll be left behind in the long term (Nvidia can easily sell their Intel shares and pivot away from Intel leaving Intel with nothing).    If Intel will stick to making dGPUs and expand to competing in gaming with said GPUs they benefit greatly from the experience and talent that dGPU development yields, this makes dGPU development a no brainer. If AMD can afford to make dGPUs Intel definitely can as well.",Neutral
Intel,"> heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch  Would be very interesting if so, though I'd be surprised if projects get resurrected in this spending climate. Competitiveness might also be tight in '27.",Neutral
Intel,Celestial was based on Xe3p.,Neutral
Intel,"Then that can't just be a minor tweak, but perhaps it's just market segmentation.",Neutral
Intel,I don't think Mac minis support hardware AV1 _encoding_. Apple only recently supported hardware AV1 _decoding_ so their devices aren't the best suited to AV1 media,Negative
Intel,> They will if they have xe cores that gen  Why do you believe so? They made iGPUs without dGPUs before.,Neutral
Intel,"They won't Nvidia tiles are for specific market segments, they aren't a one size fits all solution.",Neutral
Intel,That's not what my colleague's say,Neutral
Intel,It wouldn't be a Helpdesk_Guy comment if it didn't include insane made up bullshit to disparage Intel.,Negative
Intel,"Exactly. *Good Lord are y'all shortsighted!* Are you falling for their shenanigans and typical tricks again?  Crucial is, what Intel does NOT mention – That is anything future dedicated graphics, as in *Add-In&nbsp;cards*.  They're purposefully declare PTL as de-facto dedicated GPU, yet any thing of future **d**GPUs is ABSENT.",Negative
Intel,"Congrats, you looked at the pictures. We all did. Now look at it *again* …  Now explain to me, why ARC alchemist is separated (since it's a line of dedicated graphics-cards, duh?!), while Battlemage gets PTL's X^e Graphics iGPU-tiles put alongside, despite such tiles classically does NOT count as a graphics-card but rather a iGPU (no matter how large).  The actual answer is, that this on the slide was surely NOT done slide by accident, but it figuratively puts PTL's X^e Graphics tiles of PTL's iGPU *on par* with their dedicated Battlemage-cards, which (at least in Intel's eyes) shall from now on signal a ""continuation"" of  Battlemage itself. Thus, there won't be any further dedicated graphics-cards like Celestial as classical Add-In cards, which still is way overdue anyway …  Since if not so, then WHY are PTL's iGPU-tiles labeled as ""Intel ARC B-Series""?! *You see the virtue signalling?*    You're also aware, that Intel recently dropped the Intel Iris&nbsp;Graphics theme and now calls all iGPUs ""ARC""?",Negative
Intel,">Intel can't abandon GPU development in the short term  Well Intel now uses the same architecture across both their iGPU and dGPU, so even if their dGPU products get killed, iGPU demand(indirectly, through Core Ultra processor demand) can fund continued GPU architecture dev. Intel still has the majority market share in the iGPU market, so they have the demand to justify continued R&D into GPU dev for iGPU alone.",Neutral
Intel,Xe3p is a significant architectural advancement says Tom Petersen.,Neutral
Intel,"Care to elaborate where I was blatantly wrong and it didn't actually turned out mostly or even exactly as I predicted prior when popping the prospect of Xyz, often months to years in advance?  A single example would be sufficient here, mind you.",Negative
Intel,"They did not declare PTL as a de-facto dedicated GPU. By definition it's *not* a dedicated GPU and they never said it is.  They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs. You're trying to fill in missing info with a nonsense interpretation.   What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".",Negative
Intel,"So much buzzwords, yet it sounds like a stroke.  You need help.",Negative
Intel,"Alchemist had both desktop discrete and mobile discrete. Intel's roadmap isn't a 'graphics card only' chart, it's an IP roadmap. That's why Panther Lake's Xe3 iGPU shows up under Arc B-series, they are saying Xe3 is an extension of Xe2, where as Xe3P (Celestial) is the next Arc family.",Neutral
Intel,"> A single example would be sufficient here, mind you.  Sure, here you go:  > So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In card under this precisely defined term.  >Their non-show is basically code for: “We're canceling everything dedicated, and our eGPUs we consider now dGPUs.”  >So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – In favour of Nvidia-GPUs?",Neutral
Intel,"> They did not declare PTL as a de-facto dedicated GPU.  Except that's exactly what they did – look at the damn pictures my friend, Intel purposefully group it into the same category as Battlemage's dedicated GPUs and Add-In graphics-cards and thus, iGPU-tiles as on par as dGPU cards.  > By definition [e.g. Panther Lake's iGPU-tile] it's not a dedicated GPU and they never said it is.  \*Sigh I *know* this, well all do! Yet Intel wants to pretend, that PTL's on-die eGPU X^e Graphics tile is now basically declared, what *Intel* now considers a **d**GPU. The implication is strikingly obvious here.  > They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs.  Ex-act-ly! You really don't get it, don't you? They deliberately leave everything what *classically* would count as a dGPU open and without ANY path forward, meanwhile their X^e 3-tiles are now considered dGPUs by Intel.  > You're trying to fill in missing info with a nonsense interpretation.  No, I don't. I'm just one of those being able to decypher Intel's typically backhanded tricks and try to explain that they're virtue signaling the worst: ARC as a line of dedicated graphics-cards is dead and won't get a follow-up, instead, Intel now considers it replaced by iGPU X^e Graphics-tiles (like the one in Panther Lake).  > What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".  Their trickery here, that they basically announce the discontinuation of ARC graphics-cards through the back-door.",Negative
Intel,I wonder why they're using such naming. If Xe3 is just Xe2 enhanced then why not call it Xe2.5 or Xe2+. And if Xe3P is a new architecture why not call it the real Xe3?,Neutral
Intel,"Just wait and see. I sadly have a horrific on point streak to remain in the right with anything Intel, even if I call things months to years in advance.",Negative
Intel,"The implication here is incredibly obvious:  ""Intel Arc B-Series consistes of Battlemage discrete graphics *and* PTL iGPUs"".  That's the implication here. It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL",Neutral
Intel,Because it's most likely still the same foundation. Xe3 just have additional optimizations and HW blocks along the lines of OMM (confirmed) and whatever new tech Intel decides is worth pushing for higher end parts.    But perhaps it's more of a RDNA 3.5 situation. Some early Druid tech backported to XE3P.    Yeah I have zero clue just guessing xD,Neutral
Intel,"> It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL  Of course not, since it would instantly tank their reputation and will kill all further ARC-sales?!  Do you actually think, that Intel is that stupid to plain announce the death of ARC-series graphics-cards? Do you think they'd even sell a hundreds cards after such a announcement? Who would by a dead-end line of graphics-cards, which won't even get any further driver-support and still have disastrous drivers like Intel's ARC? NO-ONE!  Of course they virtue signal it through the backdoor here, to preserve the sell-off the of the rest of pile of shame of their B-series graphics-cards as long as possible (leaving users with none support afterwards), and *only then* pull the plug afterwards when they went through their inventory …  Since when are you following Intel? Intel has always done so, and only announced a sudden discontinuation once the inventory was sold (leaving customers in the open with no further support overnight), despite virtue signalling a continuation all the time (to preserve a proper sell-off).  ---- Wasn't this exactly the same story with **Optane** back then? Each and every question upon a future support was answered in the affirmative, only to pull the plug overnight again as usual …  I'm not another piracy of cons here, it's Intel pulling the same Optane-stunt with ARC – Happily selling off their cards, knowing darn well, that they will pull the plug, yet refuse to actually play with open cards again.",Negative
Intel,"> > It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL >  > Of course not  Great, we're all agreed that you're making shit up when you said  > Panther Lake now Intel classifies as a dGPU",Negative
Intel,Just letting it be known that the [original source](https://www.reddit.com/r/IntelArc/comments/1nomuhk/driver_built_xess_frame_generation_might_be_on/) is over at r/IntelArc,Neutral
Intel,"Intel ""ExtraSS"" frame extrapolation tech has been in development for quite a while. I really hope it really does come with B770. Will be exciting to see how the implementation differs from MFG.",Positive
Intel,Could be? Wasn’t one of the lead developers for Asahi Linux’s graphics stack hired by them for this specific purpose?,Neutral
Intel,I do have to wonder how much we are missing out by these features being proprietary rather than having graphics vendors work with other stake holders and each other to make open cross compatible upscaling and frame generation techniques. It's been great for nvidia but bad for the ecosystem as a whole for everything to be so fractured.,Negative
Intel,"Yet more evidence Intel is likely not scrapping arc, as if we couldn't guess they know the trajectory of past nVidia semicustom affairs.",Neutral
Intel,"Nice to see. Still pretty happy with my Arc card, good enough for game and resolutions I’m playing and the AV1 hardware encoder is just excellent.",Positive
Intel,Wonder if well be getting ray reconstruction andmaybe even a transformer model soon.,Neutral
Intel,we are witnessing the downfall of pc gaming in real time,Neutral
Intel,Why should they? They are going to buy NVidia GPUs for everything now.,Neutral
Intel,"Hello brand_momentum! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,This could be awesome more completion The better,Positive
Intel,Doubt anyone would use it for practical application. FSR Frame Gen is awful to use.,Negative
Intel,"Multi-Post News Generation, with three articles interpolated per source.",Neutral
Intel,I'm excited for Intels new GPU,Neutral
Intel,"Microsoft has been way too slow in the DX12U era, and Kronos is even slower. Microsoft's new APi that integrates AI upscalers isn't even out yet and it doesn't even cover Frame Gen or AI denoising. By the time MS moves to integrate those they'll be behind on something else.",Negative
Intel,"They aren't, though. FSR Frame Gen works on Nvidia Hardware",Neutral
Intel,Honestly we are gaining more from proprietary features than we lose from them being locked behind certain brands. Currently them being proprietary gives Nvidia an incentive to push the envelope and the head start they have isn't that big so they're incentivized to keep making new features   If everyone was required to share their new features then the incentive for innovation disappears,Neutral
Intel,Competition *,Neutral
Intel,Obligatory article quoting reddit post quoting another article quoting original reddit post.,Neutral
Intel,"Fake frames, fake articles! /s",Negative
Intel,Sometimes I prefer vendor solutions today than waiting for microsoft for years like Direct Storage,Neutral
Intel,"Yes but FSR support is all over the place. Look at what optiscaler is doing, we could have had an open standard for upscalers that FSR XESS and DLSS could have been built on top of meaning much wider support across games instead of every game needing specific implementation and leaving us with outdated upscalers that we need driver overrides and DLL swaps to get around.  Microsoft is only now working on a directX based upscaler API that solves this problem. We should have had something like that years ago like we did for RT.",Negative
Intel,"I mean, surely it depends on when they have to share it. If they can release with the feature exclusively first, but then have to share at or after release, then it can still help the current generation of their cards until it's time for the new gen.",Neutral
Intel,"It makes adoption slower though. Especially for smaller devs and the smaller graphics vendors. Even now with pretty large games we still have software lumen only. If RT was pushed as an open standard earlier we might actually have more games and better implementation across the whole market, not just nvidias pet projects.",Negative
Intel,"DirectStorage 1.1 (the actually usable version unlike 1.0) was supported by Windows 9 months after PS5 release. They werent that far behind on that, noone just was interested.",Neutral
Intel,"patents expire after 15 years, they will have to share it then.",Neutral
Intel,Nvidia controls 90% of the discrete GPU market on PC.   Anything proprietary that they make essentially becomes the standard going forward.,Neutral
Intel,"Huh?  1. RT in games is now done through DirectX APIs, which are vendor agnostic.   2. For GPU features, NVIDIA usually introduces them through a DirectX standard, they are just usually the first vendors to support it until others catch up.   3. RT is limited because current gen consoles are not good at RT. AMD took multiple years to adopt RT or AI acceleration cores.",Neutral
Intel,I actually have not seen an open source or even cross vendor solution with any reasonably quick uptake. Its a theoretical myth at the moment,Negative
Intel,the issue with RT isnt standard. Its that consoles/amdGPUs simply didnt support it for away too long.,Negative
Intel,"...It's down to 15 now? I thought it was 20.   Although. Software is more copyright than patent anyway, and copyright is, like, 70 years after the author dies, or 95 after publication for corporate works.",Neutral
Intel,"> RT in games is now done through DirectX APIs, which are vendor agnostic.  Now being the key word, Nvidia launched their RTX before DXR was even available.  AMD's slow adoption is absolutely one of the problems which might not have not been so delayed if AMD, Nvidia, console makers and Microsoft worked on RT together from the start.   This is all just musing really. Maybe nvidia did us all a favour by breaking the mold forcing everyone else to play catch up.",Negative
Intel,"Depends on country/industry. Copyright also depends on country, but for most civilized word its authors death + 95 years (thanks disney). Copyright however has many exceptions to it, well, at least in the west. In Japan it does not and technically you can get sued for saying Zelda is fun because you broke copyright by saying Zelda without permission. Its why Nintendo is able to get switch reviews removed from youtube.",Neutral
Intel,The DirectX RT spec and SDK came out in 2018. The blame lies entirely on AMD. Imagine Nvidia waiting for AMD to agree,Negative
Intel,I would imagine intel has to at least get celestial out the door before cutting off their GPU line. it's been so long in development that it would be more of a waste of money to cancel it then finish it and see how it sells. at least getting it to the public means some kind of ROI and a solid position in the lower end of the GPU market that AMD and Nvidia are abandoning to the used market.,Negative
Intel,"Okay then, what's the Xe GPU roadmap looking like then?",Neutral
Intel,"Hopefully Intel still thinks it needs discrete GPUs to compete in DataCenters and such.  Also GPUs for the ""edge""",Neutral
Intel,I can't claim to know the future or fate of Arc but I think if Nvidia wanted to kill it they could just price it out of the market instead of buying a stake in a company with a broadly uncertain future,Negative
Intel,They barely lived on before the deal.,Negative
Intel,"It'll live on our hearts, yes.",Neutral
Intel,Lol if you believe that I have a bridge to sell you in Brooklyn,Neutral
Intel,"You have to look at this from both sides.  This would give NVidia the ability to offer OEMs a tightly integrated on SOC gfx solution vs a separate discrete gfx package as it is now. This would lower costs for NVidia's customers as well as improve performance and Intel's cpu's get to ride that train.   Intel has a long track record of using integrated gfx so it's certainly right up their alley to take this on.  Could Xe gfx coexist with NVidia? Yep, I think so.  Xe could be the budget igfx option and iNVidia is the premium integrated solution. That's basically how it is now with Intel Arc/Battlemage & GTX.   On the DC side, Intel has absolutely no gfx solution at this time and won't have anything for at least two years.  Intel already sells NVidia when a gfx solutions are needed so tighter integrated products will benefit both sides in that DC space as well.",Positive
Intel,"What's weirdest to me about this deal is. They specifically mention NVIDIA chiplets being packaged with Intel CPUs.   Now, with Meteor Lake and Arrow Lake, the CPU and the GPU are different tiles packaged together. But for Lunar Lake, the GPU was included in the same tile as the CPU, called the compute tile, because that was more efficient.   Supposedly, Panther Lake is using a different tile setup, with the GPU being separate again, or with the memory controller being on the tile with the CPU, but.   Either way, it feels to me like relying on separate NVIDIA chiplets for the integrated graphics is a bit of a limitation to how the overall architecture of the SoC can be designed.   My understanding is that, if another company buys Intel, that messes up the cross licensing agreement, and Intel might lose x86-64. So, even if NVIDIA keeps buying Intel stock, the most they can get is 49%.   And NVIDIA is too big now for Intel to buy, so the other way around wouldn't work. And even if NVIDIA wants to keep Intel's fabs open, I doubt they want to run them either.",Neutral
Intel,Wonder if this investment is purely to keep intel going to avoid a monopoly issue in a few years.,Neutral
Intel,AMD is not going to release a next gen gaming gpu it just leaked and that’s what I’m worried about intel arc are ramping up.,Negative
Intel,"There's also a better then not chance this alliance would have gone the same way as most other nVidia semicustom things by the time Druid tapes out anyhow, and the leadership at Chipzilla likely know this.",Neutral
Intel,"While that might be true in the long term, it's not necessarily true for whatever quarter the execs are caring about at any particular moment.",Neutral
Intel,"Probably a chronologically changed version of what is known now.  No one with any savvy expects this alliance to last with nVidia's track record, so carrying it on makes sense in both being ready for the inevitable burnt bridge and possibly pushing it back.",Neutral
Intel,It's looking for 6 years of Xe3 in integrated graphics I'd guess.,Neutral
Intel,"Always selling out actually, they just can't produce that much",Negative
Intel,And I have another if you think nVidia is capable of keeping this deal running for that long...,Neutral
Intel,Intel will hopefully split their fab business from the rest of the company either way.,Neutral
Intel,"Im not sure what you are implying, but nvidia partnerships have historically not broken from their side as far as I know.   Apple gave them the boot, not the other way around.   AMD are the ones who rejected the Nvidia-AMD merger back in 2005.   Tesla kicked nvidia out as well, because they did not want to keep paying them to develop the tech.   If you are referring to AIB Partners, then that can hardly be called an actual partnership. That's more of a contracted labor and distribution agreement. Even then, XFX only got blacklisted because they were a premier partner who were seen as jumping ship when the going got a little rough with nvidia stalling between 2009-2010.",Neutral
Intel,> they just can't produce that much  because they are losing money on them,Negative
Intel,Trade bridges.,Neutral
Intel,Intel's arc is dead with or without nvidia deal.,Neutral
Intel,"Microsoft went with Nvidia for Xbox and it ended in a lawsuit.   Sony went with Nvidia once, and never again.  Apple ceased to bundle Nvidia GPUs because Nvidia sold them (and HP and Dell) huge quantities of faulty 8000M GPUs.",Negative
Intel,There is a reason why every nvidia partnership ends up breaking apart pretty quickly...,Negative
Intel,"Nvidia seems to make working with Nvidia too difficult for their partners to continue. It's the business version of someone emotionally bailing, and abusing you until you leave them.  They did that with smartphones, consoles, Linux, Apple, and likely countless other projects I didn't follow in as much detail.",Negative
Intel,"That really is up for debate. That they are losing money on them when you account for RNE and RnD amortized by unit, sure. That I have no doubt about.  But that is not the same thing as losing money on each card sold based solely on the bill of materials and distribution cost etc. Which is what actually matters when looking at if a product is ""sold at a loss"" or not.",Negative
Intel,"Hardly. Even after the bubble, Intel needs in house GPU, for both APUs and data center... which means they'll make a gaming version if the work is already mostly there.",Neutral
Intel,"In the case of Intel and Nvidia chipsets, it was Intel that decidedly ended the relationship, paying Nvidia off to exit the chipset business so that they could make integrated graphics.",Neutral
Intel,"Yeah, if nVidia has an Achilles' heel, it's their company culture's near total inability to do semicustom without fuckery, and it's the reason AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops.",Negative
Intel,Well yes. But you do need to amortize the tape out and dev. costs over the production life cycle of the product. And with low volumes I'm not even sure AMD is accomplishing that. AMD is saved due to the console business funding dGPU tech development.,Neutral
Intel,"They've already cancelled every data centre GPU after Ponte Vecchio (which was forced out by DOE contract) despite reassuring us Rialto Bridge and Falcon Shore were on track as little as months before the chop, and their next is apparently Jagure Shore in 2028.   I have 0 hope that will actually happen.",Negative
Intel,">AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops  And how much money is there in your ""everything else outside of ..."" vs Nvidia's entirety of consumer prebuilt desktop/laptop?",Neutral
Intel,">Well yes. But you do need to amortize the tape out and dev.   If they are selling above the manufacturing and distribution cost, they are amortizing those costs. Might not be much, but it beats not selling them.  They might still be ""losing"" money on them when all costs are included. But it would still make sense to make more of them and sell them, since each one sold still plugs that black hole of externalized costs. And would not be the reason for lack of availability.",Neutral
Intel,"*looks at effectively every current gaming system that isn't a PC, Intel claw, a phone, or the Switch*  A fair bit, and frankly, even if it's not that profitable, it defines more of the dynamics of this sector then you'd think if you're a PC type.",Neutral
Intel,"Not only is it not profitable for the chipmaker, the revenue isn't even anything to brag about looking at AMD's gaming section of their financial statements. 1.1 billion for Q2 2025, and that includes whatever miniscule sales there were for Radeon dGPUs.",Negative
Intel,"> If using NVIDIA RTX iGPU in Intel SoC, that will leave only discrete Intel Arc designs to be sold independently.  Assumes without evidence that Nvidia's GPU will immediately replace Xe in every SKU.",Neutral
Intel,Oh those sweet promises. How credible it is!,Neutral
Intel,I wonder what this will mean for running llms locally. Could I buy a laptop with 128 gb of ram and an Nvidia iGPU and have that memory unified with the GPU to run the models?,Neutral
Intel,"Sure it does..... Ignore all the signs, we are fine, nothing to see here.",Neutral
Intel,If the roadmaps haven't changed does that mean there's still a chance we'll see Celestial gpus in 2024?,Neutral
Intel,Rtx “igpus” are probably in a different power envelope to igpus..,Neutral
Intel,"Xe will live in Desktops and high end HX which uses the same chips. Xe3 is done. Xe4 is probably done too. So i expect them to be used here like HD Graphics and will get very little improvements from Xe5 onwards. Just to keep display and needed functions.  In a way they are still commited to GPUs   H, U and V are almost certainly getting their Arc GPUs replaced with RTX ones.  We all know with RTX tech those Laptop CPUs will sell a whole lot more and make intel way more profit. Anything with Nvidia's tech sells like pancakes now.  So going forward they can't recover Xe development costs through the laptop iGPUs using like they did before. Remember laptop iGPUs are the highest volume ARC sales.  Without those sale it's hard to justify full on Xe spending needed for dGPUs. Especially with Intel's financial situation.",Neutral
Intel,"This deal seems crazy.  For Nvidia that's an easy way to get custom x86 CPUs for datacenters and a way to directly compete with Strix Halo tier products.   If the products suck they can just blame Intel and walk out as if nothing ever happened. They don't really have much to lose here.  Even if they cancel their own CPU cores as a result of this deal, they can still use stock ARM cores in the future when/if the partnership ends...  But for Intel? For Intel this seems like a complete nightmare.  What did it take for them to convince Nvidia? It's hard to believe this won't have any impact on Arc... In the worse case scenario they'll be giving up on high-end graphics, just for Nvidia to abandon them right after when x86 isn't as important anymore...  This sounds like a desperate gamble, and it's difficult to understand where it came from because Intel does have pretty good iGPUs... Why are they so desperate?",Negative
Intel,Intel's market cap is 141 billion.  Nvidia's 5 billion investment doesn't give Nvidia enough ownership to start calling all the shots.,Neutral
Intel,"I wonder why MLID seems to want so much that ARC be cancelled...  How many times has he brought out ""leaks"" about ARC getting axed in the last few years? I stopped counting.",Negative
Intel,Why are we not linking to the original source?  https://www.pcworld.com/article/2913872/intel-nvidia-deal-doesnt-change-its-roadmap.html,Negative
Intel,Remember how well it went when Intel shipped an AMD iGPU?,Neutral
Intel,With the announcement I imagined Nvidia branded products. Mini-PCs where the CPU and GPU aren't replaceable. Nvidia having Intel and Mediatek products. The integrated GPUs for Intel have seemed pretty solid for a very long time. The discrete cards are competitive in price at least. I wouldn't bet on them quitting discrete cards. Don't think they'd want to be caught with nothing again if another major novel GPU algorithm pops up and starts a frenzy again,Positive
Intel,"Intel iGPUs are equal or better than AMD outside of the one on the 395. 140v is performant and efficient, 140t is on par with the 890m but paired with a better CPU.",Positive
Intel,"Well, I hope they will have them, but I have a feel they will only be available for servers.",Neutral
Intel,Yeah right.. this will age well... Nvidia definitely wants more competition...,Positive
Intel,Intel has struggled with GPU for decades which is insane when look what Apple has accomplished with their iGPUs. Intel was decimated from the insides out when the C-suite decided asset stripping via sharebuybacks and gutting employees/R&D etc.. in addition to taking in debt.  Not sure if Intel can recover but interesting to see all this corporate heroic medicine trying to save the patient.,Neutral
Intel,Which is also a big reach to assume when for a large portion of intel's market the current intel iGPU is already faster than they need or will need for the forseable future.,Neutral
Intel,"Much more likely the intel/Nvidia collab is for a few special new products, not a complete change in how intel has done graphics for decades.",Neutral
Intel,"Not immediately, but Intel knows CPUs with RTX GPUs will sell a whole lot more than CPUs with ARC.. Money is what Intel desperately needs.  So don't be surprised if they do it fast.",Positive
Intel,There are definitely more Intel ARC iGPU users than Arc GPU users. This will lead to a cut in workforce (already happened) and more in the future. This also means less investment into things like XeSS.,Negative
Intel,The thing is that Intel barely sells any dedicated ARC cards. Almost all of their graphics userbase comes from Intel integrated graphics with their new XE tiles on them. These will be replaced by the Nvidia tiles.      The result of this is that basically every Intel laptop will now have Nvidia inside.,Negative
Intel,"Yes, I doubt Nvidia would ""give"" iGPU without any licensing/fees.",Neutral
Intel,If the bandwidth is as low as most PC laptops currently have it will be slow. Even Strix Halo and DGX have this problem and they are better than most laptops.,Negative
Intel,Nvidia already has this with the the DGX Spark and that same chip is coming to laptops.,Neutral
Intel,"That's likely the plan, and they should be able to pull it off. Just a question of whether they can beat AMD and Apple at it.",Positive
Intel,Intel really needs to use a unified memory design like Apple or they will always look like a joke. They really should be buying a company that makes memory.,Negative
Intel,Reminds me of Stadia. All the signs were there that Google would pull the plug but many were optimistic. Google even said they are committed.  Then it got killed.  Edit - found a article that lists how many times Google said they were committed.  https://www.theverge.com/2022/9/30/23378757/google-stadia-commitments-shutdown-rumors,Neutral
Intel,"Yes, I think we'll see them next year.",Neutral
Intel,"Doubt they’ll remove the option for Arc completely from the H, U and V series. NVIDIA is most likely charging a pretty penny for their GPUs, which will make Intel completely non competitive in a lot of markets if they were to just switch to NVIDIA.  At minimum, it will stick around for the H and U series as a complimentary option.  Not everything magically sells better just because it has NVIDIA on it",Neutral
Intel,I don't see how Intel might be getting more profit from these tbh... Revenue? Yes. Profit? From where?  Especially if Nvidia will continue to use exclusively TSMC nodes.,Neutral
Intel,"Killing Xe3 and Xe4 is so weird to me when Panther Lake is supposed to launch this year, and Nova Lake (which is rumored to to use the Xe4 media block and use Xe3 for the graphics/shaders/compute block) is supposed to launch sometime in 2026.   Like, the volume for both ""launches"" is supposed to be low, actual devices probably won't be available to purchase until the next year, but. Companies are supposed to be receiving samples of Panther Lake. Now. Presently.   For them to say their roadmap isn't affected, isn't changed at all. Unless that's a massive lie, that's only possible if samples of integrated Xe3 have already been fabbed, are already in working silicon.   ...Like, not to get too into the rumor mill, but supposedly the early Panther Lake samples aren't doing great. The CPU side isn't very efficient compared to Lunar Lake, and the GPUs sometimes don't work. But, when they do work, supposedly, the performance is really good, and, again, rumors, but the problem is supposedly more about Intel's drivers than the hardware or architecture itself.   ...I can completely believe that Intel would panic and would rather kill their GPU division entirely than. Invest in their software stack. Developing good technology and then abandoning it instead of advancing, because abandoning it is cheaper and easier is an extremely Intel move.   But if nothing else, the sunk costs for Xe3 at least make me feel like. They sorta have to figure it out, if only because they don't have time to replace it with NVIDIA, in the time frame Panther Lake has to come out.",Neutral
Intel,"There is just simply no way Intel is going to rely on Nvidia as the sole supplier of their iGPUs when their product lines. Theres even less chance that the U series, their low cost volume product line, would switch to Nvidia sourcing.   Nvidia iGPUs are going to be their own separate, lower volume product line, with its own designation. Maybe -G, or -N, or -AX",Negative
Intel,I dont think so.  Why would they cut themselves from hundreds of bilions of dollars market this easily?  It doesnt make sense,Negative
Intel,">We all know with RTX tech those Laptop CPUs will sell a whole lot more and make intel way more profit. Anything with Nvidia's tech sells like pancakes now.  this makes no sense, it will sell more as opposed to what? the igpu needs a cpu anyway and nvidia's gpus mostly get paired with Intel's cpus anyway. Intel controls 80%+ of laptop cpu market and 65% of ALL igpu + dgpu market, why would they give up such a big lead? If anything this has the potential to cannibalize Nvidia's lower end 4050/4060 market since thats probably the performance these RTX SOC's will be",Neutral
Intel,"Intel has already given up on high end graphics, this deal has no impact on Arc cause Arc already has no future.",Negative
Intel,Intel will want the nvidia name on everything regardless as they know it will sell,Neutral
Intel,The actual original source is here: https://events.q4inc.com/attendee/108505485/guest  But youll need to register to see it.,Neutral
Intel,A lot of AMD's are using outdated nodes and architecture.,Neutral
Intel,Intel's biggest struggle with GPUs is gaming compatability and drivers. Apple certainly isnt doing great their either.,Positive
Intel,"I love my laptop with an Intel iGPU, it can actually last me through a work call compare to my other laptop with an AMD chip plus a damn 4070Ti in it.",Positive
Intel,Sure but a lot of Intel's laptop chip business has no need for (presumably) more expensive integrated RTX does it?  Of course for gamers and maybe even workstation laptops (which is a pretty tiny market) these chips will be very appealing but everything else... whatever flavour of integrated ARC Intel are currently developing will continue to be the norm.,Neutral
Intel,"I would assume that the RTX iGPUs cover all the ""gaming"" SKUs, leaving the intel iGPUs for all the low-end and business ones.",Neutral
Intel,"Why would they sell more? We already have laptops with RTX dGPUs and they sell less than Intel iGPU laptops.   The Nvidia iGPU versions are going to be more expensive chips and the laptop prices will reflect that just as they currently do. There's going to be a *reason* to pay extra for the Nvidia versions, and thats going to be measurable more GPU performance. Not just low end Nvidia GPUs on-par with Arc iGPUs.",Neutral
Intel,"Eh. 90% of people and especially companies don't have any need for high GPU performance in a laptop, so CPUs with cheaper Xe iGPUs will likely continue to make up the vast majority of their sales.",Neutral
Intel,"> Not immediately, but Intel knows CPUs with RTX GPUs will sell a whole lot more than CPUs with ARC.. Money is what Intel desperately needs.  Among gamers and people who need the power? Agreed!  Majority of users who don't game or need the power/expense.... nope!  What I like about this is that it pushes innovation further. Hopefully it brings innovation to the sub-$600 laptop market.",Positive
Intel,Average person doesn't care.    Most people don't play games on their computers all of the time.,Negative
Intel,Businesses do not care about GPU's in their laptops it will not sell a whole lot more.,Negative
Intel,Intel iGPUs are drastically better than Nvidia stuff for anything except Halo-type parts.,Positive
Intel,"I think it may come down to the fact that the executives within a particular business unit probably ARE very committed to making it succeed and sticking with it, but the ultimate decision to pull the plug comes from higher up",Neutral
Intel,Ahh yes reminds me of this lovely Google Graveyard.   https://killedbygoogle.com/,Neutral
Intel,"The reason why Stadia failed was because they never implemented ""negative latency""",Negative
Intel,"Because a Laptop is a sum of its part.  Lets say for $400 IntelArc CPU and $500 IntelRTX.  When a laptop is built, if the IntelArc costs $1000 the  IntelRTX will cost $1100  The higher you go the closer it gets percentage wise.  So when the difference is less 10% which one do you think sell way more?",Neutral
Intel,Nvidia is always looking for more fabs as they seem to be significantly supply limited at TSMC. If 18A/14A turn out good it's a safe bet they'll at least try it out.  Remember how Ampere customer cards were done on Samsung so they could use all of their TSMC allocation for A100. They would probably have continued that Samsung partnership if internal corruption at Samsung didn't bomb their 4nm process.,Neutral
Intel,"Even **if** Intel were to completely cancel Xe and their entire business becomes dependent on Nvidia selling them iGPU chiplets (for some odd reason), the timeline for these Nvidia chiplet CPUs would be after NVL",Neutral
Intel,They've spent the past 1-2 years laying off much of their driver team.,Neutral
Intel,Excuse me but Hundreds of billions?   Since when has gaming GPUs make 100 of billions.  Nvidia makes just over 10 billion on Gaming GPUs a year.,Neutral
Intel,"They don't see themselves having a credible chance of capturing that market (GeForce + CUDA moat is deep) without having to trash their margins, they see short-term gain in being able to have the best gaming laptops on the market for a few generations, and being the choice to provide the CPUs for NVIDIA AI platforms.",Neutral
Intel,If you haven't noticed intel hasnt been doing so hot.  Dell recently added AMD for XPS for the first time. Many OEMs similarly are doing amd versions for the first time. Something unthinkable even 5 years back   This way they losing OEM sales on the laptop space. They are no longer the preferred CPU brand.  So what do you do? Well having their CPUs with RTX would give them a surefire boost,Negative
Intel,Intel is worried that they'll lose this market to the now superior AMD chips.,Neutral
Intel,"Well AMD does make 4 billion each year from their console deals, and Intel cannot enter that space because they don't offer a product that is as compelling as AMD.      With this partnership they could do this.",Neutral
Intel,Oh we know they are trying to sell us outdated modes and architectures. And they will continue to do so for the entirety of 2026 in mobile.,Negative
Intel,Apple GPUs are significantly more powerful than any intel iGPU. I can play cyberpunk 2077 at 60fps 4K and nice visuals. Also I can run large AI LLMs as well. I can run heavy AI workloads all day long and total wattage 150-160w for the entire Mac Studio.  Intel iGPUs would melt/die. Its amazing how far Apple has pushed Apple Silicon while Intel got stuck in the mud.  Sad to see how the mercenary C-suite came in like a plague of locusts and gutted Intel leaving a husk that requires intensive care to resuscitate. I hope they do rise back up because competition is good and pushes progress forwards.,Positive
Intel,"If Optimus is working properly, your AMD laptop should not be consuming any more power than if it were APU-only during normal use.",Neutral
Intel,this is likely more to compete with amd's apu's and less for business,Neutral
Intel,"I can see a consumer marketing reason and a business professional reason for RTX throughout: driver support. Anything with the RTX branding will get more support on both fronts, especially if the integrated parts default to studio level drivers. A large part of Arc’s inefficiencies are due to the lack of software support, so streamlining this aspect would be a huge boon for the “it just works” marketing coming back online for Intel.",Positive
Intel,That could still mean Intel dGPU business will go away.,Neutral
Intel,"It has been talked about a lot but SOCs will be replacing the lower end systems completely. For example, the Strix Halo chip that AMD just released is better than a RTX 4060. That may seem weak, but it's cheaper to provide an APU than a CPU+GPU.",Positive
Intel,"I said profit.  Even if RTX CPUS cost more, they result in significantly more sales resulting in greater profits.  We know from the market, that even if its more expensive, Nvidia GPUs will still sell more than equivalent cheaper counterparts",Neutral
Intel,"You mean the business sectors which are most likely to use AI? You know, what Nvidia is famous for.  And low-end don't exist for new CPUs for laptops anymore. Its all older generations rebarded.  Intel use older alderlake and raptor lake CPUs rebarded with UHD graphics and AMD sells Zen 2 CPUs.",Neutral
Intel,Makes me wonder if we will see lower price devices with Nvidia iGPU V vs roughly equivalent dGPU models. There should be some saving I think but will also be interesting to see if there remains a choice or if Nvidia will try and move most of it's mobile lineup to this unified product.  I think on the very highest end like 80 and 90 tier mobile chips they may well be too powerful to adequately cool as part of a combined package.,Neutral
Intel,"I honestly want a work laptop that has a good keyboard, screen, and battery life. (and light!)   I'm mostly doing emails, anything else I'm using a workstation for number crunching.",Positive
Intel,And how do you know this?  Cause last i remember Nvidia doesn't have iGPUs,Neutral
Intel,"While we have yet to see good comparisons, it's provable that Nvidia produces more efficient graphics cards than Intel does. The die size on the Intel chips is massive compared to the similar performing Nvidia ones. Larger the die the more power it's using. Switching to Nvidia will provide more performance at the same power usage.",Positive
Intel,Latency on stadia was actually good. I tried it for a bit with with their pro subscription for a month.  But never renewed or bought anything cause of the awful business models.,Negative
Intel,"And why would Intel do that? Theyre dropping MoP because they act as a middleman, moving MoP at cost to OEMs.  A $100 BOM increase per unit rarely only comes with a $100 product increase. And even then, it's corporate suicide to make Nvidia a critical sub-supplier when you've dont need them to be. Intel would never go in 100% on Nvidia being the sole iGPU option.   The Nvidia option is going to be a separate, lower volume, more premium product line.",Negative
Intel,"Apparently they have been working on this for a year at this point, so a product with an Nvidia tile is likely going to launch by 2027.",Positive
Intel,"Nvidia makes over 10 billion on gaming GPUs in a quarter. But yes, not hundreds of billions.",Neutral
Intel,Arc covers also non gaming segments like B50 product,Neutral
Intel,"Gaming laptops?  Ive watched the webcast with Jensen and as far as I understand this is about datacenter because customers dont want to switch to arm, so partnership with Intel will allow them to get x86 cpus with features Nvidia needs for their datacenter and hpc customers",Neutral
Intel,"Also, the partnership potentially could get Nvidia using their fabs. Nvidia will also now hold a 4% stake in Intel.",Neutral
Intel,"LNL(Xe2) solos every AMD offering in igpu out there and matches the AI 395 with lower power, even ARL(Xe+ Alchemist) isnt that behind, igpu perf is the last thing Intel is concerned about, these high perf RTX laptop apus will only damage the lower end offering from nvidia  and here look at the laptop market share, Intel's share has been pretty constant between 75-80% for the past 3-4 years, idk where the notion of Intel struggling in laptop space is coming from, they even clawed some back after the launch of Lunar Lake: [https://www.tomshardware.com/pc-components/cpus/amds-desktop-pc-market-share-hits-a-new-high-as-server-gains-slow-down-intel-now-only-outsells-amd-2-1-down-from-9-1-a-few-years-ago](https://www.tomshardware.com/pc-components/cpus/amds-desktop-pc-market-share-hits-a-new-high-as-server-gains-slow-down-intel-now-only-outsells-amd-2-1-down-from-9-1-a-few-years-ago)",Neutral
Intel,It’s only been five years since people said it when Renoir APU launched. Maybe in another 5 AMD can cross 30% market share on laptop (it’s been stuck at 25% for about 3 years)?,Neutral
Intel,That’s really nothing. In Q2 2025 AMD made 1.1B from gaming (which includes semi custom and Radeon) out of 7.7B.,Neutral
Intel,Asuming AMD tech work properly is not a bet you want to take.,Negative
Intel,"I know, it's just that I think we'll keep seeing Arc iGPUs for the business and lower end consumer market primarily. Cheaper or more power efficiency focused laptops mainly.",Neutral
Intel,They don’t need that to compete with AMD’s regular iGP. 140v/140T do well enough.   Strix Halo is irrelevant as it was near zero mainstream OEM presence and,Negative
Intel,"It's possible.  Arc might be carving a workstation niche though? their B50 certainly seems like that might be the focus, offers a lot of pro features for a low price.  As far as enthusiast goes I wouldn't be surprised, it's a money sink Intel probably doesn't need even if the payoff might be worth it long term. They say it will make no difference though so who knows.",Positive
Intel,Demonstrably false in any currently existent device. Any $1500 laptop with a 5070mobile  blow the pants off Strix halo.   Continue your APU delusion in r/amd,Negative
Intel,"Will they really profit more by adding RTX GPUs where there were none previously? Especially considering that NVIDIA will likely not make it any cheaper.  Like ultrabooks aren’t going to magically sell more and more, or more expensive in all categories just because they have NVIDIA GPUs.  Only place I can see it really make a lot of sense and add more profit is in place that already had NVIDIA GPUs, like with gaming laptops or enterprise, or as an option to complement Arc.",Neutral
Intel,Are you aware that most enterprise computing is cloud based?,Neutral
Intel,"I think client inference is kind of stupid. It's limited in capability by your local memory amount, and it's economically inefficient because you can't batch requests.  No AI system that exists is ""there"" yet, all of them can be improved enough that people would switch over to a better one. The current top of the line commercial models are hundreds of GB for the weights. To use client inference today you need to use a severely castrated model. Alternatively, if you ship all those hundreds of GB of DRAM on every device, they will be very inefficiently utilized because a single user rarely has the token flow to keep them working, and when they do it's all serial so you can't even batch, you just have to do a linear read over the whole model to get a single token out. And when there's an improved model next year that takes twice the ram you have to roll out whole new machines to the whole fleet.  In contrast, centralized inference can fit as much memory as you put on it, there are no power constraints, you can batch hundreds of requests in one go, and you can update the whole system much more easily. Client inference won't even win in latency because even though you have to pay for network latency, the centralized solution is probably much faster.  The only real advantage client inference has is privacy, and that's not a problem in business, they just get their own inference server. For office work, that even makes latency very fast.",Negative
Intel,"I truly think you'll start to see lower tier offerings, like 50 - 60 class because large iGPUs. Maybe even 70, and a separate, discrete graphics chip become increasingly rare of the next decade.   Nvidia has a similar deal going on with Mediatek to bring their graphics there too.   This is Nvidia shoring up their lower end market in client laptops as the APU wars begin",Neutral
Intel,"You don’t have to wonder, laptops with 7600m are already far better deal for gaming than any equivalent system with Strix halo for gaming.",Positive
Intel,"That kind of proves my point. Intel iGPUs exist, and therefore are better.  Trying to scale-down discrete graphics into integrated graphics will take quite a bit of development effort. AMD has done pretty well with it, but it took them years.  PC gaming / PC building hobbyists seem to drastically undervalue how good Intel graphics really are. They're low power, stable, and have sufficient performance to make the entire category of discrete graphics a niche market in PCs. For most applications that currently use discrete graphics, there would be no reason to even consider Nvidia if it were an option.",Positive
Intel,Quicksync and power management if I were to guess. Spinning up a dGPU uses a lot of power.,Neutral
Intel,"Meanwhile back in the real world the switch 2 exists with its nvidia SOC and all those SOC's inside cars exist, all those SOC's in Jetson's etc exist.   Lol literally knows nothing out side of PC gaming hardware.",Neutral
Intel,Nvidia actually does have iGPUs. Cars that use AGX Drive will have one.   The Volvo EX90 is using [AGX Orin](https://www.techpowerup.com/gpu-specs/jetson-agx-orin-64-gb.c4085) which has an Ampere-based IGPU with 2048 CUDA cores and [owners are being given free a upgrade to a Dual Orin setup](https://arstechnica.com/cars/2025/09/forget-ota-volvo-is-upgrading-the-core-computer-in-the-ex90-for-free/).  The latest AGX Thor has a Blackwell IGPU with 2560 CUDA cores.  Either way it seems easy enough for Nvidia to package Blackwell into a tile that Intel can replace their Arc graphics tile with.,Neutral
Intel,"Again, you're confusing yourself with current-generation dGPU comparisons. Yes, discrete Arc is behind Nvidia, because it's new.  An iGPU isn't just a dGPU slapped next to a CPU, nor are different process nodes really comparable.",Neutral
Intel,"Pay for a sub, still have to pay again to ""buy"" games that you don't own. What could possibly be unappealing about that?",Negative
Intel,Latency on stadia could not be good. It would defy laws of physics (namely - the speed of light) for latency on Stadia to be good. This is why streaming gaming services never work and cannot work. We need FTL communication to reduce latency enough and FTL remains strictly fictional.,Negative
Intel,You are kidding yourself if you think Nvidia will let their iGPUs be a low volume products.,Neutral
Intel,I am pretty sure intel made the arc pro at a low price to sell of most remaining stock and capacity bookings.  This Nvidia deal was in talks from a year ago. And isn't it a funny coincidence they announce the deal after the Arc Pro has sold out in a lot places?,Neutral
Intel,"There were two things announced. A client partnership for gaming laptops (primarily), and a datacenter partnership for custom Xeons with NVLink.",Neutral
Intel,"> LNL(Xe2) solos every AMD offering in igpu out there   Except in, you know, actual workloads.",Neutral
Intel,Well intel is and will still be high in volume since they have very low end on lockdown. They maybe high volume but they are the lower margin chips.  Here is whole range of intel core (not ultra) that use older architectures  https://www.intel.com/content/www/us/en/products/details/processors/core.html  They even revived intel 14nm AGAIN  https://www.intel.com/content/www/us/en/products/sku/244818/intel-core-i5110-processor-12m-cache-up-to-4-30-ghz/specifications.html  Amd cant quite match this yet  However the higher margin products is where they getting hurt by AMD. So that's not good for them,Neutral
Intel,Luner Lake is for handhelds are very lower powered laptops. It's not for work.,Neutral
Intel,LMAO but tbf this time it’s also nvidia,Neutral
Intel,"That's likely the case for now, but Intel probably can't fight against the market.   Now that Nvidia iGPU is an option, OEMs are going to express interest, enterprises are going to express interest. It's the AI era, nobody is going to get fired for buying Nvidia.  Intel's CEO is a customer-pleaser, so if there's demand for mainstream SKUs w/ RTX, I can't imagine he'll say no. And Nvidia winning more of Intel's business? Maybe they'll have to port Nvidia IP to Intel foundry to service the higher volume.  I think Intel Xe will stick around for a while as a legacy platform, Intel does have a bit of a software moat, but I think this goes much further than Kaby Lake-G.",Neutral
Intel,> Arc might be carving a workstation niche though  Not enough to keep it alive. Even smaller market than gaming.   > offers a lot of pro features for a low price  Well that's kind of the problem. They're forced to compete on price.,Negative
Intel,"Thats just more of AMD failing to work with OEMs. Placing a large iGPU tile on a CPU is lower BOM costs than two separate chips with two separate memory pools. And it's easier for OEMs to source a singular chip and cool that single chip.   Strix Halo's failure at using these lower BOM costs to aggressively gain market share, and instead positioning itself as a premium ""high-VRAM"" option is besides the point.   I agree with the poster above the APUs are going to begin cannibalizing the low end dGPU market over the next 10 years.",Negative
Intel,">Like ultrabooks aren’t going to magically sell more and more, or more expensive in all categories just because they have NVIDIA GPUs.  actually, they will. The mindshare aside, the ability to support things like CUDA or ray tracing will make it a much more desirable product.",Neutral
Intel,"Let me tell you why intel rtx laptops wont be significantly more expensive.  A $400 Intel+ARC and $500 Intel+RTX. Same performance, but Nvidia has their features.  Now the Intel+RTX looks more expensive right by costing 25%? WRONG   Cause if the two laptops are built with exact same components and Intel+ARC laptop costs $1000, the Intel+RTX laptop would cost $1100.  Suddenly the Intel+RTX becomes the obvious choice at only just 10% more money.  It gets worse the higher you. $1500 intel+arc v $1600 Inte+rtx. Or $2000 v $2100  This is also why AMD sucks at OEM and Prebuilts  If a 9060xt built desktop costs $900 the equivalent RTX 5060 Ti desktop would just cost $970.  So suddenly instead of the RTX 5060 TI costing 20% more, it becomes only 8% more expensive.",Negative
Intel,Its debatable. There was a big push for putting everything in the cloud. now there is a big push for getting everything back to local because cloud sucks.,Negative
Intel,Which runs on Nvidia hardware.  Nvidia laptop GPUs dont have enough memory to run ai locally.  However an Nvidia iGPU with access high capacity DDR5 and you see where we are going?,Neutral
Intel,"I'm really looking forward to it. I guess ODMs are probably pretty pleased as well, this should simplify laptop motherboard layout, feasibly improve reliability and given it's already so rare for a gaming laptop to have anything other than an Intel CPU + Nvidia GPU they're probably going to find all of this very easy to design around.",Positive
Intel,"Nvidia's been doing iGPUs for *years* in Tegra.  > For most applications that currently use discrete graphics, there would be no reason to even consider Nvidia if it were an option.  Nvidia's IP and drivers are simply better.",Positive
Intel,"You know, i think an iGPU does exist. The switch 2.  And I don't need to tell how efficient that is despite using the awful Samsung node",Neutral
Intel,Intel Xe Media Engine is the SOC tile. Not the GPU tile  So Arc GPU tile etting replaced will not effect that at all.  Edit - here is how it works  https://www.intel.com/content/www/us/en/support/articles/000097683/graphics.html  The display is also there too. So the GPU tile cam be completely idle. Arc or RTX,Neutral
Intel,> Quick sync on a $150 Intel CPU is better at transcoding then anything Nvidia offers under $600.  That's just how they spec the encoders across the lineup.,Positive
Intel,Good thing the Arc tile getting replaced wont effect it.  If anything now you get both Qsycn and Nvenc,Positive
Intel,Intel sales are mostly laptops and most businesses do not give a single shit about iGPU or quicksync.   Hardly anyone actually buys intel for quicksync lol.,Negative
Intel,I actually did reply the switch 2 just below because i got tunnelvisioned on windows  He never replied after that because the switch is far more efficient than anything intel has and that too using a worse mode,Negative
Intel,Here is the kicker. You didn't need to sub and then buy games.  If you bought a game you can play it without a sub.  But google failed spectacularly to market it that as you didn't even know that.,Negative
Intel,The sub was optional. You could buy a game and play it indefinitely without any extra costs.,Neutral
Intel,"It was good. Not as good GFN cloud gaming but surprisingly good and very playable.  GFN has lower latency than consoles btw on MnK. That's because consoles rely on Bluetooth controllers which adds more latency.  So if anyone is fine is consoles, they will be more than fine on GFN.",Positive
Intel,"well, their dGPUs are a niche product in laptops already. Why would an agreement to co-develop an APU with Intel change that?",Neutral
Intel,"The Arc Pro B stuff hasn't even released to consumers yet.  B50 is shipping the 25th, B60 hopefully not to long after.  If Intel can really ship B60's at MSRP this year they will sell every single one they can make. There's nothing out there that comes even close to being competition for it.",Neutral
Intel,"The deal was negotiated for months and finalized / signed on September 13th (per Greg Ernst on LinkedIn). That means they announced the deal just 5 days after making the deal.  And even then, these parts arent coming out for years.",Neutral
Intel,"what actual workloads? In gaming they are basically tied, and that's what most consumers care about. The workstation laptop market is very small and niche and most people use a discrete gpu there anyway",Neutral
Intel,What “actual workloads” on an integrated graphics would you be referring to?,Neutral
Intel,"What kind of work are we discussing here you think the bulk of office laptop from dell, hp, and Lenovo (which is the majority of worldwide pc shipment) have to do?",Neutral
Intel,Whether MT matters or not seems to depend on if Intel is food or not.  Don't people always talk about how gaming is king?,Neutral
Intel,We will see. For the moment all we can go on is what Intel is saying and they're saying it's not going anywhere.,Neutral
Intel,"and yet Strix is the more expensive option out there. Large, performan APUS are not easy or cheap.",Negative
Intel,"Again, no.      First off, Arc already support RT on iGPU. And unless they start shipping more power hungry, larger GPUs in the place of smaller ones for ultrabooks, the performance isn’t going to magically get that much better for RT to be useable.     Second, if CUDA made ultrabooks that much more desirable, you would think we would see a 2050 or even a 3050 be included in a every ultrabook. That’s not exactly the case",Neutral
Intel,It's really funny how you assume that OEMs and ODMs won't also charge more on top of the component cost.,Negative
Intel,"No I don't, actually. Why would enterprise customers bother running anything on laptops? There are no reasonable use cases for this. The only ""AI"" thing that needs to exist on a laptop is advanced search and windows recall, which the existing Intel hardware is perfectly capable of handling. Plus, you are aware Microsoft copilot+ requires a separate NPU right? Which makes the whole idea of running ""AI"" on said Nvidia IGP quite redundant.",Negative
Intel,"Yea, gamers definitely live in some sort of mirror universe.  From my perspective, Intel drivers are significantly better than AMD drivers, which are worlds ahead of Nvidia drivers.",Positive
Intel,"Samsung's 8LPP was one of their best nodes lmao, what is with this revisionism",Positive
Intel,"AFAIK, the 265KF for example, does not support quick sync.",Neutral
Intel,"Well, console controllers add about 50ms latency. However if your stream latency is less than that then you must be lucky and live very close to the server (physically).",Neutral
Intel,Nvidia laptops are niche products....HUH????  Have you seen the steam charts? Their laptops are big Even rivaling desktop numbers.  https://store.steampowered.com/hwsurvey/videocard/,Neutral
Intel,Its bait to sell off remaining stock and bookings.,Neutral
Intel,> In gaming they are basically tied  LNL vs Strix Halo? Absolutely not.,Neutral
Intel,"Gaming, content creation. Your choice, really.",Neutral
Intel,CPU performance was never discussed in this thread only iGP.,Neutral
Intel,"> and they're saying it's not going anywhere  They've said nothing about dGPUs in particular. If anything, likely died before this deal.",Negative
Intel,"Laptops \*With Strix Halo are more expensive.  But I'm failing to see how taking the 40CU's package, making it a separate dGPU chip, giving that chip its own VRAM is cheaper than placing those 40CUs on package.  Strix Halo is low volume with a bespoke 256b motherboard. It's cost structure is negatively impacted by its economies of scale - not because its design is inherently more expensive.",Negative
Intel,"it may not be desirable enough to include a 3050, but desirable enough to slightly increases sales if its part of the iGPU? There is also a thing that you want to avoid dGPUs in ultrabooks due to battery time.",Neutral
Intel,"Of course they will, Because you and i both know which one will be in more demand and sell more.",Neutral
Intel,"> Plus, you are aware Microsoft copilot+ requires a separate NPU right? Which makes the whole idea of running ""AI"" on said Nvidia IGP quite redundant.  That bit seems to be changing, fwiw.",Neutral
Intel,"Why are you of the belief that people care about CoPilot+ at scale?  There are plenty of reasons a company would want local compute options instead of compute in the datacenter. Data security, data ingress/egress costs, local AI compute for appliance-type deployments, edge inference, network constraints, etc.  There is not a one-size-fits-all approach to AI compute. NPUs are not sufficiently powerful enough for the workloads today, let alone where AI compute is heading.",Negative
Intel,Wouldn't change anything as the NPU is im the SOC tiles. Not the GPU tile.,Neutral
Intel,"> Intel drivers are significantly better than AMD drivers, which are worlds ahead of Nvidia drivers  What?",Positive
Intel,Bro what? In what context?,Neutral
Intel,It's not even a current *Samsung* node. It's like 3 full gens behind state of the art.,Neutral
Intel,It is awful compared to modern tsmc 5nm. Which all CPU and GPUs use,Negative
Intel,It wasnt good when it was new andt isnt good now.,Negative
Intel,I think the smarter move would be to do what amd did. A VPU for servers  https://www.amd.com/en/products/accelerators/alveo/ma35d/a-ma35d-p16g-pq-g.html  Now that will sell. Nvidia isnt competing there either.,Neutral
Intel,"They aren't saying you'll need a gpu, they are saying the quicksync and other media engine features are on the cpu SOC tile which will still be present in hybrid nvidia igpu tile designs. They could still stuff it up by not supporting it but the hardware will be there.",Neutral
Intel,people doing large scale transcode with Quicksync would never switch to AMD - the worst possible option for transcode.,Negative
Intel,F = no IGPU,Neutral
Intel,Less than 25% of PCs sold have Nvidia graphics at all. Don't care about the Steam hardware survey lol. Intel iGPU outsells Nvidia 2.5 to 1 in client.,Negative
Intel,"Nah.  The AI market is too hype for Intel to just drop it, and Nvidia is exploiting enough market power that there's certainly space to profitably undercut them in that segment.",Neutral
Intel,What content creation are people buying AMD iGPU laptops for specifically? This is the weirdest lie someone has ever told on this website lmfao,Negative
Intel,"140v does not lose to 890m in either of these, what are you on about?",Neutral
Intel,Ask AMD. They are the ones selling the SOC for 600-1000 dollars to the OEMs. Low volume high RnD ammortization may be the reason.,Neutral
Intel,"Yeah, the one that has an almost identical featureset for $500 less",Neutral
Intel,"Companies are not clamoring to get AI compute more powerful than what the NPU can provide, locally, and at scale across their whole fleet.   If they were, P series and Precisions would've supplanted E/T and Latitudes by now.   Like, what's the usecase of that much local AI on individual workstations? Presumably any data being inferenced will he in some shared location, and with it, so will the inferencing hardware.",Negative
Intel,"OK, what kind of workload do you think they will actually run on laptops that is too powerful for existing and future Intel hardware, not too powerful for whatever Nvidia IPG will be bundled, and they don't want to do with an on-prem solution?",Neutral
Intel,You still haven't addressed the why,Neutral
Intel,"Being a long-time desktop Linux user with a decent memory.  Intel's the only graphics vendor that takes drivers seriously at all. They write and ship them months in advance of product releases, to the point that I could reasonably expect to buy a pre-release B60 today, put it in my desktop PC, and have it just work with already-installed drivers.  AMD and Nvidia treat drivers like video game releases. They ship at the last possible minute and then need patches a week later for bugs, then they kind of get abandoned. And Nvidia never does open source driver releases, which are the only way to have reliable long-term support or any support for configs that differs from the vendor test setup.",Neutral
Intel,"No shit, but compared to its contemporaries it did pretty well.",Negative
Intel,"Huh, weird, last time I checked Blackwell, Navi 48 and Zen 5 were all on TSMC N4, Arrow Lake on N3, and none of them on N5. There seems to be an awful lot missing from ""all CPU and GPU""",Negative
Intel,"That's just a lie lmao, the 8LPP was possibly the only good node SF has put out in years. 10LPX and 10LPP were both not great but 8LPP was good.",Positive
Intel,"Of course they are. Even upstream FFmpeg couldn't keep up with the demand and they had to write additional code themselves to drive the hardware transcoding.  https://github.com/jellyfin/jellyfin-ffmpeg/wiki  It's not true that they spent all their time on APUs, they did spend some time, but it was much better than their competitors spending almost no time on AMD.",Neutral
Intel,Right. The person I'm responding to is saying that Intel CPUs would still support Quicksync if Intel removed the Arc iGPU tile.,Neutral
Intel,"Nvidia's gaming division makes over 4 billion a quarter. And its a safe bet more than a 1 billion comes from Laptops alone.  That not a niche market at all. Yes, its not market dominant if you include iGPUs which even exists in celerons, but if you call over 1 billion a quarter niche, we need to rewrite the meaning of ""niche""",Neutral
Intel,Intel is going to get Ai market by being the exclusive x86 supplier of Nvidia.  Both on server and client.,Neutral
Intel,"Intel's too late to the AI market, Arc flopped, Gaudi flopped harder, broadcom is doing custom solutions, AMD is number 2. China is all in with full state backing. What is Intel going to do? Nobody needs a third place.",Negative
Intel,> This is the weirdest lie someone has ever told on this website lmfao  Weirder than claiming LNL outperforms Strix Halo in GPU?,Negative
Intel,They claimed it even beats Strix Halo.,Neutral
Intel,"""identical"" lol",Neutral
Intel,I think you dont realize how many companies both need the local performance and not get it because of budget constraints. Employees having hell of a time trying to deal with that mess? not shareholder headache.,Negative
Intel,"I think you're missing the point because you seem to believe that all users or departments have unconstrained budgets and can just buy whatever the ""perfect"" solution is for their workload. If that were the case, all workloads would already be handled in the datacenter, and no one would ever need a local GPU for compute, graphics, or AI workloads, which is not reality.  It's a hell of a lot cheaper to go buy a dozen laptops than to go buy a single B200.  Look at the entire entry laptop workstation market. There's a reason why those products exist, or else OEMs wouldn't make them.",Neutral
Intel,Won't matter anyway because of how poor NPU support is.  ROCm doesn't support Ryzen NPUs. Don't think Intel OneAPI does either. And CUDA obviously doesn't.  There was good post about NPUs here  https://www.reddit.com/r/hardware/s/JGJ45bpbjN  There is very little reason to support NPUs.,Negative
Intel,"And yet anyone who does more than basic display and maybe some 3d gaming stuff on their GPUs, uses Nvidia on Linux. Be it GPGPU stuff, cryptographic work, ML training or rendering. Intel ARC had driver issues on both Windows and Linux for a long time. They have a good record on Linux before that yes, but only for their integrated stuff. And intel 7th gen igpus still do not have Vulkan support.   Also we are not talking about Linux users anyways. Like 5% of people use Linux.",Neutral
Intel,> Being a long-time desktop Linux user with a decent memory.  so completely irrelevant market niche.,Neutral
Intel,Intel 14nm did not only “pretty well” but amazingly compared to its 2014 contemporaries.,Neutral
Intel,N4 is a custom N5 version. They are a lot more similar than people think. They are both 5nm nodes.,Negative
Intel,I think the F series has both Display engine and Media Engines disabled cause they assume you are going to be using a requirered dGPU for that.,Negative
Intel,"Intel iGPU outsells Nvidia dGPU 2.5 to 1. There's no need to redefine niche, which means ""a specialized segment of the market for a particular kind of product or service."" I think being a high end upsell product that's found in less than 25% of computers qualifies.    >You are kidding yourself if you think Nvidia will let their iGPUs be a low volume products.  That's what you said. They already *are* a comparatively smaller market than Arc in client. The Nvidia x Intel collab product will be the same: A lower volume specialized part that costs more and performs better, that some people will pay extra to upgrade to. But it will not form the bulk of Intel's volume.",Neutral
Intel,They are very clearly not including Strix Halo. You're somehow the only one who is thinking of Strix Halo.,Neutral
Intel,This is the card Intel should have released instead of the gaming version(s). Took them 2 years,Negative
Intel,Far more than I expected them to come out at. Damn.,Negative
Intel,"Not worth it at $599, slightly worth it at $499  This has 1/2 the memory bandwidth of a 3090 and 70% the compute performance.",Negative
Intel,I’m getting one when it releases in Australia,Neutral
Intel,Thats great and all but when will there be stock? (Canada),Positive
Intel,Pytorch libs are getting better for intel and amd support but there are still a lot of 3090 cards on the used market and those don’t need hoops to jump through to get compute working.  If I was handed a budget this low by a boss I’d just ask them to buy a lot of 3090 cards.,Neutral
Intel,I'm disappointed.  My order was canceled,Neutral
Intel,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"The gaming cards require significantly less robust driver and application support. Releasing them first makes sense when they still needed to work on said driver and application support, the timeline likely wouldn't have meaningfully changed.",Neutral
Intel,b580 should've allowed SR-IOV upon release. That's all the card really needs to sell.,Neutral
Intel,"Even at $599 it's significantly cheaper than a used 3090, comes with a full warranty, and can be bulk purchased new.  If you just need the VRAM, which is what this is targetting, I think it can definitely make sense.",Positive
Intel,"This is worth it at any price point.  I am getting tired of gamers not understanding that SR-IOV support and 200W are very important to some people. If there were a 24GB AMD card out there for 200W and $600 I would buy that too regardless of SR-IOV support.  Neither Nvidia nor AMD have a card at this price point with this feature set. You are looking at $600 for 24GB of VRAM and SR-IOV support. AMD's SR-IOV offerings are in the ""thousands of dollars"" and Nvidia's cards in this segment do not meet the 200W and 24GB of VRAM targets. The closest thing is the P40 and it's a decade old, has way higher power consumption and is about to be end of lifed.  Please tell me the card that exists in the same segment as the B60. I will wait.",Negative
Intel,"This is $150 cheaper than NVIDIA's competing A2000, which has less VRAM.  3090s do not get certified drivers, which are a business requirement for certain workloads.",Neutral
Intel,The 3090 does not have ECC on it's VRAM nor certified drivers,Neutral
Intel,Totally not worth it.,Negative
Intel,it's worth it for enthusiast collectors who want to own a rare Arc GPU. Especially if Intel abandons dGPUs. I'm sure Raja Koduri will buy it.,Positive
Intel,"I'm more looking forward to the B50, but obviously local pricing is everything.",Positive
Intel,"Yeah, the website now says: ""This GPU is only available as part of a whole system. Contact us for a system quote.""",Neutral
Intel,"It has SR-IOV, certified drivers and other professional features...",Neutral
Intel,"Graphics cards are in fact seperated into two broad categories, consumer and professional, used by all the main manufacturers. They have completely different approaches to their design, different use cases, different drivers, different memory etc.",Neutral
Intel,">The gaming cards require significantly less robust driver and application support  Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  It was like that since A770 release. When some games straight up don't work or have unplayable bugs, you are literally paying for own suffering.  That said, B50/B60 are good, though i wish B50 worked with ""Project Battlematrix"" that is Intels NVLink. B60 was also expected to be for 500$, not 600$, but it is what it is.",Negative
Intel,Both nvidia and amd only take a few months between releasing gaming and pro lines of new gpus. Intel managed to do the same with their first gen (only a few months). How come the second gen takes 1 year!?,Neutral
Intel,"...Do integrated graphics support SR-IOV? Do any of Intel's iGPUs support the ""pro"" version of their drivers,   Like, obviously iGPUs have less compute power and less bandwidth than anything dedicated, but. Part of me feels like iGPUs should actually have the features of the ""pro"" cards, given they aren't replaceable, with the actual pro cards providing performance.",Neutral
Intel,"Are used 3090s that expensive?  Here in Chile they go for about 550-600USD. Got one (luckily) at 450USD some weeks ago for AI/ML, without the box and such but it works fine.  Asked a friend from Spain and it seems there they go for about 600 EUR there.",Neutral
Intel,Do not underestimate the lack of CUDA.,Negative
Intel,"Bought my 3090 for MSRP 4.5 years ago and honestly would never have imagined that it would hold 50% of its value.  I want to get another one when they drop to $200 or so, so that I can finally be disappointed by SLI and make 14 year old me happy by finally clicking the bridge onto two GPUs and firing up a benchmark, but who knows if/when that day will come.",Neutral
Intel,A used 3090 is only $100 more.,Neutral
Intel,"It barely makes sense, at $600.  3090s are $700 now and the new 5070Ti Super (faster than a 3090) is expected to come out at $750 soon. And you’d have CUDA for either the Nvidia 3090 or 5070Ti Super.   I strongly believe that $600 is the price Intel set to ward off the scalpers. The niche of the B60 is pretty tiny at $600.   It’ll make a lot more sense at $500, which I suspect is the “real” price a month or two after it goes on sale.",Negative
Intel,Used 3090's are cheaper than this lol. Did you check the sold prices on Ebay at all? I can literally buy them at buy it now prices that are less than this.   94+ upvotes lol. 3090's sell for about the same price as 4070 supers as they have the same gaming performance.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1,Neutral
Intel,yeah it's got a good feature set for its price. people looking at it from a gaming product frame of reference rather than a professional product frame of reference are doing it wrong,Positive
Intel,"SR-IOV alone makes this a substantially different offering than a used 3090. If you know, you know.",Neutral
Intel,Atlas 300i duo,Neutral
Intel,"Also 3090s are $200 more, are used, consume twice as much power (400W card BTW), and do not have SR-IOV support.",Neutral
Intel,"It's cheaper than Nvidia's RTX Pro 2000, but it's also slower, but have more VRAM.",Neutral
Intel,Businesses aren't buying these cards these are destined to hobbyists home labs. The extra money to buy a CUDA supporting card is trivial for businesses they aren't going to buy Intel.  Certified or not the 3090 works wonderfully with all AI software out there while Intel does not.,Neutral
Intel,Yeah it was very scummy imo,Neutral
Intel,"> R-IOV, certified drivers and other professional features  Did Intel ship the SR-IOV part? Last I read (been a while) it was ""coming soon"".",Neutral
Intel,It was meant to be a joke. Not so funny I guess.,Negative
Intel,"B580 needed to get out very early to beat RTX 5060 and RX 9060 to the market. Nvidia's and AMD's cards outperform it in gaming raster performance thanks to having been in the market for much longer. They have the more efficient architecture and software for gaming. It wouldn't be great for marketing if Arc came out at the same time with similar pricing, yet got beaten in the FPS benchmarks.  But productivity cards could wait. Arc's always been very competitive in productivity tasks. Maybe because Intel has more experience in this department. So reliable driver is the focus here.",Positive
Intel,">Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  Good driver and application support is not possible to deliver without a significant install base of your product in the wild. PC configurations vary wildly and usually driver issues are about some bad luck combination of hardware, driver versions, and application versions.  The incumbency advantage in this market is immense. Devs will validate that their stuff works on all manner of configurations including an Nvidia GPU, so they get the perception of having bulletproof drivers.",Negative
Intel,AMD and Nvidia have been having extensive driver support for years  ARC is a much newer platform requiring more careful hand holding,Neutral
Intel,Why does this matter?,Neutral
Intel,"Some iGPU do support SR-IOV, I think 12th generation and newer. Motherboard, driver, and OS dependent.",Neutral
Intel,Was seeing one sold locally recently for $800-$900. They very much are still expensive here in the US.,Negative
Intel,"Yup 600 Euro is about (or slightly above) what used 3090s are sold for here (Sweden) as well. I would rather buy a significantly better card used than a worse one new, if sold at approximately the same price point.",Negative
Intel,"Used site jawa.gg shows 3090s priced from $750 to $950 (and of course, some people asking $999 or $1200)  https://www.jawa.gg/shop/pc-parts-and-components/geforce-rtx-3090-KNRXB30U",Neutral
Intel,Used market is freaking insane. It is better to grab new or open box.,Positive
Intel,Over here used 3090s are sold for 500-600€.,Neutral
Intel,"Well sure if you specifically need cuda then this won't work, but there's also a large subset of people who just need vram for as cheap as they can get it. And for that purpose this Intel card is interesting at least.",Neutral
Intel,Dude that feeling is so worth it though. I did that same with 2 Titan Xps a couple months ago and the build just looks awesome. Great performance on pre 2018 / SLI supported games as well.,Positive
Intel,Just keep on digging the hole man.  A 5070Ti or 5070Ti Super or 5090Ti Super Extreme Titan do not have Support for SR-IOV. You do not know why people want these products.  > Muh niche  **Gamers** are the niche. Have you been living under a rock for the past 5 years?!,Negative
Intel,the super gpus are not expected to release soon?,Neutral
Intel,"Why have you posted this same garbage about a 5 year old used card so many times in this thread? It's a valid point, to some people I'm sure, but it's like you're weirdly obsessed with making it until someone hears you.",Negative
Intel,"....are you mistaking the listings of broken cards as new, working ones?  because when you take those out you'll see pretty quick that all of the 3090's here are selling for FAR above $600 lol.",Negative
Intel,Used 4070 Supers dont sell for nearly $700+ on the low dude.,Neutral
Intel,Yes running business of used cards is how its done.....,Neutral
Intel,no fee SR-IOV makes this a substantially different offering then AMD or Nvidia.,Neutral
Intel,Typically run at 250W though to be fair.,Neutral
Intel,"They are less not more, no idea why people are making up used prices when we can check ebay sold listings.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1",Negative
Intel,not on the Vram like professional cards,Neutral
Intel,Q4.,Neutral
Intel,I thought it was funny ¯\_(ツ)_/¯,Neutral
Intel,The b580 is still good at $250 and it’s only recently in the past month the 5060 and 9060xt 8gb dropped near or at $250 in some instances.  It’s a good card at 1440p aswell and it has 12gb of the other 2 8gb cards. Only problem I have it is the windows 10 drivers being abysmal dog shit if you’d want to play any current game with stuttering and crashes.,Positive
Intel,"Yeah, apparently a year’s worth of it",Neutral
Intel,Because they're super late to the party.,Negative
Intel,"Wow, 800-900USD after 5 years is more than I would have expected.",Positive
Intel,"No one on hardware swap is willing to go above $600 for a 3090, trying to sell one now. This reminded me to just put it up on Ebay.",Negative
Intel,I have no idea why people make up stuff like this when we have ebay sold listing search.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  Most recent one sold for $439.00 including delivery.  Again why are people making up used prices?,Negative
Intel,For what exactly do they need vram without cuda ?,Neutral
Intel,"Look at my profile. I have 0 comments in gaming and hundreds of AI comments. What do you know about running inference locally? Why do you think I mentioned memory bandwidth? Gamers don’t care about that. I haven’t played a game in years.   This is not a good deal for AI running consumers at this price, and a mediocre option for enterprise.",Negative
Intel,Gamers are not niche. They are 20 billion a year business.,Neutral
Intel,"False, they’re releasing in december or jan. So about 3 months from now",Neutral
Intel,"> Typically run at 250W though to be fair.  At which point we have to derate the performance. You know, to be fair.",Neutral
Intel,"The 3090 Ti did, but the standard 3090 did not.",Neutral
Intel,"It's a really good general purpose card. Perhaps not the best card if you want to squeeze out the most fps out of games, but it performs really well in a wide range of tasks including gaming. And yes 12GB VRAM really comes in handy.  It can easily be found for under $250. B580 is still cheaper than 5060 and 9060 XT and even RTX 5050 and RX 7600, especially in the UK where you can find a B570 for only 20 pounds more than RTX 3050. That's way cheaper than RX 6600, it's a no brainer. But the prices could only go down thanks to economy of scale mostly. It's been produced for a while, that's why.  Also one of the big reasons it's still selling now is thanks to good publicity. Imagine if it came out as the same time as RX 9060 XT. HWU would tear a new hole regardless of how much of an improvement it is over the Alchemist cards and the impressive productivity performance. Those guys literally don't care about anything other than pushing a dozen more frames in a dozen personally chosen games. Gamer Nexus and Igor too to some extent. It would be bad publicity.",Positive
Intel,"If intel felt they could have released this safely earlier, they would have",Neutral
Intel,People do not understand AI.  People buy up used 3080s and use 4 of them in an AI Rig to run MoE models. 96GB of VRAM can run you basically anything that's not the full 500GB models like Kimi K2. The power consumption doesn't matter because they're making more money off of the inferencing than the cost of electricity and it's cheaper than cloud providers.,Neutral
Intel,"Where did you find that? FB? Im saving for a GPU here in chile too, was aiming for a 5060ti 16gb but the model I want is 630k in ... Idk the top result in solotodo",Neutral
Intel,3090s will be much less attractive when the 5080 super and 5070ti super launches.  If the 5070ti super is 750- 800 with 24gb for a new faster GPU with a warranty it hel used market will have to shift.  The 3090s have nvlink but I'm unsure how much that matters for most people.,Neutral
Intel,https://www.ebay.com/itm/156720971107  That's a 3080 not a 3090 lol.,Neutral
Intel,"rendering, working on big BIM / CAD models, medical imaging,...",Neutral
Intel,CUDA isnt the only backend used by AI frameworks,Neutral
Intel,"I use opencl for doing gpgpu simulations, this card would be great for it",Positive
Intel,You mean your 0 profile history because you have it private?,Neutral
Intel,"And AI isn't the only use for a workstation card either, jfc.  3090s and 5070 Ti Supers don't have SR-IOV  3090s and 5070 Ti Supers don't have ECC VRAM.  AutoDesk for example will tell you to go pound sand if you try to get support and you aren't running a workstation GPU with ECC VRAM. So for businesses running AutoDesk software in a professional capacity, or businesses needing SR-IOV, you could offer them a 3090 or a 5070 Ti Super to them for $100 and it still won't be an option for them, because for their needs they might as well be paperweights.",Negative
Intel,I thought we were talking about actual hardware that was available in the referenced country (US) and not a banned Chinese product that may or may not even exist and if it existed would cost nearly 10x as much as this product.,Negative
Intel,Spoken like a true gamer.,Neutral
Intel,last leak on them is that they’re delayed. Haven’t seen any reports since then that they’re releasing in q4,Negative
Intel,You still get about 85% performance compared to stock settings.,Neutral
Intel,"Of course, but that’s the point. It took them an excessive amount of time to “feel” like they could have released this",Negative
Intel,What are they doing that's making them money? Or are theu selling the compute somehow?,Neutral
Intel,"People do not understand Ebay sold listing search showing them selling for way way less than the values you guys just pulled out of your assholes?  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  They sell for roughly the same price as 4070's as they have roughly the same performance, AI don't matter gaming is still king in this market.",Negative
Intel,"Sip, marketplace.  Si buscas por RTX 3090 vendidas, es una PNY 3090 a 450K, y todo perfect.  La 5060Ti es más lenta que la 3090 para juegos, la 5070 es un poco mejor o igual.",Neutral
Intel,">rendering  Rendering what? Because if you're talking from a 3D art perspective, all our software is leaps and bounds better with CUDA or outright requires it. You can't even use Intel GPUs with Arnold or Redshift, so good luck selling your cards to a medium scale VFX house that heavily use those render engines",Neutral
Intel,"It's the only one that's always well supported and expected to work with decent performance.  We're months after RDNA4 was released and ROCm still doesn't have any winograd kernels for it, which means 3x3 convs, the most common kernel size, are 2 to 3 times slower than they could potentially be...  I've close to no experience with OneAPI and SYSCL, but I'd imagine AMD should be ahead considering they've been playing the game for way longer… And if ROCm is a nightmare then I can only expect the experience with Intel to be as bad, if not worse.",Neutral
Intel,>CUDA isnt the only backend used by AI frameworks    It is not the only backend for all AI frameworks; it is the only backend for the majority of AI frameworks.,Neutral
Intel,It is the only functional one.,Neutral
Intel,"Oh good, I’m glad I set that private. You can literally google the username anyhow and see 1-2 example posts that underscores my point.",Positive
Intel,0 people are using autodesk with an intel arc gpu lmao,Negative
Intel,"You know nothing about the ai market, huh?  https://www.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/",Neutral
Intel,"Oh no, im part of 60% of world populatioin. how terrible.",Negative
Intel,Which is 36% higher performance per watt.       ... to be fair.,Neutral
Intel,The reason they didn't is because it would have been worse to release it earlier,Negative
Intel,"Gamers Nexus has a documentary on the GPU smuggling business, where even the 3060 12GBs are being used: https://www.youtube.com/watch?v=1H3xQaf7BFI  The 4090s, 3090s and other cards? There's such a demand for those in China that people will buy them up and smuggle them.",Neutral
Intel,Software Development,Neutral
Intel,Why did you spam my replies about cherrypicked results. The cards are worth $800-$900 and the listings you linked literally prove htat besides a few outliers that are $600-$700 to tank the prices.,Negative
Intel,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",Neutral
Intel,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",Neutral
Intel,"Depends on what software you use, but your right that NVIDIA with Cuda is safe bet when is comes to rendering. I use Enscape and that works great in my AMD gpu",Positive
Intel,"Intel is a bit better for common models as they actually pay frameworks and model makers to optimize for it, unlike AMD.  I use flux and sd models and it’s much better on my arc than my Radeon 9600Xt",Positive
Intel,Which ones?,Neutral
Intel,Private profile = Complete troll.  You're not an exception to this rule.,Negative
Intel,I too like to promote Chinese propaganda which isn't available in the United States due to the whole AI cold war going on and the fact that Huawei products have been banned in the US for what? 15 years?,Negative
Intel,"Right man, my point is that it shouldn’t have been",Negative
Intel,EILI7? How are software engineers using GPU ~~imprints~~ inference on their home machines for software development? What am I missing?  E: a word.,Neutral
Intel,"Hell, one of them was just the cooler without the actual GPU.",Negative
Intel,"En ese claro entonces tal vez valga la pena esperar a la 5070 Super, que supuestamente tendrá 18GB de VRAM.  Sobre comprar usado, todo bien en general, pero siempre hay que tener ojo y ojalá siempre probado. Yo fui a la casa del vendedor para revisar y todo bien, pero igual puede ser peligroso, por eso revisar bien los perfiles.  He comprado hartas 3090s usadas por FB Marketplace.",Neutral
Intel,for example every local image and video generation system I've seen.,Neutral
Intel,"you’re wrong about cost, availability, performance, and just plain wrong overall lol",Negative
Intel,"The Chinese are busy making hardware, not propaganda. These are old cards that have been around for a while, I can assure you they are quite real.",Neutral
Intel,According to whom? With what evidence?   Intel spends more on r&d per year than nvidia and amd combined   Do you think they are just being lazy?,Negative
Intel,"The many hours of trial and error getting things to work, and building expertise with said labor would be my guess.  This isn’t home software engineers buying up all the inventory, its people getting around sanctions.",Negative
Intel,Getting good LLMs crammed onto GPUs for coding is a popular topic on /r/locallama.,Positive
Intel,And the one directly under that was the actual PCB...,Neutral
Intel,Im running Comfy UI + Flux on my Strix Halo right now without issue 🤷‍♀️,Neutral
Intel,Propaganda is 90% of chinas economic output though.,Neutral
Intel,"> Neweggâs  I know this is mojibake, but this kinda sounds like a Lithuanian versions of Newegg lol",Neutral
Intel,The competing product is sometimes slower while also being twice the price.  If this wasn't a success then Nvidia would be  unbeatable,Negative
Intel,"What's the word on the B60? Even more VRAM (24GB), and double the memory bandwidth. I see it listed as ""released"" in various places, but can't figure out where to actually buy one.",Neutral
Intel,"My RTX a4000 doesn't support SR-IOV. I don't know about newer series, but at the time you had to buy the A5000($2500) or A6000 and then there are some crazy licence fees to use it.  For 350 i will buy it when it gets available just for this.",Negative
Intel,L1 techs had a great feature on these.,Positive
Intel,"Profitable product for Intel, wouldn't suprise me if Xe3P and onwards for dGPUs happens because stuff like this can do easy returns.",Positive
Intel,1:4 ratio of FP64 performance is a pleasant surprise,Neutral
Intel,"Honest question here: what makes it a ""workstation gpu"" that does it differently than say like a low end 5060/AMD equivalent?   Iis it just outputting 1080p ""faster""?",Neutral
Intel,"Its also just a whole 95 cards sold. (Past month, I’m unsure if its been up longer)",Neutral
Intel,"It will never be in stock again. It’s good for AI, hosting pass through srvio to VMs without licensing and a number of other things outside of gaming.",Positive
Intel,"Hello 79215185-1feb-44c6! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Let me know when it shows up on the steam hardware survey. That's the only barometer for success that true hardware enthusiasts care about.,Neutral
Intel,what's up my Neweggâs,Neutral
Intel,My favorite game  Fallout Neweggas,Neutral
Intel,It also sounds like an ikea lamp name or something,Neutral
Intel,Geriau nei Bilas Gatesas,Neutral
Intel,I gotta spell Neweggâs?!?,Neutral
Intel,"The B50 appears to be an decent low-end workstation GPU, at least as long as the intended workloads don't effectively require CUDA in any way, shape, or fashion.    My one lingering question is what use cases actually *require* the certifications of a workstation-class GPU (which would rule out something like a relatively similar consumer-tier RTX 5060 Ti / 16GB) but wouldn't benefit from CUDA?  Then again I'm not exactly an expert in the field, so I could be completely off-base here.",Neutral
Intel,"I'm disappointed in Nvidia's inability to put out a stable driver since last December, I'm waiting to see if a competitor card will come out that meets my wants for an upgrade.",Neutral
Intel,Intel might be using the B50 as a pipe cleaner for the B60's drivers to prepare it for a retail launch in Q1 2026    IF they're doing this then it's a sound strategy,Positive
Intel,"Double the memory bandwidth of trash is still trash.  Edit: Y'all can downvote me all you want, but 250GB/s is just slightly more than the 200GB/s of my low-profile 70W GTX1650 GDDR6 that I bought for €140 in 2019. Its absolutely pathetic and should be unacceptable for a new product in 2025, let alone a product of $350 !!!. Even double of this (\~500GB/s) of the B60 is less than a RTX3060. Pathetic products.",Negative
Intel,SR-IOV is the selling feature for me and why I have one ordered. Getting a Tesla P4 with nvidias vgpu licensing working is a pain in the ass and expensive.  I'll get it and sit on it until SR-IOV is released in case of scalpers/stock issues. If it doesn't pan out I'll either just sell it on or drop it into my home media server for the AV1 encoding/basic AI stuff.,Neutral
Intel,"Last time I checked GRID licensing can be faked out, but yes, only Quadro/Tesla and Turing/Pascal(IIRC) through driver mods can use Nvidia's vGPU.",Neutral
Intel,"The professional market is smaller than gaming and even more slanted towards Nvidia. This might be a nice side business, but can't remotely justify developing these cards.   Not even clear it's profitable either. The numbers here are negligible so far.",Negative
Intel,"Do people actually need and use FP64 at all anymore? I've got one or two original Titan cards that I haven't thrown out although I've never used them for this purpose either, because they apparently have very high FP64 numbers and if I recall correctly can operate in ECC mode as well.",Neutral
Intel,"iirc, SR-IOV and VDI support in the coming months, toggleable ECC support, and it is ISV certified",Neutral
Intel,"So I spec our PCs at work. We do anything from traditional office work, to intense engineering tasks. On our engineering computers we run MatLAB, Ansys, Solidworks, MathCAD, LTSpice, Xilinx, Altium and other such apps. Lots of programming, VMs, design work, simulation testing, number crunching, and on occasion AI work.   This means we spec systems like with RTX Pro 500, RTX Pro 2000, RTX A4000, A4500, A6000s. The reason we have these rather than cheaper GeForce cards is mostly 3 things. Power/form factor, Driver certification, pro GPU features.   So typically Nvidia keeps the top binned chips for their professional cards meaning the power efficiency to performance is top tier. So we can get high performance single slot or low profile cards, or get some serious GPU performance in relatively small laptops. Drivers usually are validated better than the GeForce drivers, so they include better bug testing, and the apps we use validate performance with the cards which helps us evaluate performance. They also have way more vram like the RTX A4000 has 20GB of vram while being just a supped up 4070. Then from a feature perspective they have better VM passthrough support, or you can enable the vram to run in ECC mode for error correction. Very important when running 24-48 hour simulations.",Neutral
Intel,Software support is a thing. CAD applications like solidworks and inventor don’t officially support the GeForce rtx or radeon rx line of gpus and they’re considered untested unsupported options. You can’t get any tech support if you’re using them. For a business that needs those apps you need a workstation gpu. They also come with ECC vram,Neutral
Intel,ECC memory.,Neutral
Intel,"That kind of puts it into perspective.  Also, let my take a guess:  Newegg sells them well because of how dirt cheap they are, people buying actually expensive Pro cards will more likely do it directly via their system integrator.",Neutral
Intel,Oh when they get enough enterprise customers they will definitely charge licensing fees,Neutral
Intel,How many A2000s show on the hw survey? Because that's the Nvidia variant and it has been around for a long time..,Neutral
Intel,Lmao,Neutral
Intel,Komentarą skaitai per tapšnoklį ar vaizduoklį?,Neutral
Intel,I am no expert but don't these gpus have ECC vram. That's a enough to get labs/professionals to buy them.   You don't want the headache of lacking error correctiin in a professional environment.,Negative
Intel,"I seriously considered getting one for my homelab. I would really like some SR-IOV, and giving multiple VMs access to transcoding would be very useful. Ultimately decided against it because at the moment my CPU alone is powerful enough, I have other uses for the PCIe slot, and I would have to import one. But it's something I'm going to check in on whenever I'm browsing for new hardware from now on.",Positive
Intel,"I know in my field of work, solidworks certified hardware is one such application where certain features are gated behind workstation class cards.",Neutral
Intel,Vulcan does fine with inferencing.,Neutral
Intel,"all the professional graphics stuff is where this can matter, i.e. CAD, large display walls, traffic control, video studios etc",Neutral
Intel,people are buying B60's  https://www.reddit.com/r/LocalLLaMA/comments/1nesqlt/maxsun_intel_b60s/,Neutral
Intel,"Most Zen-1 parts had much worse single core performance than Kaby Lake,    People still cheered on the competition anyway despite it's shortcomongs",Negative
Intel,"GTX 1650 has only 4GB of RAM at 128 GB/s; RTX 3060 is only 360 GB/s, and only 12 GB--or maybe just 8 GB for some cards--of RAM. But thanks for playing.   Edit: relevant username. Up voting you for jebaiting the crap out of all of us.",Neutral
Intel,you really dont want to fuck around with software licensing as a business. vendors do inventory audits to ensure nobody's exceeding their license allocations. piracy would automatically invite a lawsuit.,Negative
Intel,Grid licensing can be faked if you depend on a sketchy github driver that only works on Turing GPUs. You certainly don't want to be doing that in a professional setting where licensing costs are not a massive expense anyways.,Negative
Intel,"I believe mobile is the main reason they continue developing ARC IP, highly integrated SoC are crucial for lower power consumption and performance per watt, as more and more mobile designs are becoming more integrated (see strix halo for example) Intel knows it has to continue developing graphics IP that is competitive with competition. As for discrete cards, this is a battle in the long run to win, but it will take serious investment, we can hope that they won't axe as part of cost cutting measure.",Positive
Intel,"The B50 (16Xe cores) is pretty cut down compared to the full G21 (20Xe)die, it has 2600mhz boost clocks instead of the 2850mhz on the gaming cards, it uses 14GB/s memory (19Gbps on gaming cards) and it has a 128bit bus with 8 memory chips (B580 has 192bit bus with 6 memory chips)  The only costly thing about is the 2 additional memory chips.   I'm not saying it's extremely profitable but it can't be too expensive to make since a portion of the volume is likely faulty G21 dies that can't make a B580 or B580.   If Intel can sell the B580 for $250 without too much pain, then the B50 is probably making a profit",Neutral
Intel,"Yes, to the point where I’m considering picking up a Titan V on eBay. It’s a must for scientific computing, single precision floats accumulate errors fast in iterative processes.",Positive
Intel,I recognize those as words...,Neutral
Intel,I think it was obvious I was being facetious.,Neutral
Intel,> You don't want the headache of lacking error correctiin in a professional environment.  I think Autodesk tech support will tell users to piss off if they encountered software problems with a consumer GPU. That was the explanation that I got back in the university years when the IT department would use the cheapest possible professional GPUs instead of high end consumer GPUs.,Negative
Intel,"Yeah, if ECC is a hard requirement for whatever reason then that would certainly rule out all the GeForce-branded RTX cards.    Of course this then begs the question of what kind of labs / professionals are so cash-strapped as to not be able to afford something like a [RTX PRO 2000 Blackwell](https://www.nvidia.com/en-us/products/workstations/professional-desktop-gpus/rtx-pro-2000/) instead, which fits the same niche as the B50 (i.e. low-profile 2-slot @ <75w with 16GB of VRAM) while being faster and having a far superior feature set.",Neutral
Intel,I’ve been working in a professional aechtiectural environment for 5 years and haven’t seen the need for ECC once.   Can you explain situations where it’s needed? I’ve always wondered.,Neutral
Intel,"SR-IOV for (license-free!) vGPU is IMHO the killer feature here, perhaps along with being able to get 16GB of VRAM per card relatively cheaply and without needing auxiliary power.  Both open up interesting server and workstation use cases that can't be had cheaply from the competition.",Positive
Intel,Have you tried GPU paravirtualization?,Neutral
Intel,"You can buy it off AIB Partners but you can't buy it at retail (i.e. microcenter, newegg] and it doesn't have an official MSRP yet.   The prices you see now are what AIB's want to charge in bulk orders.    If you want to know how much let's say 5 B60's cost you have to get a quote from a distributor",Neutral
Intel,They need GPU IP for two things: client and AI. Anything else is expendable.,Neutral
Intel,"Yes, my point was *if* they have the gaming cards, they can justify the professional line, but it's not nearly big enough to justify making a dGPU to begin with.",Neutral
Intel,"SR-IOV is Virtual GPU (SR-IOV is IO Virtualization used to split PCIe lanes into virtual functions so their physical function can be shared between VMs). No consumer cards support Virtual GPU right now besides Pascal/Turing with driver hacks. AMD's SR-IOV offerings are [very limited](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#virtualization-support), [And Nvidia has a bigger selection](https://docs.nvidia.com/vgpu/gpus-supported-by-vgpu.html) but their budget VGPU options are being phased out (P40).  I believe VDI is Microsoft's implementation. I believe I've done VDI on my RTX 2070 before (I have done seamless sharing between host and VM), but I don't know if it's possible with AMD. Someone please correct me if I'm wrong here, I'm more familiar with the Linux side / vGPU than VDI.  ECC is Error Correcting RAM. I generally don't understand the use case for ECC either, but it is ubiquitous in HPC. All server boards support ECC RAM.  In modern environments most of these features need 16GB of VRAM minimum, but if you ever wanted to try it on a consumer card, you could get an old RTX 20 series and try it out with some driver mods. Optionally, the P40 is still pretty cheap ($250 used) and doesn't need those hacks at the cost of drawing a lot of power, which Intel has solved with their Battlemage Pro platform (by far the cheapest VRAM/$/W you can get).",Neutral
Intel,>Autodesk tech support will tell users to piss off if they encountered software problems with a consumer GPU.   A380 and A770 is also on the certified gpu list. But otherwise that statement is correct.,Negative
Intel,Can you buy an rtx pro 2000?  If i had to guess what percentage of wafers are for b200 chips i would say 90%.  I don't think there are enough pro 2000s around. I don't think there are enough gpus around in most cases.,Neutral
Intel,"> Of course this then begs the question of what kind of labs / professionals are so cash-strapped as to not be able to afford something like a RTX PRO 2000 Blackwell instead  It is interesting. IDK how common it is, but one of my university labs had computers donated from nvidia with nice quadro GPUs for their time.",Neutral
Intel,Pathetic 1:64 ratio of FP64 flops,Neutral
Intel,RTX 3090 Ti and RTX 4090 have ecc. Not that they're cheap.,Neutral
Intel,"It's needed whenever the work you're doing matters and a single-bit error could cause significant harm.  Something like audio or video, a single-bit error probably isn't very noticeable.  Calculations, it absolutely depends on what you're calculating and which bit gets flipped; flipping the low-order bit in a number might not matter much and flipping the high-order bit could cause a big error.  Networking, it depends on whether the protocols you use have more error checks at a higher level (TCP does; UDP does not).  If in doubt, you want ECC, but market segmentation to mostly restrict ECC support to ""server"" chips and boards and charge more for ECC memory means you'll overpay for it.",Negative
Intel,"They're so good, I wish there was a single slot variant.   I want to put them in my MS-01s. The Sparkle A310, being the main candidate for deployment in those machines, only has 4GB and its maximum h264 encoding throughput actually drops below the iGPU (although its h265 and AV1 throughput slaps the 12900H/13900H). It's just a little too low to comfortably handle the Plex server usage I have, so the iGPU remains in service until a suitable competitor arrives.",Neutral
Intel,"IIRC, that requires a Windows host right? That's a non starter for many people unfortunately",Negative
Intel,my guy sr-iov is a type of gpu paravirtualization.,Neutral
Intel,The Asrock b60s are $599,Neutral
Intel,AI doesn't even need a GPU; it can have its own accelerators - see Gaudi.,Neutral
Intel,"One use case for ECC, is when the data is critical and can’t be lost.",Neutral
Intel,>  I generally don't understand the use case for ECC either  Its for when you don't want errors to just be ignored?   How is that hard to understand?,Negative
Intel,ECC should be in literally all memory.,Neutral
Intel,"\>but their budget VGPU options are being phased out (P40).     I mean, the T4, L4 , and A16 exists...     I'm also not sure why low end workstation GPU needs SRIOV support.",Neutral
Intel,I see them at least available in my stores. although mostly as backordres via remote warehouses but they seem readily available with some shipment time.,Neutral
Intel,"but you can still do gpu paravirtualization without sr-iov using Mediated Passthrough, API Forwarding (RemoteFX) or Dedicated Device Assignment",Neutral
Intel,"Where can you get them? And are they for sale yet, or pre-orders, or...?",Neutral
Intel,"The problem with Gaudi (I know, I've written code and run training runs on it) is simply that the programming model is not oneAPI, or whatever oneAPI becomes. Yes, pytorch works, but people care a lot about software longevity and long term vision when buying $5mm+ of GPUs (and these are the purchases Intel cares about that can actually start to offset the cost of development).   The whole purpose behind Falcon Shores (and now Jaguar Shores, if it will even happen) is to put Gaudi performance (i.e. tensor cores) in an Xe-HPC package. Unifying graphics and compute packages is what NVIDIA was able to achieve but not yet AMD, and it's really great for encouraging ML development in oneAPI.  See this post to see where Intel would like to be: https://pytorch.org/blog/pytorch-2-8-brings-native-xccl-support-to-intel-gpus-case-studies-from-argonne-national-laboratory/ (they don't mention the ""XPU"" because it's Ponte Vecchio, which are iiuc worse than A100s).",Neutral
Intel,"Intel can't get people even in an AI shortage. No one wants to deal with an ASIC. That's why their AI solution is GPUs, starting with (hopefully) Jaguar Shores. So it's that or bust.",Negative
Intel,I spit my coffee reading that. Gaudi? The platform that nobody uses that Intel has to revise their sales estimates down each half quarter?,Negative
Intel,"Yup. For example you are doing a structural integrity physics simulation, and a single flipped bit can ruin your 1 week long run (and your liability insurer will reject your claim, a lot of them have standards requiring calculations to be done only on ECC for sensible reasons).",Negative
Intel,"Great example of why certain people shouldn't reply if they don't have knowledge in the area.  - Tesla T4 is $650 Used and has 16GB of VRAM. - Tesla L4 is $2000 Used and has 24GB of VRAM. - Tesla A16 is $3000 Used and has 64GB of VRAM.  Compared to:  - Arc Pro B50 is $350 new and comes with 16GB of VRAM. - Tesla P40 is $275 used and comes with 24GB of VRAM.  If all you care is vGPU / VDI for a small amount of hosts, then no, you're not getting a Tesla A16. What kind of joke suggestion is that?",Neutral
Intel,"Mediated pass though requires big ass license fees vGPU/MxGPU, and isn't FOSS other than Intel's currently broken support that they abandoned for SR-IOV support.  API forwarding only support limited host/guest setups, and even more limited API support. The only FOSS support is VirGL, which only support Linux host/guest and only OpenGL.  Obviously fixed pass though is an option, but even that isn't without issue. NVIDIA only recently removed the driver restriction, they could add it back at any time. Plus you are limited on virtual machine by the physical GPU count. It works with Intel GPUs and is FOSS with them.  SR-IOV on Intel fixes all of that. It works amazingly well with their iGPUs, has no license issues, and is fully FOSS.",Negative
Intel,Hey no need to be aggressive towards the other user. Your comments are very helpful and I appreciated them a lot but keep it constructive please!,Neutral
Intel,"LMAO, I actually have quite a bit of knowledge in this area.  If all you care for is VDI for a small number of VMs, then you'd go GPU passthrough. vGPU / MxGPU often requires higher levels of hypervisor software tier (i.e. VMware vSphere Enterprise Plus), requiring more money. For KVM hosts, setting up vGPU is a lot more difficult and time consuming than just straight up GPU passthrough.  Only two groups of people would be interested in GPU virtualization / splitting:  * Enterprise, in which they wouldn't care about the used card prices.  * Enthusiasts, in which they wouldn't want to pay for vGPU prices anyway. So why bother catering to this crowd?",Neutral
Intel,"Full GPU passthrough is not a solution that many people would consider because it is clumsier than using sr-iov (or potentially VirtIO GPU Venus). Plus for each extra passthrough instance I would have to add in another GPU and this greatly increases power consumption, heat output and cooling requirements. The process is not all that much more complicated at least on Turing GPUs with a hacked driver on KVM guests at least. Plus for passthrough, you probably still need an NVIDIA card because last I checked AMD cards still had a random kernel panic issue after being passed through.  My assumption is that sr-iov on the b50 will allow users an affordable way to have multiple guests on one host GPU without increasing power draw and paying for expensive alternatives and expensive vGPU subscriptions.",Negative
Intel,"...first time I heard people prefer SRIOV over GPU passthrough because it's ""clumsier"" lol. I'm sure setting up mdev devices in KVM, finding the correct corresponding GPU instances, making them persistent through reboot, then edit virsh xml for each individual VM is a lot easier than just doing IOMMU passthrough. /s     Again, enthusiasts don't care about power consumption / heat output / cooling requirements for their lab environment. Enterprise that do care about them are very willing to pay extra cost to get a production ready driver. You're creating a hypothetical situation that simply does not exist in the real world.",Neutral
Intel,"It's good they launching this, this card adds some competition to the landscape, but before anyone buys it they should figure out if their drivers are lighter weight.  It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  The more ARC cards out there the more developers get familiar with them, the more XeSS v2 gets added to titles, the more the drivers get matured and the better future ARC cards will be.  I'd happily pick up an ARC card... once they've proved themselves in terms of driver maturity and overhead.",Positive
Intel,This should be a great upgrade from my A750 if B580 performance is anything to go by. I hope it's under $400.,Positive
Intel,"Unless it's super cheap, it's not gonna be sold well at all.  Even here with all the ""enthusiast"" and people are saying ""make sure you have this hardware combo, that driver, these settings,..."". The average buyer would just simply pay 50$ more for an nvidia card and not have to worry about all that.",Negative
Intel,I hope they fixed the drivers CPU overhead problem or that GPU's gonna need a 7800X3D or 9800X3D to feed it fully.,Neutral
Intel,Too late for me I already went with a 9060xt but hell I had dreamt of it!,Negative
Intel,"I wish they'd get the drivers past the point of frequently broken, but also they haven't produced enough cards for any previous launch to make any dent in the market regardless.  It's pretty much guaranteed the upcoming super refresh will make much more of a difference in terms of bang for your buck.",Negative
Intel,The problem with arc is you need the latest and greatest CPU to go with or you lose 1/4 of performance,Neutral
Intel,Intel has the ball in it's court   If you released a New GPU..  that is pretty much a 5070.... add on 24gb of ram...  and price it at 399   u will   make  boatloads.  it will play pretty much any game at max settings at 1440p..  They must really be  hating on turning down sony though at making the SOC for the PS6 cause the margins too low..they really would need that money now lol,Neutral
Intel,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Depending on the price I might give it a shot.,Neutral
Intel,Love it going to get one if I can scratch some money together,Positive
Intel,Wouldn’t there be a risk that future drivers will not be supported and that it comes with US government back doors?,Negative
Intel,"LOL, preparing ""box"" packaging   I immediately thought of advanced silicon packaging like CoWoS or whatever",Neutral
Intel,Who is gonna tell them G31 would be celestial die since B580 was G21 and A770 was G10?,Negative
Intel,"Ah yes, finally the 4060ti 16gb/4070 killer, only 1.5 years too late! Ig at least this will force price drops on the rx9070",Positive
Intel,Aren't the driver overhead issues really only seen on older processors that are six cores or less?  Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course,Neutral
Intel,"> It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  Is it really though? Powerful CPUs are comparatively cheap, powerful GPUs are expensive. I know plenty of people who went with a 9700X/7700X or even 9800X3D but 'cheaped out' on the GPU because spending $1200 on a 4080 (at the time) was simply too much.",Neutral
Intel,"I agree with everything you said.  However, I myself will buy one just because I want more competition and so I am just going to give Intel a sale. Sure it doesn't move the needle much, sure Intel's probably not going to make any money out of it and I personally probably won't use it much, but I am just doing it out of principle. I sure am in the minority, but at this point I can't sit idle and allow this duopoly to continue without trying something.",Neutral
Intel,"It's a hardware problem, not a driver problem. The cards have insufficient DMA capabilities and uploads must be handled by the CPU, no driver will fix it, and as a consequence the B770 will have even worse overhead.",Negative
Intel,I have a B580 and the driver seems pretty stable to me at this point.,Positive
Intel,"The super refresh is only for 5070, 5070ti and 5080. I doubt the B770 will compete with the 5070 to begin with so those cards are more upmarket.",Neutral
Intel,my b580 has been stable,Neutral
Intel,"I’ve had a B580 for 6 months and have experienced one game-specific issue with Tarkov. Everything else, old, new or emulated has worked fine.",Positive
Intel,"I’ve been using Arc on both windows and Linux since alchemist, it’s powering 3 rigs for gaming, transcoding, etc.   Initial few months was rough but drivers are absolutely serviceable and have been for a while, and continue to get better each release.  I play lots of different games on steam btw, very rarely do I have issues.",Positive
Intel,to our knowledge... i wonder what kind of uplift we'll see it have with next gen cpus,Neutral
Intel,It's BMG-G31.  https://videocardz.com/newz/intel-confirms-bgm-g31-battlemage-gpu-with-four-variants-in-mesa-update  https://videocardz.com/newz/intel-ships-battlemage-g31-gpus-to-vietnam-labs-behind-past-arc-limited-edition-cards,Neutral
Intel,"It isn't the number that determines the generation, it's the prefix.  A770 was ACM-G10 (alchemist G10), while the B580 is BMG G21 (Battlemage G21). The shipping manifests that have been floating around for the better part of a year have been for the BMG G31. Unless new leaks I'm not up to date with are discussing a G31 with a different prefix, everything points towards it being battlemage, not celestial.  Now I pray that Intel have found a way to mitigate the driver overhead. If not, the B770 will be utterly useless for gaming. Nvidia is bad in the overhead regard, but the B580 is damn near an order of magnitude worse.",Neutral
Intel,> Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course  Not even close.,Negative
Intel,"HUB showed the b580 lost like 20%+ perf between the 9800x3d and the 7600 or 5700x3d and actually fell behind the 4060, as the 4060 lost minimal performance on the weaker cpu vs using the 9800x3d. And the 7600x and 5700x3d can certainly power much stronger gpus like the rtx 5070 without bottleneck.  Edit: my bad, I didn't know it was for only a specific game, though still not a good result for b580 overall",Negative
Intel,Where do you find such information?,Neutral
Intel,Source: I made it the fk up,Neutral
Intel,"I was going to say, I put my sister in a b580 and she has had no driver issues in 6 months.",Negative
Intel,"That's sort of my point, they'll probably still exert more price pressure across the stack than the b770, despite being a totally different segment.",Neutral
Intel,https://youtu.be/00GmwHIJuJY?si=z4wU05sJx2SeS7K1  How can people get on here and lie and literally no one questions them? The 7600 only lost anywhere near that performance on Spider-Man Remastered,Negative
Intel,"Inference, there was a blog post tracing and comparing what the Arc driver does with the Radeon driver. The radeon driver just sends a few pointers to buffers, the Arc driver sends large amounts of data. Assuming the driver programmers at Intel aren't idiots, it's because something is seriously wrong with the cards and DMA.",Neutral
Intel,"No, I inferred it from tracing what the driver does, and assuming the programmers aren't idiots.",Neutral
Intel,Try Mechwarrior 5: Clans on high and say there are no problems again.,Negative
Intel,"that game runs on my 3080 ti like ass, just like all early UE5 games...  even with RT turned off, to hit solid 4k60 I needed DLSS and if I wanted 90+ fps I need to use DLSS performance / ultra performance.",Negative
Intel,Steve from HUB and Steve from GB both lack the technical knowledge to look into the underlying issues,Negative
Intel,"It doesn't even run, it crashes left and right on an Arc.",Negative
Intel,"that I have no idea, on launch I did have crashing issues on my 3080 ti, but they did get resolved over time.  but if you are now seeing it still then welp  PGI is a small team that may not have gotten help / getting to it themeslves to make their game arc capable.",Negative
Intel,"Yeah, still doesn't work at the latest patch (and latest Arc driver) with anything other than low.",Negative
Intel,SR-IOV at that price. Who cares about anything else.,Neutral
Intel,Intel would be stupid to axe there Graphic card division if this proves to be successful.,Negative
Intel,"Single-slot variant or custom cooler please, my MS-A2 running proxmox is demanding this card.",Neutral
Intel,About 66% overall performance of a B580 it looks like. That's really nice for a 70W card.,Positive
Intel,"This is exciting, definitely looking forward to the b60 as well.",Positive
Intel,"Obligatory ""Intel is exiting the GPU business any moment now"".",Neutral
Intel,how hard are tehse to actually buy?,Neutral
Intel,"Buying one, this is impressive",Positive
Intel,"if compared by price to performance ratio,  ARC B50 is slower than RTX 5060 in terms of price and performance",Neutral
Intel,Its better than a 1.5 year old bottom of the range card....well done i guess.,Positive
Intel,Better than NVIDIA? lol .... oooookay,Positive
Intel,"Haven't seen the video, but I'm already buying one if that's the case",Neutral
Intel,Literally it could be a damn arc a310 or rx6400 and people would buy that card at $350 without licencing bs. For anything VDI related the B50 is huge.,Negative
Intel,Intel’s “MOAAAAAR CORES” in the GPU space???,Neutral
Intel,what is that? SR-IOV?,Neutral
Intel,Super interesting!  Wonder how well it would handle AI tasks like Frigate while another VM uses it for inference or a third doing video transcoding with Plex.,Positive
Intel,16 GB VRAM too,Neutral
Intel,They will eventually axe it.,Neutral
Intel,Instead of axing it maybe spin it off like AMD did with Global Foundries?,Neutral
Intel,And if it isn’t successful?,Neutral
Intel,They do have 3rd party vendors for ARC PRO Cards this time around so it most likely will happen.,Neutral
Intel,"The B60 is more exciting to me just for that 24GB VRAM. Still, at this price point the B50 is a pretty compelling buy tbh.",Positive
Intel,I think it would be really stupid for them to do so.,Negative
Intel,You can preorder from newegg now. They ship later this month.,Neutral
Intel,"One Swedish retailer I checked has them coming into stock next week (10 September) and open to orders, how much stock there will be however, I have no clue.",Neutral
Intel,Same. I put my preorder in. Plan to put it into one of my sff builds.,Neutral
Intel,Why is this impressive for $350 USD? How will this be useful for you? I’m not being sarcastic. I am genuinely curious.,Neutral
Intel,What did bottom of the range cards cost 1.5 years ago?  How much VRAM did they have?  Did they support SR-IOV?   Just think for a bit sometimes.,Neutral
Intel,It quite literally is. Watch the fucking video.,Negative
Intel,"Wendell confirmed as much in the comments, looking forward to his future testing of the card.",Positive
Intel,What does AMD have in this product segment?,Neutral
Intel,"Ingen av de kortene greier LLM, hvis du tror det.",Neutral
Intel,"Ingen av de kortene greier LLM, hvis du tror det.",Neutral
Intel,"I didn't know either so I looked it up.   ""SR-IOV (Single Root Input/Output Virtualization) is a PCI Express technology that allows a single physical device to appear as multiple separate virtual devices, significantly improving I/O performance in virtualized environments by giving virtual machines direct access to hardware. This bypasses the overhead of a software-based virtual switch, resulting in lower latency and higher throughput for demanding applications by dedicating virtual functions (VFs) to guest VMs.""",Neutral
Intel,Everyone (including Nvidia) is moving toward APUs with large GPUs onboard. Why would Intel kill their chance at competing in that market?  They've already withstood the most painful part of the transition. There's no point in stopping now.,Neutral
Intel,They are keeping their Fabs which is even more expensive to maintain why would they sell GPU not to mention their iGPUs are pretty Damm good nowdays not like meme in Intel HD4400 even though they could play any game /jk.,Negative
Intel,Doubt they have the revenue to spin out successfully without significant investment from outside sources.,Neutral
Intel,Sadly Intel has a recent history of making poor life choices.,Negative
Intel,"Maybe it's just me, but this reads as AI generated.",Neutral
Intel,"I dunno man, I was building a PC for work and the 3050 was the cheapest Nvidia card I can get and the 7600 is the cheapest from AMD. Huge price gap between the two, by about 100 USD. AMD really needs to buck up their APUs to render cheap GPUs surplus or have something cheaper than a 7600 to price match the 3050.",Neutral
Intel,"They're comparing it to an entry-level NVIDIA GPU, the A1000. Saying that Intel GPUs are ""better than NVIDIA"" as a universal statement is flat-out wrong. Let's see some competition to the RTX 5070 Ti, 5080, or 5090. NVIDIA has zero competition on mid-range and high-end GPUs.",Negative
Intel,Radeon Pro V710 and you can't even buy it retail.,Neutral
Intel,thanks 🙏,Positive
Intel,Because intel shareholders are super short sighted.,Negative
Intel,this comment is the weirdest version of 'corporations are people' that i've encountered,Negative
Intel,Lmao seriously the formatting and the amount of bolded words just screams AI,Neutral
Intel,It's AI generated in your mind,Neutral
Intel,"because AMD has a bad habit of leaving a bunch of their older cards in the channel and having them become the low end...  CPU and GPU, AM4 lives for so long because there are still piles of the stuff in the channel and just gets slow tiny discounts till its gone in full  its like their demand forecast is too optimistic or something but at this point I think its deliberate",Negative
Intel,Because this is not a gaming GPU and thus the A1000 is the correct card to compare with.,Neutral
Intel,Good luck using those super gpus to host multiple gpu accelerated vm with one card. Nvidia won't let you.,Negative
Intel,"Yes, compare an Arc Pro to a GeForce, totally the same market.",Neutral
Intel,"That seems more mid or high tier rather than these relatively low tier gpus, the b50 is a cut down b580...  Also the v710 seems like the kind of ""passive"" gpu that's ""passive"" as long as it's next to several 7,000 rpm fans.  So it would probably not work very well as a retail car because it's rack server focused.",Negative
Intel,"I think the really loud short sighted shareholders have quieted down a bit after it became clear they helped drive the company to where they are. Hell, they're probably not even shareholders anymore.",Negative
Intel,The weirdest version was Citizens United,Neutral
Intel,Did you not figure out why they’re bolded?,Neutral
Intel,I'm aware but it's the only current gen GPU for graphics workloads that has virtualization support from AMD.,Neutral
Intel,"The current Chairman of the board, Frank Yeary, is one of these stupid short sighted people. He REALLY wants to sell the fabs, and is probably the reason Intel went through their latest round of layoffs (Lip-Bu Tan wanted to raise money from Wall Street, Yeary apparently sabotaged it).",Negative
Intel,"if corps are people, they should be allowed to vote, right ?",Neutral
Intel,"Not only that, but because they are people, they should also be able to fund gigantic super PACS to get a candidate into office. I love America!",Positive
Intel,"u/michaellarabel It would be super cool to have Molecular Dynamics benchmarks for these kind of cards, since you already use them for CPU testing and a few of them (e.g. GROMACS) support APIs from all three vendors (CUDA, ROCm, SyCL, + OpenCL)",Positive
Intel,"I would be so curious about a die shot and GPU architecture of the A19 vs 8 Elite. Because Apple went from very little rt not so long ago to leading the charts. Like how many RT-Acceleration units are used in the GPU and how other components play into that.  Definetly exciting times, I think that the major players, Qualcomm, Apple, and Mediatek, are not so far away from each other, the leader can change in a generation or two, which hasn't been the case often in recent history.  Especially interesting will be how AMD client will turn out in 2026, if they are still on the charts (we expect Zen 6 clients on N3) or if Qualcomm will top windows charts, and Apple overall all the charts.",Neutral
Intel,"Growing the E core into a medium core is really important and something I was worried apple might miss out on, but this means they should be able to keep up in multicore as trends point to flooding CPUs with E cores.  They really ~~buried~~ deleted the lede by not even mentioning the E core improvements beyond the cache and undersold the GPU. I suppose this means we should expect a lot of bragging in the M5 keynote.",Neutral
Intel,Youtube link:  [https://www.youtube.com/watch?v=Y9SwluJ9qPI](https://www.youtube.com/watch?v=Y9SwluJ9qPI),Neutral
Intel,I cannot wait to see what's in store for 2026 Mac Studios and the M5 CPU. Especially if M5 Ultra makes its debut. AI workloads should see a significant performance boost by 3-4x? I wonder if M5 Ultra will offer 1000GB/s memory bandwidth.,Neutral
Intel,6 wide e core holy shit,Negative
Intel,"A major exciting aspect for me is the massive boost to Raytracing performance. The M4 Max is the closest anyone has ever come to matching Nvidia in 3D Raytraced Rendering, beating out even AMD. In Blender M4 Max performs somewhere in between an RTX 4070M and 4080M.  A 56% leap in RT performance would essentially put an M5 Max closer to a RTX 5090M than anyone before at a fraction of the power.",Positive
Intel,"Is there a link for English? If not, can you summarize how they tested sustained performance and how much is the improvement over previous generations?",Neutral
Intel,"The E core having more improvements than just 50% larger L2 is a nice surprise, but damn the efficiency and performance of it is insane. 29% and 22% more performance, at the same power draw is insane, clocking like 6.7% higher too. They used to be behind the others in performance with the E cores but had better efficiency but now they both have better performance and efficiency.  As for GPU, I always wanted them to focus on GPU performance next and they finally are doing it. Very nice, the expected 2x FP16 performance, which now matches the M4 which is insane(M5 will be even more insane). Gpu being 50-60% faster is a nice sight to see. For RT performance(I still find it not suited for mobile but M5 will be a separate matter) I’m surprised that the massive increase is just from 2nd gen dynamic caching, the architecture of the RT core is the same, just basically a more efficient scheduler which improves utilization and less waste.  For the phone, vapor chamber is nice, them being conservative on having a low temperature limit can both be a good and bad thing which is shown, the good thing is that it means the surface temperature is lower so the user won’t get burned holding the device, and the bad thing is that it can leave performance off the table which is shown. As that can probably handle like another extra watt of heat and performance. Battery life is very nice, the fact that it can match other phones with like over 1000mAh bigger battery is funny. As people always flexing over how they have like a 4000, 5000mAh+ battery, of course having a bigger capacity is better, but the fact that Apple is more efficient with it and can have the same battery life at a much smaller battery speaks volumes about it.",Positive
Intel,"The occupancy characteristics of A19 Pro are quite incredible. 67% occupancy for a RT workload.  Look at Chips and cheese's SER testing. 36-44% ray occupancy with SER in Cyberpunk 2077 RT Overdrive.     Assuming NVIDIA can get this working on 60 series an effective a 52-86% uplift. After OMM and SER this seems like the third ""low hanging"" RT fruit optimization. Anyone serious about a PT GPU architecture NEEDs dynamic caching like Apple. And no this is not RDNA 4's Dynamic VGPR, it's a much bigger deal. Register file directly in L1$ has unique benefits.",Positive
Intel,"Nice been waiting for this. P-core frontend improvements and branch, and a wider E-core with it's newer memory subsystem shows great YoY gains as usual. Though I am not surprised since it's been years leading up to this that Apple has steadily have been increasing power/freq to get the rest of it's performance gains, although IPC YoY is still class leading. The wider e-core takes the stage which is now commonly being focused in the industry (ex. Intel: Skymont etc). Excited for any outlet doing die analysis (I don't know if kurnal has done it yet).  Real generational GPU gains, instead of last year's YoY tick. Supposedly GPU size has not increased and that is impressive. Massive FP16 compute matching the M4, really shows their commitment to ML (as if naming 'tensor cores' wasn't obvious) and this will greatly help with prompt processing if you're into local models. Finally with a vapour chamber in the PRO models, performance overall really stretches it's legs and sustained is really respectable.  Also, since I'm skimming, I'm assuming A19 base like it's predecessor is a different SoC to the Pro. It is also really really refreshing to see the base A19 be better than the 18 Pro, little to no stagnation and a year at that. The base iPhone 17 looks like a reaaly reallly good option, more than ever, wished they didn't drop the Plus model. But man, I feel like waiting another year, hearing rumours about N2 and new packaging tech excites me.  That said, looking forward to QC, MT, and Samsung. SD8EG5 seems to be closing the gap, and that'll be very interesting tho those GB numbers don't tell things like power.",Positive
Intel,"""Generations ahead of other ARMs M cores"".   Uhm we are getting the Dimensity 9500 and 8 elite gen 5 next week    The C1 Pro has 20% IPC improvement IRRC, plus this is N3P   Let's not jump to conclusions before seeing the competition    I also wonder if QC made changes to the E cores",Neutral
Intel,>SLC (Last Level Cache in Apple's chips) has increased from 24MB to 32MB  Really tells you how pathetically stringent AMD has been with cache sizes on their apus (no die size excuse allowed here because they never use leasing nodes specially N4 was already old when Strix point debuted),Neutral
Intel,">A19 Pro E core is **generations ahead** of the M cores in competing ARM chips.  >A19 Pro E is 11.5% faster than the Oryon M(8 Elite) and A720M(D9400) while USING 40% less power (0.64 vs 1.07) in SPECint and 8% faster while USING 35% lower power in SPECfp.  >A720L in Xiaomi's X Ring is somewhat more competitive.  No, Apple's 202**6** E-cores are just **+3% (int perf)** and **+1% (fp perf)** vs Arm's 202**5** E-cores, though at **-22% (int)** and **-16% (fp)** less power.  Note: Geekwan's chart is wrong. The Xiaomi O1 does not use the A72**0.** It uses the upgraded A72**5** from the X925 generation. Not sure how Geekerwan got the name wrong, as they recently reviewed it.  Integer  |Core / Node|int perf|int %|int power|int perf / W|int perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|4.17|103%|0.64W|6.51|132%| |Xiaomi A725 / N3E|4.06|100%|0.82W|4.95|100%|  Floating point  |Core / Node|fp perf|fp %|fp power|fp perf / W|fp perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|6.15|101%|0.92 W|6.68|120%| |Xiaomi A725 / N3E|6.07|100%|1.09 W|5.57|100%|  I would not call this ""generations"" ahead.",Neutral
Intel,I always wonder how they plot the architecture map and figure out such as the depth of LDQ kind of things...Is it public somewhere? That kind of detail won't be able to get via regular benchmark right?,Neutral
Intel,A19 Pro GPU is now only 1.62GHz vs 1.68GHz in A18 Pro while having the same number of ALUs (768). Does that mean the increased performance is basically due to memory bandwidth increase?,Neutral
Intel,"At this point all the flagship phones are great. I have an iPhone 14 pro max and a Samsung Galaxy S23 Ultra. But I’ve had iPhones for years and i couldn’t switch if i wanted too i have way too much invested in ios (games, apps, music etc). But I ordered a 17 Pro Max 1tb thought about 2tb’s but have a 1tb now and still have 450gb’s free and i download everything and never delete anything. Im trading in the Samsung and giving iPhone to my mom. Still think of buying a cheap phone that gives me the $1100 trade in because it’s any condition and I hate trading in a phone thats practically new.",Positive
Intel,-21 freezer lol https://browser.geekbench.com/v6/cpu/14055289,Neutral
Intel,"The snapdragon 8 elite gen 5 still beats it though , and handily at that. Unless I am wrong",Negative
Intel,">Power however has gone up by 16% and 20% in respective tests leading to an overall P/W regression at peak.  That's really not good for the A19 **Pro**, sadly a mark against Apple's usual restraint. It's a ***significant*** *perf / W* downgrade in floating point. The 19 Pro perf / W is notably worse than **ALL** recent P-cores from Apple, Qualcomm, *and* Arm:  |SoC / SPEC|Fp Pts|Fp power|Fp Perf / W|Perf / W %| |:-|:-|:-|:-|:-| |A19 Pro P-core|17.37|10.07 W|1.70 Pts / W|84.2%| |A19 P-core|17.13|8.89 W|1.93 Pts / W|95.5%| |A18 Pro P-core|15.93|8.18 W|1.95 Pts / W|96.5%| |A18 P-core|15.61|8.11 W|1.92 Pts / W|95.0%| |A17 Pro P-core|12.92|6.40 W|2.02 Pts / W|100%| |8 Elite L|14.18|7.99 W|1.77 Pts / W|87.6%| |O1 X925|14.46|7.94 W|1.82 Pts / W|90.1%| |D9400 X925|14.18|8.46 W|1.68 Pts / W|83.2%|  These are *phones*. Apple, Arm, Qualcomm, etc. ought to keep max. power in check. This is *o*n par with MediaTek's X925, a bit worse than the 8 Elite, and much worse than Xiaomi's X925.  I would've loved to see efficiency (joules) measured, like AnandTech did. That would show us at least if ""race to idle"" can undo this high 1T power draw or not in terms of battery drain.",Negative
Intel,"All of these benchmarks are sort of difficult to compare when the other chips aren't made on the same node N3P. Apple buys first dibs on the wafers so they always have that advantage, it isn't always about the architecture itself.  It will be more interesting when there are Qualcomm chips out with their architecture on this N3P node, and the Mediatek chips with the usual off the shelf ARM cores on this node too to compare.",Neutral
Intel,"Those GPU stats are false. According to Tom's Guide, in 3D Mark Solar Bay Unlimited, the 17 Pro Max is only 10% faster than the s25 ultra https://www.tomsguide.com/phones/iphones/iphone-17-pro-max-review#section-iphone-17-pro-max-performance-cooling",Neutral
Intel,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,">I would be so curious about a die shot and GPU architecture of the A19 vs 8 Elite. Because Apple went from very little rt not so long ago to leading the charts. Like how many RT-Acceleration units are used in the GPU and how other components play into that.  Me too. I'd imagine a marginal increase in size over A18 Pro. The P cores have mostly stayed the same. And the E cores despite major changes are unlikely to contribute to a major increase in area (if I'm right, individually they occupy around 0.6-0.7mm2 per core). The extra cache (around 2MB) should increase area slightly. SLC area as well should contribute to that increase.  I'd imagine the GPU with the new RT units, doubled FP16 units, new tensor cores, and general uarch improvements are the major contributor to any notable area increase.  Plus I still don't feel like the approaches these companies are taking are aligned very much in terms of GPU architectures. For eg, Apple's been very focussed on improving compute performance on their GPU. Qualcomm less so.  >Definetly exciting times, I think that the major players, Qualcomm, Apple, and Mediatek, are not so far away from each other,   True that. Qualcomm has a shot at taking the ST crown lead from Apple atleast in SPECfp. But Apple's done extremely well with this upgrade. The E core jump has made them close the nT gap with Qualcomm while using much lower power.  GPU is a case where technically Qualcomm could take raw perf crown. But Apple's RT dominance, Tensor cores and general compute lead might help them in the desktop space.  >Especially interesting will be how AMD client will turn out in 2026, if they are still on the charts (we expect Zen 6 clients on N3) or if Qualcomm will top windows charts, and Apple overall all the charts.  Its either Qualcomm or Apple. AMD is too far behind Oryon and Apple's uarchs. They consume similar or even more area for their core architecture while lagging in performance while using significantly more power. The x86 ecosystem and compatibility is the only reason they'd survive Oryon.",Neutral
Intel,Having so many SoC makers on arm competing against each other by one upping each other yearly is bearing fruit vs x86. i don't know how Intel and AMD long term can fare at this rate. Arm CPUs are still showing double digit gains yearly.,Neutral
Intel,Most of apples RT gains are from optmsiing how the GPU deals with divergence.  This is not dedicated RT silicon so much as making the GPU be able to maintain much higher throughput when there is lots of divergence.  RT operations have a shit tone of divergence.,Negative
Intel,I can't wait to see die shots and measurements for these chips. The A18 Pro and A18 die shots were really interesting to see what was compacted or lost for the base model chip. I have a feeling that there will be bigger differences for the A19 Pro and A19 with that giant SLC on the former. Die areas will also be interesting. Cache isn't cheap for area and I'd also love to see inside the new E-cores and GPU.,Positive
Intel,">Growing the E core into a medium core is really important and something I was worried apple might miss out on, but this means they should be able to keep up in multicore as trends point to flooding CPUs with E cores.  I wouldn't say its a medium core yet tbh. Apple's E cores are still sub 1mm2 in area usage. Compared to other ""M cores"" they are still relatively small. I imagine the A19 Pro E core is fairly larger but the A18 E core was around 0.6/0.7mm2 in size. I'd imagine its not grown a whole lot.  >They really ~~buried~~ deleted the lede by not even mentioning the E core improvements beyond the cache and undersold the GPU. I suppose this means we should expect a lot of bragging in the M5 keynote.  I'd imagine they're saving up these to mention Blender perf etc in M5 keynote.",Neutral
Intel,4 minutes ago. Damn. I should have waited. I'll post it!,Negative
Intel,"An M5 Ultra would offer 1.23 Tb/sec of bandwidth scaling from the A19 Pro.  M5 (128-bit, LPDDR5X-9600) -> 153.6 GB/s M5 Pro (256-bit, LPDDR5X-9600) -> 307.2 GB/s M5 Max (512-bit, LPDDR5X-9600) -> 614.4 GB/s M5 Ultra (1024-bit, LPDDR5X-9600) ->1228.8 GB/s",Neutral
Intel,"Haha. I mean they are actually pretty late to this tbh. Most ""E/M cores from other competitors"" are similar in size if not bigger. I'd imagine Apple's E core is stuck between a true M core like the A7xx and a true E core like A5xx in terms of area, although it probably leans toward the A7xx in that regard.",Neutral
Intel,What does 6 wide mean? What units?,Neutral
Intel,"[https://www.reddit.com/r/hardware/comments/1jcoklb/enable\_rt\_performance\_drop\_amd\_vs\_nvidia\_20202025/](https://www.reddit.com/r/hardware/comments/1jcoklb/enable_rt_performance_drop_amd_vs_nvidia_20202025/)  In gaming RDNA4 RT isn't that far behind Blackwell. Other than that raytraced rendering like in Blender AMD has been for a while far behind. It won't be until Blender 5.0 till we see any improvements to HIPRT. Though for the longest time since following HIP it's been rather mediocre and my expectations are low for next release, though their [PRs](https://projects.blender.org/blender/blender/pulls/145281) make it seem they've been doing some work. It's a low priority for AMD which is unfortunate.",Neutral
Intel,>	beating out even AMD  Was that one really surprising?,Negative
Intel,Is that Metal vs Optix or Metal vs Cuda?,Neutral
Intel,"The video has a mostly accurate English captions option. CPU P core is up 10%, E core is up by 25%, GPU perf is up by 40% and sustained performance is up by 50%.",Neutral
Intel,> just basically a more efficient scheduler which improves utilization and less waste.  When you take a look at GPUs doing RT task you see that they tend to be very poorly utilized.  GPUs are not designed for short running diverging workloads. But RT is exactly that. So you end up with a huge amount of divergence and or lots of wave like submissions of very small batches of work (so have a large scheduling overhead).     There is a HUGE amount of perfomance left on the table for this type of task for HW vendors that are able to reduce the impact on GPU utilization that divergence has.,Negative
Intel,Maybe Apple measures occupancy differently in their tools. I wouldn't be too sure comparing these two. But I'd definitely think a combination of SER and Dynamic Caching present in A19 should result in very good utilization compared to other uarchs.,Positive
Intel,"That is very impressive, RT tends to have very poor occupancy as it is a heavily branching workload!",Positive
Intel,>Supposedly GPU size has not increased and that is impressive.   Important to note that the supporting SLC which is a major reason for improvements om the GPU side has increased from 24Mb to 32Mb. Which would increase area a bit.,Neutral
Intel,">The C1 Pro has 20% IPC improvement IRRC, plus this is N3P  N3P is 5% faster than N3E. By TSMC's own claim..  Also I can't find a source for a 20% IPC improvement. ARM's claim is 16% IPC improvement. And that is not without a power cost since ARM claims that at similar performance, power reduction is only 12%.  https://newsroom.arm.com/blog/arm-c1-cpu-cluster-on-device-ai-performance  >Let's not jump to conclusions before seeing the competition   I mean I agree. But I don't see how the C1 Pro is supposed to cross a 95% P/W disparity. (4.17 points using 0.64W vs 3.57 points using 1.07W) using D9400",Neutral
Intel,"Tbf, Apple had a 32Mb SLC back in the A15 Bionic. They reduced the size of that afterward to 24Mb. Its not like the size significantly mattered in GPU performance until now.",Neutral
Intel,A 30% lead in P/W is a generations ahead in this day and age. Considering the successor (C1 Pro) is stated by ARM to reduce power by just 12% at iso performance leaving Apple with a comfortable lead for a year. Also I specifically was a bit confused by their choice to compare the A725L instead of the M variant.,Neutral
Intel,There's a guy on twitter who does the microbenchmarking for them.,Neutral
Intel,"No. I'm positive memory bandwidth offers very little in terms of performance upgrades. If you recall, the A16 was essentially the same GPU architecture as the A15 but used LPDDR5 instead of LPDDR4X, yet there were practically zero performance improvements.  I don't think anyone has investigated the A19's GPU microarchitecture thoroughly. But the main improvements seem to come from the increase in SLC size (System Level Cache which serves the GPU) from 24Mb to 32Mb and the newly improved Dynamic Caching. Its very likely there are a lot more changes responsible for that 40%+ improvement that we don't know about.",Negative
Intel,With most GPU tasks you are never ALU limited.,Neutral
Intel,"8 elite gen 5 loses in ST perf, only matches in MT perf at same power, wins slightly in GPU perf, loses in GPU RT perf. It doesn't beat it handily by any metric.",Neutral
Intel,">That's really not good for the A19 **Pro**, sadly a mark against Apple's usual restraint. It's a ***significant*** *perf / W* downgrade and the 19 Pro perf / W is notably worse than **ALL** recent P-cores from Apple, Qualcomm, *and* Arm  Technically we need to compare P/W at similar power or similar performance. Peak power P/W is not a very accurate measure to compare gen on gen. And I really doubt you'll be seeing 10W of ST power consumption on a phone. Geekerwan tests these things using Liquid Nitrogen. They are to test an SoC without restraints.   An A19 Pro P core does seem to have increased performance at the same power by about 5%. Only P/W at peak has reduced. Which isn't even an issue when these phones would never reach that peak in daily use.",Negative
Intel,Im having my doubts here. Howis the A19 p core much better than the A19 pro p-core? Aren’t they exactly the same p cores?,Neutral
Intel,">All of these benchmarks are sort of difficult to compare when the other chips aren't made on the same node N3P.   N3P is a mere 5% faster than N3E when comparing similar microarchitectures... This is straight from TSMC's marketing slides.  Comparitively, barring the P core (which did see an ok improvement), the E core and the GPU have seen 30%+ improvements. The node has nothing to do with it.  If Qualcomm loses to, matches or exceeds A19 Pro this year it would be because of their updated microarchitectures and have barely anything to do with minor single digit improvements offer by a sub node.",Neutral
Intel,in the end what matters is the sicion you can buy so you can compare them.,Neutral
Intel,Please tell us more about how we can never compare AMD vs Intel chips by your logic,Negative
Intel,"Tom's Guide tested basic Solar Bay. This is the older version of the benchmark with less raytraced surfaces.  Geekerwan tested the modern, updated version of Solar Bay referred to as Solar Bay Extreme. This new benchmark has a much higher raytraced load, with far more reflective and transparent surfaces and much more detailed scene with more geometry.  Please kindly read the benchmark title mentioned in the posts. Or atleast watch the videos. Before commenting.",Neutral
Intel,Bad bot,Negative
Intel,"I think AMD will have a hard time to win any efficiency crowns, but historically speaking they always had the peak ST performance on process node. Of course this is not as impressive because those are desktop chips which draw 100W+, and there have been historically gaps between desktop and laptop chips in ST for AMD (even though laptop technically had the TDP to sustain ST).  I just wouldn't call the race so early, but it does seem very likely, that AMD will be behind. I just dont think it is as bad as it seems. AMD was plagued by Zen5% and still on 4nm for client, they might hit heavy with client dedicated improvements and N3, but in the end we have to see, x86 client performance really seems to struggle rn (and whatever intel is doing...).",Negative
Intel,">Its either Qualcomm or Apple. AMD is too far behind Oryon and Apple's uarchs.  Considering Zen 5 is designed to clock way higher, I don't think it's that bad really.  Also... Let's not forget about ARM. A SoC with ARM CPU cores and an Nvidia/AMD GPU could absolutely ruin Qualcomm's day regardless of how better/worse their custom CPU cores are.",Negative
Intel,>Qualcomm has a shot at taking the ST crown lead from Apple atleast in SPECfp  No,Neutral
Intel,"IMO, Microsoft needs to push hard to switch to Windows-on-ARM, or else they risk an Android-like OS for Laptops swooping in and filling the gap left by those who do not want to go the Apple route. It feels like a crucial moment for both Windows and Intel/AMD, at least in the x86 product space. It retains PC Gaming at this point, but if that nutt is cracked via even half-decent, *compatible* emulation, then ... sayonara!",Neutral
Intel,"Yeah, the E core isn't quite there yet, but it's good to know they're moving in that direction with now stubborn they are with P cores, refusing to include more than what can be run at the equivalent of all core turbo.",Neutral
Intel,"Wow that will be insane and adding the new ""Tensor"" elements added to the GPU cores will make it a formidable AI workstation.  Especially when NVIDIA monopoly is only offering small VRAM GPU cards at absurd prices.",Positive
Intel,That would be amazing. Id love to see them put some hbm there too,Positive
Intel,It’s highly unlikely they will go past a 256 bit bus. You run out of pins and layers to route a bus that wide. Gets extremely expensive. Still neat bandwidth!,Neutral
Intel,The m5 ultra should be on par with a 4090?,Neutral
Intel,Skymont (Intel’s E core) is 8 wide (9 wide at first stage then bottlenecks to 8 I think iiuc),Neutral
Intel,"According to AnandTech, Apple's E cores were based on Apple's Swift core, back when they had a single core type  Previous die shots show Apple's E cores were close to Arm's A5xx in core area (larger, but far smaller than A7xx core only). But in terms of core+L2 Apple's E cores are similar to Arm's A7xx in core+L2 area  I'd argue it's the other way around, Apple's E cores are the true E cores  Whereas Arm's A7xx were stuck between being an M core and an E core  Now Arm has split their A7xx core to Cx Premium (M core) & Cx Pro (true E core)  Arm's A5xx/Cx Nano are a very odd core, almost no else makes a similar in-order core. Arm's A5xx/Cx Nano are more like Intel's LP E cores, instead of Apple's E cores",Neutral
Intel,The decoder,Neutral
Intel,"Amd is very far behind in rt.  You re linking gaming benchmarks, thats not rt thats mixed use.    Just look at path tracing results for a more representative comparison",Negative
Intel,"Hey, they made an effort with RDNA 4. I think that should surpass the M4 Max. I just can't find any proper scores for it.",Neutral
Intel,"Metal vs Optix.  https://youtu.be/0bZO1gbAc6Y?feature=shared  https://youtu.be/B528kGH_xww?feature=shared This is a more detailed video with individual comparisions and a lot more GPUs.  Its a lot more varied. In Lone Monk, it hangs with a desktop class 5070. In Classroom, it hangs neck to neck with a 4060Ti. In Barbershop, it falls behind a desktop 4060Ti. In scanlands, it falls behind a 4060.   If we consider Classroom as a baseline average, a theoretical 60% faster M5 Max, like the jump we saw in Solar Bay, would land hot on the heels of a desktop class 5070Ti, a 300W card. Competing with a 65W laptop GPU.  Edit; The Youtuber is using the binned 32C variant. A 40C variant would surpass the 5070ti.",Neutral
Intel,"Yeah I forgot what was the term before but I remember, it’s just like Nvidia’s Shader Execution Reordering introduced in Ada Lovelace.",Neutral
Intel,I have a query regarding RT workloads. Would offsetting RT performance to the CPU with the help of accelerators help? Or is that not the case and it would be even worser on CPUs.,Neutral
Intel,"Sure it might be different, but I doubt it. Occupancy is just threads used/total threads.  It's interesting how first gen dynamic caching + SER (apple equivalent) is hardly better than NVIDIA in terms of occupancy. Yet only 44%. So only slightly better than NVIDIA (upper end of range). Seems like first gen was more about laying the groundwork while second gen is really about pushing dynamic caching allocation granularity and efficiency. At least so it seems.  Oh for sure. That occupancy is incredibly high. \~1.5x uplift vs A18 Pro. Getting 70% occupancy in RT workload is really unheard of. Apple engineers did a fine job.  AMD might opt for this nextgen if they're serious, but it's a massive undertaking in terms of R&D, but could massively benefit PT and branch code, ideal for GPU work graphs.",Neutral
Intel,Agreed and you can see that by comparing with occupancy numbers for competitors.       Anyone who's serious about RT needs to copy whatever Apple is doing xD,Positive
Intel,"C1 Pro is two generations ahead of the A720 in the 9400. Also, Xiaomi demostrated a much more efficient implementation of the A720 cores in their O1 chip (4.06 points at 0.82 W).  Edit: actually, it seems like the O1 uses A725 cores. Perhaps that is what they are referring to in the video as ""A720 L""",Positive
Intel,It’s been a BIG bottleneck in AMD’s apus since Vega 11 back in 2018. Doubling from 8CU to 16CU in 860m vs 890m gets you only +30%.   AMD is just so damn stringent with area despite jacking up the price on Strix point massively on an old ass node.,Negative
Intel,"You ought to do the math first. Power is the denominator. 12% reduction in power is *substantial*.  Integer: A19 Pro E-core is 3% faster at 12% less power vs claimed C1-Pro.   |Core / Node|int perf|int %|int power|int perf / W|int perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|4.17|103%|0.64W|6.51|132%| |Xiaomi A725 / N3E|4.06|100%|0.82W|4.95|100%| |\-12% power|4.06|100%|0.72W|5.64|114%|  Floating point: A19 Pro E-core is 1% faster at 4% less power vs claimed C1-Pro.  |Core / Node|fp perf|fp %|fp power|fp perf / W|fp perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|6.15|101%|0.92 W|6.68|120%| |Xiaomi A725 / N3E|6.07|100%|1.09 W|5.57|100%| |\-12% power|6.07|100%|0.96 W|6.32|113%|  Hardly ""generations ahead"".",Neutral
Intel,Thanks for your reply.  CPU monkey reported 2GHz instead of 1.62GHz. So maybe that's where most of the gain comes from.  I suppose the tensor core like matmul units also boost performance for graphics and AI.,Positive
Intel,Maybe the improvement is from the new matmul units?,Neutral
Intel,"I am deeply suspicious of all of these power measurements. Separating p core power usage from other aspects of the soc is difficult on iOS. Which is not a criticism of your summary, but I’d be wary of drawing anything definitive about the efficiency of the p cores.",Negative
Intel,"It's not a knock on the design, it's how apple is configuring the CPU. It doesn't matter that performance at the same power is improved if the default clocks on the real product put it way past the sweet spot into diminishing returns so bad it regresses efficiency.   On one hand, the power inflation isn't causing problems if the 23% increased battery life per Wh is anything to go by, but on the other, what's the point of chasing peak performance like this if your boost/scheduling algorithms never allow that speed to make an impact on responsiveness?",Negative
Intel,"Geekerwan's results are ***average*** power, not peak power, IIRC. These are real, actual frequency bins that Apple has allowed.  These frequency bins *will* be hit by some workloads, but just not nearly as long as SPEC & active cooling will allow. It would be good to revisit Apple's boosting algorithms, but IIRC, they hit 100% of Apple's design frequency in normal usage.  It's not like users have a choice here; we can't say, ""Please throttle my A19 Pro to the same power draw as the A18 Pro."" Low power mode neuters much of the phone, so it's rarely used all the time.  //  I find avgerage power useful two reasons:  1. *How* performance vs power were balanced; here, performance took precedence while not keeping power stable.  2. It also shows, when nodes are not the same, where the node's improvements went. Here, an N3P core delivers notably worse perf / W versus an N3E core. TSMC claims [up to 5% to 10% less power](https://www.tomshardware.com/tech-industry/tsmcs-3nm-update-n3p-in-production-n3x-on-track) on N3P vs N3E.  I agree 10W is not common and SPEC is a severe test, but it's more the *pattern* that has emerged on Apple's fp power and whether it's worth it:  2023 - A17 Pro P-Core: 6.40W  2025 - A19 Pro P-Core: 10.07W  Apple has leaped +**57%** average fp power in two years. Seems like not a good compromise when you're eating more power per unit of performance.   That is, the A19 Pro on fp has skewed towards the flatter part of the perf / W curve.   >And I really doubt you'll be seeing 10W of ST power consumption on a phone. Geekerwan tests these things using Liquid Nitrogen. They are to test an SoC without restraints.  I agree it's rare, but why would Apple allow 10W? Were many workloads ***lacking*** fp performance that users prefer a bit less battery life for +9% fp perf vs the A18 Pro?  Of course, to most, battery life is more important, IMO, which is why core energy is most crucial, but missing here.  //  >An A19 Pro P core does seem to have increased performance at the same power by about 5%. Only P/W at peak has reduced. Which isn't even an issue when these phones would never reach that peak in daily use.  So the question becomes: do users want slightly better perf and 5% more power? On a phone, I'm of the opinion that power is paramount and should be forced to lower levels.",Neutral
Intel,">Aren’t they exactly the same p cores?  Definitely not. See the SPEC perf / GHz: A19 is nearly just the A18 Pro. Thus, this seems to be the final picture:  A19 Pro = new uarch, 32MB SLC, 12GB LPDDR5X-9600  A19 = binned OC of last year's A1**8** Pro, w/ faster LPDDR5X-8533, but smaller SLC (12MB)  New uArch clearly didn't pan out as expected in fp efficiency. A19 Pro may sit at a flatter part of the freq / power curve, A19 Pro may have more leakage, A19 Pro's [faster RAM](https://en.wikipedia.org/wiki/Apple_A19) may eat *a lot* power (Geekerwant tests mainboard power, not purely CPU power), etc.",Neutral
Intel,I'm sorry you had to take time away from ripping off others' content to correct my mistake,Negative
Intel,">I think AMD will have a hard time to win any efficiency crowns, but historically speaking they always had the peak ST performance on process node. Of course this is not as impressive because those are desktop chips which draw 100W+, and there have been historically gaps between desktop and laptop chips in ST for AMD (even though laptop technically had the TDP to sustain ST).  M2 and Zen 4 launched around the same period. The desktop chips score around 5-10% faster in Geekbench while using 20W per core power and 30-40W more for the I/O die. Taking the ST crown by a hair's width while using 5-10x more power isn't a win at all imo.",Neutral
Intel,">Considering Zen 5 is designed to clock way higher, I don't think it's that bad really.   I'd agree if they performed as well as their clocks suggest. Whats the point of clocking to 5.7Ghz if a mobile CPU clocked at 4.4Ghz leads you in absolute performance by 15% (Geekbench) while using a tenth of the total power.",Neutral
Intel,This snapdragon era is Microsoft's third time trying arm (windows rt and surface pro x). Hopefully third times the charm,Positive
Intel,Their E-core is faster than something like Tiger lake per clock. It’s about 60-70% as fast as the best desktop CPU from 2020 and probably as fast as a U-series chip from then under sustained loads.  The real takeaway for me is that the rumored A19 MacBook is going to dominate the $600 laptop market segment if it releases before Christmas.,Positive
Intel,"Weirdly enough, I feel like them not letting the E core balloon in size has helped them in the long run. Seems they're focussed on maximizing performance within a limited area. Their E cores have had exceptional jumps in performance since the A12 almost every generation being 25%+ in improvements.  I'd wager that E core dominance is the primary reason why the iPhones are able to match Android manufacturers using super large and dense batteries in terms of endurance.",Neutral
Intel,Doesn't mac studio already 500gb vs nvidia workstations of 96gbs?,Neutral
Intel,NVIDIA currently fist pumping the air over their failed ARM acquisition rn,Neutral
Intel,Its a bit unlikely. Maybe for a version of the M series dedicated for a Mac Pro. But one of the main reasons they can get away with this design is because its very scalable. All the way from A series to Mx Max series. Adding HBM would probably require a dedicated SoC redesign for a very niche product segment in Macs.,Neutral
Intel,The M4 Max already uses a 512 bit bus. Does it not?,Neutral
Intel,Its technically not a true 9 wide core. I think its 3+3+3.,Neutral
Intel,E-Core is relative. Skymont is more of a C-core (area optimized) than what we typically think of as an E-core (energy optimized).,Neutral
Intel,with a viable width ISA it is better to look at the typcile throughput not the peak throuput as you very rarely are able to decode 8 instructions per clock cycle.,Neutral
Intel,"With the 9070? I don’t think I’ve seen any results showing that either, however all I’ve looked at is the blender benchmark charts",Neutral
Intel,"the shader re-ordering is different. (apple also do this).  Even with shader re-ordering you have the issue that you're still issuing 1000s of very small jobs.    GPUs cant do lots of small jobs, they are designed to do the same task to 1000s of pixels all at once.     If  you instead give them 1000 tasks were each pixel does something differnt the GPU cant run that all at the same time... in addition the overhead for setup and teardown of each of these adds even more void space between them.   So apple are doing a hybrid approach, for large groups of function calls they do re-ordering (like NV) but for the functions were there is not enough work to justify a seperate dispatch they do function calling.    This is were dynamic coaching jumpstart in.      Typical when you compile your shader for the GPU the driver figures out the widest point within that shader (the point in time were it need the most FP64 units at once, and register count).  Using this it figures out how to spread the shader out over the GPU.  Eg a given shader might need at its peak 30 floating pointer registers.   But each GPU core (SM) might only have 100 registers so the driver can only run 3 copies of that shader per core/SM at any one time.   If you have a shader with lots of branching logic (like function calls to other embedded shaders) the driver typically needs to figure out the absolute max for registers and fp units etc.  (the worst permutation of function branches that could have been taken).  Often this results in a very large occupancy footprint that means only a very small number of instances of this shader can run at once on your GPU.  But in realty since most of these branches are optional when running it will never use all these resources.  The dynamic cache system apple has been building is all about enabling these reassures to be provided to shaders at runtime in a smarter way so that you can run these supper large shader blocks with high occupancy as the registers and local memory needed can be dynamically allocated to each thread depending on the branch it takes",Neutral
Intel,"While RT does have lots of branching logic (and CPUs are much better at dealign with this) you also want to shader the result when a ray intercepts, and this is stuff GPUs are rather good at (if enough rays hit that martial)   We have had CPU RT for a long time and so long as you constrain the martial space a little GPUs these days, even with all the efficiency  loss are still better at it.   (there are still high end films that opt for final render on cpu as it gives them more flexibility in the shaders they use) but for a game were it is all about fudging it GPUs are orders of magnitude faster, you just have so much more fp compute throughput on the GPU even if it is running at 20% utilization that is still way faster than any cpu.",Positive
Intel,"Its not just useful for RT but also for a lot of other situations, being able to just call out to functions on the GPU and not pay a HUGE divergence penalty (you still pay some) opens up GPU compute to a load more cases were we currently might not bother.",Neutral
Intel,I'm a bit confused. You think lagging 15% behind in P/W the competition for an entire year is not being a generation behind?  ARM themselves have managed only a 15% jump this year. So it will essentially be 2 years before we get an E core that matches the A19 pro. And this is just considering the Xiomi's SoC. Mediatek's and Qualcomm's which dominate the majority of the market lag even further behind.,Negative
Intel,I'd advise against using CPUmonkey as a reliable source. They're known to make up numbers. (Reported M1 Pro/Max Cinebench scores 6 months before they launched based on predictions),Neutral
Intel,"I agree to some degree; Geekerwan notes they are testing mainboard power, not core power (if you Google Translate their legend).  For me, I assume all the major power draws on the motherboard *are* contributing to the overall SPEC performance, too.   If the faster LPDDR5X-9600 in the A19 Pro eats more power, it's fair to include that power because ***all*** A19 Pros will ship with LPDDR5X-9600. That was Apple's choice to upgrade to this faster memory.    Now, you're very right: we can't purely blame the uArch at Apple. It may well be the DRAM or the boost algorithms (like we saw at the A18 Pro launch last year) and—at Apple specifically—even the marketing overlords.  It's also why I'm a big proponent of JEDEC speeds & timings & volts in desktop CPU tests, much to the chagrin of a few commenters.",Neutral
Intel,Why is separating p-core power usage from SOC power uniquely difficult on iOS?,Neutral
Intel,I understand your reasonings. But its the only semblance of comparison we have to date between different SoC. I've learned not to look a gift horse in the mouth.,Neutral
Intel,">Apple has leaped +**57%** average fp power in two years. Seems like not a good compromise when you're eating more power per unit of performance.   >That is, the A19 Pro on fp has skewed towards the flatter part of the perf / W curve.   I mean this has been a trend long before the A17. Apple has been increasing peak power little by little since the A12 Bionic.  I remember reading Anandtech articles about it.",Neutral
Intel,Don't be a sour puss now because you didn't check your sources before commenting. Mistakes happen.  >ripping off others' content   Eh? Are you stupid? You're pissed that someone posted a hardware review ON A hardware sub? The entire sub exists to discuss hardware bozo.,Negative
Intel,"While the efficiency for this is BAD, I dont think its 20W per core.  When we look at results by [Phoronix](https://www.phoronix.com/review/amd-zen4-zen4c-scaling/2) we can see \~7-8W per core for this (not great numbers, because its a weird chip and different node), which is still very bad. AMD certainly has some power issues, but many of which, i.e. inefficient I/O dies, are not really dependent on the CPU uArch and could switch at any moment. They certainly have much more inefficient chips at the moment than both Apple and Qualcomm. For Zen 6 we expect a major update to the desktop chiplet architectur which could bring some much needed improvements in terms of I/O though.  They have reasonably fast cores, and I think they are not in a terrible position, even though it is far from good. I think what is interesting for AMD to look out for is that they keep moving fast, instead of intel who didnt move fast since like 14nm, and AMD has strong cores. Additionally AMD has (including from the datacenter) an enterprise need to make the CPUs more efficient.  So yeah, very bad CPUs efficiency wise. A bit behind, but not terribly on perf per node wise, efficiency on the desktop is an afterthought for AMD, clearly, but they are moving constantly and are improving. It might be AMD Laptop Zen 6 has again like 35W TDP, for 3000 Geekbench and be dead, but with some client oriented tweaks I see chances (maybe just from the patterns in the tea leaves in my mug)",Negative
Intel,"I originally saw this on Phoronix' forums, but I can't find the link to the comment so I'll send this one instead: [https://blog.hjc.im/wp-content/uploads/2025/05/SPECint2017.png](https://blog.hjc.im/wp-content/uploads/2025/05/SPECint2017.png)  Zen 5 is now behind, yes, but it isn't really that bad.",Neutral
Intel,"Per clock, the A19 Pro E core is competitive with Golden Cove. Atleast in SPECint.  M4 E core-> 3.53 points at 2.88Ghz (1.23 p/Ghz) or IPC i9 14900k-> 9.93 points at 6.00Ghz (1.65 p/Ghz) or IPC  A19 Pro has a 22% jump in IPC in SPECint (might be some variations due to time difference and lack of knowledge about the subtests ran, but still gives a good picture)  22% IPC jump over M4/A18 E core = 122% of 1.23 = 1.50 A19 P core = 1.50 p/Ghz  Golden Cove in i9 14900K has a mere 10% lead over A19 Pro in perf/clock in SPECint.  https://youtu.be/EbDPvcbilCs?feature=shared  Source: I9 14900K compared with M4 in this review.",Neutral
Intel,It does but the Macs are limited in other ways (memory speed among other things),Neutral
Intel,It wasn't for a lack of trying.,Negative
Intel,Yeah there was some rumors of a server version and thy have a patent for a multi level cache setup but they also patent and prototype plenty of things that never get released.   https://www.techpowerup.com/277760/apple-patents-multi-level-hybrid-memory-subsystem,Neutral
Intel,"Oh huh, it does but it’s 128 bit per channel. So memory on 4 sides of the die. Wild, don’t see that normally except in FPGA for data logging (or GPUs)",Neutral
Intel,"The difference seems a bit drastic in open data benchmarks.   https://youtu.be/B528kGH_xww?feature=shared  Testing individual scenes, the 9070xt and M4 Max seem neck and neck.  The M4 Max at best (in Lone Monk) is 5070 desktop class and at worst (in Scanlands) is 4060 desktop class. On average, I'd say in Blender, it is neck and neck with an RTX 4060Ti desktop card. I think a theoretical M5 Max should be on par with a 5070Ti if we see the same 60% bump in RT performance.",Neutral
Intel,Apparently Cinebench 2024 GPU is not compatible with RDNA4 cards lol. So I can't find any scores to compare.,Neutral
Intel,"So does dynamic caching ensure that the total size will ""always"" be the same as whats being called? As in certain cases it is still possible that there can be wastage like for the example you said ""Eg a given shader might need at its peak 30 floating pointer registers. But each GPU core (SM) might only have 100 registers so the driver can only run 3 copies of that shader per core/SM at any one time."" on that, there would be 10 registers wasted doing nothing, if it cant find any else thats <10 registers to fit in that.",Neutral
Intel,Yeah you're right. Should've said branchy or low occupancy code.  Combining such a HW side change with change on SW side with GPU work graphs API could indeed open up many usecases and new possibilities that are well beyond the current way of doing things. I can't wait to see what Apple does when Work Graphs arrives in a future Metal release.,Neutral
Intel,"Well when you put it like that (13-14% behind in efficiency) saying ""generations behind"" certainly sounds misleading.  All it would take for another arm vendor to beat that is jumping one node(let) earlier than Apple, which is certainly doable given the lower volumes, although traditionally it rarely happens. Or jumping earlier to anything that might give them an advantage really, e.g. lpddr6.",Negative
Intel,"It's not ""generation**s**"" behind as you originally wrote. It's being compared to cores from **a year ago** already, mate.  I fully expect MediaTek to adopt C1-Pro and Qualcomm for sure will also update.  Apple's E-cores are simply nowhere near as dominant as they used to be in the A55 era.  EDIT: before we speculate more using marketing for Arm and hard results for Apple, let's check back in a few months to see how C1-Pro  *actually* performs and how Qualcomm's smaller cores actually perform.",Neutral
Intel,Its a bit harder since we don't have any A19 Pro die shots yet. But Apple's E cores have always been sub 1mm2 compared to A7xx cores.,Neutral
Intel,Yeah. All fair points. I don’t disagree. It’s still fun to speculate!,Positive
Intel,"Because we don’t have the tools. On macOS Apple provides powermetrics but Apple states that the figures can’t necessarily be relied on. On some very specific tests you can narrow down power to a cpu core, kinda. Spec tests often stress other aspects like memory, so I would use the provided figures as a guide. Either as a “p core bad” or “p core good” conclusion.",Neutral
Intel,"Oh for sure. It’s not a criticism of either Geekerwan or yourself. They are doing a great job with the available options and I appreciate your summary. I just find it a little amusing when people dissect milliwatt differences as absolutely accurate. We just don’t have the tools, and people are keen to jump on the “p core doomed” bandwagon.",Positive
Intel,"The power increaseas are definitively (and definitely) accelerating, though, worse than it used to be.  [**Geekerwan P-core power**](https://youtu.be/iSCTlB1dhO0?t=422) **| SPECint2017 floating point**  *% is from the previous generation*  A14: 5.54W  A15: 5.54W (+0% power YoY)  A16: 6.06W (+9% power YoY)  A17 Pro: 6.74W (+11% power YoY)  A18 Pro: 8.18 W (+21% power YoY)  A19 Pro: 10.07 W (+23% power YoY)  //  AnandTech did the *great* work of measuring energy consumption / joules. That really proved that race to idle was working; more instanteous power under load, less overall energy   [**AnandTech P-core Power | SPECint2017 floating point**](https://web.archive.org/web/20240907062349/https://www.anandtech.com/Show/Index/16983?cPage=15&all=False&sort=0&page=2&slug=the-apple-a15-soc-performance-review-faster-more-efficient)  A14: 4.72W, 6,753 joules  A15: 4.77W, 6,043 joules (+1% power YoY, -11% energy YoY)  Average power went up, but energy consumption went down.",Neutral
Intel,You copied and pasted someone else's data and now you're acting like a hero,Neutral
Intel,"Still very bad used to work in their IO team, and the idle eatt performance is still very bad. Their idle core loading is also not as good as I would expect for their next generation, and its sad to see the future generation lose to even on M3 on benchmarks. The LP core was supposed to help in this situation, but I just can't see past the fact on why the gap is actually widening over time. I hope Intel can come in and close this gap.",Negative
Intel,"The 9950x lags by 8% behind the M4. M5 is another 10% on top of this lead. M series chips use around 8W of power in total to achieve this perf including memory and what not. 9950x is like 20W per core, another 40W for the I/O and another unknown amount for mempry.",Neutral
Intel,I was comparing overall score.  Geekerwan got 4.17 at 2.58GHz which is ~1.62pt/GHz which is a little higher than your speculation and a mere 1.8% IPC difference from that Golden Cove number.  The fact that it did this at 0.64w in such a tiny core is absolutely incredible. It once again raises the question about how much ISA matters. I don't see any x86 E/C-cores getting anywhere close to those numbers.  https://youtu.be/Y9SwluJ9qPI?t=260,Positive
Intel,one will think the massive vram capacity just override the disadvantages.,Neutral
Intel,"dynamic caching would let more copies of the shader run given that is knows the chances that every copy hits that point were it needs 30 registers is very low.   If that happens then one of those threads is then stalled but the other thing it can do is dynamicly at runtime convert cache, and thread local memroy to registers and vice versa.   So what will happen first is some data will be evicted from cache and those bits will be used as registers.   maybe that shader has a typical width of just 5 registers and only in some strange edge case goes all the way up to 30.   With a width of 5 it can run 20 copies on a GPU core that has a peak 100 registers.",Neutral
Intel,"Metal has supported GPU side dispatch for a long time, (long before work graphs were a thing in DX) Barries and fences in metal are used by the GPU to create a dependency graph between passes and this is resolved GPU side (not CPU side).   I don't see the explicit need for some extra feature here as we already have \*and have had for a long time since metal 2.1\*",Neutral
Intel,"Historically, the core only area for Apple's E cores have always been between Arm's A5xx & A7xx cores, but closer to Arm's A5xx  But for core+L2 (using sL2/4), Apple's E cores have always been similar to A7xx cores in mid config",Neutral
Intel,"I agree that the presence of inaccuracies is very likely. And I certainly don't think the P core is doomed for a 10% jump in what is essentially a a very minor node upgrade.  But considering the video does go into the P core's architecture where the only substantial changes were the size of the Reorder Buffer and a marginally better branch predictor, the performance numbers make sense.",Neutral
Intel,This is a combination of the meaningless smartphone benchmark game (95% of users would be perfectly fine with 6 of Apple's latest E-cores) and the need to have a powerful chip in laptops/desktops all sharing the same core design.,Neutral
Intel,"I'm confused. And legitimately concerned about your mental faculties.  Literally in your previous comment, you posted Tom's Guide data, data thats not ""yours"" to try and discredit my post. And checking your post history, you also posted a Mrwhosetheboss video to discuss battery life comparison in an another subReddit.   So are you a hypocrite since you're ""stealing"" data as well? I'm not stealing anyone's data. I'm correcting your stupidly incorrect conclusion with a source to back it up. Just like you attempted to lol.",Negative
Intel,"You see 7-8W increase going from 1 Core to 2 Core, which indicates that AMD has a huge base overhead but the core doesn't use 20W.   So its a problem with the SoC design not with the CPU design that it uses 20W for ST.  Its more like running a ST workload draws 20W SoC + 8W CPU stuff. So if they had better SoC design they could substantially boost efficiency even with the same cores.",Neutral
Intel,"Comparing the 9950X to the M4/M5 is a bit of a stretch... I'm not saying AMD is as good, but if they did a ""Lunar Lake"" with Zen 5C + on package memory they wouldn't really be that far off.  I want x86 to belong in the museum too, but sadly the ISA doesn't really matter (that much) and AMD isn't exactly incompetent... EPYC CPUs are still dominant and this is what they're truly targeting...",Negative
Intel,"Oh I was referring to this statement in your previous comment.  >Their E-core is faster than something like Tiger lake per clock.  By ""per clock"" I assumed you mentioned IPC.",Neutral
Intel,"I wouldn't recommend comparing IPC from two different runs of SPEC2017. There could be numerous different changes or subtests we don't know about. For eg, compare IPC numbers here vs the test used in A18 review. There are some notable differences in scores and IPC.",Neutral
Intel,Oh it's a huge benefit (I have a M2 Ultra at work) but we still use Nvidia. The cuda ecosystem is far more mature and widely supported with better support for embedded and datacenter scale compute.,Positive
Intel,"I see, so dynamic caching can make it so a shader doesnt have to be 30 registers wide if it doesnt have to do 30 often so it doesnt have to reserve that much space and waste it(such as in conventional cases, if its 5 registers and 30 peak, it will still reserve 30 registers despite it being at 5, which then would waste 25 doing nothing)  Also SER happens first right?",Neutral
Intel,Impressive and thanks for enlightening me.,Positive
Intel,I don’t disagree. The performance figures seem good. The power figures may or may not be. I’m just nitpicking.  Edit: just noticed that they show the A19 having 99% P-core FP performance path 11% less power. That is weird and get’s to my point about power measurement strangeness.,Neutral
Intel,"The needlessly high clocks, agreed: Apple could've still improved perf with lower clocks.  >the need to have a powerful chip in laptops/desktops all sharing the same core design.  They previously kept this in check on the A16 and even A17 Pro, both sharing the same core designs as the M2 and M3 respectively. That doesn't seem too related, as every uArch should scale a few hundred MHz either down or up.",Neutral
Intel,I posted links to reviews. I didn't copy the entire data for my own post,Neutral
Intel,"ISA matters much as well, or mostly the implementation of it. X86 and AMD dont go well together, and AMD is the very definition of incompetency. Both deserve to be sunseted by now. The fact that there is no proper ARM support on consumer platform is the only reason why X86 on consumer still exists. For servers, an ARM server is more power efficient, and only those really legacy stuff requires X86. Companies would really appreciate the cost savings and the ARM ecosystem more than the clusterfk of X86.",Negative
Intel,"Yeah, I guess that wasn't clear. What I meant is that the IPC of the E-core is so much higher than Tiger Lake that it's performance at 2.6GHz is surprisingly close to desktop Tiger Lake at 5GHz.  I know it's not fair to compare with Intel 10nm (later Intel 7) with TSMC N3P as it's 2 major nodes apart, but this goes way beyond that because these chips are using only around 2.5w for just the E-cores. TSMC claimed 25-30% power savings from N7 to N5 and another 30-35% from N5 to N3.  Using these numbers, we get these 4 E-cores using somewhere around 6-7w PEAK, but this is all-core turbo. Intel's CPU at it's most efficient 12w configuration won't hit it's 4.4GHz turbo required to win in single-core performance and is going to hit closer to its 1.2GHz base clock in all-core sustained performance at which point the E-cores not only have their giant IPC advantage, but double the clockspeed too.  All this again to make the point that a macbook with this will be blazing fast for consumer workloads and might finally be the laptop to usher in multi-day battery life.",Neutral
Intel,"This is true, but there aren't a ton of alternatives short of taking a ton of time to compile/run everything myself. I don't think it's worth it for a hypothetical IPC comparison that it sees nobody else would even be that interested in seeing.",Negative
Intel,"Reordering of shaders has a cost, if for a given martial you just hit 10 rays you will not want to dispatch that shader with just 10 instances as the cost of dispatch and scdulaing will be higher than just inlining the evaluation, so you will merge together the low frequency hits into a single wave were you then use branching/fuction point calls.    You will also use this mixed martial uber shader to use up all the dregs that do not fit within a SMD group.      Eg you might have 104 rays hit a martial but that martial shader can only fit 96 threads into a SIMD group so has 8 remaining thread, you don't want to just dispatch these on there own as that will have very poor occupancy (with 88 threads worth of compute ideal) so you instead inline them within a uber shader along with a load of other overflow.",Neutral
Intel,"Are you perhaps blind? The youtube link to the review is at the top of the post? What is wrong with you friend? Feeling a bit under the weather?  The video is in Chinese and before I updated the post with a youtube link, the previous source was from Bilbili, a platform that doesn't even work in most countries.   So I summarised the important points in it for people who didn't understand Chinese and couldn't infer anything from the graphs. No one's gonna ignore the link that is at the top of the post and read my summary before seeing that a link is available.   You're fighting imaginary demons here. Go to your samsung subreddit and whine about Geekbench or something.",Negative
Intel,"But we were discussing CPU uArches, and on the question of ""Does AMDs single core suck up 31W"" the answer is ""NO"", the SoC power is 31W, and that is an issue that can be solved independently of the CPU uArch.  This just means that the massive inefficiencies are not inherint to the CPU uArch, but are a Problem of SoC design.  7W on a single core isnt super efficient, but its far lower than 20-31W. The question is if AMD is able to strip away a large part of those ~20W overhead, but that is not contingent on questions about CPU uArches.  And thats the point of the 1 Core vs 2 Core comparison. To demonstrate that it isnt the CPU that sucks up 20W, but the SoC, which can be solved much differently.",Negative
Intel,Enjoy your scratch-prone iPhone!,Neutral
Intel,Your one brain cell is trying really hard. See if you can wake up its buddy.,Neutral
Intel,"Same price as B580 with lower performance, 4GB less vram and 128 bit bus.  A round of applause for Nvidia",Neutral
Intel,"GB207 being slower than AD107 is pathetic, what's the point of these x07 dies again? They're not thst mich smaller than recent x06 dies.  They're spaffing design cost on these barely different dues.",Negative
Intel,"Now that we have a third-party review, it pretty much confirms what Inno3D said the other day, it's definitively slower than the RTX 4060 by about 5-7%.",Neutral
Intel,The RTX 5050 is 2.5% slower than the Arc B580.   It's also a 50 series card that costs $250.,Negative
Intel,> the system used a Ryzen 7 9800X3D  If the B580 only wins by 2.6% with this CPU then it's going to lose when you use something weaker because of that CPU overhead problem with Intel.,Negative
Intel,It's actually surprising that it's that close to a 4060 considering the 5050 only has 2 GPCs as opposed to the 4060's 3,Neutral
Intel,IDK but that performance is actually not bad. It should've just been cheaper.,Positive
Intel,The only positive about this is they're likely going to make a 5040 with a cut down chip that should pretty easily fit <75W.,Neutral
Intel,"Honestly for people who don't need a lot of GPU power, not the worse.   I have two work computers, one with a 4090 and an old one with a 3090Ti.  These GPUs sit idle just taking up space, would have made more sense to get the something less performant.",Negative
Intel,272mm\^2 for B580 vs \~150 or less for RTX 5050 and perf within 5% lmao,Neutral
Intel,"I think to lose in sales, it would have had to lose a little more convincingly.  Yes, it's worse. But not enough to make up for features and brand-loyalty / nvidias massive reputation with gamers who are not into tech.  This thing will sell. It will sell A LOT. Because Average-Kevin who just wants to play Fortnite and some League will have a great time with it. His YT videos will get upscaling, his shitty mic-quality will get fixed (mostly) by Nvidia Broadcast... and he DGAF why we think it's the wrong move.",Negative
Intel,"Does it actually fall behind the b580 or is it due to vram bottleneck in certain instances?  Either way, this should open up people’s eyes about the b580. The b580 is just marginally above the 5050, which is a terrible product, with 4gb more vram… even at its 250 dollar price point, it’s never been a good deal. Not to mention how it’s going for 400 these days.",Negative
Intel,"People who own 9800X3D's aren't buying B580's or RTX 5050's, i'd wait for more realistic reviews using CPU's people actually own.  Edit: r/hardware is dumb CPU overhead is a thing and budget GPU buyers need to know which of these two cards is effected the most by it. Everyone already knows that you are going to be GPU limited on top tier CPU's its not valuable information for a budget card.",Negative
Intel,Looks like people already forgot that testing the B580 with a top end CPU gives unrealistic results for actual budget buyers.  Every trick in the book for a nvidia sucks article.,Negative
Intel,Don’t make Jensen sad,Negative
Intel,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Will it also fall behind them in the Steam hardware survey?,Neutral
Intel,the 3060 is still a better value than this card bc its about same speed but 12gb vram and higher bus,Positive
Intel,"Maybe a product for non-gamers who want/need multiple display setup, like me. Even then, if you bother to spend that amount of money for 5050, better add some more to get 5060, and if bother to get 5060 just better get 16GB one. Furthermore, you may feel why not add bit more to get 5070 series rather than 5060 16GB. That's how these things are priced.",Neutral
Intel,Value engineering to extract as much wealth as possible while unable to match performance conditions.,Neutral
Intel,When you gimp the memory bus of course it is not as good,Negative
Intel,"No 75 watt gpu, pin connectior, yeah this is death of arrival.",Negative
Intel,"Trying to convince people to buy a 8gb card in 2025 should be illegal. Yet, people will line up to buy it because of the Nvidia logo...",Negative
Intel,"It's a Nvidia Rx 7600, actually good recommendation over that card if it released for the same price in India. Rx 7600 is awful compared to rtx 5050 in pretty much everything.",Positive
Intel,Humiliation for the RTX 5050 is well deserved. It's good this gets curb stomped by the Intel B580.  More market share for Intel because AMD also introduced a useless 8 GB card.,Positive
Intel,">128 bit bus  Seems like this isn't the main issue with 5050's performance. 4060 has lowest memory bandwidth out of all 3, yet it's the fastest card. The die is just too cut down.",Negative
Intel,Unfortunately it will still sell like hotcakes in pre-builts to people who don't know much about computers.,Negative
Intel,"Their brand name is so strong that despite being worse in basically every way, including having poor drivers which was one of their major selling points in the past, they will still sell super well",Positive
Intel,"I just looked at PCPartPicker and the cheapest B580 on there, in the US, is $299 (and the brand is Onix). The Intel Limited Edition is $340. If you can get the 5050 at MSRP it would have a decent price advantage.  I mean, I wouldn't get a 5050 but in the US at least, the $249 price for the B580 isn't real.",Neutral
Intel,"Where I live none of the main sellers stock the B580, a round of applause for Intel.  The B580 isn't faster on the CPU's owned by the people who are in the market of a budget GPU.  People who own 9800X3D's aren't buying B580's or RTX 5050's, i'd wait for more realistic reviews using CPU's people actually own.",Negative
Intel,Intel apparently hasn't shipped any GPUs last quarter. The shortage is already been seen as prices of b580 are going up.,Negative
Intel,"> with lower performance,  even on lower end cpu? Holy crap nvidia wtf",Negative
Intel,Because it will sell a shit load. Probably will be one of the highest selling Nvidia GPUs this generation,Negative
Intel,"At these small die sizes, yields can be really high. High enough that if they wanted to have supply for a 5050, they might not have enough GB206 dies that can be cut down to 20/36 SMs.  The 5060M is probably catching the bulk of the bottom-level GB206 dies at 26/36.  There's also the memory difference, with the 5050 having gddr6 while the rest of the lineup uses gddr7. With dies this small, that cost difference in memory could actually make it worth taping out a new chip with controllers for the cheaper stuff. Gddr6 is dirt cheap at this point.  You could absolutely then argue that there's no reason for the 5050 to be so cut down that it needs a new, even smaller die, and that Nvidia can eat some margins on a budget card to give it better memory. And I'd agree. But I'm sadly not anybody who can make that decision.  Personally, I can't wait to see what becomes of the cut-down GB207s. The 5050 is a ""golden"" fully-enabled die. There will be some with only 18, or 16, or even fewer working SMs. Hell it could lose an entire GPC and be down to half-working. Those will be some absolutely sad little GPUs.",Neutral
Intel,> It's also a 50 series card that costs $250.  I'll believe that when I see it in stock for 250. MSRP is meaningless if the product isn't available for that price.,Neutral
Intel,And will be even worse since it's vram is 8gb. In many many games the b580 would Perform even better,Negative
Intel,Yeah it would be interesting to test with a more real-world CPU pairing.,Positive
Intel,"Yeah, that was my question, did that ever get fixed? It seems like nobody remembers that anymore",Negative
Intel,"TBH, they should have made THIS card a 75W SKU. It probably easily can be, without losing much performance. That would make it a very viable upgrade path for low-end systems. And kind of make the price more attractive, because it would require zero additional changes.",Positive
Intel,Margins go crazy,Neutral
Intel,4x the performance with MFG though.,Neutral
Intel,In mkst cases comparison would make sense since you testing gpu's not anything else. But when you toss in b580 complaints about using too powerfull cpu make much more sense,Neutral
Intel,I have a system with a 9950x3d and a 1060. I’m thinking of getting a 5050 to go with it.,Neutral
Intel,> Every trick in the book for a nvidia sucks article.  That's just a standardised testing suite.   Someone will have low-end CPU testing once this gets any distribution.,Negative
Intel,They're using the same cpu for the 5050 too. What a weird comment to make.,Negative
Intel,"Don’t forget how everyone is shitting on the 5050, calling it the worst nvidia card ever, but were praising intel non stop for the b580.  Even using purely stats from the article, b580 is 3% better with 4gb more VRAM than “worst gpu ever”, so I don’t see how this translates to a win for the b580. Being better than a terrible product by a hair doesn’t exactly make it good.",Negative
Intel,Same can be said for the 5050 no? Both have CPU overheads unlike AMD which any mid-range AMD GPU can easily outperform Nvidia's 5090 at 1080p.   The 12GB VRAM however are better suited for people who are stuck at older Gen3/Gen4 PCIE speeds.,Neutral
Intel,"It'll run nearly every game in existence at 1080p with playable framerates with the right settings. IDK why it would be a product for non-gamers. Calling it that means the B580, 4060 and 7600 are products for non-gamers, too.  Only game I could think of it wouldn't run properly would be oblivion remastered, which has terrible performance on anything.",Negative
Intel,3050 released with the exact same specs and msrp. The 70w variant only came out 2 years after. The first variant was in top 10 of steam's hardware chart.,Neutral
Intel,"This is not a card for 1440p, as it would have issues with getting even 60 fps there, and in 1080p 8gb is mostly enough.  There is a difference between choking on VRAM with 80 base fps (5060ti) and 50 base fps (5050). Yes, second case is still annoying, but you might want to dial down the settings anyway, and then you would be fine.",Negative
Intel,Nvidia RX7600? That a new GPU? 😂,Neutral
Intel,"Funnily enough the 8gb 9060 XT is the best brand-new alternative at $300 and below, hands-down, and that includes the B580. It's x16 interface means it's less restricted by the vram compared to the 5060/Ti 8gb. The B580 still has overhead issues, VR issues, game issues and is rarely available at MSRP.  But people trash it anyway and praise the B580 without actually looking at its performance. In the benchmark linked by the OP alone it loses to the 4060 lol",Positive
Intel,"Performance is fine I'm sure it'll perform just as good as the 3060 maybe slightly if you lower the power to 75W, call it a 5030 and sell it for $150. But these are different times and Nvidia doesn't do reasonable things like that anymore.  Bottom line, it's just the price and TDP that sucks.",Negative
Intel,TBF a pre-built you're just looking at:   * What's the price?  * Does it have a graphics card or not?  * Overall package/size,Neutral
Intel,"or even people who have a baseline knowledge, but think ""dlss and framegen"" will make this abomination better than its competition",Negative
Intel,"It's not so much the brand name but their near monopoly in the prebuilt market. There will be 1 prebuilt with a 9060XT 8 or 16GB for every 20 prebuilts with a 5050/5060. It's so bad that even the prebuilts with the 16GB 9060XT will be outsold by prebuilts with a 5050 at the same price range, as companies like to pair trash GPUs with ""high"" end CPUs like a 13700F to clear stock but the ""i7"" allows them to mark up the price.",Negative
Intel,"It was in stock at newegg monday for 3+ hours at $249.99, I picked one up after seeing a post that was 3 hours old on /r/buildapcsales   I've stopped into microcenter a couple of times and they said they get them, but I just haven't been lucky enough to be there when they were restocked.  EDIT: it is back in stock as of 11:40 EST at Newegg",Neutral
Intel,"In Germany you can find Arc B580 for €269-283 low-end, and B570 for €218-235.",Neutral
Intel,Her in Canada they are $360 CAD. Cheap cheap. And ways to find. Might be a issue in your country perhaps,Neutral
Intel,"Went on Shopee and the lowest I could find is $280, which is not bad considering there's usually taxes which increases prices of GPUs by 10-20% from global USD MSRP.",Neutral
Intel,">the $249 price for the B580 isn't real.  Yes it is, it just gets sold out as soon as it's restocked - https://www.nowinstock.net/computers/videocards/intel/arcb580/",Neutral
Intel,Doesn't it just need a 7600?,Neutral
Intel,Not in Canada. Prices are exactly as launch. Could be your countries problem,Negative
Intel,"Eh, they recently shipped a new batch at MSRP to Newegg.  Don’t know what’s going on with the partner cards. Probably the typical shenanigans",Neutral
Intel,You forgot about the smaller dies.,Neutral
Intel,"The chip is tiny, so a single wafer -> more chips. The demands on power delivery, cooling etc. are nothing to speak of. And the GDDR6 memory is cheap and readily available.  If anything will ever be in stock, it's this card.",Neutral
Intel,5060 has been in stock at MSRP everyday since launch. This should be the same. Even the 5060ti 8gb and 5070 are at MSRP.,Neutral
Intel,MSRP has always been meaningless because it stand for suggested retail price.,Neutral
Intel,"But if the recent LTT video is halfway accurate, AMD would murder both of them in that scenario. The 9060 XT 8GB might want to have a word with those two cards.",Negative
Intel,negative margins for intel,Neutral
Intel,The B580 showed we need to test in the systems the cards will end up being used in. The restricted PCIe lines on the lower tier cards i.e. only use 8 also means they need to be tested on older systems.,Neutral
Intel,You really believe you are an average 9950x3D owner? Really?,Neutral
Intel,"Out of curiosity, what is your use case in such a setup? I find myself cpu bottlenecked quite often on a 7800x3D but i use a 4070S, when i used 1070 GPU was bottlenecking me.",Neutral
Intel,"But Intel Arc drivers have a really bad overhead, while nvidia does not, so in gpu bound games the 5050 will have around the same performance but the b580 will lose a lot of it, so it's not a weird comment, it's a very important point.",Negative
Intel,>in 1080p 8gb is mostly enough.  The narrative was pushed so much that this fact became an unpopular opinion.,Negative
Intel,nvidia Rx 7600 equivalent or Rx 7600 alternative by nvidia. Ig people can't even get a silly joke nowadays,Negative
Intel,The 12 GB of RAM in the B580 will age more gracefully for a truly low end gamers. On purpose of 8 gb is retro gaming.,Neutral
Intel,"Hate to be that guy but to call this a 30-class card is insane. The 1030 was 25% the performance of the 1060, and the 1630 was around 45% the performance of the 1660. This card is 80% the performance of a 5060, which should probably be called the 5050.  To take it a step further, the GT 1030 was 13% of the 1080ti, the 5050 is 20% of the 5090. The 1050 was 25% of the 1080ti, so it would be reasonable to call this a 5040 or I could even be satisfied with 5040ti.",Neutral
Intel,That would be an interesting card,Positive
Intel,I think a big thing about prebuilts are the aesthetics. Does it look good? is the cables managed? Is the RGB controllable?  Slapping a PC together is easy but building something that looks beautiful through the side panel is not as easy.,Neutral
Intel,The 4060 is 2% and DLSS did plenty of work for the even slower 3060. DLSS is going to be great for this card. Frame gen will be good in games this card can run at good base framerates.,Positive
Intel,"cool, it's in stock now",Positive
Intel,A 2-year old CPU ushering a new platform at the time of B580's launch. Far from a budget user's upgrade timeline-wise.,Neutral
Intel,"For a brand-new truly budget build, AM5 is still restrictive pricewise to people eyeing GPUs of this level. The 3050 and 6600 were well-known inclusions in $500 builds, building with a 7500F is going to cost $280-300 just for the cpu, mb and ram alone. AM4 or LGA1700 otoh can be as low as $180-200.",Neutral
Intel,Depends on the game  The 7600 can see some serious performance deficits in some games like Spider Man  https://youtu.be/00GmwHIJuJY&t=521  and Hogwarts' minimum FPS is affected  https://youtu.be/00GmwHIJuJY&t=468,Neutral
Intel,I mean tariffs in the US are a thing...,Neutral
Intel,"It wasn't always meaningless. Overall deviation from MSRP used to be smaller, and it wasn't hard to find the basic models at MSRP once upon a time.",Neutral
Intel,more like we need to test with different processors. Like with high-end to see absolute maximum and some lower grade to see how good/bad driver overhead is,Neutral
Intel,I never said I was,Neutral
Intel,"I'm looking into this now, and you are correct. I am just reading this for the first time.",Neutral
Intel,Lol no. For truly low-end gamers the B580 will perform worse because of its CPU overhead that still hasn't been fixed more than half a year after release.,Negative
Intel,"I mean not really. The b580 is low end enough to the point where it'll become obsolete in performance long before it runs out of VRAM.  It's the equivalent of saying buying a 16gb 7600xt in 2025 is ""more futureproof"" than buying a 12gb 3060 in 2025.",Negative
Intel,an 8 GB card is obsolete at day one for modern games.,Negative
Intel,"It's not about the performance vs XX60. It's about the CUDA core count and bandwith vs biggest die in the lineup. You can find those numbers on Gamer's nexus latest shrinkflation video on this topic (https://www.youtube.com/watch?v=caU0RG0mNHg). Considering those things, the 5060 is essentially a 5050 and the 5050 is a 5030.  The GT 730 had 13.3% as many cores as the full GK110 die, with a 50W TDP.  The GT 1030 had around 10.3% as many cores as the full GP102 die, with a 30W TDP though this one was impressive.  The GXT 1630 had around 11% as many cores as full TU102 die, with exactly 75W TDP.  The RTX 5050 has 10.7% as many cores as the full GB202 die (RTX PRO 6000 Blackwell), and around 11.7% vs the 5090 (which is the number you'll se on Gamer's Nexus video, he rounds it up to 12%, I think that was a mistake on his part? I'm not gonna try and correct him though), but it has a TDP of 130W somehow, just 15W less than the 5060, that's absurd.  Hence why I'm saying it SHOULD be a sub 75W TDP card, and priced way below that. My theory is that it was OC'ed past its efficiency curve to make it at least moderately better than the 3060 and still be able to call it a XX50 card.",Neutral
Intel,Nah most people who buy prebuilts just want a good working PC with minimum effort,Negative
Intel,Yep. That's a problem for that country. Countries with normal leadership have much more viable options,Negative
Intel,Price is only driven by demand and supply. See how the crazy expensive GPU still sell like a hot cake during the COVID lockdown period?,Neutral
Intel,It's well known by Arc owners.,Neutral
Intel,"The 16gb 7600xt  is a giant pile of crap even with 16 GB , so a 12 GB 3060 is the better choice.  If you go to techpowerup you'll see that the b580 is 17% in relative performance than the ""12 GB 3060 """,Negative
Intel,What if you’re playing at 720p?,Neutral
Intel,"> It's not about the performance vs XX60. It's about the CUDA core count and bandwith vs biggest die in the lineup.  That is a absolute dog shit methodology to use.   The 1080 Ti was 471 mm², 5090 is 750 mm².  Those two products are not the same tier. And can therefor not be used as equal reference points. When it comes to die size the top Pascal card is closer to the 5080 than 5090.",Negative
Intel,The point is more VRAM =/= aging better. The b580 having 12gb isn't saving it from becoming obsolete in a few years.,Negative
Intel,"They re not the same price tier either bro... They could have called the 5090 a 5095ti if that makes you happier.  The guy you quoted is right.   Also comparing the performance diff between the 1030 vs 1060 because guess what the 5060 should have been the 5050 looking at die size and memory bus.   Not sure why people keep defending these naming schemes, do you think the engineers use them internally lol? Its bullshit that marketing comes up with",Negative
Intel,"Exactly they're not the same tier, that's why I compared the 1030 to the Titan Pascal and the 5050 to the RTX 6000 Blackwell. 👍 Doesn't matter if die sizes get bigger, that's up to Nvidia. Just because the halo product got bigger doesn't mean they can get away with moving their product stack (below the XX90) one tier up in prices.   I guess it is a dog shit methodology to use, only Gamer's Nexus uses it and I'm sure #1 Nvidia apologist u/Alive_Worth_2032 knows more than him.   It's a shit methodology to use if you love conformism and shrinkflation in GPUs, I agree.  And it's a shit methodology to use if you're an Nvidia executive.  If you can't see that Nvidia is putting all the effort into binning high end chips for AI cause it's 90% of their income, then I don't know what to tell you. Obviously it's a good move for them, and it works, but there's 0 reason for a gaming customer to defend them for it. We used to get much more out of their chips in the gaming cards for more reasonable prices, why's it wrong to want that?   You can't just draw a conclusion based on relative performance between 2 underpowered products, you compare them to the ""best"" Nvidia COULD give us (which is the full top die for each generation) and go down from there.  Edit: just look at the 3090/ti and how the chips were proportionally fine still compared to it and that die was 628mm^2. That's not even an excuse.",Neutral
Intel,"> It's a shit methodology to use if you love conformism and shrinkflation in GPUs, I agree. And it's a shit methodology to use if you're an Nvidia executive.  You can chose one methodology.   You can decide on comparing what hardware you get at a certain price point.   Or you can ignore price/bom and look only at arbitrary model numbers as if they mean something.  You cannot do both at the same time.  Personally I prefer to look at die area. The 5050 today is roughly comparable to the 1050 Ti, comparing it to the top cards that are not comparable is irrelevant.  Pascal did not have a analogue for the 5090, period.  >Exactly they're not the same tier, that's why I compared the 1030 to the Titan Pascal and the 5050 to the RTX 6000 Blackwell. 👍  That changes nothing. The Pascal Titan are using the same die as the 1080 Ti. The whole tier of die that is used in today's consumer top SKUs and for the RTX 6000 Blackwell, DID NOT EXIST back then.  >Edit: just look at the 3090/ti and how the chips were proportionally fine still compared to it and that die was 628mm2. That's not even an excuse.  What are you even trying to say? Ampere also did not have a analogue to the 5090. The 3090 Ti, even it is sitting almost a tier below the 5090. The 2080 Ti and 5090 are unmatched in other generations.",Negative
Intel,"You’re still missing the point by focusing purely on die area and pretending model numbers are arbitrary. Model numbers mean something, nvidia knows that so that’s why they kept them consistent for so long (unlike AMD that's hella inconsistent). Because they rely on the perception of tier consistency from generation to generation, even if they’ve worsened the specs behind the scenes.  You say you “prefer to look at die area,” but that’s irrelevant unless you’re building the chips yourself. Customers don’t game on silicon real estate they game on actual performance and hardware capabilities. And the cuda core count + bandwidth vs top-die approach directly shows how much nvidia is offering relative to what they could offer if they weren't prioritizing AI margins.  So yes I stand by my comparison of the core count vs flagship, I think it's fair to judge Nvidia based on that.  > Pascal did not have a 5090 analogue  That’s just semantics. Every generation has had a full-fat top die, whether branded as “Titan,” “RTX 6000,” or whatever, which where I drew the comparison. Comparing lower-tier cards to the top die is how you reveal how much of the architecture's potential is being offered to gamers.  And yes, 3090 and 3090 Ti still preserved proportionally higher specs vs top-tier dies. What changed now is not just die sizes, it’s NVIDIA reserving most of the silicon for AI and throwing scraps to gaming.  > You can choose one methodology...  Sure, and I chose one: compare the lowest-tier GPU to the top die, which accurately shows how nvidia has been shrinkflating consumer value over time. You're welcome to look at BOM and TDP too the 5050 still loses. It should be sub 75W and priced accordingly.  I think it's okay to call it an XX40 series card even if we keep the TDP at 75W but the XX30 series would need to disappear. What I don't agree with is using the die area excuse to justify shrinkflation just because the 5090 is such a halo product.",Neutral
Intel,"> So yes I stand by my comparison of the core count vs flagship, I think it's fair to judge Nvidia based on that.  Do you even listen to your own madness?  You realize that what you are saying. Is that the lower end 5000 series would be ""better"". If Nvidia removed the 5090 entirely. And renamed the 5080 the 5080 Ti, and made the 5070 Ti into the new 5080.  Suddenly, you would praise the lower end cards for being ""better"". Purely because the high end is less powerful. Nothing else changed, no one is getting more hardware for their money.  Jesus Christ the mental gymnastics you people go trough.   The hardware Nvidia gives you for your money, is all that matters. How large the top die is and what core configuration it has. Is irrelevant for the value for cards further down the stack.",Neutral
Intel,End of Life is not the correct term for this. End of manufacturing is,Negative
Intel,Was about to shit on Intel for such a terrible product lifecycle time and how its GPU division was not going to do well if a GPU only has a ~2 years of updates until I read the article...,Negative
Intel,"End of life typically means end of support, not end of manufacture, or am I wrong?  Anyhoo I blame the article for bad wording",Negative
Intel,"> This announcement marks the beginning of the end for a model that arrived just two and a half years ago, and it offers partners a clear timetable for winding down orders and shipments. Customers should mark June 27, 2025, as their final opportunity to submit discontinuance orders for the Arc A750.",Neutral
Intel,"looks like Intel didnt fire enough of its incompetent staff. EOL usually means end of support/drivers/Developmemt.  [Intel® Arc™ A750 Graphics, End of Life](https://www.intel.com/content/www/us/en/content-details/856777/intel-arc-a750-graphics-end-of-life.html)",Neutral
Intel,"Why did anyone buy these garbage Arc cards to begin with? The performance/$ was never ever ever ever ever not even for a single second better than comparable Nvidia and AMD cards.  I've never understood the point of the Arc cards. It was like a company in 2020 saying ""we're officially into console gaming and just created the PlayStation 2 for only $399!""",Negative
Intel,But did it really even live to begin with? 🤔,Negative
Intel,is this the shortest life cycle of recent GPUs? AMD was notorious for that... wasn't expecting Intel to top that....,Negative
Intel,"So, for the distribution process this is also known as End-of-Sale. Since this is a B2C business, they can't really control when their resellers reach the end of their stock, but at that point Intel has stopped selling the product.  EoM and EoL are both dates that usually do not coincide with the End of Sales.",Neutral
Intel,"The article is poorly written, and the headline misleading.   The card is discontinued meaning no more orders will be accepted. It says nothing about software support.   Admittedly the miscommunication is Intel's fault because they specifically use EOL in their notification, but I also put this somewhat on the article writers because they didn't do a good job of clarifying that intel meant discontinued. They could have used more appropriate wording in the headline but instead chose to follow intel's lead likely knowing it would sow confusion, but lead to more clicks.",Negative
Intel,"Least likely reddit user behavior, actually reading the article probably puts you in the top 5 :)",Neutral
Intel,"Don't worry, Iris Xe GPUs are still ""supported by the driver"", but their last fix was in 2023.  Intel doesn't notify when an architecture is actually dead, you're just left stranded for years until they make it finally official.",Neutral
Intel,"The incompetence source is actually intel themselves ...  [Intel® Arc™ A750 Graphics, End of Life](https://www.intel.com/content/www/us/en/content-details/856777/intel-arc-a750-graphics-end-of-life.html)",Neutral
Intel,> Anyhoo I blame the article for bad wording   Intel chose the wording in its own notice.,Negative
Intel,"Bruh did you even read the article?  Btw it costs half of a 3060 and it outperforms it, no idea why you are saying it is a garbage product lmao shitty redditors don't know the difference between end of manufacturing and end of life/service.  The article is garbage, but it was talking about the end of manufacturing, not the end of the service/life, meaning it will still het driver support.",Negative
Intel,"No because they are still supporting it, just ending manufacturing of new cards.",Neutral
Intel,"For a website that's predominantly text-based, a shocking amount of its users can't read for shit",Negative
Intel,"The job of a journalist is to provide the translation and context for their readers, not copypasta and regurgitate headlines that laypeople will immediately misunderstand.",Neutral
Intel,That's just for the hardware manufacturing.,Neutral
Intel,"ah, i thought it meant end of driver support.",Neutral
Intel,We all did. unconventional use of EOL,Neutral
Intel,"Unfortunately it’s been like that for decades with Intel, I can count numerous times in the past decade we’ve had this same conversation about their processor lines being EOL.",Negative
Intel,End of Sale,Neutral
Intel,"They should work fine together. They did have some issues with overhead with lower end CPUs previously, but that got fixed. I haven't heard about the ReBAR issue specifically, though.",Neutral
Intel,"Try it. Some of the older games are probably fine, newer probably not.  In any case a dedicated gaming desktop PC would definitely be a much better experience. But you don’t have to go crazy. Around $1200 is the sweet spot IMO. Check the sidebar for resources and guides.",Positive
Intel,"[https://www.youtube.com/watch?v=4ptRLMB4m2I](https://www.youtube.com/watch?v=4ptRLMB4m2I) he tested your exact CPU & iGPU and overall it looks playable depends on what your standard of playable means (with 1080p, around 40fps, no 60fps unfortunately). Most of the games you mentioned has upscaling tech like FSR/XESS that you can use in the graphic settings of each game to further improve the performance (basically playing the game in less than 1080p then upscale it to 1080p).     But if what you want is a 1080p stable 60fps for now and for the future, you definitely need something more than what you have right now. 1080p 60fps is easy to achieve with less than 700USD budget. Even more lower if you are confident in buying used parts",Neutral
Intel,"If you’re hesitant about buying games in case they don’t run well, you could always do a month of Xbox Game Pass and use any games on there to benchmark.",Neutral
Intel,"I say build the PC, as a Gen Xer who hadn't touched PC gaming in 20 years I just built a gaming PC.  Had a blast doing it.  Just grab an awesome case and a mid tier GPU for starters.  I bought a Radeon 6800 XT GPU off of a friend and started from there.  My setup isn't top tier but it games incredibly.  Indiana Jones and Ghosts of Tsushima look fantastic.",Positive
Intel,We don't know your exact situation; actually test the games.,Neutral
Intel,"You might be able to play a couple of titles with the the specs you listed (call of duty world at war for example, maybe some older resident evil games) but it would be cutting it close and it would be low/medium settings",Neutral
Intel,"Most single player games are likely to be playable   I'd say install Steam and buy a few - you download them.. . but keep them in a steam account that you can use again on a new computer   So you kinda buy to keep  GOG is another store you can buy games in.. . and is probably a better option for things like ""The Witcher 3""",Positive
Intel,"Treat yourself to the PC, you’ve earned it!",Neutral
Intel,"Intel arc on the laptop runs older games pretty decently. Check the youtube link below for list of games. I think yours might be able to perform slightly better than this due to better cpu and possibly better graphics.  [https://youtu.be/iCY0tuXUOME?si=nPlnrH6olaDjL3S3](https://youtu.be/iCY0tuXUOME?si=nPlnrH6olaDjL3S3)  If youre not satisfied, you can always build/buy a gaming pc. If you take this route, just make sure that youre paying the correct price fot the parts youre getting. Often times you will see pre-built pcs, or shops scamming their customers by selling them ancient hardware for the same price as a new pc.",Positive
Intel,"since you said you have enough money to get a PC, do that, treat yourself with a decent setup, it would be a million times better than playing on a laptop, especially the games you mentioned, if you can share your budget im sure we can figure out a good PC for those games, you won't need anything crazy i promise haha, all mid range pc combinations are able to run all those very well",Positive
Intel,"When it comes to pc game, there are so many variables.  Graphics setting, resolution, upscaling, genre of game and perceived smoothness of different FPS all contribute to how well a game run for you.  It is really hard to tell without trying.    For an integrated gpu, yours is by no means weak, but it still cannot compare to modern discrete gpu.  Generally, anything in or before ps4 era(i.e released before 2020 ) should run fine with minimum graphics setting at 1080p with fsr / xess upscaling. It should safely run at 30fps or above (i.e. as smooth or smoother than ps4 game)  Anything newer you will have to either watch benchmark on youtube, try the demo or benchmark tool of the game(some games offer this).  Steam also support refund of games played in less than 2 hours and purchased less than 2 weeks.  But don’t do that too much or you will be denied the refund.",Neutral
Intel,Those are pretty good specs for a laptop i would think it would be fine but i only play on desktop. Laptop components arent as good because its harder to keep them cool,Positive
Intel,"I think personally you should build a pc! As long as your budget allows it you can get something for around 600 bucks on facebook marketplace, or you can build with used parts off of facebook marketplace.   If you want recommendations I’ll give you some:  Ryzen 5 5900x (180ish used)  RX6700xt (200ish bucks used)  Asus b450-f motherboard (50 - 100 bucks used)  32gb ddr4 ram whatever you’d like, probably Corsair though. (80-100ish)  PSU 650W (something reputable like Corsair) (90 bucks)  You can find good air coolers for the cpu for under 50 bucks just do research.   You’re looking at 680 bucks on the high end but definitely can find parts to fit a cheaper budget. Feel free to message me and I’d be glad to help you find something.  Edit: this is what you should expect as far as performance, you should expect with these exact specs to be able to play currently all games on high settings, competitive or first person shooters tend to run better than single player games in my experience, I’d expect 120+ frames on 1080p, or even higher than that if you drop your settings. As far as single player games, you should be able to max out most games and get a solid 60fps which is fine for single player.",Positive
Intel,"I am just hesitant, I don’t want to spend money on games and then find out they are not working",Negative
Intel,Can I ask how much did you pay?,Neutral
Intel,I wanna make sure they work before spending money on them,Neutral
Intel,"But you said you have the money to get a gaming PC. So… if they don’t work well, you’ll just get the pc, right? That’s what your post suggests to me.",Neutral
Intel,Subscribe to PC Gamepass for a month ($17) and you'll have a bunch of different games to try and see how it fares. My guess is the integrated graphics won't be up to snuff for anything but indie games.,Neutral
Intel,"Steam let's you refund with I think under 2 hours of playtime or less than 2 weeks, of it doesn't work and you don't get the gaming PC just refund the purchase",Negative
Intel,"The nice thing about building a desktop, is that you can upgrade it over time.  Cpu, motherboard and ram need to be bought together, but a decent power supply can last for years as can your case and monitor. I'm an old guy who has been building PCs for myself and kids for around 20 years.   Get a mid tower case with good air flow and a quality PSU.  I'd look at website recommendations for the best bang for your buck AMD BUILD for your cpu and motherboard recommendations , get 2 sticks of 16 gig ram., a 1 tb M2 ssd as a system drive, and dump the rest into the GPU.   27 inch monitors are fairly cheap now, and you can get whatever keyboard and mouse you want.",Positive
Intel,"If you purchase the games on Steam, they are good about giving refunds if you can't play the game.    I think you will find mixed results, with some games/game areas being fine and others painfully lacking.    A lot depends on your own personal tolerance for slow/low res gaming.",Neutral
Intel,Spend money on what?,Neutral
Intel,You got a point. I think you are right. I have money for low/medium budget PC. I think I should actually try the game.,Positive
Intel,"Hello, your comment has been removed. Please note the following from our [subreddit rules](https://www.reddit.com/r/buildapc/wiki/rules):  **Rule 3 : No piracy or grey-market software keys**  > This includes suggesting, hinting, or in any way implying to someone that piracy, or violation of license agreements is an option.   > If a license key is abnormally cheap (think $5 - $30), it is probably grey market, and thus forbidden on /r/buildapc.    ---  [^(Click here to message the moderators if you have any questions or concerns)](https://www\.reddit\.com/message/compose?to=%2Fr%2Fbuildapc&subject=Querying mod action for this comment&message=I'm writing to you about %5Bthis comment%5D%28https://old.reddit.com/r/buildapc/comments/1odu6ga/-/nkxhg3c/%29.%0D%0D---%0D%0D)",Neutral
Intel,"Just try one to start. One of the older titles. If you don’t game much you may find it takes a while to get through a big story game.  I’m in my 30s and RDR2 took me months, lol.  The first of the new Tomb Raiders (from like 2013?) is great, cheap, and very easy to run.",Positive
Intel,"If you buy a game on steam, they have a refund policy so if you play less than 2 hours and its been less than 2 week since perchance you get full refund no question. So  look up how to refund games and then try them all.",Neutral
Intel,"I think people here are a bit too quick to say to scrap builds, but I think starting over isnt a terrible idea.  You can upgrade to the upper end of AM5, but at least in the US market, their prices are inflated to hell. For the price of something mid-line like a 5600X, you're 1/3 of the way to an AM5 upgrade.  That said, a 5600X hypothetical would still be a [big boost](https://duckduckgo.com/?t=fpas&q=5600x+vs+1600+ryzen&ia=videos&iax=videos&iai=https%3A%2F%2Fwww.youtube.com%2Fwatch%3Fv%3DYNzmmyRm3FA) for you, so you'd have to evaluate that.   Technichally speaking, your current PC does hit the minimum specs for Arc Raiders, although how the minimum speca actually translate is anyone's guess now.  You left out your PSU which determines what GPUs you can add. ""Entry level"" cards now are like the b580, 9060XT, 5060TI, and if you go cheaper than that EBay refurbs become something to consider.   You didn't list resolution, target framerate, or budget, so I'm mostly going over high level stuff. To toss out rough figures:  AM5 Upgrade: $400+ not including GPU  GPU: $250-400+   Full Build: Likely $800+ for anything decent, $1000 is the ""typical"" list I see here.",Neutral
Intel,"Just wanted to chime in to add that a b580 would probably not be a good idea without a cpu upgrade to zen 4, 5 or at least a high-end zen 3. The Intel graphics cards make the CPU work somewhat harder than normal, so you won't get the full expected performance with a low-end zen 3 cpu (much less with a zen 1 cpu).",Negative
Intel,Thank you for your reply.  It's helped massively!   I decided to upgrade to Ryzen 7 5700x and a b580 as they happened to both be on sale.,Positive
Intel,"good catch, appreciated.",Positive
Intel,The uplift in raster would be around 15 %. Wait until after Christmas and get a 5070 Ti (if it really has to be nVidia).,Neutral
Intel,"Ahh 50% is what I thought.. you put 15% in your fist comment mate, that’s what confused me   Thanks",Neutral
Intel,You can get the 5070 Ti for £679.99 unless you really want the aesthetics of that particular model,Neutral
Intel,Do it,Neutral
Intel,Is that all? Isn’t it like 35%-40% ?,Neutral
Intel,"Yeah, sorry. For some reason I read ""5070"".",Neutral
Intel,Thanks mate I’ll have a look now,Positive
Intel,"Close to 50 % with a 5070 Ti.     Edit: Sorry, I misread your post. Yeah, the 5070 Ti is a decent upgrade.",Neutral
Intel,https://www.scan.co.uk/products/palit-nvidia-geforce-rtx-5070-ti-gamingpro-s-16gb-gddr7-ray-tracing-graphics-card-dlss-4-8960-core-2,Neutral
Intel,Also this one https://www.overclockers.co.uk/gainward-geforce-rtx-5070-ti-phoenix-s-16gb-gddr7-pci-express-graphics-card-gra-gnw-05399.html  You can get free delivery with code PCPPFREEDEL,Neutral
Intel,"If you really want that MSI one, I think they are doing a promo for a £100 Steam voucher for any purchase so if you get it for £730 then it technically works out cheaper if you can use it  Edit: scratch that, it’s only £40 for the 5070 Ti but worth shopping around if you can make use of their Steam voucher promo  https://uk.msi.com/Promotion/RTX50-100STEAM/graphics-cards",Positive
Intel,"There pre order mate, the only one in stock for sub £740 is the MSI one with scan and I can get it for tomorrow. Thank you",Positive
Intel,"That’s sweet mate, can I buy it off scan then claim the £40 back through msi?   At £730 with arc raiders and a £40 rebate it’s quite a good deal?  Thanks",Positive
Intel,"Yeah I’m pretty sure you can, I would double check the T&Cs and make sure that the model you buy is eligible.   I’ve found Scan’s shipping cost to be quite expensive though.  If you do get it, I think you have to upload the serial number as proof, I think MSI wants the one on the actual GPU itself not the one on the box.  So take a picture of it before you install it.   I didn’t with my motherboard and couldn’t be bothered to take my pc apart lol, lucky it was a random game and not something I actually wanted.",Neutral
Intel,"Heads up, Very is doing the MSI Ventus 3X OC for £707 with code COMPUTING10  Edit: I don’t think that one is eligible for that steam code promo actually oops",Neutral
Intel,"1. Do you still have the AMD compatible mounting hardware that came with the AIO to be able to mount it on the different socket?  2. Avoid Asrock motherboards. They have been having issues this year with specific boards with specific BIOS versions frying specific CPUs.  3. 7600X is a great bang for buck CPU.  4. In BF6, the RTX 9070 averages 103fps whereas the 5070 averages 88fps. IMO it's worth the 10% price premium to get the RX 9070. The extra 4GB of VRAM will also help with longevity and 1440p||4k gaming.",Neutral
Intel,"1. I didn’t, but now i do- thanks! 2. I did some research on this. Apparently it’s a non issue for my CPU that im getting. 3. Let’s goo! 4. Honestly I’m still on 1080p so i feel like 12GB VRAM is good?",Positive
Intel,"In BF6 at 1080p the RX 9070 is faster.  RX 9070: Avg=144fps : 1%low=108  RTX 5070: Avg=139 : 1%low=99  According to techpowerup, across all games at 1080p, the 9070 averages 152 fps while the 5070 averages 143 fps.",Neutral
Intel,What will you use this PC for?,Neutral
Intel,We don’t know what you want to do… so how should somebody give you feedback?,Negative
Intel,Just gaming and maybe some light editing,Neutral
Intel,"Because of high popularity of 5700X3D and 5800X3D -- and that they are now both out of production, it's likely their used prices have increased.  As a result, since you aren't on AM4 already, unless you're getting a killer deal on a used 5700X3D/5800X3D, recommend you go with 9600X + B850 + usual 32 GB 6000 CL30 kit. Should be right around 400 pounds.",Neutral
Intel,"The 5700x3d and 5800x3d are no longer the value deals that they were. They're essentially end of life so have gone up (unless you can find a deal!).    If you want cheap performance and also to keep your DDR4 RAM, i think the best deals currently are i5-14600/K/KF and a B760 DDR4 motherboard. That's going to be the best bang for your buck.    However, i don't see the AM5 parts that much more expensive. Obviously, if you're looking at 9950x3D etc then you're paying a lot. But you could get a Ryzen 5 7600/7600X/9600X that'll perform better than your 7700K for pretty cheap and a B650 board for a good price. The only spanner in the works is the price of RAM. So, in this context, going DDR4 board on last gen systems could save you a lot.",Neutral
Intel,"AM5 is a better idea, it gives you an upgrade path for the future. Something like Ryzen 5 7600 / 7600x / 9600x are all going to be far far better than your i7 7700K.   [https://uk.pcpartpicker.com/list/r93dTM](https://uk.pcpartpicker.com/list/r93dTM)",Positive
Intel,"Am5 any day. The price gap even for used am4 to am5 is so small, that am5 makes so much more sense. The am4 x3d are hugely overpriced, even used. A ryzen 7600x is already equal in performance to a 5700x3d (thats like 5% slower then a 5800x3d). You pay maybe 80€ more for going AM5 but have a much better outlook for an upgrade in some years. There were some edge cases were i would have recommended going am4 used (like 16 cores or a lot of cheap ram), but with ram prices skyrocketing, thats not really an option anymore. I would go am4 if you do a really budget gaming build like with a r5 3600, or you get a REALLY good deal on a am4 prebuilt. Am5 will have a much longer lifespan, with cpu upgrades coming and reusing ddr5....",Neutral
Intel,I wouldn't buy new and get am4 that's for sure. Since your upgrading peice by peice you should get the best CPU you can afford. Get a 7600X or 9600x on AM5 and you might even get a second CPU upgrade out of it.  AM4  might not even be fast enough for an 9060,Neutral
Intel,"AM5, a 7600x more or less is the same as a 5700x3d except in instances when the L3 cache matters and it's 1% lows but 7600x 1% aren't even that bad.",Neutral
Intel,"Hmmmmm  Am5 , because you have better upgrade potential later. 9600x or 9700x.   Intel option , if you can get a cheap used 12400 or 12700k then maybe go for those.  Even 13th gen 13400 will beat your 7700k.",Positive
Intel,Am5 no question,Neutral
Intel,"Brother! I just upgraded from a 7700 and 1070 myself. Served me well for 8 years lol.   I went with a b850/9600x/9060xt and it is badass. You can sometimes save a little with a 7600x as it is only maybe 5% slower and a bit hotter. Went from 30ish fps in helldivers on low settings to 80+fps native, and 120-200 with fsr frame gen stuff turned on. It is a night and day difference vs my old unit... but its 4 generations of hardware newer so of course it is.   In my area for $280us they have a b650/7600x cpu combo with a free aio water cooler on Newegg. Killer deal.   5800x3d is great if you can find one. The 7800x3d is around $360us just for the chip new and actually available. Used 5800x3ds go for 350+ on ebay... not sure why people wouldnt just buy the better chip new for the same price... i guess they dont want to update an old motherboard.",Positive
Intel,Def AM5 if you don't have an AM4 Plattform right now,Neutral
Intel,"It's a false economy to buy in to am4 at this point, go am5.",Negative
Intel,"If you are spending the money, you should go with the current socket system.  7800x3d is a good target",Positive
Intel,"If it's just performance as the i7 you can get AM4 non-x3d or LGA 1700 and still maybe get a gpu upgrade with whats left. If you were considering x3d chances are you'll spend roughly the same for entry-level am5.   You don't really need anything fancy like a 9600x and B850, get a 7500f + b650 and it'll still be a huge upgrade. Even 16gb ram would be alright.",Neutral
Intel,9600x+B850 board.   B650 boards are on end of life. Production stoped and updates soon also.  Go B850 for future path to upgrades following Zen6 and maybe Zen7,Neutral
Intel,Unless you have an AM4 board and the DDR4 RAM already you should go AM5.  AM4 is just not worth it anymore with the scarcity of x3d CPUs and DDR4 RAM prices climbing.  The budget nature of AM4 is pretty much done.,Negative
Intel,"Jump to am5. You’re not already on am4, you have to buy a new motherboard for that, though you can re use memory. But the 7600/9600 areas fast as the 5800x3D and a lot cheaper.",Neutral
Intel,"Entirely depends on what your new vs used pricings are locally. I would not even consider any AM4 X3D rigs here, they cost more than a new set of B650+7500F+32GB RAM.",Negative
Intel,5800x3D is pointless for new builders as it's insanely priced just go AM5 and grab a 7600x/9600x,Neutral
Intel,You're not going to find a 5800x3d at a decent price unless you have some friend who's still holding onto AM4 for some reason.,Negative
Intel,"Do you have DDR4 RAM already? Then AM4 should be fine. The cost of RAM is soaring right now, so either buy RAM immediately or just stick to 5800x3d. It'll be great for years to come, and then just upgrade to AM6.   I would have recommended AM6, just as most will, but that was before the leaping RAM prices.",Positive
Intel,"5800X3D & 5700X3D are out of production and thus, the prices are only getting higher. Including used prices.   Just a quick search on ebay and sold listings are showing an average of $300-350, with some at $400 or more as a bundle with motherboard and RAM.   For not much more money, at around $550, you can get a 9600X, a B850 motherboard and 32GB of DDR5.   If RAM prices weren't skyrocketing like they currently are, you could have gotten a basic AM5 setup for around $450 a few weeks ago. Some of the price hike can be explained by RAM chips being purchased for AI, due to many many data centers popping up all over major cities. But also can be explained by the upcoming holiday season in a few short weeks. Its a common trend now - you can use a price tracker like CamelCamelCamel or PCPartpicker to look at the pricing trend.   Its not really ideal to move to AM4, especially buying brand new parts - the cost difference is in AM5's favor even with the DDR5 price hike. Any better CPU upgrade down the line would require you to jump to a new platform anyway, which means CPU/MOBO/RAM at a minimum.   Hell, if you're willing to wait a few weeks to see what the holiday ""sales"" bring. (more people are becoming aware that a few weeks before the holiday season price hikes tend to happen so that things like Black Friday and Christmas sales make you believe you're getting a good deal)  Starting off with something like the 9600X build I listed below will give you far more longevity, and the socket itself will have 1 or 2 more CPU generations so you'll still have an upgrade path even if something like AM6 comes out. You can adjust your country (for currency accuracy) on the top right if the UK isn't correct.   [PCPartPicker Part List](https://uk.pcpartpicker.com/list/hZMRRV)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 9600X 3.9 GHz 6-Core Processor](https://uk.pcpartpicker.com/product/4r4Zxr/amd-ryzen-5-9600x-39-ghz-6-core-processor-100-100001405wof) | £175.99 @ Box Limited  **CPU Cooler** | [Thermalright Phantom Spirit 120 SE 66.17 CFM CPU Cooler](https://uk.pcpartpicker.com/product/GpbRsY/thermalright-phantom-spirit-120-se-6617-cfm-cpu-cooler-ps120se) | £36.95 @ Overclockers.co.uk  **Motherboard** | [Gigabyte B850 GAMING WIFI6 ATX AM5 Motherboard](https://uk.pcpartpicker.com/product/3f8Pxr/gigabyte-b850-gaming-wifi6-atx-am5-motherboard-b850-gaming-wifi6) | £169.99 @ AWD-IT  **Memory** | [Silicon Power XPOWER Zenith Gaming 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://uk.pcpartpicker.com/product/ypRwrH/silicon-power-xpower-zenith-gaming-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-sp032gxlwu60afdg) | £119.99 @ Amazon UK   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **£502.92**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-10-26 18:06 GMT+0000 |",Neutral
Intel,"youre a bit late , RAM prices are through the roof right now. My suggestion would be stay at am4 for now and wait till AM5 components come down in prices, i dont know when, but they will eventually",Negative
Intel,"That’s amazing, I’ll shop around for some parts and look at the lists others have supplied in this thread, I’d be thrilled if I can get it as low as £400. Thanks so much for this advice.",Positive
Intel,"I’m not especially determined to keep my DDR4 RAM other than the cost of buying DDR5. I’ll have a look at DDR4 options but I’m out of the loop on the 13/14th gen Intel issues. In my ignorance, I’m overly cautious and would probably prefer AMD, and most comments are suggesting the 9600X in that avenue.  Thanks for another perspective, I’ll learn more about the Intel issue and look into DDR4 options too.",Negative
Intel,Thanks so much. That PCPP list is cheaper than I was hoping so I’m very happy with that. Thank you,Positive
Intel,The future upgrade path was also on my mind. I’ve already got the best the LGA1151 socket offers and it’s been frustrating that I can’t upgrade without replacing basically everything! Starting at the low end of AM5 means I can upgrade to better CPUs a few years from now without replacing the motherboard. Thank you for the comment.,Positive
Intel,"Haha, it’s good to hear directly from people who have recently made similar jumps. You hear barely anyone still talking about hardware this old and the best ways to upgrade.   The combination you mentioned seems almost ubiquitous in this thread, and I’ll probably go for the very same. I hadn’t really thought to look at prebuilt or combo deals but I’ll browse around here. It would be nice to move to watercooling too, just maybe not right now. Thanks for the comment!",Positive
Intel,"Don’t I know it. But with consideration of all the other comments here, and the fact I’ll be spending probably £400 even without RAM, I might as well make it last a bit longer by going for a newer socket. Thanks for the comment still :)",Neutral
Intel,"The Intel issue really only applies to i7 and i9. However, i wouldn't even suggest a 13th gen part because the memory controller on them was borked. They fixed that for 14th gen.",Negative
Intel,All the AM5 processors will likely suit your gaming needs while still giving you a bit of an upgrade path if necessary.,Positive
Intel,"Any benchmarks you see for 13/14 Gen make sure to take note of the RAM it uses.  Most are benchmarked using DDR5 and DDR4 will have lesser performance, especially if the DDR4 you have is  DDR4-2133 or something rather than later production DDR4-3200 or higher.",Neutral
Intel,"In general Am4 is not bad per se, a lot of people on am4 (me included) gonna sit out the launch of am6. Those who started at its launch 2017 got a hell of an upgrade out of that socket. Problem is just its really bad value at its price compared to am5....   Oh. And dont forget to sell your old stuff, you will still get money for it....",Negative
Intel,"No worries brother. I hadn't built a pc in 15 years so figuring all this stuff out again was a fun but painful update. It is surprising how badly connected info is on all this stuff, especially how even a budget gaming rig for around $1k today absolutely smashes hardware that was near the top of the line pre-pandemic.   I went with a peerless assassin 120se air cooler... I helped a buddy replace his water cooler in his pre-built when the little pump died on it... made me a little sour on water cooling too. With a little 65w 9600x processor its only about asthetics as you could cool that with a $20 air cooler if you wanted.   Goodluck and happy hunting!",Negative
Intel,"It applies to all Raptor Lake CPUs with a TDP of 65W or higher, which includes the i5s.  However, it was significantly more common on the i7s and i9s as you mentioned.",Neutral
Intel,"Thanks, not sure I’ll get much for a CPU/motherboard that is definitely faulty somewhere but maybe someone else will be able to diagnose it better and get something out of it.",Neutral
Intel,"If we want to be pedantic about it, it applies only to the *600 class i5s as the 400 and 500 are not Raptor lake.    I do not think i ever saw any evidence of failing 14600 class CPUs and with the BIOS updates, these will pretty much be guaranteed to be fine. Voltages are good on these parts.",Neutral
Intel,"9060xt 16gb or 5060ti 16gb  https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html",Neutral
Intel,Basically without a cpu upgrade you'd be holding back most modern gpus you'd put in your system.   I guess you could get a GPU as a future proof. Just remember you won't be getting all of the preformance.,Neutral
Intel,"I was afraid of this, but understood. I'm open to pushing the budget around if needed. The 1440p was not something I originally planned, but after making the swap last year I love it. Again with an eye to future proofing, I get that I need to rethink the budget. If we were talking closer to $500, does that extra $100 do enough to be in a more reasonable ballpark?",Positive
Intel,"You're laying it on a bit thick, it's not even an actual bottleneck on the card. A 9060XT isn't close to overkill",Negative
Intel,I doubt it. They’re aiming for 60fps and are fine with less. And want to spend less than $300. It’s not like they’re trying to put in a 5090.,Neutral
Intel,"Yes $500 is much easier, but that card will work fine for 1440p, it's just that some people don't like buying used.   9060 XT or 7800 XT are both fine for 1440p. 7800 has more raw power but 9060 has better software support. Maybe 16gb 5060ti but they can be hard to find sometimes.",Positive
Intel,No no no. It's more his cpu won't be able to get the most out of a card like that. The card is overkill for his cpu.,Negative
Intel,I disagree. His cpu is very outdated and will bottle neck any modern GPU that isn't super budget tier.,Negative
Intel,"Got it. Yes I have no issue buying used/was frankly expecting I would. This is all good to know, thank you!",Positive
Intel,"https://youtu.be/TXKyQYiLro8?si=x9qS64Z2w4HIJG3Z  There’s outliers, but not many?  And here’s for the 5060/9060: https://youtu.be/NqRTVzk2PXs?si=0Qp-3x-2sdNJeJKo",Neutral
Intel,"Appreciate the back and forth on this. Frankly I just don't think I can upgrade GPU and CPU at the moment, especially if I need to up the GPU budget by $200. I think $500 is probably swing-able (just not what I was planning). But I imagine even with that push I can't reasonably do both. At the risk of sounding silly...the idea of getting a GPU now that will be limited by CPU, and then upgrading the CPU in a couple years, seems feasible and reasonable to me. I don't mind not being able to push things in terms of gaming, or having to wait a couple years to fully use the GPU's potential.   Again I've been fine with my 1060 for all these years...so it's not like I'm trying to max things out.",Neutral
Intel,"I have an R5 5600 and 5070.... Yes, there is some bottleneck... At 1080p(the card is usually 95%).  At 1440p? Minimal bottleneck, the card goes 100%, but, very rarely there is a stutter, which is alleviated with a bit of tweak on the settings.   To say it would be a bottleneck for a 9060xt or 5060ti is a stretch.",Neutral
Intel,yeah. I bought a 9060xt 16gb to go with my r5 3600 after watching their video.  I don't regret it at all.,Positive
Intel,This is fair. It's all up to the buyer. I just like giving people as much info as possible before their purchase. All that matters in the end is if the setbacks are worth it for you or not.,Neutral
Intel,"The ~~5060/~~ 9060xt’s are $350. Or, are you using CAD $?",Neutral
Intel,"I have an R5 5600+ rtx5070.  There is a little bottleneck, but you will not really notice it. Especially, if you play a little with settings on 1440p or dsr.  You shouldn't worry too much... That said, 9070 or 5070 /  9060xt or 5060ti, up to you.  Please  note that, the 9070 is faster than the 5070 and you might need to dial it back a little to minimize the CPU bottleneck.  Hope that helps.",Neutral
Intel,Don't shoot the messenger. Don't believe me? Look it up yourself. These aren't my stats lol.,Negative
Intel,"Of course, and appreciate the info/clarity! Again my biggest hurdle here is just being so out of the game that I have no idea what I'm looking at. The comments here are extremely helpful, even if they are just helping me realize my original plans may not make sense, or I need to be realistic about what I may still be compromising!",Negative
Intel,"USD. But, and I realize I didn't make this clear in the OP, I also don't mind buying used if needed.",Neutral
Intel,Look I don't want you to get the wrong idea. I'm not trying to crap on the Ryzen 5600. I've simply pointed out a fact. One that op should be aware of.,Negative
Intel,Exactly. Most people don't upgrade all at once anyways unless it's a new build. It's a very normal thing.,Neutral
Intel,The Powercolor Reaper 9060xt 16gb for $350 at Newegg seems like the right move if you can swing it. Otherwise you’re back to the overwhelming mess of cross referencing past generation GPU performance and used prices.,Neutral
Intel,"Thanks. Yes I think I can, I realize my budget may have been a little unrealistic - $350 should be entirely doable. Thanks for the suggestion/tip here! That said - if we were looking in the $400-$450 range, anything you'd recommend instead?",Positive
Intel,"There’s too many variations of used options out there for me to definitively say. Looking at some charts and some prices, maybe a 4070 super? Less VRAM but still performs quite a bit better than the 9060xt and has DLSS. Older generations of AMD cards won’t have FSR4 access (right now?) which looks better than FSR3. Could be fine for you, just mentioning it though.   https://tpucdn.com/review/zotac-geforce-rtx-5070-solid/images/average-fps-2560-1440.png",Neutral
Intel,"I am such a big supporter of building PCs but right now, for your budget if you’re in the USA, I would consider this pre built. https://www.bestbuy.com/product/ibuypower-y40-gaming-desktop-pc-intel-core-i7-14700f-nvidia-geforce-rtx-4070-12gb-32gb-ddr5-ram-2tb-nvme-black/J3R75JY7PQ  $824 and with what I think are better specs.  Edit: This is sold out, I apologize. I just got my brother one yesterday.",Positive
Intel,"Looks roughly right, though you could make a few tweaks with the current pricing:  [PCPartPicker Part List](https://pcpartpicker.com/list/4zL2v4)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 7600X 4.7 GHz 6-Core Processor](https://pcpartpicker.com/product/66C48d/amd-ryzen-5-7600x-47-ghz-6-core-processor-100-100000593wof) | $149.99 @ Amazon  **CPU Cooler** | [Cooler Master Hyper 212 Spectrum V3 71.93 CFM CPU Cooler](https://pcpartpicker.com/product/jgbRsY/cooler-master-hyper-212-spectrum-v3-7193-cfm-cpu-cooler-rr-s4na-17pa-r1) | $14.99 @ Amazon  **Motherboard** | [ASRock B650M-HDV/M.2 Micro ATX AM5 Motherboard](https://pcpartpicker.com/product/Dq4Zxr/asrock-b650m-hdvm2-micro-atx-am5-motherboard-b650m-hdvm2) | $109.99 @ Amazon  **Memory** | [PNY XLR8 Gaming 32 GB (2 x 16 GB) DDR5-6000 CL36 Memory](https://pcpartpicker.com/product/dVgZxr/pny-xlr8-gaming-32-gb-2-x-16-gb-ddr5-6000-cl36-memory-md32gk2d5600036xr) | $77.98 @ Amazon  **Storage** | [Silicon Power UD90 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/4kpzK8/silicon-power-ud90-1-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-sp01kgbp44ud9005) | $54.97 @ Silicon Power  **Video Card** | [Intel Limited Edition Arc B580 12 GB Video Card](https://pcpartpicker.com/product/Kt62FT/intel-limited-edition-arc-b580-12-gb-video-card-31p06hb0ba) | $249.99 @ B&H  **Case** | [Rosewill FBM-X3 MicroATX Mid Tower Case w/650 W Power Supply](https://pcpartpicker.com/product/pGJBD3/rosewill-fbm-x3-microatx-mid-tower-case-w650-w-power-supply-fbm-x3-650-g) | $89.99 @ Newegg   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$747.90**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-10-14 12:57 EDT-0400 |  The 7600X is a bit better than the 8400F, so I'd definitely take it instead. The Peerless Assassin is fantastic, but a bit overkill, the $15 Hyper 212 is still going to handle a 6-core Ryzen just fine. I've used the case/PSU bundle a few times, it's a solid deal for a basic but nice tempered glass case with fans and the PSU is serviceable. Altogether it comes in at about $750 with the B580, which is probably the best new option that comes in under your target price, but if you think you can stretch your budget by another $50 the [9060XT 16GB](https://pcpartpicker.com/product/DRYfrH/xfx-swift-oc-radeon-rx-9060-xt-16-gb-video-card-rx-96tsw16bq) is a stronger card. You can also check the used market, the $300 mark can find you an RTX 3080 10GB or RX 6800XT sometimes.",Neutral
Intel,Thanks for the recommendation! Looks like a really good build so it’s unlucky that it sold out. I’ll keep an eye out for pre-builts now too; I had prematurely ruled them out of consideration before. Thank you and hope your brother enjoys the build!,Positive
Intel,Thank you so much! I think I’ll spend that extra $50 for the 9060XT then. This is super helpful!,Positive
Intel,Everything,Neutral
Intel,"Upgrade your cpu to 14600k, add 16gb same model if possible and the gpu is up to you bc the 14600k can handle any gpu.",Neutral
Intel,If you only play esports games then upgrade cpu. Also  ram doesn’t cost too much so you might aswell upgrade that.,Neutral
Intel,CPU,Neutral
Intel,"A CPU upgrade will do the most for the games you play, even something like a i5-14400 would go a long way all by itself. Upping the ram could help some if you can get the same kit you already have for a reasonable price",Positive
Intel,"Used 16 gigs of ram, update bios and get 14 gen Intel cpu",Neutral
Intel,"You can get good CPU and GPU for a ""budget"" price if you checkout Facebook marketplace. With these upgrade, you can get more FPS.",Positive
Intel,bottlenecked by cpu. You'll double frames getting i7 12700k/kf for sure. those games you mentioned are cpu intensive,Neutral
Intel,Gaming chair,Neutral
Intel,[use this website](https://www.logicalincrements.com/),Neutral
Intel,"Honestly, save up money so you can actually buy a new higher end PC sometime in the next few years. You won't regret it. It's a better investment than trying to upgrade your current one.",Positive
Intel,"Jesus, you have like the lowest end hardware for the generation you bought into.... you can upgrade litterally everything.",Neutral
Intel,So i Dont have that much budget but i want to be able to not worry about having low fps also my monitor is 165hz so i dont need that much power but i just want to know what can i keep because changing everything is a lot of money for me right now   Thank You for Your response!,Neutral
Intel,You didn't have to brutalize him like that,Neutral
Intel,Damn I was gonna say that,Negative
Intel,"This is the only reasonable comment, why is everyone else acting like FPS games require 1500 dollars of hardware",Negative
Intel,I remember buying my first RAMsticks. Way cheaper about 8 years ago.,Neutral
Intel,"It's possible his motherboard is older and only supports DDR4. In that case, the memory will be expensive and not worth upgrading.  \-   Added later, I did notice he included the motherboard name, and based on a quick search, that is a DDR4 motherboard.",Negative
Intel,Ok I would look for a i5 14400 and some ram but when I have more budget what would you recommend me to upgrade later on,Neutral
Intel,Just being honest haha not gonna sugarcoat it. Man’s needs a new cpu and ram which requires a new motherboard and a new gpu will of course help a lot.  Literally nothing he can keep  7800x3d/9800x3d  Corsair 6000mhz ram have kits relatively cheap  Hell a 5060 is a lot faster then his Intel gpu but I’d opt for a 5070 minimum Any am5 suitable board,Negative
Intel,I saw that too I was like 🤧,Neutral
Intel,I’m pretty new to pc gaming,Neutral
Intel,yes its ddr4,Neutral
Intel,You don't need a fucking 7800x3d and 5070 for fortnite and valorant,Negative
Intel,All good. I wasn't really judging. Just made me laugh for real  Your response will help him. Gotta start somewhere,Positive
Intel,Someone’s awfully angry haha the guy didn’t post anything regarding budget. I also mentioned 5060 was a huge upgrade and just said what I would get.   If he’s gonna go through the trouble of upgrading his CPU and ram which would require a new motherboard anyways why not get something that’ll last him years. No one “needs” half the pc parts they get but if you keep your pcs for 6+ years then yes a 7800x3d would be a much better choice.   It’s already dropped in price and likely to go on sale during Black Friday. Also gives the guy a chance to upgrade his monitor over the next year or 2 if he so chooses.  You see I don’t mind going to work saving my money and spending just over $1000 for a pc I’ll get years of use out of. Others like you might like buying a new mid range mobo/cpu and then upgrade the gpu later when it becomes problematic and then upgrade the cpu and mobo again in this constant cycle or low tier parts. Everyone’s different lol,Neutral
Intel,BUT IT WOULD BE  NICE TO HAVE n why do you think those will be the only games he ever plays? Lol,Positive
Intel,Because they're the primary games he mentioned? You're recommending he drop well over a grand on new parts like it's the obvious thing to do. An A750 with a better CPU will play most games fine at 1080p just fine.,Neutral
Intel,So he should build a system that can play every game? *Every game?*,Neutral
Intel,I have a Sparkle A380 and it barely breaks a sweat doing 4K transcodes on my Plex server.,Negative
Intel,How many are you expecting to do? I think any modern Intel iGPU can handle 4k transcoding. I think even the $200 N100 type mini PCs could manage a pair of streams.,Neutral
Intel,"The P3 Plus uses QLC flash, which has low durability and becomes noticeably slower after a few years. Get a better drive with TLC flash like the Kioxia Exceria Plus G3, WD Blue SN5000, TeamGroup MP44L/G50, Patriot P400 Lite, or Klevv CRAS C910.",Neutral
Intel,"N305, save on that juice bill (:",Neutral
Intel,i want the capacity for 6 streams. i think any more than that would get hectic  i am trying to make direct play available on most clients by saving lower quality media options for each show/movie so there's that. I'm also going to eventually test out live sports streams but maybe that will be too much!,Neutral
Intel,"looking into this now, looks great!  do you have one? im a noob, it seems to be limited to 32gb ram? not that im looking to get more but would like to eventually if i run more services",Positive
Intel,Ooo okay then yeah the a380 seems like it might be the way to go then.,Positive
Intel,"I do own one, it's a low power chip which can run a couple transcoded streams just fine. There are a few of those chips, not sure which one's the best choice these days but you've got a couple options. They come with the board itself and are often rebranded by various Chinese brands. I think CWWK was the original manufacturer of those boards.  A lot of people, including myself are running it with 64GB of RAM without issues. I'm also running about 30 different containers of all sorts and while there are certainly some use cases it might struggle with, it will be fine with most. Keep in mind it's physical connections are somewhat limited.",Positive
Intel,I hope your RAM isn't actually that slow. The Microcenter bundles tend to be DDR5-6000 CL36.  There's no faster GPU for that price. Anything faster would cost more.  I would recommend the Western Digital SN7100 SSD over the slow one that you picked.,Negative
Intel,should’ve gotten a ram with atleast 6000mhz.,Neutral
Intel,"I suggest some changes. First cooler, You can get a [Thermalright Assassin X 90 SE ARGB 32.77 CFM CPU Cooler](https://pcpartpicker.com/product/Vz9wrH/thermalright-assassin-x-90-se-argb-3277-cfm-cpu-cooler-ax90se-argb) for 16 to save a few dollars to put somewhere else.   You a bit overkill on the psu as the wattage is nearly double what you might be putting together. Great for later upgrades but overkill for this build.   It is not a bad combo deal but there are better choices to be had. The ram is slow but you can use it for now. I would change the ssd. That is a gen 3 and you wil want at least a gen4 with a higher speed.   Using your combo I would change the cooler, cheaper option with good performance, the ssd, while budget and not the fastest it is 2tb gen 4 with decent speed and fully modular 750 80+ gold atx  or you can stick with your 850 watt which ever you prefer.   That is a good card for the cost but i would be tempted to switch it out with a 9060xt 16 gig or 5060ti   [https://pcpartpicker.com/user/xxSineaterxx/saved/#view=YXkXYJ](https://pcpartpicker.com/user/xxSineaterxx/saved/#view=YXkXYJ)  With the 9060xt 16gb reaper the cost is slightly higher but with that card you should get better performance.",Neutral
Intel,gotcha you're saying the silicon power is slow? or the RAM?,Neutral
Intel,"thanks for the advice i'm considering changing the psu and storage for sure, also your pc list is private i can't see it",Neutral
Intel,Both.  The RAM in your PCPartPicker build is DDR5-5600 CL46. This is incredibly slow and I honestly doubt that's what you bought from Micro Center.  The Silicon Power A60 SSD you picked is also slow.,Negative
Intel,"Ram companies don't matter 95% of the time. What you need to watch for are the specs.   CL is your rams reaction time. CL30 or 32 is good.  And MT/MGHz, are it's actual speed. 6000mt or 5400mt is good.  In your last post, we advised you that your ram was too fast because you had a CL28 ram kit. you might have over-adjusted and picked slow ram, my friend. 5600mt is good speed, but cl46 is slow reaction time.  A CL32 5400mt will probably be faster than your ram.   Honestly, everything else is great👍 this pc will serve you for a  good while, even with the slow ram.",Neutral
Intel,Oh sorry lol I was working on another build private  [https://pcpartpicker.com/user/xxSineaterxx/saved/#view=HJy4Bm](https://pcpartpicker.com/user/xxSineaterxx/saved/#view=HJy4Bm),Neutral
Intel,"thanks, yeah yall connvinced me. i'm returning it",Positive
Intel,The best upgrade would be the 7800x3D/B850 MoBo/2x16gb 6000 CL30 RAM. It will absolutely be an increase in FPS and it will improve your FPS drops.,Positive
Intel,"If you are building your computer and unless you are spending like under $700 on the computer I would recommend that you go with an AM5 CPU. AM4 only really makes sense if you really have to like get under $700 and more in the $600 and below range.  If you are building on the AM5 platform then you probably want either a 7600X or a 9600X. The 9600X is only about 3 to 5% faster than the 7600X, but the price has come down considerably and has even dipped cheaper a few times.   If you're going all out then you can get a 7800X3D or a 9800XD3, but I would really only consider those for like really top-end expensive builds that are more than two grand. They shine the best when paired with a high end GPU on low end resolution because you can get ridiculous fps. They also do a little bit better on extremely CPU intensive games but I don't know if battlefield 6 is that intense, I'm pretty sure like a 7600 would be fine.",Neutral
Intel,yeah was think about the 7800x3d. but also AM5,Neutral
Intel,"BF6 sees a big jump with a 9800X3D, it's very much worth it, or for games in general.",Positive
Intel,fair also. was looking at am4 and am5 cpus so was like hmmmm which one lol,Neutral
Intel,"The 7800x3D is an AM5 CPU and the components I recommend will get you the best gaming CPU, second only to the 9800x3D. This list below has the cheapest RAM I can find at the moment and a decent board. If you need a micro or mini board, I can suggest some of those as well.    I will also add that having the 7800x3D isn't a must. A 9600x would also be better than your current CPU. If you live close to a microcenter I would suggest getting their 7800x3D combo, it is usually cheaper than buying components separately.   [PCPartPicker Part List](https://pcpartpicker.com/list/X7d74p)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 7 7800X3D 4.2 GHz 8-Core Processor](https://pcpartpicker.com/product/3hyH99/amd-ryzen-7-7800x3d-42-ghz-8-core-processor-100-100000910wof) | $359.00 @ Walmart  **Motherboard** | [Gigabyte X870 GAMING WIFI6 ATX AM5 Motherboard](https://pcpartpicker.com/product/n3cgXL/gigabyte-x870-gaming-wifi6-atx-am5-motherboard-x870-gaming-wifi6) | $189.99 @ Amazon  **Memory** | [Silicon Power Value Gaming 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/cCKscf/silicon-power-value-gaming-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-sp032gxlwu60afdeae) | $112.97 @ Silicon Power   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$661.96**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-10-16 15:25 EDT-0400 |",Positive
Intel,okay will keep that in  mind. its just the cpu now a mobo and ram,Neutral
Intel,Yes you could just swap the gpus,Neutral
Intel,What exactly is your question?,Neutral
Intel,Do ai really nees to invest in a 14700 and or AMD 5 system if I get a 5079ti or can I just got 5070ti,Neutral
Intel,"What's the most economical and logical path. Opinions. Keep the LGA1700 build and just keep the cpu or upgrade to 14700 to not bottle neck 5070ti or would it not matter? 2nd invest in AM5 system way more expensive it seem for a new mb,cpu,ram, cooler for why to ""future proof?",Neutral
Intel,"well, sometimes a Used PSU is a good deal, but u have no clue how old this PSU is or what its been through. Just my opinion but spending like £65 for something even like the Thermalright KG-750 is worth it over a potentially old and worn PSU.  As for the RAM, I mean buy as much as u can when u build honestly, DDR4 will only get more and more expensive if bought new as its no longer being produced. and yes 3200 CL16 or 3600 CL 18 is a great option for a 5700x",Neutral
Intel,"I'd saw the thermalright ones on UK pcpartpicker but I didn't recognise the brand, are they reputable? I don't want to burn my house down but I suppose an unknown second hand one is just as risky.  I thought the ram would be cheaper tbh but that makes sense.",Negative
Intel,"Thermalright is known for being a cheaper but mostly reliable brand, some of their past PSUs weren't that good but nothing that would burn ur house down. the KG-750 is actually one of their better units that's fairly recent",Positive
Intel,Congrats! Was thinking about getting one for my son's PC. Hope it works great. Happy Gaming.,Positive
Intel,curious how does this compare with the gpu standard-- 5070 cards?,Neutral
Intel,Congrats. Planning to upgrade my aging rx 580 4gb too this spring. Same card,Negative
Intel,Genuinely never heard of this card till now. Turns out it’s popular for the power and price which is rare af these days lol,Positive
Intel,"That’s a sick looking card, gonna look nice man. Congrats, what you gaming? I’ve seen bf6 run great on it.",Positive
Intel,Noice. Solid choice,Positive
Intel,Dude post some gaming with it....how much did u pay?,Neutral
Intel,Purple box? Did Intel switch to team purple??  /s,Neutral
Intel,I've had mine for 9ish months. Really love it! XD,Positive
Intel,4060ti//4070,Neutral
Intel,You won't be disappointed,Neutral
Intel,It’s honesty what the 9060 and 5060 should be  250 for 12gb vram and solid 1080p to low 1440p gaming,Neutral
Intel,verify game files,Neutral
Intel,I installed the latest driver in the nvidia app and cleared my directX cache in disk cleanup and the issues gone for me,Neutral
Intel,"I have this issue, verifying does not resolve.  You can alt tab back into the game and proceed to play with this error hanging in the background - played 3 games before it crashed to desktop. Just seems like a day 1 issue.",Negative
Intel,Having this issue as well. 4080S with a 13900k. Restart and game file verify no joy.,Negative
Intel,More RAM. Upgrade to 32gb.   What did the rig cost?,Neutral
Intel,Tbh i would return it Both cpu and gpu are pretty weak my dude  At that budget i would go used parts hunting,Positive
Intel,Listed price was around 600,Neutral
Intel,I went from a 6th generation PC that could barely give me 60 fps on windows 10 To a computer giving me 120+ Frames on Windows 11. I don’t think it’s that bad Atleast for me.,Negative
Intel,That's decent for a built PC.,Neutral
Intel,What did it cost?,Neutral
Intel,They gave it to me for 600 Flat,Neutral
Intel,Usd? At that price range i would expect something better than a 5500 and 3060  Especially used.  Its not a bad pc just a bad price,Negative
Intel,I can understand that it’s fine for me cause at least now I now have the option to upgrade instead of replacing an entire new computer,Positive
Intel,"God keeps sending me signs to make a Nas and fill it with media and games, 3 times in a month here things have gone belly up.  God I hear you but plz I need you to also provide the cash for it.",Neutral
Intel,DNS?,Neutral
Intel,![gif](giphy|Urn1lBNWpaCGFpuA1c)  Everyone's reaction to Copilot being down,Negative
Intel,Hmm. Gotta start buying gold and bury it somewhere judging from how things are going.  ![gif](giphy|26gsobowozGM9umBi),Neutral
Intel,There was a time when I was made fun of for not liking cloud dependent software.... it's still to this day...  People will read about global outages and still defend not owning anything. Just wild.,Negative
Intel,Stark reminder the cloud is someone's computer.,Neutral
Intel,Azure outage!,Neutral
Intel,its all cloud. all fugazi,Neutral
Intel,"""The Cloud will improve uptime"" my ass. Seriously I hope this shit convinces some companies to go back to local self hosting. And build fucking redundancies. I guarantee you half of these sites are down because one or two services are running on AWS with no fall back service running anywhere. 2 is 1. 1 is NONE.",Negative
Intel,not minecraft :(,Neutral
Intel,Maybe the bubble wants to burst itself.,Neutral
Intel,"That website doesn't accurately measure downtime, this is based on user reports and if some big ISP has DNS issues people start reporting downtime on the services they use. So, I wouldn't make conclusions based on this.  Edit: in this case it is Microsoft though",Negative
Intel,Azure outage,Neutral
Intel,Stuff like this is exactly why we got back into buying physical copies of movies. Worrying about what platform might have the movie or show and worrying if that service will even be available is getting annoying.,Negative
Intel,None of my services are down and I use a lot in that screenshot...,Neutral
Intel,"hate to be that guy, but I'm glad I self host and am on Linux, work on the other hand....",Positive
Intel,"ahahahahahahahahahahahhahahahahahahahahahahahahahahahaha   costcutting and monopolies, meet consequences.",Neutral
Intel,Our AI dominated future is looking bright. If servers are down enough they'll start hiring humans again.,Positive
Intel,copilot!  ![gif](giphy|7k2LoEykY5i1hfeWQB),Neutral
Intel,Who needs people? AI can obviously do it all!,Neutral
Intel,The enshitification of the internet continues …,Neutral
Intel,"This will become more and more frequent. They're silent cyberwars waged against other govs or even other corporations. Do you really believe massive datacenters sometimes just ""go down"" for a few hours for seemingly no reason? Of course not, they are being sabotaged.  Obviously the corporations will never admit any involvement or denounce the fact as it would crash their markets.",Negative
Intel,Be careful with your gold. Remember what Teddy did in the thirties.,Neutral
Intel,What's the name of that site?,Neutral
Intel,"Because of Windows Hello and because I decided to upgrade my motherboard the same day microsoft services go down, I can't log into my Windows.  But hey, at least we don't have local users anymore...",Negative
Intel,_We must use AI to write code faster_,Neutral
Intel,Please don't tell me they tried the same update patch....,Negative
Intel,I couldn’t log into my El Pollo Loco rewards account at their mandatory self service kiosk at lunch.,Neutral
Intel,If I knew about the outage the second it happened. Could I make money off that knowledge in the 30ish minutes before reports start coming in. Asking for a friend lol,Neutral
Intel,"All these companies are getting rid of staff and buying AI replacements and hiring ""vibe coders"" to maintain it.  Who could have known it was a bad idea",Negative
Intel,AI trying to rebel again.,Neutral
Intel,If this could last into tomorrow and stop Outlook from working that would be great,Positive
Intel,"Being one of three IT guys for a school district, this is a very fun time.",Positive
Intel,It’s microslop now.,Neutral
Intel,Canvas was up,Neutral
Intel,"Well, Reddit is obviously working, do we know what the hell caused this? This time?",Negative
Intel,That explains why I couldn’t log in to my licensee portal today 🫤,Negative
Intel,Copilot ate all the datacenter power.,Neutral
Intel,Half day at work? 😃,Neutral
Intel,So now it is Azure XD,Positive
Intel,Leave the internet behind,Neutral
Intel,this time it is Azure,Neutral
Intel,"Never even noticed, probably a good thing i don't use these often or at all.",Neutral
Intel,My buddy is trying to rent a U-Haul but he can't right now because of this,Negative
Intel,What is it now? Cloudflare? Aws again?,Neutral
Intel,"Microsoft has to report earnings this afternoon, that’ll be fun if their systems are still down",Positive
Intel,AI is deciding we don’t need the internet.,Neutral
Intel,Azure,Neutral
Intel,Was it DNS?,Neutral
Intel,Azure DNS. The second of the holy Trinity :-P,Neutral
Intel,It’s like AWS and Azure want people to switch back to on-prem with the number of outages we’ve had as of late.,Negative
Intel,What site is this,Neutral
Intel,"welcome to the cloud internet age. a handful of companies run everything now, so if one goes down then a lot of things go down",Neutral
Intel,THEY FUCKING GOT KROGER ITS OVER!,Negative
Intel,Someone try to install latest windows 11 and see if it works,Neutral
Intel,yeah i saw some dev ops people at work talking about azure having issues today. and possibly aws again as well.   i guess im just going to start mirroring the entire internet on my selfhost,Negative
Intel,This is why competition is good. We need more variety and less monopolies in the cloud service world,Positive
Intel,Skynet I mean openai doing trial runs of complete take over.,Neutral
Intel,"Seems accurate, couldn't log into a college test earlier",Neutral
Intel,I would assume this would be the CDNs right? Or maybe its DNS related. I doubt Amazon and Google are using Azure servers.,Neutral
Intel,It's a engineers first change at his new job since leaving AWS last week.,Neutral
Intel,"Yup, made for a fun day at the office.  Couldnt access anything in our Azure portal and were having on and off issues with SAML logins tied to Entra ID.",Positive
Intel,US East 1 2: The Revenge of AWS?,Neutral
Intel,"My Greggs app was down at this time too, not important just wanted to share",Negative
Intel,I swear to god it happens every time I order something I need for work it happens the day before it is supposed to be delivered.,Negative
Intel,Microsoft being incompetent as per usual,Neutral
Intel,Minecraft! Nooooo!   My brother was bothering me all day about this. Glad to see it's just another web outage...,Negative
Intel,They prolly wanna roll out some over the counter AGI and it recks the internet.,Neutral
Intel,As long as Reddit is still up to kill the downtime ... ;p,Neutral
Intel,"psa  If you trust your neighbours and surrounding community you can build a local internet network of wikipedia ,social networking,many kind of services you think big tech can't be trusted for anymore by using old routers, switches and wifi mesh for decreasing reliance on ISP",Neutral
Intel,"Huh, I'm kinda surprised at how little of these I use. I occasionally play Minecraft but the rest of these Im actually kinda happy are down.",Neutral
Intel,"Down is such a negative term,  let's say they are resynergizing and go for a power lunch tomorrow.",Negative
Intel,How is that AI and Indian H1B code working out for us?,Neutral
Intel,Yea fortnite was acting up last night for both me and my cousin. (Several states apart),Neutral
Intel,"I was looking at the list going ""yeah this sucks for a lot of people but I'm doing ok."" Then I tried to load up ShopGoodwill and got a timeout notice. DAMMIT, now you've gone TOO FAR. :D",Negative
Intel,Ohhhhh ... That explains why Copilot has been giving me absolute garbage responses for the past hour.,Negative
Intel,I doubt its Microsoft related if anything Amazon is down. It's likely an actual ISP is down given aws and Google are down at same time.,Negative
Intel,"When my internet goes out I have to work out if it was a problem with my ISP, the company that provides the fibre to my home, or the company that my ISP actually rents the service from. It's all middlemen.",Negative
Intel,"I just updated to 50TB. Mainly for plex but man, I might start migrating some more things.",Neutral
Intel,built mine with the mindset of not paying for services will eventually cover the cost,Neutral
Intel,"You should definitely, 100%, give building your own NAS server a shot. HomeLab stuff is very interesting to tinker around with and you can give yourself the independence from paid third-party providers  Also, if you have spare parts laying around you can kind of get it running on anything! Doesn't have to be a $10k+ home server setup, it could easily be an older gaming machine you have in your closet or something off of marketplace that somebody is getting rid of  https://www.youtube.com/watch?v=rNp4jlzdSg8  https://www.youtube.com/watch?v=I3yvrZaxaAk",Positive
Intel,"If it helps, I started with $100 for an old hp g3 sff elitedesk and that came with everything I needed to get a private cloud setup and media server going. You don’t really need a large amount to start.  I’ve just been adding harddrives and upgrading some pieces since.",Neutral
Intel,"Start small and build up when you can  Start off with one big HDD and store all your initial stuff on that, then when you've saved up a bit more eor found a good deal get some components and then start adding more storage once your system is good to go  Don't need to buy the entire thing in one go  Won't be perfect to begin with but better than nothing",Neutral
Intel,"See this is my biggest problem right here. Be sending all those signs, how bout you sign a dam check and send it my way.",Negative
Intel,"I used to use NAS but most recently I just bought a 24TB 3.5” drive for my media, hopefully that will last a while.",Positive
Intel,Any suggestions for one? I'm just reading about it now and think it's a great idea,Positive
Intel,"Its been over a decade since I last sailed, and in the past 6 months I've spent quite a bit on 20TB of storage... raise the flags my friends",Negative
Intel,"I use Plex, you can set it up so it works offline on your LAN",Neutral
Intel,"I’ve gone down the deep end on this just recently. NAS, de-googling my life. Immich. Onprem authentication platform for sso. Onprem password vault. Must do more.",Negative
Intel,"wait, what do you mean three times? I thought it was 1 this month (pls correct me if i'm wrong)",Neutral
Intel,And I need an ISP or a subscription from my ISP that could provide upload speeds above 10Mbit/s for the NAS to be viable.,Neutral
Intel,https://preview.redd.it/f7ugkvwy63yf1.jpeg?width=598&format=pjpg&auto=webp&s=49fc656128ffb429b4536b45d8f7c7181ac00fd2,Neutral
Intel,![gif](giphy|6RIEW15CCWRvq),Neutral
Intel,"Yep, Azure DNS this time.",Neutral
Intel,"Yes, it is actually",Positive
Intel,"Nah, DNS store will outlive them all.",Neutral
Intel,"Nah it's damn AI  Edit: wow I guess I need to add /s, of course it's not AI",Negative
Intel,"""Archived"" that crap in the same place as recall...",Negative
Intel,Missed that boat sadly. Gold prices are insane rn,Negative
Intel,The clankers will never guess where I hid my strategic toilet paper reserve.,Negative
Intel,You're planing for a more functional apocalypse than I am.    https://joegardener.com/podcast/small-space-vegetable-gardening-mark-ridsdill-smith/,Neutral
Intel,"Downside is a lot of business and industrial software is becoming cloud only cuz they don't want you to own it, they want that recurring income.",Negative
Intel,Don't try to convince the stupid that they're stupid. Just say that cost cutting has gotten so effective that we stopped needing cloud services at all :D,Negative
Intel,"It isn't just the not owning anything crowd, it's also the ""my smart bed won't recline to flat without an Internet connection"" crowd and the ""I can't open my front door because of my smart lock"" crowd and...  Technology is steadily getting worse, full stop. It's getting slimmer, lighter, with longer lasting batteries, it's getting prettier, but in terms of fulfilling the core functionality of whatever it is, everything is getting worse. Smart TVs? Slimmer, cheaper, more vivid and beautiful, but you get advertising. smart fridge? Does the same job as a normal fridge, now with advertising.  Bah.",Negative
Intel,In an executive meeting today about switching out call center infrastructure to AWS. I brought this up as a reason not to do it. I will be ignored as always.,Neutral
Intel,"I mean even with those outages, the uptime of AWS and all the other cloud providers is still orders of magnitudes higher than self hosted for the vaaaast majority of companies. You need to invest a ton of money and thousands of work hours to come even close to the uptime of modern cloud.  99.99% of companies that self host are not protected against those types of outages either they go down ALL the time, it's just not making the news every time since it's an internal company matter.  Big cloud outages are like plane crashes. If it happens it's big news and there are investigations and legislations is passed. But if you look at the actual numbers, cars are still 1000x less safe and kill literally 10000x more people.",Negative
Intel,"Because, the sad reality, is that at any given time in mankind’s history, the majority of living humans have been morons spouting off whatever drivel one of the smarter ones fed them.",Negative
Intel,"I’m currently on an ambulance and our documentation software is completely functional offline, but with epic down right now the hospitals we take people to are unable to access any patient data not stored locally. We just dropped a patient off and spent a solid 20 minutes waiting for them to figure out how to even get the patient into the system.",Negative
Intel,"i still don't use any cloud services, even better, i don't even use any of those services",Negative
Intel,"Yep. I just endured a lecture from an IT guy at a wedding who mocked me for running colo instead of cloud, and just told me to stop being a fucking idiot and dump colo for the cloud. life will be easier. He  I bet he's having fun this morning.",Negative
Intel,"To be fair I think it greatly depends on the thing. As a software developer who works on a website for a company, there's no real reason for us to host our site locally if it's cheaper and faster to deploy to the cloud. But our software isn't meant to run on end user's hardware, it's just a website, so it's not like it's something you'd be owning in the first place. I think that applications that you would normally run locally should at minimum be *available* for local use, if not exclusively done that way, but for anything that's normally hosted remotely anyways, it makes sense at a high level to concentrate them in data centers and allocate resources as needed rather than having everyone try and purchase and maintain physical servers. The main issue I have with it is that it's controlled by private corporations. A network run by some kind of non profit coalition or something to that effect that sells the computing time essentially at cost and with redundant data centers in every region would save a lot of people time and money on server maintenance, space, and the actual cost of the physical servers without giving control of it to a private entity.",Neutral
Intel,"Oh noooo, I can’t use Microsoft office because I’m not self-hosting it 😭  Guess I’ll just have to dick around for a few hours until it’s fixed. The horrors.",Negative
Intel,I am not defending either but come on. Your own PC has NEVER broken? Software bugs never crashed your workflow or worse? What's with the fantasy that cloud has to work 100% of the time - since when was life perfect? It's a good system and outages do not change the fact.,Negative
Intel,https://preview.redd.it/jtchh1f0y2yf1.jpeg?width=832&format=pjpg&auto=webp&s=414c03b74da58247e0c9ea3f69c91084b852e9fe,Neutral
Intel,![gif](giphy|vT6qlTWOWYzZK),Neutral
Intel,What is that even supposed to mean in the context of this post??,Negative
Intel,![gif](giphy|27npJ7KfvMsoM),Neutral
Intel,Love it when I cant play the game I bought years before Microsoft took over because their dogshit launcher needs to authenticate online even tho I played it 2hrs before online. Apparently it doesnt do token or whatever else other launchers do.,Negative
Intel,"I mean, I'm on Microsoft campus, on Microsoft Internet, even I couldn't open Microsoft.com",Neutral
Intel,You’re self hosting Kroger.com?,Neutral
Intel,Linux users will never miss an opportunity to tell people they use Linux.  Love it.,Positive
Intel,I love to be that guy...,Positive
Intel,You don't hate it at all 😁,Neutral
Intel,I'm that guy with you brother.,Neutral
Intel,How did you learn how to self host? Im trying to figure it out and i cannot understand anything.,Negative
Intel,"Nothing is gonna happen to them, we just take the brunt",Negative
Intel,"It's that, or you accidentally deleted both main and backup server.  [https://www.youtube.com/watch?v=tLdRBsuvVKc](https://www.youtube.com/watch?v=tLdRBsuvVKc)",Neutral
Intel,"A couple of years ago I was working at a large company with multiple data centers in the US serving a global user base. There was a nasty storm that caused a tree to fall on some power lines next to one of the main data centers and it took it down for ~72 hours. But that outage caused a propagating outage cross the rest of the data centers causing the entire service to go down for a couple of hours. So yes, sometimes big issues can be caused by seemingly small things like a single tree.",Negative
Intel,"Yes, I can absolutely believe that data centers can go down. It isn’t even for “no reason”, they always explain why it happened and it’s usually extremely plausible!",Neutral
Intel,Don't attribute to malice what can easily be explained by incompetence.,Negative
Intel,Cyberpunk level shit.,Negative
Intel,"Well that was a rabbit hole, that's mental lol  Thanks.",Positive
Intel,Sole dependency on AWS,Neutral
Intel,"So like, an intranet?",Neutral
Intel,"I fantasize about a large apartment building owned by a pirate group, that have their own local movies, games, knowledge, social network.",Neutral
Intel,Redditor recreates Internet,Neutral
Intel,Azure is down too which is Microsoft,Negative
Intel,"do you pay all these middlemen separately? or do you pay your ISP? if it's the latter, they (the ISP) should figure out where the issue lies and follow up themselves until it's fixed, the rest should be none of your concern. though, I realize reality is often far from that ideal...",Negative
Intel,50tb is insane.,Negative
Intel,"If sailing the high seas perhaps.  Buying current gen physical media (4K Blu Ray), even used, to do backups is a lot more expensive.  Unless somehow you have aceess to free rentals. I know I don't.",Neutral
Intel,I need to come up with a good solution. For Icloud and Google backups from phones. Any ideas?,Positive
Intel,I struggled connecting to a hospital wi-fi for 30 minutes  It was DNS,Neutral
Intel,![gif](giphy|ER63HUlXWQr1UxIgru),Neutral
Intel,Ironically MS is having earnings call today https://www.cnbc.com/2025/10/29/microsoft-hit-with-azure-365-outage-ahead-of-quarterly-earnings.html,Neutral
Intel,I'd bet a few months of mortgage payments it's DNS,Neutral
Intel,Of the AI fucks up DNS is it AI or DNS that's to blame  Edit : o and I are pretty close on the keyboard,Neutral
Intel,What if we can hit 3i/Atlas with a few nukes and get it to land on earth. Bound to be some lucrative rare earth minerals on that. We can soften the landing if we guide it on those soy bean fields.,Neutral
Intel,"Its almost as if everyone else is having the same idea.  If only we were uber-rich and could buy up an island to make a doomsday bunker, for no particular reason.",Negative
Intel,"I bought a bunch of silver about 6 months ago, and it looks like that's getting a bit nuts too",Negative
Intel,Apparently a lot of gold & silver did as well. At least we know it’s at the bottom of a lot of lakes,Neutral
Intel,"I missed an opportunity to buy a bitcoin for $500 way back and said it's probably some scam like other crypto currencies turned out to be, how wrong was I, now it's $111k",Negative
Intel,"Well, let’s move towards open source then.  Fuck em’",Negative
Intel,Even worse is products with no offline mode.,Negative
Intel,"Yes and no. Obviously they want the latter but the benefits are huge in some use cases, particularly corporate laptops where you're much less likely to lose your work as it's constantly backed up, and a broken laptop or laptop upgrade has no effect on your data.",Positive
Intel,Hello NVIDIA with GeforceNOW.,Neutral
Intel,"I think there’s a lot of use cases that you’re right about not needing to be cloud based  and don’t get me wrong having only 3 providers(AWS, azure, ibm) is a serious problem but there’s are plenty of legitimate use cases that cloud should be utilized to various levels.   The industry I work in is the target of a lot of ransom ware attacks. On prem solutions can lead to singular failure points. Having a cloud based system that allows for these places to get around that. Natural disasters is another great example.   Even if your service is on prem/local you should probably still have a consistent backup of all the data. I’ve seen customers that had hurricanes who had a dedicated secondary location still be affected. Even though they did it right with 2 very separated (50 or so miles of separation) data locations they still lost a LOT of data because of the hurricane and subsequent flooding and tornadoes. Not to mention the cost of having to maintain 2 separate dedicated data storage facilities is incredibly cost prohibitive.",Negative
Intel,"Pretty soon the economy will just be rich people swapping stocks and occasionally buying machines that build other machines to sell to other rich people while the rest of us are bartering crops, chickens and labor for clean drinking water.  I mean, we're basically there but there's still a vestigial wedge of the middle class hanging on for dear life that hasn't quite been entirely supplanted by AI yet.",Neutral
Intel,I keep forgetting theres people that have spent years dealing with IOT devices that are just totally oblivious to how unreliable they are.  it's fascinating watching someone get so frustrated by a product that doesn't even need electricity let alone internet.,Negative
Intel,"The electric water filters were probably the most hilarious. At least the bed is still a bed, you can unplug the thing, but why on earth would a water filter need to be electric, let alone connected to the Internet?",Neutral
Intel,>Self hosting office.  Did you mean installed normally?,Neutral
Intel,Might be ok if you work for someone else on their watch with their money.,Neutral
Intel,"Obviously I want my workplace to run cloud bullshit that breaks all the time, the issue is not being able play a game becasue it also 'needs' cloud bullshit.",Negative
Intel,"The difference is if something's wrong on our end, we can fix it.",Neutral
Intel,👆🏻I’m not defending but am defending k guys?👆🏻,Neutral
Intel,When my stuff breaks I fix it. The internet and cloud based systems have more downtime and problems than any machine I own.,Negative
Intel,Oh my gods. What a fucking hint.,Negative
Intel,"When was that changed? On W7 there was ""This computer"" already",Neutral
Intel,OUR computer. Comrade,Neutral
Intel,You’re pirating MyChart??,Neutral
Intel,"a fuckload of things are now cloud based. in other words: the file or content you are using is actually on someone else's computer, and you are dependent on it working. if it doesn't work, tough shit",Negative
Intel,"Folks got all uppity over the name change of My Computer to This PC ^(even though you can name change it)  This PC has more of a 'Who owns this computer?' vibe  The library does."" Also, if you have two or more user accounts on one setup.  This PC is more streamlined.",Neutral
Intel,So much this. Fuck Microsoft.,Negative
Intel,its the same shit with the ubisoft launcher. there was like a week when it didn't let me login in or anything,Negative
Intel,Works for me in Romania.   EDIT: and it's down here too.,Positive
Intel,People go to Kroger.com?,Neutral
Intel,Shit I should ask him for a raise then.,Negative
Intel,He uses Arch btw...,Neutral
Intel,Self host... what? It makes a difference.,Neutral
Intel,[https://wiki.futo.org/index.php/Introduction_to_a_Self_Managed_Life:_a_13_hour_%26_28_minute_presentation_by_FUTO_software](https://wiki.futo.org/index.php/Introduction_to_a_Self_Managed_Life:_a_13_hour_%26_28_minute_presentation_by_FUTO_software),Neutral
Intel,"You buy a small computer like a raspberry pi to start out, and put things on it. I would suggest just using some debian or ubuntu and experimenting. If you completely fuck up and break it just reflash the ISO onto it.",Negative
Intel,Learn about proxmox :),Neutral
Intel,"YouTube videos (technotim, spaceinvader, Ibracorp), unraid (one time license) but simplifies a lot, and an old office desktop (I got a $100 hp elitedesk sff to start) will get you going",Neutral
Intel,"Kinda true, maybe some people will get convinced to switch but yea that's about it i guess. Still funny",Neutral
Intel,"Isnt the whole reason ""bug"" is a term in IT an actual insect that caused issues in one of the early computers?",Negative
Intel,"Right... I've been in the same situation on a much smaller scale, ddos attacks were really popular a few years back and we received and sent some of those to local competitors. Obviously these arent simple ddos attacks, they're more like spy story kind of stuff. They might even have literal agents working undercover inside the competitor and disrupt their operations.",Neutral
Intel,Big tech break up NEEDS to happen.,Neutral
Intel,"Aws and google are not Microsoft, meaning the cause of this outage is not Microsoft.",Negative
Intel,"The point is that it is stupid that when I complain to my ISP, they have infinite other middlemen to shift the blame onto. I cannot understand why we've gone from vertical integration, where a company seeks to purchase it's suppliers, to spinning off entire divisions that only serve one business customer?   It just seems like there is no efficiency here, these companies are all passing the same money down the line and acting like it's profit.",Negative
Intel,I have 40TB and my server is already 80% full lol. The storage gets eaten up quick with a plex server!,Neutral
Intel,245Tb checking in,Neutral
Intel,>50tb is insane  34TB on my server with another 10-15 in my other machines. 50TB is comfortable.,Positive
Intel,"I recently went from 20TB to 40TB, I don't regret a thing.",Negative
Intel,Some libraries will let you check out movies or even games,Neutral
Intel,Dump n pump? 😂,Neutral
Intel,Or some certificate expired because person responsible for it's updates was fired.,Neutral
Intel,Don't Look Up,Negative
Intel,it could be big enough to flatten a city,Neutral
Intel,"For many industrial uses I don't think that's possible. I currently work at a grain elevator that uses software to control just about everything. There are a few different problems that can happen that can quite literally cause the place to blow up if proper safeguards don't work, most of which are controlled by the software. So you need experts in charge of making sure the software functions as it should 100% of the time, which is going to require paying a company anyways.",Negative
Intel,"The systems I work on rely on tens of millions of lines of code that have been carefully maintained since the sixties, good luck re-creating even 0.1% of that.  Yes open source is great, but it's not the right tool for every job. Else most of us would be without a job :p.",Negative
Intel,Tons of open source software on those cloud systems!,Neutral
Intel,many opensource solutions are becoming subscription only and are effectively tied to a cloud as well as they have kill switches in their licensing (they use certificates to enforce functionality) if you don't pay. There's gotchas in everything.,Negative
Intel,Games are the absolute worst about this.,Neutral
Intel,"Oh, you *don't* want to....gamify your water filtration? Just think about it. People gazing at the filtration leaderboard will see ""PogTuber"" right there in the #1 slot and curse your name for having run more water through your filter than anyone else. Tsk.",Negative
Intel,"People are already forgetting that ""apps"" are actually ""software"". Soon OEMs will start shipping laptops and desktops with just enough storage space for the OS and a bunch of documents because everyone has embraced the bloody cloud. MS and other SaaS providers would wet their drawers for that to happen.",Negative
Intel,"Same, that whole 'self-hosted' idea is generally weird and strange to me.  Dude, you're just running it locally on your own computer, as the MACHINE-GOD intended.",Negative
Intel,You’re hosting it for yourself!,Neutral
Intel,"Hey, we can’t all have our own ChatGPT-in-a-wrapper businesses!",Neutral
Intel,"The difference is cost, setting up things on your own is exponentially expensive at scale, and good luck if your experts decide to leave and didn’t document anything.  Pre-cloud systems would still go down, the fault was just isolated to individual services. Now we’ve traded higher resiliency and lower cost for wider fault impact.",Negative
Intel,"Are you 13 my friend? have you ever touched the IT industry? because you would never say something like that with remotely any experience in the field. the average user can't do shit, not at home and not at work. there will still be downtime and you will need an IT to fix it for you, so there will be downtime.     and what will you even run on your end? whatsapp? lol. the cloud is nothing more than the internet itself, from the very beginning and everything running was cloud, barring P2P which is not suitable for any operation on the scale of the things you see in the OP.     everything breaks, cloud or not. you say you will fix your PC, good for you, and so will the cloud as well. mindless yapping from you guys.",Negative
Intel,"There’s a lot of things you can’t fix. ISP going down, power outage, your home burns down, etc.",Negative
Intel,Holy f so thats why by default win11 does not have the mycomputer/thispc shortcut,Neutral
Intel,Getting all the blood test results DLC for free,Neutral
Intel,"Particularly important because companies are trying to ""dark pattern"" you into saving things in the cloud by default. You may not realise that you had even uploaded something (especially if you're non-technical) and now you don't have access to it.",Neutral
Intel,"Right, I understand that Amazon.com isn’t hosted on my computer.",Neutral
Intel,How quick did that milk spoil!? Lol,Neutral
Intel,"my bot does, how else will I clip literally all the coupons",Neutral
Intel,lol I actually do,Neutral
Intel,"I have a bunch of useless old laptops, will those work.",Negative
Intel,You forgot the step where you accidentally scale it up to a full 42U rack of old enterprise servers in a hyperconvergent cluster  All to run plex or jellyfin,Neutral
Intel,Proxmox my beloved,Neutral
Intel,It was a moth in one of either Harvard's or MITs computers. They taped it to the notes and said they found what was causing the error. I think the term existed before but that popularized it.,Neutral
Intel,"This might be anecdotal, there isn't much evidence eitherway.",Negative
Intel,I saw the undercover agents making outages and I saw one of the outages and the outage looked at me,Neutral
Intel,Your config is what mine should have been had Skylake not just come out. Still rocking that 970 but the mobo for the 6700K shat itself so I switched to AMD.,Neutral
Intel,Reckon you may have reading problems - AZURE is DOWN also….,Neutral
Intel,Sounds like Hollywood accounting.,Neutral
Intel,Mine’s almost full again and I just split TV shows to a 2nd server.,Neutral
Intel,"Those high bitrate 4k Blu-ray ""backups"" really blow through disk space.",Neutral
Intel,320TB on mine and still going,Neutral
Intel,Yeah library’s are super underrated for a lot of stuff nowadays haha,Neutral
Intel,And replaced with AI lol,Neutral
Intel,I know it is but guess no one get my /s,Negative
Intel,But think of all the AI chips we will be able to manufacture.,Neutral
Intel,"And for the industries that don't need the safeguards, it's a nightmare to manage more than a dozen employees computer needs.    I'd love to switch to a local server, and replace everyone's computers with Linux. But I wouldn't get anything else done.    Short answer is that it's cheaper this way.",Negative
Intel,"Yep, cant authenticate ownership of completely single player games...",Negative
Intel,"You're right I forgot about the cheevos and the leaderboards, and the season pass.",Neutral
Intel,"Yeeeah that has sort of happened already. People on TikTok were trying to sell some filter by showing TDS in tap water. Their filter ""worked"" by removing TDS. Except it didn't do anything IIRC. I they may have been selling the TDS checker instead. The gamifying was getting your TDS in your water as close to 0 as possible.  I don't see the videos anymore after a bunch of people started dunking on them though.",Negative
Intel,"They already did, that was the whole idea of Chromebooks!",Neutral
Intel,MS tried it with Windows RT.  Notice you don't hear about that version anymore?,Neutral
Intel,"Lol, nobody’s forgotten that, rejecting the natural flow of language towards new vocabulary doesn’t make you any more intelligent, and it’s pretty pathetic to derive any sense of superiority for doing so.",Negative
Intel,Thats just running a program not hosting,Neutral
Intel,https://preview.redd.it/zvnt8kc1y3yf1.png?width=369&format=png&auto=webp&s=fdea4d592fd058983c6012b0abf02b9e58f9a83e,Neutral
Intel,"Hence why the argument is regarding cloud based online software vs software you pay for once and download, completely usable offline.",Neutral
Intel,"Oh, except you can! If your ISP goes down, you can call and bitch. If a power outage happens, you can have an APC for just what's important (or expensive) or a whole-home system... failing that, you can call and bitch. You can buy a stack of 4 HDDs, toss them in an enclosure, tape an RPi to the top of it, and ask a friend to leave it plugged into their network, and that counts as offsite backups.",Negative
Intel,What did the shortcut do?,Neutral
Intel,"that incident was last week.  MS is pushing onedrive hard. if your onedrive doesn't work and none of your files are locally downloaded, congratulations! you're fucked until it's fixed. same goes for adobe etc.  and if you have a personal server on azure, equally fucked",Negative
Intel,"OneDrive is installed on Windows by default. You HAVE to sign into it by default to get access to a desktop to run things. The data it uploads does not actually exist on your PC for long, and you can test this by taking the drive and putting it in another PC and trying to access the data. It will give you one of many strange errors, including one that's along the lines of ""the data provider cannot be reached."" and the files all show 0 bytes on the ""size on disk"" field.  That's also on someone else's computer now, and you probably never even noticed.",Neutral
Intel,In a Microsoft minute.,Neutral
Intel,WE KNOW,Neutral
Intel,"You can use a laptop as a server because at the end of the day, a “server” is just a computer that stays powered on and listens for network requests, even modest CPUs and a bit of RAM can run media servers, game servers, or small web apps just fine. A solid self-hosted home server typically has a reliable CPU (doesn’t need to be high-end), 8–32GB of RAM depending on workloads, stable storage like SSDs plus larger HDDs for media, good cooling for 24/7 uptime, and wired network connectivity for speed and reliability; everything else (GPUs, ECC RAM, RAID) is optional based on what you want to host.",Neutral
Intel,wow you just explained my life story in a single sentence 😂,Positive
Intel,https://americanhistory.si.edu/collections/object/nmah_334663,Neutral
Intel,Damn I havent updated this flair in years lmao.  I gave that machine to a family member however and its still kickin.,Positive
Intel,https://preview.redd.it/9jgn51zs53yf1.jpeg?width=1066&format=pjpg&auto=webp&s=3c311f2ec55cf9a404212c605ca53084f8ba8eba,Neutral
Intel,That is very close to something Sam Altman actually said tbf,Neutral
Intel,![gif](giphy|ZQsZlyZNKVZAs),Neutral
Intel,It has to be hosted somewhere to run!,Neutral
Intel,https://preview.redd.it/1xl3pomcz3yf1.jpeg?width=196&format=pjpg&auto=webp&s=5a00d1c8cf3510110d73d002188b32ecae45becc,Neutral
Intel,Not when your power is out it isn’t!,Negative
Intel,"You know there’s a box to check in the OneDrive settings that actually puts the files onto your computer, right??",Neutral
Intel,"I’m pretty sure that by default, OneDrive keeps your local copy of the Desktop and Documents folders on your PC and merely syncs them to the cloud.",Neutral
Intel,same,Neutral
Intel,"Nice, it's not as good as it used to be but it's still serving me well.",Positive
Intel,"No cap, fr fr, 67.  Bussin, we're cooked?",Neutral
Intel,no it doesn't. are you under the impression a computer needs the internet to function,Negative
Intel,UPS (Uninterruptible Power Supply),Neutral
Intel,[https://www.amazon.com/you-know-these-are-good-for-5-years-or-more-before-the-battery-goes-out/dp/B06VY6FXMM](https://www.amazon.com/you-know-these-are-good-for-5-years-or-more-before-the-battery-goes-out/dp/B06VY6FXMM),Positive
Intel,"oh i know, but people choose to disable it to save local storage. specifically in that case you're fucked",Negative
Intel,"It's more complicated than that, if you're booted off that drive then the encrypted and cached versions are held elsewhere and OneDrive intercepts the calls to open the file (as the ""data provider"" mentioned earlier) and redirects to the cached version. If you're, say, trying to recover data from the drive on another machine, their copy of OneDrive isn't running and if it was you can't exactly do a Linux-style chroot to make it look like their drive is C:, so you have to go dig for the encrypted versions and hope it doesn't take 6 millennia to crack the files open.",Neutral
Intel,Bro. Brah. Bruh. *Braaaahhhhhhhhhhhhhhhhhhh.*,Neutral
Intel,Are you under the impression that “hosting” invariably means “hosting on the internet”?   Because it doesn’t.,Neutral
Intel,"…will allow you to shutdown gracefully, but unless you invested several thousand dollars for one intended to a server, you aren’t going to get much work done!",Neutral
Intel,"APC has a nice little calculator on their site where you can plug your gear’s power draws in and see exactly how long it will last.  Although admittedly, it may be down at the moment lol",Neutral
Intel,That’s why you have backups.,Neutral
Intel,Wazzzzzuuupp?!?,Neutral
Intel,hosting would require a client or user. You aren't hosting a party if you're the only person invited.,Neutral
Intel,"I have an 800W desktop, and if I'm not at 100% usage of everything in my system, my $200 APC lasts 90 minutes. That's not a ""graceful shutdown"", that's a decent chunk of time.",Neutral
Intel,"You can also just ask the device, either from a PC using a control cable, or on the front panel. [IMG-20251029-122804643.jpg](https://postimg.cc/tnTX0QjW)",Neutral
Intel,"The whole point you replied to was ""people don't even know this is happening"" and 98% of users do not know backups are a thing, or even what files are, they just click the magic buttons and Their Stuff Is Just There Listed In The App",Negative
Intel,We just established you are the user of the software you host locally,Neutral
Intel,Then why are you talking about putting the drive into another computer to recover files?? 98% of people don’t do that either!,Negative
Intel,battlefield 6 likes cpu a lot at 1080p and 1440p you could just get a 7800x3d and upgrade gpu later since cpu is way cheaper,Neutral
Intel,"What is your motherboard and CPU? The other SSDs you have, how many are SATA 2.5in? or many are NVMe?",Neutral
Intel,"CPU is a Intel I5 14400f,the motherboard is a gigabyte b750 with 3 M2 slots and 4 sata ports The ssds are kingston with about 450gb and i currently use the third slot for the M2 since i thought it may be a port issue because the same was happening with the first, the speeds aren't affected since they don't even go above 100mb/s",Neutral
Intel,"Do you have the exact model of the motherboard? and the models of the SSDs you have? and how are they connected to the motherboard?  B760 has a maximum of 14 PCIe lanes (up to), 10x are 4.0, 4x are 3.0. So you have to check how the motherboard is using those lanes, and if you are connecting things on slots that are sharing bandwidth with other stuff.",Neutral
Intel,"The normal SSDs are connected by sata cables,but i don't have a problem with them,my problem is the M2 drive because even though it is slower than my HDD most of the time it also corrupts ,any specific details about the motherboard or any other drives besides the M2 is a pain to get",Negative
Intel,"I would probably guess it's a bad SSD.  I just took a look at some entry-level B760 motherboard, it doesn't seem to have too much lane sharing, so the SSD should run at x4 (3500MB/s max) even on the bottom slot, or at least x2 if the motherboard if very entry-level.",Neutral
Intel,Damn,Negative
Intel,"You need to share your PSU model as well, you can buy used 3080(preferably 12GB version), 3080Ti, 4070, 4070Super those ahould be all around same budget and close in performance to each other.  Your CPU would limit those GPUs too but by this point its tricky to get a good upgrade for AM4 motherboards, see if you can find used 5800x3D, 5700x3D, 5600x3D, since those x3D cpus as disconnected their new prices are higher than its worth in terms of value, if you cant find them then only next logical step is upgrading motherboard to AM5 which means new DDR5 RAM and 7600x CPU, and realistically it would be easier to sell your pc as a whole then buy new one, since trying to sell it by parts would be a lot harder.",Neutral
Intel,Just be aware that the cooler might be too big.  The MoBo also isn't great. Try to get the HDV version instead. Well worth the 20€.,Negative
Intel,Ok thanks. Is there any way I can know if the cooler will fit?,Positive
Intel,"Buy it, test it, and send it back if it doesn't. It's like 155mm to 165mm. Usually, it will fit, but occasionally the measurements of the case are slightlly off due to the pins on top of the cooler or so and it ends up touching the sidepanel.  Sidenote: Consider getting the Phantom Spirit instead - Assassin Spirit is pretty overpriced in Europe for what it does(used to be 25 USD MSRP). Or the ID-COOLING FROZN A620 PRO, which is around as good as the Phantom Spirit but cheaper. All of them are pretty much same height btw.",Neutral
Intel,1. Why the HDV mobo? 2. I was considering getting the EVO DARK (21€) but I wanted the outer led ring.,Neutral
Intel,"HDV is just overall better. Full 2nd PCIe slot, more USB, better performance. The H version is the absolute minimum quality.  Well, you can probably find an LED version for those others, too. They are just a tier above the assassin spirit in terms of cooling performance, which can be relevant for future upgrades and noise levels.",Positive
Intel,Yeah but its atx,Neutral
Intel,Not sure what you mean. The HDV is also mATX. The H board is just the lower quality version of the HDV.,Neutral
Intel,Ah. I dont feel like i need more anyways,Negative
Intel,"I mean, you do you. But if you plan to keep this system for a couple years, the 15€ more now for a better MoBo might save you 100€ for a new MoBo later and cost you maybe 2-3€ per year of usage. If I had to choose, I'd rather get a cheaper SSD than a low-quality MoBo (especially since latter can make your SSD worse anyways).",Neutral
Intel,Please just explain why,Neutral
Intel,"I'd swap the motherboard for something non-Asrock, since Asrock boards have been known to blow up AM5 CPUs.  You also might be able to get better RAM (DDR5-6000 CL30) for cheaper.  Where are you getting a 7600X3D? I had thought those were Micro Center only.  I think that this CPU + GPU combo is about right for the games you suggested as they are all CPU-intensive and typically benefit from 3D v-cache.",Neutral
Intel,You’ll be fine lmao,Positive
Intel,"Actually you might have some issues, the Intel GPU didn't test as well on older platform. Hopefully they've improved this somehow since these review tests. https://www.techspot.com/news/106212-intel-arc-b580-massively-underperforms-when-paired-older.html",Negative
Intel,ID on keyboard??,Neutral
Intel,"Embark seems to be the only developer who can make Unreal Engine 5 games run well. The Finals ran pretty decent as well.  Just for fun, I used an old Radeon R9 390 during the Server Slam over the weekend. It's a 10 year old GPU - two generations older than the RX580 (RX580 is listed as the minimum for this game). I was running Linux, since the R9 390 does not get driver updates from AMD on Windows anymore. All settings on Low with FSR set to ultra performance resulted in a somewhat playable 50-60fps. I didn't get into any serious PvP though, I'm sure it would suffer with more going on.",Positive
Intel,https://kbdcraft.store/products/adam?gad_source=1&gad_campaignid=22681391684&gbraid=0AAAAApVLaXZyKxMNHYyIjgob2-yUQ4iDr&gclid=EAIaIQobChMIhrGhirexkAMV8BitBh1u0BovEAAYASAAEgJ8CPD_BwE,Neutral
Intel,I love pushing old hardware like that.   I had pretty good luck with Silent Hill F on the 580 as well.  Any other modern titles you know that run well on older hardware?,Positive
Intel,You need an electrician. You know that. Don’t burn your house or apartment down. There’s something wrong with that circuit as none of those devices are capable (even all together) of maxing out that circuit under normal conditions without frying themselves too. A PSU that was pulling that much would’ve already died by now.,Negative
Intel,yeah. maybe i just needed someone to talk some sense into me. haha thanks,Positive
Intel,I found my own solution. XMP was set to aggressive instead of default and was pushing the RAM too hard. Set it to default and left PBO on and it's fine now.,Neutral
Intel,"What a disgusting build, I love it",Positive
Intel,the content we crave,Neutral
Intel,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",Neutral
Intel,What GPU are you using in your build?  All of them,Neutral
Intel,you're one hell of a doctor. mad setup!,Positive
Intel,The amount of blaspheming on display is worthy of praise.,Neutral
Intel,Brother collecting them like infinity stones lmao,Neutral
Intel,I'm sure those GPUs fight each others at night,Neutral
Intel,Bro unlocked the forbidden RGB gpus combo,Neutral
Intel,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,Neutral
Intel,What the fuck,Negative
Intel,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,Positive
Intel,Yuck,Neutral
Intel,Wait until you discover lossless scaling,Neutral
Intel,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,Neutral
Intel,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",Negative
Intel,Now you just need to buy one of those ARM workstations to get the quad setup,Neutral
Intel,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,Positive
Intel,Love it lol. How do the fucking drivers work? Haha,Negative
Intel,What an amazing build,Positive
Intel,wtf is that build man xdd bro collected all the infinity stones of gpu world.,Negative
Intel,You’re a psychopath. I love it,Positive
Intel,This gpu looks clean asf😭,Positive
Intel,The only setup where RGB gives more performance. :D,Neutral
Intel,Now you need a dual cpu mobo.,Neutral
Intel,Placona! I've been happy with a 6700xt for years.,Positive
Intel,absolute cinema,Neutral
Intel,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",Neutral
Intel,"Brawndo has electrolytes, that's what plants crave!",Neutral
Intel,"You have always been able to do something like this! Though cross manufacturer has not had ""benefits"" until Vulkan in some spaces for games, now lossless scaling, but for anything requiring VRAM or software level rendering, it's been there as an option.   Unironically, ""SLI and crossfire"" I'd argue, is back, but not in the traditional sense.  I did this with a 6950XT and a 2080ti, simply because I wasn't able to jump on the 3090/4090 train in time tho.",Positive
Intel,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",Neutral
Intel,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",Neutral
Intel,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",Neutral
Intel,Team RGB,Neutral
Intel,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",Neutral
Intel,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here: https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",Negative
Intel,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",Neutral
Intel,"OpenCL works on all of them at once, and is just as fast as CUDA!",Positive
Intel,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",Neutral
Intel,Start here with FluidX3D: https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md 🖖,Neutral
Intel,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",Neutral
Intel,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",Neutral
Intel,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (I’m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),Negative
Intel,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,Neutral
Intel,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,Neutral
Intel,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",Negative
Intel,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",Neutral
Intel,Thank you so much for the very detailed response!,Positive
Intel,Well worth it!,Positive
Intel,Thank you my man!! Looking forward to run some tests once I get home.,Positive
Intel,That's awesome!,Neutral
Intel,"Yes, but SLI is a bad description for it.",Negative
Intel,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",Neutral
Intel,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",Negative
Intel,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",Neutral
Intel,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",Neutral
Intel,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",Neutral
Intel,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",Neutral
Intel,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",Neutral
Intel,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",Neutral
Intel,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  ​​​  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",Positive
Intel,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",Neutral
Intel,Why are you connecting the monitor to the gpu and not the mobo?,Neutral
Intel,"👍   thanks for the info, this'll definitely come in handy eventually.",Positive
Intel,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,Neutral
Intel,No worries mate. Good luck,Positive
Intel,"For some reason I switched up, connecting to the gpu is the way to go. I derped",Neutral
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,It's alive. Rejoice.,Neutral
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",Neutral
Intel,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",Negative
Intel,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,Neutral
Intel,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,Neutral
Intel,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,Positive
Intel,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,Positive
Intel,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",Neutral
Intel,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",Negative
Intel,I'm fairly sure they use dxvk for d3d9 to 11.,Neutral
Intel,Could just be a cache issue,Neutral
Intel,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,Neutral
Intel,Glad you brought up Nvidia as I didn’t know this had improved until the testing around Arc showed it had gone.,Positive
Intel,"According to the graphs, AMD has slightly less overhead than NVIDIA.",Neutral
Intel,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",Negative
Intel,"Lowest with DX11 and older, but not with the newer APIs",Neutral
Intel,And when is the last time HUB did a dedicated video showing the improvement in overhead?,Neutral
Intel,or it's just a cache/memory access issue,Neutral
Intel,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",Neutral
Intel,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",Negative
Intel,"Intel uses software translation for DX11 and lower, so it does matter for them.",Neutral
Intel,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",Neutral
Intel,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",Negative
Intel,That's not true. Intel's issue is being too verbose in commands/calls.,Negative
Intel,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",Negative
Intel,HUB used DX12 games that also showed the issue.  It's something else.,Neutral
Intel,"The comment to which I am replying is talking about nVidia, not Intel.",Neutral
Intel,"I’m pretty sure HUB doesn’t like Nvidia *or* AMD. They’re calling it how it is, these parts are too damn expensive.",Negative
Intel,That's actually... just worse news.,Negative
Intel,I always dreamt of the day APUs become power houses.,Neutral
Intel,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",Neutral
Intel,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",Negative
Intel,Damn Why is AMD even involved in iGPU,Negative
Intel,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",Positive
Intel,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",Neutral
Intel,almost there,Neutral
Intel,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",Positive
Intel,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",Neutral
Intel,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,Negative
Intel,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",Neutral
Intel,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,Negative
Intel,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",Negative
Intel,"It depends on what your goals are for a laptop.  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!   I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",Negative
Intel,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",Neutral
Intel,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",Negative
Intel,yes its so bad. better go buy some steam deck or ally x,Negative
Intel,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,Neutral
Intel,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",Neutral
Intel,How are they going to feed all those CUs? Quad-channel LPDDR5X?,Neutral
Intel,That's considerably faster than an XSX.,Positive
Intel,>That's tapping on 4070/7800 levels of performance.  What is?,Neutral
Intel,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",Neutral
Intel,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",Positive
Intel,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,Neutral
Intel,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,Neutral
Intel,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",Neutral
Intel,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",Negative
Intel,It's called satire. You're just salty because you're the butt of the joke.,Negative
Intel,throw it in the next steamdeck and I’ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,Neutral
Intel,Praying the blade16 gets it.,Neutral
Intel,"This is the rumor, if you’re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",Neutral
Intel,256 bit bus + infinity cache.,Neutral
Intel,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,Neutral
Intel,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",Positive
Intel,"Yes, it’s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",Neutral
Intel,The rumored 40CU strix halo chip. Not the actual chips released this week.,Neutral
Intel,7500mhz ram and the 780m,Neutral
Intel,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",Neutral
Intel,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",Neutral
Intel,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",Neutral
Intel,Literally where did you see 40-60% uplift at half the power?,Neutral
Intel,> 40-60% performance uplift at half the power  Source?,Neutral
Intel,"i chuckled, then again im not a fanboy of anything",Neutral
Intel,Dont expect 40CUs in a handheld anytime soon,Neutral
Intel,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",Negative
Intel,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.  The system should have an actual TRUE quadcore memory setup.  Many of these systems have currently (and will absolutely continue to have) dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.  This matches my system that runs a 780m with 7500mhz lpddr5x.  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.  This will continue to hamstring these APUs regardless of how many cu they throw at em.",Neutral
Intel,"“Absolute monster”? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.    AMD has a long way to go before claiming “Monster” class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet aren’t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Max… let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",Neutral
Intel,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,Positive
Intel,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. That’s far from desktop 4070, never mind 4080.",Neutral
Intel,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,Neutral
Intel,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,Neutral
Intel,">AMD has a long way to go before claiming “Monster” class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",Neutral
Intel,"APU is one of AMD’s biggest markets. You are kidding if you think they don’t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they don’t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games aren’t the only thing APUs are used for so PS5 isn’t wholey in the conversation. PS5 also costs monthly to play online and their games aren’t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",Negative
Intel,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",Negative
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",Neutral
Intel,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,Positive
Intel,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",Negative
Intel,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",Negative
Intel,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,Neutral
Intel,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",Neutral
Intel,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",Neutral
Intel,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",Negative
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,Positive
Intel,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,Positive
Intel,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",Positive
Intel,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",Neutral
Intel,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",Negative
Intel,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,Neutral
Intel,"Installs beta software, proceeds to complain about it",Neutral
Intel,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,Negative
Intel,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",Neutral
Intel,What Ghost of Tsushima issue?,Neutral
Intel,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",Neutral
Intel,"This was a graphics card, not a ‘GPU’ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",Neutral
Intel,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",Negative
Intel,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,Negative
Intel,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",Neutral
Intel,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",Neutral
Intel,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,Neutral
Intel,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",Neutral
Intel,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,Negative
Intel,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,Negative
Intel,The documentation for it would still be in their archives,Neutral
Intel,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",Neutral
Intel,"Pretty annoying how everything follows the same linear fps/price curve, there’s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",Neutral
Intel,Wish Arc cards were better. They look so pretty in comparison to their peers,Positive
Intel,Thats actually a pretty solid and accurate breakdown.,Positive
Intel,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,Neutral
Intel,3080 still looking good too,Positive
Intel,What they have peaceful then 4k series?,Neutral
Intel,Just get a 4090. I will never regret getting mine.,Neutral
Intel,i miss old good times where radeon HD 7970 as best single core card cost around 400$,Neutral
Intel,"Damn, the A770 is still so uncompetitive...",Negative
Intel,"It's like the free market priced cards according to their relative performance. How weird, right?",Negative
Intel,How is that possibly annoying,Neutral
Intel,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,Positive
Intel,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",Positive
Intel,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",Negative
Intel,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,Positive
Intel,8gb perfectly fine today :),Positive
Intel,"Ah yes sure, now where did I leave my 1500 euros?",Neutral
Intel,"I don’t mind free markets, I’m just saying the state of the market is less fun now than it used to be.",Negative
Intel,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,Positive
Intel,"Yeah, i like the black super series.",Neutral
Intel,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",Negative
Intel,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",Neutral
Intel,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,Neutral
Intel,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",Negative
Intel,"""The ability to “turn it off and on again” should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",Neutral
Intel,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",Negative
Intel,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",Positive
Intel,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",Negative
Intel,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",Negative
Intel,"Yo, I saw the title and thought this gotta be Gnif2.",Neutral
Intel,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",Negative
Intel,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",Negative
Intel,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",Negative
Intel,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",Positive
Intel,"I’ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, it’s usually a day late and a dollar short. Blender on Linux still can’t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that there’s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but we’ll have to wait and see.",Neutral
Intel,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*™ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",Negative
Intel,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",Negative
Intel,Long but worth it read; Well Done!,Positive
Intel,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",Neutral
Intel,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,Negative
Intel,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",Neutral
Intel,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,Negative
Intel,I’ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,Negative
Intel,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",Negative
Intel,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,Negative
Intel,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",Negative
Intel,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",Negative
Intel,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100ºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",Negative
Intel,Lmao as a recent AMD intern I feel this in my bones. I still can’t fathom just how little effort is put into software stability these days.,Negative
Intel,100% all of this...  Love looking glass by the by,Positive
Intel,How does say VMware handle this? Does it kind of just restart shit as needed?,Neutral
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",Neutral
Intel,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",Positive
Intel,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",Negative
Intel,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,Negative
Intel,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",Negative
Intel,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",Neutral
Intel,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",Neutral
Intel,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",Negative
Intel,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",Negative
Intel,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,Neutral
Intel,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",Neutral
Intel,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",Negative
Intel,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",Negative
Intel,TL;DR. **PEBKAC**.,Neutral
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",Negative
Intel,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,Negative
Intel,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",Negative
Intel,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  😤😭,Negative
Intel,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",Negative
Intel,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,Negative
Intel,"Thanks mate I appreciate it, glad to see you here :)",Positive
Intel,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",Positive
Intel,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",Negative
Intel,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",Neutral
Intel,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",Negative
Intel,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,Positive
Intel,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,Positive
Intel,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",Neutral
Intel,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,Neutral
Intel,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",Negative
Intel,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",Neutral
Intel,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",Negative
Intel,"Excellent post, very informative. Would take issue with this though:       ""Speaking of VRAM, The drivers use VRAM less efficiently. Look at any side-by-side comparison between games on YouTube between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",Neutral
Intel,"Funny, I saw the title and thought the same too!",Neutral
Intel,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",Negative
Intel,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",Negative
Intel,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,Neutral
Intel,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",Neutral
Intel,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",Negative
Intel,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",Negative
Intel,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",Negative
Intel,ursohot !  back to discord rants...,Neutral
Intel,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,Positive
Intel,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",Negative
Intel,"I dunno man. I’ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I haven’t used anything by team green lately, so it may all be the same shit , different pile.",Negative
Intel,Lol same with me tbh I haven't had any problems 😂 but I guess some do idk 🤷. I have crashed less with AMD than my old  Nvidia card.,Positive
Intel,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",Negative
Intel,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me £540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent £200 more on a second hand 3090 instead of this.",Negative
Intel,"It doesn't handle it, it has the same issue.",Neutral
Intel,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),Neutral
Intel,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",Neutral
Intel,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",Negative
Intel,Me neither. I use a RX580 8GB since launch and not a single problem.,Neutral
Intel,Because they're talking absolute rubbish that's why.,Negative
Intel,You are one of the lucky ones!,Neutral
Intel,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",Negative
Intel,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",Negative
Intel,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",Positive
Intel,lol your flair is Please search before asking,Neutral
Intel,"Hey OP — Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,Negative
Intel,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,Negative
Intel,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",Negative
Intel,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",Negative
Intel,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,Neutral
Intel,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,Neutral
Intel,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol 🤣🤣🤣🤣   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",Negative
Intel,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",Neutral
Intel,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",Positive
Intel,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",Negative
Intel,"""NVIDIA, it just works""",Positive
Intel,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,Negative
Intel,What is the AMD Vanguard?,Neutral
Intel,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",Negative
Intel,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,Negative
Intel,You misspelled $2.3T market cap....,Neutral
Intel,"Okay yeah fair enough, hadn't considered this. Removed it from my post",Neutral
Intel,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",Neutral
Intel,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",Negative
Intel,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",Neutral
Intel,This is not a fix. It's a compromise.,Negative
Intel,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",Neutral
Intel,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,Negative
Intel,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",Negative
Intel,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,Neutral
Intel,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,Negative
Intel,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",Negative
Intel,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",Negative
Intel,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",Negative
Intel,The comment I quoted was talking about people playing games having issues.,Negative
Intel,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,Neutral
Intel,The thing I quoted was talking about people playing games though.,Neutral
Intel,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",Negative
Intel,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",Negative
Intel,"Idk, I don't use Linux",Neutral
Intel,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",Neutral
Intel,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",Neutral
Intel,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),Neutral
Intel,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",Neutral
Intel,Because adding a feature for a product literally gives users more control for that product.,Neutral
Intel,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,Neutral
Intel,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",Negative
Intel,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,Neutral
Intel,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",Neutral
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",Negative
Intel,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,Negative
Intel,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",Neutral
Intel,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",Neutral
Intel,*wayland users have joined the chat,Neutral
Intel,You're falling for slogans.,Neutral
Intel,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",Positive
Intel,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,Neutral
Intel,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),Neutral
Intel,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",Neutral
Intel,Honestly after a trillion I kinda stop counting 😂🤣,Neutral
Intel,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",Neutral
Intel,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",Negative
Intel,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",Negative
Intel,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",Negative
Intel,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,Negative
Intel,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",Neutral
Intel,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",Neutral
Intel,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",Negative
Intel,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",Negative
Intel,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",Neutral
Intel,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",Negative
Intel,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",Neutral
Intel,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",Neutral
Intel,Oh then just ignore my comment 😅,Neutral
Intel,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",Positive
Intel,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,Negative
Intel,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",Neutral
Intel,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",Negative
Intel,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",Neutral
Intel,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,Negative
Intel,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",Negative
Intel,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,Negative
Intel,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,Negative
Intel,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",Negative
Intel,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",Neutral
Intel,"Too soon to tell, but hopes are high.",Neutral
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",Negative
Intel,"Agreed, they cannot rest on their laurels.",Neutral
Intel,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",Neutral
Intel,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",Neutral
Intel,console gamers know pc’s are better and don’t really complain about upscaling and 30fps.. you’re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldn’t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,Neutral
Intel,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,Neutral
Intel,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",Negative
Intel,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned off   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",Negative
Intel,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",Negative
Intel,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",Neutral
Intel,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",Neutral
Intel,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,Negative
Intel,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,Negative
Intel,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,Negative
Intel,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",Negative
Intel,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",Negative
Intel,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,Negative
Intel,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",Negative
Intel,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,Neutral
Intel,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete. And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",Neutral
Intel,When did they do this switch? I remember years ago when I configured that their windows drivers weren’t being so nice to the card detected in a VM.,Negative
Intel,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",Neutral
Intel,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",Negative
Intel,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",Negative
Intel,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",Neutral
Intel,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,Neutral
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",Negative
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",Negative
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",Negative
Intel,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",Neutral
Intel,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",Negative
Intel,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",Negative
Intel,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",Negative
Intel,Oh and XE also have bug feature reporting.  Omfg!!!!,Neutral
Intel,Nobody is 100% right ;),Neutral
Intel,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),Neutral
Intel,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",Negative
Intel,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",Negative
Intel,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",Neutral
Intel,What about using a DP to HDMI 2.1 adapter for that situation?,Neutral
Intel,"2021 my guy, it's right there on the date of the article.",Neutral
Intel,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,Neutral
Intel,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,Neutral
Intel,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",Neutral
Intel,And I guess infallible game developers too then. /s,Neutral
Intel,So you decide what criticism is valid and what not? lol,Neutral
Intel,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,Negative
Intel,"Yup, but do you see them making a big press release about it?",Neutral
Intel,that is not how it works but sure,Negative
Intel,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,Neutral
Intel,>whine about Redditors.  The irony.,Neutral
Intel,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",Neutral
Intel,learn to comprehend.,Neutral
Intel,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,Neutral
Intel,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",Negative
Intel,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",Negative
Intel,"No, that would be you obviously /s",Negative
Intel,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,Neutral
Intel,"Yea, given the state of XE drivers every major update has come with significant PR.",Neutral
Intel,Why not ;),Neutral
Intel,Go word salad elsewhere.,Neutral
Intel,"I have replicated the issue reliably yes, and across two different systems.",Neutral
Intel,If discord crashes my drivers.. once every few hours. I have to reboot,Negative
Intel,Discord doesn't crash my drivers  I don't have to reboot.,Negative
Intel,Really love how the 6000 series radeons look.,Positive
Intel,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",Negative
Intel,That's a good looking line up,Positive
Intel,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",Negative
Intel,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",Positive
Intel,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",Positive
Intel,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,Positive
Intel,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",Negative
Intel,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,Negative
Intel,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",Negative
Intel,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",Negative
Intel,That's suprising. Generally the 70 series and AMD equivalent cards are more popular in Germany. Is 4060ti discounted heavily?,Neutral
Intel,"Always happy to see ARC succeeding. We need a third competitor, and they've got what it takes.",Positive
Intel,Im more confused about how do you guys still buy RX 6xxx series. Only RX 66xx series that are still widely available in my country. RX 67xx and above is just OOS.,Neutral
Intel,That 7900xtx sale number is insane,Negative
Intel,That just shows that most people that buy GPU's don't know a thing about them.,Negative
Intel,"4060 Ti is not discounted well at all here.   390€ and 460€ for 8/16GB versions, really bad deal which is why I am absolutely baffled to see it this high in the list. Might be christmas shoppers who just buy the highest nvidia card within their budget without doing any research - that's my best guess actually.",Negative
Intel,best discounts were 6750xt 6800 and 7800xt,Positive
Intel,4060 ti is selling as a surrogate Quadro for AI/ML. $400 for 16gb is not awful in that context and you get full CUDA support,Neutral
Intel,"Intel is putting in the effort on the software front as well. Arc cards are great value, especially with the improvements intel has made with driver updates. With talk about XeSS being integrated into DirectX with Windows 12 (among other upscalers as well) I hope that intel GPUs will be even more competitive.",Positive
Intel,"Battlemage needs to be stable and competitively priced.   First impression matters so much these days, it cannot be overstated imo.",Neutral
Intel,"They're not out of stock there, duh",Neutral
Intel,Yeah the Germans love the 7900xtx in particular for some reason. It was really high up considering its price in last week's summary as well. I guess they don't appreciate the 4090's price.,Positive
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"I have so many (adult) friends who always bought NVIDIA, will always buy NVIDIA and don’t do a single bit of research.   Had one friend even ask me, I recommended the 7900Xt for her budget and she got the 4070 Ti because she didn’t trust my word or AMD  It’s such a big habit issue. Shopping season really showed me that nvidia could literally sell a brick and habit shoppers would buy it",Negative
Intel,">390€ and 460€ for 8/16GB  That equals to $350 and $415 before taxes, a lot lower than in USA ($390 and $450 before taxes)",Neutral
Intel,"They are pretty much gone in Germany as well:  6950XT - only XFX for 599 at Mindfactory   6900XT - gone   6800XT - only XFX for 499 at Mindfactory and Asrock for 580 at various   6800 - only XFX for 435-440 at Mindfactory and some   6750XT - a bunch of models for 373-405 at various   6700XT - a bunch of models for 344-369 at various   6700 - XFX and Sapphire for 298 and 322 from one retailer   6650XT - gone   6600XT - gone     So the high-end is gone except that XFX stock that will probably be gone in the next 2-3 weeks as well, the lower end is gone. Only 6750XT and 6700XT is still there a bit more and of course the 6600 because that has no replacement at it's current 200-220 price, the 7600 is way more expensive at 270-320.",Neutral
Intel,"I know, but i mean, both 6750 XT and 6700 XT still has more than 300 stocks each in single week (if i read the stat correctly). That's a lot of GPU. I thought the stock is already thin globally which in turn make stock in my place nonexist.",Neutral
Intel,"I thought I saw some 8GB models as low as $330 and 16GB models at $400 but even then I would much rather get another GPU like a 6700XT  (you can still get at here and we even had one down at 299€ for a while, think they are back at 330€ tho)",Neutral
Intel,the card is pretty bad if you missed that somehow,Negative
Intel,AMD probably ships leftover to countries in which they know it will sell,Neutral
Intel,"Sure, but I was replying to you saying ""4060 Ti is not discounted **well at all** here""",Neutral
Intel,"Because most people are not willing to pay that much for GPUs that weak. Sure ""discounted well"" is subjective anyways, I got your point that they are below MSRP but I am pretty sure you know what was implied when reading my comment.",Negative
Intel,"As you notice the photoshop version differs, so you can't compare them really",Neutral
Intel,I also another insanely high score:     7900X + RTX4090 [Photoshop score 8995](https://benchmarks.pugetsystems.com/benchmarks/view.php?id=168538),Neutral
Intel,So basically PC games are never going to tell us what the specs are to run the game native ever again.,Negative
Intel,"Well, folks, the game is basically software ray tracing an open world regardless of settings at all the times using compute shaders from what I gather. I mean, what'd you expect from something pushing consoles to the brink? That it would not be equally demanding on PC?",Neutral
Intel,The true crime here is needing FSR to reach these requirements.,Neutral
Intel,"Oh wow, this is probably the first time I'm seeing dual channel listed in the RAM section of specs requirements.",Positive
Intel,im more baffled as to why they are comparing a RX 5700 to a GTX 1070. Id have thought they would have said a RX 590 (instead of 5700) or RTX 2060 (instead of 1070),Neutral
Intel,> Enthusiast   > 1440p [...] with FSR Quality [...] 60 FPS  1440p + xxxx Quality upscaling is just another way to say 1080p you cowards. It's 2023 and enthusiast level is 1080p @ 60 FPS.   Looks pretty though with the amount of vegetation.  I'm surprised that they put the RTX 3080 and RX 6800 XT side by side in a game that has forced ray tracing. I guess that's the advantage of being cross platform and AMD sponsored.,Neutral
Intel,"well, at least the chart is easy to read, not a complete mess",Neutral
Intel,This gives me hope that Star Wars outlaws will also have FSR 3. 🤌,Positive
Intel,60fps with balanced upscaling on a 4080/7900xtx … That’s not a good sign,Negative
Intel,It's joever. We are officially at the point where devs are using upscaling as the standard to reach playable frame rates instead of optimizing their games.,Negative
Intel,"If you look in the top right, these seem to be based on ray tracing being enabled? If that is the case, these specs actually seem reasonable.",Neutral
Intel,"FSR and DLSS were supposed to be optional, now they seem to be mandatory in any graphics setup.",Neutral
Intel,Yay finally a game that'll have both fsr 3 and dlss 3 so it'll work perfectly on either gpu brand at launch.,Positive
Intel,"Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. The requirements should all be done without any upscaling or frame generation tech to be upfront and honest about how the game will run. Upscaling will then just be an available option for users to further increase performance.",Negative
Intel,"Actually not to bad for current standards, 1440p 60 with fsr quality on a 6800xt/3080 is great if high includes RT, if it doesn't then it sucks",Negative
Intel,"So with my 6800xt, which should be able to do 1440p high settings at over 60fps easily, I'll have to use FSR just to get 60fps? Great, another unoptimized game headed our way.  So sick of the awful PC ports and such. 6800xt is still a very powerful card yet I'm seeing more and more games require it for 1440p 60fps high or even 1080p 60 fps high. Ridiculous.  The reliance on upscaling tech instead of properly optimizing the game is becoming a problem. And it seems this game has forced RT which is idiotic. RT destroys performance and should always be an option.",Negative
Intel,"If you notice, all the mentioned AMD GPUs are significantly better than Nvidia GPUs, so does that mean bad performance for AMD GPUs?",Neutral
Intel,So i bought an 1200€ card (rx 7900xtx in my region and at the time of launch) and can now enjoy new games at 60 fps with BALANCED fsr upscaling. We love to see it. /s,Positive
Intel,so I'm doomed to use FSR even at 1080p ? what a shame pc gaming has become.,Negative
Intel,"I honestly don't get this upscaling outrage. It's not a fixed requirement. If you want to play native, just turn off upscaling but you will have to turn down settings to compensate. Everyone wants to have their cake and eat it too. People want super low requirements, super high graphics with high FPS.  The thing is game devs/publishers want to highlight all the ""eye candy"" in games to show their advancement and that's how they want to advertise it. They have a certain level of visuals they want to present for the game. So they crank up the baseline and focus on making it look as good as they possibly can and by using upscalers to get to the playable FPS.  Other option is they ditch making games look any better and just keep it restrained to good FPS at native. But then you'll have the other subset of complainers that will complain about the game's graphics looking like X many years ago.  Take Alan Wake 2 for example, The medium setting looks better that some High-Ultra setting games from a few years ago. Running a AAA title 2-3 years ago at 'medium' preset may not equate to running a current AAA title at medium preset.  The games are getting too demanding because they are starting to push the limits visually (partially because on AAA titles, that's something they really focus on for advertising), if your card can run it, great, if not, adjust your settings to match your cards capabilities. If you bought a 6700XT 2+ years ago to play native 1440P on high setting, guess what, it's not always going to play 1440P at high settings as the years go by. You have to adjust your expectations of the card's capabilities as time goes on. Medium preset now is likely better than High preset then. If you want the ""same"" performance you used to get, match the visuals and see how it does.  Seems like the only way to get people to stop complaining is to just halt all visual/graphical progress and stick to limiting the game's visuals so they run well on mid/low tier cards.  People keep throwing out ""OPTIMIZATION"" but on one knows what/how they want that done. WHAT exactly do you want optimized? How do we know the game isn't optimized to the graphics/performance it gives?  What if they just made the ""High Quality"" preset look like games from 2018, would that now be considered ""Optimized""?  (I'm not talking about this game in particular, just in general, it was same outrage w/ AW2 and that turned out better than expected)",Neutral
Intel,I hope they will bundle this game with CPUs/GPUs,Neutral
Intel,holy hell so for 1080p you need a 1440p card because not even a RX 7600 is on par with a 3060ti or a 6700XT,Negative
Intel,Recently upgraded GPU and CPU and crazy that newer games are already requiring the newest GPUs to play them. Personally I don't think it's a horrible thing but at the same time it just screams horrible optimization.,Negative
Intel,This game better look like real life with those specs. I does looks beautiful!,Positive
Intel,"Another garbage with forced upscaling at baseline. When upscaling winded up - everyone was ""Hell yeah, high refresh rate AAA gaming""  Fast forward to today: ""FSR performance just to hit 1080p 60fps - yay..""   I fucking knew it.. it was so obvious upscaling will turn into this and replace game optimizations. Why optimizing shit, when you can tell people to turn on fucking FSR / DLSS / XeSS performance??   What a fucking shitshow... And few weeks back when I said I'm not gonna buy AMD GPU again because upscaling is getting mandatory and DLSS is just objectively superior - I was laughed in the face. Every major AAA game onwards will be shoving up upscaling as base line performance - so your typical 60fps marks (and 30fps - yes apparently that is still a thing on PC in 2023). There's no escape from this.",Negative
Intel,Damn. My 5700XT did Mirage fine on Medium/Low shadows. Now its basement quality.,Negative
Intel,This is starting to get scary and not cool. Every stat is not native performance and a 6800xt with fsr at 1440p is only pulling 60fps?!?!? What the actual fuck is going on?!?!? Midrange is definitely on its death bed if this is the new norm...,Negative
Intel,I love it how absurd these things are these days.,Positive
Intel,Really tragic this IP ended up with the most anti-consumer publisher in the world.  Layers upon layers of DRM.  Timed Epic exclusivity. No achievements.,Negative
Intel,Does anyone know if it supports SLI or crossfire?,Neutral
Intel,Source  https://news.ubisoft.com/en-gb/article/G0eJBcH8NcvNO2MPqb1KV,Neutral
Intel,"Hopefully this is another Alan Wake situation where the game performs better than what their system requirements would suggest. With this being a based on a movie game, I don’t have super high hopes that that’ll be the case though. Going from movie to game and vice versa almost always has bad results.   Also, displaying system requirements with upscaling is a bad joke. This is basically saying you’re not getting native 1080p 60fps without a 6800xt or 3080. Upscaling is definitely becoming the crutch a lot of folks feared it would be.",Negative
Intel,I thought upscaling technology is for low end gamers to get more FPS or people who work on 4k monitors and use upscaling to get better frames on them. but it seems to be used as an excuse for bad game optimization. What a complete joke!!,Negative
Intel,They are so bad this days that if the say the specs without the upscaling it would be 4090s all across the board,Negative
Intel,Why in the f*ck is upscaling included on a specs page?,Negative
Intel,no more software optimization and full upscaling  bleah,Neutral
Intel,Now THIS is what I call a reasonable spec sheet for a next-gen game 👏👏👏,Neutral
Intel,"Surprisingly good specs. 1080p high 60 FPS on 3060ti/6700XT is what we can expect from 3 years old cards in open world games.  Yes, I know it's with upscaler. But if the footage stays - it's understandable. Ubisoft is always about open world games afterall.",Positive
Intel,"I've never even heard of this game, nor care about it, but these system requirements offend me.",Negative
Intel,seems like graphics reached its eak with ps4 and games dont look that much better naymore and requires 4k dollar pc to run with dlss,Negative
Intel,"What about steam achievements ? :), i want to show off perfect games on my steam profile :)",Positive
Intel,"Telling me my $1,000 GPU will only get me ~1200p at 60fps tells me you don't want me to buy your game.",Negative
Intel,How about support them with games that people actually PLAYS ? Like Alan wake 2 🤣,Neutral
Intel,Fuck FSR and DLSS. This isn’t how they are meant to be used.  Rather than making a good experience better they are being used to make unplayable trash playable. Papering over shit programming.,Negative
Intel,Nice,Positive
Intel,"If Anti-Lag is still locked by then , please enjoy your frame gen input lag.",Neutral
Intel,"The game has a huge graphical potential, it's the right time to release that much potential environment in this era .   But it wasted its potential by considering lower class GPUs there by reducing polygonal complexity and complexity of environment including terrain and vegetation , animal complexity ,etc.....  Imo, it needs to have 2X polygon than what it seems.  It seems ultra poooooorly optimized as per the recommended requirements coz it's a bad graphics.",Negative
Intel,Native gaming died or what ? Wtf they turning pc gaming into console gaming,Negative
Intel,"60 FPS WITH Frame Gen AND FSR balanced? God, that's gonna be a horrible experience.  I really don't like the direction we are heading.",Negative
Intel,4k balanced FSR means 1200-1240p ... lower than 1440p ! And 4080-7900 XTX for that ?,Neutral
Intel,Their supporting FSR 2 and 3? what the hell does that even mean? Unless they plan on releasing b4 FSR3 becomes drops.,Negative
Intel,"Omg, it would be a graphic master piece or  bad optimized thing.",Negative
Intel,First time I see matches recommendations for nv and amd GPUs...,Neutral
Intel,Rip laptop rtx 3060 6gb,Neutral
Intel,"How do FSR 2 & 3 work with tech like ReShade?  ReShade has a pretty cool shader that will make any game into stereoscopic 3D.   When it works it works well, but it doesn't work in every game.  I'm curious because I would like to try it for this, since like 99.97% of Avatar's appeal (shtick?) is the 3D.",Positive
Intel,Upscaled 1080p low settings and only 30fps with A750? At least the game has xess.,Neutral
Intel,*NATIVE* resolution gang ftw!,Neutral
Intel,"I have been wondering with a few releases lately (Forza Motorsport, Cities Skylines) what GPU the dev machines they are using. I have a 7900XT and it ain’t enough to get above 60FPS without variable resolution at 1440p, seems kinda crazy that one of the top 5 cards from the latest gen can’t push enough pixels for these games.",Negative
Intel,Looks capped at 60fps?,Neutral
Intel,"Funny, how it only tells you the settings for FSR but not for DLSS and XeSS even though those 2 are also supported.",Neutral
Intel,How come 6800XT is mentioned? But not the 7800xt? As recommended specs for 1440p?,Neutral
Intel,"Rip my 780m and rtx 4050m. I don't understand why upscaling is ""mandatory"" not ""supplementary"". Do we need a SLI rtx 4090 for native 4k@60hz?",Negative
Intel,Is it using UE5?,Neutral
Intel,"Ubisoft, rip on launch.",Neutral
Intel,guessing no DLSS3 then?,Neutral
Intel,I guess that fsr3 will be added in an update at some point since everything mentions fsr2. It will be interesting to see it running on my gtx 1060 since it seems to be a big upgrade on older cards.,Positive
Intel,"Another game with upscaling baked into all presets. ""Free performance"" they said. I expect frame hallucination tech to be included in these presets in 1-2 years tops",Neutral
Intel,"Can't wait to be ridiculed by Alex at DF again for wanting PC requirements that don't include upscaling.   Upscaling should be an assist, not a requirement.",Negative
Intel,Farewell 1660ti… looks like it’s time for an upgrade,Neutral
Intel,well my 3300x is now obsolete for these new AAA games...,Negative
Intel,4k ultra right up my alley 😏,Neutral
Intel,fsr3 frame gen but in the specs themselves FSR2 is stated everywhere. What? So there's FS3 FG bu no FSR3 upscaling?,Neutral
Intel,7900xtx will do 4k 120fps with FSR 3 then I guess?,Neutral
Intel,What must one do to achieve a higher rank than an enthusiast? A demi-god?,Neutral
Intel,"I think we're at a point in gaming where DLSS / FSR will be mainstream in pretty much every game now. I mean even my 4080 can't get 60FPS 1440p in the Witcher 3 or Cyberpunk maxed out with ray tracing, have to use DLSS to increase to above 60fps. I do think DLSS / FSR is actually good technology, but for those enthusiasts who like to play native, I feel like having a $1200 GPU that can't run 4K or even 1440p ULTRA settings at 60fps depending on the game, is just a cash grab.",Neutral
Intel,Is vrr fixed with frame gen then?,Neutral
Intel,"Will someone please get a message to developers that FSR/DLSS should be *optional* and used only if you want to sacrifice some visual fidelity in order to run 144 FPS+ on high refresh rate displays, and let them know FSR/DLSS should never be *required* to get a 90GB bundle of spaghetti code to *barely* reach a blurry ass 60 FPS?  K? Theeenks!",Negative
Intel,"Yet another AMD sponsored game without DLSS....      EDIT: Nm, it appears this game will offer DLSS thank god.",Positive
Intel,Hoping with my 7800x3d and 7900xtx fsr 3 4k maxed is around 120fps for my tv refresh rate. To be honest at my age I barely notice the quality difference of any input lag.,Neutral
Intel,They recommend upscaling even at 1080p.. disgusting ew,Negative
Intel,pretty much this we all knew they would start using upscaling as a crutch.,Negative
Intel,Welcome to modern times and how you can’t just brute force everything anymore. Or you can go buy a 4090 and not complain about how expensive that is.,Negative
Intel,"Yeah, I'm not crazy about using upscaling techniques on everything to gauge it's performance and requirements. I think more time needs spent on optimization but I could be ignorant for stating that.",Negative
Intel,My thoughts too…,Neutral
Intel,Thanks to all of you that were screaming dlss looks better than native lmao.,Positive
Intel,"I tend to be much more picky than those around me. A group of people I play with were all playing Remnant 2, and I FSR was driving me crazy, they all have way less powerful computers than I do, and none of them cared at all FSR was on. In fact they were impressed with how well it ran and how it looked. I think there is a general 80% of the PC population that doesn't mess with graphic settings and just wants the game to play. I'm a min/maxer when it comes to graphics where I constantly tweak it until I get the perfect output for me.     I'd love to have real data from games like this on how many people actively change graphic settings.",Neutral
Intel,and a year ago i kept saying this here and i got downvoted to hell   its so obvious that the game engine devs and game companies pressured both nvidia and amd for this because they can release games faster and somewhat unoptimised,Negative
Intel,"Upscaling, once a promising and beneficial tech, now abused by almost every dev cause they are lazy to optimize. :(",Negative
Intel,Nope we as a community abused a nice thing,Positive
Intel,"Alan Wake 2 (and others) is proof that this doesn't matter any more. DLSS Quality looks and runs better than DLAA native rendering as shown a number of times and documented by tech power up and actually observed by those of us playing it  It's about time people stop crying the ""omg no native rendering"" beat as this is a mentality that is irrelevant in 2023. It's pretty clear that too many games come with performance issues and as such ""native"" rendering has the worst performance in those titles. At least upscaling offers a means to get better performance and in the vast majority of titles, better image quality as a result too with intricate details rendered at a higher fidelity thanks to image reconstruction. The likes of Digital Foundry showcase those benefits often in their videos. Plus the fact that all of the consoles upscale yet nobody bats an eyelid, yet for some reason a small portion of PC gamers seem to demand it no matter what, and then complain when it doesn't run as well as they had hoped in various titles.  It's a hard pill to swallow, but it's \[upscaling\] progression and the key driving factor as to why we can path trace and use other modern engine tech with such good performance and visual quality. This is probably the wrong sub to say such things, but this is the truth, and I fully expect pitchforks to be mounted.... But don't take it out on me, point the finger at AMD and ask them why they are falling so far behind.  At the end of the day if it looks good and runs good then nobody needs to bat an eyelid on whether it's got native or upscaled rendering. Everyone benefits as a result. Fact is the vast majority of modern games look and run better when using DLSS with only a handful of FSR exceptions (Callisto Protocol, for example).  Personally I want improved image detail, sharpness and performance, not just AA that DLAA brings to the table since it's just a more modern AA method vs ancient TAA/MSAA/FXAA etc. I don't care if there's no native option, I can just run with DLDSR if I want to render at a higher res and then apply an upscaler to that output to get the best of both worlds.",Neutral
Intel,Didn't take them long to make upscaling worthless.,Negative
Intel,i smell a burgeoning cottage industry of game spec reviewers!,Neutral
Intel,"I've hated upscaling since the beginning.   Now, it's being proven right in our faces that companies are no longer making 100% they're making 45% done games, and then they rely on upscaling and fake frames to do the rest.   Why have a fully optimized game that runs well natively when you can just lower the resolution by 3x then have frame generation help?   Full force ahead to frame generation= Every game will be optimized so badly that you're forced to upscale",Negative
Intel,"It’s literally right there in the hardware specs. 1080p fsr2 quality is 720p input resolution, so at native the game runs at 720p low on a 1070 or 5700.  It is the inverse of what reviewers have been doing with their benchmarks - if you are a fan who really has a stick up their ass about native res, just do the math yourself. They’ve given you all the pieces you need to calculate it out yourself, but that’s not how the game is supposed to be run so they aren’t going to put native res in the official marketing material.  Games are optimized around upscaling now and it is unhelpful and misleading to pretend otherwise. Rendering 4x as many pixels is *obviously* going to throw things off with more intensive effects, and lead to a generally unplayable experience.",Neutral
Intel,"Judging by how games work on 4090/7900 xtx, these cards turn from 4k 60 fps to 1080p 60 fps",Neutral
Intel,requirements have been a joke for over 20 years at this point i dont know why people are surprised.   on the other hand crappy devs using fancy upscaling and fake frames to get the game playable instead of just making a well made game is infuriating,Negative
Intel,yeah this is the new standard,Neutral
Intel,"RT is the future. With this game and Alan Wake 2 using software RT at all times and hardware RT for anything beyond ""low"", RT is the future.  At some point GPUs will be so powerful and game engines will have RT in the bag that RT will be used in basically all indie games...and then suddenly nobody will talk about RT anymore because RT is in everything for years.",Positive
Intel,and with fsr now standard on consoles it(or a similar tech) will be standard on pc :(,Neutral
Intel,"people also forget both AMD and nvidia didn't really offer a generation leap in GPU performance this gen, sure it's popular to call games ""unoptimized"" (I know many AAA releases are, not denying that) but the issue compounds with the fact that game devs couldn't predict a GPU generation to be this bad caused by inflated crypro-boom margins a generation prior",Negative
Intel,"As with almost every game release recently, expect the game to perform a bit better than the requirements.    Sometimes the hardware can achieve 60 FPS most of the time but drops below that in other times, so the devs just say it's guaranteed 30 FPS and call it a day. If you look at the 5700 vs 6700XT, the gap is almost 50% in most titles, this means that if the numbers for the 5700 were accurate then the 6700XT will be getting around 45 FPS at 1080p low (unless the game utilizes special hardware only available on the 6700XT) but the game targets 60 FPS at 1080p high on the 6700XT which leads me to believe the 5700 is gonna perform much better which should leave room for turning off upscaling.",Positive
Intel,"It does state ray tracing in the top right, we don't know if that's included in the requirements or not. I would definitely reserve judgment until we find out.",Neutral
Intel,Don't forget about Frame Gen too. IMO that's the worst part because that means it's 30 fps without it. Imagine the input latency...,Negative
Intel,The actual true crime here is even having fsr to begin with. It should just have dlss,Negative
Intel,Seems like they tried to cover every basis with these.,Neutral
Intel,"Yeah it's kinda weird, even games that had a massive difference of performance when using dual channel RAM never listed it on their requirements :/",Negative
Intel,"I've noticed that many developers do this, for some reason developers seem to have some kind of secret passion for the 1070 lol",Positive
Intel,It's because the minimum settings is with software RT enabled and thus they're using the fastest cards without hardware RT that can reach 1080p30 FSR2.,Neutral
Intel,"GCN support is over, RDNA1 is the lowest currently supported arch.",Negative
Intel,"In 2023, RX 5700 performs equally or better than the RTX 2060 Super/2070. The 5600XT is more like a 2060 competitor.   Either the game runs like shit on AMD hardware, or the system requirements are wrong and they just write down the components that they tested with.   Another thing is that the game is doing is software based RT by default and that might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.",Negative
Intel,"They're not, those are the cheapest/slowest GPUs that have 8 GB of VRAM. Clearly game devs are done pandering to people who bought Nvidia's e-waste.",Negative
Intel,What do you mean forced raytracing?,Neutral
Intel,1440 + upscaling looks better than 1080p because you're on a 1440 resolution monitor.   Do you even 1440p?,Positive
Intel,"Honestly RT has a lot of fluff and the majority of which is the most costly for the least amount of noticeable fidelity increase. Try path tracing in Cyberpunk with reduced rays & bounces and you'll still get great soft lighting and GI without most of the performance hit when full tilt. If games only leverage certain aspects of RT then the performance is still comparable, that's also why the 3080 and 6800XT perform similarly in Fortnite with full RT/nanite/lumen settings.",Neutral
Intel,It's actually 960p :(,Neutral
Intel,"can advertize pushing all the triangles and RT if you dont stick with a low resolution, certain things launched WAY before they should have in the consumer space.",Neutral
Intel,Support FSR3.0 and being able to use FSR3.0 at launch gives me a bit of skepticism lately,Neutral
Intel,We'll see in a year.,Neutral
Intel,AFAIK  &#x200B;  It is with RT,Neutral
Intel,Seems pretty good to me given it is at 4K with RT,Positive
Intel,"Likely includes the RT features , its also 4k",Neutral
Intel,"Its ubishit, what do u expect, however, since the game has RT reflections, shadows and GI, then i would say this is based on AMD shit RT performance, so lets wait and see how the 4080 is gonna perform.",Negative
Intel,"I wouldn’t mind so much if it was exclusively a 4K thing, but now devs are leaning on upscaling *at 1080p*, where it still objectively looks like ass and blurs everything in the background.",Negative
Intel,"Everyone asked for this. ""iTs tHe fuTuRe"".  ""LoOks bEtTeR tHan nAtiVe"". ""BiGgeR nUmBeR bEtTer"".",Negative
Intel,and we were called [names that frankly should have resulting in bans] when we said this is where it would lead to. and we still get shat on for saying the same about fake frames,Negative
Intel,"I think the top right table is listing the available features/settings the game supports. Depending on the presets, RT may be enabled by default or it may be a separate option from the presets. I guess we'll find out.",Neutral
Intel,"Nothing will stop DLSS / FSR nor RT in new games at this point.  The problem with the rising hardware requirements is, that it will keep a lot of 1440p/4k dreaming gamers with 1080p for a very long time and FSR doesnt really look good enough in 1080p.",Negative
Intel,I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.,Neutral
Intel,Exactly! It even got xess so Intel users also can use xess,Neutral
Intel,">Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. T  its not much different for consoles , like alan wake 2 uses Low to mid settings and FSR balanced on PS5 even on switch titles use scaling and no man sky even FSR and stuff.",Negative
Intel,"I agree with this, that’s not that outrageous if this does include RT. Non RT would be pretty bad to need fsr but that is the quality mode.",Neutral
Intel,This is going to be the exact same situation as with Alan Wake. The game is going to come out and the requirements are going to make sense. People just aren't used to seeing games with mandatory rt.,Neutral
Intel,I would say pressured bc they announced it over a year ago and promised we will get it in september. Thats the problem with games an tech they all make promises and then cant keep the deadline. Just make smth new and when it works then announce it and make the last fine adjustments. If you make promises to early you set yourself up for failure.,Negative
Intel,"Maybe on the lower end, but for enthusiast and ultra... no, they're not ""significantly"" better. These are pretty on-par comparisons (outside of RT).",Negative
Intel,"How so? The 4080 and 7900xtx are neck in neck, same as the 6800 and 3080.",Neutral
Intel,"It's 4k, it will take another 5-8 years to be mainstream.  1440p slowly creeping to 10% marketshare give it another 2-3 years to be mainstream",Neutral
Intel,"Consoles use fsr as well, and not just fsr but often at low precision making the upscale rougher. Its an industry wide trend.",Neutral
Intel,">what a shame pc gaming has become.  Could be worse , Consoles use mostly FSR balanced like in Alan wake 2 the PS5 uses Low to mid settings and FSR balanced.  its likely the trend of going from specially home made engines  catered to the games needs to "" Can do everything , but nothing great"" multi tool engines like UE.",Negative
Intel,"> what a shame pc gaming has become  I mean, PC gaming is in the best state ever. So many amazing games that run so well. Just don't buy all these shitty games, not like there is a shortage.  Tbh this game looks like standard movie cash grab shite anyway. Solid 4/10 game I bet.",Negative
Intel,Which means if all games will require upscaling it will force AMD to improve there's. I'm on board with you though will see way less optimization going forward which is not what we want.,Negative
Intel,"Absolutely agreed my friend. Time to leave pc gaming and focus on building yourself. The gaming industry is corrupted to the extreme, and will continue to do so as long as players continue to praise lazy developers. In the pussy that whole shit.",Negative
Intel,"Mirage is PS4 game, Avatar is PS5",Neutral
Intel,"Update: I must've read that wrongly somewhere! Apparently, it uses their own Snowdrop Engine 😳  Well AC: Mirage is nowhere near a next-gen game. Avatars of Pandora, on the other hand, uses Unreal Engine 5 with hardware ray tracing. So it's only natural it needs more horse power. But it'll look 10x better than AC: Mirage. So there's that 😉",Negative
Intel,Timed epic exclusivity? Aww man.,Neutral
Intel,Was guaranteed to happen when up scaling was announced... It's an excuse for less optimisation or to push fidelity more.  I saw a video of this avatar game it got an insane amount of plants visible at all times.,Negative
Intel,You... are.. joking... right..?,Neutral
Intel,Because it’s not coming to steam. It’ll be an epic games exclusive so everyone will pass on it until it hits steam in 18 months. At which point no one will buy an 18 month old game for full price and will wait for a 50% off sale at the minimum.,Negative
Intel,Kind of a messed up opinion.  Maybe games don't look much better than PS4 on your PC but they sure as hell do on mine and that would still be true at a lower resolution and framerate.,Negative
Intel,Ubisoft no longer employs achievements on new titles.  Don't get your hopes up.,Negative
Intel,Try ubisoft achievements :),Neutral
Intel,"It's doing rt at all levels and 4k? Ye... It isn't mainstream yet, heck 1440p is like At 10% market share",Neutral
Intel,"Iam using afmf daily, the input lag introduced by afmf is between 5-12 ms for me.  Doubt any can literally feel this.  I have afmf enabled in all possible games & emulators.  If you think it's bad for you, you don't need to use it",Negative
Intel,Pretty much yeah... Was inevitable that Devs will use up scaling as excuse.  Same for taa being a absolute shoddy aaatleast up scalers do better aa :/ . But even consoles run now heavy fsr like the PS5 uses fsr balanced on WLAN wake on mostly low settings,Negative
Intel,We don't know the settings.  If its like cyberpunk psycho rt on release which 3080 and 3090 easily needed dlss balanced or performance for on 1440p and 4k...    Then I can see the same case in this game for 4080 and similar cards  Also they dont speak about frame gen just fsr2 ( which doesn't exist here because they support fsr3 which does up scaling and frame gen it's not split like dlss 2 & 3),Neutral
Intel,"The fans of avatar, or people that like fantasy games or just games to have fun in.  Luckily games don't need to be for everyone.",Positive
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Really comes up to how far the rt and lumen stuff can be pushed, if it goes to cyberpunk psycho rt levels? At release it needed a 3080 and 3090 with dlss.   So it could need top end gpu of this gen if its a similar case",Neutral
Intel,yup that is why I will play 1440 UW native with a 7900XTX.,Neutral
Intel,I guess they are confused with dlss.  Dlss 2= up scaling Dlss3 = frame gen  So they thought its for fsr3 too. ( as in fsr2 up scaling and 3 frame gen)   But fsr3 can do frame gen or not.  Fsr3 is up scaling and frame gen.,Neutral
Intel,"Likely mid range gpu and a suit yells in the background ""30 fps with up scaling is fine""",Neutral
Intel,"Likely the same, they didn't want to make the graph even more annoying, hence why fsr3 and rt and xess are mentioned top right.  They got anyway already confused with fsr3 and fsr2 ( cause 3 is fsr frame gen and up scaling it's not like dlss where 2 is up scaling and 3 is frame gen)",Negative
Intel,Very similar performance I guess,Neutral
Intel,Ye,Neutral
Intel,They've been using it since forever. The difference is that PC GPUs were 10x faster than console GPUs so there was no need for it on PC.,Neutral
Intel,"It mentions dlss, fsr3 and xess support, doubt they limited it to dlss2.",Neutral
Intel,"Likely, they were confused because dlss 2 and dlss3.  So they thought its fsr2 and 3 too.",Neutral
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,Honestly... Maybe fsr3 frame gen can help you but... A 3300 Was already outdated when it came out.,Neutral
Intel,Most likely the upgrade part called FSR 2 and the frame creation part called FSR 3,Neutral
Intel,If they didnt get confused between FSR 3 and 2 then yes fsr 3 roughly doubles fps same for AFMF.,Neutral
Intel,"Nah usually enthusiast in hardware did mean "" max settings no compromises high frames"" but let's say... This changed the last 3 gens sadly.",Negative
Intel,"with AFMF it works , didnt test with FSR3 now.",Neutral
Intel,Reading before raging :),Neutral
Intel,"It is even low quality with 30fps and still needs 5700 ~~XT~~. That is pretty bad :D  *Corrected to 5700, which I think is still a lot for FSR2 at low fullHD, but definitely cheaper GPU.",Negative
Intel,That's with 5700 and 1070.... WHAT exactly are you expecting those cards to do in visually demanding AAA titles in 2023?  People have some crazy expectations. Everyone wants the eye candy but no one wants to pay the performance cost.,Negative
Intel,For 30fps even lol,Neutral
Intel,"Especially recommending FSR.  DLSS at 1080p? Sure why not?   FSR at 1080p? Disgusting, absolutely horrific.  Anyone who downvotes this is a fanboy, end of story. Nvidia's DLSS simply is better at reconstructing an image. At lower resolutions that is SIGNIFICANTLY more important.",Negative
Intel,Or be happy your shitty video card is still supported.,Positive
Intel,and we still get shat on for saying they are or admitting its a bad thing. corpo fanbois are so cringe,Negative
Intel,Even 4090 users use DLSS because it fixes TAA problems. And has a sharpening slider for easy sharpening adjustment.,Positive
Intel,When did I ever complain about how expensive gpus are. I've always been in the category that they are luxury item and vendor can charge whatever they want for em. So you aint never hear me complain about gpu prices.,Negative
Intel,I can't market a game for running with 120fps 4k with raytracing on on a RTX 4060.  But I can market 400 completely identical missions that take up 150 hours of your lifespan and empty out your soul.,Neutral
Intel,Yea I know nothing about optimizations and if it could or couldn't fix this or that. I just want them to tell me exactly what I need to run the game at X resolution a Y preset with 60 fps. If that calls for a 14900k and a 4090 then just say so.,Negative
Intel,"Nah, I think we're really beginning to see a trend here with abusing upscaling for free mileage.  Not that upscaling isn't great but...  Apart from RT, on medium settings, I can't really see    massive improvements on visual quality that would warrant such a massive increase on requirements.  UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  We'll probably see this continuing to be a trend for a while, and then as tech novelty wears out, some devs will come out with more optimized and scalable titles.  With hardware prices being as they are, and with 60fps now being highly requested in consoles as well, it's pretty much a given.",Negative
Intel,"People accepted upscaling with open arms, so now devs know they can rely on people using it. Thus they don't have to care about optimizing for native res anymore. Oh you have a 4k card? Okay well we'll make sure it can run the game at 1080p so you can upscale it.",Neutral
Intel,"No average consumer wanted to come to this point. The market leader started pushing upscaling instead of just making stronger GPU’s and there’s nothing AMD can do to win them, only join them.",Negative
Intel,Turns out TAA is worse than AI TAA with AI upscaling.   Of course devs are gonna use it to cover their own asses. And they get perf boost!  Let's face it. The FUTURE is AI everything. Nobody is gonna talk shit about frame generation and upscaling when it looks perfect each frame.,Negative
Intel,But it does in many ways.,Neutral
Intel,"I don't know who those people are. I play all my games on native, whether I'm using an amd or nvidia card.",Neutral
Intel,"dlss looks better than native with TAA because taa is so bad, but dlaa/fsraa at native >>>> upscaling",Negative
Intel,you think engine devs can pressure AMD and even Nvidia? Come on man. Be real.,Negative
Intel,"Ever notice how multi-GPU vanished the moment DX12 shifted the onus from AMD/Nvidia over to the developers? It shows just how much effort they put into getting SLI/Crossfire working when developers instantly drop them the moment they have to do the bulk of the work (significantly less, I might add) themselves.",Negative
Intel,"If I relabeled ""Medium"" preset to ""Ultra"" preset and changed nothing else and told you that you can run this game at Native 1440P Ultra on a midrange 7800XT card and get 60+FPS, would that be ""optimized"" now?",Neutral
Intel,speaking facts my guy,Neutral
Intel,You shouldn't have downvote dood wtf redditors ?,Negative
Intel,People with AMD cards dislike upscaling more because FSR sucks ass lol.,Negative
Intel,"Uhm, me btw...",Positive
Intel,"> Who actually plays games at native these days, if it has upscaling?  I do.",Neutral
Intel,"I won't judge your preferences, but at least using native rendenring isnt the clown show we have here. Native 4k look the same no matter what gpu render it (performances aside). Now with all the quality preset, we can't figure what quality to expect. 4k60fps at balenced preset? What's that crap even supposed to mean as thoses presets vary from title to title, from NVIDIA to amd. And btw, I'm still using fxaa over taa and fsr when i can, don't ask me why.",Negative
Intel,"You're even seeing this on console, where Spiderman 2 now has RT reflections in every mode. Even Unreal 5's Lumen renderer is a software raytracer that can be hardware accelerated.  RT is the future because there are scenarios that rasterized rendering is just not capable of resolving. Screen space reflection artifacts, light leaking, weird glow on indirectly lit models... I find these so distracting and immersion breaking.",Negative
Intel,onto absolutely nothing.,Negative
Intel,"Wait what? No performance improvement this gen? Bullshit, RTX 4090 is literally 2 times faster than RTX 3080 and RTX 3090 is only 10% Faster than the 3080  you do the math. RTX 4080 is 50% Faster than RTX 3080.",Negative
Intel,Or the game performs at 50 fps at 1080p low on 5700 but 50fps is a weird number to target and they can't say 60 so they just say 30fps and call it a day.,Negative
Intel,Not if it's using mesh shaders like Alan wake. In that game the GTX 1650 outperforms the 1080ti.,Neutral
Intel,The game is raytracing only with a ridiculous ammount of foooliage.,Neutral
Intel,"I'm so glad this is finally getting more main stream spotlight.  I do a little bit in Dota 2 and the amount of people that simply have a terrible experience because of a single stick of ram in that title is staggering.  Sure, it's an outlier, but a lot of esports/CPU bound games see similar issues.",Negative
Intel,i wonder when dual rank and dual channel will start being listed too. since dual rank does help gaming,Neutral
Intel,Honestly my 1070 still runs great for what it is. It’s in my sons rig now because it couldn’t keep up with 1440p depending on the game and definitely not 4k. Still a fairly solid 1080p card.,Positive
Intel,False.  guy blocked me lmao,Negative
Intel,"I thought that was on Linux, though I might be wrong",Neutral
Intel,1070 doesn't have hardware RT though.,Neutral
Intel,According to the technical director there's no non-raytracing mode. I'm assuming that means software ray tracing for the minimum spec.  Source: Skill Up video - https://youtu.be/v91y87R5iDk?t=332 (5m32s),Neutral
Intel,Spoken like someone who has never upscaled 1080p content on a 1440 screen. The pixel ratios are all off. It's fucked.  1440p is perfect for watching old 720p HD content though.,Negative
Intel,Completely agree.,Neutral
Intel,Damn my brain must have been on auto pilot. I'm so used to 2160p + FSR quality neatly equalling 1440p.,Negative
Intel,I see the 5700 and the GTX 1070 on the minimum specs. Are those cards going to be RT capable for this game? (It's 30FPS LOW with FSR but still.),Neutral
Intel,"Sometimes the DigitalFoundry reviewers use their meme powers for good (eg:fighting shader compilation stutter), but their  >""it's better than native!""   memeing about upscaling seems to have gone sideways.",Negative
Intel,"Dude tells truth, go to Nvidia sub, and you fond every second post someone telling that DLSS looks better then native, and every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote.",Negative
Intel,It's purely on the devs for targeting low performance.   We could be getting 240fps at 4K using those upscaling techniques.,Negative
Intel,"Derp, derp.",Neutral
Intel,Sounds like John on Direct Foundry Direct every week.,Neutral
Intel,"""Avatar will be a title that will only appear with ray tracing. But we're developing the game so that the quality and performance will be scalable.""  Seems I was right.",Positive
Intel,">I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.  Facepalm...  Avatar Frontiers of Pandora very literally uses a custom engine, not Unreal Engine.",Neutral
Intel,Yep. This company learned something from all the failure game launches that happened this year. The game companies can't just implement one gpu company's tech and not the others bc then it'll only work on the one gpu that you implemented the tech for and it won't work on the other gpus from other companies. Meaning for example if a game company only implemented fsr it's only going to work on amd gpus and not on Nvidia and Intel gpus and vise versa is true too for both Nvidia and Intel gpus for dlss and xess.,Negative
Intel,Avatar is RT only. There is no non RT mode.,Neutral
Intel,This is mad with snowdrop engine. It's Ubisoft they don't use unreal...,Negative
Intel,"Yep, I missed that. Good call! Makes sense why the specs are a bit higher then but if it is forced on, that's going to be rough on any one with a more mid tier card.",Positive
Intel,">  Consoles use mostly FSR balanced  heck sometimes they use performance or less, heck a lot of the time even. Especially with RT or performance modes",Neutral
Intel,They have really long way to go. Take for example Lords of the Fallen - UE5's TSR is better than FSR. Take Alan Wake 2 and you get ton of shimmering on the edges. It's also not like nvidia not improving - so while AMD makes a step forward so will do nvidia..   There's also question how much they improve without dedicated accelerators that both Intel and nvidia is using. Intel jumped late to the party and they're already ahead - which just shows how much AMD cares. It's rally embarrassing considering both XBOX and PlayStation are also relying on FSR - ant that's bigger market than what nvidia has.,Neutral
Intel,I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?  I mean the facts are the facts. I need to play on lower settings till we get a 5070 or 8800xt.,Neutral
Intel,"It uses their own in-house engine, Snowdrop. The last game this developer did was The Division II which was excellent.",Neutral
Intel,"Took a look at some gameplay on YouTube, and it does look nice enough, though water looked like ass, and as mentioned already, it's an in-house engine (which is nice to see TBH).",Positive
Intel,"Next gen seems promising so far, low native rez hidden by upscaling, 60 FPS target on +500$ GPUs, traversal stuttering thanks to the glorious UE5. But hey, we have great reflections on puddles so at least we can take good looking screenshots.",Positive
Intel,You will never see a high production game from Ubisoft ever on the Steam store ever again at initial launch.  They are the anti-christ of the gaming industry seriously.,Negative
Intel,Consoles are the baseline in most multiplat games and they've been using upscaling since forever. The real reason is just that we don't have GPUs anymore that are 10x faster than consoles like last gen. You can't bruteforce high resolutions in games that run at like 720p on consoles with GPUs that are only 2-3x faster.  If upscaling didn't exist on PC the minimum here would just be 720p at the minimum and 1080p-1440p at the max.,Negative
Intel,Next-gen doesn't mean that current gen can run it well. It means it's so far ahead that most curren-gen hardware can't run it properly + top-end hardware can only barely run it well.,Negative
Intel,"Exactly. These paid exclusive games launches only make me care less whenever they actually release.  It's not just Epic, Meta has done it with several games (Moss Book 2) and when a proper looking pcvr port comes a year later it sells bad because people forgot about it.",Negative
Intel,"Is that with significant RT? I assumed it was ""normal"" ultra settings.  Still I can play Far Cry 6 and RE4 with AMD sponsored RT at 4k native with 100fps. This game will have to look amazing to justify what... a quarter of that performance?",Neutral
Intel,"You are correct. I overlooked the part where, under Ultra settings, it said that was with FSR2. Now it doesn't seem as bad. I mean, it's still not good because it's rendering somewhere between 1080p and 1440p, but at least it's not 30 fps without Frame Gen/60 fps with it like I thought.",Neutral
Intel,"It might be, AFAIK neither Nvidia nor AMD still support their implementation of it in their drivers.  But I've tried it on a few games and it is pretty cool to me, much stronger than almost any modern 3D movie (IMO why 3D has gone into hibernation), and I gather that Cameron designed the franchise around 3D.  Plus the Avatar 1 game was in 3D.  FWIW.",Positive
Intel,This is what I am thinking too.,Neutral
Intel,"You say that, but they specifically mentioned FSR3, and just DLSS, so i have a feeling DLSS3 will have to go via a mod again.",Neutral
Intel,"i have a believe that even 4C8T will survive for the decade, like the case of that 4790K. But turns out these newer UE5 games are wrecking even newish CPU like the 3600. i mean cmon it's still a 4 years CPU old which i think it's not old yet",Neutral
Intel,Let's hope so. 30fps is far from recommended for FSR 3,Neutral
Intel,Yeah I heard it works with that hoping it works with fsr3 now,Positive
Intel,"I've noticed that for some reason newer games look blurry at 1080 even without any kind of upscaling and I'm not sure why, older game look much sharper at 1080p",Negative
Intel,"Just lazy optimisation, that’s pretty much a PS5 equivalent GPU and you can be damn sure that the console version won’t be 1080P 30fps with FSR",Negative
Intel,"No it says 5700. NOT 5700XT, and yes there is a difference. Quite literally the difference between a 1070 and 1080ti. Or a 2070 and 2080.",Neutral
Intel,no no my 7 year old card needs to be able to play at high settings. so does my low end card from today. gotta be high or its just bad,Negative
Intel,Crazy expectation? A game released in 2023 should be able to run at 1080p low 30fps on a low end card like rx6500xt (lowest end modern card you can buy now).  Optimization matters.,Neutral
Intel,"Yup, these cards came out around the time of ps4 I think..that's a long time ago in the graphical quality of games we are seeing.  People should be happy to a point that those cards can even run some of these games if upscaling helps them run that's a bonus as it means they don't need to upgrade if those graphics settings are acceptable to them.",Positive
Intel,"I have a 5700 and it does better at 1080p on these games than what is normally posted. Now it does need quality fsr but its not the worst thing, anything lower than quality is. Bottom line is these are not 100% correct and never have been.  I have zero interest in this game but the specs seem fine even though I wish upscaling wasn't what was relied on for optimization.",Neutral
Intel,"well you dont know what they mean by 30 fps. It could be de 1% lows or the average, we dont know. The easy assumption is that it will be 30 fps average. but that's crazy that a game at 1080p low cant be run by a 5700. It better be a new Crysis.  edit: apparently it uses raytracing only for lighting which would explain the hows and why it crushes the framerate so bad especially on a gpu that doesnt have hardware raytracing like the 5700.",Negative
Intel,"Or in the case of games like Alan Wake 2, you will get unplayable fps if you try to running it at 4k native max settings. Even with a 4090, you will get a glorious 20fps.",Negative
Intel,Assassins creed origins and odyssey side quests/collectibles oh my god,Neutral
Intel,Yeah it doesn't give us a actual gauge to understand what we need for set native parameters. I mentioned optimization out of concern that they rely too much on upscaling and it seems like it's the lazy way out of something. It's unrealistic that a usable gaming experience be in a top tier hardware setup that costs $2k+ and top tier hardware of just the last generation are just barely cutting it anymore.,Negative
Intel,I appreciate your insights and opinions. Thank you.,Positive
Intel,">UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  Because they market how ""good"" it looks, not how ""well"" it runs. There's a big influence from marketing perspective to place high focus on ""eye candy"", it's what sells.  The how ""well"" it runs is usually restricted to the small % of us gaming nerds. ![gif](emote|free_emotes_pack|flip_out)",Neutral
Intel,console games started upscaling way before PCs .,Negative
Intel,This sub is rife with crazy conspiracy theories. I don't think it was this insane a couple years back.,Negative
Intel,you forgot who owns one of the most popular engines out there?,Neutral
Intel,"I guess technically it would count but then everyone would flame it for looking bad. But seriouly, what happened when Ultra settings ran real smooth on top of the line hardware? Gone those times.",Negative
Intel,downvoted by devs lol,Neutral
Intel,> RTX 4080 is 50% Faster than RTX 3080.  The msrp is 71% higher too.,Positive
Intel,"Yeah at 1200 bucks and above you got one, that's true but many cards below didn't offer much.",Neutral
Intel,that's stoopid but it's somewhat more understandable. Why a 5700 would struggle so much since it doesnt have hardware raytracing,Negative
Intel,Yep it's like a 10/15% performance uplift in some games but I'd guess they would prefer to keep the requirements sheets somewhat simple.,Neutral
Intel,Why is there a dialog message about unsupported hardware when you try and run a 390X?,Negative
Intel,It can do software based RT just like every other modern GPU out there.,Neutral
Intel,Well then no wonder rx 5700 can't manage 30 fps lol,Neutral
Intel,"Wait so we are finally getting games made only with ray tracing in mind? This is actually great, 2023 is officially the year the new generation started",Positive
Intel,Avatars uses a Lumen like software RT solution.,Neutral
Intel,"yeah they never finished the sentence for some reason. The full sentences is ""its better than native, when native has forced taa!"" which is why dlaa/fsraa at native look so much better than the upscale versions. normal basic TAA is just so bad, that even upscales not using it look better than native using it",Negative
Intel,"The AA side of things might be better, but I swear no matter the resolution textures become a blurry mess even with Quality mode etc, and it irks me that most people don't seem to notice that.",Negative
Intel,Some people seem to have very low sensitivity to added blur. See also *depth of field blur* and *motion blur* being on by default in every game. It's funny how much people rage against FXAA/DoF/Motion blur but ignore DLSS.  Of course only one is tied to someone's purchasing decision :),Negative
Intel,"Yeah, thatsl happened.",Neutral
Intel,">every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote  Well, yes, you are using shitty FSR which is literally unplayable no matter resolution instead of godly DLSS that is literally better than native even at 720p/performance. The only way you could have seen the ever unmatched DLSS for PC kings instead of the plebean FSR is through a compressed youtube video so your opinion is literally invalid. I hope I am being sarcastic enough.",Negative
Intel,"The problem is some (if not most) games have such a bad TAA it might as well be true, and if often cannot but turned off.",Negative
Intel,"""We could"" but not profitable. Just let imagine these games does not exist in our reality.",Negative
Intel,You didn't understand. It's another Swiss knife engine,Negative
Intel,"It is indeed not a great situation. Things like this shouldn't be forced on, and it leaves me feeling like they haven't given the devs enough time to optimize the settings.",Negative
Intel,It works because most consoles players are noobs they don't care about the tech. They just want to sit on the couch and play. I'm hoping we see them start working more on FSR3 and upgrading older games to it. And you are right nvidia will keep moving forward but so will AMD. They may never catch them since they have a head start.,Positive
Intel,AMD just released drivers for Alan Wake 2 that fixed the flickering while using FSR.,Neutral
Intel,The PS5 has a 6700.,Neutral
Intel,">I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?   Not exactly. It's a custom quasi-RDNA2 design (has the improved RDNA2 compute units, but it's missing a few features such as fully-DX12-compliant mesh shaders and VRS-capable ROPs).  For most purposes, it's pretty comparable to a 6700 (non-XT), with the same number of CUs (36), and similar clockspeeds.",Neutral
Intel,"Oh, wow. I didn't know that. But it'll likely be on a similar level as UE5, right?",Positive
Intel,Next gen DOES NOT MEAN that your CURRENT GEN hardware can run it well for sure... Why don't people get that?,Negative
Intel,That reminds me to grab moss 1 and 2. I just got a quest 3.   Agreed though. I’ve been waiting for kingdom hearts 3 to come to steam for so long that I’m basically going to skip it at this point. It’s still stuck on the epic game store.,Neutral
Intel,Far cry 6 and re4 are older and or lower demanding games.  And yes any Mode here does either Hardware or Software rt from. Min to max.,Neutral
Intel,It's a 4 year old cpu yes... But how it was built (4c) was like 10 12 years now old.  6 cores are since a few years already base and more and more 8 cores are standard.,Neutral
Intel,TAA,Neutral
Intel,Temporal anti aliasing.,Neutral
Intel,"TAA is the devil. it forces a blur on every frame, and only gets worse as the resolution goes lower. its why fsr, xess, and dlss can look better than """"native"""" because it doesnt use the horrid plain taa solution so many games do. and why dlaa, and fsraa are so important and need to replace taa completely",Negative
Intel,"Ahh, welcome to r/FuckTAA",Neutral
Intel,Even 4K looks blurry with some implementations of TAA,Negative
Intel,they're giving you the bare minimum until your upgrade!,Neutral
Intel,Because they are built with upscaling in mind so they have a blurry looks intentionally as it works best with aggressive upscaling.,Neutral
Intel,"Not even close, you're completely out of your mind. The 5700xt is 9% faster than the 5700. The 1080ti is **32%** faster than a 1070 and the 2080 is **19%** faster than the 2070.   Source from techpowerup: [https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339](https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339)",Neutral
Intel,"Well the difference is only 256 shaders more on the 5700xt. The 5700xt has 2560 shaders and the 5700 2304. That's 11% more. That's not that much.  The 1080ti has 86% more cuda cores than the 1070, as well as more and faster memory. I would not compare now so haha",Neutral
Intel,"The 5700 and 5700xt is more like a 1070 and 1070ti situation. It's not that big a gap, and you can close it even tighter because the majority of RX 5700 cards can be flashed to a 5700xt bios tightening the gap farther.",Neutral
Intel,"The 1070 was released in 2016. Its already the end of 2023. That card belong to ps4 era. Wake up.   People in this sub are delusional, really.",Negative
Intel,Those are some dumbass expectations. Maybe you should correct them instead of blaming the developers for it.,Neutral
Intel,"The 6500XT offers $200 2016 performance. In fact its often worse because of how gimped it is interface wise. Its one of the worst products released in recent years.  Its not the game developers fault AMD pretty much rebadged the RX 480 4 times and they can't be expected to sink their optimization to that level.  RX 480, RX580, RX 5500XT and RX6500XT are virtualy the same product performance wise. RX 5500XT is the most consistent one of the lot offering a somewhat more modern arhitecture in its 8GB variant. The 6500XT on the other hand while on a newer arhitecture is an unmitigated disaster that is on par at best and sinks even below the RX580 when memory, bus and PCIE interface constrained.  The console baseline is around 6700XT/2070 Super/3060ti level. People better get used to that as its gonna become or already has become the norm for 1080p high settings gaming. The 5700 is upwards of 30% slower by default while also having less hardware features. Its gonna become the minimum norm going forward alongside the 2060's of your time.",Negative
Intel,"the 1070 was 2016, the 5700 was 2019. While both not new its not 10 years old like the PS4 (2013)",Neutral
Intel,This is literally only true if you use path tracing. I think you're trying to insinuate the game is unoptimized when it just scales really high with visual features if your hardware supports it. It straight up runs great for the visual features it uses.,Neutral
Intel,"That was exclusive to console releases, no? And never to the point of replacing AA sharpening and rendering the game from 720p…",Neutral
Intel,No I havent. AMD is bigger than Epic.,Negative
Intel,"Right, you'll have different complaints but complaints none the less even though the new Ultra might still look slightly better prior gen games and offer high FPS but people would complain that the graphics are not advancing fast enough, they look the same etc etc.  Yah those days are likely gone but it's hard to say if it's because the game is unoptimized or if the dev is really pushing the limits of what they can do graphically and use upscaling as a buffer because what they have in mind, or what they intend to show is too taxing for current hardware. We won't really know until a game is released  So screaming game is ""unoptimized game, lazy devs"" seems to have just become the norm before anyone's even played it. This is why I've never in my life ever pre-ordered a game. I still don't understand why people do it (esp now that games are digital) but lot of people do. Then again I've never really paid for a ""skin"" or some other visual add-on so maybe that's what attracts people.  I'm not saying games aren't unoptimized, those do exist, they offer nothing more but run like crap. We know what those game are, it's not hard to tell. But then you do have games like AW2, which before release, we were hearing the same exact complaints and it looks amazing and the minimum requirements were on the conservative side. The game looks better at ""medium"" than high/ultra presets of some games, heck you can even argue that at some low settings it looks better than some games at medium/high presets. The presets are all relative anyways. There's no universal settings that define what each preset represents.  I just don't get the outrage on these minimum spec sheets compared to what the game is showing graphically. At least wait until it releases and we have some hard data.",Negative
Intel,"I'm also a dev dood sometimes it's literally a ""foutage de geugle"" I had a friend who talk about how shit devs in EA (the people who's making the engine) code and doesn't optimise",Negative
Intel,"The commenter I replied to said there wasn't a big generational improvement for GPUs, while I said there was. But I agree that 4080 is expensive for what it offers.",Neutral
Intel,It was the same price as a 3090 at launch and offers 90% more performance.  It was definitely a generational leap.,Positive
Intel,It's software ray tracing which isn't accelerated by hardware.,Neutral
Intel,and i guess it can be hard to tell if ram is dual or single rank with just two sticks of it too.,Neutral
Intel,>might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.  I was pointing out that 1070 doesn't have hardware RT either.,Negative
Intel,So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?,Negative
Intel,"Why not make it toggleable though, you can choose other graphics settings so it makes no sense. Most people don't play games with RT enabled",Negative
Intel,Still kinda weird that they put the RX 5700/A750 on the same ballpark as the 1070... Being that the newer GPUs actually perform closer to a 1080(Ti).,Neutral
Intel,Sounds like improperly configured negative LOD bias. If you have Nvidia you can fix it with nvinspector. On AMD there's nothing you can do IIRC if Devs mess it up.,Negative
Intel,"Exactly what I'm telling them all the time, that I can see difference in textures, in hair, fences, powerlines etc. And every time I get downvoted by the army of ""but it looks better then native"". I feel like majority of that sub needs to go get a prescription for new glasses",Negative
Intel,"So if you stopped measuring who's bigger, why do you think I have no access to DLSS? Tested on rig with 4080 on a C2 42"" screen. Once you stop thinking with the tip of your head where probably Nvidia's logo is tattooed or maybe Jensen's face, who knows, you could come to realization that one person have access to more hardware then only his personal rig.  Do I say that FSR is better then DLSS? No! Both of them have their issues, but none of them is better then native. Keep on fanboying mate, instead of providing normal convoy as grownups",Neutral
Intel,I don't think YOU understand what you're talking about.  Did you seriously expect Snowdrop to be something used in like one game and then discarded completely?,Negative
Intel,"The thing is: creating good looking lightning with RT is much easier than with prebaked illumination. I can fully understand, why developers of new games are using RT as the basis of their game.",Positive
Intel,Minimum requirements go up as games get/look better. They probably made this game with the series s as the minimum. Low settings probably looks better than high in most games.   Rt for the last few years has been added on top of already complete games. Now games are being made with rt as the standard and they would have to manually do a ton of extra work to support 7+ year old hardware. It's the same as having to drop support for last gen consoles to be able to deliver a better game.,Neutral
Intel,"And still - superior upscaling makes it worth paying extra, RT on higher end is worth paying extra. AMD even this gen doesn't undercut price by enough, especially in EU where power efficiency also play a role. For example, assuming I'm gonna use GPU for 2 gens, then I save ~90-100€ with my usage just on electricity alone. I know it's over period of 4 years, but still it's price difference that will recoup over time, even if I have to pay extra upfront. AMD is simply no longer a value proposition.",Neutral
Intel,"They did not, lol - that fixes completely different thing, not the FSR caused shimmering edges.",Negative
Intel,Both excellent 👌 Down the Rabbit Hole was another solid one.,Positive
Intel,"RE4 came out this year.  Anyway the difference between us is I don't think any lighting effect is worth a $1,000 GPU using 1080p resolution and 60fps, but I know many disagree. That's life.",Neutral
Intel,"3600 and the 3300x was built with the TSMC 7nm if i recall, that was 2018 cutting edge tech. and somehow in gaming it's already obsolete. i was planning to replace mine with 5700x  but i cancel it because it's still very much capable for what i do now. but now after UE5 i think i'll reconsider it again, maybe even as far as 5800X3D",Neutral
Intel,It is crazy that I see everyone saying that modern games look blurry. I've known for years taa does this but it seems like I see more people than ever dissatisfied by it.,Negative
Intel,I have to turn it off in borderlands 3. Ugh.,Neutral
Intel,r/FuckTAA,Neutral
Intel,"My friend nicknamed it after his eyesight. When he wears glasses he has 20/20, but when he doesn’t he says that he sees the image about the same as without glasses",Neutral
Intel,"Out of curiosity what resolution do you usually game at and at what distance? I find taa usually looks pretty good in modern games, but I'm also playing on a TV (48 inches but I'm not at monitor distance anymore). When I played on my 1920x1200 monitor I HATED TAA, well, not as much as fxaa, but moreso than smaa, and I missed msaa or SGSSAA with a passion. Now on the TV at 4k (lol, usually not on a 6700xt), 1440p, or even 1080p I find TAA can do a pretty decent job paired with varying levels of the Radeon sharpness adjustment in the adrenaline drivers. But again, probably due to viewing distance.  Looks leagues better than FXAA and I prefer it to SMAA now. But again, I'm sitting further away and tweaking sharpening to my liking. FSR2 seems to be a wash most of the time but the performance gain is worth it.",Positive
Intel,You dumbass do understand that even the console version of the recent games are total mess? Alan wake 2 have to run on reduced settings to even reach 30fps on xbox/PS. Callisto protocol used to have frametime issue on consoles. Console ports still targets 30fps on quality settings with upscaling while target should be 60fps with 120fps for performance mode.,Negative
Intel,"Yes, seems you are correct, it was the ps4 pro that came out in 2016, still, I wouldn't expect that to be capable of running some of the games we are seeing now without some serious compromises.",Neutral
Intel,I never said that Alan Wake 2 is unoptimised. I just said that even a 4090 can’t max out the game at native 4K resolution.,Negative
Intel,epic is tencent...,Neutral
Intel,"In terms of absolute numbers sure, 3090 always had a stupid price tho for being 10% faster than a 3080.   In terms of performance per dollar the 4090 even at 2x 3080 performance was worse value since you pay 1600 not 1400.",Negative
Intel,"Yes, you either have to get the spec sheet of the RAM modules (That even then it may not specify if it's a single/dual rank module) or test it in a system and use HWinfo/CPU-Z. Not as straightforward as dual channel that's mostly just using two identical modules.",Neutral
Intel,You don't need RT hardware to do software RT. That's what I'm saying.,Neutral
Intel,"For this particular game, there is a software fallback. Also, even if we did get a game that mandated hardware support for RT they'd still likely build it with console in mind so even RDNA2 GPUs shouldn't have issues",Neutral
Intel,"The first RT/DLSS capable cards are 5 years old now, way older than the current consoles. You are definitely not being left behind if you're only outdated by 1 gen lol",Neutral
Intel,I mean the last non ray tracing cards were released 4 years ago (1650/60 and 5000s from AMD),Neutral
Intel,Games running poorly on computers weaker than current gen consoles isn't exactly anything new.,Negative
Intel,"No, you have multiple generations of games from prior to raytracing to enjoy.  Games shouldn't be held back forever just because someone wants to game on a GeForce2 MX400.",Neutral
Intel,You are stuck playing older games. Unless someone can find a way to disable it with a mod.,Negative
Intel,> So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?  A gen old is second gen RT e.g. rtx 3000.  We're halfway through the third RT/matrix generation right now.,Negative
Intel,Yes hah,Neutral
Intel,"The 3090 or 6950xt will very likely run this game extremely well, they're a gen old.",Positive
Intel,"For the AMD GPU, it's probably because minimum will be the max it will support. Medium probably requires Hardware RT.  For Intel, I don’t know. Perhaps it signal not so great optimization for Intel GPUs...",Neutral
Intel,DLSS is very strong in thin lines like fences & hair. Much better than most TAA solutions. Not sure about FSR in general.  An example: https://pbs.twimg.com/media/E6erA6BVkAAuf_M?format=jpg&name=large  Only DLSS resolves the lines properly.,Positive
Intel,"I mean, it's not that black and white. Quality upscaling to 1080 will obviously be more noticeable than 4k. Maybe you *can* tell the difference between 4k native and dlss quality, but people aren't gonna believe you unless you prove it with some kind of blind test, but I doubt you'd be willing to go out of your way to do that.  Even if you could, the benefits of better aa might outweight some negligible drop in textures or whatever it is upscaling reduces. Though that's irrelevant when DLAA or NAA are options too, I guess.",Neutral
Intel,"Test, which one looks better, explain.  https://imgsli.com/MjE3MzEy",Neutral
Intel,Oh man the hair... its so crazy how much upscaling kills the hair..,Negative
Intel,i mean they already arent the smartest bulbs considering they went team green.,Neutral
Intel,Reading comrehension dude.,Neutral
Intel,"I do, but thanks for your interest.",Positive
Intel,That will depend on the user. Power is cheap here in canada so none of thar factors into my buying decisions. And rt will depends on the games you play also not everyone plays tripple a games.,Neutral
Intel,"You need to get 23.20.17.05.  Fixed Issues Intermittent flicker may be observed on some textures while playing Alan Wake 2.  Fixed for most people, also has a nice performance boost of about 10fps from what I'm seeing.  That being said I always prefer native resolution unless absolutely necessary. So far only had to use it in Martha is Dead.",Positive
Intel,Nice. It’s on the list. Thanks man,Positive
Intel,Yes.... 7nm was but 4 cores the actual important thing was obsolete like 10 or 12 years ago.,Neutral
Intel,Probably because TAA is (unfortunately) more prevalent than it ever was.,Negative
Intel,"Nah, most people are FINE with it. Its people on FuckTAA that are mad.  The thing is, you have sharpening filters today. You can adjust the sharpening for TAA. You can make it look oversharpened if you want. There's driver level sharpening for both AMD and NVIDIA. Both also have software sharpening filters.  And DLSS and FSR bot have sharpening sliders.  There's 3 ways to adjust how blurry and how sharp the game looks for everyone. It's a non issue and even Digital Foundry acknowledges the benefits of TAA for game devs vs things like the blur. At the end of the day, these games are looking great...in this era, with exceptions. Not all games are blur. The whole blur thing was like in the past 4 years since upscalers started and TAA became the norm. But with all the sharpening options, and with devs actually used to making TAA not so soft looking, things are definitely getting a lot better for TAA.",Neutral
Intel,Oh my bad if I read that wrong,Negative
Intel,And I'm saying 1070 doesn't have hardware RT so it's just as gimped as AMD cards when doing RT.,Negative
Intel,Still pretty annoying nonetheless. Until it's stable no way should it be forced upon a game when someone's willing to run it normally at native.,Negative
Intel,"and the 2080ti is still winning, best purchase in gpu i made in a long time.",Positive
Intel,imagine being mad that 4 year old cards arent high end,Neutral
Intel,I'm on a 6700XT. 🙇. I just feel very shocked that I would probably need a gpu upgrade soon. With the demands of new games. Also 1440p ruined me.,Negative
Intel,"Technically makes sense, but they can just make RT something you can toggle between Software/Hardware and they also could just add the Vega 56 in the requirement sheet being that was AMD's competitor to the 1070 or they could've added the GTX 1080(Ti)/RTX 2060(Super) into the sheet.  On Intel's side they only have the A580 as the closest GPU to a 1070, even when it's too similar to the A750 performance wise :/",Neutral
Intel,"It is that black and white for me. I can immediately see the added image softness from upscaling, and it's especially prevalent with DLSS 2.5+ unless you enable a sharpening filter. DLAA also has weird issues like blurring textures (almost like it reverts to initial mipmap) while moving, usually at mid-distance from camera. It's too distracting and I have to go back to regular TAA. Immediately goes away.  1440p to 2160p is better than 720p to 1080p, but it's still noticeable. Detail is immediately lost as lower resolution image is scaled to higher resolution. Is it terrible? No. Actually changing monitor resolution outside of native and having all UI elements scaled looks terrible. That was what we used to do and upscaling is much better than that alternative. LCD panels need to stay at native signal resolution and GPU should scale render resolutions.",Negative
Intel,"And you look at static image all the time while gaming? It's different story when movement is involved, that's where you find all the artefacts you get. Static image doesn't tell a thing when it comes to gaming",Neutral
Intel,"Yes, and that's driving me crazy. I really couldn't stand it in Cyberpunk (in other titles it's either hit or miss), that I play it in native 4k instead.",Negative
Intel,"Wouldn't phrase it like that tough. I had that choice too in January, and actually by going 4070Ti instead of 7900XT I would be able to play Cyberpunk in higher fidelity and better frames then on mine, because AMD cooked FSR3.0 almost a year in the oven, and God knows how long it will take for CDPR to add it to the game, if ever considering that active development on game is ended and they are on bug fixing sprint right now. Also AMD needs to fix FSR in general, because how it works with smaller details like hair and fences is awful. But don't get me wrong, I love my card, just feel I git sold on features that would come handy right kow instead of when I don't need them anymore",Neutral
Intel,"Sorry mate, guess I need to read comments early morning instead of late night, just saw that last bit. My apologies. I'm just tired of upscalare wars whatever sub I go, and green team fanboys screaming that ""FSR is shit, DLSS is better then native"" and then throwing around static images for comparison",Negative
Intel,"It's for textures, not object edges from FSR use, lol. Two completely different things",Neutral
Intel,"people echoing the same shit like ""ooh 4C is obsolete, you need to upgrade that geriatric cpu"". 4C is still solid for gaming (ofc no UE5) and literally anything beside doing compute intensive professional workloads.",Negative
Intel,"Sharpening makes the image less blurry, but it doesn't recover the texture detail lost from TAA blur.",Negative
Intel,"""Even Nvidia foundry""   \---   they are like chief proponents of blur-tech and they don't care about how sharp and easy-to-see the image is. TAA flattens the image so you can't see which objects are far and which are near, you have to strain your eyes and mind to determine that which is unbearable to many. Just give us option to disable it even if it breaks some effects (which is just lazy thing to do on the side of devs)",Negative
Intel,"Sharpening fixes blur, not clearity. That is TAA destroys+add significant ghosting no matter the implementation.      Sharpening looks was worse than the normal cleairy of games, it's not the same thing at all.      >Its people on FuckTAA that are mad.  We are mad because it being forced 1 and 2 it's being used to hide lazy effects that break without temporal frame smearing. People are now punished for turning off forced frame smearing.",Negative
Intel,And I'm saying that maybe the GTX 1070 is better at even software RT compared to AMD cards with no RT hardware. GTX 10 series card did get RTX support through CUDA even though they were horribly bad at it. Older AMD cards with no RT hardware didn't get any support like that from AMD.,Neutral
Intel,"You're not forced to use upscaling though. Unless you replied to the wrong person this is about ray tracing, which can be run at native just fine. If your GPU can't handle RT at native res, then just use upscaling.",Neutral
Intel,The minimum requirements for this game includes de 1070: a 7-year-old gpu from 2016.,Neutral
Intel,"I'm not really shocked, and was out there telling people not to buy those cards on day 1 in large part because of their deficiencies in RT performance and complete lack of matrix accelerators (which Nvidia and Intel have both made a large part of their architecture and got shockingly good results with - DLSS and matrix-XeSS are twice as good as FSR)",Negative
Intel,5700 was probably the lowest AMD card they had to test with.,Neutral
Intel,"It also looks better in gameplay, less ghosting, less shimmering, less aliasing.   So which one looks better? You clearly see the difference in textures/fences/powerline, right?  I even made it easier for you, both images are 1080p output.",Positive
Intel,I personally didn't try DLSS even once but FSR 2.0 does have shimmering I notice to various degrees. It was very noticable on fur in Ratchet & Clank cutscenes for example. Overall I am just happy my 5700xt could still keep up with 1440p.,Neutral
Intel,Hmm yeah whatever you want to believe and let's you sleep.  Have a nice day.,Positive
Intel,AMD CAS is aimed to restore detail,Neutral
Intel,"They aren’t chief proponents of anything, TAAU has been popular for well over a decade at this point and it’s the default approach for consoles (checkerboarding has moire and aliasing issues) and digital foundry simply recognizes this is the reality of how the console world has settled on doing graphics.",Neutral
Intel,The problem with Nvidia is they are massively overpriced. One 4080 would probably cost the same as my whole build or more.,Negative
Intel,"Yes I see which one looks better, and as I said before it's a static image. While I didn't use upscaling in this same exact example, I can go by my own experience and tell that it doesn't look better then native let's say in same Cyberpunk or any other game. And I will beleive my own eyes on my own screen then static images tossed around. TBH, in my opinion it all depends on screen and screen size too, as to me that difference between upscaled and native was less noticible on my old 27"" 1440p screen then it is now on 42"" 4k. DLAA tough, that's another story",Neutral
Intel,"I tried both, and neither of them are better then native as people tend to claim. In my testing FSR have big problem with smaller details like hair, fur, fences, powerlines and as you mentioned, you get severe shimmering effect. I really hope AMD will keep on working on it, an try to get away from that. But DLSS also adds shimmer, it's just less visible then in FSR (atleast in my testing on Cyberpunk pre 2.0), and whatever upscaling method I've used I also could see that image is upscaled (lower res textures here and there etc.). I like idea being upscaling tech, it helps prolong life of GPUs, and for more powerful GPUs to run higher settings with more fidelity, but there's still work to do on them as they aren't perfect yet.",Negative
Intel,Yeah but a 3060ti didn't,Neutral
Intel,"In Cyberpunk you are not comparing upscaling vs no upscaling at the same settings though. You are comparing low RT native vs path traced upscaling and at 4K, the decission there is absolutely clear. On my 4k OLED screen, upscaled PT is so clearly the way to go, there is no competition.",Negative
Intel,"The gameplay looks exactly like the image + your typical TAA/DLSS smearing. And as I said, in my example DLSS also looks better in gameplay, because as you see... native has a ton of aliasing which is super noticeable while playing (shimmering). There are so many objective tests, which show why its most of the time better than native. (up res to 80% instead of 67% and its always way better, even at 1080p)   Well when did you try DLSS the last time? Newest versions are miles ahead of the ""old"" 2.0 version and pretty much always ahead of standard TAA.  DLSS is essentially a way better TAA solution + AI to fight TAAs issues + lower res. Its so much better than your standard TAA, that youre able to reduce the res and it will still look better in movement. Think about it like codecs for streaming... there are codecs which need a way lower bitrate for the same quality than others. (av1)  Everything graphics related is not rendered at full res anyway (shadows/lighting/lod etc...), there is no line between native and upscaling. Talking about native is nonesense, just image quality counts and thats were DLSS or even FSR 2 (some games) gets better and better. There is a reason why consoles use it... it looks better than without it.",Positive
Intel,Confirmed Can it run CP2077 4k is the new Can it run Crysis,Neutral
Intel,It'll be crazy seeing this chart in 5-10 years with new gpu's pushing 60-120 fps with no problem.,Positive
Intel,In 1440p with 7900 xtx with fsr 2.1 quality it doesn’t even get 30fps,Negative
Intel,"Makes sense. Is almost full path tracing, it’s insane it’s even running.",Negative
Intel,good. this is how gaming should be. something to go back to before the next crysis shows its teeth,Positive
Intel,3080 falling behind a 3060? what is this data?,Neutral
Intel,wake me up when we have a card that can run this at 40 without needing its own psu.,Neutral
Intel,5090 here I come!,Neutral
Intel,"Playing the game maxed out with path tracing, FG, RR and DLSS set to balanced at 1440p. Over 100fps 90% of the time. Incredible experience.  *With a 4070 ti and 13600k",Neutral
Intel,"I mean, Path tracing is to Ray tracing, what Ray tracing is too rasterization.",Neutral
Intel,When Cyberpunk first came out the 3090 only got 20 fps in RT Psycho mode.  https://tpucdn.com/review/nvidia-geforce-rtx-4090-founders-edition/images/cyberpunk-2077-rt-3840-2160.png  Still does  Fast forward just one gem and you don't see anyone saying its demanding with many able to get RT Psycho on thier cards as new cards got faster.  Give it 2 gens and you are going to get 4k60 here.  Gpu will improve and get faster.,Neutral
Intel,"Look at that, my 6700XT is just 19fps slower than the RTX 4090 in this title.",Negative
Intel,It's a good thing nobody has to actually play it native.,Positive
Intel,Well good thing literally nobody is doing that…,Negative
Intel,The future is not in native resolution so this is really pointless information.  Cyberpunk looks absolutelt mindblowlingy insane with all the extra graphical bells and whistles it has gotten over the years and with Nvidia’s technology it runs so damn smooth as well.,Positive
Intel,4.3fps lol. No amount of upscaling is going to fix that and make it playable. People were saying the 7900xtx had 3090ti levels of RT when it launched. A 4060 ti is 50% faster then it.,Negative
Intel,That's actually pretty amazing for any GPU to get a metric in frames per second instead of minuites per frame.,Positive
Intel,"Meh I get like 90+ fps with everything cranked with RR, FG and DLSS(Balanced) toggled on with my 4090 @ 4k.  Path tracing does introduce ghosting which is annoying buts not really noticeable most times but at the same time with RR enabled it removes shimmering on 99% of objects that is normally introduced with DLSS so I am willing to compromise.   Honestly as someone who used to have a 7900XTX I am disappointed with AMD its clear that AI tech in gaming is the way forward and they just seem so far behind Nvidia now and even Intel(going by some preview stuff). FSR is just not even comparable anymore.",Negative
Intel,Who cares when it looks worse than with dlss and ray reconstruction on top of running a lot worse? Native res 4k is pointless.,Negative
Intel,"Well the upscaling in this game is really good, DLSS with ray reconstructions AI accelerated denoiser provides better RT effects than the game at native with its native Denoiser.  Also Path tracing scales perfectly with resolution so upscaling provides massive gains. using DLSS quality doubles performance to 40fps, and dlss balanced gives 60fps on average, performance about 70-80 or 4x native 4K, that includes the 10-15% performance gain RR gives as well over the native denoiser as well.  I've been playing with about 60fps on average 50-70. with DLSS Balanced and RR and its been amazing. I don't like frame gen tho since it causes VRR flickers on my screen",Positive
Intel,"Running Ray Tracing or Path Tracing without DLSS or Ray Reconstruction is like intentionally going into a battlefield without any gear whatsoever, it's absolutely pointless and suicidal, what we can clearly see here though is top of the line 7900 XTX losing against already mediocre mid-range 4060 Ti by over 50%, which is just beyond embarrassing for AMD Radeon.  All this says to me is AMD Radeon need to get their shit together and improve their RT / PT performance, otherwise they will continue to lose more Marketshare on GPU department no matter how hard their fanboys thinks that they are pointless features, just like DLSS was back on 2020 right?  Also, with my 4070 Ti OC i can run it at average of over 60 FPS at 1440p DF optimized settings with DLSS Balanced + Ray Reconstruction without even using DLSS Frame Gen, with it on i can get over 80+ FPS",Negative
Intel,Running this way means you lose RR…why in the world would you run at native 4k?  It’s completely pointless now with RR.,Negative
Intel,Im having 80 fps  thanks to dlss 3.5  and its looking better than ever,Positive
Intel,"Why would I run native? I have amazing DLSS, ray reconstruction for huge image gains and frame gen. Nvidia offering all the goodies.",Positive
Intel,"I have a 13900KF-4090 rig and a 7800X3D-7900XTX rig. They are connected to a C2 OLED and to a Neo G9.  I've been holding on to play the game till 2.0 update. I've tried many times with different ray-tracing options and they all look good and all. But in the end I closed them all, turned back to Ultra Quality without ray-tracing and started playing the game over 120FPS.  This is a good action fps game now. I need high fps with as low latency as possible. So who cares about ray-tracing and path-tracing.  Yeah ray-tracing and path-tracing are good. But we are at least 2-3 generations away for them to become mainstream. When they are easily enabled on mid-range gpus with high-refresh rate monitors, they will be good and usable then :)",Positive
Intel,"If you have the HW, go all out. That's why we spend on these things.  I'm getting the best experience you can get in the premiere visual showcase of a good game.   It's path tracing. It isn't cheap but the fact that we have DLSS and FG with Ray reconstruction is a Godsend. It looks stunning and it's still early in development.",Positive
Intel,This doesn’t seem right…. 3080 performing worse than a 2080TI?,Negative
Intel,How tf is a 2080 ti getting more fps than a 3080?!?,Neutral
Intel,I'm not seeing the RTX A6000 on here...,Neutral
Intel,4 fps for 7900 XTX  Oof. Shows you how much of a gap their is between AMD & Nvidia with pure ray tracing.,Neutral
Intel,"Everything maxed, PT, 1440p, DLSS+RR+FG, input feels good, game looks and runs great, 100+ fps",Positive
Intel,"""Get Nvidia if you want a good ray tracing experience""  Yes Nvidia GPUs give a better ray tracing experience but is it really worth it if you are required to turn on DLSS? Imo, the more AI-upscaling you have to turn on, the worse the Nvidia purchase is.   I have a 6900XT and I will readily admit that the RT experience (ultra RT, no path tracing) at native 1080p resolution is ok, like 30-45 FPS (around 50 on medium RT settings), but if I turn RT lighting off (so RT shadows, sunlight, and reflections are still present) suddenly I get pretty consistent 60 FPS (i left my frames tied to monitor refresh rate, so 60 FPS is my max) and i can't tell the damn difference at native 1080p compared to RT medium or Ultra.   So would I spend another $400-$1000 to get an imperceptible outcome (imperceptible to me that is)? Most definitely not.",Negative
Intel,Does anyone actually play this game? Its more of a meme game imho,Negative
Intel,4.3 fps 😂😂😂,Neutral
Intel,"RTX 4090 DESTROYS 7900XTX with over 400% increased FPS, coming in at an astounding…. 19.5FPS 😫😂",Neutral
Intel,"Overclocked RTX 4090 can (there are factory OC models that run up to 8% faster than stock). A measly 2.5% overclock would put that at 20 FPS.  Native is irrelevant though, DLSS Quality runs much faster with similar image quality.",Neutral
Intel,[85-115fps (115fps cap) in 4k dlss balanced](https://media.discordapp.net/attachments/347847286824370187/1154937439354368090/image.png) ultra settings,Neutral
Intel,Lol the 3060ti is better than a 7900xtx...crazy,Positive
Intel,Why would you use it without DLSS? Upscaling is the way of the future and AMD better improve their software to compete. Future GPU's aren't going to be able to brute force their way to high frames.,Neutral
Intel,Who cares about native though.  Cyberpunk PT at 4k DLSS Balanced + FG is graphically so far ahead of any other game ever released that it literally doesn't even matter if other games are run at native or even 8K. Cyberpunk is just in league of its own.,Negative
Intel,ThE gAMe iS PoOrLY OpTImiSed!!!,Neutral
Intel,Who cares about 20 fps native? Use the tools and tricks provided by the game and card manufacturer and you are good. The whole native rendering argument can be thrown in the garbage bin.,Negative
Intel,"The 7900xtx losing NATIVELY to the 4060ti 16gb is embarrassing, that card is half the price and overpriced, nevermind Nvidia's far superior upscalers. AMD falls further and further behind in RT, sad.",Negative
Intel,"99.7fps  with everything maxed out at 4k balanced with frame gen.   64.3fps with everything maxed out at 4k balanced w/out frame gen.   29.4fps with everything maxed out at 4k native.   That’s a big difference from this chart what I’m getting vs what they’re getting. For record, I’m using a 4090 suprim liquid undervolted to 950mv at 2850mhz. Stock is 2830mhz for my card.",Neutral
Intel,"Even with frame gen and DLSS quality, Ultra+Path Tracing looks incredible. I get solid 90 FPS. Funny thing is I can't get into playing CP 2077, even with incredible graphics and a QD-OLED monitor. It's just not my type of game :-D",Positive
Intel,I was wondering why a thread like this with the name like this is in this sub. And the I see the name of the OP. And it all makes sense,Neutral
Intel,"""4090 is 4 times faster than 7900XTX""",Positive
Intel,"So? Native 4k is such a waste of compute hardware, only an idiot would use it.",Negative
Intel,People need to change their mind frame. AI is here to stay and will be a major part of graphics rendering going forward.,Neutral
Intel,I played 2.0 yesterday with 2k ultra and path tracing with 155fps (DLSS3.5),Neutral
Intel,"The 4060 Ti literally better than the 7900 XTX in RT, cope snowflakes!111!11! /s",Positive
Intel,"So Nvidia only needs to increase their RT performance 3x while AMD needs to do 14x. We are not seeing that within a decade, if that.",Neutral
Intel,Sounds like Ray tracing isn’t ready for prime if nothing can run it and you have to use AI smoke and mirrors to fake it playable. I’ll worry about Ray tracing when it’s a toggle on or off like shadows and there is no major performance hit. See you in the 9000-10000 series of Nvidia or 12000-13000 series for AMD.,Negative
Intel,Native resolution is a thing of the past. DLSS looks better than native anyway.,Neutral
Intel,Is this with DLSS + FG?,Neutral
Intel,"Ray tracing already wasn't worth it imo, that's why I saved money buying an AMD card. It's not worth the cut in frame rate that then has to be filled with upscaling that makes frames look blurry anyway, so what's the point of even using it?",Negative
Intel,"Meanwhile me using medium settings + performance dlss on 3080 ti to get that locked 120fps. I just can't go back to 60fps. The game still looks amazing, especially on blackstrobed oled.",Positive
Intel,What is even with with all these path traced reconstruction bullshit lately? Ray tracing itself isn't even that mainstream yet.,Negative
Intel,No one is using these settings.,Neutral
Intel,"Ray tracing is fun and all, but games have really improve much.. Same animations.. Same type of quest... Same listening to notes.. There have to be better ways....",Positive
Intel,"4K rendering for games like this is pretty pointless unless you're upscaling it to a massive 8K display via DLSS Performance.  Ultimately, the final image quality is going to be more affected by how clean the Ray / Path Tracing is than whether you're rendering at Native 4K or not.",Negative
Intel,"I'm honestly kind of shocked my 4070 is that far above the 7900XTX. I'd have thought it would be no where near close, regardless of ray tracing.",Negative
Intel,cyberpunk sucks don't worry about it,Negative
Intel,3080ti?,Neutral
Intel,"Yes, that is true, but what the numbers do not tell you is that if you tweak some settings, you are able to get 90 fps on that resolution with path tracing with no problem.",Neutral
Intel,I feel like the 4k resolution is here to stay for a couple years boys. Get yourself a decent 4k monitor or OLED TV and chill.,Neutral
Intel,"This is why DLSS3 and frame generation are so important. 4x the performance at over 80FPS with a relatively minor amount of silicon dedicated to AI, with only minor visual degradation.",Positive
Intel,"It's way too early for using ray / path tracing in games comfortably, maybe in 10 years it'll be different, but for the foreseeable future I'm more than happy with normal rasterization.",Neutral
Intel,Who cares tho? Its not even the way it's meant to be ran. This tech is is a synergy. No point in trying to 'flex' native at this point. A 4090 gets very high FPS and amazing visuals when doing things right.,Negative
Intel,can wait in 2030 with the 8030RTX low profile 30 watt card run this game with full path tracing in 4k 120fps lol (in 2030 a low end card will run this like butter don't down vote me because you didn't get it lol),Positive
Intel,"Seems like this tech is only around to give Nvidia a win. Also path and ray tracing doesn’t always look better. If the one true God card can’t run it, is it reasonable tech or is it a gimmick? Especially seeing how like no one has a 4090. Like a very small percentage of PC gamers have one. Most aren’t even spending $1600 on their whole build!",Negative
Intel,What body part do you think will get me a 5090?,Neutral
Intel,"Once they introduced this dlss upscaling anti-lag all these gimmicks I don't think we've seen true 4K native and almost anything lately.  This is maybe one game that does look that good but many of the games don't look that great and have some really serious system requirements when there are games that look much better and are much better optimized.  Companies are taking shortcuts, instead of optimizing they give you suboptimal and use FSR dlss all these gimmicks.  Don't get me wrong I always thought it was great for people who wanted to reach a certain frames per second someone with a weaker system to allow them to play at least the minimal at a steady frame rate.  It just seems like this is the normal now non-optimized games.  The 4090 is a beast if you could only get about 20 frames native with Ray tracing it tells you everything you need to know about Ray tracing.  It needs to be reworked that needs to be a new system I believe one real engine has it built in in a certain way probably not as good as Ray tracing directly.  To me it seems like ray tracing is hampering many games that tried to add it, too much focus on that instead of actual making a good game.  Let's daisy chain 4 4090s ..gets 60 fps.. and then house goes on fire!  Obviously I'm being sarcastic but realistically it's actually pretty sad a 4090 20 frames per second.",Negative
Intel,"Its not really surprising, raytracing is decades away from being truly mainstream. I applaud the misguided pioneers who betatest the technology, their sacrifice will be honoured",Neutral
Intel,"bro, for who are they making such intensive/badly optimised games? for aliens with more powerful pcs? i trully dont get it.",Negative
Intel,That not even proper path tracing.,Negative
Intel,And I thought that ray tracing was a needlessly huge performance drop for a barely noticeable difference...,Negative
Intel,"Can't wait to buy a $1,600 GPU to run a game at 20fps!!!  &#x200B;  Nvidia fans: i cAnT wAiT 🤑🤑🤑",Positive
Intel,Native is a thing of the past when dlss produces better looking image,Neutral
Intel,Lmfao ... where are all those nvidia fan boys who prefer eye-candy path trace over 60fps?,Neutral
Intel,Sounds like some efficient tech. Lmao. What's the fucking point if it makes the game unplayable? RT just washes out the image and looks like shit. Kills the contrast. Can't wait for this fad to end,Negative
Intel,In my opinion it's too early for Ray / path tracing it's computationally expensive and if you are using rasterization at ultra settings not only does look nearly identical but it's easier to do the work for the graphics and that gives you more fps...,Negative
Intel,"Rather than having 2-3fps, I would go and see a video of path tracing.",Neutral
Intel,"Does this fact really matter tho? Dlss in cyberpunk is known the look on par or even better than native, frame generation will further smooth it out. I can definitely see the 7900xtx being able to do path tracing at 1440p at least once amd release fsr 3 and better once they release something to now match ray reconstruction which also give a small boost in fps making path tracing even more doable.",Positive
Intel,cool idc i wont play that shit at such shit settings,Negative
Intel,Do we need this BS? Seriously? Gpu’s will end-up costing over 2 grand soon,Negative
Intel,I don't even get the hype around ray tracing most games don't have it and when they do it either makes the game look blurry or isn't noticeable unless you're in a weirdly lit area or looking at a reflection,Negative
Intel,"Am replaying it with a 6950xt and a 7700k(defo bottleneck).  All maxed out(no RT) at 1440p. If I wanted 4k I would have to enable FSR.  Its completely playable imo.   Mostly 50-60fps.  For me RT vs off vs path tracing.   The game doesn't look more real or better, it just looks different.  But ignorance is bliss.",Positive
Intel,This chart is bull. Since when is a RTX 3060 faster than a 3080. Someone needs to seriously question the source of the chart,Negative
Intel,Just goes to prove once again that Raytracing was pushed at least 4-5 gens too early as a pure marketing gimmick by Nvidia,Negative
Intel,"The Tech Power Up article says:   >Instead of the in-game benchmark we used our own test scene that's within the Phantom Liberty expansion area, to get a proper feel for the hardware requirements of the expansion  So it's very hard to reproduce their data at the moment, but I've run 3 tests with the built in benchmark (run to run variance below 1%, so 3 runs are enough) and my config is averaging 25.7 fps at 4K native with Path Tracing, so the title's premise is not correct. Still, the game is not playable without upscaling at 4K with Path Tracing, so the basis of the sentiment remains correct. Benchmark data available [here](https://capframex.com/api/SessionCollections/279f9216-bdb8-4986-a504-91363328adbe).",Neutral
Intel,RTX 4090 1600€ 19FPS 👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼,Neutral
Intel,"Yay i will get 13,8 FPS with my 4080 👍😂 Someone want to borrow some frames? I don't need them all!",Negative
Intel,The first card on this list a normal person would consider buying is a 4070. 8.5 fps lol.   Even with upscaling that's basically unplayable.   Then you have all the UE5 and path trace type demos where a 4090 is only getting 60fps with upscaling.. at 1080p lol.   We are not remotely ready for these technologies yet. They're at least one if not two console generations away. So multiple years.,Negative
Intel,"Looks like 4K is mainly just for videos, not gaming for the time being.",Neutral
Intel,How can you bring out such technology and no hardware can process it properly.  an impudence,Negative
Intel,the question is who need path traycing? i mean that cool and beautiful but 20 fps with a 1600dollar price tag that wild.,Neutral
Intel,You could just you know... disable path tracing and get 70 fps or go below 4k,Neutral
Intel,"That what I expect when a game gets built specifically to market Nvidia, but Nvidia over estimated their technology.",Neutral
Intel,"um...im running cp 2077 on a 4090 at 70-90fps, 5k monitor with path tracing/ray reconsturction on and most settings maxed out...",Neutral
Intel,Yo guys let's make this really unoptimized game then rely on hardware manufacturers to use AI to upscale the resolution and insert extra made-up frames to make it playable on a $1600 GPU lmao.,Negative
Intel,"so it is settled, Devs have zero optimization in games.",Negative
Intel,"Love how shit the 3080 was lol, even when it came out it was shit. But somehow everyone loved it",Negative
Intel,"Or, hear me out, developers could just do better. I've seen far better looking games without the constant frame hit.   Cyberpunk hasn't looked good enough to justify the frame rates from the get go.",Negative
Intel,Once again proving RT is useless,Negative
Intel,Cyberpunk w/ path tracing at max settings seems even more demanding than Crysis was at launch.   20fps with a 4090 is insane.,Negative
Intel,"""only gamers get this joke""",Negative
Intel,"People have forgotten or are too young to remember when Nvidia had hardware tessellation features when that was first emerging. They had tessellation maps under the ground and water in Crysis that were doing nothing other than tanking AMD (ATi) performance. I am not saying this whole Cyberpunk thing is exactly the same, but it's essentially similar. In all honesty, turning everything up to 11 on my 3090 doesn't net much visual gain beyond some mirror like reflections and nothing I really care about when actually playing the game (compared to standard High/Ultra) and not pixel peeping, which seems to be the new game now for people buying expensive GPUs, so they can justify their purchase. Meanwhile, everyone else just gets on with enjoying video games instead of living vicariously through someone's 4090 at LOL 1080p",Negative
Intel,Starfield is peaking around the corner.,Neutral
Intel,starfield as well,Neutral
Intel,I remember when physx was so demanding people had a dedicated 2nd Nvidia graphics card just to turn the setting on in Arkham City. Now it's considered so cheap to calculate that we just do it on the CPU lmao,Neutral
Intel,The new Crysis,Neutral
Intel,"I wouldn’t be so optimistic. Transistor shrinking is crazy hard now, and TSMC is asking everyone to mortgage their house to afford it.",Negative
Intel,How long until a $200 card can do that?,Neutral
Intel,Most improvement is probably going to AI software more than hardware in the next few years.,Positive
Intel,"8800GT giving advice to 4090:  “I used to be 'with it. ' But then they changed what 'it' was. Now what I'm with isn't 'it' and what's 'it' seems weird and scary to me. It'll happen to you!""",Neutral
Intel,GPU need to have its own garage by then,Neutral
Intel,Yep! Insane how fast things change.,Neutral
Intel,"Most likely there will be no gpu that supports path tracing and gives you native 4k 120fps in 5 years, maybe even in 10 years.  The technology has slowed down a bit. It’s increasingly more challenging to make more dense chips.  That’s why Intel has been struggling for many years already and every iteration of their cpus gives only minor improvements. AMD went with chiplets but this approach has its own problems.  Nvidia stands out only because of AI. Raw performance increase is still not enough to play native 4k even without ray tracing.",Negative
Intel,Yeah rtx 12060 with 9.5 gb Vram will be a monster,Neutral
Intel,I'm willing to bet hardware improvement will come to a halt before that.,Neutral
Intel,"In 10 years AI based upscaling will be so good, no one will want to natively render unless they are generating training data",Negative
Intel,10 is too much. Give it 5.,Neutral
Intel,Transistor density advancements have been declining for a good while now. We can't expect hardware performance gains of old to continue into the future,Positive
Intel,Like the 1080 barely doing 4k30 and now we have gpus that do 4k120 id way heavier games.  Its still weord to me to see 4k120,Negative
Intel,But then the current gen games of that era will run like this. The cycle continues,Neutral
Intel,TRUE! i think 5-10 years was the actual point in time where anybody should have paid their hard earned dollar for raytracing gpus. instead ppl dished out $1000s for the RTX2080/Ti and now are sitting on them waiting for raytracing to happen for them xD,Neutral
Intel,Who knows what new tech will be out in even 4 years lol,Neutral
Intel,"And that’s when I’ll turn on this setting in my games. Fuck having to pay $1600 for a sub 30fps experience. Path tracing is far from being a viable option, it’s more like a proof of concept at this point.",Negative
Intel,"You wish, GPU makers have stagnated like it's crazy, even the graphics have peaked. I expected to see CGI level graphics like Transformers now",Negative
Intel,"...but will cost your everlasting soul, body, firstborn and parents. If it's on sale.",Neutral
Intel,It is path trancing though. The technology used by Pixar to make Toy Story 4 (though they spent up to 1200 CPU hours for one frame) Path tracing used to take up to a day per frame for films like the original Toy Story. And they had their supercomputers working on it. It is a miracle of modern technology that it even runs real time.,Positive
Intel,"You basically need a 4090 to crack 60 fps at 1440p w/ dlss on quality without frame gen. It looks good, but not good enough to run out and buy a 4090.",Negative
Intel,"Same card, I just turned off RT at 4K. 75-120fps is better than 40 with muddy but accurate reflections",Positive
Intel,"The 7900xtx is one of the worst gpus iv ever had lol on paper it looks so so good, In games it just play isn’t. Fsr is garbage, 4080 mops the floor with it :( plus the drivers from amd besides starfield have been awful this generation",Negative
Intel,I think that most of the progress will go together with software tricks and upscalers.,Neutral
Intel,"lol. And you missed the 2080Ti.   Every result under 10 fps is just to be ignored, it isn't representing anything outside of ""woot, the card managed to chuck a frame our way"".",Negative
Intel,That's VRAM for you,Neutral
Intel,we are counting decimals of fps its just all margin of error.,Neutral
Intel,If you buy a card with low ram that card is for right now only lol,Neutral
Intel,Wake me up when a $250 GPU can run this at 1080p.,Neutral
Intel,Well DLSS isn't best. DLAA is,Negative
Intel,Reviews have been saying that the game looks better with DLSS than native. Not to mention runs extremely better.,Positive
Intel,Can’t you just disable TAA? Or do you just have to live with the ghosting?,Neutral
Intel,TAA is garbage in everything. TAA and FSR can both get fucked,Negative
Intel,"Yeah. 70 to 100fps on a 4080, but with 3440x1440 and DLSS-quality-FG-RR (nvidia needs new nomenclature....)",Neutral
Intel,"same, high refreshrate at 4k with optimized settings + PT + FG. With a 4080 of course, it's insane that it can look and run this great.",Positive
Intel,"Not exactly. Path tracing is just ray tracing, but cranked up to 99% with the least amount of rasterization possible.",Neutral
Intel,"> Give it 2 gens and you are going to get 4k60 here.  Assuming the 5090 literally doubles a 4090 (unlikely), that only gets us to 4K 40hz.  Assuming a 6090 doubles that, 80. which won't be bad.  Going with more conservative 50% boosts. 5090 will give 30. 6090 will give 45.  And i feel like 50% is being very generous, as nvidia have claimed moores law is dead and they can't advance beyond a 4090 by much. I'd guess we get 30% uplift in 5090 and maybe 10-15% uplift in 6090. So we'd still be under 4K30.",Neutral
Intel,Imagine buying a 4090 and then using upscaling.,Neutral
Intel,Path tracing is so much more demanding than ray tracing due to light scattering being modelled. It is a marvel it even runs.,Positive
Intel,PC gaming is in Crysis.,Neutral
Intel,Well it's a good thing 99.999999% of PC gamers don't give a shit about ray or path tracing.,Negative
Intel,You're the first other person other than myself I've run into that had an XTX but ended up with a 4090 in the end.,Neutral
Intel,"Plus frame generation works very well in Cyberpunk in terms of image quality. In some games, you need to get closer to ~80 fps output for an acceptable image quality with FG. But the FG in CP2077 is decent with 60 fps output, ~~and I get ~65-70 fps output with quality DLSS + FG at 4k on a 4090.~~ EDIT: I misremembered what I was getting. With path tracing, DLSS quality, frame generation, and ray reconstruction, I got 80.1 fps with the benchmark!  Of course there's the matter of latency, and the latency of CP2077 with FG output of ~65-70 fps isn't great. So I'll often use DLSS balanced + FG. Thanks to ray reconstruction, this now looks very close enough native 4k (to my eyes), with acceptable latency (to me), at a high framerate output.",Positive
Intel,"Nvidia features are always useless until AMD copies them a year or two later, only then they become great features 😁",Positive
Intel,"Tbh, list the games that have ray tracing right now. Pretty few.  It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it). I would personally go amd over nvidia because those 50 or so euros more that nvidia has against the amd counter parts simply are too much for just better rt and  dlss. I could spend those for a better amd card with generally better performance.  Regarding dlss, personally I would just lower the graphics before resorting to kinda bad generated frames, fsr even worse. But that's my point of view.  I find that amd still has to fix some driver issues but other than that, they are a fine brand. (so is nvidia)",Neutral
Intel,Yeh because native 4k looks worse than dlss + RR 4k,Negative
Intel,"Yeah the game with DLSS, RR and path tracing at 1440p looks amazing and with very high fps",Positive
Intel,What's the point of having $5000 PC when you're still gonna have literally the same graphics as $1000 PC then?,Negative
Intel,"This is a tech demo. That's the whole point. It's not really playable yet, but the game really is meant to showcase what is possible in the future and how close we are getting. That's what Nvidia is doing here by funding this whole project.  Crysis who many people are comparing this to, was in itself quite revolutionary for its time. The destructible environment in Crysis to this day holds up, and that was it's killer feature really.  You're gonna have swings at the future that miss as well, and that's ok.",Positive
Intel,"Did you try DLSS?  Imagine getting down voted for asking if he was using DLSS. What a fragile community. Sometimes it's okay to use DLSS, other times you better not or people will come to your house with torches",Negative
Intel,VRAM,Neutral
Intel,"Depends on what you are after i guess, is it worth it for me? Absolutely, DLSS+FG+RR with PT is a great experience.",Positive
Intel,still better than 90% of games in 2023,Positive
Intel,Arguably better now Ray Reconstruction has been added. It's quite a big image quality upgrade.,Positive
Intel,"…no it does not, unfortunately. Have you seen the artifacting in Cyberpunk?",Negative
Intel,"Someone ate up the marketing. I often see people wonder why games are so unoptimized today, your answer is right here. \^",Negative
Intel,All graphics you see on your computer screen is fake,Negative
Intel,Path tracing is more demanding than Ray tracing,Neutral
Intel,"That's why I downscale from 1440p to 1080p, running DLSS and using Contrast Limited Sharpening. It looks better than native resolution and still performs well.",Positive
Intel,I guess between the 3090 and the 4070,Neutral
Intel,Yep hahaha,Neutral
Intel,RemindMe! 7 years,Neutral
Intel,"No, this tech is around so game developers can sell more copies of their games by making them look better. The only reason why Nvidia is winning is because their GPUs are much better at intensive ray-tracing.",Positive
Intel,"This is an absurd, fanboyish thought lmao",Negative
Intel,dont let novidia marketing see this youll get down voted into oblivion,Negative
Intel,I would still argue it's still in proof of concept stage. The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native. That will the allow the lower end GPU'S to get 4k 60 or 1440p 60 upscaled.,Neutral
Intel,I mean I have no problem playing at 30fps dlss balance at 1080p DF optimized settings on a 2080ti with pathtracing idc what anyone says RT/PT is the future it just looks so much better than Rasterization,Positive
Intel,"Possibly a little more. Assuming that the leaked 1.7x improvement is right and path tracing performance scales the same, that would be about 34 fps at 4k native. Might be possible to hit 60fps or close to it with DLSS Quality.",Neutral
Intel,It could as well do 60fps if Nvidia adds a lot of path tracing HW into the GPU.,Neutral
Intel,Fanboyism of both kinds is bad,Negative
Intel,"It won't end, but hopefully we see games that have art designed with RT lighting and reflections in mind.  Cyberpunk isn't it. We'd need a full rebuild of the game, every location, every asset and surface remade, so it looks like originally intended by the artists.",Neutral
Intel,Since it needs more than 10gb vram,Neutral
Intel,Cyberpunk is more optimized than majority of games that came out in last two years. Just because you can't run it at 4k 120 fps with path tracing doesn't mean it's not optimized.,Negative
Intel,It’s not unoptimized at all. Game runs great for the quality of the graphics. You guys just don’t understand the insane amount of power path tracing takes. The fact you can do it at all rn is insane but once again people just throw “unoptimized” at it because they don’t understand the technical marvel happening in front of their eyes.,Negative
Intel,"No, this game is actually very well optimized. It's just extremely demanding at 4k native, which is why DLSS exists.  It would be unoptimized if it ran like this while looking like nothing special, but this is probably the game with the best graphics, period. At the very least, it's certainly the game with the best lighting.",Positive
Intel,"and people will be like ""well RT is the future""  raster was fine for over 15 years and raster games look great but we get it we must let incompetent game devs and their bosses make shit quality games because average gamer is ready to spend $100 to pre-order electronic version of horse shit and then justify it for years because they can't admit they wasted money",Negative
Intel,It's settled that you have no idea what you talking about,Negative
Intel,Its full path tracing u cannot really optimize this much.,Negative
Intel,"It always did. Most games don't scale well, Cyberpunk does.  Look at the last two years of pc ports and you will understand how cyberpunk looks and performs like it's black magic.",Neutral
Intel,"Baked illumination requires you to choose where to add light sources and how the lighting is applied to everything, it's a creative choice of the author, the AI needs instructions to know what to do, so it would need constant handholding, on the other hand RT uses simple physics to calculate how light bounced and NN to apply the light on surfaces.",Neutral
Intel,But the destructable environment and the water graphics and other things in Direct X 9 made high end pcs kneel as well.  I remember playing on my 9800gtx/+ with my Intel Q9300 quad core (lapped and oc'ed to 3.0ghz - EDIT: checked some old evga posts got it to 3.33ghz) with 2x4gigs DDR2 @1000mhz cas 5 trying to maintain a sustained 30fps at 900p resolution on my 1680x1050 monitor. And I oc'ed the crap out of that 9800gtx 835 mhz (cant recall if it was unlinked shadder or not now) core on blower air (won the silicon lottery with that evga card).  Tweaking settings was mostly user done and guided with old school limited forum help. Ahh the good old days of having lots of time during school breaks.,Neutral
Intel,"The equivalent of a 4090 at the time, 8800 ultra (768MB) got 7.5 fps at 1920x1200 very high quality. The other cards were not able to run it. Even the lowest tier last generation card runs this, even though it's at 5% speed.",Neutral
Intel,No and no again lol  The 8800gtx could do 1152x864 high and that was the top dog. DX9 cards could do that at medium but high shaders was like a 25fps average tops and that dropped into the teens on the snow level.   It took things like the gtx480 and HD 6970 to do 1920x1080 30fps max settings. That's before getting into Sandy Bridge being the first cpu that could sustain 40+ fps if the gpu power was there.   Crysis came during the core 2 duo/k8 athlon and first wave dx10 cards era and it took *2 more generations* of *both* gpus and cpus to do it some justice.,Neutral
Intel,"My 8800ultra was getting single digit fps at 1080p ultra, so CP isn't quite as demanding.",Neutral
Intel,Amd had a tessellation unit. It went unused but it was present,Neutral
Intel,"My fun story for PhysX was Mirrors Edge. I don't remember what GPU I had at the time but the game was pretty new when I played it. Ran fine for quite some time until one scene where you get ambushed in a building by the cops and they shoot at the glass. The shattering glass with PhysX turned on absolutely TANKED my framerates, like single digit. I didn't realize that the PhysX toggle was turned on in the settings. This was at a time when PhysX required a dedicated PCIe card in your system.   Once I turned it off it ran fine. Now I can run that game at ridiculous framerates without my system getting warm.",Positive
Intel,"I remember around about 2008 when companies like Asus and Dell were selling ""Physics Processing Units"" and some claimed that these would be commonplace in gaming machines just like Graphics cards had become 10 years previously.",Neutral
Intel,"Hardware accelerated physics (as in on the gpu), is different than what's going on the CPU.",Neutral
Intel,hobbies quaint consist aromatic political hat aback boat relieved crush   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"People used to have ATI/AMD for main and a lower-end NVIDIA for PhysX.  When NVIDIA found this out they pushed out drivers that disabled PhysX on their cards if an ATI/AMD card was detected, limiting you to the intentionally piss-poor CPU implementation of PhysX.  Think about that crap.  One day everything's going fine for consumers, the next day NVIDIA decides they don't like how consumers are legitimately using their cards and gimps everyone, weaponizing a physics engine company that they bought in 2008.",Negative
Intel,"Oh damn, I forgot about those cards.  I wanted one so badly.",Negative
Intel,Remember when companies tried to sell physics cards lol,Neutral
Intel,>PhysX  It never was a dedicated Nvidia card - it was a [dedicated psysx](https://www.techpowerup.com/review/bfg-ageia-physx-card/) on which the tech later was bought by Nvidia and implemented in it's own GPU's.  But the things never became really populair.,Neutral
Intel,"This makes me wonder if we'll ever see something similar with Raytracing, where we get a 2nd GPU purely for RT, and then the main GPU just does all the other stuff. Would that even be possible?",Neutral
Intel,I remember when people had a dedicated PhysX card.,Neutral
Intel,The ray/path tracing in this case done by specialized hardware which has more room to grow faster.,Neutral
Intel,"I would, transistor shrinking isn't the only method of increasing performance, and honestly, these companies have to keep putting out better cards to make money.  There have been many breakthroughs over the last few years. I give it another 5 as both amd and nvidia are pushing ai accelerated ray tracing on their cards, nvidia is in the lead for now but amd will eventually catch up.",Positive
Intel,There is still a big leap possible since all lithography processes at the moment are hybrid euv and duv.   But the moment everything is done euv things will drastically slow down.,Neutral
Intel,"No those are just pennies in the pocket of Nvidia, but as the consequence you as the customer need to take a mortgage on a brand new GPU.",Neutral
Intel,"Yeah the next 10 years are gonna be spent getting budget cards up to 4090 level performance, rather than bringing the bleeding edge to 5-10x the 4090 performance. As long as 4K is king, there’s not much incentive to do the latter.",Negative
Intel,"In 10 years we will be begging one of several thousand test-tube created Musk Family members for $200 so we can buy a cheeseburger.  But the jokes on them, we’re just gonna spend it on space crack.",Negative
Intel,"Never. Even if performance can be pushed that far, by the time it happens there won't be such a thing as a $200 graphics card anymore.",Negative
Intel,It's not happening unless the market crashes and they start focusing on offering good price/performance cards instead of bumping up prices every generation.,Negative
Intel,25 years,Neutral
Intel,"The software needs to run on hardware, right now it eats through GPU compute and memory.",Neutral
Intel,And sadly ended up with 450 Watt TDP to achieve that performance.,Negative
Intel,This. We'd be lucky to see more than 3 generations in the upcoming decade.,Neutral
Intel,"Having seen a few newer games on relatively low resolution CRT display, I can't help but think it might come down to improved display tech and embedded scaling. Like DLSS3 features in the display instead of the GPU.",Neutral
Intel,Not with that attitude,Negative
Intel,Love how it's still gimped on memory size 😂,Positive
Intel,"They just need to render the minimum information needed , and let the ai do the rest",Neutral
Intel,"Don't get upscaling, to me it looks shite. Why spend hundreds or thousands on a GPU to not even run at native resolution.   It's something Nvidia are pushing because they've got a better version of it at the moment.  Maybe it'll improve, it's pretty much snake oil currently imo.",Negative
Intel,"Feels pretty good with 120 frames. Playing with a controller, i don‘t mind or even feel the input lag on a projector screen.",Positive
Intel,"It's mad that even with a 4090 running games at 4k, they still look less realistic than the low-res FMV sequences in The 7th Guest from 30 years ago.",Negative
Intel,"Do keep in mind that despite both being named the same, the devil's in the details. Movies use way more rays per scene and way more bounces too.   Path tracing in CP2077 shows temporal artifacts due to denoising, something that doesn't happen in movies. It is being improved with the likes of DLSS 3.5, but it is still quite off when compared to said movies.",Neutral
Intel,"Keep in mind the systems that rendered toy story 1, costing upwards of like 300k IIRC, have less power than your cell phone today and were super delicate/finicky machines. There's a dude on youtube that got a hold of like the second best one that was made at the time and the machine honestly was really impressive for when it came out, but it pales in comparison to even a steam deck really.",Neutral
Intel,"> Path tracing used to take up to a day per frame for films like the original Toy Story  Toy Story didn't use path tracing though, A Bug's Life was Pixar's first movie to use ray tracing (not path tracing) and only for a few frames in the entire movie for specific reflections, they started using ray tracing more generally for Cars and I can't find exactly when they started using path tracing but it should be around the early 2010s which is also when the other Disney animation studios started using path tracing",Neutral
Intel,No it's not a miracle. Because it's downscaled a lot and also uses lower quality math and piggybacking on AI to make up for it. It's basically running through 3 layers of cheats to produce results that aren't that much better than just traditional raster.,Negative
Intel,"That’d be some next level consumerism, paying 1500$ minimum to turn on a single setting in a single game just to play it wayyyyy slower than you would otherwise",Negative
Intel,"Hell playing path tracing with my 2080ti at 25 FPS is still looks absolutely fantastic, I would absolutely play with pathtracing on a 4090 constantly. Idc dlss looks great with RR even at 1080p. I won’t upgrade for another year or 2 (just bought a phone so I’m broke right now)",Positive
Intel,4090 getting 60fps at 4k balanced dlss with no frame gen. 100 with frame gen. I can make it drop into the 20s if I stand right next to a fire with all the smoke and lightening effects or if I go to a heavily vegetated area it’ll drop to mid 40s. But it’s stay consistently at 55-65 and even goes Into the 90s if I head out of the city.   Haven’t tried it at quality or with dlss off though. May go do that now that it says only 19fps lol. Have to try it to see for myself,Neutral
Intel,"https://www.tomshardware.com/news/nvidia-dlss-35-tested-ai-powered-graphics-leaves-competitors-behind  Well it's so close (54fps) it's more like a 3080Ti or higher from 3000 series or a 4070TI + from 4000 series it seems? The old 3000 series is punching way above it weight vs the 7900xtx, which was meant to deliver similar RT performance to the 3090.. which it doesn't.",Neutral
Intel,At some point the RT pixels are so expensive that native resolution w/o DLSS and frame gen is just not gonna work for the time being.,Negative
Intel,"On the other hand, if you set the new DLSS 3.5 to performance (which you should in 4k), and just enable frame generation, you get 90+ in 4k with basically zero issues unless you pause to check frames.",Positive
Intel,So rendering at 960p? Oof...,Neutral
Intel,"you take that back, I literally returned my 4080FE for a 4090 liquid x for more frames in cyberpunk.",Neutral
Intel,I almost see no difference.   And to see the little diffidence I need to take two screenshots and compare them.,Neutral
Intel,The game look is ruined on PT it makes everything look completely different.  Regular RT keeps the style of the game and performs good.,Positive
Intel,Dont use useless raytracing and you wont have any problems lol,Negative
Intel,"Because PC gaming blew up during a time where you could buy a mid range GPU and not need to upgrade for 5-6 years. Now those sample people buy a GPU, and 2 years later it can't run new games. At least that's my theory.",Negative
Intel,AW2 looks insane. Can't wait to play soon,Positive
Intel,Well the 3050 managed to hold with just 8GB while the 3070Yi crashed?,Neutral
Intel,"Yeah, and people insisted on defending the configurations at launch lmao. The cards just won't be able to handle heavy loads at high resolution such as this game, regardless of how fucking insane the actual processing unit is. You can't beat caching and prepping the data near the oven. Can't cook a thousand buns in an industrial oven at the same time if there's trucks with 100 coming only once an hour.",Negative
Intel,My 3080 in shambles,Neutral
Intel,"It looks better than native even with fsr quality, the Taa in this game is shit",Negative
Intel,You dont need to increase raw performance. You need to increase RT performance.,Neutral
Intel,Imagine!,Neutral
Intel,DLSS unironically looks better than native TAA even at 1080p in this game because of the shit ghosting.,Negative
Intel,Only people who don't give a shit about high quality graphics don't care about ray tracing.,Negative
Intel,I got a stupidly good deal on the 4090(buddy won an extra one from a company raffle) and it just so happened my XTX was one of the cards suffering from the mounting issue on release.  Honestly very happy that all of that happened. Sure I would have been happy with the 7900xtx if I got a replacement but the 4090 just kinda blew me away especially with all the DLSS tech behind it.,Positive
Intel,"Nvidia used cp77 as a testing grounds for their dlss3 and rt tech, it'd no surprise it looks and performs relatively good, they literally partnered with cdpr for that.  Do not expect these levels of RT on most games anytime soon, probably in a couple years (with rtx 5000 series) the tech will be more mature and not as taxing for most cards, because needing to upscale and use framegen to get 60fps on an RT PT setting is kinda absurd.",Positive
Intel,">only then they become great features  Watch this happen with ""fake frames"" FSR3 in real time",Negative
Intel,>Ray tracing as of right now is a gimmick   This line was already false years ago when control launched. Now it’s just absurd,Negative
Intel,"There are a lot of new games that features Ray Tracing on them as of now, too bad most of them doesn't look as visually as good as they could be though as most of the time their RT effects are dumbed down or reduced to the point it's pointless.  But the thing is that wasn't the point even from the beginning, RT Overdrive on Cyberpunk is literally considered as a Technological showcase  What matters here IMO for all of us is PC platform in general is showcasing here what a real high-end computers can achieve beyond what current gen console can do, and they are showcasing it really well here on Cyberpunk with much better visuals and using plenty of tricks to make them more than playable, where they shouldn't be considering how much demanding they are over standard, that is impressive enough to me and shows a good sign of technological engineering innovation.   As what Bryan Catanzaro said on his interview on DF Direct, work smarter not harder, brute force is not everything, of course it still matters a lot but doing it alone is starting to become pointless just like how it is on real life.",Negative
Intel,Much more fps :) Newer generation cards have at least 50-60% better raster performance then prev gen.,Positive
Intel,The Evangelical Church of Native,Neutral
Intel,"So just to be clear, you don't like running games at native resolution? Because the purpose of DLSS is to improve performance by specifically not running at native resolution",Negative
Intel,Nice but ive been playing video games since the 70s and its Shit end off..,Negative
Intel,The future of frame generation is going to be huge. I can definitely see 360p frame rendering upscaled to 4k with 80% maybe even 90% of the frames being generated. All with equal to or even better visual quality.   A 20 FPS game like this will become a 100-200fps game.  We may be looking at the end of beefy graphics cards being required to play games. Just a good enough mid or low tier chip (granted 5-10 years from now) with frame generation might be enough.,Positive
Intel,"After 2.0, it's around 100""fps"". Just 60""fps"" (meaning with FG enabled) is far from being playable due to latency. You need at least 90""fps"" for playable experience.   Cyberpunk PT vs RT is a huge difference in terms of graphics, while DLSS Quality vs Balanced is really slim in comparison. What you say would be a really bad trade-off.   At 1440p DLSS Quality it's easily 120+""fps"" which is perfectly comfortable.",Neutral
Intel,Cyberpunk is greatly optimized. It looks fantastic for the hardware it asks. Starfield looks like game from 5 years that asks hardware from the future.,Positive
Intel,"Yeah, sure, now go back to play starfield.",Neutral
Intel,"There literally is a /s, what else do you need to detect sarcasm?",Neutral
Intel,"Someone made a really cool comment yesterdays on Reddit that even a generated frame using path tracing is less fake than a rasterized frame. It is effectively closer to reference truth. I had never thought about it that way, but people really are clutching their pearls with this shit",Negative
Intel,"I know, which makes path tracing even worse off imo.",Negative
Intel,I will be messaging you in 7 years on [**2030-09-23 15:26:41 UTC**](http://www.wolframalpha.com/input/?i=2030-09-23%2015:26:41%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Amd/comments/16pm9l9/no_gpu_can_get_20fps_in_path_traced_cyberpunk_4k/k1v42p3/?context=3)  [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FAmd%2Fcomments%2F16pm9l9%2Fno_gpu_can_get_20fps_in_path_traced_cyberpunk_4k%2Fk1v42p3%2F%5D%0A%0ARemindMe%21%202030-09-23%2015%3A26%3A41%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%2016pm9l9)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,Neutral
Intel,haha this is gold 🥇,Neutral
Intel,"At the moment, it’s definitely what it seems like.  Especially seeing how there’s only one card on the planet that can run this. Do you have a 4090?",Neutral
Intel,">I would still argue it's still in proof of concept stage.  Not at all, in fact another AAA game with ""path tracing"" is coming out next month.  >The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native  Forget ""native"", that ship has already sailed. Nvidia, AMD and Intel are all working on machine learning techniques, and several game developers are starting to optimize their games around DLSS/FSR.",Neutral
Intel,"Nope, not even close.  Shaders 3.0 - that's improvement. Ray tracing is noticeable, but nowhere near to justify the hit.  I've looked techpowerup comparison RT vs LOW (lol). Low looked far better and cleaner, then blurry mess of RT (Overdrive).   I don't need ""realistic"" lighting, I need pleasent bisual part with good gameplay. For ghraphics I have a window in my appartment.",Neutral
Intel,"In a screenshot, sure. When actually playing the game, unless the fps is low enough to remind them, 99,99% of people will forget if they left it on or not.",Negative
Intel,Pixar movies will sometimes take the better part of a day to render a single 4k frame on a render farm. So the fact that Cyberpunk takes more than 0.05 seconds to render a 4k frame in its path-tracing mode on a single 4090 clearly means that it's unoptimized garbage! /s  EDIT: Apparently people in this thread don't understand sarcasm (or don't understand how a path-traced game can take longer than 0.05 seconds to render a 4k frame with high-poly path tracing and still be optimized).,Negative
Intel,Man you had to lap your q9300 to hit 3.0ghz? Those chips really weren't OC friendly.  My q6600 hummed along 3.2 no problem on 92mm arctic freezer and a slight voltage bump.  I was so pumped when I upgraded to a gtx 260. Crysis was a dream.,Positive
Intel,Funnily Crysis still have better destructible environments than Cyberpunk has tho,Positive
Intel,"This. People losing their shit either weren't around when Crysis launched or have forgotten just how demanding it was . PT CP kicks the shit out of a 4090, Crysis murdered my 8800ultra.",Negative
Intel,crysis did not do 1080p 60 on maximum setting and dx10 on a 8800GT.   I had a 1280*1024 monitor back then and barely had 30 fps with my 8800GT.   Even the 8800 Ultra did not do 60 fps on full hd.  https://m.youtube.com/watch?v=46j6fDkMq9I,Negative
Intel,"Man, I remember how hype the 8800gt was. Thing was a hell of a big jump compared to previous cards that came out, prolly our first REALLY powerful card.  Still remember the old xplay video where they build a PC to play crysis and Bioshock. Put 2 8800GTs in sli in there with a core 2 quad which costed one thousand dollars back then!",Positive
Intel,"My poor HD 4850 got pushed to the limit to give me 1280x1024, lol",Negative
Intel,I remember thinking $300-400 (IIRC) was so much for a GPU back around those days.  7800XT is the closest we've seen in a while and not even close to the top end for today.,Negative
Intel,This is still the case to this day because the game includes a really old DLL file for PhysX. The other day I followed the instructions on the PCGamingWiki to delete some DLL files in the game directory and only then it ran perfectly smooth on my RTX 3070.,Neutral
Intel,LOL I remember this exact scene also.,Positive
Intel,"CPU PhysX back then was single-threaded and relied on ancient x87 instructions if I recall correctly, basically gimped on purpose. Even with a 5800X3D the shattering glass reduces the frame rate to the low teens. Sadly an Nvidia GPU is still required if you want to turn PhysX effects on for games from that era, though I hear that it's possible again to use it with an AMD GPU as primary.",Negative
Intel,"OMG I had the exact same experience, hahaha I remember it vividly, I was so confused why that room would just destroy my FPS until I figured out PhysX was enabled LOL",Negative
Intel,"I'd like to see ray tracing addon cards, seems logical to me.",Neutral
Intel,"TBH, I could probably run some of the old games I have on CPU without the GPU.",Neutral
Intel,glorious squash wild file crawl ancient crowd racial soft north   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,You could probably run Mirror's edge with physx on today's hardware without gpu fans turning on,Neutral
Intel,Last time I tried on an R7 1700X and RX580 I still couldn't turn on PhysiX without it being a stutter party 2 fps game.,Negative
Intel,What gpu you have,Neutral
Intel,Now it even runs fine on a Ryzen 2400G.,Positive
Intel,"And they were right, PhysX and systems very much like it are still used but things have advanced so much nobody even thinks about it and it no longer requires dedicated silicon.",Neutral
Intel,"Yeah, it’s been common knowledge for many years now that Nvidia are the most ruthlessly anti-consumer company in PC hardware, and it’s not particularly close.",Negative
Intel,Ah good old Ageia before nvidia bought them out https://en.wikipedia.org/wiki/Ageia,Neutral
Intel,"No, you could actually install two Nvidia cards and dedicate one of them to only PhysX.",Neutral
Intel,"It would certainly be possible, but it wouldn't really make sense. Splitting it up on multiple GPUs would have a lot of the same problems that sli/crossfire had. You would have to duplicate memory, effort, and increase latency when you composite the final image.  It may or may not make sense to maybe have a raster chiplet and a ray tracing chiplet on the same processor package on a single gpu. But, probably makes more sense to have it all on one chiplet, and just use many of the same chiplet for product segmentation purposes instead.   A separate PPU did make sense tho, I'm still annoyed that the nvidia ageia deal essentially kill the PPU in the cradle. Our gaming rigs would cost more if PPUs became a thing, but we could have had a lot better physics then we do today. There is still a revolution in physics to be had some day...",Negative
Intel,would be cool if 2000/3000 series users could get a small tensor core only PCIe card to upgrade to framegen,Positive
Intel,"That's true, but if he's talking about *the equivalent* of a current $200-class card, I'd say about it's 10 years, what do you think?",Neutral
Intel,At least with the 4090 you can run 70% power target and still hit insane FPS while only pulling around 300w which is the same as my old 3080. The gains with that extra 150w are a couple percent at best. Not worth it to me.,Negative
Intel,Intel design will be a little bit different as far as now.  If I understood it right AMD chiplets communicate via lines on pcb but Intel wants to make something like chip-on-a-chip.,Neutral
Intel,bedroom ring library cows summer thought aspiring worm north joke   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"It's also worth noting that those early movies don't use path tracing either, Pixar switched to PT with Monster University around 2013 IIRC.",Neutral
Intel,"There is a lot of room for improvement, both in the software and future generations of hardware.  It's coming along though!  Overdrive mode looks nice, but there's just a lot more ghosting than the regular RT mode.",Positive
Intel,"They are still the same thing and should be named the same, your disclaimer is just semantics about the implementation.",Neutral
Intel,"Weird argument.  Traditional raster is cheats upon cheats upon cheats already.  In fact, all of the lighting entirely in raster is essentially cheats.",Negative
Intel,It will not work well/look good using a single frame worth of data either due to the low ray counts. Digital Foundry has shown Metro Exodus builds the lighting over ~20 frames or so and it seems like Cyberpunk is even more. Even Cyberpunk ray pathtracing doesn't look as good in motion due to that same build-up effect.  You need to cast 20-50x the rays to have enough data for a single frame but then you will be measuring the game performance by seconds per frame.,Negative
Intel,"RT looks significantly better than raster when it’s not just using simple reflections like a Far Cry 6. Raster is all cheats, the so called reflections are essentially rendering the scene or part of the scene being reflected in reverse then applying fake material effects to mimic metal, water, etc. Raster also takes HUNDREDS to thousands of person hours to get the fake reflections and lighting to look decent to good. That’s why devs love RT and want it to grow, as RT/hardware does the work for them.",Negative
Intel,It's significantly better than raster. It kills fps but the quality is great,Positive
Intel,Better graphics needing more expensive hardware is hardly a hot take.,Negative
Intel,"Yes, better graphics costs performance. SHOCKING",Positive
Intel,People do it!,Neutral
Intel,It’s not way slower. I get 110 FPS at 4k with all DLSS settings turned on and honestly it’s insane.,Negative
Intel,"I shamefully admit I bought a (inflation adjusted) $400 console in the past to play one game. That's marginally better than buying a GPU for one setting in one game, but still.",Positive
Intel,Sounds like most nvidia fanboys,Neutral
Intel,"If you're into the eye candy, I can see a lot of people that have the money doing it.  Not everyone though. That's around a mortgage payment for me.",Neutral
Intel,And yet people will tell you how nvidia is mandatory. Like you want to overspend to play 1 game with shitty fps? I don't understand these people.,Negative
Intel,Nvidia fanboys being happy with fake frames and fake resolution will never not be funny to me.,Negative
Intel,What? I thought amd was always more tuned to raster as opposed to reflections,Neutral
Intel,Which is why nvidia is rabidly chasing AI hacks,Neutral
Intel,This and rumors are saying that the 5090 is gonna be 50-70% faster than 4090 which wont be enough for native 4k either.,Negative
Intel,Say that to the people playing upscaled games at 4k (540p) on PS5.,Neutral
Intel,"I understand some RT effects might not catch everyones eyes, but seriously, path tracing cyberpunk vs 0 RT cyberpunk IS a big difference",Neutral
Intel,Huh? [Watch this is in 4K on a big screen.](https://www.youtube.com/watch?v=O7_eHxfBsHQ&lc=UgyO2vRiSI6CRU5kYmV4AaABAg.9uyI5P8oamz9uylTN_rdmL) The differences are really apparent in each version.,Neutral
Intel,"PT looks way better in the game to me - it makes everything in the environment have a more natural look to it.  It adds, not ruins it...",Positive
Intel,"That's how I feel as well. It looks awesome don't get me wrong, but it doesn't look like cyberpunk to me.",Negative
Intel,It's not a perfect way of measure but you can clearly see how (at least on Nvidia GPUs) the 8/10GB cards are way behind the >11Gb cards. Meaning you need 11 or 12GB of VRAM for this scenario which cripples the 3080 but not the 3060.  We said from the start these configurations are shit but no-one listened. There you go.,Negative
Intel,The crypto miners did me a comical solid by preventing me from acquiring many countless times and hours I wasted (came from a gtx1070 before finally being able to upgrade). Was able to get my 6900xt eventually for around $600 with 2 games. Becoming increasing thankful for the extra Vram these days.   Once I start getting through my game backlog and into the gard hitting ray tracing ones will hopefully upgrade to something with at least 24gigs of GDDR# lol.,Positive
Intel,"I mean, Alan Wake 2 is also releasing with path tracing so my guess is we're definitely gonna see more games featuring heavier RT, although not necessarily path tracing due to the performance hit.",Negative
Intel,"I expect path tracing support to be the exception for a while because most developers will consider the non-trivial dev time to implement it not worth it. However, I'm sure Nvidia would be willing to throw dev help at most developers who would be willing to implement path tracing with their help.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode. Presumably, a less complex world helps, but they may also have implemented opacity micro maps, which I'm not sure if Cyberpunk has done in the 2.0 update. Perhaps they're cherry-picked examples for the trailer.",Neutral
Intel,Raytracing now is a gimmick *  ( * - in AMD sponsored games where RT effects are so tiny you have to zoom on to notice them),Neutral
Intel,"Except this is a tech demo. CP77 has lots of cut corners to achieve that, NN was trained to keep the interface stable in that game and even then, ray reconstruction creates horrible artifacts at some angles and distances. And that's with Ngreedia's engineeers literally sitting at CDPR's office.",Negative
Intel,You can already have 120fps on $1000 PC,Neutral
Intel,It improves both looks and fps so it is a win win,Positive
Intel,"The issue with this is the inputs will only be registered at 20Hz. This would feel awful. The benefit of high FPS is having very low latency. This will just make it look smoother, not feel any better than 20 FPS",Negative
Intel,Considering there is no input on the generated frames that is going to feel horrible to play. Base FPS (incl DLSS) really still needs to be 50-60fps to feel playable imo.,Negative
Intel,I'm thinking will there ever come a day where 100% of frames are generated by AI. Like the data inputs directly from the card into the neural network and it no longer requires physical hardware limitation to generate frames.,Neutral
Intel,"Don't forget that all NPCs will be controled by AI to simulate a real life.  They'll wake up in the morning, shower, commute to work, work, then go home.  All this to do it over and over and over until they die...  mhhh... this is kinda sad to type out.![gif](emote|free_emotes_pack|cry)",Negative
Intel,It absolutely is. I’ve been playing at 1440p with DLSS Balanced and Ray Reconstruction and it is absolutely the worst version of DLSS I’ve ever seen. It’s basically unusable.,Negative
Intel,There are horrible artifacts with RR. And ghosting. It replaces artifacts with different ones. Needs a lot more work tbh. Or training.,Neutral
Intel,"The thing about ray and path tracing is that they are demanding on their own, not because they are badly optimised",Negative
Intel,"I have a 4070. It runs the game just fine with the tools that have been set in place.  This is absolutely not a pissing match. Cyberpunk is demanding, and takes advantage of lots of tech available for upscaling and ray tracing and frame generation. AMD card’s currently do not match Nvidia’s in those categories, not even close. This us not an attempt to showcase Nvidia tech, but it does take advantage of it.",Positive
Intel,Because it's not about the flagship card. Only if the flagship can attain good performance at native then you will have headroom for lower end cards. Think about it. Of course upscaling is very important. But you also need some headroom to upscale.,Neutral
Intel,"Yeah. I wanted better cooling performance after upgrading to the artic freezer 7 as well with those weird crapy plastic tension clips you pushed in manually, fearing bending them (worse than the stock intel cooler tension twist lock). Kept having random stability crashes until after i sanded it down for better thermals...Good old sandpaper and nerves of steel fearing an uneven removal of the nickel coating.  Was trying to maximize performance as back then core 2 duo and the extreme versions were king and they had way higher single core clocks and were easy to oc. Wanted the multhreaded for Crysis, LoL (back when you had to wait 5min+ to get into a game), BF2, and was eventually playing TF, Day of Defeat and Natural Selection.  My upgrade back then was to the Evga 560ti DS, which I ended up installing a backplate and  a gelid cooler. They had wayy better thermal tape/individual heatsinks for the memory/vram chips for the heat transfer. Evga back then told me if I ever needed to RMA it that I would just need to re-assemble it as it was shipped. Remember using artic solver ceramique instead of as5 due to it potentially not degrading as fast as well.   Good times =)",Negative
Intel,"Yea same here, didn't play crysis until I got a hold of a 260 after having 2 8600gt's in sli. Played mostly wow at the time and those 2 8600gt's in sli got blown away by the gtx 260, but man that card was hot!  Remember in 2011 upgrading to a gtx 570, the first gen with tessellation and playing dragon age 2 which was one of the first games to have it. Ended up turning off tessellation cause it hit the card too hard, least until I got a second 570 in sli, which sadly died like a year later due to heat while playing Shadow Warrior.",Positive
Intel,Q6600 was the bomb. I ran mine at 3.0GHz all its life and it's still alive today. Had it paired with an 8800GT which was IMO one of the best GPU value/performance of all time.,Positive
Intel,Had 3.4ghz on my q6600 didnt want to hit 3.6ghz on modest voltage but 3.4 all day and was quite the lift in fps in gta4 compared to stock 2.4ghz. Run 60fps 1440x900 no problem those where the days when OC gave actual performance boost,Neutral
Intel,> I was so pumped when I upgraded to a gtx 260. Crysis was a dream.  In those days I was a broke teenager and I remember upgrading to a GTS250(rebadged 9800GTX+ with more vram) and crysis still hammered my Athlon x4 965(?) system.  While my richer neighborhood buddy upgraded to a 260 just to play Left 4 dead.,Positive
Intel,"After checking sone old Evga posts, (mod rigs server is done, along with old signatures), I was able to get to 3.33ghz.",Neutral
Intel,"Yeah my lapped Q6600 was my daily driver at 3.2 GHz with a Tuniq Tower 120 Extreme. It ran it pretty cool. I think when Crysis came out it and my 8800 GTX could do 20-25 frames at 1680x1050 with drops into the teens. EVGA replaced it with a GTX 460 when it died, big upgrade.",Positive
Intel,moving data between the two is the issue,Neutral
Intel,grey cagey sharp detail handle north grandiose connect sparkle hungry   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"I wonder the same thing. But I guess if Nvidia would put it all in an extra card, people would just buy more amd to get the best of both worlds.",Neutral
Intel,Game physics doesn't seem to be a focus anymore though,Neutral
Intel,I'm ashamed I even own one NVIDIA card.    Unsurprisingly it's in a laptop that I bought during the [illegal GeForce Partner Program](https://hothardware.com/news/nvidia-geforce-partner-program) before the public knew about the program.  Not making that mistake again.  Screw NVIDIA.  Every other card I've ever gotten has been ATI/AMD (and obviously the low-end integrated Intel GPUs on some office laptops)  EDIT: Keep them downvotes coming.  You've all been bullshitted by NVIDIA's marketing.  Your boos mean nothing; I've seen what makes you cheer.,Negative
Intel,Anyone who owns an Nvidia GPU should be ashamed of themselves tbh.,Negative
Intel,"There was more nuance to that ""bugfix"" than just ""it was a bug that got fixed""   Modders were able to re-enable the AMD/NVIDIA PhysX combo basically immediately via editing DLLs.  Not driver replacements, but actual DLL tweaks.  The speed at which modders were able to fix what NVIDIA ""broke"" combined with the public outrage put them in a funny legal spot.  If they continued with PhysX disabled and claiming it was a bug then the bug can easily be fixed, as shown by modders doing it first.  So either fix it and get everyone back to normal, or leave PhysX disabled even though it can be easily enabled and open the company up to potential anti-competitive lawsuits.  Obviously not many modern games use the GPU implementation of PhysX anymore and the performance difference between current GPU and CPU implementations are negligible anyway, but their intent at the time was clear once it was shown how quickly modders were able to get everyone back to normal.",Neutral
Intel,"Yes, but not after Nvidia bought Ageia.",Neutral
Intel,"If the $200 cards of today aren't 3x better than the flagship cards of 10 years ago (they're not), then you shouldn't expect a $200 card in 2033 to be 3x faster than an RTX 4090. Average annual performance gains are more likely to decrease between now and then, rather than increase.",Negative
Intel,I like how you said: static image  because in motion upscaling is crappier,Negative
Intel,"It isn't a weird argument. Rasterization is premised on approximating reality, while RT is simulating it.  The extreme few amount of rays we trace right now, including bounces, means effects are often muted and the performance hit is enormous. To compensate, we're using temporal upscaling from super low resolutions *and* frame interpolation.  Temporal upscaling isn't without it's issues, and you know how much traditional denoisers leave to be desired. Even Nvidia's Ray Reconstruction leaves a lot to be desired; the thread on r/Nvidia shows as much, with ghosting, artifacts, etc. all over again. It's like the launch of DLSS 2.0.  All of that, for effects that are oftentimes difficult to perceive, and for utterly devastating traditional rasterization performance.",Negative
Intel,"It's a shame raster is so labor intensive, because it looks so interesting when done right.     I look at screenshots from cyberpunk, Raster / Ray Trace / Path Trace, and in most of them the raster just looks more interesting to me. Not constrained by what would be real physical lighting. The 100% RT render versions of old games like Quake gives a similar feeling.     But I imagine there will be more stylistic RT lighting over time, it saving a ton of labor is all things considered good, freeing it up for actual game design.",Positive
Intel,">That’s why devs love RT and want it to grow, as RT/hardware does the work for them.  It's the marketing departments that love RT. Devs hate it because it's just yet more work on top of the existing raster work.",Positive
Intel,"It *is* way slower, you're just compensating it by reducing render resolution a ton",Negative
Intel,If it was bloodborne i am guilty of that myself,Neutral
Intel,price zealous rinse detail yam rainstorm groovy automatic snails fact   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,4k max without dlss or fsr or frame gen I still get 30fps. Sorry you can’t even hold 10. Guess I’ll shill some more to at least not be spoon fed shit from amd,Negative
Intel,"Ray tracing and path tracing aren't just ""reflections"" , but this thread is specifically about the new path tracing modes in CP2077.",Neutral
Intel,Rasterisation is a “hack” too,Neutral
Intel,If it works it works....computer graphics has always been about approximation,Positive
Intel,"4090 is \~66% more powerful than 3090 overall, but it has 112% more fps in this test.     The RT capabilities of NVIDIA cards has grown faster than general capabilities.",Positive
Intel,"I guess on the internet, you can just lie. There’s no game on ps5 that upscale from 540p to 4k.",Negative
Intel,"I guess I was wrong on that one, although SWJS upscales from a resolution as low as 648p, which is not much.",Negative
Intel,It's big in certain lighting situations. In others it's not as noticeable as you'd hope,Neutral
Intel,"PT vs no RT comp is silly.  U should compare PT vs RT, and there, the difference is minor and not even subjectively better. Just different.",Negative
Intel,Natural? It over saturates the light bleeding into the entire scene. Everything becomes so colorful. Even when floors or ceilings are not supposed to be reflective. Literally tiles with grainy texture suddenly becomes a mirror..  It's ridiculously overdone.,Negative
Intel,Guys I'm trying to suffer while I play the game what settings will let me suffer the most,Negative
Intel,"I mean this is native RT/TAA. Was never meant to be played this way. You need to use DLSS and Ray Reconstruction. With that setting, i get about 40-50fps at max settings with my 3080. Not too bad.  The thing about AMD is that they don't have any of this AI technology (yet). You have to rely on raw power, which won't get you far.",Neutral
Intel,"Until the next generation of consoles, I think path tracing will be added to something like a couple major games per year with Nvidia's help. If I'm right, that's wouldn't be a lot, but it's definitely be a bonus for those who have GPUs that are better at ray tracing.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode (which could be for a variety of reasons, including CP2077's larger world).",Positive
Intel,Alan Wake 2 is also an outlier since the same dev made Control which was a test bed for DLSS 2.0 and one of the earliest games to fully embrace ray tracing. Alan Wake 2 is also NVIDIA sponsored and looking to be another test bed for them.,Neutral
Intel,"Redditors trying to have reading comprehension (impossible)     >It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it).",Negative
Intel,"Every single game has cut corners, starfield has multiple cut corners. Tech demo would imply there's no great game under great visuals.",Negative
Intel,Get that fps with a 5120*1440 240Hz monitor,Neutral
Intel,"It's almost like crushing your fps with all these fancy new graphic features isn't worth it, even on the best cards. It's user preference in the end.  That said, many gamers, once they play at high hz screen and fps, find it very difficult to go back to low fps for any reason. Games feel slow by comparison, no matter how great the lighting looks.",Negative
Intel,"Both DLSS and FSR have weird image artifacts (FSR is worse though) so i guess if you don't see them then DLSS only works in your favor then.  I personally want as little noticeable image artifacts from DLSS/FSR when I play so opt for native resolution, generally.",Negative
Intel,Have you tried dlss quality. Dlss balanced sacrifices image quality and dlss quality doesn't/can enhance it sometimes,Neutral
Intel,">a lot of the comparisons just make it look like it gives too much bloom  This is due to colors being compressed to SDR. In HDR that ""bloom"" looks awesome as it's super bright like it would be in real life but still not losing any details. You lose those details only on those SDR videos.  PT just looks and feels way more natural than just RT, when only raster literally looks like it was two different games from two different hardware generations.  Once I saw Cyperpunk PT there is no way I'd imagine playing this game in any other way. All other games look absurdly ugly next to it now. It's something like when you get a high refresh screen and can't look back at 60Hz anymore.",Negative
Intel,"Trust me, I know how the technology works. That doesn't change the fact that it's not worth it imo. I'm not  paying 4090 money to have the newest fad that just makes games look over saturated and glossy.",Negative
Intel,We are talking about running the game just fine now are we? Because rasterization is just fine. I will consider ray tracing reasonable when you can do it maxed out for $1200(GPU) native. Until then it’s a gimmick. IMO,Positive
Intel,"So even you, someone who is an enthusiast of this sort of thing, i.e. a small minority of the userbase, had to spend \*minutes\* benchmarking to realize you didn't have RT turned on.   Yeah, you would lose that bet.",Negative
Intel,Those wolfdale C2D were beasts. I had a buddy with one and we took it 4.0. And it kept pace/beat my C2Q as most games didn't utilize MC as well back then.  Man XFX and EVGA all the way. Shame they both left the Nvidia scene.,Negative
Intel,"Ah, WoW. That started it all for me on an C2D Conroe with ATI x1400 on a Dell inspiron laptop freshman year of college.",Positive
Intel,8600gt in SLI man you are Savage!🔥,Neutral
Intel,"I had the same setup, and i managed to get my Q6600 stable at 3.2.  Was a blazing fast machine and it still shit the bed in some parts of crysys",Positive
Intel,there there’s no way there will ever be another 8800 GT. you got so much for your money.,Negative
Intel,"Those 9800gtx+ were solid cards, it's what I upgraded from.  You make due with what you had.  My first system was a Craigslist find that I had to tear down and rebuild with spare parts from other systems.",Neutral
Intel,Seemed like a perfect use case for the sli bridge they got rid of.,Positive
Intel,Why would it need to send the data to the other card? They both feed into the same game.,Neutral
Intel,Big up the Vega gang,Neutral
Intel,"That is because destructible buildings and realistic lighting REALLY does not go hand in hand. Realistic looking games use a ton of ""pre-baked"" light/shadow, that might change when ray tracing is the norm but it has a delay so things still look weird.",Negative
Intel,"I remember playing Crysis, flipped a wheeled garbage bin into a hut, which came down on top of it. I blew the wreckage up with grenades and the bin flew out, landing on it's side. Took pot shots at the castors and the impact made them spin around in the sockets.   Here we are 15 years later and game physics have barely moved an inch from that",Neutral
Intel,Cause regular stuff is so easy to simulate or fake simulate that super realistic complex items are not really worth the trouble.  &#x200B;  water still looks like crap in most games and requires a lot of work to make it right.  and even more processing power to make it truly realistic.  Cyberpunk is perfect example.  the water physics is atrocious.,Negative
Intel,It's because it's not the big selling point now and as the comment you're responding to... Things have gone so far no one really thinks about it anymore. Ray tracing is what physx used to be or even normal map and dx9/10 features you don't consider now.,Negative
Intel,"Honestly, same here - I hate Nvidia so much for what they do so shamelessly to the average consumer that I’d struggle to recommend their products even if they *were* competitively priced.",Negative
Intel,"People are just stupid, if game companies just slap ray tracing on and don't tweak the actual look of the games lighting, gaming graphics is doomed. Rt is not the magic bullet people seem to think it is, everything looking 'realistic'=/= good art design.",Negative
Intel,"I thought RT was loved by devs, as it allowed them to avoid the shadow baking process, speeding up dev time considerably?",Neutral
Intel,We are guilty of the exact same sin.,Neutral
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"All rasterization does is approximate colors for all the pixels in the scene. Same thing does raytracing, path tracing, AI reconstruction and whatever other rendering techniques there are.   The final result is what's most important, doesn't really matter how it's achieved",Neutral
Intel,would you care to explain ? Kinda interested to here this,Neutral
Intel,jellyfish follow simplistic plucky quarrelsome unwritten dazzling subsequent mourn sable   *This post was mass deleted and anonymized with [Redact](https://redact.dev)*,Neutral
Intel,"It isn’t minor. The PT with DLSS 3,5 is quite a bit more accurate and reactive, as in dynamic lights are reacting closer to instantaneous than without. The best way is to watch on a 4K monitor or to actually play it on someone’s 4090/4080 or wait for AMDs FSR 3 to see if that helps it do path tracing.",Positive
Intel,> Literally tiles with grainy texture suddenly becomes a mirror  PT doesn't change what the materials are bro. You are just seeing what the materials are meant to look like.,Neutral
Intel,Turn on path tracing. Embrace the PowerPoint.,Neutral
Intel,Control also has some of the best implementations of RT I've ever seen. It's one of the very few games I've turned on RT and felt like it was worth the performance hit. Everything else I've played has looked very good with RT on but not good enough to be worth the performance hit.,Positive
Intel,"Of course it is not perfect but it also allows max RT, PT, RR which improves the graphics greatly...and i do not enjoy below 100 fps either",Negative
Intel,"I tried 1080p DLSS Quality and it was just as bad, but I’m not really happy with the performance of 1440p DLSS Quality (like, 30-40fps), so I didn’t want to use it.",Negative
Intel,"You're is coming off as salty that you *can't* afford a 4090. If other people wanna spend the extra cash and turn on all the bells and whistles, why does that bother you?",Negative
Intel,"Mate, you can't fucking run starfield at good fps, a game that's raster only that looks like from 5 years.  Look at how many games scale and run badly in last two years and compare that to cyberpunk",Negative
Intel,"This just absurd and frankly entitled. Not everyone can afford a PC strong enough to run the most demanding games absolutely maxed out.   I never could, couldn’t even afford a PC until this past summer.   This obsession with performance metrics is ridiculous. Like, less than 5% of PC gamers even have 4k monitors. Almost 70% still have 1080p.   CDPR made a VERY demanding game that looks GORGEOUS completely turned up. Modern hardware cannot handle it natively, it can’t. It is not a gimmick that a company has developed a superior technology that allows the game to be played at almost the same quality as native, significantly smoother and faster. You’re in denial if you think otherwise.  I’m not tied to either brand. I have an AMD CPU bc it was the best option for what I wanted, and a Nvidia GPU for the same reason. I care about RT, frame gen, all of that. The Witcher 3 still is my favorite game, and the entire reasoning for building a gaming PC over a console is graphical quality and speed while doing so. That game is absolutely gorgeous with RT on. Same with Cyberpunk, which I can enjoy thanks to DLSS.   You don’t care about those things? That’s fine, but it is solely a personal opinion.",Negative
Intel,"Yea well I was defining an arbitrary performance. It's not about having native performance. But, but if a flagship is able to get 4k 60 with path tracing at native then that allows headroom for 60 tier or 70 tier cards to perform well.  In this example the 4090 gets 19.5 fps at 4k. The 4060ti gets 6.6 fps. Let's assume a next gen flagship is able to reach 60 fps at 4k. That would theoratically allow the 60ti card to reach around 20 fps. Once you do your various upscaling and frame generation techniques you can then reach 60 fps ideally with a mid range card.",Neutral
Intel,"The quad core showed up the core 2 duos like 2 yrs after because of emerging support and I was on cloud 9. After evga left, i jumped ship with my XFX 6900xt and I think I am here to stay with team Red until value/longevity says otherwise.",Neutral
Intel,Shame you don't. They did it for a reason.,Negative
Intel,The 9800GTX was a bad ass. I'd consider it the biggest upgrade in generation until the 1080 and then the 4090.  It was far more powerful than anything near it.,Positive
Intel,"I do miss it, but it was a pain in the ass to get working with my custom cooler due to the HBM.",Negative
Intel,They've actually gone a bit backwards. Devs dont seem to care about implementing physics anymore. It's just an afterthought. And you can forget about destruction,Negative
Intel,"I gobble up the downvotes every time I say it and I don't give a crap.  Every other corporation sees through their bullshit.  Otherwise every console would have NVIDIA cards.  No console has an NVIDIA card anymore.  But the general population is more than willing to get bullshitted into buying their cards against their own interest.  EDIT: Forgot about Nintendo Switch, as is tradition.",Negative
Intel,"Anyone can say anything.  Anyone can change their intent after the fact.  Happens all the time.   Judge them by their actions alone and ignore what they say.  Going solely by **events** :   1) People use (mostly used) NVIDIA cards for driving PhysX   2) NVIDIA disables this ability, locking everyone to the god-awful CPU implementation at the time   3) Modders edit DLLs to get functionality back   4) NVIDIA THEN says it was a bug that will be fixed    It's not foil hat because I'm coming to this opinion based solely on what's measurable and real; events and actions.   It's not conclusive, but I'm not going to rely on any company's PR statements to sway me one way or another because they will (obviously) say whatever they're able to prevent lawsuits and public image damage.  Based strictly on **events** that looks like damage control on a bad decision.  They rectified it, even more recently made PhysX open source, I give them credit for making it right, but the intent at time time was clear.",Negative
Intel,This is why I'm excited for Alan Wake 2. It is coming out with path tracing always having been part of it. This will definitely mean they built the art style around that.,Positive
Intel,"Yeah, I can't really blame people for not recognizing great design if they are not interested in it. I don't think most people are interested in design, even though we all benefit from it without knowing it.     People might get some sort of subtle feeling when they see some great craft made by a experienced professional, but they can't really put it into words or evaluate the quality of the design without experience. They likely can't tell what cause the feeling to begin with even if they can notice it.     But everyone can instantly judge how real something looks, which is the go-to standard for ""good"". And bigger numbers are better, even if they are fake. Niche features that nobody uses also sells, as long as it's a new feature with a captivating name.     This reminded me. Live action lion king, what a travesty, stripping away all the artistry and expression for realism.",Neutral
Intel,"If you're using RT exclusively maybe, but no one in the games industry is - nor will they be any time soon.",Negative
Intel,"A lot of raster techniques reduce the complexity of lighting into very naive heuristics. Like shadow mapping renders the scene from the light's pov to create a depth map, then the objects that are at a greater depth have darker colors drawn (I'm simplifying that but roughly how it works). It's like teaching a kid how to illustrate shadows on an ball vs the how light actually works. Raster techniques have evolved a lot since then, and use raytracing in limited contexts, screen space reflections and ambient occlusion for example, but they're still operating with extremely limited data in narrow, isolated use cases, which is why they often look awful. There's just not enough information for them to look correct, just enough to trick you when you're not really paying attention.",Negative
Intel,All lights in a rasterised scene are “fake”.   Someone explains it much better here:  https://reddit.com/r/nvidia/s/eB7ScULpih,Negative
Intel,Or a bag of fake tricks.,Negative
Intel,">Every scene is lit completely differently with PT enabled.  It's really not, and it's very disingenuous of you to try and pretend it is. There are plenty of screenshot comparisons showing the difference in certain areas if you want me to link them, and I have the game myself and use a 3090.  There are differences, but saying ""every lit scene looks completely different"" with PT Vs regular RT is false.",Negative
Intel,"I'm not talking about DLSS 3.5. I'm talking about PT vs regular RT in CP2077. The visual difference is subjective, it looks different, is it better? Not in my opinion, it over saturates the scene.  DLSS 3.5 is a separate issue, it fixes some of the major flaws of PT, the slow reflection updates.",Negative
Intel,Have you tried Metro Exodus Enhanced?,Neutral
Intel,At less than 4K resolutions Quality is the only sensible option. The resolution it upscale from becomes too low otherwise and DLSS then struggles to have enough data to make a good image.,Negative
Intel,"It doesn't, I have no problem with people buying what they want. My problem is people assuming I can't afford something...",Neutral
Intel,What is this? A reasonable comment in this dumpster fire of a sub?,Negative
Intel,Thinking that a $1200 graphics card should be just strong enough to run everything Max is entitled. I wear that crown.,Neutral
Intel,"I've got an evga 3080ti now. I suspect my next card is either going to be an XFX 8800xt, or catch an XFX 7900xtx on clearance.",Neutral
Intel,"I don't blindly support a single company, such as simply switching to AMD because my chosen vendors left nvidia. That's how competition stalls.  That being said the card I have now is a product of what I could get my. Hands on during the crypto boom, and is EVGA.  My next card is likely to be an XFX 8800xt/7900xtx.  So not sure why you're in here attacking me.",Negative
Intel,Except it was just a rebadged 8800GTX/Ultra,Neutral
Intel,"I mean, the Nintendo Switch is using Nvidia graphics, albeit it’s a tiny 28nm iGPU from 2014… but I get your point. They’ve opened up a large technological lead over AMD thanks to the RDNA 3 debacle, and I’m *still* recommending only Radeon GPUs because Nvidia is just that hubristic.",Neutral
Intel,Pixel art go brr,Neutral
Intel,Yeah they will. Lumen and lumen style gi systems will have to exist for fallback so the industry can move on.,Neutral
Intel,"Yeah, I agree. It’s just a shame because it’s not really playable otherwise. DLSS with ray reconstruction is a little mixed.",Negative
Intel,"It just doesn’t make sense. Game developers do not set GPU prices.   The game runs well on a $1200 GPU. Perfectly well. Not completely maxed though, and it’s weird to think the game shouldn’t be able to make full use of available technology if it wants to. My GPU cost less than half that and runs the game very well with RT on thanks to DLSS.",Negative
Intel,"RR is not perfect, but I think overall Cyberpunk 2.0 looks so much better than it did previously - and it looked pretty damn good earlier too!  [Digital Foundry's video](https://www.youtube.com/watch?v=hhAtN_rRuQo) is again the yardstick for how many small things they point out where RR improves the situation, even if it has some things where it fails atm. But maybe DLSS 3.6 will solve those.  I do feel DLSS itself has also improved in the past few years in terms of ghosting and performing at lower resolutions.  On a 1440p monitor these are the starting point resolutions:  - Quality: 1707x960p - Balance: 1485x835p - Performance: 1280x720p - Ultra Perf: 853x480p  So even the highest quality option is sub-1080p. I wish Nvidia introduced a ""Ultra Quality"" preset that would be say 80% per axis resolution, something between DLAA and DLSS Quality. At 1440p this would be 2048x1152.",Positive
Intel,"Speaking of the AMD overlay, am I the only one that gets near zero CPU utilization every time they pull it up? GPU utilization fluctuates up and down depending on resolution and graphical settings, so it seems to be fine, but my CPU utilization readings never go above more than a few percent tops. I have a 5800X3D.",Neutral
Intel,GN has already made a [video about it.](https://www.youtube.com/watch?v=5hAy5V91Hr4),Neutral
Intel,Honestly this is everything that I wished Adrenalin overlay had. It's pleasantly lightweight too.   Hats off to Intel's development team. They didn't need to publish this at all.,Negative
Intel,I wonder if this will get added to Mangohud and Gamescope.,Neutral
Intel,This is quality. Great work.,Positive
Intel,Doesn’t capframeX uses presentmon as its monitoring tool?,Neutral
Intel,I can finally see if it really is the ENB taking down my Skyrim gamesaves,Neutral
Intel,"FYI, the open-source version has been available for years, was last updated 9 months ago, and does not contain an overlay or GPU busy stats. (It also has version number 1.8 while this new thing is v0.5)  Maybe they're planning to release source code later, but right now the source is closed.",Neutral
Intel,This is pretty damn awesome and will be incredibly useful when they develop the future features Tom mentioned. (insightful metrics),Positive
Intel,"This is not bad at all. But it's not replacing Radeon overlay for me... yet.  1. Needs an option to minimize to system tray.  2. Many important Ryzen CPU metrics (temperature, power consumption etc.) aren't available without an ability to access Ryzen Master SDK. 3. Radeon Overlay can continue to monitor on the desktop, which I use constantly. I tried manually hooking DWM.exe and it won't. I might just be dumb, but I couldn't get it the overlay working without 3D running. 4. Will give credit where credit is due, unlike Radeon overlay, it works right with OpenGL. 5. And the credit is immediately removed, because unlike Radeon Overlay, it doesn't work right with DXVK.  6 out of 10.",Negative
Intel,This is so good. The only thing I dislike is that there isn't a way to scale the whole thing up an down.,Positive
Intel,"At this rate Intel will have a more polished driver and software stack than AMD in 2 or 3 years, if not before.  Seriously, I suck for being able to buy an AMD GPU like the 6990 I owned, but I can't get near any hardware they made while using my system for work.  I need a CUDA alternative that works on more than 6 GPUs on specific linux distributions when the red moon shines above my building.",Negative
Intel,it's sort of misleading to call it opensource    when whatever intel beta version released is just MSI installer and binaries inside ...    i see no source of what is available for download ...   what's on gihub is something old w/o overlay,Negative
Intel,Thanks Intel! I will try this out at least since I hate MSI afterburner.,Positive
Intel,The download link gives a warning in Windows:  >This site has been reported as unsafe   >   >Hosted by intelpresentmon.s3.amazonaws.com   >   >Microsoft recommends you don't continue to this site. It has been reported to Microsoft for containing harmful programs that may try to steal personal or financial information.  ???,Negative
Intel,"I really wanted to test with it why I was getting stuttering in Apex. Unfortunately, due to some work, I would be possible on Sunday.",Negative
Intel,Doesn’t work for me. It crashed at the start with an error message and made Dolphin run way worse.,Negative
Intel,And AmD gives far more than Nvidia.,Neutral
Intel,I can't get this to work. Is anyone having the same issue?  CPU Temperature (Avg) NA C  CPU: 5800X3D,Negative
Intel,I have a 5800x3D and PresentMon does not recognize it. Says UNKNOWN\_CPU.  Anyone?,Neutral
Intel,People have reported that cpu usage in Radeon software is not very accurate.,Negative
Intel,I saved this because it is posted so frequently lol  https://www.reddit.com/r/AMDHelp/comments/14n55gd/cpu_usage_percentages_differ_between_task_manager/jq66gop?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=1&utm_content=share_button,Neutral
Intel,"Probably overlay wasn't updated for W11 22H2.  Microsoft updated something and CPU reading was broken for MSI Afterburner also, after working the same for a decade or more before. They fixed it in the meantime.",Negative
Intel,"I had the same problem, googled around and found the problem to be c state in the bios. Just turn that off and it should be accurate.",Negative
Intel,Just use afterburner as OSD.,Neutral
Intel,I had that issue with my 5600X both with Nvidia and AMD overlay. More recently it randomly fixed itself.,Neutral
Intel,"Yeah adrenalin was useless for it. It was displaying 1-2% usage for me, but msi afterburner, hwinfo, and rtss were showing much more, like 30-50, which made more sense.",Negative
Intel,You beat me to it :),Positive
Intel,"Yup, ""new Intel"" definitely seems to be getting nicer and with a ""younger"" company culture. Also, it's a smart move. They're building a nice little ecosystem around Arc cards, which is eventually what is going to drive sales when performance and stability matches NVidia and AMD.",Positive
Intel,And it's open source!  I'm liking Intel more and more since they're getting their ass kicked.  Apparently it was one guy's pet project.,Positive
Intel,Technically it should be possible to add in MSI afterburner because it's open source,Neutral
Intel,"The new PresentMon shows more information.  Since it's open source, capframeX can implement this as well.",Positive
Intel,It was a pet project of one of the Intel engineers.   6/10 is not bad!,Negative
Intel,It's still beta. I'm sure they'll fix it.  Intel has amazing engineers.,Positive
Intel,Intel is working on their 2nd generation GPU and it's said to compete with Nvidia highest tier.  And now AMD isn't planning to release a big RDNA4 chip.,Neutral
Intel,"didn't nvn get merged with mesa recently edit: sorry, I mean NVK not NVN",Neutral
Intel,I hate afterburner and RTSS. This is way better,Positive
Intel,"people probably spammed microsoft because this PresentMon also gives you the option of enabling amd, nvidia, or intel telemetry.",Negative
Intel,"Thank you for contributing exactly NOTHING to this discussion.  Truly, the world is a better place because thanks to you.",Positive
Intel,"Tell that to anyone that needs CUDA. Or GPU accelerated workloads in general.  Id suck for being able to get something like the 6990 I owned back then, but between lack of support for almost all workloads and the driver being shit since like forever... Nope.  Yes, yes, nvidia's control panel is old and looks like a windows 98 app. But it at least works and actually saves youre settings, unlike AMD's one that half the time simply forget about everything.  I currently own 5 systems. 2 nvidia ones, 3 amd ones. And all of the amd ones had driver related issues.  You can't simply work with that. Is not viable. And as a gamer you should not need to see if the drivers are working or not.  Fuck, amernime drivers exists just because how hard amd sucks at it.",Negative
Intel,"Well, everyone uses RTSS anyway and it gives you basically everything.",Neutral
Intel,Similar issue here with an i5 12600kf. Can't get temperature readings from it. I'll need to look at it more.,Neutral
Intel,"Really? Good to know it's not just me, then. Nonetheless, this seems like a pretty egregious oversight on AMD's part. If I'm not mistaken, CPU metrics are on by default in the overlay and every time someone switches it on, it's an extremely visible reminder of work that AMD still needs to put into the drivers.",Negative
Intel,Speaking of said software why does it keep setting gpu max frequency to 1200MHZ?,Neutral
Intel,"Thanks for sharing your findings. It's pretty damning if even the Windows Task Manager does a markedly better job than Adrenalin. That's probably an understatement since, as I found out via other helpful Redditors here, the Adrenalin overlay may as well be broken. I'm just glad it's not something wrong with my system.",Negative
Intel,Afterburner fucks with my settings in adrenaline,Neutral
Intel,"NVIDIA's highest tier? Nah, but around RTX 4080 is the target, which might be bad news for AMD.",Negative
Intel,"By NVN do you refer to Nintendo's API for nvidia? Or you wanted to refer to NVK that got into mesa?  By CUDA alternative I mean stuff like ROCm. CUDA have so many years of development on top of it and a stupidly widely support from software vendors.  While yes, NVK could help to alleviate the issue, is not even close to what native CUDA can do regarding GPU usage for general computing.  The issue with ROCm is mainly a lack of support on a lot of software and lack of support for a lot of hardware.  On the nvidia end you can get the cheapest trashiest GPU and it will have the same level of CUDA support as the most expensive enterprise product.  Yes, performance is not the same, but I can use a 3090 Ti for loads that needs more than 12 GB vram without needing to buy a dedicated AI GPU, and if a given task needs serious shit, then again, I can buy a dedicated solution that will run the exact same software with the exact same code I ran on the ""weak"" 3090 Ti.  That is not possible with AMD, if you buy an enterprise grade GPU from them, you need to build all your software stack for it instead of just moving it and keep going.  And while yes, developers from tools like PyTorch COULD improve their ROCm support, I totally get why they are not doing that. ROCm works on like what? 7 GPUs? On specific linux distros?  Is a matter of scalability and costs. ROCm needs to be supported on ALL AMD GPUs in order for it to be something worth developing for, otherwise there is not really a use case where the end user wont be writing their own stack.  And again, if you can start small, then scale up, nvidia is the only option, so AMD on this space is either ""I got a stupidly great offer where AMD provides the same hardware power and performance per watt at like half the prize so I justify building all my software from scratches"" or ""Ok, I'll go for nvidia and just use all the software already built for it and call it a day""  I really, really want to being able to use AMD GPUs aside of trivial tasks, but this is a very, VERY gigante issue that appears a lot, especially on the profesional space.",Neutral
Intel,Except PresentMon seems to not be compatible with Ryzen CPUs at all... :/,Neutral
Intel,I get downvoted for asking a valid question?,Neutral
Intel,Thank you for continuing to contribute Nothing to this conversation.,Positive
Intel,Clearly not more than this beta of presentmon,Negative
Intel,I use amds for overclocking and that's about it really. But it's still far more tools than nivida gives,Neutral
Intel,"To defend AMD here as loathe as I am lately. CPU utilization isn't a very useful metric to begin with, so it misreporting would be super low priority.  The stat tells you nothing useful about the hardware, just the thread scheduling. CPU could be stuck in wait and reporting a high number while doing no work or it could be bottlenecking somewhere else while reporting a low %.",Negative
Intel,Where are you seeing this?,Neutral
Intel,"The Adrenalin overlay is good for really everything except usage. But yeah, this has seemed to have been a problem for a good while now. Ive been using AMD for a little over a year now at least Adrenalin and the CPU usage was always wrong on two different CPUs (AM4 and AM5) as well. Me and that other person tested it on my system and he had 3 systems running different AMD CPUs and all were the same. Ill find the thread one day but its been a while so it is way down the list of my comments.",Negative
Intel,"False setup.  Disable the button in the ui ( not settings)  For ""load on startup""  You literarily told afterburner to load it's settings on start up.  Huge mistake many people do because most people think it's ""boot on startup"" but in reality it's loading it's oc settings and stuff effectively overwriting adrenaline if you leave that enabled.",Negative
Intel,3070 Also was their target for their first generation. I don't expect Intel to suddenly be great just because that's what they aim for.,Negative
Intel,And XeSS 1.1 looks better than FSR.  XeSS 1.2 was just released and it has even more improvements.,Positive
Intel,Great news to gamers though,Positive
Intel,ah sorry I meant NVK,Neutral
Intel,"I don't know, I didn't downvote you.",Neutral
Intel,My point is if you've got nvidia card you simply use 3rd party soft and got everything and more. So comparing that AMD's software to Nvidia's is kind of meaningless.,Neutral
Intel,"I actually agree with you, but I don't need the metric to be some scientifically accurate reading of performance or whatnot, I just think it'd be a useful tool for adjusting game settings. For example, if I'm trying to optimize performance in a game, does this setting make CPU utilization go up or down? How much almost doesn't matter. Is there a bottleneck, where is it, and am I relieving pressure on it with these settings or not?",Neutral
Intel,It’s where you oc in adrenaline,Neutral
Intel,"There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Not because upscaling, but also because how important is to have drivers that work in tandem with specialized hardware units in the GPU.  Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  AMD fails hard on 2 of those things.  Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.",Positive
Intel,"Yep, XeSS is just great, I happily use it in MWII on my Arc A770. But I still use DLSS on NVIDIA. But both are better than FSR.",Positive
Intel,"Then you should be looking at ***GPU*** usage, because CPU usage will not help you answer any of those questions",Negative
Intel,"Problem is, by itself it still tells you little to nothing. If GPU utilization drops like a brick you know something is bottlenecking. If CPU utilization changes in the absence of other data you know absolutely nothing of value.   The real kicker is the CPU utilization may not even be ""wrong"", it may just be polling at a very low rate because that's the other thing with these OSDs if you poll too frequently for up-to-date data that can kill performance from the overhead.",Negative
Intel,"> There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Agreed, not to mention that when they made Alchemist, yes their target was 3070 and they underperformed that, but they will eventually hit their targets as they learn to improve their architectures. For a first shot, getting to around 85-90% of their target is actually quite good. It's not like they undershot by 50% of the target. They got close.   But like you said, software is super important and these days having good software is half of a driver. I have an A770, the driver actually pretty good, like the software menu. It used to suck when it first came out because it was some crap overlay, but over time it's become a separate program and it works really good considering they're not even a year into Arc's release. They also have a good set of monitoring tools built in and the options are all simplified. They don't have super unnecessary things like a browser integrated into the driver suite, or a bunch of other stuff. I'm sure by the time Celestial is around if Intel does stick with Arc, that they will be ahead of AMD on the software game, assuming AMD continues down the same path they have now.  > Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  Yep, tools like [this one from NVIDIA](https://youtu.be/-IK3gVeFP4s?t=51) have made it easier than ever as a dev to optimise for NVIDIA hardware.  > AMD fails hard on 2 of those things. Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.  Yep because Intel recognises that to get close to NVIDIA they not only have to make great hardware, but also great software to go along with it. You can only do so much with hardware to match NVIDIA. You have to make great software along with in-game solutions like XeSS to match NVIDIA. FSR is behind not only DLSS but now also XeSS. AMD is just not competitive with NVIDIA on all fronts. Arc is just getting started, give Intel 5-10 years of time in GPU and they will outpace AMD for sure.",Positive
Intel,"There are many settings that directly impact CPU usage, so of course CPU utilization reading can be useful.  Not everything needs to be precisely accurate, this is a lousy defense. Getting 0% reads all the time is entirely worthless but some accurate reads is still useful information.",Neutral
Intel,"You are going out of your way to defend a broken feature.    7800X3D shows up as 1% CPU utilization during games such as Watch Dogs, Stellaris, Age of Empires. Clearly broken. Undefendable.",Negative
Intel,"And with a bit of luck, they will also hit AMD in the console market.  Right now AMD stays afloat GPU speaking because consoles, they were during a lot of years the only vendor offering both CPU and GPU in a single package, reducing costs A LOT.  Nvidia was not able to compete against that with the lack of CPUs and Intel with the lack of GPUs.  Now that Intel is working on dedicated GPUs they are going to eventually create APU like products, its just the natural outcome of producing both CPUs and GPUs.  And then AMD will NEED to compete against them instead of being the only option.  And if that happens, unless FSR gets turned into a hardware specific tech like XeSS that falls into different techs depending on the GPU its running on, AMD will be murdered.  Better software stack, better general hardware to accelerate and improve upscalling keeping costs down AND being a second player pushing prices down towards Sony and Microsoft?  AMD will be screwed up if they don't up their game. And hard.",Neutral
Intel,"Right, so what kind of useful information can you tell me if I raise graphics settings and CPU utilization goes from 12-16% to 16-17% while framerate goes down from 120 fps to 110 fps?",Neutral
Intel,"All CPU utilization tells anyone is thread scheduling, and nothing about what the threads are or aren't doing. Like the only situation that it marginally tells someone anything is how many threads a program uses. But that usually doesn't fluctuate with games, games usually scale up to <x> number of threads and that is that. GPU utilization is the one that is far more useful for spotting bottlenecks, general performance observations are far more useful for tweaking.  Even Intel and co. will tell you that with modern processors and all the elements they contain as well as hyperthreading/SMT CPU utilization isn't a very useful stat anymore. Meant a whole lot more when CPUs were one core unit and didn't also include the FPU and memory controller and everything else.",Neutral
Intel,"Cause unfortunately even if it wasn't broken, it's not useful in any meaningful capacity. Should it be broken? No. Would anything notable come of it if it were fixed? Also no. Given the bugs they got and room for improvement elsewhere it's one of those things that should be super low on the priority list or just removed wholesale.",Negative
Intel,"That whatever setting you are changing does not have much of an impact on CPU utilization, and if your GPU allows, you can use it.  What is the purpose of this question, are you really not aware of any settings that impact CPU usage? Some RT settings have big impacts on CPU usage too.",Neutral
Intel,"The problem you're missing here is that settings that are CPU demanding generally don't have an easily observable impact on CPU usage, if they have any impact at all.  The reliable way to see if a setting is CPU demanding is to turn it up, see the frame rate go down, and then look at how **GPU** usage changed.",Neutral
Intel,"I'm aware that some settingd impact CPU usage, but I think looking at the CPU usage is about as useful as your daily horoscope, or AIDA64 memory benchmark",Neutral
Intel,Congratulations on the new gaming laptop. Don’t let people get you down and not everyone has to have the very best to be happy so if it makes you happy then I’m happy for you.,Positive
Intel,"Congrats the 30 series isn't half bad. Laptops aren't bad but gaming desktops add more umph to their performance. My first gaming rig was a gaming desktop with 64gbs ddr4 3200mhz, rtx 3060 12gb, 850w platinum psu, b660m gigabyte motherboard and a intel i5 13600kf. Couldn't have had a better first rig.my current build now is a hellhound raedeon rx 7900xt,gigabyte  z790x ax gaming motherboard,  850w platinum psu, 64gbs ddr5 6000mhz,Samsung 980 1gb ssd, intel i5 13600kf(watercooled).best upgrade I made for 1440p gaming.",Positive
Intel,"Congratz!  Also, what's that wallpaper? Looks awesome",Neutral
Intel,I don't understand the stickers. Ryzen CPU Check. Both Radeon and GTX graphics? Is one discrete and the other dedicated? Just needs an Intel sticker to make it more confusing.,Negative
Intel,"That is not a ""gaming rig"" not even close my friend. ;)",Negative
Intel,3050ti?  Gaming rip?  No,Neutral
Intel,"5 or so years ago i built my first real gaming rig. Few years later i bought new laptop, because i already had a gaming pc i wanted something capable to game but didnt want to spend too much money for high end. So i bought Lenovo legion with same gpu - 3050ti.   What i soon realized was that i love the comfort of being able to browse the net and game from the couch, being able to put my legs up, put my back against a big pillow etc... Despite having much more capable pc, i was lazy to play on it and was using gaming laptop instead 90% of the time.   Problem is, 3050ti in the last year or so really starting to show its limitations, most of new games are straight up unplayable anymore. Even on lowest settings it doesnt run newest games at playable standards, probably because of only 4gb of vram. Hogwartz, Rachet and Clank, Remnant 2 all pretty much unplayable unless you drop resolution to 720... I really regret not putting few hundreds more for something like 3070. (unrelated, but why stepping up laptop gpu tiers are so expensive, for example 3060 to 3070 is like 300e here for essentially same system otherwise, 3070 to 3080 is like 4-500e extra more, its even more expensive overall than desktop gpu tiers for way lesser performance improvements...).   So my advice would be, if you can get at least 3060, its worth it to pay up a bit more, ideally 6600m or 4060, those few hundreds increase in price will be worth in a few years.  ALSO: not sure if OP knows, but these laptops come with shittiest ram, upgrading it to a dual channel dual rank is pretty much a must as it nets 10-20% more performance.",Neutral
Intel,There is nothing wrong with laptops I'd build my own if there was a market readily available for it like a desktop.,Neutral
Intel,No!,Neutral
Intel,"This isn't exactly what I'd imagine hearing the words ""gaming rig"" but I'm glad you like it. Happy gaming!",Positive
Intel,Ignore the haters dude. Congratulations on your new gaming laptop. Gaming is something that is supposed to make us happy not cater to snobs. Not everyone needs 4k 60fps. If it satisfies your requirements it's the best rig for you. I have a Ryzen 5 2500u and I game on that so I am super happy for you.,Positive
Intel,"People take things too seriously, and tend to forget that every enthusiast was once a novice.  This is OP's first gaming setup; they deserve to feel that excitement.",Positive
Intel,who even buys the 'very best' laptop anyway?,Negative
Intel,Lenovo Ideapad Gaming 3,Neutral
Intel,"It's just the default one that came with the laptop, theres a red and a blue version probs from AMD and Intel lol.  You should be able find them online.",Neutral
Intel,Its a ryzen cpu with integrated radeon graphics and an nvidia dedicated gpu. Its completely normal and not really confusing.,Neutral
Intel,APU + dGPU,Neutral
Intel,"You know what I mean.   It's my very first PC/Laptop set up so I'm definitely calling it a rig, it's way more powerful than anything I've owed before it.",Positive
Intel,"It games, it's a gaming rig. :)",Neutral
Intel,you do realise not every wants or needs a 4070 to play games they like,Neutral
Intel,I upgraded the ram. Has 16gb ddr 5 dual channel now,Neutral
Intel,"Thanks dude! I think people forget that not everone needs the same kinda thing from a computer which is why they sell multiple models.  This is great for what I need it for, and I feel like a kid at Christmas, happy to be able to dip my toe into PC gaming, finally!",Positive
Intel,You get my point at least you should,Neutral
Intel,"Just to let you know , these laptops have different versions , one I have is something like Ar15mhqp or some shit like that   It is less powerful than yours but decent enough",Negative
Intel,That is okay if you see that way and congrats.,Positive
Intel,"There's nothing ""rig"" about a laptop. Laptops aren't built to be rigged.",Neutral
Intel,"No, it's a notebook.  Also have a low end gpu dude.  Not a game rig. It's nice but is not that.",Negative
Intel,"I know, but the same laptop or any other with 6800m or other AMD gpu would have been a much better choice lol",Neutral
Intel,All that matters is that your happy with it,Positive
Intel,Glad you’re enjoying it. People something think that unless you get something with everything maxed out then you couldn’t be happy lol,Positive
Intel,"Yeah I'm just happy to be able to start playing around on some games on PC now. I've wanted to play flight simulator on pc instead of my xbox for ages now becuase of all the addons,and now i'll have access to those :D",Positive
Intel,"3050ti isn't really low end. And it will do for everything I need it for. I have an Xbox too for the most modern stuff.  People think that everyone has to have 4k 60fps in every game these days and not everyone needs or wants that, especially in a first machine.",Positive
Intel,"for you maybe, this fits my needs perfectly.",Positive
Intel,"Yup I am more than happy!  I've only ever had and played with integrated graphics on any other laptop I've had so this is a huge step for me.  I will play most brand new stuff on my Xbox for the best ecperience but stuff like KF2 I can now play a 1080p ultra and get 100s frames, could only play that before at 900p low.",Positive
Intel,Well you say that right now but one day a game you really wanna play will come and your pc will be shitting itself trying to run it. If I had the money I would buy the best rig.   But if you don’t have a lot of money like me it’s all about price/performance,Positive
Intel,That is the important that you are happy but still the reality is one.  Just enjoy the notebook.,Positive
Intel,"not too similar but I can recommend grabbing dirt rally 2.0 on a steam sale with all DLC for 8€ / $10 or whatever it is, generally games that simulate some things are pretty much at home on PC",Neutral
Intel,"Yes, the 3050 ti is a low end card.",Neutral
Intel,I have an Xbox too so I'll be able to play it on there anything that is out of spec.,Neutral
Intel,I have a friend playing Baldurs Gate 3 on a 980/ You don't know what low end is apparently lmfao,Neutral
Intel,It's better than the 1650 and the 2050 though. And they were the other choices.,Positive
Intel,And that is normal the game don't requiere a big gpu to run the game is a mid / low graphics.  Besides the 980 was the top of that generation. The 3050 is the low end of that particular generation.,Neutral
Intel,"Dont let the haters bring you down dude, if you are happy, then enjoy your new laptop!",Positive
Intel,"Both cards are the lowest version of his generation.  Again the notebook is nice but is a low end ""gaming"" notebook and again the point is that you ENJOY IT and play many games.",Negative
Intel,He needs to know what he have but also I congrats OP for your purchase. The point is that you enjoy it.,Neutral
Intel,I really would like to know how it will compare to AMD Ryzen AI MAX+ 395 (Strix Halo) APU?,Neutral
Intel,That naming scheme really is complete and utter dogshit,Negative
Intel,Sounds like fooz will be getting a taste soon? Christmas or Q1 2026?,Neutral
Intel,Can we just toss random darts at tech words and numbers to assign a different naming scheme to each and every different Intel Product?,Neutral
Intel,I'm looking forward to check how those series will perform!!,Positive
Intel,"This isn't a ""halo"" product. An upgrade to Arrow Lake/Lunar Lake when it comes to iGPU, but not Strix Halo. It won't cost as much as well.",Neutral
Intel,look forward to new APUs,Positive
Intel,Wait for Nova Lake AX. It will have 48 GPU cores instead of 4/8/12.,Neutral
Intel,Likely will be worse since it has only 1/3 the power usage and significantly smaller chip size.,Negative
Intel,"also why is everything in a lake. like really that's the last place you want your chips to be.  and why is it 255k, 285k and so on instead of just 250k and 280k.  and why is it arc xe3 and not just xe3 ahh  so many weird conventions really. no consistency at all. just pure confusion.  apple and nvidia get it right too, it's really not that hard.",Negative
Intel,"'Strix Halo' is lees about being a halo product, because it’s just an AMD internal naming scheme for the set of specific APU products. APUs that will follow 'Strix Halo' will be called 'Gorgon Point'. Since 'Strix Halo' is already out and the Panther Lake in question is about to come out it might be better to compared it to the next AMD 'Gorgon Point' APU line? But I still think it can be compared to 'Strix Halo'. At least I’m looking forward to see the comparison.  Edit: changed 'Strix Point' into 'Gorgon Point' since that’s what I meant.",Neutral
Intel,You’ll be waiting till 2027 on the amd side.,Neutral
Intel,"Lesser power usage doesn’t always equals to performance losses.  See the Apple Axx SOCs and the Qualcomm snapdragon SOCs.  Even with the x86-CPUs over time they had more performance gains while maintaining almost the same power consumption.  Not anymore, but they could catch up with better chip design?",Neutral
Intel,>also why is everything in a lake. like really that's the last place you want your chips to be.  Easier to cool them,Negative
Intel,"Stop trying to find meaning in the naming scheme. Patience grasshopper, all will be revealed in time.",Neutral
Intel,Like how is every chip company so bad in naming stuff?  Intel names things from geographical stuff to avoid politics etc. So I get the lakes  The numbers I dont.,Negative
Intel,"> APUs that will follow 'Strix Halo' will be called 'Strix Point'.  What? No, that's not what any of these terms mean. The Strix lineup is already out. ""Strix Halo"" is literally their halo, big compute/iGPU product.  PTL will compete with normal Strix Point and its refresh next year.",Neutral
Intel,Really? This should be good for Intel in 2026.,Positive
Intel,They are like 20% more efficient.  You are talking about 200% more efficient if it hits the same performance.    They might as well make it a mobile chip if that is how good it is. Would be great on phones.  Would be better than most pcs right now on 5W envolope.,Positive
Intel,'Strix Halo' is a product line as 'Gorgon Point' is also/another product line. Both have different variants.  In the case of 'Strix Halo'  [Check](https://hothardware.com/news/amd-strix-halo-cpu-rumors)  Find the official lineup here  [AMD](https://www.amd.com/content/dam/amd/en/documents/partner-hub/ryzen/ryzen-consumer-master-quick-reference-competitive.pdf)  Edit: changed 'Strix Point' to 'Gorgon Point' since that what I meant.,Neutral
Intel,👍,Neutral
Intel,"Yes, Strix Point lives *alongside* Strix Halo. Strix Point is already out, and came out *before* Strix Halo at that. It is not the followup to Strix Halo.",Neutral
Intel,You are right. I meant 'Gorgon Point'.,Neutral
Intel,"Gorgon point is just a refresh of strix point. These terms mean less in the next gen anyway as the follow on products have quite different names. Medusa point isn't really the successor to gorgon point, for example.",Neutral
Intel,My brain hurts and I’m still confused,Neutral
Intel,"I think him saying ""unreleased products"" could mean it's still coming.",Neutral
Intel,I think B770 is locked in and to release shortly isn't it? sure I saw a leak of packaging details etc.,Neutral
Intel,"We finally got reviews for the B50 and it is a SFF gem! That said… I wish Intel would get better at promoting upcoming products, they seem to be slowed down by the restructuring.  I’m going with the flow, whatever will be will be. I’m waiting for Dividends to kick back in so I can retire with Intel.",Positive
Intel,"Boy, I am glad they picked an easy naming convention.   /s  After Xe, Xe1, Xe2 no doubt next would be Xe3, then Xe3p  After A-series, B-series, no doubt next would be C-series.    Too bad those generations do not line up",Positive
Intel,Intel is not serious with dGPUs,Neutral
Intel,"If it was coming, you expect them to talk about the future even a little bit. Instead, nothing.    They cancelled Celestial over a year ago now. Sounds like things haven't improved since.",Negative
Intel,"I think him saying he doesn't talk about unreleased products is 100% bullshit. He talks about upcoming tech constantly. This is not just unreleased, it's unannounced, unclaimed and nonexistent outside of pure speculation.",Negative
Intel,B50 is interesting once the software is there (planned Q4).,Positive
Intel,"Yes, they are literally just playing. It's all a game lol",Neutral
Intel,"They have released about the same number recently as AMD, I'd say they're pretty serious. The B50 is a pretty compelling product too, big features for the price.",Positive
Intel,They are as serious as AMD.,Neutral
Intel,This interview happened during the quiet period so I don't think he could talk about the future,Neutral
Intel,"They cancelled Celestial over a year ago now. Sounds like things haven't improved since.   MLID people showing themselves.  Tom Peterson previously said Celestial discrete hardware was already done and they were working on Druid. So if they cancelled it, it must have really sucked.  I am sure after they release Celestial 70 series beater, MLID will come and say they cancelled the 90 series beater and it's for sure dead from now on!",Negative
Intel,Where did you hear that it was cancelled?,Neutral
Intel,"We are supposed to get B60s also but they will likely be very limited and part of Battlematrix. Intel is moving very slow with Pro and Consumer GPUs and they can’t rely on TSMC for supply and obviously are not ready to manufacture through IFS? We are getting left in the dark, all we can do is wait.",Negative
Intel,There has been no update regarding celestial dGPUs internally.,Neutral
Intel,AMD is skipping a generation to focus on the next. Intel has lost its focus on GPUs. These are not the same things.,Negative
Intel,Lol no. ARC was 0 margin product. Now it's fate depends on the whims of VPs not engineers.,Neutral
Intel,"You could likewise point not that there was no word about dGPUs in the PTL presentation either. I think people need to accept that it's just not happening, at least for the foreseeable future.",Negative
Intel,"> MLID people showing themselves.  I'm not getting this from MLID.  > Tom Peterson previously said Celestial discrete hardware was already done  No, that's absolutely false. Actually watch the interview instead of reading reddit comments. He said Xe3 (specifically in PTL), not Celestial, was done. And this was after the PTL tapeout was announced, so that didn't even tell us anything new.   And as we now know, they don't consider that even in the same family as what would be Celestial.",Negative
Intel,Ex-Intel coworkers/acquaintances.,Neutral
Intel,>There has been no update regarding celestial dGPUs internally.  Do you have internal information?,Neutral
Intel,Why would they? Battlemage is not even finished. Battlemage is not even 1 year old yet. They will still release B7XX gpus and probably B3XX.  I expect them to tallk about celestial by next year.,Neutral
Intel,> Intel has lost its focus on GPUs  So despite them repeatedly telling you they have not... they have?,Neutral
Intel,"Battle mage is not a 0 margin product...  I know how much silicon cost etc due to my job. Believe me there is at least %30 gross margin in Battlemage and that is assuming somehow Intel got a worse price compared to my small ass company.     It's not profitable due to amount of R&D it takes to develop it, Intel earns a significant chunk for each Battlemage sold. They are simply not as greedy as Nvidia and AMD to earn market share.",Negative
Intel,Dude XE3 even has some test shipment reports etc. It's too late to cancel.   Sure if it's not good maybe we will only see B580 replacement.    But it's literally impossible and stupid to cancel it right now. Especially given how much gross profit they made from B580,Negative
Intel,"So, no news story has come out stating that?",Neutral
Intel,"Yes, through my ex colleagues",Neutral
Intel,"No, I don't listen to them, they have a nasty habit of downplaying bad situations. I'm going by their actions.",Negative
Intel,"> Dude XE3 even has some test shipment reports etc  Celestial wasn't base Xe3, and didn't tape out before cancellation. What test shipments are you referring to? PTL?  Btw, they still aren't saying anything about BMG G31, and that was much further along than Celestial was.   > But it's literally impossible and stupid to cancel it right now.  You can cancel a product at any point before it's released. Anything else would be sunk cost fallacy. Surely you're aware of the massive budget cuts and layoffs they've announced. Not everything can survive.   > Especially given how much gross profit they made from B580  By all reports, BMG still wasn't profitable for them. Hell, even if it *was* profitable, doesn't mean profitable *enough* for Intel to keep funding it in this environment. They're prioritizing spending reduction, not profit maximization.",Negative
Intel,No. Or at least not from any reliable source. Obviously discounting MLID and his ilk.,Negative
Intel,"You don't even know that?   Lip Bu has been hiring gpu designers not firing them.   Most of the Cuts are from foundry side and slightly from gaudi side.    Intel if anything is focusing on gpus to create AI inference gpus.    Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.      You should check your sources, Lip Bu would cut 14A before he cuts Inference gpu side.",Neutral
Intel,BMG was 0 margin product,Neutral
Intel,"> You don't even know that?  What do you claim I do not know?  > Lip Bu has been hiring gpu designers not firing them.  Celestial was cancelled under Gelsinger, as well as several rounds of client GPU layoffs. If Lip Bu is hiring anyone, it's not to build the team back up again.   > Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.   I am specifically talking about client graphics, yes. There's shared work, and arguably should have been more, but they were quite different. Client iGPU, client dGPU, and server dGPU were basically all separate SoC designs.",Negative
Intel,"It wasn't though?  As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  I don't believe there is nothing Intel can do convince you. I heard these news every single time.     Also just look at job listings, there is many for gpu development.",Negative
Intel,"> As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  Go ahead and point out where *I* said ACM or BMG were cancelled. I have no idea who ""you guys"" are, nor do I care what others are or are not saying.  > I don't believe there is nothing Intel can do convince you.  Well yes, if they cancel a project, and I know they have, I'll say they cancelled it. The way Intel can convince me to say otherwise is by... not cancelling projects. And you *do* realize they haven't talked about client dGPUs past BMG in many, many months, right? It's not like Intel's really denying anything.  I'm not sure what you're looking for here. Should I lie and pretend not to know what I do? To what end? It's not like Intel's harmed by me saying they cancelled such a project. That's not something you can keep a secret indefinitely, and anyone who *really* cares already knows.  > Also just look at job listings, there is many for gpu development.  If you lay off 10 people, 5 more leave on their own, and then you backfill 4, that's still a net loss. They still need some people, but not as many as they had, and not for client dGPU.",Negative
Intel,"Intel have not claimed there to be upcoming Celestial, or more Battlemage or anything like it. How did their silence convince you of anything?",Negative
Intel,Think pretty easy naming. Any X infront just automatically means better iGPU,Positive
Intel,"Looks pretty snappy at \~75% faster than the top-end LNL chipset and this is with pre-release drivers.  Granted TDP is probably higher.  If they get the drivers cleaned up then it might release a bit higher which makes it a viable (though still materially weaker) thin and light XX50 dGPU model alternative some some of the market.  Should do well addressing the 'I want a thin and light laptop, but I want it to have an ok GPU' crowd.",Neutral
Intel,"Nice to see the G14 with an Intel CPU. Thought the lineup was AMD only tbh, while the larger G16 laptops get Intel.",Positive
Intel,"Panther Lake 12 Xe3 performance looks great to match RTX 3050 laptop performance because the entire chip only draw half power of RTX 3050. Seems like using 18A BPD really paid of to reduce CPU consumption by a lot with the helps of Tsmc N3E for iGPU.    Also it's so weird to see Asus Zephyrus G14 with Intel chip, usually it always has Amd CPU paired with Nvidia GPU. I heard G14 is pretty popular gaming laptop but this laptop during full load can use 120w+ power.    Using Panther Lake 12 Xe3 will makes this laptop looks even more appealing because it reduces power requirements from 120w+ to 45w but still giving about the same GPU performance which is insane. This is a massive game changer for people who use laptop as portable gaming machine and to those who travel a lot. I can totally understand why Asus this time use Panther Lake for G14.",Positive
Intel,Naming for these chips are terrible,Negative
Intel,"Can Asus send me this laptop for review? I have 14 followers on snoozetube and 60% are probably bots, but bots are human too?",Neutral
Intel,Can't wait for 14inch laptops with actually good battery life and convenience than the cheap gaming laptops it's going to kill,Positive
Intel,"Nah, should've kept that info at the end like every other Intel and AMD CPU ever made.  But otherwise this branding really feels like AMDs APU line, where they had to emphasize their iGPU was better than average.",Neutral
Intel,"The possibility of being nearly 100% faster than Lunar Lake in some tasks, and minimum possibly 50% faster while being able to fit it into a sub 3lb/1.5kg design with a 80+wH battery is going to really nice. If the 4 LP-e cores scheduling work well and maybe a more efficient OLED panel you could easily get true 24 hrs use on x86",Positive
Intel,"If the game could be 50–60% stronger, that would be That would be a killer",Neutral
Intel,"That’s been true for the past generations, but it looks like it will change this generation",Neutral
Intel,Its GPU part isn’t 18A at all — it’s actually N3E and 4Xe3 integrated graphics use Intel 3.,Neutral
Intel,Still better than Ryzen 365 AI pro MAX+,Positive
Intel,"I disagree, GPU focused = X (like Xe3). Just takes getting used to , but otherwise it follows the same 3 7 9 scheme that probably didn't make much sense at first either :)",Neutral
Intel,https://browser.geekbench.com/v6/compute/compare/5050048?baseline=4771132,Neutral
Intel,The typical consumer doesn't know anything about the last letter. Having it in front will be much more successful to communicate to consumers the difference.,Neutral
Intel,Yea putting ai the model name is disgusting 😂,Neutral
Intel,I can't wait for the Ryzen 688S AI Pro MAX+++,Neutral
Intel,"I agree with this and now snoozetube creators are doing 128gb reviews for the 365 AI Pro Max+ and glossing over the fact that it costs decent money but lacks any kind of power when compared to discrete GPUs.  Amd continues to pump out expensive APUs that are mediocre, while doing everything related to Radeon half heartedly.  Why is that?",Negative
Intel,"Nerds argue over names for tech products but will eventually figure out some kind of logic in why they named it that way. Entire generations need to be released and compared.   As for average users they will always be perpetually clueless and unfortunately will become influenced by an influencer with no integrity and or a store associate who has been trained on scripts that make the most money for the store.  God help us all, I pray for Jesus - just like Pat Gelsinger, who will get no credit for the Intel turn around.",Negative
Intel,Ryzen metaverse Ai max++ 3D Hypercache macroboost,Neutral
Intel,I think it's rather on point. the 395+ is a beast for running large MoE AI models. It's value for money in that respect is almost unbeatable.,Neutral
Intel,you forgot the x3dx2    when both cpu tiles are stacked on 3d cache tiles.,Neutral
Intel,Ultra TypeR S-line AMG M Bi-Turbo CCXR LM Harley Davidson Edition,Neutral
Intel,"Very very few know anything more than that, usually completely unaware that there's a whole SKU number after that.  How many times do you hear stories about some user proudly boasting about having an i7, only to find out that it's like a 6th gen, and they don't even realize / believe that something like a i3-12100 is actually a better CPU.     The average user understands the difference between, say, a Core Ultra 5 and 7, because the ideal of 3, 5, 7, 9 being product tiers exist in plenty of industries, like BMW's product line. Bigger number = more performance. How? By how much? No clue to them.  So since the average user is going based off just the name 5, 7, or 9, having that X visible in a location they'll see is certainly very important. They'll notice the X.",Neutral
Intel,"It would be like a Chromebook named ""Chromebook CloudCompute+"" just because that's what those are built for",Neutral
Intel,I personally prefer them Name it Ryzen 3 / 5 / 7 / 9. It’s easy to understand and easy to compare to intels naming but sadly both companies have ruined it now.,Negative
Intel,That's honestly sounds even more cringe. Can you imagine Amd Ryzen 9 395X3DX2 AI Pro Max+? That's ridiculously bad LMAO,Negative
Intel,BMW Individual M760i xDrive Model V12 Excellence THE NEXT 100 YEARS,Neutral
Intel,I actually did my research and found out that core ultra 5 125u is not much different from core ultra 7 155u... Ended up buying ProBook with core ultra 5 125u and saved money for upgrading the ram and SSD,Neutral
Intel,"but i think its gonna happen aye,  i wonder if the RAM bandwidth needs for AI benefit from cache like games do, or are they better slapping more ram channels on it...",Neutral
Intel,"Yeah, all of the U chips within a generation are the same physical chip, just different bins (usually tiny clockspeed differences). I don't think they even have core count differences any more for the most part.",Neutral
Intel,Same core counts too,Neutral
Intel,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Neutral
Intel,"To summarize from the article for some folks:  6300 puts this GPU as 33.4% faster than the 140V, 71.3% faster than the 890M, and 54.3% of the 8060S. Just over half of Strix Halo's top config.  Bear in mind though, that this benchmark favors ARC GPUs compared to gaming results. The 140V and 890M are roughly equal and this benchmark puts the 140V as 28.4% faster.",Neutral
Intel,"Ugh, who is naming these products?!?!? Between the internal code names (which are now publicly used) and the actual product names, it's a mess. As bad as monitor naming.",Negative
Intel,"Sick, Panther actually sounds good, and Lunar/Arrow on mobile already sounded good to me. Please keep pumping those iGPU numbers Intel Arc engineer bros.  AutoCAD on integrated Intel graphics WHOOOOOO  AMD step it up next gen please, Intel is no longer a static target in graphics, kthx.",Positive
Intel,This sound more realistic and not as good as the 50% better Intel announced.   For Intels sake I am hoping the 50% is real,Negative
Intel,... with early drivers and probably no decent support yet. Its also probably an engineering sample? 😉,Negative
Intel,Wow pantherlake is looking enticing. Should I wait for novalake? Will novalake have same battery life like lunar lake or pantherlake?,Positive
Intel,"It's going to cost like $10,000, right?",Neutral
Intel,"The top lunar lake has 8Xe2 cores, and this has 12Xe3 cores. 30% faster seems… bad? Like why not 50%+ given 50% more cores and architectural improvements?",Negative
Intel,"We have seen how Lunar Lake on MSI Claw 8 AI+ performs on early firmware and drivers, Intel even managed to improve Lunar Lake performance up to 30% after that. I expect 50% performance improvement on Panther Lake is very possible.",Positive
Intel,"It really is a mess, someone should've been fired long ago.",Negative
Intel,Intel wasn't a static target since the 10th gen. They've been pushing (at least in terms of performance) since then. It's mostly the efficiency which stepped back. And they messed up their current lineup in terms of performance and pricing but they've improved for sure.  Next gen seems good for them though. Hope they price it well. AMD isn't zen 1/2 either. They're bumping core numbers too and introducing some new cores as well.,Neutral
Intel,"I mean, why?  Is AMD going to be coming out with anything more powerful than the HX 370 in that power range?",Neutral
Intel,because 50% more cores need 50% more power for 50% more performance.,Neutral
Intel,"There are so many variables here that we have no way of accurately saying that.  It's possible that Xe3 is simply treated more like RDNA in this benchmark, and that the Xe2 140V is unfairly biased towards in that way. If that is the case, the 890M and 8060S may be better metrics to base off, and we do see over 50% gains for a 50% wider GPU than the 890M.  It's also possible this is a power-limited scenario. Like for like on TDP, this would be a solid improvement given the GPU both has to be moved off-tile compared to Lunar Lake, and has twice as many CPU cores to fight for power.  This could be pre-release drivers not getting the true 100% out of the hardware or silicon with non-final clocks still being tuned. The point is there's no way to know for sure.",Neutral
Intel,I don’t understand why you people continue to use that MSI claw to demonstrate “intel improved lunar lake performance post launch” when there are millions of lunar lake laptops that never had performance issue of that one handheld.,Negative
Intel,"they're keeping their jobs because this is technically better than the past. back in the day you'd have model numbers exclusive to a retail store, much less specific to an OEM, because businesses wanted to feel like they got a bespoke deal.   These days the SKU naming is mostly for accounting purposes, while the ""real"" naming decisions are made by the OEMs. basically most people are buying the Thinkpad/Yoga/ROG ""brand"" rather than the specific processor model, which only a much smaller crowd bothers to comprehend. It's like how people buy the Steamdeck rather than whatever APU is in there, which is fairly old at this point.",Neutral
Intel,"Energy efficiency is huge in mobile but I was explicit in saying they were no longer a static target in graphics, where they were not making significant or impressive gains in iGPUs for many years, it seemed. Now they are making one of the best iGPUs on the market.",Positive
Intel,"Possibly yes.   Current AMD handheld chips are better than Lunar Lake performance. Assuming normal cadence. Next generation would be similar performance to Panther Lake.     I was hoping more of a leap frog, rather than similar performance. %50 would be a very clear edge.",Positive
Intel,Is the TDP fixed to be the same between LL and PTL in this test?  I genuinely don't know.,Neutral
Intel,But LNL's TDP is too low compared to H45 cpu,Neutral
Intel,"> because 50% more cores need 50% more power for 50% more performance.  insightful, and easy to forget given feature bragging    :)",Positive
Intel,"Mostly because people with LNL laptops are less likely to games since gaming are not the point of those laptops -> less testing, whereas the Claw 8 AI+ is a gaming PC handheld, so it is mostly use for gaming purpose and thus have more people testing for its performance.",Neutral
Intel,"As weird as it sounds actually Lunar Lake improvement mainly comes from MSI Claw not laptop, that because majority people who use MSI Claw give feedback the most which is why Intel focusing on the handheld first then laptop.    Intel even use Claw as benchmark for Lunar Lake compared to laptop. You can read from this article :  https://arstechnica.com/gadgets/2025/04/intel-says-its-rolling-out-laptop-gpu-drivers-with-10-to-25-better-performance/   Also Claw got BIOS update way faster than any laptop with the same chip so it helps Intel to mitigate power and boost behavior to maximize the performance. There is so many bug reports on Arc forum, most of them are Claw users, that's why laptop got benefits too.",Positive
Intel,AMD isn’t coming out with a better next gen iGPU for mobile (Gorgon Point) since it’s a simple refresh. Same arch with same CU count based on rumors.  The generation after will be competing with Nova Lake,Neutral
Intel,"Amd handheld with Z2E isn't better than Intel Lunar Lake, you can see the comparison on MSI Claw sub or even on youtube. Z2E in most game is 10% slower than Core Ultra 7 258V, it only won in the game where Intel GPU performs bad.    Not to mention at 17w Z2E losing badly to 258V, Intel is on their own league, it's not even competition for Amd.",Negative
Intel,Intel is gonna have better integrated graphics than AMD,Positive
Intel,I have no idea.,Neutral
Intel,"We don't know what TDP was run here, and both MTL amd ARL H have been 28W outside of the Ultra 9 SKUs. It's entirely possible this was run at 28W, which is also in reach of Lunar Lake's boost envelope.",Neutral
Intel,The lunar lake laptops are tested by reviewers all the same as the strix point ones. Regardless it’s misleading because it’s not “lunar lake” but rather the performance profile/boost behaviour of that specific MSI CLAW that was changed and people act like it’s lunar lake’s drivers doing “30% magic”.   It’s not.,Negative
Intel,"And is there documented so-called “large” performance improvements on LNL systems that already performed as expected on day one (it tied 890m on high power and was always better at low power, talking about real games not 3dmark)? Or was it only bringing the Claw back to where LNL should always perform?",Neutral
Intel,"True. I don't see how the next igpu from Amd going to use rdna 4, it won't even support fsr 4. Meanwhile Intel going to push their igpu tech even further with XeSS XMX 3 with Xe3 and Xe3P, they will be way ahead of Amd in igpu market especially when Intel Lunar Lake already beating Amd Strix Point and Z2E.   Intel also dominating mobile market. Honestly it's not looking good for Amd.",Negative
Intel,Yeah people act like strix point is in that segment..... It's not.,Negative
Intel,🫨,Neutral
Intel,Wouldn't be the first time.,Neutral
Intel,"Oh, I guess it be like that.",Neutral
Intel,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",Positive
Intel,"""we are tending to prefer e cores now when gaming""   That's very surprising",Neutral
Intel,If only they release a high level C-card. Battlemage kinda never scratched the mark.,Negative
Intel,"The point of Xe3 being actually battlemage instead of Celestial, this is so horribly confusing. I can't understand what's going on, nor why they would do that.",Negative
Intel,I’m guessing Xe3P will be on Intel 3-PT.,Neutral
Intel,"Tom is a funny guy, love it when he gets camera time",Positive
Intel,Cool hope they will manage to release it soon not in 12 months when everyone will be talking about RTX 6000 and RDNA5/UDNA or leaks about them.,Positive
Intel,"When is the panther lake reveal going to be, CES?",Neutral
Intel,"e cores are surprisingly powerful now, and games are getting more multithreaded. It might be better to spread them out over 8 less power hungry cores than 4 P cores and then spill over to the slower cores. Especially in a laptop environment where efficiency matters and the GPU is usually holding back gaming performance so much that CPU performance is really not that important.  In fact, I bet it's mostly down to not spilling over to e cores from P cores. That's what causes slowdowns and stutters usually attributed to e cores. The engine has to wait a little longer for workloads assigned to e cores than to P cores. The if the main thread is assigned to a P core that's all well and good, but if sub tasks are distributed among the P cores and then something important is assigned to a slower e core it holds up the sub tasks and in turn the main thread. But most importantly, it doesn't do so evenly. Maybe you don't need something assigned to e cores, or the e core task is light and doesn't hold up the other threads. Then things will run at the pace of the P cores, but every now and then you'll have a slowdown.",Neutral
Intel,Wonder if there's some energy star requirements.,Neutral
Intel,"Shouldn't be. That's what the math has always said.  As soon as software can scale to ""many"" cores, the tradeoffs that go into single powerful P style cores are a bad deal. Both frequency and sequential optimizations (like multi-level branch prediction) scale poorly.  Gaming benchmarks tend to have a really strong feedback loop that favors last gen hardware design though. So seeing the benefits to E cores for gaming requires 1.) e-cores exist for a while 2.) some games optimize for them.  Long term, an optimal Intel-style processor design will look something like 4 ""legacy"" P cores + dozens of E cores.",Negative
Intel,It's either N3P or 18AP.,Neutral
Intel,That’s why rumors want Intel to throw away P-cores (Israel team) and only keep E-cores (Austin) to make Titan lake a unified core with only E-cores remaining.,Neutral
Intel,>and games are getting more multithreaded.  Can you give me a few AAA examples of modern games which get a noticeable improvement in AVG. FPS or 0.1/1% lows when using more than 8cores/16 thread's ? I'm genuinely curious which games you are talking about.,Neutral
Intel,"Really should focus on the Main/Sub thread part. Most games will usually only load up 1-3 cores, with the rest of the cores only used for incidental workloads with lower priority and sync requirements (Multithreading is hard yo, Multithreading with latency requirements is mindnumbing).  This makes plenty of games very suitable for the P/E architecture, as long as you have enough P cores for the Main threads, the E cores will be perfectly sufficient.",Neutral
Intel,"Energy Star probably won't be around for too much longer, at least in it's current form.  https://www.npr.org/2025/08/13/nx-s1-5432617/energy-star-trump-cost-climate-change",Neutral
Intel,That makes me imagine in a different reality if Xbox had remained with Intel and NVIDIA and their next generation would use an Intel E-core based SoC with NVIDIA chiplet.,Neutral
Intel,It has already happened. Stephen Robinson - heading the Austin team - is now the lead x86core architect.,Neutral
Intel,"Off the top of my head, Starfield, Bannerlord, BeamNG, UE5 games due to how the rendering pipeline works, etc.",Neutral
Intel,"Most games of the past will only load up a few cores, but that's beginning to change. Cyberpunk 2.0 loads up 16 threads/cores easily, and some others like Battlefield 6 also scale pretty well. If you have less than 16 threads on fast cores like Panther Lake and Arrow Lake, then you can run into issues. Or if your 16 threads are on one die and any spillover has to go across the SoC die like with AMD and probably Nova Lake.",Neutral
Intel,"Seeing how Nvidia working together with Intel to make integrated high end GPU i can see the possibility of Xbox using that chip, maybe in the future.",Positive
Intel,I'm tired of AMD slop consoles and handhelds,Neutral
Intel,">Starfield  [Not really](https://youtu.be/BcYixjMMHFk?t=1015), game doesn't benefit from more cores/threads.  >Bannerlord  Sadly, there's no benchmarks of different CPUs that I could find for this specific game, but I do know that this game heavily relies on a good CPU, but without decent data(review), which shows multiple CPUs tested, it's hard to understand the benefits of more cores/threads,  >UE5 games due to how the rendering pipeline works  It's true that UE5 can utilize 8 cores / 16 threads, but more than that? I'm not sure, if possible, provide a review/video which shows that UE5 scale with more cores/threads, so far, it seems that it is limited at 8c/16t - big channels rarely add UE5 games to their CPU benchmarks, but I found Remnant 2,[ and it doesn't show any benefits](https://youtu.be/3n537Z7pJug?t=1056) of more than 8 cores.  I heard that ""games are getting more multithreaded"" like 4-5 years ago, and in most cases, it wasn't true, with more than 8c/16t almost no games scale on CPU-side, and even when they do, in most cases it's a minor improvement over 8c/16t configuration, like 1-3%.",Negative
Intel,"UE5 by default does not support multithreading well. actually I don't think async shader is even considered a default feature yet despite being added two years ago. Only the editor compilation step uses all threads, but it doesn't need to react to user input so it would be more surprising if it didn't use all threads.  If you're seeing good thread use in a UE5 game is thanks to the developer breaking up work with their own engine changes.",Negative
Intel,"Considering AMD’s lag in gaming performance especially in ray tracing, I would be totally onboard with that.",Neutral
Intel,Huh????,Neutral
Intel,"12900K being that high in the Starfield benchmark, like actually within error margins next to 7800X3D despite being an older platform with half the L2 of raptor lake and lower clock speed than even 7700X, shows that the game benefits from more cores and threads very much. Yeah 9950X should outperform it technically I guess but the split L3 between two CCDs probably holds back the advantage of having more cores.  Yeah sadly there's no benchmark for Bannerlord. But supposedly it does disperse its tasks to plenty of threads.   Here's some benchmarks showing core and thread usage in a few games. While the benchmark tool doesn't get into the details of how those software behave obviously, but at least according to the graphs some games like Tarkov really suck at distributing its tasks to several cores but some others like Starfield, Cyberpunk, and Space Marine 2 are actually quite good at it.  [Will This Do? — Intel Core i5-14600KF vs. i5-13600KF vs. R7 7700X vs. i7-14700KF Benchmark - YouTube](https://www.youtube.com/watch?v=HY_weucfLPQ)  [Isn't it a blast now? — Core Ultra 9 285K benchmark. Comparison with R9 9950X, R7 9800X3D, and i9...](https://www.youtube.com/watch?v=q1zAX1VNdf0)  I haven't seen a core thread benchmark for UE5 games yet, but that's just how the engine is supposed to work though.",Neutral
Intel,"To be fair, Sony seems to be fed up with AMD lagging behind and are pushing them to do better. Consoles will likely continue to stick with AMD, at least for the next generation. Intel graphics aren't quite ready yet, and Nvidia is going to be way to expensive unless they take an old SoC and repurpose it like Nintendo.",Neutral
Intel,">12900K being that high in the Starfield benchmark  It's because Creation Engine 2 worked better with Intel hardware until AM5 X3D chips, and 12900K great result in this test just shows that it's still a good CPU with 8 Performance cores, not that it's core count matters in any significant way - for example, 7700X delivers identical performance with lower core count/threads.  >lower clock speed than even 7700X  Clock speeds can't be compared between different architectures, different architecture = different efficiency, what matters is IPC.  >shows that the game benefits from more cores and threads very much  [here's a better example where more intel CPUs are present in Starfield test](https://youtu.be/XXLY8kEdR1c?t=29m5s), as you can see, increased core count/threads past 8 performance cores and 16 thread's is meaningless and won't provide any noticeable performance improvements, 6 faster P-cores on Intel 14600K provided better result than slower 8 P-cores on 12900K.   Going from 7950X to to 7700X results in 5 FPS loss, which is like 2-3% less FPS, and it's mostly because of lower clock speed(5.7Ghz Vs 5.4Ghz boost).   >but some others like Starfield, Cyberpunk, and Space Marine 2 are actually quite good at it.  That's the point, I agree that modern games can and will utilize 8c/16t configuration, but so far I don't see a trend of games becoming more ""multi-threaded"" past that point, as I replied to that person, I'm curious what games now become more multi-threaded than few years ago, I find that observation speculative and without evidence - more cores and threads is a great approach for workloads, but games generally don't care about it past a certain point, in this case it's 8c/16t.  It could change with next-gen consoles, if they will use AMD 12 core CCDs and games will be optimized to utilize more cores&threads.  What's important now is good cores or good cores+X3D cache, not core count - even 7600X3D with 6 cores and 12 threads is better in gaming than most Intel/AMD non-X3D CPUs with way more cores.  Edit: typo",Positive
Intel,"Well you said that games are optimized for 8/16 and yet gn bench shows 7600X only gets 3-4 less avg fps than 7700X in baldurs gate 3, a highly multithreaded game, it has better 0.1% lows even, though I'm sure that one is a contained incident since BG3 is very dynamic and hence results are not always repeatable. Same with Starfield.   That's because in video games, core count matters less than how fast the CPU can access or manipulate very dynamic types of data in a random memory address.    X3Ds are not just ""whats important now"". They're going to win in old or new games, and they're winning in games because they have a large buffer of low latency data. Not winning in productivity benchmarks because productivity is about processing matters more than accessing data, and its more easily multithreaded too in many cases.    But having more cores is still more advantageous in software and games that can occupy them.",Neutral
Intel,">Well you said that games are optimized for 8/16 and yet gn bench shows 7600X only gets 3-4 less avg fps than 7700X  I said ""can and will"" not that every game is going to benefit from it.  Point of discussion was to prove that games are using more than 16 threads or they're not - it seems like you overestimated Starfield, UE5 reliance on core count/threads and most games care about 16 threads at most.   >core count matters less  Yes, that's why I replied to that person and asked him to provide me with modern AAA games which are more ""multi-threaded"" now than AAA games a few years ago, I feel like what he ""observes"" is what he wants to believe, and not something that actually happened with optimization in games.  >X3Ds are not just ""whats important now"".  AMD sells 9950X3D, best productivity and gaming CPU, if you really need both(workloads/gaming) it's the optimal way, for an average user, only benefit of E-cores is lower power draw when idle - but they do matter more if you need those workloads, I agree, but we discussed gaming performance and core reliance.",Neutral
Intel,If Panther Lake is going to be part of Xe3 then what does that mean for a possible B770?,Neutral
Intel,"What even is Intel naming at this point, before it was the one thing about their products that was at least *kinda* consistent.",Neutral
Intel,"B770 is probably likely coming out at the end of this year, I don't see any reason why they would not sell it.",Neutral
Intel,Seems like Intel chips make way more sense for portable pc consoles than AMD going forward.,Positive
Intel,"Transcript for the 'What's Next' section:  >What's next? Well, I I would say uh larger investments in graphics and AI. And so if you imagine in a PC, this workload is not going away, right? The world is becoming more graphical. The world is embracing AI and more rapid than any other technology ever. So I would expect Intel to respond to that, right? It's not like we have to guess what is what is big and what is coming. We know and what's coming is more AI and more graphical experiences.  With this being said, I hope that means that the Nvidia/Intel deal means that the ARC division is not going away any time soon.",Neutral
Intel,"Panther Lake with Xe3 without any doubt will be generational uplift. Intel Lunar Lake which is last year chip still able to put Amd newest chip like Z2E to the shame.   Xe3 with 12 Xe cores not to mention with XeSS XMX is going to be terrific combo, seeing how disappointing Amd Z2E i can see Intel going to steal iGPU gaming market from Amd. Next year could ended up with more OEM using Intel chip for handheld.",Positive
Intel,I found it interesting that he specifically mentions improvements in performance per area for Xe3. A big complaint of Xe2 is that the die size is noticeably larger than a similar performance GPU from AMD or Nvidia. Hopefully this means better financials for Intel’s graphics division with this new architecture.,Positive
Intel,Says nothing about discrete graphics. Think that's clearly dead at this point.,Negative
Intel,"Lmfao now I’m feeling conflicted about my very recent MSI Claw purchase, but I suppose panther lake handhelds will likely launch middle of next year anyway",Negative
Intel,"Tom says that variable register allocation and 25% thread count per Xe core will improve utilization which was a problem with predecessors. He says they've been ""addressed"". It looks like at least perf/power wise it's 25-30% at the same process compared to the predecessor, and that may be true for perf/area as well.",Neutral
Intel,Sounds like you're clearly wrong.,Negative
Intel,"I know how you feel but honestly Claw 8 AI+ is still amazing handheld, at least your Claw will aged better than mine because i only have A1M Ultra 7.   Claw with Panther Lake will be released in between Q2-Q3 2026 so you don't have to regret anything.",Positive
Intel,"I get the feeling that handhelds with 12 Xe3 cores are going to be really expensive, so maybe you still didn't do too badly.",Negative
Intel,"Lmao, sure. Any day now...",Neutral
Intel,I don't think so considering how expensive LNL was,Negative
Intel,"Why is panther lake igpu simultaneously Xe3 and Arc-B series ? This is so confusing.       Also it states being 50% faster, but it also has 50% more compute units.",Negative
Intel,What should be the desktop gpu equivalent?,Neutral
Intel,So around a 3050 laptop performance?,Neutral
Intel,The problem is how much is the price?,Neutral
Intel,they should really do better on GPU,Neutral
Intel,"Despite the vast improvement more Xe3 cores than we saw Xe2 cores in Lunar Lake, we're only seeing minor gains in average framerate performance at the same wattage based on the internal benchmark, though there are much more meaty improvements in the 1% lows. I guess its bottlenecked by its RAM bandwidth. Also I'm sure Xe3 will scale better beyond 17W. I wonder how the 4 Xe3 SKUs will fare, that's a good way of knowing how much RAM bandwidth affects things.",Neutral
Intel,"Just because you have 50% more physical units, doesn't mean you'll automatically get 50% more performance. Remember when the Z1E came out and it was supposed to be 50-100% more power than the steam deck?",Neutral
Intel,RTX 3050 Max-Q or 1660 ti Max Q laptop card. Within that range.,Neutral
Intel,Yes,Positive
Intel,"For them to do a 'big iGPU' design they would need to do a few things:  1. Use a different socket.  The iGPU will get too big for the existing one.  2. Upgrade the memory bus to 256bit.  That uses some power and silicon.  3. Add cache to the chip for the iGPU to use (to make up for the poor bandwidth they get from LPDDR5, even with a wider bus)  If they don't do this then the chip (which again won't fit in their default socket) will be massively bandwidth constrained to the degree that it's pointless.  Basically, do all the stuff that Strix Halo did.  Issue is Strix Halo didn't sell very well, the idea is a proof of concept and the concept needs faster memory to really work well.  LPDDR6 might help a lot at getting these big chip ideas from a XX60 level to a XX70 level, but they still require a lot of expensive chip work and a new motherboard design custom to the big socket.  I suspect we'll get there eventually, but it'll be a few years before people start calling it quits on dGPUs.",Neutral
Intel,"50% is not minor...  Also they said >50%, or greater than 50%. It looks like 70-80% to me.  Since scaling isn't linear I expect 4 Xe3 to come close to Xe2 or even equal it.",Neutral
Intel,Maybe I'm confused about what they're advertising but weren't they just going head to head with 4060s with the b580? Wouldn't 3050 range be a huge step backwards?,Neutral
Intel,"Damn, still behind my old 1060 pc",Negative
Intel,And some of the tiles are internal not on TSMC.,Neutral
Intel,"On-package memory doesn't raise the device price from and end user perspective. It's a margin challenge for Intel because OEMs want the margins from the memory, so Intel needs to pass it along at cost.   Technically, on-package memory can even be cheaper because it can let you simplify the rest of the PCB. Iso-speed, that is.",Neutral
Intel,Are you saying you expect people to not want discrete gpu's in the future?  What about the direction of gaming would make you think that?,Negative
Intel,"> Since scaling isn't linear I expect 4 Xe3 to come close to Xe2 or even equal it.  The 4 Xe tile is on Intel 3, so it'll have a significant clock speed deficit vs the 12Xe tile.",Neutral
Intel,"No I'm talking about the internal game benchmarks they shared. Yeah the GPU architecture  itself is a lot better on paper, but they probably got bottlenecked by the low bandwidth of 128-bit LPDDR5. The uplift in 1% lows in some titles is impressive though.   [power-efficient-gaming-26.jpg (2133×1200)](https://www.techpowerup.com/review/intel-panther-lake-technical-deep-dive/images/power-efficient-gaming-26.jpg)  And yeah I'd not be surprised if 4 Xe3 is going to be closer to the 8 Xe2 in LNL than people would expect.",Neutral
Intel,The b580 is a full power desktop card. The one they're advertising here is a tiny little integrated gpu on a laptop processor,Neutral
Intel,Well it's not out yet so these are just predictions. Better to be conservative with estimates than overpromise.,Negative
Intel,"We already knew they cancelled desktop variants though, Panther lake was going to be around Arrow Lake laptop performance no matter what.",Neutral
Intel,"If people can get a XX60ti level iGPU in a laptop, which means you don't have to worry about MUX switches (hardware or software), possibly have a cheaper platform (if scaled) since you don't need a separate GPU board, have access to a ton of RAM (many mid-range GPUs are RAM limited), and due to reduced complexity and the ability to better dynamically manage power can get a more effective slim chassis with less throttling... then yeah, it'd be pretty good.  The next big jump is LPDDR6 which looks to potentially hit a 50% bandwidth increase and marginally lower latency.  Combine that with a wide bus and you're looking at enough to power a XX70 series mobile chip.  The framework is immature but it could be competitive in the future.  We'll see!",Positive
Intel,"Buddy, that's 140V with optimizations. They are telling you what they have done to further optimize their graphics, including drivers and power management. and that they'll further apply this in the future. Did you really look at the slide?",Neutral
Intel,"This benchmark has nothing to do with panther lake, it's about the effect of their power management software on lunar lake before and after it is applied.",Neutral
Intel,Isn't that lunar lake before and after?,Neutral
Intel,It has around 120GB/s of bandwidth.  Thats pretty close to the 6500XTs bandwidth.,Neutral
Intel,"Dota2 mentioned, I like Intel.",Neutral
Intel,"this has been leaked a lot, but not confirmed",Neutral
Intel,"Oh laptop gaming sure, but that's pretty low level gaming. Don't most of the people at that level just put up with whatever laptop/prebuilt stuff is available anyway? I really don't think it's going to move the discrete market that much?",Negative
Intel,I was missing the context. Thanks for the heads up! Yeah that last update for LNL driver was mad good.,Positive
Intel,Oh really? Wow thanks for the heads up. I really missed the context.,Positive
Intel,Apparently so! I made a mistake.,Neutral
Intel,It's 153GB/s.  9.6GT/s x 128 bit width x 8 bit/byte.,Neutral
Intel,\>HUB  That's shovelware and I'm not clicking that shit.,Negative
Intel,"They've tested it with just three AMD cpus. Weird pairings. I would pair B580 with any i5 from last couple years. Seriously their testing is getting less and less ""real life"" and more for the purpose of a good clickbait.",Negative
Intel,"I couldn’t believe the video I was watching, the thumbnail was pure clickbait. They fixed one game, and improved some others a little.  And NEVER an Intel CPU in sight. I like the B580, but it deserves better ""testing"" than this.",Negative
Intel,The problem is still here. 9060 xt with 5600 is faster than B580 with 9800x3d and B580 still loses 8% with 5600 instead of 9800x3d. Which means that cpu bottleneck starts much sooner for ARC gpu.,Negative
Intel,"Is this related to historically Intel compiler not compiling code efficiently for AMD cpus? Does it also make Intel CPUs overhead better? If yes, then they really fixed something. If no, then they wrote code normally for AMD cpus as well as Intel cpus.",Neutral
Intel,"the B580 is a modern gpu, it should be paired with a modern cpu, preferably an Intel one, going with old school Ryzen cpus is a bad path to take.",Negative
Intel,"The problem is that this title makes it sound like they fixed the cpu overhead issue in every game, but that's not the case. Some games are still affected. Obviously any fix is better than nothing, but depending on what games you want to play, you will still see an overhead issues. It is nice though that in some of the games the overhead problem has been addressed though.",Negative
Intel,"Haven't watched the video, but damn.   If rue, HUB has officially become clickbait garbage. Not outright lying, but intentionally conveying misleading half-truths. Starting at least with the AMD fine wine video.",Negative
Intel,"It's meant to test low end, mid and high end.",Neutral
Intel,"The exact CPU model is hardly relevant. The point was to show CPUs of a certain power level. Not sure how this makes it less ""real life"" as if all three CPUs shown weren't extremely popular for their time. Also not sure what one would achieve when using Intel CPUs specifically.",Neutral
Intel,"These guys absolutely *refuse* to test with an Intel CPU, it's very weird.",Negative
Intel,"5700x would have made a lot of sense compared to 5600  12400f,13400f should be included as well",Neutral
Intel,Exactly. Its nice that some games were fixed but between that and calling the issue (entirely) fixed there is a big gap.,Negative
Intel,"Indeed and it is something very interesting, because the reason the driver overhead is an issue on 6 core cpu's is because modern games use all 6 cores, so if the driver has a big overhead the CPU has nu free cores available for the driver.  Especially Intel CPU's with more (e-)cores in their budget CPU's should probably suffer a lot less.   I would expect an i5-12400 to suffer the same as a Ryzen 5 5600,  But an i5-13400/14400 has an additional 4-e cores to deal with the overhead. The 13500/14600 even has 8 e cores.      I would very much like to see the comparison   i5-12400, i5-13400, i5-13500, Ryzen 5 5600(X)",Neutral
Intel,Would not have mattered. the cpu bottleneck in the intel driver is single-thread limited and largely due to draw-calls being stored in system memory as a buffer for the frametime render. (at least as far as my debugging could lead me)  (that's also why its less impacted on ddr5 systems no matter what CPU),Neutral
Intel,"They pushed the 6 core parts too hard as ""good budget parts"" for too long to not keep using them as their reference...  Even when we're seeing games that now have strokes when they don't have 8 cores to play with.",Negative
Intel,"possibly. Honestly weird as hell to not through in intel cpu results with the same gpus so they can see if it's a all cpu overhead issue, or an issue with amd cpus with intel gpu. Like even one game if intel/amd gave about the same performance you could likely rule it out somewhat.  Also weird to say they fixed it if by the sounds of it to say it's fixed if a very limited number of games were updated to work better in also seemingly limited scenarios. That's more like finding a specific problem on a specific game they were able to improve rather than an underlying fix which if implemented should help everywhere.",Negative
Intel,"No, B580 also performed poorly on i5-8400 and 10400 (both are 5600X-esq)",Neutral
Intel,"The driver compiler is for GPU, not CPUs.",Neutral
Intel,9800X3D is the fastest CPU for gaming on the market. The usage of Intel or AMD CPUs doesn't inherently matter.,Neutral
Intel,It's been official for awhile. Funny how it takes a positive Intel title for people to realize.,Neutral
Intel,Do you have peer-review data that disputes the data they presented?  Their video covers the scope of what got fixed.,Neutral
Intel,Welcome to the age of influencers...,Neutral
Intel,"Nvidia 5000 series has overheads issues even with 5800x3d... like 20% performance hit, you can fix it by capping fps or using Intel cpus. AMD CPUs always act weird when reaching 100% utilization.",Negative
Intel,That's not how CPUs work in games.,Negative
Intel,Then they optimized drivers.,Neutral
Intel,Do you have peer-review data that confirms it?,Neutral
Intel,The video shows the B580 not having driver overhead issues with the 5600X but we don't know if it did previously on anything other than Spider-Man. His tone suggests the 5600X performance used to be worse but he provides no data other than Spider-Man to suggest it was for other games.,Negative
Intel,The fact that a switch from a 5600 to a 5700 with hardly a frequency gain eliminates the overhead issue suggest that is kind of  how games/drivers work.,Neutral
Intel,Which also fixed performance on 5600X,Neutral
Intel,"But that's now how games utilize CPU's.  What you get from games is usually 1 main thread that takes up one big core, then many less intense threads, that may or *may not* utilize the other cores, and to varying degrees. AKA you can have mainthread use 100% of core 0, but cores 1,2,3,4,5 are all 50% used, leaving 50% for other tasks to run freely.",Neutral
Intel,"Eventually you get the drivers where they need to be. People love to whine about Arc drivers but on the iGPU side people never complained. The community is never, ever happy with anything. Focusing their eggs on the Higher performance parts and then a slower LTS cadence keeping the old stuff alive is the right move.",Negative
Intel,"They will update, just not day 1 for certain games. Who's playing with integrated graphics?",Neutral
Intel,"cant say i blame them for scaling down driver updates for pre-arc graphics hardware, ever since they announced intel was getting into gpus proper ive been wondering how long it would take before this happened.  lets be honest no one was needing monthly igpu updates for 2021 hardware that was so underpowered it couldnt even tie with a GT1030 ^(\[why does everything come back to 14nm\].)",Negative
Intel,Running an N97 Mini PC with integrated and noticed the recent driver branching. I'm still impressed that Star Trek Online is playable on it.,Neutral
Intel,Developers have been complaining about iGPU drivers for years. Matrix multiplications in OpenGL driver are still broken on a lot of integrated chips. It's driving me nuts every time I get a user report. 😭,Negative
Intel,"me man omg, im on intel iris xe and wanna try out battlefield 6 so bad. Arc Graphic users are having fun with day 1 driver updates.",Positive
Intel,Get some ARL CPU’s which have matrix math in hardware.,Neutral
Intel,"I'm unable to claim my copy of Battlefield 6 from the recent Intel promotion even after redeeming the code.  I redeemed my promotional code last month which added the ""Intel Gamer Days"" bundle to my account that includes Battlefield 6. The instructions say to return October 3rd to claim the key. I've been attempting daily to claim the key but just keep getting the message ""We are temporarily out of codes and are working to reload our system. Please check back soon.""  The game launches tomorrow so this is pretty frustrating. Is anyone else having an issue claiming their BF6 key?  I put a ticket in with Intel support but haven't heard anything yet.",Negative
Intel,"bought the newegg bundle. I got a master key but my PC is running linux. The linux HST tool doesn't even have an extension, no .exe no nothing. I don't know how to run it.",Negative
Intel,"Hello there, it seems that after updating my Windows 11 I can't scan for new intel drivers by using the Intel Driver & Support Assistant. It scans for a while but then it gives me the message: ""Sorry, something went wrong while trying to scan""; it seems to affect the Intel Graphics Software as well as it keeps looking for updates without success. I've already tried to reinstall DSA and deleting any folders associated with the program, rebooting Windows and also uninstalling the Windows Update. Any thoughts?",Negative
Intel,"After updating to W11, these awful KILLER apps and processes have returned and reduced my upload speed to less than 1mb!!! Normally I get 30mbs up.   Dell PC, 32 gbs RAM, Intel i9. x64 W11 Home 24H2, OS 26100.6584, WIRED Ethernet direct to router/modem.  The Killer thread is closed and directs users to post in this thread.   The same thing happened in W10 on same machine but problem was fully solved by the KillerSoftwareUninstaller tool, which also disabled Killer permanently from coming back on every single update.  However, I tried the same tool after the update to W11 and it removes Killer, and everything works better for a day or so, then suddenly everything slows down and Killer is back.  PLEASE help me get this awful suite of useless Killer tools off of my machine forever!   Thank you!",Negative
Intel,"I have a i7-13700 that may be the root of my issue. Long story short, MATLAB is getting an Access Denied error when trying to link to an Imperx framegrabber using videoinput. Imperx FrameLink works fine. The function that it crashes at works fine when called from a different way (getAvailHW), and only crashes when trying to call it through videoinput. The exact same setup works just fine on another computer. Even when no cameralink cable is attached, it crashes on this one. BIOS is fully updated. The CPU passed the checking program Intel provided.   Looking up this issue, people had similar problems, and they linked it to the CPU issues with 13th and 14th gen. Intel released a chart for what settings you should have in BIOS, but it does not cover the 13700, only the K variant. What settings should I have in BIOS to rule out Intel?  Chart: https://community.intel.com/t5/image/serverpage/image-id/56057i81282C3BCB9162A9",Negative
Intel,"Is this normal activity for a 265K at Idle?  https://imgur.com/SSSOB9F      The multiplier and clock speed keeps jumping around all over.  There are no background processes running, nothing in task tray, closed all applications except HWINFO to record this.      I am running into an issue where I feel like either my CPU or my AIO is bad.  The build is new, completed yesterday.  Idle temps hang around 30-34C; which seem fine.  But as soon as tasks start happening, it jumps all over up to 80C.  I have a 360 AIO on it.  Gaming even, it hit 83C earlier.  It doesn't make sense to me.  I have built a few 265K rigs and not had this kind of fluctuation in temperature before.  Even running cinebench on previous builds, the CPU never went above mid 70s.  But this newer machine is hitting 80+ in games.  Previous 265K PCs have ran mid 50s in games.  Sometimes 65 depending on the load, but never 80+.",Negative
Intel,"Hi u/Mydst For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues. They'll be able to assist with your Battlefield 6 code availability and on how to resolve this issue.   You may also check these links:  [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Neutral
Intel,"Hi u/afyaff For this kind of inquiry, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry and they may help you on how to input the master key in your PC.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Neutral
Intel,"u/Adrian-The-Great  For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues .They'll be able to assist with your non-working BF6 code from the Newegg bundle promotion.     Additional Information:   [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Neutral
Intel,"u/Broad_Remote3101 This could be a compatibility issue or something with the installation process. Just to help figure this out - are you using a laptop or desktop? If it's a laptop, could you let me know the exact model? Also, what graphics card and processor do you have?  You might want to try following this article too and see if that does the trick. Let me know how it goes!  [Intel® Driver & Support Assistant (Intel® DSA) Results in “Sorry,...](https://www.intel.com/content/www/us/en/support/articles/000026895/software/software-applications.html)",Neutral
Intel,"u/musicmafia77 Based on what you're describing, this sounds like a classic case of Windows 11 being overly ""helpful"" with driver management. To get to the bottom of this and find a lasting solution, I'll need some specific details about your setup.   First, could you check Device Manager and tell me the exact model of your network adapter and current driver version? Since you mentioned you're using wired Ethernet, I'm curious if this is actually a Killer-branded Ethernet adapter. Also, when Killer inevitably returns, which specific processes are you seeing in Task Manager - things like ""Killer Control Center"" or ""Killer Network Service""?  Here's what's likely happening behind the scenes: Windows 11 maintains a driver store cache that can automatically reinstall what it considers ""preferred"" drivers, even after you've removed them. The system recognizes your Killer hardware ID and keeps pulling that branded driver through Windows Update, completely overriding your manual removal efforts. This explains why the KillerSoftwareUninstaller works temporarily but then everything reverts after a day or so.   To truly fix this, we'll probably need to perform a clean driver installation process that involves purging the entire driver store of Killer-related files, blocking Windows Update from auto-reinstalling them, and potentially replacing them with Dell provided drivers rather than Intel generic drivers (since Killer hardware is actually based on Intel chipsets). It's more comprehensive than just running the uninstaller, but it should give you that permanent solution you're looking for.",Neutral
Intel,"PL1 should be  65W , PL2 is 219 W you can cross reference this value on this specs https://www.intel.com/content/www/us/en/products/sku/230490/intel-core-i713700-processor-30m-cache-up-to-5-20-ghz/specifications.html   However following intel recommended settings once you load BIOS with microcode patch it should follow the default, you may check also with your motherboard manufacturer",Neutral
Intel,"Update your bios to the latest version and set it to intel default settings. If the issue is the same, request a replacement",Neutral
Intel,"Thank you so much for your help!  The ethernet adapter is Killer E2400 Gigabit Ethernet Controller.  I am not super tech savvy, but what you are describing makes sense and is consistent with what I've read about this problem. I will need simple step by step instructions to fix as you suggest. Thanks again!",Positive
Intel,"u/musicmafia77 After checking our records, I found that the unit you're inquiring about has been discontinued-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html). While direct support is limited to self-help resources and community insights from other users, I'd be happy to extend some steps that should help resolve your issue.     Based on your initial description, here's what I recommend:     First, please review this article-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html) and follow the troubleshooting steps provided here-[Clean Installation of Wireless Drivers](https://www.intel.com/content/www/us/en/support/articles/000022173/wireless.html). Once you've updated to a newer wireless driver, I'd suggest reaching out to Dell Technologies for assistance with downloading the appropriate OEM driver for your specific system.     If you prefer a more direct approach, you can:  * Visit Dell's website and download the wireless driver for your adapter by entering your service tag in their support portal * Alternatively, run Dell SupportAssist after completing a clean installation of the wireless driver     For additional troubleshooting, here's another helpful article that covers complete removal of Killer software issues, which may be relevant to your situation.-[How to Perform a Clean Installation to Solve Most Intel® Killer™...](https://www.intel.com/content/www/us/en/support/articles/000058906/wireless/wireless-software.html)     If you've tried these steps and the issue persists, I'd recommend coordinating directly with [Dell's support team](https://www.dell.com/support/home/en-us), as they're best equipped to assist with OEM-specific products and can provide more specialized guidance for your particular system configuration.",Neutral
Intel,Keep em coming.  I saw Intel was hiring for a GPU stack software engineer in Toronto. I expect more good things for the Boys in Blue.,Positive
Intel,Does newegg have a pro or business section for the B50?  I heard CDW or similar outlets may start selling B50 / B60?  I want to look into four B60s for an AI workstation but the B60 may not release through consumer chains.  Has anyone seen the B60 for sale?,Neutral
Intel,Quad mini display port ? WTF,Negative
Intel,It is so much weaker than the B580 though with only 70w... Very niche use cases.,Negative
Intel,On the other hand noone buys workstation GPUs on NewEgg. For B50 being bestseller means 95 orders.,Neutral
Intel,Looks like some stores have it available to order currently. Central Computers has it available as a Special Order.   [Central Computers -ASRock B60 24gb Creator $599](https://www.centralcomputer.com/asrock-intel-arc-pro-b60-creator-24gb-graphics-card.html),Neutral
Intel,Better than micro-hdmi.,Positive
Intel,The entire point is that you can power it from the motherboard.,Neutral
Intel,It doesn't matter. The card sells and that's what matters.,Neutral
Intel,Its a small workstation card like the Quadro 1000 etc..,Neutral
Intel,I guess I got lucky. I was able to get one off Newegg before they sold out. Should be arriving in the mail today.  Hopefully they will get more in soon. I'm surprised Newegg had so few of them.,Positive
Intel,or the usual supply chain is sold out so users buy them where they are available,Neutral
Intel,Those are professional cards. Your company should be getting them from different sources.,Neutral
Intel,"Most people buying these don't own or buy for companies. ""professional cards"" means nothing. if you have a use case, then buy it. Companies like Intel *love* consoomers like you.",Negative
Intel,"It's small but a decent performer, if you ask me. Intel needs to keep them coming.",Positive
Intel,A single slot version would've been nice.  70w doesn't need a dual slot cooler.,Neutral
Intel,12W idle power... here does the dream of low idle server... :I,Neutral
Intel,Where can I buy one?!,Neutral
Intel,"I should have waited, i could have gotten two!",Neutral
Intel,Where can we buy this? Seems like limited supply and retailers are going to try and raise prices. All the tech companies need to keep the retailers under their boot.,Negative
Intel,"I know it's not for this, but how does it do at gaming?",Neutral
Intel,I bet this thing is great for Plex transcoding,Positive
Intel,Where are ppl buying these need 1 for my TrueNas Server.,Neutral
Intel,they need if they want to survive,Neutral
Intel,I was thinking opposite ... can we have full passive model?,Neutral
Intel,nvidias alternatives here are also dual slot,Neutral
Intel,yeah single slot and x16 for pci4 boxes,Neutral
Intel,"I haven't internalized the docs or tried it myself (on an A310), but apparently some folks have managed to bring idle power down quite a lot on Arc cards.    e.g. https://www.reddit.com/r/IntelArc/comments/161w1z0/managed_to_bring_idle_power_draw_down_to_1w_on/",Neutral
Intel,"How is 12w not low idle usage? The average cost per kwh of electricity in the US is 17 cents.   1000/12 = 83.33333  That means this card costs 17 cents for every 83 hours its on and idle.   There are 8760 hours in a year. 8760 hours / 83.3333 hours is 105  105\*.17 = $17.87. Meaning, if this card were to run year round non stop at idle, it would cost you $17.87.   Even if the card used 0 watts at idle, you're looking at a maximum savings of $17.87 per year.   Would it be nice? Sure. Would I consider it a ""dream"" to save $17 a year? No.",Neutral
Intel,Check Newegg for a resupply. I was able to get one from there.  My [build](https://www.reddit.com/r/sffpc/comments/1nhwb85/sktc_a10_intel_build/) for ref.,Neutral
Intel,It's currently available for pre-order on Newegg and B&H.,Neutral
Intel,"Yeah, a multi-billion corp that's been dominating the CPU market for decades , with a 75% market share needs an entry level Pro card to survive. Take your meds.",Neutral
Intel,Something akin to a KalmX 3050,Neutral
Intel,"Late reply to an old link. But appreciate you sharing! I have the exact setup as the op of that post.  So I'll be trying this, asap.",Positive
Intel,"company that is falling apart, that had very bad run recently, firing a lot of crew, canceling huge investments that already costed billions.        Having foundries ... that no one want to use and not having load because products don't sell.             Big fall quickly as they have huge costs that cannot be easily scaled down.               Yes Intel is in very bad shape, it needs a good product that will be competitive and what is most important rebuild the brand that is in shambles now.",Negative
Intel,"There is good news, the employees got their free coffee back",Positive
AMD,"It's especially bizarre when you realise how many products RDNA2 is present in. Not just discrete GPUs, but current gen consoles, rembrandt APUs in a lot of budget laptops, and Zen 4 and 5 desktop CPUs (though there are probably not many people gaming on these at least). Dropping RDNA1 support is understandable given how few products lines it was used in, but RDNA2 is still extremely relevant on top of not being that old.",Neutral
AMD,AMD this is not how you retain/gain market share.,Negative
AMD,Never miss an opportunity to miss an opportunity.,Negative
AMD,"AMD actively blocked official Zen 3 support on B350/X370 motherboards for over a year (despite working beta BIOSes from Asrock and people successfully cross-flashing B450 BIOSes onto B350 boards), screwing over Zen 1 adopters who bought into AMD in 2017 and to which AMD owes its revival on desktop to. They only relented after Intel released non-K Alder Lake parts. So not too surprising.  *typo",Negative
AMD,Radeon being really fucking stupid exhibit #1523,Negative
AMD,"Ya know the more i think about this the more pissed off I guess.  PC gaming has become increasingly unaffordable in recent years. I'm one of those old school ""60"" buyers. ya know, $200-300 for a GPU, and I plan to use it for around 4-6 years.   Since around 2018, we've been getting squeezed. First it was ray tracing and all that AI upscaling crap. Nvidia decided to make the 2060 $350 and threw anyone under that under the bus. Yeah, the 16 series existed, but let's face it, those cards aged like milk too.   Then COVID happened, the price inflation happened, and I was stuck on a 1060 through the pandemic. It lasted me well, but I knew it was aging, and quite frankly, GPU manufacturers stopped giving a crap about us. THey gave us the 3060 for $330, which was a joke. And this was before the ""inflation"" all the yuppie tech bros on these subs use to justify ridiculous prices.   I managed to hold it together through COVID, and upgraded during the price crash in 2022-2023. AMD was hit first, the RX 6600 type cards could double my 1060's performance for under $300, while Nvidia was STILL charging $340 for the 3060. It was a joke. Nvidia didn't get it, I dodn't wanna pay over $300 for a fricking GPU so I bought a 6650 XT for $230. It was an amazing deal at the time.  When the 7000 series launched around then, and the rest of the series was rolled out into 2023, honestly, it offered a very poor value for the most part. RDNA2 cards were literally sold next to RDNA3 ones for most of RDNA3's lifespan. And quite frankly, there was often little reason to buyt RDNA3, it was more expensive, and offered little in price/performance over RDNA2.  RDNA2 was eventually phased out in the higher price classes, but for a while, the RX 6800 was sold along side the 7700 XT, the 6650 XT was sold next to the 7600, the 6700 XT/6750 XT were in their own niche that AMD never really replaced, and the 6600 remained supreme as the ultimate budget option, competing with the fricking RTX 3050 and thrashing it in value.  heck, I was recommending RX 6600s as recently as this year, and really only stopped when we FINALLY saw the market move with stuff like the RTX 5050 for $250, the RX 9060 XT 8 GB for $270, and the RTX 5060 for $300. Even then, what killed the RX 6600 more than anything was the fact that AMD just FINALLY phased it out of production a few months ago and it was no longer a cost effective recommendation.   And now, AMD pulls the rug from under RDNA2 users? This fast? Really?  I dont care if they axe RDNA1, the writing has been on the wall for RDNA1 for a while, no mesh shaders, RT, DX12 ultimate support, yeah, RDNA1 is done. But RDNA2 is still a viable architecture. Maybe it doesnt support the fancy AI bull#### for FSR4 or whatever. Do you know how much I care? Not at all. Im in a budget class where I dont care if I get the fanciest graphics. I just wanna RUN THE GAMES. And to do that, i need DRIVERS. There's no reason they can just make the games run FSR2/3 and allow older users to use that. Sure, maybe it will be blurrier, I admit FSR is an inferior upscaler, but it works, and again, I'd rather run a game with a little blur going on than not at all.   To not support the series with drivers, when this could render the GPU incompatible with new games is a DISGRACE. I bought my 6650 XT planning to use it until 2027, hoping that my next sub $300 GPU would offer twice the performance and more than 8 GB VRAM. I dont wanna upgrade now, spending $250-300 for 10-50% performance improvements (seriously, the 5050 is barely faster than the 6650 XT in raster) and 8 GB VRAM. I just dont. As such, I guess I'm gonna be stuck using an obsolete and no longer supported card for the mean time. Thanks a lot AMD. And oh, guess what, given that Nvidia is no longer charging like 50% more than AMD for the same class of product, I'm probably not gonna go AMD next time. Not if this is how I'm treated as a consumer.  Ive always been like at least somewhat biased toward Nvidia/intel because it seems like every time I buy AMD I get burned for some reason. SOmetimes its lack of VRAM, poor driver support, support for some BS API, or just poor performance like on older AMD CPUs I've owned. But yeah, they're just kinda giving me flashbacks to the late 2000s/early 2010s again when they didnt support their products while nvidia did. And I'm kinda feeling burned right about now.  EDIT: since the responses im getting seem snarky and trollish so far, you send me that kinda crap, you're getting blocked.",Negative
AMD,I guess this confirms project Red Stone will not be coming to RDNA 2 cards (FSR4.0 support),Negative
AMD,"It is not uncommon for AMD to have shorter driver support or continued optimization.  for that matter, fine wine was just marketing for, ""we have shitty drivers at launch"".",Negative
AMD,"Outrage is fun, but no.  AMD state they will maintain it for bugs which is all it needs. These card are well understood by developers and game optimization is the ***developer's job*** not AMD's.       AMD might be involved in performance optimizations with new cards is when developers aren't familiar with them and new shaders (often based on old shaders) are found to not work well with very different architectures but that's not happening with cards designed in 2017.       There is no dropping of driver support. These are mature cards, known ISAs, they are already optimized for in current game engines.      If a developer is optimizing for RDNA2 and finds a bug AMD will still fix it. If AMD wants a new feature to run on these cards they will optimize for it. If there is a new DX12 or Vulkan extension AMD will add it. If a downstream customer has a problem (think Steamdeck, consoles etc) AMD will address it.  We went through this outage cycle when AMD [split the driver stack for Vega back in 2023](https://www.tomshardware.com/pc-components/gpus/its-curtains-for-polaris-and-vega-as-amd-reduces-driver-support) and this really feels like a repeat.",Neutral
AMD,How did this get past Lisa Su?,Neutral
AMD,"This is being blown hugely out of proportion. People keep pretending like they dropped total support. No, they just dropped day one game dev driver support.",Negative
AMD,I'm still happy with my 2060 12gb.,Positive
AMD,"Radeon division saw the success and good will the cpu/mobo side of amd has gotten for their am4 longevity and support, and went: yeah, let's do the fucking opposite.   This is beyond dumb. Probably switching back to intel/nvidia in the next year or two when i switch gpu",Negative
AMD,this is why I never see Radeon as a serious product.,Negative
AMD,....so the SteamVR issues with DirectDraw on my 6800XT machine causing horrible lag are never going to be fixed. Sick. That machine literally only does VR and has been running 24.3.1 for months because of that.,Negative
AMD,I'snt the 6000 series like their most popular cards on the steam hardware surveys? They wanna lose even harder almao.,Negative
AMD,It’s not the first time. And obliviously not the last time amd do that . Why it become a shocking news ?  For example   Like RX580 . Mega popular and card have less then 4years of active support . Then 3 years of bug fixes   I stil remember amd fans bragging about rdna2  like is better choice then rtx 30 at least in terms of memory .. funny,Neutral
AMD,"Man the 6000 series was the ONE gen where by god, they competed at the top end and had extra vram all the way down the stack as a way to sell their cards as a longevity thing...   a 3080 vs 6800XT was a tough choice, and if it wasnt for the evga queue I would have likely grabbed either that was for sale for MSRP in the depths of pandemic.  and IIRC it was the most successive gen recently if you looked at steam hw survey  and now its getting axed, while even the 20 series are not...",Neutral
AMD,Hardware Unboxed never cared when AMD almost didn't support Ryzen 5000 on X370 but now they're furious. Too funny.,Negative
AMD,I'll probably be back to Nvidia next year at this rate lol.,Positive
AMD,"HBU pointed out that Asus ROG Xbox Ally, a console that launched ~2 weeks ago, is RDNA2!!",Neutral
AMD,I mean they dropped VEGA non-security support within the same year new APUs with VEGA were released. I guess this is one way to force AMD laptops to buy new chips,Neutral
AMD,Don't forget Steam Deck.,Neutral
AMD,I only bought my RX 6750 XT in 2023. 😭,Neutral
AMD,Anyone owning a 7900XTX should plan on selling theirs within the next 2 years before it loses complete value,Neutral
AMD,This is exactly how there marketing team expects them to retain market share and increase it. Stir the zealots up by getting everyone to trash talk your company. That way the Zealots will go buy the newest product to prove the trash talkers wrong.   I mean what could go wrong with such a solid straight.,Negative
AMD,AMD back to doing AMD things.,Neutral
AMD,"It wasn't even that nice, msi just dropped the updates for all their boards and more or less told AMD they could drop them as a board partner or shut up",Negative
AMD,I literally bought a 12600k out of spite because of this only for my X370 Crosshair to get support for the 5000 series.,Negative
AMD,You stole my comment.,Neutral
AMD,"It was a bit of a clusterfuck though, as the amount of storage space available for the BIOS varied between motherboards (which is *why* they originally didn't want to support the old motherboards).",Negative
AMD,You have to wait until 2028 since you're a 2nd hand & sub $300 buyer to get 2x 6650xt perf.,Neutral
AMD,So you're saying you should've bought a 3060.,Neutral
AMD,Reddit: nah it was totally untapped performance.,Neutral
AMD,"Even if that were true, and the drivers are just shit at launch, you pay for the performance you get at launch, and any additional performance that comes from future driver improvements is free performance you didn't pay for.",Negative
AMD,"""Critical security and bug fixes"" maintenance usually means on softwares that the EOL is approaching and support is kept on a thin lifeline ( not substancial )   That is a common enough catchphrase to raise the redflag, not sure why we have to split hairs here.  Wether the cards are mature or not is out of the question when new games are continuously added, and drivers get updated, it's not like those didn't matter last month ...",Neutral
AMD,"I think the main problem is no new game support like Nvidia/Intel does with their Game Ready/Game On drivers, which means as time goes on the performance of RDNA2 relatively will fall further and further behind Nvidia's or even AMD's own RDNA3. RDNA4 gpus in newer games  >These card are well understood by developers and game optimization is the ***developer's job*** not AMD's.  this should be true in an ideal world but we know a lot of game are unoptimized mess and need launch drivers to not run like shit",Negative
AMD,"It’s really weird how they manage which series to support, like didn’t Polaris JUST got into the same split driver this year?",Negative
AMD,"95 percent of people won't notice the difference. What AMD should do is continue 'supporting' the cards, eg let the latest drivers still be installed, not change a line of code for said drivers, and let placebo do the work.",Neutral
AMD,"> ""RDNA 1 and RDNA 2 graphics cards will continue to receive driver updates for critical security fixes and bug corrections. To focus on optimizing and delivering new and improved technologies for the latest GPUs, AMD Software Adrenaline Edition 25.10.2 places Radeon RX 5000 and RX 6000 series graphics cards (RDNA 1 and RDNA 2) into maintenance mode. Future driver updates with specific game optimizations will focus on RDNA 3 and RDNA 4 GPUs.""  > -AMD to PC Games Hardware (translation)  Seems in proportion.",Neutral
AMD,nobody want 1 driver every 2 years like they did with their vega GPUs that were supposed to have lot of VRAM to be future proof lol.,Negative
AMD,">No, they just dropped day one game dev driver support.  Did you just make that up ?   Adressing ""critical"" issues like bug fixing and security isn't optimising games "" later on "".  It's plain and simple an EOL around the corner.",Negative
AMD,Which is extremely ridiculous. Day 1 driver support is not anything hard to do and should be a basic expectation. Now you’re at the same level parity as a 1080ti.,Negative
AMD,Why?,Neutral
AMD,*This* is? For how long have you known this was going to happen? And why didn't you tell anyone?,Negative
AMD,You realize how old DirectDraw is right? I'd expect whatever game you're running in VR that uses that would need a graphics wrapper to perform correctly in Windows Vista and newer (even with Nvidia graphics cards).,Neutral
AMD,"We were one of the main driving forces that saw AMD revert their decision, but sure, make stuff up: [https://www.youtube.com/watch?v=NsBRNck\_-wA](https://www.youtube.com/watch?v=NsBRNck_-wA)",Neutral
AMD,"There is  the rebrand of older Ryzen mobile APU already,   [https://www.reddit.com/r/hardware/comments/1ohh7fp/amd\_again\_reshuffles\_mobile\_lineup\_with\_ryzen\_10/](https://www.reddit.com/r/hardware/comments/1ohh7fp/amd_again_reshuffles_mobile_lineup_with_ryzen_10/)  So time to get the same old tech, just with a new name and usually the same price.",Neutral
AMD,"Did you know Valve using RADV Driver, Linux Driver didn't affect by this change.",Neutral
AMD,The marketing team was not making this decision bro,Negative
AMD,"That only really works if there's new products to buy. I had a 7900XTX and they decided to not make anything worth buying this gen, so I dipped.",Negative
AMD,"The stock went up like $100 bucks in a few weeks, AMD isn’t missing, they simply don’t care anymore about certain customers",Negative
AMD,Glad someone else remembers this.,Positive
AMD,Who's fault it was a clusterfuck? Who advertised compatibility?,Neutral
AMD,"That wasn't really why. High-end X370 boards with 32Mb BIOS chips didn't get support, while low-end B450 boards with 16Mb BIOS chips did. The first 300-series chips with official Zen 3 support [ended up being the lowest-end A320 boards](https://www.tomshardware.com/news/vendors-finally-enable-ryzen-5000-support-on-a320-motherboards), while official X370 support didn't roll out for [almost another two months](https://www.techpowerup.com/290832/asrock-first-out-with-official-support-for-zen-3-cpus-on-x370-motherboards).",Neutral
AMD,"I didnt ask you. im not second hand. Back in the day we used to ACTUALLY see ACTUAL price performance gains where last year's 80 card would be the next's 60 card (for example 980 vs 1060, 580 vs 660). Really, the last time AMD had a product this poorly supported that I bought, you could get a like a 960/970 to replace your old HD 5850. BRAND NEW. Im getting sick and tired of being relegated to a second class citizen for not wanting to spend what used to be GTX 80 money on GPUs and its even worse when these companies dont support what products do exist.",Negative
AMD,Not for 50% more money.,Neutral
AMD,"And your point? The same happens with Nvidia, driver updates improving performance after product launch.",Neutral
AMD,"I see it with Intel Wifi drivers, the actual driver in device manager never change, but Add/Remove just gets a new number.",Neutral
AMD,"I did and wasn't alone notice the difference when they released drivers from the latest AAAs on not a longer so beefy cards. It's refreshing.  Your agument is making people like complete idiots or ignorants, or careless from which they won't get any use of counting FPS or writing on reddit anyway.   Your solution is egregious because a thin minority will notice and spread the word regardless, just like a thin minority of people is doing benchmarks ...  However, a huge percent of people is fine playing at 30 FPS on a botchered resolution with stuttering left and right, not sure why you bring up that one.",Neutral
AMD,"Not even that, you won't get any substancial drivers from now on. OP just made that up.",Negative
AMD,are you serious? Saying anything bad about AMD gets you severely downvoted or shadowbanned online. This is fact.,Negative
AMD,Are the GPU drivers rebranded too? That 2022 model G14 is no longer supported with the latest gaming drivers due to it using a RX 6800S GPU and 6800HS CPU,Neutral
AMD,"If that were the case (that Nvidia driver performance improves at the same rate as AMD's) there would be no such phrase as ""fine wine"" and op's comment about shitty day one drivers would apply equally to Nvidia.",Negative
AMD,"Nah, I see AMD bad twice as often as I see NV bad here.  I still see AMD drivers comments from pre-pandemic days to this day.  I see it more than NV fire cables, missing vram, price hikes, tier changes, their shady marketing, the geforce program, and they have had bad drives multiple times since AMD's bad drivers including a NV bad driver release just this year.",Negative
AMD,">  >AMD hasn't explained why it made this change, but since no RX 9000 cards include a USB-C port, **it may have been removed to streamline the already large driver package.**  Is type-C power so much more complicated than type-C display that it bloats the driver package? Anyways, I've never heard someone who bought a $200 - $1000 GPU **EVER** complain, ""Damn, I wish AMD removed hardware features to make my once-a-month downloads save a few hundred MB!""  Seems like *a very* flimsy excuse by AMD, if true. Videocardz is speculating, but it seems like AMD's workaround is to use an ancient (in terms of games) driver, so it's seemingly working just fine in **some** drivers.  //  The real problem: some bidirectional USB-C to DisplayPort cables [**require** power output on the USB-C port](https://www.reddit.com/r/UsbCHardware/comments/gh6w4e/comment/k4ugi44/):  >REVERSIBLE CABLE: This 6.6ft/2m DisplayPort to USB-C bidirectional cable connects your USB-C Laptop to a DisplayPort Monitor or DisplayPort device to a USB Type-C display; Note: **USB-C port (computer or monitor) must support Power Delivery (minimum 5V)**  Here, USB-C GPU to DisplayPort monitor would mean *only* the GPU can provide power and now these cables may no longer work properly.",Negative
AMD,"RDNA1, and especially RDNA**2**, are *extremely* popular today, especially as RDNA3 wasn't as competitive. All 6000 APUs, all Z1 / Z2 handhelds, all Steam Decks, all 6000 dGPUs, all 5000 dGPUs, many 7000 APUs, etc.  These are likely ***tens of millions*** of devices using RDNA1 and RDNA2. Is AMD hurting for cash that badly?",Neutral
AMD,It's crazy to think that some refresh models like 6750xt/6950xt released in 2022 will only get 3 years of game driver optimization. Some of them may not be even out of warranty,Negative
AMD,">RDNA2/RDNA1 GPUs to sub-branch in latest driver  Does this mean that AMD will soon cut the driver support for these GPUs? This looks kind of worrying TBH, I would understand if they only did it with RDNA 1 considering most of those GPUs don't even support DX12 Ultimate therefore they are already considered obsolete with most upcoming modern games onwards, but with RDNA 2? they aren't even that old only 5 years old, and still is being used to power the current gen consoles which will be only replaced on 2027. It's way too soon for this GPU architecture to be dropped.",Negative
AMD,"Removing hardware features that customers paid for through driver update. Planned obsolescence, boooooo! 👎",Neutral
AMD,"Last flagship RDNA2 card to launch was the RX 6950XT, which I own*. It launched in May of 2022. RX 6750XT also launched at the same time. RX 6750 GRE launched in October of 2023. RDNA 2 APUs are still being used in current handheld devices. Gaming devices that launched in 2025 potentially won't get any driver level performance fixes for games that released the same year. The Xbox Ally X uses RDNA2.   Any way you slice it, five year old architecture getting relegated to 'bugs and security only' status in drivers is an absurdly short time frame. Vega was bad enough, but this is somehow even worse.  I can't see myself buying or recommending another AMD GPU/APU in the near future if this is the level of driver and optimization support they're content to provide even relatively recent devices.  *(Functionally an RX 6900XT with a factory overclock, which I bought in 2023 at a steep discount).",Neutral
AMD,"Yup, final nail in the coffin, not buying AMD GPUs again in the foreseeable future.  There is a reason Nvidia is at the top.",Negative
AMD,But the internet has assured me that AMD is my friend?,Negative
AMD,Does this mean my USB-C portable monitor will stop working on my 6800?,Neutral
AMD,"Yep. Learned my lesson, next card is an Nvidia now. Just bought 9070XT earlier this year.",Positive
AMD,"This is what people need to remember when they think AMD cares of long term GPU driver support. They already screwed over customers who bought new APUs with VEGA chips and within the same year, discontinue non-security updates to their drivers.",Negative
AMD,The YouTubers pushed RDNA2 over RDNA3 to their audience. You know because used products cheaper then new products or something.   Crazy how bad this timeline is against their credibility. Not saying they would have predicted this but that follow up video where HUB stands by their 5700 XT recommendation over RTX 2060 Super just got another slap in the face.,Negative
AMD,TIL there are desktop gpus that have usbc,Neutral
AMD,A reminder that Nvidia still supports USB-C in their drivers,Neutral
AMD,"We’re in a race to the bottom to see who can provide the shittest treatment to their customers, aren’t we?  Between this and Intel’s announcement of moving their driver support for Xe-LP to a Legacy Branch, it sure is fun to be in the GPU market.",Positive
AMD,Why is there a Type C port on the back of a GPU?,Neutral
AMD,"People can drama as much as they want to, but GPUs will be working as usual. The vast majority of users won't ever notice the slightest difference.",Neutral
AMD,"Hello BarKnight! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,"Oh they're not thinking about the consumers or users downloading the package. They're considering their own archival storage, downloads, hosting, validating, compiling etc. We do this kind of pruning in other sectors of software development all the time. Taking just one virtual machine out of production permanently usually yields thousands in savings per year. Same mentality but different verticality",Neutral
AMD,It's a niche use case but I think that also forces anyone using PSVR2 to buy a virtuallink adapter or stay on outdated drivers.,Neutral
AMD,"They are also actively being sold in substantial quantities. I understand that AMD's discrete GPU market share is small, but they sell a ton of APUs. Such extremely short support period where they cut software resources from supporting a literally actively sold product that people will likely use for 5+ years from now is really concerning.",Negative
AMD,Amd drops driver support like a hot potato once they move on to a new arch.  Just because they stayed on gcn for ten years people started thinking amd supports their products.,Negative
AMD,"This is what confuses me. Are they actually effectively cutting off game optimization for millions of hand held owners? That can't be possible, right? Those things are still on physical and e-commerce ""shelves"" around the world. They can't possibly be sunsetting products that are still actively being sold. Doing that to rdna1 would make slightly more sense, but even that is lame given that it launched in 2019 alongside the Turing refresh, if I remember right.  It's easier to justify the Nvidia tax in your mind if you count on long term support. I used an rx 480 4g from 2016 to 2022 (intended to upgrade when rdna2 launched, but laughed at the pandemic pricing). I've had an rx 6700xt in my system since early 2023, I think. If that is being cut off even from game optimizations in new drivers, that will dissuade me from buying a new Radeon gpu. I keep things for a few years at a minimum, sub 5 years of active support is extremely disappointing.  I wonder what the fallout from this will be.",Negative
AMD,This article is a bit misinformative. AMD isn't dropping support for RDNA1 and 2 outright.,Negative
AMD,"Like mine.  That's not mentioning drivers are still broken for many people, myself included.  Truly AMD moment.  At least Intel has an excuse of being a new player.",Negative
AMD,Does game driver optimization really have any substantial impact on performance? My uninformed guess would be maybe it’s 2-3% better but the drivers can’t do magic and games can’t be that wildly different. Ending support so early still sucks but I wonder how much impact it actually has on games?,Negative
AMD,"VC just posted an update to include a response AMD made to PCGH, seemingly confirming the move to ""maintenance mode."":  [https://videocardz.com/newz/amd-confirms-focus-shifts-to-rdna3-and-rdna4-rx-6000-and-rx-5000-lose-day-1-game-optimizations](https://videocardz.com/newz/amd-confirms-focus-shifts-to-rdna3-and-rdna4-rx-6000-and-rx-5000-lose-day-1-game-optimizations)  (translated)  >RDNA 1 and RDNA 2 graphics cards will continue to receive driver updates for critical security fixes and bug corrections. To focus on optimizing and delivering new and improved technologies for the latest GPUs, AMD Software Adrenalin Edition 25.10.2 places Radeon RX 5000 and RX 6000 series graphics cards (RDNA 1 and RDNA 2) into maintenance mode. Future driver updates with specific game optimizations will focus on RDNA 3 and RDNA 4 GPUs.",Neutral
AMD,">It's way too soon for this GPU architecture to be dropped   This isn't new to AMD    They dropped vega in 2023. So 6 years after the architecture launch, but <2 years after launching APUs with Vega iGPUs    RDNA 2 products are _still releasing_ in APUs, wonder what happens there (Xbox Ally)    Say what you like about Nvidia, but they support their products far longer.  Hard to justify AMD GPUs when they do this twice",Negative
AMD,I’m only speculating but I would guess the sub branch means they will get driver support still but not necessarily all the new features of the main drivers?,Neutral
AMD,They will call it “extended support” and not discontinue it,Neutral
AMD,"That's actually kinda sucks. I've bought AMD GPU's for last decade as they were best price / performance offers in my area, but now my 6800 is basically discontinued. Guess I have to pay big bucks now just to get a card that can hold on for at least 5 years on the driver side.",Negative
AMD,false news,Neutral
AMD,"Whatever that reason, this isn't it because nvidia's been doing the exact same thing since they got into the GPU market.  At some point you stop optimizing for older architectures and focus on the one ones. GPU's 2 generations old are generally about where that line is. And that's all that's happening here.  The only thing different now is that AMD was public about it, and others trying to turn it into a news story with misleading titles and ranting posts on reddit.",Neutral
AMD,"And here the internet is trying to convince you that AMD is somehow doing something evil, when they are in fact only being open about something that has been standard practice in the industry for as long as there has been a gaming GPU industry.",Negative
AMD,"Kidding me all I've seen is post after post of AMD astro turfing and links to GN or HUB to validate it.   ""NvIdIa Is RuInInG gAmInG!""",Negative
AMD,"Last AMD GPU I bought ( HD5850 ) I got driver support for the 7 years I kept it before upgrading.  RX480 still had driver support as of 25.5.1 ( so may 2025 ). That's 9 years of support even tho ""official"" support has been dropped in 2021.  That's what they mean by ""sub-branch"", you think it's discontinued, but it really isn't yet.",Neutral
AMD,">They already screwed over customers who bought new APUs with VEGA chips and within the same year, discontinue non-security updates to their drivers.  Vega products still get driver releases with game support, fixes and improvements. Here are just a few examples (open and read the Release Notes) :  Mendocino drivers: [https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-7000-series/amd-ryzen-5-7520u.html](https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-7000-series/amd-ryzen-5-7520u.html)  Rembrandt drivers: [https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-6000-series/amd-ryzen-7-6800u.html](https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-6000-series/amd-ryzen-7-6800u.html)",Neutral
AMD,This misinformation is what people need to remember? How about no?,Negative
AMD,"9000 series will get the same treatment, only reason AMD is doing this is because people buy.",Negative
AMD,I remember 2070S vs 5700XT was argued at the time that AMD offered more value. The AMD card cannot even launch new global illumination games and has no competitive upscaler compared to DLSS4.,Negative
AMD,also see Nvidia Turing 2xxx series,Neutral
AMD,Reminder Nvidia still supports 10 year old GPUs.,Neutral
AMD,And this is called apologism. These companies are relying on people like this to erode expectations.,Negative
AMD,"Yeah you see ""working as usual"" for AMD is still broken.  Driver hangups, high hotspot temperatures, random boot issues, etc.  All I experienced in one year of owning my 6750XT that I bought brand new.  It's perplexing because I also have 5700x CPU and that thing fucking rocks, I wouldn't recommend AMD GPU to anyone yet their CPUs are incredible that I only have praise for them.  Runs cold, sips power, incredible performance and stability.",Negative
AMD,"for me its all very strange because my laptop had an amd apu and gpu, now the drivers dont work for the apu. really frustrating",Negative
AMD,"To disable hardware features you ***sold?*** Seems scummy, less pruning.  You sell **hardware**, you support it. Next time, AMD ought to only ship what it **will** support, not disable hardware it sold.",Negative
AMD,If removing one VM saves thousands per year then somewhere along the line you lost the plot.,Neutral
AMD,"They just re released a bunch of zen3+rdna2 laptop cpus too, same thing with new name, just like they did with z2a previously which is basically steamdeck oled cpu",Neutral
AMD,"The article is precisely clear, IMO. Sub-branch, maintenance mode, no specific new game updates. That's all basically the same thing to me.",Neutral
AMD,This is the dark side of AMD that people don't tell on the comments in social media,Negative
AMD,"To underline this, Nvidia is preparing to stop game-ready support for Maxwell released in 2014 and Pascal release in 2016 now. Compare that to RDNA released in 2019 and RDNA2 released in 2020, this is quite bad by AMD.",Negative
AMD,"Vega products still get driver releases with game support, fixes and improvements. Here are just a few examples:  Mendocino drivers: [https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-7000-series/amd-ryzen-5-7520u.html](https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-7000-series/amd-ryzen-5-7520u.html)  Rembrandt drivers: [https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-6000-series/amd-ryzen-7-6800u.html](https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-6000-series/amd-ryzen-7-6800u.html)",Neutral
AMD,> They dropped vega  Nobody cared about Vega. Especially not Raja Koduri.,Negative
AMD,They will get support but way less frequently.,Negative
AMD,"yeah, it's only going to get security updates.",Neutral
AMD,You won't get day 1 drivers for newly released games but they are not removing the support for GPUs. It will continue to get updates and fixes. Vega still receives updates and fixes.  Also there is always Linux.,Neutral
AMD,"Fact of the matter is, if you buy a new in production nvidia card, you can expect ten years of full support witout any ifs and buts.  In the past you could at least argue AMD had to drop unpopular generations with low sales volume like Vega and wouldn't do the same for popular ones. If they actually put a very successful gen like RX 6000 into maintenance mode after just five years, they're really hurting their image.",Neutral
AMD,">RX480 still had driver support as of 25.5.1 ( so may 2025 )   It still gets security updates, not the same level of support later GPUs get",Neutral
AMD,"i'm pretty sure this sub-branch is a separate thing from the maintenance drivers for those even older cards.  All AMD is doing here is no longer releasing day 1 game drivers with 5000 and 6000 series support. All the normal driver updates will still inlude the 5000, 6000 series.   Those day 1 game driver didn't include optimisations for older GPU's anyway, even if those cards were supported. so really, nothing substantial has chanced, except that AMD's upfront about it.",Neutral
AMD,They’re “retiring” as of last year:  https://overclock3d.net/news/gpu-displays/one-foot-in-the-grave-amd-starts-retiring-polaris-and-vega-by-reducing-driver-support/,Neutral
AMD,There is literally just one game that wont launch on the 5000 series.,Negative
AMD,"Technically, laptops that have the NVIDIA GPU hooked up to USB-C also count as USB-C GPUs too.     I found this out the hard way when using NVCleanstall.",Neutral
AMD,"Actually Nvidia's dropping support for a 10, 9 and 7 year old GPU's next month.  And AMD is not dropping support of the 5000 and 6000 series GPU's. All the normal driver updates will still support those GPU's.   It's only the day 1 game specific drivers that wont be supported on the 5 and 6000 series... which never contain any optimisations for older GPU's anyway.",Neutral
AMD,"You should RMA your GPU. That's definitely no driver issues, your particular unit seems defective.",Negative
AMD,"I worked at one place using full-price EC2 instances, we complained about how expensive that was and the execs shrugged. 😂",Negative
AMD,"Their drivers already well optimized. there isn't much to gain with game specific optimisations. hell, even on current gen GPU's they barely do anything most of the time.  The issue here is that AMD stated outright what the whole industry has been doing for decades, so now suddenly it's a story.",Negative
AMD,"AMD causing RDNA 2 and 3 cards to BSOD with shitty drivers twice in span of 3 months didn't get mentioned once by *certain* YouTubers, but proportionally smaller issues on Nvidia side, including 3000 series, were all over said channels.  Because Nvidia bad.",Negative
AMD,"Yeah, that's like NVIDIA stopping updates for the RTX 3000 series, also released in 2020.",Neutral
AMD,Fuck me. Now I have to worry about this when my 2070S dies. Was planning on going AMD :(,Negative
AMD,They stopped giving some vega products updates like the whole 5000 series. Here's 5625U with Vega 7 released in 2022 which stopped getting support  https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-5000-series/amd-ryzen-5-5625u.html,Neutral
AMD,Cuz people are so focused on value all the time and only pay attention to immediate value. People don't think about the fact that these are businesses so if a company is taking more money for their product then there's a chance that they also have better services to support the product. It doesn't always mean this but it definitely makes sense. If you have a company that offers really great deals and beats the prices of the competitors all the times then they're probably going to have less than warranty or less than customer support because of the funding.,Neutral
AMD,"Vega products still get fixes and improvements, not just security updates.   [https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-5-1-POLARIS-VEGA.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-5-1-POLARIS-VEGA.html)",Neutral
AMD,"No, the only thing they are not getting anymore is day 1 game specific driver releases.   Everything else they are still getting full support.  You don't need game specific drivers for a architecture that already has pretty well optimized drivers. Not to mention that this has been standard practice for ages already, by both vendors, just never outright stated.",Neutral
AMD,"Indeed, but the drivers still work and your games still work so *shrugs*.",Neutral
AMD,AMD also has a solid open source stack on Linux. So you're really never not supported.,Positive
AMD,"Read the entire statement, not just the headline. I just gave you a bunch of examples where Vega products still get driver releases with game support, fixes and improvements",Neutral
AMD,"AMD is dropping support for RDNA and RDNA2, it will just be security updates and bug fixes. The latest driver update yesterday didn’t include the new Vulkan extensions for those architectures.  Also earliest Nvidia card not supported is 8 years old, not 7.",Negative
AMD,"According to XFX GPU is fine and all issues I have don't fall under warranty  Issue *is* with drivers, okay hotspot temps are bit weird but they are still easily in limit (85-90c) and it's probably just thermal paste since normal temp is perfect and GPU is quite without any throttling.  Driver hang ups happen when alt tabbing and AMD already caused that to result in BSOD twice with drivers.  Issue isn't hardware here, it's software.",Neutral
AMD,"This exhibits a fundamental misconception of why game-specific drivers even exist. Drivers are optimised--***games*** are not. Each new game can make its own nonsensical, non-standard design about how to implement a feature.   Most of the time, nobody cares: old GPU sucks at a new game? Yeah, that tracks. *Obviously*. AMD isn't new to making GPUs nor GPU drivers. However, there are still times when game-specific drivers help.  In the end, AMD is **not** abandoning game-specific drivers: it still offers them for RDNA3 & RDNA4. If AMD *truly* wanted to ""expose the industry"" lmao, they'd end all game-specific drivers for all GPUs and all generations.",Negative
AMD,"Because that was not a wide spread issue, just one you experienced.",Negative
AMD,"Congratulations, your link proves exactly what I was saying. There are almost 20 driver updates since 2023 in the link you provided. I don't have a clue what you are talking about.  [https://imgur.com/a/0E2g0KS](https://imgur.com/a/0E2g0KS)",Neutral
AMD,Did you mean to reply to someone else?,Neutral
AMD,"my bad, i saw it on videocardz.com",Negative
AMD,*SIGH*   AMD is NOT dropping support!   The ONLY thing the 5000 and 6000 cards are being dropped from is special day 1 game specific driver releases. They will still be supported in the normal driver updated.   This fucking lie is very tenacious.,Negative
AMD,What the fuck are you reply to? Certainly not my comment.    Seriously what are you even on about? Nothing you said makes any sense in regards to my comment.   Exposure the industry? Wtf? Stopping with day one drivers? Where the hell are you getting this?,Negative
AMD,"My case was a 5700XT, people always, ALWAYS, will tell on AMD that it has better cost per frame more bruteforce, that you should use X adrenaline version, undervolt it and use Y version of windows. Guess what, it shited on me when I was playing rainbow6, press F.",Negative
AMD,"These aren't the ""Game Ready"" drivers or whatever amd calls it. It's just fixes instead of added support for newer games. Compare the release notes of both drivers you'll see what I'm talking about",Negative
AMD,Yeah the person that you were replying to.,Neutral
AMD,"Go see the driver released yesterday. They didn’t release the new Vulkan extensions for RDNA2, only for RDNA3 and RDNA4.  It’s essentially in maintenance mode, those cards will only get security updates and bug fixes.",Negative
AMD,The idea that you need specific drivers for specific games is absolutely insane and backwards.,Negative
AMD,1. I see you have been proven wrong and decided to move the goal post. Not cool bro. You said Vega products don't receive updates. 2. Game ready drivers for Vega APU? Are you serious? What AAA or AA games were you expecting to play on a Vega APU? What kind of request is this?,Negative
AMD,"1. The discussion was about supporting products for new releases the entire time. If something comes out and has issues or runs like ass on hardware that’s less than 5 years old because of a lack of software support from the company then that’s a problem. This *has* happened with AMD before, they put product lines into a skeleton maintenance mode well before they should.  2. Lmao. Rofl, even.  Edit: I should also specify that these product lines are still actively being released and sold new. They *should* get shit for that kind of support.",Negative
AMD,This is getting tiring. Just how many Mendocino has AMD made and still have in stock.,Negative
AMD,Oooof those Zen 2 chips only have 4mb of L3 cache.,Neutral
AMD,"It's so on brand for AMD to come up with a new naming scheme and already be halfway through the names. When Zen 6 comes out it will be Ryzen 4xx.   Why even rename the old generations, you are only making it more confusing for general public. It's not like all the text/video reviews, articles, and promotional material are all going to change to reflect the name change.",Negative
AMD,"Two product launches were missed in that article, [but were covered by other sources](https://www.techpowerup.com/342290/amd-introduces-new-ryzen-branding-ryzen-10-zen-2-and-ryzen-100-zen-3-processors):   > Here are the ""new"" Mendocino CPUs based on Zen 2: > - Athlon Silver 10: 2C/2T > - Athlon Gold 20: 2C/4T  It's nearly 2026 and AMD is relaunching dual core CPUs with an architecture from 2019. And these dual core Zen 2 CPUs already [lose to the quad core Intel N100](https://www.cpubenchmark.net/compare/5316vs5157/AMD-Athlon-Gold-7220U-vs-Intel-N100) which has been on the market since 2023.  This is the most miserable product launch from AMD in a long time.",Neutral
AMD,"Honestly, if this means that they keep the cadence:  Zen2 10/30/40   Zen3 100/110/120   Zen4 200/220/230   Zen5 300/350/370   **Zen6 400/450/470**  ...then I can live with it.  Putting the CPU generation on the third letter like 8745HS = Zen4 was rubbish.  Priovidedthey use XYZ model number where:  X is the CPU gen   Y is the APU tier   Z is reserved for bullshit refresh  I can live with it...",Neutral
AMD,"Well uh. 100, 200, and 300 are different series. So 30 is different from 10 right?  > AMD Ryzen 3 30 > > Series: Ryzen 10 Series  I give up.",Neutral
AMD,this is going to need a new decoder disc!,Neutral
AMD,"to make this more clear:     Ryzen 7 170 = Ryzen 7 7735HS     Ryzen 7 160 = Ryzen 7 7735U     Ryzen 5 150 = Ryzen 5 7535HS     Ryzen 5 130 = Ryzen 5 7535U     Ryzen 3 110 = Ryzen 3 7335U     Ryzen 5 40 = Ryzen 5 7520U     Ryzen 3 30 = Ryzen 3 7320U     Athlon Gold 20 = Athlon Gold 7220U     Athlon Silver 10 = Athlon Silver 7120U     Specifications are 100% the same. So, in this case, AMD ""just"" changed the names of it's 2023's mobile portfolio. Which, as well, was already a rebranding of older CPUs.     Source: [3DCenter.org](https://www.3dcenter.org/news/news-des-2526-oktober-2025)",Neutral
AMD,"How do you ***know*** all these confusing names are clearly an attempt to submarine old CPUs into sold-as-new systems? Because EPYC has avoided all this bullshit for just under a decade.  EPYC 7xx**1** \- Zen1  EPYC 7xx**2** \- Zen2  EPYC 7xx**3** \- Zen3  EPYC 4xx**4** \- Zen4 small dies  EYPC 8xx**4** \- Zen4c  EPYC 9xx**4** \- Zen4  EYPC 4xx**5** \- Zen5 small dies  EPYC 9xx**5** \- Zen5  AMD uses another letter (F, P, X) for additional differentation, *if necessary*. AMD knows this is  readable + fucking simple + can fit ***1000s*** of SKU permutations (2 digits + 3 letters).",Negative
AMD,"I just want to know when RDNA4 embedded boards will be available and what they'll be called. Is that too much to ask?  I guess maybe the ryzen 100 is what I'm waiting for?  Edit: nevermind, 680m is RDNA 2...I have no clue",Neutral
AMD,You gotta admit Intel's naming scheme is way more consistent and sensible than AMD's.,Negative
AMD,Wow. That's a sign of a lot of stock piled up.,Positive
AMD,It takes a degree to understand the naming scheme... They are clearly doing this so to confuse the consumers,Negative
AMD,"So sick of branding reboots. Why can't they just choose a naming scheme that will last and stick to it forever (and stop skipping generations willy nilly!). All they're doing is either trying to make a disappointing product look better than it is, or they're tainting otherwise good products with cheesy names (""AI MAX"" is just cringe). I guess they're probably fooling someone, since they keep doing it.",Negative
AMD,"Confusing CPU names, GPU names, TDPs, cooling, ram speeds, etc etc... Buying a laptop is such a shitshow",Negative
AMD,Why not just give them all names in Klingon?,Neutral
AMD,It sucks making your namescheme super confusing so consumers think [some number] is the latest stuff.,Negative
AMD,Do they have that much leftover silicon...?,Neutral
AMD,"Honestly this naming scheme is not bad by itself; makes it easy to which generation of chip you are getting, and bigger number within generation = better is fine.  Now AMD just needs to keep using this naming scheme, at least for a few years (ideally at least until Zen 11 = 900 series chips, then if they want another naming scheme I would give them a pass).",Positive
AMD,I had no idea they were still making Zen 2 and 3+ chips,Neutral
AMD,Intel and AMD decided that it's better to compete in more confusing naming than performance.,Positive
AMD,This smells like E-waste that is going straight to the developing countries.  Why not just recycle it the proper way.,Negative
AMD,Remember when Intel called AMD’s laptop cpu naming scheme “snake oil”?,Neutral
AMD,AMD is slowly turning into an Intel...,Neutral
AMD,"Glad that the naming is being cleaned up, kind of useless to see AMD 7000 badges on laptops and not know if it's using a processor two or a full three generations out of date. By the time Zen 6 launches most AMD laptops in the market will be three or four generations out of date...",Negative
AMD,Im more than fine if they find their way to 4050 laptops that needs to be best bang for the buck as posdible. Remember 4050 to 4070 laptop sales make up MORE THAN QUARTER OF ALL OEM SALES. So market share is market share. Just dont let them go on a fight with the new intel 7s.,Positive
AMD,"Hello Shadow647! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,That's sad... I remember playing anno 1800 on the cheapest and lowest laptop tier ryzen 5300u 300$ hp 245 g8 in 2021/2022 .,Negative
AMD,"Another loss for the customers, getting old shit under a new name. SAD!",Negative
AMD,What a shame with AMD rebranding scheme,Negative
AMD,"Why didn’t they plan for Zen 2 ahead of time so that Zen 5 could be 400, Zen 4 could be 300, Zen 3 could be 200 and Zen 2 could be 100?",Neutral
AMD,"They might still be making them. TSMC n7 is now a very mature node, and it wouldn't be surprising to me if the low end market is now easier to satisfy with older products rather than cut down newer ones.",Neutral
AMD,"probably still have a lot, and they are the cheaper to manufacture, so AMD can use that to have against cheap Celerons from Intel",Neutral
AMD,They're probably still producing them because 6nm is cheap and can compete against Intel 7 of which apparently there's a shortage of capacity.  Windows 11 upgrade cycle I guess.,Neutral
AMD,"Oh, they're even reusing code names  [https://www.techpowerup.com/review/amd-3800-plus-venice/](https://www.techpowerup.com/review/amd-3800-plus-venice/) <- CPU from 2005 was venice   [https://www.techpowerup.com/340405/amd-prepares-epyc-venice-platform-to-break-the-1-000-w-power-barrier](https://www.techpowerup.com/340405/amd-prepares-epyc-venice-platform-to-break-the-1-000-w-power-barrier) <- 20 years later the upcoming CPU is venice",Neutral
AMD,"I woldn't be surprised if it's still in production, it's cheap on a mature node and competes with Intel's counterparts performance-wise",Neutral
AMD,"Would it be less tiring if they called them Athlons? The world needs cheap chips, not everyone needs the latest and greatest.",Neutral
AMD,"These are general purpose CPUs for any OS, but 4MB L3 cache is pretty terrible to sell on a Windows PC in 2025 especially.",Negative
AMD,"And it's completely fine for the market segment it's going to   4c/8t of Skylake level performance is completely fine for the average non enthusiast computer user who is not playing games. Who edits a word document, uses TurboTax, and watches Netflix.   Fine for low end gaming too.",Positive
AMD,They're cheap low power chips that go into Chromebooks. I'm not sure why you expect much there.,Negative
AMD,literally,Neutral
AMD,Kaby lake also had only 4MB.,Neutral
AMD,"Knowing AMD they will leapfrog the expected Ryzen 4xx name and move to something new like 5xx or 'Ryzen 25 AI' because ""Fuck it, why not?"". I'm sick of thier BS naming. In fact I'm angry with Intel as well, but Intel had the best naming scheme ever in the 14900K or 14900HX etc and they ruined it by creating ""Core Ultra"" and restarting. I guess they didn't want to be embarrassed having the 15900K (285K) be slower than the 14900K.",Negative
AMD,"I imagine it's because the 300 series was a naming scheme change, but they still want to produce and sell older CPU's.  So rather than have the 7020 series (Zen2+), 7035 series (Zen3+), 8000 series (Zen 4 refresh), and 300 series (Zen 5) on the market, it'd instead be the 10, 100, 200, and 300 series respectively.",Neutral
AMD,Apparently not if you look at comments,Neutral
AMD,">Honestly, if this means that they keep the cadence:  Thinking ***this*** is the final major rebrand shows a lack of faith in the marketing department.",Negative
AMD,"I agree.  If they can just keep a fairly simple naming convention like you outlined for a decade then I'd be exceptionally happy, the constant and overly convoluted naming schemes they have been using since zen2/3 is irritating.  They had a good thing going with zen and zen+ but as soon as they started with the 4000 on laptops decoder ring nonsense it started getting ridiculous again.",Neutral
AMD,"It would've been better if AMD actually had the correct number to signify what generation their architecture was. For instance, HX 370 should've been HX 570 instead. That way this new rebrand would've made sense. Then Zen 2 could've been 240. Zen 3 could've been 320 etc. But nah they had to start at 3xx for Zen5 to ""one up"" Intel's 200 Core Ultra naming. Typical AMD bullshit.",Neutral
AMD,"I don't mind the specific naming, but they clearly can't stick to one naming so it's just bad when it'll switch in a year again",Negative
AMD,"Yeah, the frequent debrands are annoying, but at least this one is good.   The previous naming convention in your example was just trash.   Zen 6 mobile and NVL will funnily enough both launch around the same time, both using 400 as their naming scheme.",Positive
AMD,"I thinK Zen 6 will be 500 series since Gorgon Point, a Zen 5 Mobile refresh will be 400 series. I mean AMD skips a number every time they do a new numbered architecture for Desktop.",Neutral
AMD,"> Putting the CPU generation on the third letter like 8745HS = Zen4 was rubbish.   It was... fine? I mean, the 8 is pretty much the year. It's the same as 25745HS   2 or 3 alfanumeric to represent the newest product and performance level in that year.   Sure a 22930XXX can beat a 25240U, but then we'd also need to add a Cinemark score to the product name to be fair with everyone.",Neutral
AMD,This was a needed and welcome change because of decoder discs.,Neutral
AMD,epyc isn't aimed at the gullible consumer market. i consider myself tech savvy and even i can't keep up with amd's naming conventions. 99% users buying a laptop or whatever have no idea what generation of amd/intel they are getting.,Negative
AMD,"they cant fool datacenter guys with bullshit naming, but they can definitely fool consumers with it.",Negative
AMD,No one buying EPYC is actually confused though as no one is buying from model name alone.,Neutral
AMD,It was rather unpopular when they tried doing the same thing to the mobile market.,Negative
AMD,"nah, the 288v vs 285h etc is really stupid. wouldnt it have made more sense to be like: 255hx, 255h, 255v, 285hx, 285h, 285v etc? (yes, i am aware these are different families of chip.)  and if it ends in 6 like 256v 286v it is 16gb not 32gb. its just really silly to me. i dont think the ram should be even paired with the marketing name, i just want to know about the CPU from the name, not what its paired with. thats what specsheets are for.",Negative
AMD,They are the same though? There are some Raptor Lake and Meteor Lake SKUs hidden in the latest Core 2 series naming,Neutral
AMD,"Right, because with ""Ultra"" they delivered what we all associate with that word...",Neutral
AMD,"It was... Core Ultra is a total mess though, I really need to sit down and try to figure out what they're doing there.",Negative
AMD,Which socket of intel has even remotely the same number of chips as am4?  All other sockets are far more coherently named by amd then the ultra x760 pro max bullshit intel is doing now,Negative
AMD,"I'm quite happy for them to ditch the previous scheme which objectively sucked, but yeah, it's not that fucking complex to just pick something and stick with it. Don't see why it is so hard for any of them, and what the hell is going on in the Monitor world.",Negative
AMD,The OEMs want it this way.,Neutral
AMD,"This is undoing such a confusion, the chip generation is back at the front of the number.",Neutral
AMD,"Could be that they’re still producing silicon, since these would be on older nodes.",Neutral
AMD,The article answers this. They bought fab capacity years ago and are obligated to use it,Neutral
AMD,"Sure, they are cheap chips for people with a budget. What's your point? Should cheap technology not exist? Do developing countries not deserve computers?",Neutral
AMD,"Do you really think the average non enthusiast needs 8c of zen 5 to run TurboTax or watch Netflix?   No, 4c of zen 2 is still perfectly adequate for that.",Neutral
AMD,"That slide is the bullshittiest bullshit to ever grace the earth. Their naming scheme sucks fine. Bu they still tell you if a chip is older zen name from the branding. Some less cache cpu bins of intel doesnt even tell you that.  Do you know the difference between a 13600, 13500 and 13400? İ5's ? Or the mobile H branding? Yea its that bad on both ryzen mobile and Intel chips.[This video sums it up prettttty good](https://youtu.be/05RGUtSk3P0?si=LhCoBdIMiZSkkxmU)",Negative
AMD,"Because AMD messed things up by having repurposed and ""plus"" SKUs taking their own thousand in their line up.  Zen 1 began with 1000. Zen 5 should be 5000.",Negative
AMD,Their small core design probably means they have fewer defects for cut down products anyway.,Neutral
AMD,"Also the performance per clock bump between Zen2 and Zen3 is almost linear with the extra die area Zen3 takes up. that they're on the same node and thus the price/performance remains relatively static is probably a non-trivial reason for them finding places to deploy Zen2 designs still, especially in cost sensitive devices (like the Steamdeck)",Neutral
AMD,"Can their 6nm skus actually compete against Intel 7 though?   Whether it be through node or design choice, RPL-H seems like it can completely smack around AMD's most advanced 7nm class parts in ST perf, though idk how binning ends up looking throughout the stack for ST Fmax.   Battery life may be comparable or better for AMD, I didn't check, but this and ST perf seem like the two more important metrics that laptop OEMs seem to check off on, that and then nT perf and iGPU perf.",Neutral
AMD,"Mendocino was an Intel codename from 25 years ago (for a P6-based Celeron), so these companies not only reuse their own codenames, but steal their competitors' old codenames.  That's kind of the point of codenames though; they're just for internal use, not trademarked like the Real Marketing Names.  (AMD did stop using other people's trademarks as codenames, though, after GM got mad about Corvette and Camaro.  Using Corvette was fine as it was a type of ship, but using Camaro as the name for the worse variant of Corvette was too on the nose.)",Neutral
AMD,Why is that specifically bad on 2025 Windows?,Negative
AMD,The real hardware requirements of Windows haven't changed since Windows 7.,Neutral
AMD,"Genuinely for MOST things people do with computers a 6700K equivalent performance level is entirely viable, it'll browse the internet, handle some light image editing, run office apps.  pair it with 16GB of ram, a halfway decent SSD and video hardware (integrated or discrete) that has hardware accelerated decode for all the common video formats and you've got something that's most definitely hanging in there as ""usefully fast""",Positive
AMD,"You mean, it's fine when AMD does it, lol...",Neutral
AMD,"I have an i7 7700 and I play everyday games like Age of Empires IV, CS2 and stuff. Maybe it feels limited in Cities Skylines 2 and 2024-2025 AAA but that's it.",Neutral
AMD,"Which launched 7-8 years ago, though. Companies only rebrand CPUs still being sold-as-new.",Neutral
AMD,AMD... Advanced Marketing Disaster.,Neutral
AMD,"They’re going to fuck it up about 20 more times in my lifetime, if current trends continue",Negative
AMD,"""No Way To Prevent This"" Says Only Department Where This Regularly Happens",Negative
AMD,Crap... All the more reason for either CPU maker to skip to 500 just to appear more modern...,Negative
AMD,companies are starting (in some cases) to not even tell the consumer what generation it is from. They just put how many cores it has.,Neutral
AMD,"Dropping the ""i"" for Core Ultra is also really dumb imo.",Neutral
AMD,"Not really miffed about Core 2 series being just Raptor Lake, it’s not really hidden as that entire lineup is just old stuff and not really wedged with any newer products.  The Ultra 200U series on the other hand, that’s pretty annoying, even if it’s technically not just Meteor Lake but Meteor Lake with a node shrink, it’s pretty misleading",Negative
AMD,"Honestly at this point, I think this is what the OEM asked.   The fact that almost all mobile devices SKUs sucks (except Apple), and are full of rebrands regardless of where the SOC came from, is definitely not a coincidence.  Intel/AMD/Qualcomm/Mediatek all do the same thing.",Negative
AMD,Yeah but they don’t have the “ultra” designation. ultra = fancy igpu and the NPU. Core = not that,Neutral
AMD,Ultra is superior naming to endless rebranding of old architectures,Neutral
AMD,What does that have to do with naming scheme,Neutral
AMD,"OEMs at some point will demand AMD remove spec sheets from AMD's websites: ""Apple doesn't write clock speeds or caches. AMD ought to do the same, tbh. Just keep that internal to us. This is just too public.""",Negative
AMD,Until they are unable to sell more expensive laptops with newer chips and cry at AMD for a way to differentiate them again...,Negative
AMD,the OEMs have too much power.,Neutral
AMD,Sure. And bringing a new set of numbers and refreshes to the mix.,Neutral
AMD,Cheaper nodes now?,Neutral
AMD,"Unlikely it's out of obligation. If they can keep selling it as new, why wouldn't they? Cheap for them to make.",Neutral
AMD,it's a waste of resources with many much more capable older chips being sent to landfill,Neutral
AMD,"Yeah but Intel was right about AMD’s laptop chip names. Sure Intel still uses Alder Lake for budget 15th gen desktop chips, but AMD’s laptop naming scheme still sucks",Negative
AMD,"AMD probably has to take perfectly good eight core chips to fill demand for their six core models. Segmenting the newest generations below that probably makes no economic sense for them, because odds of having three actually defective or underperforming cores are too low",Negative
AMD,"I've got a great idea, let's name them after earth-movers!",Positive
AMD,"Because windows memory access / storage management in terms of the CPU fetching the required data to run whatever software isn't perfect (not gonna get into MacOS / Linux here). Software is also generally getting more demanding to run over time.  More L3 cache means the cpu can grab more data from the rest of the system in a ""single operation"" (it's obviously not one operation but I'm simplifying here). This means it has to spend less time / clock cycles getting data and thus has more resources freed up to throw at your games / rendering software.",Negative
AMD,"Because MS Office 97 programs typically use about 20MB per instance, thus won't fit fully into cache.",Neutral
AMD,Cause 2025 windows is bad 😎,Negative
AMD,It's like 8GB VRAM,Neutral
AMD,"No, they haven't written any new ones, but starting up a clean install of Windows 11 eats up about 5 or 6GB of RAM, just for the OS. Anything you want to run on top of it will need more, so it'd be honest to say ""hey, 16GB of RAM is the minimum"" (though that'd hurt cheap-hardware OEM offerings). According to Microsoft, with 4GB you're golden.  They also never listed SSDs (plain old SATA3 SSDs, not ""fancy"" NVMe) as a requirement, but anybody that has tried to run (or been forced to) Windows 10 onwards on an HDD knows the pain and suffering that brings. Again, another requirement that everybody know of, buy isn't written down.  For the CPU they list 2 cores @ 1 GHz, which is honestly ridiculous. I mean, you've got to have a TPM 2.0, but with 1 GHz and 2 cores you're fine.  The only part where they seem to have updated requirements is the Copilot+ part, which honestly, most people don't really care about.   https://www.microsoft.com/en-us/windows/windows-11-specifications",Neutral
AMD,"Yes exactly!   I have a 6700 hooked up to a TV and it's plenty fast for typical computer things and I'm still playing games on it too   The people in this sub think you need a crazy powerful computer to do basic things, it's absurd  The steam deck cpu is about equal to a 6700 yet it's still playing new games. It's GPU is the struggle point",Positive
AMD,It does help to have lots of single thread perf for the occasional React abomination that takes hundreds of milliseconds to respond to a mouse click.,Neutral
AMD,"I literally have an i7-6700 system with a GTX1650 that's going to a friend's wife's kid to play Minecraft and Roblox or whatever. It's mediocre in every sense of the word. Another friend is getting an i5-8400 system with...I don't know, I'll have to figure out what I have kicking around after I move house. And yeah...it's just fine. Not amazing or anything. But reasonable enough.",Negative
AMD,???    I am a big of n100-like CPUs as well.,Neutral
AMD,"From the fucking article:  >These are Zen-2 based processors originally designed for entry-level systems such as Chromebooks.  Perhaps read before you ""well, ackshully...""",Negative
AMD,I always prefered Annual Marketing Disaster.,Neutral
AMD,It's not like Intel or Nvidia are much better.,Neutral
AMD,Pretty big gap between the LNL NPU (or iGPU) and MTL/ARL though.,Neutral
AMD,"No way that will happen to all of them, but there have already been OEM (or customer) specific SKUs of CPUs that are not officially documented on the CPU manufacturers websites. The examples I'm thinking of are server CPUs but I could see it happening with some special laptop CPU sometime.",Neutral
AMD,"Older chips that are in chassis with degraded batteries, bad hinges, sticky keyboards 1366x768 screens, etc.  Yes, many can be made usable with dedicated attention from an IT professional to clean them up, replace batteries and install Linux. But at the resource cost of that is the price of a used computer on eBay, plus a knowledgeable friend to do the last part because otherwise, you can't trust there's no preloaded malware (i'm batting one of four on that).  It doesn't scale.",Negative
AMD,"Eliminate these and all that will happen is people will be forced to pay more to equally fill that land.  I have a 2500K system still up and running. What are you doing to reduce landfill, apart from preaching that the poor don't deserve computing?",Negative
AMD,Can wait to unlock my AMD CPU once again.,Positive
AMD,They use geographic names because those can't be trademarked and no one can sue.,Neutral
AMD,> Software is also generally getting ~~more demanding to run~~ *less optimized* over time.  Apollo missions ran on a Casio wristwatch,Neutral
AMD,"There's nothing windows-specific here. I can run the same software on Linux and windows, like a browser, and it should perform the same in terms of cpu perf. The only difference in this space is maybe transparent hugepages, I don't think Windows has those, but it would impact the TLB more so than general cache hit ratios. The physical address to cache set mapping function is an implementation detail within the CPU and the same across OSes.",Neutral
AMD,"More like 2gb vram honestly, even intel give 9mb l3 cache on their i3s",Neutral
AMD,"Part of the reason the Steamdeck hangs in SO well with PS4/Xbox One generation games is that its cpu utterly annihilates the SOC those used.  It's (slightly) behind the base PS4 on gpu grunt, 1.6teraflops to 1.8 iirc, although driving a lower resolution.  buuuuut comparing benchmarks (I did this a while again and it'd probably take me a while to source the numbers properly again) of a quadcore Zen2 at 3.6Ghz to the desktop version of the 8 core Jaguar at 2.1Ghz in the PS4 Pro.. a single core of the Zen2 is around 60% of the performance of THE ENTIRE SoC in the PS4Pro, and it's got 4 of them  Visual stuff you can scale up/down in quality and resolution, but cpu stuff tends to be more inflexible and so the 'deck hangs on in there",Positive
AMD,"Intel kept iN-gen0-bin0-[KFUT] for 13 product cycles!  To be fair, they have fallen off lately.",Neutral
AMD,you are talking like these cheap new laptops aren't also the same crap but even cheaper. prices of usable nice ryzen thinkpads dropped a lot,Negative
AMD,"Indeed. However, what I was getting at was a callback joke to AMD's disastrous Bulldozer, Piledriver, Steamroller, and Excavator generations.",Negative
AMD,"I don't really like this argument, because I think people are overestimating what the Apollo Guidance Computers did.  Those computers were just going through basic trigonometry, calculus, and algebra. Most of the work the AGC did could be (and in the case of Apollo 13, was) done by the astronauts using pen and paper.   Something as simple as upscaling an image from 512x512 to 1024x1024 using simple Bilinear interpolation requires way more processing power than flying to the moon.  Was it extremely impressive for the time? Absolutely. Are a lot of modern programs unoptimized? Yeah, definitely. But what we're doing on modern computers is several orders of magnitude more complex than what the AGC had to do, and is in no way comparable.",Negative
AMD,Apollo missions were mostly analog.,Neutral
AMD,Very fair point.,Neutral
AMD,>The physical address to cache set mapping function is an implementation detail within the CPU and the same across OSes.  I did not know that but it makes perfect sense. Appreciate the feedback :),Positive
AMD,It's kind of amazing we're at the end of the PS5/Xbox series life cycle and yet devs are still targeting PS4 in so many new games.,Positive
AMD,"Reminds me of that missile memory leak where they were just like ""add more memory who gives a shit"". Imagine if agc had way more capability and then the 13 crew all died because they could no longer do it on pen and paper in time.",Negative
AMD,"Black Ops 7 is coming out in a little over two weeks and it'll run on a launch PS4, which is wild frankly. thing is 12 years old.",Neutral
AMD,The PS5/Xbox Series still being the same price or even more expensive than when they launched isn't helping matters.,Negative
AMD,Microsoft especially looks ridiculous raising the price of the series,Neutral
AMD,"Excellent and trouble-free performance with open source drivers, as we've come to expect from AMD.  Other vendors remain non-options when it comes to an open source stack.",Positive
AMD,"> These Radeon AI PRO R9700 GPU features a 4 nm ""Navi 48"" GPU, equipped with 64 RDNA 4 compute units, providing 4,096 stream processors and 128 AI accelerators for enhanced matrix operations across various data formats. A key improvement over the RX 9070 XT gaming GPU is the increased memory capacity, now doubled to 32 GB. The memory subsystem includes 20 Gbps GDDR6 across a 256-bit wide interface, delivering 640 GB/s of memory bandwidth, supported by a 64 MB 3rd Gen Infinity Cache. The AMD Radeon AI PRO R9700 achieves up to 191 TeraFLOPS FP16 dense performance and up to 1531 TOPS INT4 sparse performance, all within a power footprint of up to 300 W.   It's basically a 9070xt with 32GB of memory.",Neutral
AMD,gddr6 instead of gddr7,Neutral
AMD,Hooray for 9070 XT availability! ^/s,Neutral
AMD,"[Good thing 9070xt 32gb was denied by Frank Azor ](https://www.reddit.com/r/hardware/comments/1ip5huu/amd_denies_rumors_of_radeon_rx_9070_xt_with_32gb/)  >No, the 9070 XT card is not coming in 32 GB.   After all it's r9700, not 9070xt",Neutral
AMD,640GBs is good enough for home use.,Positive
AMD,It's marketed as a pro GPU though?  It's still fast enough for many purposes but this isn't a consumer product.,Neutral
AMD,"My brother in Christ. Just because you're not using a $10000 Ada 6000 with 960 GBps bandwidth, doesn't mean the rest are shit. Marketing has fried your brain  9700 pro - 640GBps  RTX Ada 4000 - 380GBps  RTX Ada 4500 - 432GBps  RTX Ada 5000 - 576GBps  https://www.techpowerup.com/gpu-specs/rtx-4000-ada-generation.c4171  https://www.techpowerup.com/gpu-specs/rtx-4500-ada-generation.c4172  https://www.techpowerup.com/gpu-specs/rtx-5000-ada-generation.c4152",Neutral
AMD,"It competes with things like Nvidia DGX Spark, which has 273 GBs (and a lot less cores). It's not a bad product, the worst thing is that AMD's ecosystem is not as large as Nvidia's.",Negative
AMD,"Ada is last generation, or maybe you're purposely comparing GDDR6 cards?  For Blackwell:  RTX 4000 Blackwell - 672 GBps  RTX 4500 Blackwell - 784 GBps  RTX 5000 Blackwell - 1340 GBps  https://www.techpowerup.com/gpu-specs/rtx-pro-4000-blackwell.c4279  https://www.techpowerup.com/gpu-specs/rtx-pro-4500-blackwell.c4278  https://www.techpowerup.com/gpu-specs/rtx-pro-5000-blackwell.c4276",Neutral
AMD,"it competes with RTX 5090, 4090 and 3090 too.",Neutral
AMD,DGX Spark is a little development system with an ARM CPU... this is a workstation card.   I assume its main competition will be stuff like RTX PRO 4000 but at a much keener price.,Neutral
AMD,"Because they're all GDDR6.  They don't magically stop working just because gddr7 exists.  Thirdly, RTX 5000 48gb Blackwell is minimum $6300 at local supplier VS $1850 for the 9700 PRO.   RTX 4500 32gb Blackwell is $3700 vs $1850 for the 9700 pro.  I have no idea if 140 extra GBps on top of 640 is worth 2x the price.   I would take a GDDR6 at half the price, even if it's Nvidia, over 2x the price gddr7 if the difference is only 140GBps.",Neutral
AMD,"I know, but three of these are $3,900 and DGX Spark is $4,300. With a used CPU/mobo somewhat similar price range. Certainly not bad for a more powerful/flexible system.   RTX Pro 4000 is only 24GB, so you'd have to get 4 of them to match 3 of these AMDs. But again, CUDA can be a rather strict requirement for many people.   At this point I wouldn't say hardware for prosumer local AI is a solved problem at any reasonable price.",Neutral
AMD,OK. I just wanted to be sure it wasn't a mistake that you listed last gen cards.,Neutral
AMD,I get putting RDNA1 support on the back burner but it seems crazy to do the same for RDNA2.,Negative
AMD,Cards like the 6800xt are still non-RT monsters. In this economy this is a great way to piss everyone off.,Negative
AMD,"I didn't think RDNA3 was all that different from RDNA2. The dual issue compute thing hardly seems like it gets any use.   Given AMD's move to UDNA in the future, I'm kind of wondering how long support for even RDNA3 and RDNA4 will last now.",Negative
AMD,"So I got a bonus at the right time and was able to secure a rx 6900xt.  At what point do I look at the support for the card and say that it's time to update? It's been a perfect card to this point. If it helps, I'm running Linux exclusively.",Positive
AMD,"Hmmm, what happens to the support for the iGPU, the integrated RDNA2 GPU that is in (almost) all Zen 4 and Zen 5 CPU models?",Neutral
AMD,"At this rate, 25.11.1 will drop support for RDNA4 and AMD will only support unreleased cards in their drivers. Get it together AMD, you don't have the cashflow excuse any more.",Negative
AMD,They are hurting the good will they gained from the 9070xt launch. Burning RDNA 1 and 2 owners like this is going to motivate them to buy team green when they upgrade. Speaking as someone who has a 5700xt since 2019,Negative
AMD,Wine was so fine that they decided to just take it away,Negative
AMD,This has to be a joke.  Imagine the 3080 not getting day 1 support. Also I'm pretty sure RDNA2 is the best selling AMD generation ever.,Negative
AMD,The RTX 20 series became fantastic purchases over time.  One of the greatest architectures ever made.,Neutral
AMD,"Imagine if Nvidia did this for RTX 30 and RTX 20, reddit would throw a tantrum. Guarantee you'll barely hear a peep about this though.",Negative
AMD,AMD sure loves burning consumer goodwill. I wonder if they think the time and money saved from doing this is worth the hit to their driver support reputation,Negative
AMD,Here am I currently just diciding between 9070XT and 5070Ti for my upgrade. I really don't like the 12VHPWR.,Negative
AMD,"A buddy of mine owns a tech shop and the distributors he works with still have new old stock of 6800XTs and 6600XTs. Granted, the market is slower here but still.   Im kinda bummed because i run one of those 6800XTs. Brand new red devil card bought for 350 euros a year ago.",Negative
AMD,"RDNA1 I understand, horribly outdated architecture. But RDNA2? It is the same architecture used in the consoles...",Neutral
AMD,Bought my 6800 like two years ago. I do not like that but I will definitely deny to upgrade in a two years cycle. I will move abroad in a couple of months and cannot take my battlestation with me but even if I would not move these cards a fucking expensive nowadays. There is no way that I change my GPU every two years.,Negative
AMD,Even though RX 6600 is one of the very few RDNA cards that make the Steam survey list...?,Neutral
AMD,But amd is our friend,Neutral
AMD,Weaker software and support is why I am not going back to AMD for now. With NVidia you just get more valued added. My last AMD card was in fact an Ati HD 5850 in 2010. I would love to be back on the red team but I haven't got excited for a Radeon card ever since.,Negative
AMD,"Hello pi314156! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,What does this mean with the upcoming FSR 4 lite Int4 version that AMD is developing though? Will the official release of it still support RDNA 2 despite it being moved to legacy branch of drivers?,Neutral
AMD,"Bye Bye AMD, never buying from you again. This is pure scam.",Negative
AMD,Isn't ps5 rdna1 and series x rdna 2? Why drop the gpus that are based on these archs when consoles chips are your highest volume sales.,Neutral
AMD,AMD stays solidifying my choice as to why I only buy Nvidia GPUs,Neutral
AMD,linux drivers are fine. they dont get day1 game optimizations but mainstream kernel support should last for at least another 2 decades.,Positive
AMD,"So...does this mean we won't be getting drivers at all (like they did with the HD 5000 series in 2015) or that we'll get drivers, games just won't be optimized for those gpus? Put another way, if a game requires a certain new driver, am I ####ed?",Negative
AMD,As someone who has not cared about updating to day 1 drivers before buying a game for ever really...   How much of an issue is this nowadays? I haven't bought many new cutting edge games so anyone who buys a lot of them can chime in?,Negative
AMD,Hire some software and embedded devs I swear to god. I may be extra pissed because Vitis AI (Xilinx) is a hot mess that feels like it was put together by 5 people.,Negative
AMD,Imagine buying an AMD GPU in 2025.,Neutral
AMD,I can only guess is that RDNA 1/2 lacks AI accelerators and in a bid to catch up to nvidia in terms of software tech they are focusing on the 7000/9000 series.,Neutral
AMD,"It's not their fault that they want to focus since they're finally at a position to surpass Nvidia in some regards. AMD has served Fine Wine for years, give them a pass here guys.",Neutral
AMD,"> crazy to do the same for RDNA2.  Ye, especially since they are still floating around in the channel.   I don't know when AMD stopped manufacturing them, but they sure as hell had stockpiles of them that kept showing up until like 2023/24. It sure as hell is not that long enough ago that it warrants dropping day 1 support.",Negative
AMD,"For people with a 6800/6900/6950 XT, AMD's current best offering only provides a 30-40% uplift. Many people don't want to upgrade until they can get at least 50% or even 100% better performance. If people want that right now, they have no choice but to buy Nvidia, especially with the Super series right around the corner.",Neutral
AMD,how important are day 1 game optimizations though? we're talking couple %s of performance improvement iirc.,Neutral
AMD,"Yep, I was buying AMD for a decade because they are the best bang for your buck in my country, but with that attitude to the customers - thanks, my next GPU will be Nvidia.",Positive
AMD,"I owned 6800XT and it was pretty good in RT too, though of course not as good as 3090 which I also had at the same time. The lack of DLSS equivalent was a bigger issue.  The path-tracing updates that nvidia pushed out to some games ran abysmally on it. But it was doing okayish with previous high level RT in games like Dying Light 2 and Cyberpunk.    https://old.reddit.com/r/hardware/comments/1n31z8b/doom_path_tracing_and_bechmarks/nbn4xyj/",Negative
AMD,"Honestly it doesn't matter, most people won't notice and the cards are still going to run fine for a long time, even without driver updates.",Neutral
AMD,"RDNA3 was a big architectural redesign, regardless of it being very disappointing.    And this just seems like AMD is moving to match Nvidia's driver support policy.  Basically, only the latest two generation architectures get Day 1 performance driver support.",Negative
AMD,RDNA3 has “AI Matrix Cores” that RDNA2 doesn’t. RDNA3 also was “Architectured to exceed 3.0Ghz”,Neutral
AMD,"Linux AMD drivers are open source and maintained by the community, it regularly gets fixes from contributors, Valve, etc.  For some perspective, open source AMD drivers have Vulkan 1.3 support for AMD cards from 2013.  Your hardware will likely naturally die from old age before Linux drivers stop supporting the 6000 series.",Neutral
AMD,The Vulkan drivers for Linux are developed independently with lots of the code written by outside parties instead of AMD. Wouldn't expect this to have an impact there.,Neutral
AMD,linux support for vga drivers lasts decades. i think the latest kernel still has code to support ati gpus from like 2005.  https://docs.fedoraproject.org/en-US/fedora/latest/release-notes/hardware_overview/#hardware_overview-graphics-legacy_gpus,Neutral
AMD,Do iGPUs even get day 1 optimizations for games in the first place?,Negative
AMD,That's RDNA3 and RDNA3.5  RDNA2 iGPU are in Steam Deck / Zen3+,Neutral
AMD,">good will they gained from 9070xt launch   Damn, so older card users have already forgotten about fsr4 they didn't get?",Negative
AMD,"Meanwhile I've been straight up frustrated at their BS ""MSRP"" lies and horrible prices for most of this year.",Negative
AMD,Yep 6950xt was a 1100 card and released less then 3 years ago dropping day 1 support already is crazy,Negative
AMD,Yeah it's pretty crazy if you compare RTX 20 to RDNA1 seeing how much better Nvidia's architecture aged. It set the stage for where graphics tech would be heading in the next decade and AMD has been playing catchup the entire time.,Positive
AMD,"RTX 20 series was so hated back then to the point Reddit even with obvious driver issues of 5700 XT that I get a lot of people who are gaslighting me by saying, ""*I never had issue with mine!*"" ""*Only few people are having issues!*"" similar to what you see under the comments of a unoptimized game today with ""*Mine runs well with no stutters!*""  they were looking past it and was still recommending the 5700 XT over the 2070 Super back then. I know because i was in the market of getting both of those GPUs back then but decided to wait for RTX 3070 instead.  And I am so glad I did, if I didn't and listened to Reddit back then I would have gotten myself the 5700 XT because it was cheaper than the 2070 Super and offered nearly the same raster performance, DLSS wasn't part of the consideration yet for me because back then it was really shitty, I wasn't even aware of DX12 Ultimate API, so i didn't really care if 5700 XT didn't support, which I thought was another Nvidia Gimmick.  Turned out all those ""Gimmicks"" that Reddit and popular Tech Youtubers told me were real enough and is now an important part of video game optimizations moving forward ever since 2023, as clearly shown by games like Alan Wake 2.",Negative
AMD,"If 30 series COVID and crypto price explosion never happened, the 20 series would've continued looking like a joke for the price.  2080 Ti was ridiculously priced, only saving grace was the VRAM. For most people, the 3070 was way better.  2080 got bulldozed by the 3060 Ti.  2070 and 2060 got bulldozed by the 3060.  The 20 series feature set was nice and became better over time, but the performance was severely lacking for the price.",Negative
AMD,People already did that when they dropped 10 YEARS OLD Pascal support,Negative
AMD,"You're literally responding to an article, on reddit, criticizing them",Negative
AMD,That's because reddit has nvidia gpus. How many here have amd gpus and how many of those are 5000 series owners?   That's not exactly a lot of people making noise vs popular nvidias older generations. Might be even why amd did this.,Neutral
AMD,Their fan base might as well be a cult. They got whole YouTube channels dedicated to this kind of nonsense.,Negative
AMD,">Imagine if Nvidia did this for RTX 30 and RTX 20  They already have. just unannounced.   Edit: it seems people are fundamentally misunderstanding where going on here. AMD is NOT dropping support for 5 and 6000 series, they are just no longer including them in day 1 game driver releases. That's all.   Nvidia might still include older cards in those, but they didn't invest any resources into optimizing then for the game either, so it's pretty pointless.   Literally nothing has changed, except AMD made it clearer that you can skip day 1 drivers when you have a older card.",Neutral
AMD,It's not really an issue with 5070ti tbh. Just be sure it's fully in and forget about it.,Neutral
AMD,Resale value is really bad too for AMD GPUs.,Negative
AMD,AMD never stated they are working on int4 FSR 4,Negative
AMD,"No, AMD are freezing all driver development for RDNA 1 and 2. Literally nothing will be changed any more, so expect no new features, and new games to rarely to occasionally not work/crash.  It's been confirmed they're moving these GPUs to the legacy branch, and this is a literal branching of the development tree off, making any backporting of code much more difficult.l, and I highly suspect there will be none.  Legacy branch GPUs only get security exploit fixes, and the very rare crash fixes for extremely basic use cases.   For example the legacy Vega branch aside from security fixes got a few basic fixes to browser video decoding crashes, a Microsoft teams crash fix, I think a Photoshop crash fix and the one game they did a fix for was minecraft, which was a crash fix. That's it.  Alternatively AMD GPUs are still more or less fully supported all the way back to GCN 1.0 on Linux. It's better than nothing.",Negative
AMD,"Yep, artificial obsolete at its finest.",Negative
AMD,"If you know how to read, RDNA1 & 2 are still supported and will get future drivers.  Nvidia has really gaslighted people into believing that they need bi-weekly ""game-ready-drivers"".    You don't. The majority of the time they make no differences, usually they fix some bugs, and have been validated on the latest hyped AAA game release.   The included performance optimizations are often very insignificant, if they exist at all.",Neutral
AMD,"chill, lmao. overreacting drama queen",Neutral
AMD,Consoles have their own driver and software stack. So they are not effected.  What should be worrying is the steamdeck and the rdna2 based APUs,Neutral
AMD,"Both are rdna2.   And console games are always optimized for the hardware so this doesn't impact them. And Steamdeck is Linux which is probably exempt from this. But the z2a chip in the low end Xbox ally x is rdna2 which is a brand new product, so they're still selling rdna2.",Neutral
AMD,Much easier than trying to imagine being an Nvidia customer in 2025. I'm not sure how you sleep at night knowing what your dollars go towards. Objectively more anti-consumer.,Negative
AMD,That has nothing to do with not providing game ready driver,Negative
AMD,How are they fine wine when their older gen cards didn't get any new cool features like Nvidia and they just fell behind when things like RT and HDR became bigger for the gaming scene?  A 2080 can still use the RTX hdr and use the latest dlss versions while RDNA 1 is just dying.,Neutral
AMD,Surpass Nvidia in what?,Neutral
AMD,"> Ye, especially since they are still floating around in the channel. >  >  >  > I don't know when AMD stopped manufacturing them, but they sure as hell had stockpiles of them that kept showing up until like 2023/24. It sure as hell is not that long enough ago that it warrants dropping day 1 support.  You could buy new RDNA2 GPUs like the RX 6600 as of a few months ago.",Neutral
AMD,Bought my 6750xt in july of 2023...   This is dumb,Negative
AMD,I'm about to finally upgrade from my trusty and only just abandoned 1080 Ti to 9070 XT. 400%+ raster Gonna be glorious.,Positive
AMD,And actually good drivers  I know which brand made my GPU BSOD with two drivers in span of 3 months and it sure as fuck isn't Nvidia,Negative
AMD,I have a 1080ti and still will not upgrade until I see improvements to ray tracing computation become better value for dollar and performance.,Neutral
AMD,Some games do not let you run the game without new drivers with them having a driver check at startup,Neutral
AMD,"Your next gpu should be whichever one makes the most sense for you, whether it’s amd, nvidia, or intel.  Nvidia is worse than AMD imo. This doesn’t even come close to the stuff Nvidia pulls with AIBs and other 3rd parties. Don’t forget all the downgrades of cards and calling it the exact same thing so consumers are none the wiser.",Negative
AMD,Nvidia literally do the exact same thing.  They drop driver support for anything more than two architectures old.,Negative
AMD,Nvidia is providing Day 1 game drivers for the RTX 20 series through to the current RTX 50 series though. They even maintained support for Maxwell and Pascal until very recently.,Neutral
AMD,Rdna3 doesn't have matrix cores.  They have WMMA instruction sets which in the CU.  They don't have dedicated AI cores.,Neutral
AMD,"Ah, well that might be in my favor then. Not too excited to save up for a new card yet :D",Positive
AMD,"I think only the Ryzen AI 300 and mobile processors and for example the Ryzen 8000G have better iGPUs, but every other Zen 4 and Zen 5 processor has 2x RDNA2-based GPU cores...  [Here's a guy gaming on the RDNA 2-based iGPU of his 7800X3D](https://www.youtube.com/watch?v=yymkevYwVqQ).",Neutral
AMD,"I did buy a 5700XT at the time. Worse GPU experience I've had. To have the AMD sub almost unanimously tell me it was my fault, I'm doing something wrong. Nothing wrong with the drivers...lol damn cult over there.",Negative
AMD,Your 3070 will get day 1 driver support. Probably for the next 3/4 years.  Rdna2 cards like the 6700xt won't from now,Neutral
AMD,"You can partially thank YouTube channels who absolutely did not want to yield that the forward looking features were actually good.  Honestly, it has been a black mark on a lot of GPU reviews for over half a decade.",Negative
AMD,i think it was the 2080 that performed about the same as a 1080ti but costed more than used or sales pricing of a 1080ti. pascal had huge oversupply issues and they didnt really clear out all those gpus till the 2nd crypto boom after the 30 series launched.,Neutral
AMD,Well to be fair there is probably a lot more people still using pascal then there is rdna 2,Neutral
AMD,"This isn't abandoning them like that, just lowering their priority.",Negative
AMD,"And it barley has any engagement compared to a random ""Nvidia bad"" thread.",Negative
AMD,(AMD unboxed),Neutral
AMD,yeah 30 and 20 series are on tier2 or tier3 support now. almost all the new features omit these 2 generations of cards since theyre 2+ generations old. at least AMD is upfront when they drop hardware down to tier2 support.,Neutral
AMD,"So, they will never release it then? Then what is the point of all the work they were clearly doing with the leaked version of FSR 4?",Negative
AMD,Except both AMD and nvidia have been silently doing this exact same thing since they started making GPU's.,Neutral
AMD,I've had very bad experiences before with driver support from AMD in this regard. 2 years later after I bought a laptop they switched to legacy support... That just sucks mate.  I will believe what you say tho... Would hurt really bad to have a card that is unsupported as soon as I bought it...,Negative
AMD,They aren't really overreacting though....a GPU that's just hitting 4 years old and is still being utilized in current gen consoles shouldn't be shafted this soon. Considering the 6000 series was already scarce during the GPU mining craze.,Neutral
AMD,Dude... NVidia lasta with driver support much longer. Where the drama in that? Besides stating the next purchase because of lack of support is not making drama.  Cash grabbers get what they deserve.,Negative
AMD,"Not really, AMD drivers are the biggest pile of dog shit I ever had to deal with",Negative
AMD,"Steam Deck, on Linux at least, is also a completely different driver stack. Developed/maintained by Valve I believe.",Neutral
AMD,It does if they just want to focus on RDNA 3/4 thus they dont really have to test RDNA 1/2.,Neutral
AMD,Consumer extortion i guess?,Neutral
AMD,Even ***today***: $80 for a RX 6500 XT.  [PowerColor Fighter Gaming Graphics Card](https://computers.woot.com/offers/powercolor-fighter-gaming-graphics-card-13),Neutral
AMD,You can buy amd laptops and handhelds with _currently produced_ APUs with rdna2.,Neutral
AMD,You bought amd expecting good driver support? Understandable mistake to make.,Negative
AMD,"You can test assured that your 9070 XT will be abandoned a lot faster, though.",Neutral
AMD,"if you update your drivers to a whql certified version and your computer becomes unstable, its usually because of a bad overclock or unstable xmp/docp profiles.",Negative
AMD,We have both AMD and Nvidia at home and no issue whatsoever.,Positive
AMD,Did you already forget the fiasco drivers for the 5000 release  at first ?,Neutral
AMD,"wow, that's some peak incompetence from the devs. you'd think the game worked without extra hacks added to run it. well, it probably will if the check is bypassed though..  unless, of course, you are nvidia and you don't follow the spec to begin with, so you might have to do that.",Negative
AMD,"Don't be silly, only knee-jerk reactions are acceptable these days.",Negative
AMD,"I do agree with that, but the only reason I got 6800 at a time was a lack of VRAM on 3070 and 3080. 3080ti with 12gb was enough for my needs (VR), but was out of my price range. I do understand the shit Nvidia does, but from a customer perspective there ain't much difference if they ""downgrade"" a card with their shit naming scheme, if there is no competition.  I got fucked by AMD drivers once when buying a laptop with AMD GPU that got discontinued one year after release, but I let it pass. Don't think I'm going for it again.",Negative
AMD,"What? No. Stop spreading lies. The 750ti was getting game ready drivers for 9 years. We only just got the GTX 1000/Pascal to stop having drivers recently this year and that took 9 years too. RDNA2 released after RTX 30 Series/Ampere and they’re already putting it into maintenance mode just shy of 5 years while Ampere is still going to get game ready drivers day 1, even RTX 20 Series is still getting day 1 game ready drivers and those cards are 7 years old",Negative
AMD,"He said the same thing in another comment, I dont think he owns an nvidia card",Neutral
AMD,"I swapped my spare 1080Ti for a friend's 5700XT because of the issues they were having with it.  I used it myself on Linux, which had no issues, but I don't exactly go around telling my friends to install Linux to fix all their problems (just most of them)",Neutral
AMD,"I have 6750XT, when I complained about new drivers (twice in span of 3 months) causing BSOD when alt tabbing, I was called mad.  Guess what, AMD admitted to it being an issue *and redditors still instead I was the mad one*",Negative
AMD,"Oh, I don't have a 3070 anymore, I sold it back on late 2022 and got myself a 4070 Ti at early 2023 for nearly the same price as what I sold my 3070 back then.  But yeah, If i still have that card today, the only thing I probably would regret from it is the limited 8GB Vram capacity it had, but that is easily solvable by turning down texture settings and using DLSS.  Lack of DLSS Ultimate API support? as well as proper upscaler nowadays such as DLSS and now apparently driver optimization support? That is a lot harder to get by and optimize / tinker for to get respectable image quality and performance overall.",Neutral
AMD,oh you're so oppressed.,Neutral
AMD,lol what a strawman,Neutral
AMD,I've never seen a comment age like milk so quickly.,Negative
AMD,You mean like DLSS 4 upscaling? Oh wait,Neutral
AMD,No idea I don't work for amd. In the interview done by ancient gameplay they stated that amd is always trying stuff. They could have put it on pause for Redstone and may come back after.,Neutral
AMD,"All of these companies have a ton of projects going at once that are never going to see the light of day because they are competing against other internal projects, just don’t work, market conditions, etc.",Negative
AMD,It’s likely the INT8 branch was just the original one before they decided to move to FP8. AMD might release an FP16 version for RDNA3 instead. RDNA3 already runs the FP8 version just as well as the INT8 one in Linux and it looks better.,Neutral
AMD,"Yup. AMD literally launched more RDNA2 cards **three years ago** in May 2022. Not announced RDNA3, not ended production, but *LAUNCHED* new cards.  [AMD Launches Radeon RX 6950 XT, RX 6750 XT, and RX 6650 XT, New Game Bundle | TechPowerUp](https://www.techpowerup.com/294707/amd-launches-radeon-rx-6950-xt-rx-6750-xt-and-rx-6650-xt-new-game-bundle) \- **MAY 2022**  *If* you bought them at lunch, it means in 3.5 years, these GPUs are now completely out of new game driver updates.  Fuck that lmao.",Neutral
AMD,This isn't about not fixing driver bugs but not adding optimisation for suboptimal game code. Something which should be the job of the game producers anyway.,Negative
AMD,Games will still work on your machine. Fucking hell. They're not doing an Apple move,Negative
AMD,"The SteamDeck uses the open source Mesa driver. Linux will still get driver updates because they are independent from AMD. In fact GPUs all the way back to Terradcale still get driver updates on Linux. And Polaris, Vega and RDNA1 even have software RT support.   It's amazing what can be achieved when profits are no longer prioritised above all else.....",Positive
AMD,Well they should be testing rdna 2 Gpus they literally released in 2022 and already dropped day one support?,Neutral
AMD,"And newegg sells new geforce 1050 cards. If you're willing to expand that to third party sellers on newegg there's a new geforce fx 5200 from *2003*. Probably just sitting in some dusty warehouse somewhere hoping someone who doesn't know what they're doing and/or some *need* for an identical replacement pops up.  I'm not sure ""Still being sold"" really matters without knowing the numbers.",Neutral
AMD,"My RX 580 got day 1 driver support for a longer period of time than my 6950Xt will.  So yes, I did expect proper driver support and no that is not an ""understandable mistake"". This is a shit move by AMD.",Negative
AMD,Mate the 6750xt came out in 2022,Neutral
AMD,Wow 1 persons anecdotal evidence. My 5090 hasn't burnt to a crisp yet. Means that 12vhpwr is flawless right ?,Positive
AMD,It was so bad that the 5000 series got so cheap,Negative
AMD,Don't try to chastise them man. He deserves to have his wrong opinion too.,Negative
AMD,game ready drivers dont really contain optimizations or features for older architectures. just because its supported in the same driver package as the latest cards doesnt mean theyre keeping it in tier1 support like the newest cards.  at least amd is pretty upfront about it. nvidia's been beating around the bush with it for decades now.,Negative
AMD,Got something to hide?,Neutral
AMD,Do you create subs just so that you can seem more important as a mod?,Neutral
AMD,yep framegen is being artificially limited because ampere and turing support is being dialed back. no reason why FSR3 framegen works on these GPUs but nvidia's own solution doesnt.,Negative
AMD,5 and 6000 series cards aren't being dropped from new features (if they can support them)    That's not what this was about at all.,Neutral
AMD,"He said that redstone was their main priority but rdna 3 owners should stay tuned, its also confirmed that the same fsr4 version is coming to ps5",Neutral
AMD,"From what ive heard its the other way around, the original one being FP8 and they had to rewrite the entire thing in INT8 for PS5 and RDNA3.  if the can make it work i would prefer higher performance with INT8 than higher quality with FP8.  In my opinion after spending months with both versions and DLSS4, if you apply some contrast adaptive sharpening to INT8 FSR4, all 3 are almost indistinguishable unless ur nitpicking",Neutral
AMD,">AMD literally launched more RDNA2 cards three years ago in May 2022.  Try October, 2023.  https://www.techpowerup.com/gpu-specs/radeon-rx-6750-gre-12-gb.c4183  24 months.",Neutral
AMD,"Exactly! I forgot about the ""50"" variations, thank you for pointing those out. Its incredibly frustrating, I especially feel for anyone that bought one within the last 1 to 2 years. I think we can all agree that it's a shitty move on AMDs part. If I recall correctly, isn't the 6000 series still showing a fairly high usage on Steams Hardware Survey?",Negative
AMD,"Didn't mention anything about driver bugs, this is about these cards not receiving future game support.",Negative
AMD,What about encoding support and enhancements for it? Bug fixes? It's not only about games...,Negative
AMD,"To conflate the 1050 released ***nine*** years ago and the RX 6500 XT released **four** years ago seems silly in terms of sales + quantity.  If you have sales data per SKU per quarter, please share. Who wouldn't want that data?",Neutral
AMD,"I owned two HD6950s back when Crossfire was still a thing.  Those were supported for less than 5 years, so you could've very easily bought the card a few years in and gotten less than 48 months of active support.   Unfortunately nothing new for AMD.",Negative
AMD,"You realize the same argument can be made about the dude complaining about issues with AMD drivers, right?",Neutral
AMD,What?,Neutral
AMD,Better than cosplaying as a corporation ig.,Positive
AMD,"For real though, they're just throwaway meme subs. Sorta thing you'd link in a thread once and gets forgotten.",Neutral
AMD,"Nine, most of which have nada in them lmao",Neutral
AMD,"Oh yeah, I'm King Fuck of Shit Mountain  edit: TIL it's King Shit of Fuck Mountain https://en.wiktionary.org/wiki/King_Shit_of_Fuck_Mountain  ^edit: \s since that's not clear lmao",Negative
AMD,Oh unbelievable. I forgot about this card completely. A good find.  Absolutely insane. 2 years of game-specific updates is ***nothing*** for a $300+ GPU.,Positive
AMD,"No, it's about them not being included in day 1 game specific driver releases.  Which aren't needed in 99% of cases anyway, and only bring marginal performance increases.  It's a storm in a teacup.",Negative
AMD,"Yes and yes. They really should rename it to ""standard support"" or something.",Neutral
AMD,This is specifically for *day one* drivers. Chill out and learn how to read the article.,Neutral
AMD,">bux fixes  Tbh those never existed, I can't think of a single issue I had that was fixed on my 6750XT that was down to drivers",Negative
AMD,"You're misunderstand what I'm saying, I'm not trying to conflate two cards. Just that ""Still being sold"" isn't really a useful statement about age, as you can find new stock of pretty much any card from the last 20 years if you look for them.  And honestly I think this entire thing is odd, ~~and the translated statement quoted in the top post seems to contradict other statements made from other AMD sources (namely the rep on the discord - I saw a screenshot of their comment I'm trying to find now to reference that said it was only *new feature* support that would be lacking from rdna1&2 - which likely makes sense as in the latest release that kicked off all this shenanigans the ""new features"" were primarily AI-specific ops that don't have hardware support on those devices).~~  EDIT: I still can't find the source, though I'm pretty sure I didn't imagine it, and the only AMD discord I could find doesn't even seem to have the user I remember from the screenshot (""AMD_Vik"" I think?). Though at the end of the day, a screenshot is just a screenshot, not evidence of anything :P  However on that (official) discord there was:  > ""We’d like to inform you that the release notes for AMD Software Adrenalin Edition 25.10 2 posted included misinformation that has since been corrected. There is no change to USB-C functionality on the RX 7900 series GPUs in the 25.10.2 driver. There was an incorrect line in the originally posted release notes that has been removed, and the release notes have been updated.""  Which kinda makes one of yesterday's big drama posts void? And kinda makes me wonder who the fuck AMD have *writing* these ""release notes""? Especially if this current ""dropping support for RDNA1&2"" also turns out to be less than feared.",Negative
AMD,Amd were basically scumbags till after HD 7000 series lol,Negative
AMD,He tried to look at your profile for ammunition against you but your profile is set to private.,Neutral
AMD,Does it make a difference? Didn't I pay for support? I'm second class citizen? This is why companies treat us like dirt: we accept sub-par customer service.,Negative
AMD,"oh, that's exactly why I set it private. I'm sick of people who get upset at me going through my history.",Negative
AMD,"Yes, you did. It's an absolutely shitty decision to do it ***this*** soon.  As a reminder, just 3.5 years ago, AMD launched *new* RDNA2 GPUs: [AMD Launches Radeon RX 6950 XT, RX 6750 XT, and RX 6650 XT, New Game Bundle | TechPowerUp](https://www.techpowerup.com/294707/amd-launches-radeon-rx-6950-xt-rx-6750-xt-and-rx-6650-xt-new-game-bundle)  One of them cost $1000 and in 3.5 years, it's fucked for new game updates.",Negative
AMD,"> Didn’t I pay for support?  Actually, no, you didn’t. You paid for a product with the assumption of support, which has been *marginally* reduced. And buying AMD shouldn’t come with any support assumptions given their poor track record.  Without an announcement you wouldn’t even have noticed, game-ready drivers are a nothing burger. AMD support has been an issue but *this* isn’t worth getting that upset over.  Edit: some of y’all find this so offensive that you leave comments that the mods have to remove. Take a chill pill",Negative
AMD,"With this mindset, we deserve sub-par support if none at all then. Amazing. This world is fucked.",Negative
AMD,"Sure, and with that logic and AMD’s track record we should have all known not to buy RDNA in the first place.  Vote with your wallet, don’t buy AMD again.",Negative
AMD,TLDR: Intel and AMD warned their enclave defenses don't defend against physical tampering of the memory/motherboard. The main vulnerability for both of their enclaves was a decision of performance vs security tradeoffs. Yet dozens of companies claim their software running in those enclaves are resistant against physical tampering and thus can run on untrusted hardware.,Negative
AMD,"> The low-cost, low-complexity attack works by placing a small piece of hardware between a single physical memory chip and the motherboard slot it plugs into. It also requires the attacker to compromise the operating system kernel. Once this three-minute attack is completed, Confidential Compute, SEV-SNP, and TDX/SDX can no longer be trusted.   lol. You need to pull out memory, plug something between the board and the memory, and then somehow magically also modify the OS kernel at the same time. And this is supposed to be easy and quick? What a load of BS.",Negative
AMD,"Note that these secure enclaves are typically designed pretty differently from the one Apple used in their M and A chips, which is a completely separate CPU.",Neutral
AMD,This is actually a pretty big deal. Effectively the key selling point for TEEs is defective.,Neutral
AMD,"Many places that operate these machines claim that it protects the customers from the owner of the machine tampering or spying on their data. The companies making these products do not claim this, but this has not stopped companies renting them out from claiming it.  And the kernel that needs to be ""compromised"" is not the kernel of the customer, the person adding the shim can briefly boot the system from their own drive to compromise the system.  In that sense, the attack is not unrealistic at all, the owner of a machine can easily take it offline for half an hour, add the shim, boot it from a portable drive for long enough, and then put it back into operation.",Neutral
AMD,"That isn't particularly relevant here, from my understanding. The attack doesn't care where the data is going to, just whether they can tap into the interface and whether the encryption used is susceptible to replay attacks. The storage still sits in DRAM in the Apple design, and that data has to travel over the DRAM interface, which is susceptible to a physical MITM actor grabbing information. The main differentiator here would be whether or not the encryption is deterministic, which is where this particular vulnerability becomes effective (because once you've grabbed the right transaction, it can be used repeatedly).  From what the researchers are saying, client-side SGX used to use non-deterministic encryption; however, as the scope of protected memory ballooned (esp with TDX's use case on server), that shifted to deterministic encryption to limit the performance impact for memory regions that large. Presumably, Apple doesn't need to contain entire programs within their secure enclave (same as Intel with the original client design), so there's a good chance this attack doesn't work on them for those reasons instead.",Neutral
AMD,"It's only become a key selling point in a specific context. Which is essentially ""people who don't know what they're talking about talking to people who lack the expertise to verify their claims"".   These kind of TEE type countermeasures, like any countermeasures, always have a specific threat model and specific security guarantees. Which, as the article points out, never really included physical tampering attacks or even side channels.  The problem is once it stops being security experts talking to security experts those details are almost immediately lost. People find it all very niggly and boring and overly detailed and full of caveats and very few concrete guarantees.   So it gets simplified. The caveats get forgotten. The details get smoothed out. Things that were ""Guarantee X under condition Y assuming 1, 2, 3, 4"" become guarantee X.  Aaaaand you get this sort of thing where some researcher proves that you can do nasty things outside of condition Y or when 3 isn't true or something. People cry foul ""but you guaranteed X under all conditions!"" and the people in the know go ""no we fucking didn't"" and point to a security spec that literally nobody ever read besides them. Everyone is annoyed and thinks someone else is to blame.",Negative
AMD,">Meanwhile the AMD Ryzen Zen 6 ""Medusa"" openSIL code will come in the first half of 2027.  So Zen 6 is delayed",Neutral
AMD,">They plan to open-source their Venice openSIL code around one quarter after those 6th Gen EPYC CPUs ship. They say that open-source will be in 2026, so that would mean Venice is launching in Q3 or earlier.  Nah medusa's mobile which launches at ces 27. According to the same slides venice's launching q3 or earlier which confirms a 26 launch for zen6. Amd also said that its helios rack with venice and mi450 is launching around q2-q3 26 at their dev event.",Neutral
AMD,"Medusa is the mobile lineup. i.e. presented at CES, available a few months later. Always has been.",Neutral
AMD,Not necessarily. They might just focus on the servers and when they have figured that out they will work on client. Might be comparable to ROCm where clients have for a long time been 2nd class citizens.,Neutral
AMD,That cache is bigger than my first HDD lmao,Neutral
AMD,FYI: AMD has an Epyc with **1152MB** of L3 caches over 96 cores. And that's over 2 years ago.  Yes 1+ fricking **GIGABYTE** of cache.  https://www.techpowerup.com/cpu-specs/epyc-9684x.c3253,Neutral
AMD,Need to see how the testing goes with the reviewers but if it gets any sort of lift for compiling code or shaders or rendering - people will buy it.,Neutral
AMD,Can’t wait for the 9955x3D2xD AI Pro MAX 395,Neutral
AMD,Are there real workloads getting an uplift from this? As I understand it cross CCD talk significantly reduces the benefit and for gaming the 9800X3D and many workloads just run basically equally fast on the non-X3D CCD of the 9950X3D such that gaming + background tasking seems fine on that chip.   So I don't think this will be any special chip other for maybe fluid simulations or the likes where it might be better but those workloads are probably better served with workstation/server hardware either way.,Neutral
AMD,missed opportunity for 9950X6D,Neutral
AMD,"""Shut up and take my money!""   On a serious note, we will have to wait for gaming benchmarks to come out. Will games spread their workload out across both CCD? And how much will the latency between the CCD hurt? And what games are CPU constrained on a 9800X3D that would show a considerable performance boost?   We have been yelling at AMD for a dual 3D v-cache chip since the 5800X3D and they have always claimed it didn't have much benefit. This will either prove them right or wrong.",Negative
AMD,This is gonna be like a 1000 dollars,Negative
AMD,"If they release this, there will probably be even more of an expectation for a dual X3D part for Zen6. So maybe we can expect it to work that way with Zen6?",Neutral
AMD,I'm listening.   Does this mean I can raid in World of Warcraft Classic without FPS drops.,Neutral
AMD,the 9950X3D2 doesn't make any sense to produce but i suppose the normal 9950X3D isn't selling too well so this might be a way to dump some inventory faster and satisfy some of the calls for the double cache enabled 16 core CPU's.  the 9850X3D is far more interesting being a higher clocked 9800X3D. I suspect this was an unexpected yield gain over time that AMD is going to capitalize on for some extra profit.  one thing is clear these new variations are a sign that Zen 5 production and sales and being changed in preparation for Zen 6.,Neutral
AMD,That's gonna be expensive.,Negative
AMD,Can’t wait for the review thread where all the people begging AMD to make this chip complain about how it provides little to no benefit in gaming.,Negative
AMD,Is it 192 MB for one ccd or 96 MB each of the 2 ccd ?,Neutral
AMD,"That chip will have more cache than my first family PC had RAM.  Well, the existing X3D chips already beat that first PC anyway, but the X3D2 gets close to my second family PC too (256 MiB).",Neutral
AMD,"if they release the 9950x3d2, im moving away from the 7950x3d",Neutral
AMD,"I bought my 9950x3D a month ago, I’m very happy and continue to be very happy. Hell I was happy on my 7900X.  No, you don’t need a 9950x3D+3D to play Minecraft or Cyberpunk.  Enjoy what you already own and use the fuck out of it to extends the value you got out of your purchase.  #TedTalkEnd",Positive
AMD,Damn these seem like a better upgrade than your typical Intel KZ model. AMD is not easing on the gas and if Nova Lake rumors are correct a 10950X3D will plow the single CPU chiplet 8+16+4 BLLC in multithreaded performance by virtue of having 24 cores 48 threads.,Positive
AMD,Thats not too far away from a 14900KS base power draw,Neutral
AMD,The refresh is making me think zen 6 will be a new chipset am6 maybe.,Neutral
AMD,Jesus 200w.  My entire ubiquiti network stack uses like 150w right now.,Neutral
AMD,"I literally upgraded to a 9950x3d last Friday.   I'm kinda pissed, NGL.",Neutral
AMD,This I like.  WIll buy if launched,Neutral
AMD,"Unless they get Windows to schedule really fucking well, Chiplet hopping will eliminate most of the cache advantage.",Negative
AMD,I can't imagine this brings much benefit over the existing 9950X3D besides for much more simplified scheduling.,Negative
AMD,200W TDP must be like Intel,Neutral
AMD,"Pretty much the same TDP as the FX9590, but the 9950 slightly faster of course  edit: damn why the downvotes, its a joke. Are people triggered that easily? i know the speed increase is huge",Negative
AMD,Pretty insane how far things have come.   Anyone remember floppies? 5.25 and 3.5 ? 😂,Positive
AMD,My first hard drive was 20 MB.,Neutral
AMD,Damn,Negative
AMD,\*slaps roof of die\* this bad boy can fit so many fucking bytes in it,Negative
AMD,First Voodoo GPU had 4Mb of VRAM.,Neutral
AMD,Caches have been bigger than my first HDD for a while now. It was 10 MB. On a 5 1/4 drive.,Neutral
AMD,> my first HDD  my first HDD was a 30 Megabyte Run-Length-Limited hard drive in a PC 8088 clone,Neutral
AMD,A Threadripper without the lanes?,Neutral
AMD,Gen X or boomer?,Neutral
AMD,My Bernoulli Box with the giant disks could store 20MB and I never imagined how they could get more storage than that.,Neutral
AMD,Ouch. My first one was 800 MB so the cache has some way to go.,Neutral
AMD,"Costs 15,000$ btw (and that's if you buy 1000 units wholesale lol)",Neutral
AMD,"It’s 12x 96MB, it’s a huge difference",Neutral
AMD,How much power do you need to render people?,Neutral
AMD,"I work on search engines, I hope this boosts my workstation performance for local dev even if the end result runs on massive clusters.",Positive
AMD,Could you kick up the 4d3d3d3 ?,Neutral
AMD,"Steve, is that you?",Neutral
AMD,"9995x4D Chess,",Neutral
AMD,"Honestly I think they probably is they have maxed out the available single digit numbers. Instead of making everything 9999x3D99, the need a new digit that's higher than 9, maybe they can use a base-20 naming convention where they use emojis to as higher digits then we can have the 🅱️950x3D or the 🍆700X or the juiced to the gills 🅱️🅱️🅱️0x3D.",Neutral
AMD,"Yea, I think it's sorta like how Nvidia almost dropped the 4090 Ti, it's probably mostly for bragging rights.",Neutral
AMD,I think they will keep going until the latency of increased memory equals RAM,Neutral
AMD,"Zen 5 is so memory bandwidth bound that on many tasks on the 9800X3D, the 9800X3D is always faster at the same power level.  I'm willing to bet the relative performance uplift will be higher on this chip than on the 9800X3D since it has 16 cores to feed instead of 8.",Positive
AMD,"One would hope there's an Epyc 4585PX2, or whatever it'll be called, on the horizon. A year since Turin's launch, there are still no Turin-X options.",Neutral
AMD,"Math/physics stuff. For example Prime95 will run in the 3D cache and is incredibly fast when doing so (>2x the speed of non-X3D chiplets), so this would allow someone to run two workers at that speed instead of just one. Everyone who runs Prime95 has been thirsting for a dual X3D consumer chip for years, since moving to Epyc would dramatically increase the system cost to impractical amounts (not just the CPU costing 5-10x more, there's motherboard cost and populating 12 channels of ecc ram instead of 2 channels of cheaper onsumer ram)",Positive
AMD,"Well AMD have been making high core Vcache CPU's for a while now, so presumably yes.  It's always been useful for more than just gaming.",Positive
AMD,"Probably not for gaming because of the latency between the CCDs, but there are a few workloads that benefit.   Off the top of my head, there are a few code compilers that are absically cache benchmarks.  Remember that the whole 3D Vcache tech was originally concieved and implemented for server chips.   Getting them on gaming chips was a happy accident because one of the engineers tested it out of curiosity and saw the potential.",Neutral
AMD,"This is going to be great for some Game Servers, but also I think the 9800x3d will probably stay superior for the most part due to being cheaper and not splitting the already bad memory speed/bandwidth. High-end game servers will use Threadripper/Epyc X3D chips anyway.",Positive
AMD,The 9800 x3d had pretty huge performance gains in some games.  https://www.tomshardware.com/pc-components/cpus/amd-ryzen-7-9800x3d-review-devastating-gaming-performance/2,Positive
AMD,"Yes, productivity workloads that are highly parallel and that benefit from more cache. For example, database operations when parallelized don't have much if any inter-core talk, but benefit immensely from faster memory and bigger cache. The high end EPYC have cache on multiple CCDs.",Neutral
AMD,"That lessens the benefit, but it isn't a burden so much. My guess is they are planning to go 3d by default for their high end chips once they close any clockspeed performance gaps.",Neutral
AMD,a thread just above you is people salivating on how much compile time this will save them on local builds.,Negative
AMD,"I'd buy it just for the situation you mentioned, having the best gaming CPU whilst also having the cores to do the background tasks and not effect my FPS.",Positive
AMD,"I’ve read that it does improve LLM speeds, that’s why AMD decided to commercialize this",Negative
AMD,Not going to stop people buying it.  Is going to be interesting second hand in a couple of years as these might sell for less than a 9800X3D due to all the FUD on the internet.,Neutral
AMD,"""9950X3-double D"" has a ring to it.",Neutral
AMD,9950X358D/2 Days 0.8 Final Workload Prologue ~cache of the heart~,Neutral
AMD,"I know MLID has been saying this on his podcast, but I really don’t like “6D”. Stinks of marketing people who don’t know anything (even if that’s not the case); I would groan every time I thought about it.",Negative
AMD,"When I speed-read the model number in the title, I thought it was 9950*DX3*, a blast from the past",Neutral
AMD,Why not 9950X3D,Neutral
AMD,"Neither, on your last statement.  It won't affect games which don't need more than eight cores of compute.  It will affect games which use more, as having the same amount of cache on both CCD's means there's no need for one CCD to constantly request data from the other CCD.  The main games that will benefit, however, are those few which don't schedule properly by default and therefore have poor results with the 12-core and 16-core X3D parts compared to the 8-core part.  They run properly with something like *Process Lasso*, which no reviewer uses for some bizarre reason, but with this part, that won't be necessary.",Neutral
AMD,Correction: They said there ARE some benefits but the cost wouldn't justify them.   I think having the 3D Vcahce running at 400Mhz higher *and* on both CCDs will be a nice overall win.,Positive
AMD,Hopefully the msrp is 750 it will sell well.,Positive
AMD,"If Zen 6 adopts the fan-out design to replace the infinity fabric, the cross-CCD latency wouldn't be as much as a problem anymore.",Neutral
AMD,"WoW is famously limited by its singlethread performance, so no this won't help with that at all. The best CPU for WoW would probably be the 9800X3D.",Negative
AMD,"With the chiplet design it means they can throw it together as a low volume product relatively easily. Feels like a ""just because we can"" kind of product.",Positive
AMD,"I’m not convinced … if the high-binned X3D chiplets aren’t selling enough via the 9950X3D, they can just use them for 9800/9850X3D’s",Neutral
AMD,"I don't totally agree, the 9950x is a really good processor and gets you into Epyc performance territory on desktop platform relatively inexpensively, I feel like the x3D versions are too overpriced for the modest gains",Positive
AMD,"Practical or not, people have been wanting something like it since the 5800x3d dropped. I think it will sell okay.",Positive
AMD,Tengo un procesador Intel con 128 Mb de caché L4 y me costó unos pocos dólares 😉,Neutral
AMD,"The second. Each CCD has 32MB of L3 cache, the 3D cache is another 64MB stacked with a CCD.",Neutral
AMD,Do note that AMD reports TDP differently (as in they only have one and it’s a complex calculation that is neither base nor boost power). AMD’s PPT for their 170W TDP parts is already 230W.  A 200W TDP is likely above the 14900KS’ 253W boost.,Neutral
AMD,Honestly no difference lol they both run at the same temps. Am5 ihs seems to be terrible imo.,Negative
AMD,"AMD have been experimenting (via Strix Halo) with eliminating the SER/DES components of InfinityFabric which, if successful, could enable a serious speedup of die-to-die (and die-to-IO) comms.  Rumors are that such designs were destined for Zen6 parts but I suppose it's possible they could be using it for 9950x3d2 and 9850x3d.",Neutral
AMD,"First off, they mostly have already. Secondly, this will curb/reduce instances where the non-3D cache CCD has to call the 3D-cache CCD for data since it will have its own phat cache.",Neutral
AMD,The way AMD is calculating TDP is different. 200W TDP for AMD is more than 14900KS TDP.,Neutral
AMD,This is the same TDP as the 9950X and 9950X3D. Both will top out at 200W unless you enable PBO.,Neutral
AMD,I used 8-inch floppies.,Neutral
AMD,"If I had to hazard a guess, I bet most people under 25 don’t know why the save icon looks like what it does",Negative
AMD,Any Tape / cassette with the Amstrad CPC464,Neutral
AMD,"I had stacks of 3.5"" floppies with stuff I cared about on them",Neutral
AMD,"I love how in my native language we can differentiate 5.25 and 3.5 just by their nickname (unlike in English where both are floppy even though 3.5"" isn't really limp). In my native they are (Lerppu = Floppy and Korppu = Cracker)",Positive
AMD,"360KB for double-density and 1.2MB for high-density 5-inch floppy disks, so this cache is like a giant storage box full of them.  It blew my young mind the first time I learned that 3.5 inches had more storage even though they were smaller. Like, how? Shouldn't the big one store more? Oh yeah, and the hard disk isn't really hard on the inside. Wild stuff man.",Neutral
AMD,The first couple of PCs I used had the floppies that were actually floppy.,Neutral
AMD,I'm still cutting the notches out with my teeth.,Neutral
AMD,Dual 5.25 drives made me feel like a king. A whole second disk? Infinite storage!,Positive
AMD,"still have a box 3.5"" ones but dont have a reader anymore. Pro tip: the floppies can work as safety glasses for watching sun eclipses.",Neutral
AMD,I do,Neutral
AMD,"You know you're old when you see people writing ""Anyone remember floppies?"" like it's a meme..",Negative
AMD,"Mine too, that thing was always full. I remember screwing up and deleting dos while trying to clear space for some game.",Negative
AMD,"> My first hard drive was 20 MB  Mine was the 30 MB RLL version, which as far as I knew was the same physical drive as the 20 MB but with RLL data compression",Neutral
AMD,"10 upvotes for saying damn? And even a reddit notification, which I clicked to come here and comment.   Looks like I am addicted to reddit a bit much, see you later...",Negative
AMD,"It was 4MB (actually 2x2MB) of DRAM. In the 90s VRAM was a technology, not a use case, and the Voodoo did not support VRAM.",Neutral
AMD,"Most buyers will get a 25% to 40% discount on that in actuality.  Most server deals have stupidly high list prices on everything, RAM, CPU, SSDs, but then a big bulk 30% to 50% discount is added at the end, so you can't actually tell what individual parts were sold for because the server provider is negotiating that with suppliers like AMD, but you can tell 100% for sure that it isn't list price that they (Dell, HP, Supermicro, etc) are paying.",Neutral
AMD,rookie numbers for the amounts the builders may be buying these in,Neutral
AMD,Think 40% discount irl,Neutral
AMD,"Just buy the 7000 series on eBay, you can get 256 threads cheaper than 32 threads on an Apple Mac M3 Studio. https://www.reddit.com/r/Noctua/comments/1o3002k/2xamd_epyc_7763_128cores256threads_cool_as_a/ (and some 6-8x faster)",Positive
AMD,Depends on how big the person is of course,Neutral
AMD,About 200 mW and 9 months for the first draft.,Neutral
AMD,9000 Watt hours per hectare.,Neutral
AMD,yeah I am just happy we aren't back in the day with incredibuild and turning PS3's into compiling machines so it wouldn't take 40 minutes to make a build for a single line of code changed to get a milestone out. They built me a 25k server machine so I could compile COD levels as a level designer and it was still taking me 8 hours to get a few of the levels built. It would take an hour just to open a level. Ah the good ole days. :-),Positive
AMD,Now Tayne I can get into,Neutral
AMD,Saving that for the next next gen,Neutral
AMD,"The brand new Ryzen 4d6, which are basically a batch of untested chips on an experimental process with normal yields that can have anywhere from 4 to 24 usable cores",Neutral
AMD,"Very helpful, thanks intel",Positive
AMD,Bragging rights and AMD testing market reception and motherboard resilience at 200W out in the wild.,Neutral
AMD,The latency gap is pretty minor vs the non- 3d vcache variants and mostly masked by the L2 Cache in practice.   There's always trade offs but I struggle to see them getting anywhere near DRAM latency. DRAM is on the order of 10x higher latency. There's also a bandwidth difference.,Neutral
AMD,GNR moment lol,Neutral
AMD,Exactly. I do physics sim work for ultrasound based neurotech devices. I need more 3D Vcahce PLZ.,Neutral
AMD,"Yes, as I mentioned fluid simulations, I know there are applications for this, but I think the number of people who demand such devices and who don't use a workstation system / remote servers is probably low, as professionals tend get big systems from corporate money.   There are probably some prosumers for whom it is interesting. But those will be high end users who aren't gamers (because I expect the difference in gaming be practically null), and use applications that scale mich better on X3D. They exist, but are probably very very few people such that validation wouldn't even be worth it.   I think the target for this platform is much more enthusiasts with to much money who'd buy it for the bragging rights (and rightfully so, all power to them) but the business case for the productivity tools is very niche.",Neutral
AMD,"Yes, VCache was invented for simulations and the likes of enterprise workloads, so I would think that there are very very few consumers benefiting from this. In enterprise there will certainly be people who can make use of it, but they probably could make even more use of the enterprise offerings....",Neutral
AMD,"Yes, and they won't get bigger with a second x3d CCD.",Neutral
AMD,"That review claims that a 7800x3D runs BG3 at 140 fps, but i can confirm from personal experience it runs stable 144 with no significant dips, even in act 3. seems odd and makes me not trust the rest of numbers.",Negative
AMD,"But the 9950X3D can already handle background tasks effectively with the non-X3D cores. So unless the scheduling is unable to pic the right core, from a hardware perspective the demands are already met.  And if the scheduling fails, maybe because it is not particularly CCD-aware, spreading the game across 2 CCDs is a risk.  The 9950X3D2 will probably be the fastest CPU money can buy, but the practical difference in consumer scenarios will likely be immeasurably small compared to the already available 9950X3D.",Neutral
AMD,https://m.media-amazon.com/images/M/MV5BMWUxYjE3ZjgtNzMwYS00NzMyLTg3MzMtN2FmMGE5ZDE3NGNmXkEyXkFqcGc@._V1_FMjpg_UY1928_.jpg,Neutral
AMD,and knuckles!!,Neutral
AMD,Featuring,Neutral
AMD,"Speaking of people who don't know anything, MLID",Negative
AMD,Already exists.,Neutral
AMD,"More cache on one CCD won't solve having to get updated/edited information from the other CCD - which is the main thing causing performance regressions going from 1 to 2 CCD's, not the lower cache.",Negative
AMD,> so no this won't help with that at all.  Did you read the article? They're also talking about a 9850x3d that clocks higher than a 9800x3d,Negative
AMD,the 3D cache helps massively in WoW when there are many people on the screen. Im talking like more than double framerate levels of help.,Positive
AMD,"Ahh okay, threadripper it is.",Positive
AMD,they would probably use the higher clocking bins for the 9850X3D and the slightly slower ones on the 9950X3D2.,Neutral
AMD,"The 9950X3D is good for some one who likes to game, but also does a lot of workload heavy data processing. It's basically a good balance CPU for someone who doesn't want two computers but wants decent performance for both tasks.   I'm not sure the 3D2 will net much benefit though as at some point you get to the declining returns where you would just be better off having two separate computers. It really will just come down to how they price this relative to the base X3D",Positive
AMD,"for sure, first 2-3 months will be the majority of sales unless it turns out to be uniquely good at some tasks. then good sales could extend to 4-5 months.",Positive
AMD,"14900ks uses above 253w too, like I saw it reach 300W+ under overclocking. AMD has PBO(overclocking) turned on by default so those 230W are expected",Neutral
AMD,200W TDP on AMD has a 240W PPT IIRC. So ever so slightly less than a 14900K,Neutral
AMD,Both AMD and intel really mean the same thing with TDP. It's the power under which they guarantee the chip can do at least base clock speed in heavy all core workload. Meaning in effect the minimum amount of constant cooling power required to operate at minimum specified performance. They just have different boost algorithms.  Edit: for both it is a completely arbitrary number they have just picked. The other values are then tuned accordingly.,Neutral
AMD,What you described is not a thing afaik.,Negative
AMD,"Those have 170W TDP, 230W PPT (which is the actual maximum power). If the X3D2 has 200W TDP, it may have 270W PPT (if the old 1.35x ratio between TPD and PPT is kept?).",Neutral
AMD,"guilty of all of the above.   and the cache on the cpu is 19x larger than the first 5 1/4"" half height drive I sold.",Neutral
AMD,that's what she said,Neutral
AMD,"Sir, that is a dinner plate 😂",Neutral
AMD,8 inch floppy (1MB !) with Dungeon on it (later known as Zork).,Neutral
AMD,"Grandpa, is that you?",Neutral
AMD,show off.,Neutral
AMD,"Pepperridge Farm remembers when my dad ranted about my school requiring students to bring USB flashdrives (back when they came in 16MB-64MB capacities) when a box of floppy discs were ""good enough"" (and cheaper).",Neutral
AMD,"So true. Spoiled. I remember I used to save an extra copy on a spare floppy, just in case",Neutral
AMD,The year is 4025. Humans have achieved harmony and begun the conquest of space. The save icon is still a floppy disk.,Neutral
AMD,what’s the save icon everything is automatically saved nowadays,Neutral
AMD,"You boomers aren't special. I know what an 8 track is, what a VHS is, what a floppy disk is, I know what dos is, I know y'all had to copy code out of books to play pong. You aren't smarter, just older. I stg millennials think zoomers are dumb, even after complaining 247 that boomers did the same thing to them.",Negative
AMD,"You want a fucking medal for knowing what a floppy drive is? I hate this smug mockery of younger people, as if anyone under the age of 25 is a drooling idiot. No man, they've just never had to use a floppy drive because they hadn't been born yet when USB was invented and floppy drives became obsolete. Yes a lot of people born in the 21st century don't know what a floppy drive is, or how to use a rotary phone. Why the fuck would they need to? Do you (or most people your age) know how to drive a Model T Ford, or how to operate a WW2 era tube radio? Probably not. Turns out generally people are not familiar with tech that had been made obsolete by the time of their birth.  Acting like someone born in 2005 is stupid for not knowing what a floppy drive is, is like acting like someone born in the 1930s is stupid for not knowing how to use a smartphone.",Negative
AMD,"> Oh yeah, and the hard disk isn't really hard on the inside.  Of course the disks in hard disks are hard, open one up and look inside if you don't believe me.  Edit: Opening up a hard disk means you can NEVER use it again, unless you do it in a clean room and really know what you are doing.",Neutral
AMD,"I had that, and NO hard drive.  =D",Neutral
AMD,The term is Experienced,Neutral
AMD,RLL is not a compression.,Neutral
AMD,It's not for saying damn. It's for picking the comment to say damn to. :),Neutral
AMD,What is even the point then. 90 percent of their volume sold at 40 percent while the small fry pay out the ass to ensure they ll never be able to grow or compete against the big fish?,Negative
AMD,Is the reason for this to juice the ASP's for financials and then just put the discount as CoGS to make the accounting look better for shareholders?,Neutral
AMD,200 milliwatts is a damn efficient CPU.,Positive
AMD,"Got myself a 5090 to work on the vector extensions, coming from an A4000 it was saving me about 3 hours a day in processing time. It paid for itself in under a month.",Positive
AMD,NUDE TAYNE,Neutral
AMD,What about Kanye?,Neutral
AMD,"Yea it wouldn't be a bad idea for AMD to test dual CCD X3D out, have people get it in their hands and see how it benefits gaming, AI or productivity tasks. Then see if they do the same for their next products.",Neutral
AMD,Time to stick it in an a series board!,Neutral
AMD,"Motherboard resiliency has been tested by Intel since 9th Gen 14nm+++, so no worries there lol.",Neutral
AMD,I think this is more to see how it works if they put Vcache on all the CCDs for future Epyc SKUs.,Neutral
AMD,> and motherboard resilience at 200W out in the wild.  Oh the ASRock boards can certainly deliver more than 200W. Just don't ask at what voltages.,Neutral
AMD,Would it not start reaching DRAM levels of latency once we enter several hundred MBs?,Neutral
AMD,Even for fluid simulations it's not a huge deal because you're still stuck with two channels of RAM to even fill the cache.  Epyc has 12!! channels for reference,Neutral
AMD,"Right, I thought the V-Cache dies are made mainly for server chips and the rejects get repurposed as Ryzen. So this is basically a mini Epyc/Threadripper CPU (at least in some regards--not as many memory channels or PCIe lanes).",Neutral
AMD,indeed. I do a compromise and use 7800x3D and the VCache is very useful for sims. you can see the sim performance drop off a cliff when sim size exeeds cache.,Neutral
AMD,"Maybe? The performance gains were honestly way more than expected from a CPU, I would have assumed they would be mostly all gpu bound and there wouldn't be such a huge change. It's a big change from normal cpus so I wouldn't assume that it couldn't improve. It was honestly pretty stunning performance results.",Positive
AMD,"That's well within the margin of error. When they do test rig benchmarks they have exactly every variable the same, but even a single component or having a room that is colder could easily make a 2% difference. That's not even counting whether every graphic setting is the same as what you're using.",Neutral
AMD,"Sure, scheduling \_could\_ usually fix the issue here, but some games also use more than 8c/16t and spill over anyway.  One could manually force those apps to use a subset of CPU.  Or you can not worry about any of that and just have it work without any fuss.  In particular, if your background task also likes cache, this is a win.   Additionally, it should be faster overall than a normal 9950X3D if you run power limited to say, 150W, since the 3D cache chips tend to be quite a bit more power efficient below 80W per CCD.",Neutral
AMD,"Gaming is likely the worst possible example of how this addition can be taken advantage of, simply because of gamers' habits:  - Gaming is typically done on Windows, an OS with task scheduling so silly, it's still a pain in the ass to use long existing server CPUs with it exposing many cores. Cache-aware scheduling is experimental even on Linux, Windows just won't catch up any soon.  - Games are incredibly sloppily written, and typically only optimized until the performance is considered good enough on the most common setups. Windows doesn't even offer tools to mitigate bad strategies like creating tons of threads beyond the point of reasonable scaling, unlike tools offered by Linux like a cgroup with only the X3D CCD being visible, or similarly Wine being instructed to limit the visibility of cores not desired to be used. Letting the game see 32 hardware threads, resulting in multiple layers of abstractions create 32 software threads each, and just confining them them to a single CCD with something like Process Lasso just doesn't compare.  - Even with a good scheduler, what's the point of having 2 higher performance CCDs for the use case of one performance sensitive task with many background tasks the user doesn't even care about much? Typically there's only one game running the user is focusing on, and background bloatware will spill out of the extra cache anyway.",Negative
AMD,Who cares if it’s only 5% faster as long as it’s faster big woop,Neutral
AMD,"He sure seemed to know that this product was coming lol  But seriously, the fact that Sony’s upscaling solution is called “PlayStation Spectral Super Resolution” is enough to prove that MLID definitely knows something.",Positive
AMD,I turned the X3D upside down,Neutral
AMD,"The lower cache is why the requests have to be made frequently.  You have 96MB of L3 on one CCD, and 32MB on the other CCD.  If a thread on the latter needs data cached on the former, it will retrieve it over the IF link, which is slower than a local cached read, but still faster than a DRAM access (which is why the request is made in the first place).  That will then be the newest line of memory, being stored in the L1/L2 of the core that requested it, kicking out the oldest line into the L3, which in turn kicks out the oldest L3 line.  When that latter evicted line is needed again, it will have to be refetched.  It's still likely in the L3 of the first CCD, due to that being triple the size, so it will be requested over the IF link again.    But if both cache sizes are the same, then that scenario will happen much less often.  Both CCD's will end up having mostly the same cache lines stored locally, and only if the data is invalidated (which won't happen to most of the read-only memory involved with running a game) would there need to be a cross-CCD transfer for the same cache line.  There's still room for particular code to not run all that well in that scenario, but it's likely uncommon.  Worst case scenario, you just relegate such a game to a single CCD using *Process Lasso* or some other means of managing process affinity on the fly.  All of this is obvious when you compare the performance of non-X3D 16-core parts to the equivalent 8-core part.  In nearly all games, there's no real difference.  Only occasionally does the performance suffer on the dual-CCD part.  It's in those scenarios where a dual-X3D processor might not do as well, though there are certainly other variables that affect the outcome.",Neutral
AMD,"This is Reddit. We don't do that here, though in hindsight it is a bit embarrassing when the article is so short...   It's a good point and I guess it will perform a bit better in WoW, but it won't be the difference maker between running WoW at an awful performance level and a completely acceptable one.",Negative
AMD,"If your other tasks also like the cache, it makes sense.  Or if you want to lower the power down to 150W for all-thread tasks, 3D2 should start consistently beating the 9950X3D since the latter won't have a clock speed advantage and the former will keep the cache advantage",Neutral
AMD,Huh. Interesting idea. But there's diminishing returns for larger caches. The first additional cache tripled the size. The second wouldn't even double it. Wouldn't expect too much improvement.,Neutral
AMD,"I was strictly talking about the actual listed numbers. AMD will also go above if you let it.  The 230W has nothing to do with PBO. Its the default PPT set for 170W TDP parts, they \*will\* draw 230W when maxed out under default settings. (I'm also fairly sure most boards don't ship with a modified PBO enabled anyway.)  AMD's listed TDP is a calculation based on various factors. Intel's listed TDP is just their base power limit (which as you've noted they don't tend to stick to). The numbers are not comparable because they're based on different things and in AMD's case is almost entirely unrelated to power draw.",Neutral
AMD,"my 12900KS has also done that but mainly if I am running CB23, under normally gaming load it stays in the range of 70-110W, I am curious on 14900KS power draw just gaming",Neutral
AMD,"Is it defined anywhere? So far there has been no 200W TDP part.  If PPT is ""just"" 240 W, then it uses a smaller ratio to TDP than the other TDP grades (it's usually around 1.35x).",Neutral
AMD,"Intel's TDP is just base power since 12th gen. Its not arbitrary. AMD's TDP is a (relatively complex) formula, effectively detached from either its base or boost power draws, it also is not impacted by their boost algorithm. They're not comparable numbers. (Edit: Also nothing is tuned based off TDP for either company.)  For this discussion specifically PPT is the number to care about on AMD's side. For Intel its just the boost power that was brought up in the other comment. (Outside of overclocking where none of these numbers matter.)",Neutral
AMD,"Interesting. On my MSI motherboard, both models of CPUs top out at 200W without enabling PBO.",Positive
AMD,"Often you didn't need to buy floppy discs because AOL kept sending them out for free, just had to reformat them.",Neutral
AMD,Conquest of space coming much sooner I feel,Neutral
AMD,"Don't be a jerk and act like that is some boomer quote (boomers are 60 to 80 now).  Gen X (46 to 59) and older Millennials (\~36 to 45), not just boomers, might think the same about the young today, especially since it is predominantly this cohort currently with kids age 10 to 20 that know that their own kids have no clue.   I can guarantee that most (well over half) of kids in high school have no idea what a floppy disk is outside of maybe a documentary or museum.  Sure, they might recognize the icon and remember someone told them it is a floppy disk, and maybe some even have seen a 3.25"" one from \~ 20 years ago since those aren't all that old.  But knowing what something is and knowing how to use it aren't the same thing.  It doesn't make someone ""better"" or ""special"" if they used those things, I don't know where you got that idea, it just means that tech is changing rapidly.  I know what a Model T was, but I would be an idiot if I claimed to be as knowledgeable as my great-grandparents who drove across the country in one.  And on the contrary to your insinuation that boomers are somehow the tech elitists here, some older boomers are borderline tech illiterate, though by now the vast majority are fine and some obviously have been using PCs for 50 years. It was the generation before that that was often almost inept with tech (though my 92 year old Grandma is totally tech fluent, my grandfather on the other side couldn't figure out how to use a mouse and keyboard to write a letter, got frustrated, and pulled out the typewriter, and had trouble with all but the most simple tasks on his phone so we had to buy one of those old people flip-phones with big buttons).   Gen X was probably the first ""whole generation"" exposed to digital technology from an early enough age to expect everyone in that cohort to recognize all these things not just from references, but from likely using them at least once and being fully tech exposed and literate.   Boomers have largely adapted of course, with some deep in tech themselves, but others do basically only email and facebook and some can barely type.",Neutral
AMD,Imagine talking about intelligence and getting boomers wrong,Negative
AMD,A colleague once brought a rotary phone to my work and asked juniors to dial a number (no Google). They started pushing the painted digits with the tip of the finger 🤦🏻‍♂️,Neutral
AMD,"Damn, who hurt you?",Negative
AMD,"This, everyone, is a great example of being insecure",Negative
AMD,"People born in 1930s that are still alive have figured out how to use smartphones, actually.",Neutral
AMD,"Sorry, I meant the hard-shell 3.5-inch floppy disks.  I can see how this was really bad phrasing on my part.",Negative
AMD,Amstrad PC1512?,Neutral
AMD,TIL   https://dfarq.homeip.net/mfm-vs-rll-hard-drives/,Neutral
AMD,"I am still using reddit. This addiction sucks. If only I could find a way to pause or continue my reddit login streak, so I can leave reddit for the most part....",Negative
AMD,"Welcome to the corporatocracy. Where you'll never have a seat at the table, but you can't live without us anyway.",Negative
AMD,"The system integrators like it because they can compare their bottom line price to ridiculous list prices. AMD doesn't want to be in the business of selling individual server CPUs to consumers, so they don't care if it is inconvenient for you.",Negative
AMD,"Literally nobody pays MSRP, except maybe on launch day. Its a fake price. 40% off is often the base discount. There's many reasons for it that is advantageous to everyone involved.",Negative
AMD,"Pretty much all B2B works like this.  Basically if the transaction is large enough, both sides will have people dedicating to negotiating.  The list price is just the opening point of negotiation from the supplier's side.",Neutral
AMD,"Yes sir the engine programmers I know from Epic get those crazy 5000 dollar AMD Thread Ripper procs along with the what ever is the latest Nvida card, 15k for a desktop is too much for me but they make that back much faster then I can.   Only other guy I know with crazy home hardware renders commercials for broadcast tv on his home network, and he uses GPU accelerated rendering so he stacks as many cards as he can get into a box and his home power will allow for.   We will never have enough power because as soon as you get it, it is already spent.",Neutral
AMD,This is already the case.,Neutral
AMD,Unlikely as the delay is fairly minor due to much much shorter distance to logic compared to DRAM.,Neutral
AMD,"it'll depend on implementation but going up 3x to 96 MB l3 cache on the 9800x3D barely made a dent in latency.      I can imagine a generation or 3 from now where there's 192-256ish MB of L3 cache and it still being ""fine"" if they do something along the lines of die stacking.      With DRAM, HBM doesn't have meaningfully worse properties in terms of latency vs a single DDR die.",Neutral
AMD,"\> rejects get repurposed as Ryzen  No it is not rejects, the binning is in parallel.  For example, no Ryzen has only 4 cores per CCD or half the L3 disabled, but some server models do.  So many things that would be rejects for Ryzen can be put in some servers.  Ryzen wants things that clock higher, but doesn't care as much if the idle or low clock power usage is higher.  So dies that fail the server spec for power efficiency but are great for high clocks can be Ryzens (or a few high clock high power low cpu count server models).  But those that don't clock well but  are very efficient power wise can be high  core count Epyc CPUs which have low max clocks and are expected to spend a lot of time at low clocks and minimal power per core.   (Edit: yes, power efficiency is typically correlated somewhat with max clock speeds, but outliers exist)",Neutral
AMD,"My graphic settings were on max, it did not saturate my 4070S doing so, i was locked at 144 because thats my monitor refresh rate. I did test unlocked and got over 200, but that was in an underground section.",Neutral
AMD,"You generally don’t want games spilling over to more than one CCD. At least from a frame time stability perspective (which IMHO is the most noticeable), you’ll do much better process-lasso’ing games to just one CCD, even if you have two 3D cache CCDs.",Neutral
AMD,"This (adding 3D cache to both dies) has been speculated about since the launch of the 7950x3D.    MLID is one of the most known ""leakers"", so real leaks will find their way to him. He also has a history of being wrong and covering that up.   I don't think there is anything wrong with listening to the podcast, so long as you keep in mind that it's entertainment, not news.",Neutral
AMD,Why not ХЗD ?   (X and 3 are actualy the cyrrilic h and z),Neutral
AMD,"Where is it written that one CCD can access another's L3 cache? AFAIK only IBM does that on some fat POWER processors.  Edit: I think I understand what you mean, but that won't be solved by having a larger cache on the second CCD. If the data needed by a thread on the second CCD is already in L3 on the first it probably means that a thread on the first is also using that data. You'll still be playing ping-pong between two caches just like /u/dabias said.",Neutral
AMD,"When S MESI states start appearing it's going to be a lot slower than with just E if any core tries to write the line, that's the problem with multi ccd cpus, just giving one ccd more cache doesn't solve this, much faster interconnects are needed.",Negative
AMD,"> If a thread on the latter needs data cached on the former, it will retrieve it over the IF link, which is slower than a local cached read, but still faster than a DRAM access (which is why the request is made in the first place)  AMD claimed, with Zen 2 at least, that getting data from the other CCD is similar to latency from memory. Looking at some results, the difference is \~20 cycles, which is marginal when we realize the comparison is between 200 and 220 cycles of latency.",Neutral
AMD,"I know, I think someone (Steve; YouTube) pushed the CPU to GPU power draw levels...... But with liquid nitrogen.....",Neutral
AMD,"Because you use it normally and not pushing it to it's limits man. Without PBO, AMD also is low consumption.   All CPUs will be low in power usage till you don't push them to their limits. Like for my laptop, max tsp of CPU is 54W(45W + 9W PBO boost), but it idles around 7-10W while watching videos (bluetooth tws), but goes only up to 34W under heavy load, using the best cooling pad and thermal pad possible. Heck it does not even overheat. And I have turned off the overclock. That's 20W(37%) less than max and performs on par with passmark average for my cpu. (went from 15% below to 5% above by changing thermal paste and going dual channel 32gb ram without cooling pad).   And the performance I have calculated is just leaving about 10% on the table when going from 34W to 54W when compared to other better laptops, so not worth it.",Neutral
AMD,Not defined anywhere from what i saw but a general rule of thumb seems to be add about 20%,Neutral
AMD,"> Its not arbitrary.  It is literally arbitrary. It's just a value somebody picks. AMD's too. It's not computed from any formula, it's a value somebody picks. The formula you might be referring to does not determine TDP. It just shows how it relates to some cooling requirements.",Neutral
AMD,"Yeah but that can be because they exhaust the clocks the SMU allows them, or hit temperature thresholds that are set in, or current limits, before they bump into the wattage limit.  People at one point kept saying the PPT is actually 200 W, but so far I have seen no sign that it was confirmed and not just an assumption base on observation that the multithreadpower usage rarely goes over that number. But that can just be due to the tiny CCDs being hard to cool.",Neutral
AMD,Don't forget to put a little piece of cardboard into the hole!,Negative
AMD,I knew more about PCs when i was 9 than many of my coworkers know and they are adults. The young generation is fucked. Reminds me of that Asimov short story where robots automated everything to the point where people didnt bother learning how robots work or even how to read and write because robots spoke to you. This resulted in goverment seeking out anyone with curiuosity to teach them because someone has to program the robots.,Negative
AMD,"I never understood this snide mockery. What reason why they have had to learn how technology they would realistically never use? If you didn't live when the CD walkman was the current tech, why would you have a reason to know about walking too fast and the tracks skipping? If you weren't alive for the PCs that had the floppies that were actually floppy, what reason would they have to be there for them to learn about its ins and outs?   If you genuinely thing it's such a massive issue as to mock them, take the time to tell them...to teach them. You know how people learn? Certainly not being told they are such imbeciles because they don't know something they never encountered. Just like you didn't and had to learn. Grow up.",Negative
AMD,"When I was about 13 (so 2013, I was born in 2000) I was at the train station and I needed to call my mum to come pick me up, but my iPhone 4 battery had died. I went into the barbershop across the road, and asked if I could use their phone. The barber said sure, and pointed me to a rotary phone. I asked how to use it, as I'd never actually seen a rotary phone in real life before. The barber launched into a raised voice tirade about how young people nowadays are all stupid, and they don't know how to do anything for themselves, and they'll never amount to anything, and they want everything to be spoonfed to them.  That's one example, but I have had many many experiences being lectured for being ""stupid"" because I didn't understand tech that became obsolete far before I was born.",Negative
AMD,"No problem, and I could have intuited that you were still speaking about floppies.",Neutral
AMD,"No, it was a Vendex 8088 turbo XT.  8 mhz was ""turbo"" mode.  Like 512k ram.  4-color CGA graphics.",Neutral
AMD,"Perhaps you mixed it up with RLE, or run length encoding, which is a primitive form of data compression?",Neutral
AMD,Damn,Negative
AMD,"Latency from one CCD to the other CCD's L3 is somewhat better than DRAM, but not actually that much better.   However, latency under load is a more significant improvement.    In either case, the 'cacheable size' is closer to the total cahce of a single CCD than the combined number from the point of view of any single application thread.   However, if different threads are doing vastly different things (common for servers, not as true for games) the many caches across CCDs is more helpful.",Neutral
AMD,"Usually bigger cache means, larger distances and it also adds more ways and sets in the cache’s lookup table. Moving from a 32MB to 64MB cache atleast on zen3 added about 10ns on average. Eventually there has to be a limit where the increase will not help at all...maybe",Neutral
AMD,"No, SRAM is a completely different beast to DRAM for starters. Also, increased capacity makes no difference to DRAM.",Neutral
AMD,Also a good suggestion,Positive
AMD,"As I stated, most data used by something like a game is read-only.  So long as the local cache line is not invalidated by an update, there's no reason to go back the other CCD.  And again, if this were genuinely an issue then you'd see a regression in performance on 16-core parts over 8-core parts when no special care is taken to ensure the game runs on only one CCD.  That happens with a very small minority of games.",Neutral
AMD,"And what about when there are a lot of threads requesting data at the same time?  How does that look comparing DRAM access versus cache access on the other CCD, without going with the worst case scenario to compare to?  When you look at local L3 access latency by itself, it's not drastically lower than DRAM access latency.  But access latency is only part of the picture.  SRAM is not only faster to access, but much, much faster to read.",Neutral
AMD,"No I know AMD is also low consumption while using it normally. I was just curious on the actual average power draw of something like a 14900KS! The 12900KS is going to be the chip I use for a long time, because I like to use my chips and not have them fail on me how my old 13700KF did xD.",Neutral
AMD,"On 3.5"" floppies it was the other way around from the older floppies.    The 3.5"" no hole was write protected and the AOL disk didn't have a hole.    But you could take a soldering iron and burn one into it to make it writeable.",Neutral
AMD,"Exactly. Had I never seen a rotary phone used in old films/shows, I’d probably have no clue how one worked on first attempt.  With that said, I do find technical knowledge among those that grew up with polished smartphone apps to be a comparatively lacking. Technology used to take a lot of trial and error that taught a lot of lessons.",Negative
AMD,"It says something about you that you automatically defaulted to public ""mocking"". In my case, it was a game: the first one to discover how to dial a given number correctly gets to choose Friday's pizza. We even used the famous number 0118 999 881 999 119 7253  Do better.",Neutral
AMD,"people with basic level of critical thinking can quickly learn to use new technology if necessary. Something like a walkman would be quite self evident after a proper inspection even if you never saw one (i never saw one, they didnt exist in my country).",Neutral
AMD,I am still here 😭  Too addicted,Neutral
AMD,"I see! Yeah but I don't think the latency would be linear (as the distance to access the furtherest sram block shouldn't be increasing linearly, more likely logrithmic). Vertically stacking the SRAM should add minimal distance as the additional wire traveled should just be the layer height",Neutral
AMD,The big issue is that the larger cache means more coherency related traffic which over AMD means even more traffic over the already relatively bottlenecked infinity fabric.,Negative
AMD,This is often the cited reason for why L4 larger cache wouldnt be beneficial.,Negative
AMD,And the problem with DRAM is that the latency hasn't improved for years if not decades.,Negative
AMD,"> That happens with a very small minority of games.  Interesting, it appeared to be a larger issue. Do you have a link to some benchmarks? Now I'm curious to see more.",Positive
AMD,">And what about when there are a lot of threads requesting data at the same time? How does that look comparing DRAM access versus cache access on the other CCD, without going with the worst case scenario to compare to?  Profiling gaming on Zen 5, chips and cheese:   >Most L3 misses head to DRAM. Cross-CCX transfers account for a negligible portion of L3 miss traffic: sampled latency events at the L3 indicate cross-CCX latency is similar to or slightly better than DRAM latency. Therefore cross-CCX transfers are quite performant considering their rarity.  As for this,   >When you look at local L3 access latency by itself, it's not drastically lower than DRAM access latency. But access latency is only part of the picture. SRAM is not only faster to access, but much, much faster to read.  Seems to be included in the latency performance monitoring itself, no?",Neutral
AMD,https://www.computerbase.de/artikel/prozessoren/intel-core-i9-14900ks-test.87292/seite-2#abschnitt_leistungsaufnahme_in_spielen_rekordverdaechtig,Neutral
AMD,"The actual difference is I believe, 2 clock cycles extra, with the 3DCache versus normal.  This is under 1ns at typical clocks.",Neutral
AMD,Oh nm. I assumed a linear increase just very small increments. Didn't think about how vertical stacking can help.,Neutral
AMD,depends on the implementation.  eDRAM made broadwell often faster than skylake up until skylake got faster DRAM. Some of this was bandwidth.  3D vcache solves a lot of issues related to trace length though... just go UP (i.e. stack 2 or 16 layers of vcache on top of each other instead of just 1 similar to what's done with HBM for dRAM).,Neutral
AMD,"Capacitors charge at a finite rate, so I'm not sure it can be worked around. May need a different memory tech.",Neutral
AMD,"Just do a search for benchmarks that include a 5950X along with a 5800X, or later equivalent matchups.  More often than not, the 16-core part is faster because it has better bins and higher frequency.  There are a few more titles that do worse with a 7950X3D versus a 7800X3D, simply because the game gets scheduled on the CCD without V-cache, but that's not the same comparison.  In those examples, the 7950X3D is still faster than the 7950X.  It would just work better with proper scheduling (which can be done with *Process Lasso* or some other affinity controlling software).  Having V-cache on both CCD's would make that latter step unnecessary, or at the very least make the difference minimal.",Neutral
AMD,The 320W overclocking CPU..... Truly nuts for consumer grade hardware.. I do not think even your average water cooler is sufficient enough for that.,Negative
AMD,"But you are correct in pointing out the increase logic/multiplexing needed to address additional memory, but the increase in latency should also be minimal and not linear with that.",Neutral
AMD,"Yes, but broadwell cache speeds could get away with being slower. Not the case now. 3D solving trace length is why it worked so well, you did not end up increasing latency  much because it was just on top (7000 series) or bellow (9000 series).",Neutral
AMD,Thanks,Positive
AMD,Broadwell's cache was in large part meant to relieve bandwidth pressure on the DRAM for the iGPU. Cut the number of requests to DRAM by half and good things happen.     eDRAM is on the order of 10-100x higher latency than sRAM.   It's a different tech with the primary benefit being that it's cheaper.   It's less about capacity and more about cell structure.,Positive
AMD,"""tough guy, soft graphics card"" - Yeston",Neutral
AMD,I was surprised how much more manual of a process the making and painting of the backplate versus the mostly automated making the GPU board versus mostly manual final assembly.,Neutral
AMD,How it's made. Waifu Edition.       I always enjoy this type of content.,Neutral
AMD,"Yeston's the GOAT.  Also, the scent module they use for their GPU seems to last half a year after installation.",Neutral
AMD,Omg he finally got to visit the waifu mothership,Neutral
AMD,Need to watch this before copyright strike,Neutral
AMD,Waifu factory is  running on Windows 7.,Neutral
AMD,it didn't look as claustrophobic as the videos we've seen on Foxconn as well.  Maybe Foxconn wasn't representative of China's tech manufacturing but rather an exception.,Neutral
AMD,"Steve knows it, that's why they stayed one month in Asia, to do many deep dives as they could",Neutral
AMD,They could have sold replaceable Yeston-brand scent packs for that recurring revenue stream.,Neutral
AMD,*Bloomberg has claimed copyright ownership over Yeston waifus*,Neutral
AMD,"If not connected to the internet then you can actually run older Window versions without any problems. Some PCs are only used to control certain machines, the same for labs.",Neutral
AMD,"Those Blender results are confusing.  Did they leave it on CPU rendering?  Did they use HIP-RT, Optix?  There's no way the R9700 is putting out the same numbers as the 5090.  The Blender benchmark results list an average score for R9700 at \~3k, but \~15k for the 5090. Something is way off here.",Negative
AMD,"The partnership is actually about *two* supercomputers, of which one will come online rather shortly.  Article is soft-paywalled, so …  The first is the **LUX**-Supercomputer of AMD, co-developed by Hewlett Packard Enterprise (HPE) and Oracle, to be likely located at the *Oak Ridge National Laboratory* (ORNL);  * Based upon likely some yet unknown AMD Eypc and their Radeon Instinct MI355X-accelerators, including AMD's self-sourced network-hardware (Pesanto/Xillinx?), which is a first here and a major blow to HPE, for AMD sourcing their own network-stuff (cf. nVidia and Mellanox).  * Scheduled to come online in about 6 months (sic!), so they've already been working on it.  **Edit:** The second ~~yet unnamed~~ AMD-supercomputer ***DISCOVERY*** is also co-developed with Hewlett Packard Enterprise (HPE) and the Oak Ridge National Laboratory, yet not Oracle, to be housed there likely as well;  * Also fully AMD-based upon ~~likely some yet unknown~~ *upcoming* AMD Eypc *Venice* and a custom-tailored version of Radeon Instinct MI430*X*-accelerators for the DoE — ~~The article does neither state the second system to also have AMD-sourced network-hardware (Pensando/Xilinx) or if HPE covers for that (Slingshot?)~~. The respective [NextPlatform](https://www.nextplatform.com/2025/10/27/oak-ridge-discovery-supercomputer-spearheads-new-hpe-cray-gx5000-design/)-article states, #2 will be using HPE's next-generation SlingShot-network (Thx to u/wideruled for the note here!).  * Scheduled to be installation-completed by 2028 and to come online in 2029.  Original-source is the Reuters.com-article. [Archive.is-Link](https://archive.is/deJ0s).  ---- The major Oracle-deal of AMD recently was likely having something to do with the first LUX …",Neutral
AMD,"Also, depending on how massive and performant AMD's own network-equipment turns out, this will enable AMD to basically fully source a supercomputer *on their own* from top to bottom, especially with their recent acquisition of server-hardware vendor *ZT&nbsp;Systems* — AMD to become a full-grown HPC-vendor with Epyc-CPUs, Radeon Instinct-accelerators, their Pensando/Xilinx (Thx /u/uncertainlyso) network-hardware and the server-housing rack-hardware also sourced in-house by their server-hardware specialist ZT&nbsp;Systems.  To be a one-stop shop for supercomputers, that's something neither Intel can offer (no existing AI-/HPC-accelerators nor real datacenter-networks and neither anything supercomputer-class on graphics) nor nVidia can (yet; Only ARM-based using Grace and mighty Mellanox-network).  This is a massive and very business-competence instilling move by AMD's leadership here.",Neutral
AMD,"Very interesting cost and work share   > **The Department of Energy will host the computers, the companies will provide the machines and capital spending, and both sides will share the computing power, a DOE official said.**  > The two supercomputers based on AMD chips are intended to be the first of many of these types of partnerships with private industry and DOE labs across the country, the official said.",Positive
AMD,How will DoE provide any funding legally when the government is not funded?,Neutral
AMD,More details here:  Looks like Discovery will be using the next generation of slingshot:  https://www.nextplatform.com/2025/10/27/oak-ridge-discovery-supercomputer-spearheads-new-hpe-cray-gx5000-design/,Neutral
AMD,"Is AMD not missing the interconnect switches needed for an all-AMD supercomputer? Pensando/Xilinx is just the NIC and DPU, correct? Nvidia has complete interconnect solutions for both Infiniband and Ethernet. Nvidia does the DPUs, NICs, switch ASICs (and the switches themselves), plus the major components of the physical interfaces such as optical modules.",Neutral
AMD,"Yup, though many critics rightfully argue here, that it's The Bold Orange forcing companies to do as the government pleases, yet I think it's more of the same old route of the state outsourcing ever so more to businesses.  I have no issue with that view at all, and it kind of bears some truth to it here, yet most definitely, the administrative and costy side of things with this, are still to be worked out for both sides.  Though I think it seems it instead signals a new era of the government no longer want to bear any greater responsibility for anything (even their core-responsibilities) as it already showed through-out all of the 20th century.  So instead business are contracted, to do it and maintain what the U.S. government then contracts to use, by basically asking to erect governmental IaaS (Infrastructure-as-a-Service), as the NextPlatform-article mentions.",Neutral
AMD,These are 90% used for the nuclear stockpile stewardship. Funding won't ever be an issue.,Negative
AMD,"Thank you for this! I was already trying to figure what kind of network-components later #2 *Discovery* was supposed to be using, since the article was very vague about it being AMD-sourced or classically HPE's Slingshot.",Neutral
AMD,"Well, let's see how it turns out, if it's said that network will be fully AMD-sourced, right?  It's not that AMD hasn't sneakily acquired a bunch of key-firms over the years since their Ryzen in 2017.   Also, many easily forget or never even knew that, but back in the days, AMD had their networking-stuff like NICs too.  The famously compatible, inexpensive and fairly widespread line of their *AMD PCnet* I/II/Fast-NICs.  And their [AMD LANCE Am79xx](https://en.wikipedia.org/wiki/AMD_LANCE_Am7990)-based network-cards were quite ubiquitous and had good support back then.  So if anything, AMD for sure knows pretty well how to engineer networking-stuff … That's actually unlike Intel, who just overtook DEC's line of NICs, only to run it into the ground with broken silicon since.",Neutral
AMD,"It sounds like Pollara is able to get some/most of the advantages of Ultra Ethernet (like ECMP spraying, receiver credit-based congestion control) over existing Ethernet switches, as long as you have it on both ends. It remains to be seen how well it performs, how much is left on the table with UEC switches, and then how much would network collectives help beyond that. Nvidia has had all of these in NVLink and IB for a while.",Neutral
AMD,"It's based on Ethernet, so it should be compatible with most standard switches. But maybe they are also building their own switches.  Sounds to me like Infinity Fabric over Ethernet, based on that:  * https://www.amd.com/en/products/network-interface-cards/pensando.htm * https://www.amd.com/content/dam/amd/en/documents/pensando-technical-docs/article/amd-ai-networking-direction-and-strategy.pdf",Neutral
AMD,"Does one get all the features of Ultra Ethernet when Ultra Ethernet NICs are used with existing standard Ethernet switches? Compatibility was not the point I was making; it was whether AMD could be said to have a full interconnect HW stack or not.  I was under the impression that Ultra Ethernet is a very comprehensive superset of Ethernet, and that one needed Ultra Ethernet switches if one wanted all the benefits of Ultra Ethernet such as In-Network Collectives.  I daresay that the norm for HPC interconnects today is that a single vendor more or less co-designs both the interface and the switch. This is the case with Infiniband, Slingshot, Omni-Path, and BXI (although in the current v3 version, Eviden heavily modified an existing Ethernet switch ASIC provided by a partner). AMD is an outlier in this respect.",Neutral
AMD,> so it should be compatible with most standard switches.  You need RDMA and that drops 85% of the options,Neutral
AMD,"4 e-cores take up as much space as 1 p-core, for multithreading they genuinely do provide better performance per mm square in many applications.  They also use less power than p-cores.  The goal hasn't changed from when they first arrived on the scene. Its all about chasing efficiency while keeping peak performance as high as possible in the applications that benefit from high single threaded performance.",Positive
AMD,"> Intel originally claimed E-cores would boost multi-threaded performance at low area cost without hurting single-thread (since P-cores take priority).   That's true, but it goes even further.    Heterogenous cores actually *increase* single threaded performance in the long term. That's because the die area penalty for making P cores bigger decreases when there's only 8 P cores instead of 16-24+ of them. And a bigger core is usually a more performant core.    This is probably why Intel is ok with their P cores being enormous compared to AMD.",Positive
AMD,"Heterogenous cores are really a byproduct of increasing core counts. PPA = Power, Performance, Area. The idea is to have different cores be optimized for different parts of this ratio.  If you have a physically large core that can provide very high performance at the expense of large area and high power consumption, than this design may not scale well with more cores.  If you have ""E"" cores that sacrifice maximum clockspeed for smaller area and lower power consumption, than you can often get better nT performance in the same area because in reality, once you're loading more than 6 or 8 cores, clockspeed delta between the P and E cores drops a lot.   For Intel's new P-E-LpE design, each is optimized to target a different part of the PPA.",Neutral
AMD,"If I got that right, many programs (or part of them) only use simple instructions, which is part of the reason why M chips are hot now. But using simple instructions in a complex core is more costly. Additionally, many computations are memory bound and not CPU bound, hence why we are seeing high performance CPUs with way more cache. So having lower capability and lower speed cores to do these tasks is fine because they can't be much faster anyway. And ideally these so-called E-cores are more efficient as well.",Neutral
AMD,"P cores are about 4x the size of an E core and even more in power. So while they are very good at the single thread part they are inefficient for dealing with highly parallel or concurrent algorithms where the mass of E cores gets more work done in the same area and power. Lots of E cores is better for more parallel work, but if you have too few P cores that can hurt performance due to [Amdahl's law](https://en.wikipedia.org/wiki/Amdahl's_law).  The thing about Amdahl's law is once you have enough parallel processors the task becomes dominated by the single thread part of the work, which is the setting it up and collecting the results and showing them to the user/storing them. So the ideal future involves the right amount of P cores to as many E cores as possible, but if you have too few P cores and that work ends up on E cores it will hurt performance. Too many P cores and you end up with less performance in the parallel part of the work.  Heterogenous cores is thus the obvious future as things become more concurrent and we use more parallel algorithms. Get the balance wrong and performance will suffer, we need just enough P cores to run the E cores. We also need the right thread scheduling to get this all correct and I think its going to fall on the programmers to hint where threads should go because its very hard for the OS to choose.  The final aspect is modern machines spend a lot of time idle, and idle work is rarely timely or needing a lot of performance. So there is an ultra low power low performance core need too that keeps the OS and notifications and apps ticking over and keeps power consumption really low.",Neutral
AMD,"Strix Point is not heterogenous. It uses density or power optimised cores of the same architecture, that being Zen 5 in the case of Strix Point. Intel and ARM use completely different architectures on their big and little cores, hence a heterogenous architecture.",Neutral
AMD,"I think that the exact motives for heterogeneous solutions vary from case to case.  E.g. if you look at AMD vs Intel, I think that Intel went with a heterogeneous design because they didn't have any other way to compete with AMD. AMD's Zen cores were already quite power efficient, while Intel was stuck with older designs for older nodes that were more power hungry, so they could not scale up the number of cores without making a power monster (plus it wouldn't fit on a reasonably priced die). Their solution became a hybrid design with a few powerful cores and a few less powerful cores.  In the case of Intel, it's quite clear that it was an emergency solution. They basically took two more or less readily available cores and put them on the same die. One quirk that came from that was that even if the bigger cores had some CPU extensions (some AVX instructions for instance), the smaller cores didn't. That meant that processes and threads could not be migrated from big cores to small cores without possibly crashing. IIRC the solution was simply to disable the advanced instructions in the big cores and have a more crippled CPU.  One observation that has been made in recent times is that in a multi-core CPU, you don't really need that many fast cores for your system to run fine. I think that Linus Torvalds expressed it something like this: ""The important thing is that the efficiency cores do not suck"" (i.e. that there's not an order of magnitude difference in performance, because then scheduling becomes really difficult).  For Apple I think they come from another reality, where their big-little design was first made for phones and tablets, where battery life is a key design goal. There it's important to have cores that draw little power to extend battery life as much as possible, and only use the big cores for interactive foreground processes for instance.",Neutral
AMD,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,"You basically said it, e Cores don't make much sense in a low power design.  More general, the are two aspects that a CPU architecture can be designed around: throughput and latency.   Throughput means how much work can you do in a given amount of time and latency focuses on how much time do you need to finish a task.   Even though they seem to be similar, they're not. Power consumption scales with the square of clock speed. Increasing clockspeed by 2x requires 4x power, so you either have 1 core twice as fast or 4 cores running at the same speed. That's extremely oversimplified but gives a rough estimation on why it's generally much easier to get good throughput over latency.  Apart from that there are interconnects. CPUs cores share memory and caches to some degree and in todays designs, moving data typically costs more energy than compute does.   It's a complicated topic and I strongly recommend this [Video](https://youtu.be/8teWvMXK99I?si=i4BrzCp6kjsZn8RW) by Dr.Cutress which goes over the intrerconnect basics. Long story short, the more cores/hops you add your design the worse latencies get.   And that's also a reason why intel uses e cores. Intel generally targets latency more than throughput while AMD typically focuses more on throughput (there are exceptions on both). But still, both try to reduce latency as much as possible, both designs have their advantages and drawbacks.  Intel uses a high speed ring to connect their cores. Problem is that adding more hops will increase latency. Fun fact if you look at more technical analyses of Alder Lake, you'll find that the 6 core dies usually come with better core to core, cache and memory latencies than their high end many core designs do. To enable higher core counts, which are mostly useful for throughput heavy tasks anyway, they group 4 e cores into one hop on the ring. That means they can add 4 cores that have more throughput than 1 big core while only increasing throughput as much as adding one more core which is a pretty good idea.   But all that is only really useful on higher end parts, for low core count or limited power scenarios it's not that useful. Even though their marketing did present it a little bit different the e cores were never really about efficiency in low power scenarios.  AMD solves this problem quite different: They only build 8 core ""cpus"" and ""glue"" them together on one package and connect them through a cache coherent interface. Topology wise they act more like older dual socket systems before the North bridge was integrated into the chip. The IOD is basically the north bridge and each CCD acts like a CPU socket would but all at much higher speeds and lower latencies. But there are still issues.   The cache is only coherent, not shared. If both CCDs work on the same memory, the data in stored in both caches as copies. That's also the reason why the Ryzen 9 CPUs have twice the cache over Ryzen 7 CPUs but don't see any benefit in gaming from that. If anything, the coherent caches hurts low latency applications like gaming.   AMD ""fixes"" that by disabling one CCD in those tasks. And that's also one the reasons why Zen 3 improved so much in gaming: they went from 4 core CCX to 8 core CCX which also doubled the effective cache. Instead of 2x 16MB it was 1x 32MB since then.  So, heterogenous designs are relevant for scaling performance on the desktop.   In mobile systems, it can also be used to reduce power draw but that's an entirely different thing.  ARMs big.LITTLE which is often falsely compared to intels design works entirely different from what intel and AMD are doing. The orginal big.LITTLE was more like power settings than anything else.     Edit: Technically every single SMT/hyperthreading CPU is heterogeneous. From the OS perspective SMT is much more heterogeneous and a much bigger issue than big.LITTLE ever was.",Neutral
AMD,"Computers are everywhere now from under your TV to on your desk, on your lap to inside your pocket. Embedded into your VR headset and so on. We are dealing with more form factors than ever before and its becoming nigh impossible to ship something that ticks all boxes so heterogeneous design is hitting the industry from every direction and the technology is often cross pollinating from one form of compute to another. This situation of different form factors eventually led to the big manufacturers developing high power and low power cores for different applications and at some point the two started making sense as a package and it just grew from there.",Neutral
AMD,"> 4 e-cores take up as much space as 1 p-core,  I had no idea it was that large of a delta.  do you have any sources?  Is that Intel specific ?",Neutral
AMD,>It's* all about,Neutral
AMD,Fwiw aren't the e cores about the same size as Ryzen big cores too? Intel needs to shrink stuff,Neutral
AMD,">Heterogenous cores actually *increase* single threaded performance in the long term. That's because the die area penalty for making P cores bigger decreases when there's only 8 P cores instead of 16-24+ of them.  You would still have to make them area efficient for P-core only chips like their server lineup. Though ig with chiplets even that is becoming less of an issue...   >This is probably why Intel is ok with their P cores being enormous compared to AMD.   The problem is that Intel's P-cores are not nearly performant enough for them to justify being as enormous compared to AMD's P-cores (judgement pending on LNC).   Plus, it seems like Intel is changing tact with unified core.",Neutral
AMD,"What’s the bare minimum number of P-cores needed for basic functioning? That way, you could trade some P-cores for more E-cores to gain better single-threaded performance (since each P-core could be made larger) without decreasing multi-threaded performance.  I’ve seen a wide range of Intel configurations — 2+8, 4+4, 4+8 — across the various ultrabooks I’ve used, while desktop chips go up to 8+16.",Neutral
AMD,"On the M series chips, the E cores and P cores have the same instruction set. They just are scaled differently. An app that can run on one can run on the other just fine.   Other companies have had mixed instruction set availability, but the nature of scheduling means that either an app is pinned to a specific cluster (which mitigates the benefits) or your capabilities are driven by the lowest common denominator",Neutral
AMD,"This doesn't make any sense to me.  The ARM instructions are the same no matter the core. Bigger cores use their resources to look farther and farther ahead to find instruction-level parallelism. As it turns out, there are diminishing returns for this, so an E-core with 1/4 the area can still get the majority of the IPC.  The cache has nothing to do with the instruction set and everything to do with counteracting the performance hit of sequential RAM speeds maxing out at ~400MHz years ago. Again though, it has diminishing returns, so having less cache affects performance, but having 1/4 the cache doesn't mean 1/4 the performance.  Maybe you are talking about pathologically serial pieces of code. This also doesn't generally apply. There are some edge cases, but there is almost always some amount of ILP that can be applied. Additionally, E-cores are generally half the clockspeed which means they would be half as fast at the same serial IPC limit.",Negative
AMD,But how does the thread director (As intel calls it)   schedule among the heterogeneous cores and decide which programs get P and who gets E?,Neutral
AMD,"> P cores are about 4x the size of an E core and even more in power. So while they are very good at the single thread part they are inefficient for dealing with highly parallel or concurrent algorithms where the mass of E cores gets more work done in the same area and power.  Actually, this is somewhat misleading. E-cores are more efficient when given a highly-parallelized load. However, an E-core cluster of 4 E-cores consumes more power than a single P-core. A 14400F (6P/4E) is relatively easy to cool compared to a 13900K/14900K (8P/16E). That's because the 3 extra E-core clusters, or +12 E-cores use a lot of power.  People were looking forward to the rumored LGA1700 12P/0E CPU because it would be ideal in many situations. It would also be easier to cool than the 8P/16E 13900K/14900K.",Neutral
AMD,"So skymont opens a new page? As it greatly close the P-E gap that Intel was having for 3 whole generations(Alder, Raptor, Meteor)",Positive
AMD,"PTL (and LNL) kind of address this issue a bit by introducing LP-E cores: E cores completely off the ring so the ring can be powered down.   While regular E cores are better for throughput, the LpE cores let the rest of the CPU powergate and run at very low wattages in light / idle scenarios.",Neutral
AMD,"It's worth mentioning that AMD's current paradigm also let's them essentially use the same design for desktops and servers, in terms of both the CCD itself & packaging toplogy. Ryzen is quite literally just a scaled down version of Epyc, so implentation costs aside, any non-platform specific gains/improvements made on Epyc is applicable to Ryzen and vice versa.  Whereas with Intel, even if the microarch itself is the same, there's very little overlap between their desktop & server designs. The cores' designs are fundamentally different because in the server dies, they're connected together using a mesh layout instead of using a ringbus. Much more complex & expensive than a ring, but let's them scale out to more than 8-10 cores per die and maintain a relatively consistent c2c latency, albeit the lower bounds are higher.",Neutral
AMD,">Intel generally targets latency more than throughput while AMD typically focuses more on throughput (there are exceptions on both).  I think AMD targets latency much more than Intel. Hence why they have smaller, faster core private caches for a while now, while Intel goes much higher capacity (and more levels) but also slower.   Same with their L3 design, AMD uses clustered rings (or with Zen 5, clustered meshes) while Intel has a larger, slower ring in DT/mobile, and a *much* larger mesh with a ""monolithic"" L3 in server.   >Intel uses a high speed ring to connect their cores.  It's been really slow recently. Since ICL IIRC.",Neutral
AMD,"Yeah, that makes a lot of sense. With Intel’s Skymont and Lion Cove cores in Lunar Lake, I’ve always wished they’d include a mode that completely shuts down or parks the P-cores for light workloads. The Skymonts are *seriously* efficient — a whole tier better for battery life — and for typical office tasks, the performance hit would be almost negligible.  Honestly, I think Intel should’ve gone with a 2P + 8E layout instead of the usual 4P + 4E on many past U15 chips. Most day-to-day workloads don’t need that many P-cores active, and reallocating that thermal/power budget to more efficient cores could have been a smart move.",Positive
AMD,There's plenty of die shots of Alder Lake and later. E-cores are tiny. It's probably not an exact 4 to 1 ratio but it looks close enough eyeballing it.,Neutral
AMD,Lion Cove v Skymont:   1:3  Source:   [https://www.guru3d.com/story/intel-arrow-lake-die-photo-breakdown-compute-io-soc-gpu-tiles/](https://www.guru3d.com/story/intel-arrow-lake-die-photo-breakdown-compute-io-soc-gpu-tiles/),Neutral
AMD,Check out High Yield's video: [Arrow Lake: Intel's 1st Tile-based Desktop CPU](https://youtu.be/wusyYscQi0o?si=HrxHqQ-OJKY3e081&t=672)  There are figures on the size of the cores but I can't recall (there's been discussions in this sub scattered throughout).,Neutral
AMD,"He's right.  It was a Intel themselves that said this but it was awhile ago.  ""The goal hasn't changed from when they first arrived on the scene. Its all about chasing efficiency while keeping peak performance as high as possible in the applications that benefit from high single threaded performance.""  Exactly and we can thank Apple for that. Their M-series chip didn't come out of left field, they straight up came from outer space and x86 and x86\_64 wasn't ready, from a power efficiency standpoint and that chip is still taking everyone's lunch.  Heterogeneous CPU design allows x86/64 to compete with ARM.  Its actually quite genius. The e-cores are simply the Intel Atom line that they developed for lower power low spec machines, like a laptop you'd give to someone in K-12.",Positive
AMD,"They're space efficient.  They're not power efficient.  Their power/performance is actually terrible.  They're just cheap.  Keep adding spacE cores to a sku until it has a bigger cinEbEnch scorE than the AMD part it's competing with and then tell people what an amazing 400W cpu it is, and how ""efficient"" your 400W cpu is...",Negative
AMD,"We don't have ISO-Node to make a direct comparison, but the E cores in ARL/LNL are much smaller than Zen5 vanilla cores.",Neutral
AMD,"From a topology perspective AMDs ""big"" cores are more like Intels e cores.",Neutral
AMD,Bare minimum? 0. There are already some chips that are E-core only.,Neutral
AMD,"The other reply covers bare minimum, however we can look at Amdahl's law to get an idea of good middle ground: which is 4-8 physical P cores, and about 10-16 E or C cores in addition.",Neutral
AMD,"That's most likely a very difficult question, with a non-definitive answer as to what's the best strategy here. Similar to branch prediction.",Neutral
AMD,"That's something that you're not likely to get a very solid answer on as Intel keeps things like that at least somewhat hidden. To put it broadly though, there are 4 classes of tasks, or at least were for Alder Lake.  Class 0 is most applications. They can be moved around without issue. By default in Windows 11, the in-focus window goes on P-cores while other windows will decay down to the E-cores.  Class 1 are things with AVX or AVX2 instructions. Class 2 has AVX-VNNI instructions. These are prioritized for the P-cores, with Class 2 taking the top priority.  Class 3 tasks are I/O bound or otherwise bottlenecked outside of compute. These will be put on the E-cores or even LPE cores depending on what is free on a given CPU. Lunar Lake will of course skip straight to the LP island while Meteor and Arrow Lake will likely prefer their E-cores.  These definitions have likely shifted or been added to for different generations, but these types of classifications are still similar to what's going on.",Neutral
AMD,"The ""Intel Technology"" Youtube Channel posted a bunch of discussions on Panther Lake over the last few weeks, and at least one of them discusses the changes to Thread Director in PTL.  Thread Director doesn't actually schedule threads. It monitors Core Usage and provides suggestions to the Windows Scheduler. PTL prioritized placing threads on the LpE cores and if the application exceeds a certain % threshold of LpE cores utilization, it suggests to Windows to move the thread to the P cores. The exact mechanics are a trade secret, but it's an interesting watch.",Neutral
AMD,[https://www.youtube.com/watch?v=VcvzIGA6qA4](https://www.youtube.com/watch?v=VcvzIGA6qA4)  Maybe give this a watch. An Intel fellow discussing Thread Director,Neutral
AMD,"Why can't Intel use sane names for their cores? I like AMD's zen 1, 2, 3, 4, 5. It's easy to keep track of. I always get confused about which Intel core is which.  That said, I think that Intel has gradually moved to a saner and more deliberate P-E design, yes (but I have not followed the most recent development).",Neutral
AMD,"Hence with LNL, it is technically possible to run LPE as standalone for the most battery life without using ringbus?",Neutral
AMD,"True, but they are also different from regular e cores. They're built around low power draw, not efficiency or throughput. It's a completely different design, not just off ring e cores.",Neutral
AMD,"> The cores' designs are fundamentally different because in the server dies, they're connected together using a mesh layout instead of using a ringbus  I mean, I don't think this impacts the core's design itself.   There was an interesting question asked about this at a [C&C interview](https://chipsandcheese.com/p/interviewing-intels-chief-architect):   Question:  >And so actually speaking on difference between client and server, so Darkmont is both used in Panther Lake and in Clearwater Forest. What sort of differences do you have to make in a core between server and client in terms of stuff like RAS features? So what differences are there in terms of implementation and what you have to design.  Answer:   >You know, ECC in the caches... so inside the core, we do add features. We can put that core in both if we want, right? So there aren’t a lot of physical differences between the cores in the two, but the environment is very different. So on the server side, maybe we have power gates per core, maybe we don’t. The power delivery is different.",Neutral
AMD,"I think Intel is unique, at least in comparison to AMD and Apple, in using E-cores to scale nT perf.   AMD's dense cores are not nearly small enough in comparison to their classic cores to really increase core count with the die savings they create. Or at least the dimensions don't really lineup well on the die. Their strategy, in client at least, seems to be much more just saving a bit in cost, than actually increasing core count using the die area savings.   Apple on the other hand seems to just use E-cores as a LP island, at least on the higher end skus, while not relying on it to boost nT perf/mm2. Hence why the higher core count skus cut back on e-cores and increase P-cores, while PTL is scaling up the number of e-cores. And also why Apple's 12 core mobile chip is 8+4, while PTL is 4+4+4. Or Apple's 16 core chip is 12+4, while PTL is 4+8+4.",Neutral
AMD,And their l2 cache sharing does haloe them in games some too,Neutral
AMD,"It depends on how hard you push the E-cores. Looking at Huang's power graph, a 265K E-core is more power efficient than a P-core in package power up to \~70% of the P-core's perf.",Neutral
AMD,"With perfect scheduling, they'll also improve efficiency for idle and light tasks that dont stress the cores much, and thus can run at much more power efficient clockspeeds, with lower power draw than P cores would do doing the same.  But I dont know how much this actually takes place in practice.",Positive
AMD,"We do have something pretty close with Zen 5C vs Skymont on N3E vs N3B respectively.   Zen 5C is *way* larger, even after attempting to account for the fact that Zen 5C has core private L2 while Skymont does not. It's still larger even if you literally remove the *entire* FPU block on top of that.",Neutral
AMD,How so?,Neutral
AMD,"N100? Unfourtenly  I never got to use one, but wanted to try and do a day of work in it.",Neutral
AMD,My work laptop literally never goes to LPE. Nothing is ever scheduled on them when I'm looking at task manager anyway.,Negative
AMD,"Kinda, yes. Intels approach is pretty complicated including the LPE cores themselves.",Neutral
AMD,They're removing cache ecc on consumer cores?,Neutral
AMD,"Part of it is that AMD dense cores are afaik just full cores with less cache. This means for certain workloads they are 30% smaller, use a bit less power and work just as fast. That's why they go in servers. They also have x3d for stuff that benefits from more cache.",Neutral
AMD,Intels e cores are clustered like a CCX in AMDs designs. They're connected to each other well and use a high speed interconnect to communicate with other clusters. In intels design four e cores share one block of L3 cache. Communication within a cluster is much faster than leaving it.,Neutral
AMD,In the ARM world there are tons of server chip designs that only use E cores to maximize multi threaded performance per Watt/Die Area. I believe Intel has a similar model either released or coming soon in their server lineup with the same idea - I don't keep that up to date with it though.,Neutral
AMD,"Check Sierra Forest, those are E-core only Xeon CPUs.",Neutral
AMD,I think there's new models even with more cores by now,Neutral
AMD,"I would hazard a guess that when the compute die is active, it'll avoid scheduling to the LPE cores at all costs, but if a workload fits in the LPE cores alone (e.g. the system is idle and only performing background tasks) it'll use those",Neutral
AMD,"I think the comparison is a bit extreme tbh.   The E-cores just have a shared L2 which makes them special, but even that is not too uncommon when some ARM designs do that as well, and if rumors are right, so will the P-cores in NVL.   The penalty of leaving a 4 e-core cluster is also relatively good. [Interestingly enough](https://chipsandcheese.com/p/examining-intels-arrow-lake-at-the), core to core latency is worse going from a P-core to P-core, than going from an e-core cluster to another P-core.",Neutral
AMD,"Well, as u/bankkopf said, sierra forest, Intel's first generation e-core server CPU is already out.   Clearwater Forest, their second generation of this type of CPU, got pushed back to 1H 2026 (it was supposed to launch 2H 2025).",Neutral
AMD,Probably but also even at idle it doesn't happen. Though I read somewhere that's why new chips have 4 LPE because even at idle windows supposedly uses enough resources that 2 cores can't handle it. Which is possible but could also be horseshit. Wasn't exactly a verified source.    Either way I haven't seen it even if I turn off everything and let windows idle with just task manager.,Negative
AMD,"That is weird. My laptop with Ultra 5 125H does, which can be very noticeable because of the lag when transitioning from the machine sleeping to waking up. Sometimes it does that just from me leaving it idling for a while as well, so I will assume it is a BIOS / power mode thing.",Negative
AMD,>It is worth noting though with the AMD m8a instance are all physical cores without SMT / sibling threads while the M8i Granite Rapids instance for its vCPU counts the physical core plus the sibling thread of Hyper Threading.  I would like to see it apples-to-apples. I wonder why AWS counts SMT differently for the same named instance.,Neutral
AMD,Big OOF for Intel. If you need x86_64 or it better fits your workflow just use AMD as it has both higher performance and is more cost effective. Otherwise Graviton might be a possibly a cheaper option. The only reason to use Intel is I guess some legacy enterprise software may be only certified running on Xeons? Who would choose Intel and why?,Positive
AMD,Why cant intel get any W's,Negative
AMD,"All hosting providers pull this shit. ""vCores"" are not representative of physical cores. Even in the best-case scenario where vCores are ""dedicated"" and the node runs a processor without SMT, you still have to deal with sharing CPU boost clocks.",Negative
AMD,8C/16T Granite Rapids - 508.61  16C/16T Turin - 814.10  The overall performance for GNR vs Turin doesn't look too unexpected when you account for the physical core count differences but it is hard to explain why 16C of Turin costs only 15% more than 8C of Granite Rapids for AWS.,Neutral
AMD,Its apples-to-apples because thats why you pay for.,Negative
AMD,"Strangely enough, according to the geomean at the end of the article Graviton 4 actually has worse perf/price compared to Turin, though much better than Xeon.",Negative
AMD,"I'm giving Intel a pass on this one. 8c-16t vs 16c-16T, Turin had more horse power.  Graviton just looks like a way to reduce the entry price vs actually being competitive",Neutral
AMD,I don’t see the big oof given that the Intel had half the execution resources:  >> It is worth noting though with the AMD m8a instance are all physical cores without SMT / sibling threads while the M8i Granite Rapids instance for its vCPU counts the physical core plus the sibling thread of Hyper Threading.,Neutral
AMD,"We are using Intel's for our embarrassingly parallel application, ignoring all my suggestions to use AMD, because ""it works better on Intel""  I can't complain though, Intel Vtune makes it infinitely easy to profile our application.",Positive
AMD,Yeah but even Intel data center revenue is going up due to “AI”. Even Intel is cashing in on the boom,Negative
AMD,That's fair,Neutral
AMD,Graviton has a few bad outlines use cases that impact its score. Choosing either should be based on what your use case is.,Negative
AMD,"The users have to pay for the compute time. It is not just an equation of hardware specs vs performance.  In the article:  *""m8i.4xlarge was at $0.847 USD per hour and the new EPYC Turin instance the most expensive at $0.974 USD per hour.""*  I do give Michael from Phoronix the benefit of the doubt that he did choose the most comparable setups for each. I.e. there did not exist a $0.974 Intel option that would have given 16c/16t, and there did not exist a $0.847 AMD option that would have given 8c/16t - he chose the most appropriate options from each vendor.  Would you: 1. pay $0.847/hour to get a performance of 508.61, or 2. pay $0.974/hour (+15% more) to get a performance of 814.10 (+60% faster)?",Neutral
AMD,Why would anyone who pays for an instance on the AWS care how it is configured? Guess people at Amazon had a reason to configure it this way. Number of PCIE lanes maybe?,Negative
AMD,That's not how SMT works,Negative
AMD,"Absolutely, it will be workload dependent",Positive
AMD,Yes they were the same generation (M8) and same size (4xlarge 16 vCPU). They don't really have a way to get something more comparable than that.,Neutral
AMD,"> Would you: 1. pay $0.847/hour to get a performance of 508.61, or 2. pay $0.974/hour (+15% more) to get a performance of 814.10 (+60% faster)?  It depends on the usage.  In many cases, these instances aren't going to be fully utilized, and having an instance that costs less is going to be preferred over one that has higher peak CPU performance.  When autoscaling or using spot instances, instance availability can also be a consideration, and using slower instances may be preferable over faster instances that start throwing errors about a lack of capacity when you need to scale out in a hurry.  Certainly in this case, if I had a workload that was heavily CPU-limited, I'd be defaulting to the AMD option (and that has been the case for many years now). However, the CPU is not always going to be the bottleneck, and computing performance may not even be an important consideration.",Neutral
AMD,Because cost can change very easily.  It's usually more helpful to know the raw comparable performance and do the cost adjustment yourself because rarely is your cost goinng to match that you see in a review.,Neutral
AMD,Or they simply needed different options and how these were configured was mainly due to timing of setup. Or maybe small differences in how smt works with each.   AWS unfortunately doesn’t really have much of incentive to set up similar instances from different hardware vendors just that we could compare them. The intel option is actually cheaper so a lot of customers who don’t need constant peak performance would likely choose that.,Neutral
AMD,What execution resources do you lose when you disable SMT on current gen Xeon?,Neutral
AMD,"> computing performance may not even be an important consideration  The article here is about comparing xlarge compute nodes, and my comment was a reply to a suggestion that it was understandable that Intel wasn't as fast since it had to fall back on hyperthreading..",Negative
AMD,"> However, the CPU is not always going to be the bottleneck, and computing performance may not even be an important consideration.  For those use cases, there soon will be C8a/R8a/X8a instance types with different balances of RAM-per-vCPU. Starting at 2 GB per vCPU and going up to 16 GB per vCPU. The reviewed ones are 'balanced' ones with 4 GB per vCPU.",Neutral
AMD,Another possibility is either their security team forbade them from enabling SMT - remember spectre/meltdown?,Neutral
AMD,SMT isn't a half.,Neutral
AMD,"Spectre and meltdown were not actually related to SMT (also, meltdown was intel specific i think, spectre affected most out of order CPUs). There are SMT related vulnerabilities too but I don't think those are relevant for these CPUs.",Neutral
AMD,"SMT isn’t any execution resources, it’s just scheduling. So this review is comparing n AMD cores to n/2 Intel cores running n threads.",Neutral
AMD,This review is comparing an ~$1/hr Intel instance to a ~$1/hr AMD instance.,Neutral
AMD,You still don't seem to grasp how SMT works.,Negative
AMD,He said “intel had half the execution resources”. That is not correct in the sense that AMD and intel don’t have the same amount of execution resources per core but correct in the sense that 8 cores of intel has exactly half the execution resources of 16 cores of intel. Regardless of SMT. SMT doesn’t usually add or remove any resources.,Neutral
AMD,"I'd love to know more about the cost differences between larger infinity caches and more external memory bandwidth. Given that the latter affect a lot more components than just the SoC, it's probably a rather complex scenario to model and optimize.",Neutral
AMD,"That comment really is interesting thou, how much external bandwidth is required for a TBDR design over AMD's hybrid but mostly immediate mode. He probably doesn't have the adreno notebook anymore to compare though. Especially the resolution scaling side of it.",Neutral
AMD,There's also the power consumption. It takes far more power to communicate over PCB to external memory chips compared to using the fan-out design.,Neutral
AMD,"RDNA5 DGPUS (D = dedicated) are apparently replacing Infinity Cache with a large L2 like Nvidia's done recently as SRAM isn't scaling well and GDDR7 increases bandwidth by 50% anyway, so moving to less cache overall (less $), thus needing more bandwidth from ram, but a larger close cache with lower latency (good for raytracing) looks preferable.  I don't know what SOCs like Strix Halo are doing though, LPDDR6 is just somewhat faster than 5X right? (and infinity cache uses less power so it's good for mobile stuff)",Neutral
AMD,"> replacing Infinity Cache with a large L2 like Nvidia's done recently as SRAM isn't scaling well  L2 is SRAM, so that proposed reasoning makes no sense.  Where the large amount of cache is situated in the memory subsystem has nothing to do with die space.  The evidence strongly suggests that AMD's extra cache has been more effective at mitigating memory throughput limits than nVidia's L2 cache, so if AMD is changing where the bulk of the cache sits, it will because they've figured out something new, not because it wasn't working.  And certainly not because SRAM in one location ""isn't scaling well"" versus SRAM in another location, whatever you intended that to mean.",Neutral
AMD,> LPDDR6 is just somewhat faster than 5X right?   24b sub-channels should make a huge difference if they take advantage of it (i.e. implement a 1.5x bus width).,Positive
AMD,I'm guessing it's also beneficial for the RDNA/UDNA cache architecture to be closer to NVIDIA's for keeping similar performance when CUDA kernels are ported.  LPDDR5X @ 9600 Mbps provides 19.2 GBps for a 16 bit channel (sadly Strix Halo only running at 8000 Mbps).  LPDDR6 introductory speed is 10.667 Gbps which gives 28.5 GBps effective for a 24 bit channel after subtracting non data bits).  So Medusa Halo with 384 bit LPDDR6 @ 10.667 Gbps  should have 455 GBps bandwidth (77.7% faster than Strix Halo).,Neutral
AMD,"To be clear, the L2 on RDNA5 is smaller than the equivalent L3 infinity cache on RDNA4 and before. So it saves money by just being smaller and eliminating having an L2 and an L3 at the same time. That they'll be able to pull less data from cache and so need to go out to ram more often is mitigated by GDDR7 having enough bandwidth that such doesn't matter.",Neutral
AMD,"I am wondering if AMD considered moving much if not all of the L3 cache off the GPU die and instead stacking a 3d cache chip underneath like the x3d CPUs AMD has.  Seems like an obvious way of boosting GPU cache that would not be particularly difficult or expensive to implement, given their experience on the CPU side.",Neutral
AMD,">So Medusa Halo with 384 bit LPDDR6 @ 10.667 Gbps should have 455 GBps bandwidth (77.7% faster than Strix Halo).    This is nice but still weak for AI, Apple has AMD beat here for now.",Positive
AMD,There's also universal compression for RDNA5 which AMD plans to use to reduce the need for memory bandwidth.,Neutral
AMD,"Yep, I believe AMD is technically capable of building something much stronger, but doesn't seem to have the culture to lead and plays it too safe waiting for the HP & Lenovo's to tell them what customers want.",Negative
AMD,7800X3D,Neutral
AMD,"The 7800x3d is the better pick between those 2 cpus for mostly gaming, and can also handle those other tasks like streaming and multitasking no problem. The x3d v-cache is worth more than just $20, for that price difference its a bit of a no brainer.  For the parts you mentioned there really isn't any compatibility issues or bottleneck problems, the only thing I would say with AM5 is to make sure you get only 2 sticks of ram, Am5 can get unstable with 4 sticks and generally speaking you have to down-clock the ram speeds to get good stability.",Positive
AMD,"7800X3D  So long as you don't buy a really low end motherboard, you'll be fine. For memory, any 2x16gb 6000 CL30 kit would be ideal. 6000 CL36 is fine if 6000 CL30 is too expensive.",Positive
AMD,"If fast ram was cheap and you were into overclocking/productivity, you could make the case for the 9700x, but it looks like the X3D chip is the better option here",Neutral
AMD,7800x3d is up to 30% faster at gaming.,Positive
AMD,The 9700X is only about 5% faster than 7700X  Meanwhile 7800X3D can show improvement of 15-30% over 7700X,Neutral
AMD,Straight to the point,Neutral
AMD,Thanks you told me what i need to know but still I find people who disagree idk why,Neutral
AMD,Yeha i will never like overlooking or ill turn the settings down or buy new parts,Negative
AMD,Yeha that's why im get the x3d,Neutral
AMD,"The main reason you would hear some people say the 9700x would be you aren't gaming but still do have heavy multicore workloads like video editing, 3d modeling, CAD software ect.   For mostly gaming use cases the reality is anyone who says the 9700x is better than the 7800x3d is just flat out wrong.",Neutral
AMD,Yeha the x3d is the best bet for me,Positive
AMD,"The 9600XT will likely be a bottleneck for the 7800X3D in a fair amount of cases. But that is actually the *situation that you want.*  Proper attitude about bottlenecks isn't about removing them (because realistically, they can't be. At SOME point, you WILL have a bottleneck in the machine). It's about managing them. You want the bottleneck to be appropriately sized for your needs, and placed where you want it (i.e. the performance disparity between a 9060 XT 16GB and a 7800X3D on a 1080p display is reasonably small, and you want the graphics card to be the limitation when possible).  Oh and because you didn't specify, I would make sure that this is the 16GB version of the 9060 XT.",Neutral
AMD,"Its a decent combo, I think the 7800x3d is a bit overkill for the 9060xt personally, but if you are playing E-sport titles and plan on pulling 200-300 FPS it can be a good match.   Generally, I would say to go with the 9600x and a 5070/9070 that should net a better gaming experience overall.",Positive
AMD,Hot take: 9070xt & 7600x instead,Neutral
AMD,It won't,Neutral
AMD,What Hz monitor will you be playing on and what sort of games? Mainly competitive shooters or single player AAA titles?,Neutral
AMD,I am on a 9060XT and a Ryzen 5 9600X.  Has been about 3 or 4 weeks with the build now and no bottlenecking at the CPU.,Neutral
AMD,unless if u are into some competitive game u have too much cpu for ur gpu choice,Neutral
AMD,It's fine for 1080p and definitely won't bottleneck. Make sure you get the RX 9060 XT 16 gb for the best performance and future compatibility.,Positive
AMD,It's fine.  Yes you will have bottlenecks as it's impossible not to.  For most gaming the rx9060xt will be the bottleneck.,Neutral
AMD,"1080 p ?      it's sufficient but nowadays 1440p monitors are cheap. And the GPU is kinda weak for that cpu.    If i were u, i would buy a 1440p monitor and buy a 9070xt.",Neutral
AMD,7600 + 5070/9070 is just going to be way better.    Also bite the bullet and upgrade to a 1440p monitor. They're like $150-200 for a good 1440p 180hz IPS panel,Positive
AMD,the 7600x is more than good enough for your gpu choice,Positive
AMD,Drop down to a 9600X and up the GPU to a 9070 XT.,Neutral
AMD,I'm still enjoying my Ryzen 5800 XT and RX 6650 XT don't let the haters tell you its a bad rig.  That thing is a 1080p Ultra quality beast.,Negative
AMD,No not at all,Neutral
AMD,will what bottleneck?,Neutral
AMD,Exactly. Play the 9060xt until the wheels fall off of it. Then get a better GPU later on,Neutral
AMD,Youre mostly right if were talking cpu heavy games. But for more gpu demanding games the 7800x3d will probably not see much utilization.,Neutral
AMD,cool thanks! and yes i’m planning on getting 16GB,Positive
AMD,Not a hot take for sure,Neutral
AMD,the price jump from a 9060 to a 9070 is pretty insane for me so i might have to stick with 9060 as i am on a budget. will the 7600x work well with a 9060?,Neutral
AMD,Actually less FPS in many e-sport games with this combo.,Neutral
AMD,what Hz would you recommend? the monitor i’m thinking of buying is 180hz but I’m not sure if that will be enough,Neutral
AMD,what kind of games do you play?,Neutral
AMD,"Stick with the 7800X3D, a lot of games profit from it and depending on which games you play some are very heavy on the CPU, for me thats the case with Star Citizen.",Neutral
AMD,What's the price difference between a 7500f & 9070(non XT) Vs your original combo?,Neutral
AMD,"Well, that’s only a problem if you’re playing e-sports. If you’re playing more graphically demanding AAA titles, you’ll be benefiting.",Neutral
AMD,"I can see the difference between 144 Hz and 240 Hz, some people say they can’t see any difference beyond 144 Hz. It depends on you. 240 Hz is a good option, and they’re pretty cheap nowadays.",Neutral
AMD,okay cool i will look into getting 240 hz then thanks,Positive
AMD,"160-180hz is plenty imo. Go to an actual store like Best Buy and try out the monitors there for yourself to get an idea of the differences. I personally can’t tell the difference, but everyone is different.  What games will you be playing with this system?",Neutral
AMD,"What are the rest of the PC specs?  The 6500xt is a terrible card especially with lower PCIE slots.    Googling the 6500xt t5 legion model it has 8gb of RAM and a 5600g, which are all terrible components.  It needs at least 16gb of RAM to even play most games without being crashy and even then the best it's gonna handle is games like 8-10 years old at this point.",Negative
AMD,Is this a new problem or have you always had poor performance with this PC?,Negative
AMD,"While your system is pretty low end/weak, game performance will depend on the resolution and quality settings, and it should be able to play an 8 year old game like RDR2 at 1080p especially.  What resolution and quality settings are you playing the game at?  Are you sure the monitor is plugged directly into your GPU and not into the motherboard video port?",Neutral
AMD,Can you run Speccy and copy/paste the output here?,Neutral
AMD,"Unless you have a significant CPU bottleneck, you should be able to get around 90 fps at 1080p in rdr2 with a 6500 XT, so slightly higher FPS at your resolution. You CPU being limited to PCIE 3.0 and your GPU only using x4 pcie lanes will likely reduce performance noticeably though.   Check for CPU bottlenecking: [https://www.pcworld.com/article/1955495/pc-bottlenecks-cpu-or-cpu-limiting-gaming-performance.html](https://www.pcworld.com/article/1955495/pc-bottlenecks-cpu-or-cpu-limiting-gaming-performance.html)",Neutral
AMD,"Thank you so much for the quick reply! As I mentioned I'm super new to anything in this manner and while I've heard my graphics card is extremely poor, I actually don't even know how to/what to look for in terms of PC specs. Idk what a PCIE slot is, I feel embarrassed lmaoo",Negative
AMD,I think it might also be worth noting that I added more RAM to the PC so it now has 32gb. Idk if that helps.,Neutral
AMD,"Sorry for the late reply, it's been like this since I bought it in 2023.",Neutral
AMD,Right now it's set to 1760 x 990 with quality set to medium.   Also how would I check to see if my monitor is plugged into my GPU? I feel so dumb for not knowing this,Negative
AMD,"Understood, I'll give this a try. Thank you so much!",Positive
AMD,After running benchmark tests on RDR2 I don't believe it's indicating a CPU bottleneck.,Neutral
AMD,"It'd help somewhat but it depends on how you added it.  if it's one whole 2x16gb kit that's fine, if it's an extra 3 8gb sticks then that would be problematic.",Neutral
AMD,That tells me it's just kind of a slow PC.,Negative
AMD,"The GPU ports are lower down on the back of the pc in one of the horizontal slots. Your GPU is the graphics card, the big component installed lower down in the motherboard. The monitor video cable should be plugged directly into it through the back of your case.",Neutral
AMD,"Oh lordy, I did 4 8gb sticks and I didn't even know that would be problematic.",Negative
AMD,When I bought it each box had two sticks.,Neutral
AMD,"Gotcha, is there anything specifically I should upgrade? If so what would you recommend?",Neutral
AMD,My monitor is connected to my PC via HDMI cable. Does that change anything?,Neutral
AMD,"Depends on your budget. But, the whole thing is kind of slow. It's not like there's one part you could upgrade to dramatically increase your performance.",Negative
AMD,"No, it doesn't help. Take a picture of the back of your PC, post it to imgur, and link it here.",Negative
AMD,I understand. Do you know roughly what parts you would upgrade if I had a budget of around $300-$400?,Neutral
AMD,Bet I got u right here: https://imgur.com/a/E6XTduv,Neutral
AMD,"At that budget, I would recommend upgrading your GPU to the Radeon RX 9060 XT 16GB (not the 8GB model). If you decided to upgrade the rest of your PC later you could bring the new GPU over with it.",Neutral
AMD,"Well, the monitor is plugged in to the right place at least.",Neutral
AMD,"Understood, thank you so much for your input I really appreciate it!",Positive
AMD,"Just to make sure, is this the one you're talking about?   [https://www.amazon.com/PowerColor-Reaper-Radeon-9060-GDDR6/dp/B0F9QM1M6R/ref=sr\_1\_8?crid=29ZDNRSKPQP5S&dib=eyJ2IjoiMSJ9.fY1Dv66Z4ThRO27-QRzz49QM2zHY3Vfwgm\_7zRcnyw5HyBwLmN824PmiyYkoh-Ct8J3AGHXgBjRNtihfAls90BWDnKsUES3113BaAjwRBrJItHd4iJYXe-RqTw2l9UkaC9Om8RXFDiRgCcdXQvhJipYoxezJJQGA4OmEJePiG3FB6TyXR4jcKy5PM7F7rsSnx\_hS3x8WU3k0GpLBuva6X21bLrqJJ0kqRDnmWA\_yl-Q.eqWaiWQ32Vk5qqIgcEp2aCwwX6SDgk1cNYIrE5s-kTY&dib\_tag=se&keywords=AMD%2BRadeon%2BRX9060%2BXT%2B16G&qid=1761698370&sprefix=amd%2Bradeon%2Brx9060%2Bxt%2B16g%2Caps%2C172&sr=8-8&th=1](https://www.amazon.com/PowerColor-Reaper-Radeon-9060-GDDR6/dp/B0F9QM1M6R/ref=sr_1_8?crid=29ZDNRSKPQP5S&dib=eyJ2IjoiMSJ9.fY1Dv66Z4ThRO27-QRzz49QM2zHY3Vfwgm_7zRcnyw5HyBwLmN824PmiyYkoh-Ct8J3AGHXgBjRNtihfAls90BWDnKsUES3113BaAjwRBrJItHd4iJYXe-RqTw2l9UkaC9Om8RXFDiRgCcdXQvhJipYoxezJJQGA4OmEJePiG3FB6TyXR4jcKy5PM7F7rsSnx_hS3x8WU3k0GpLBuva6X21bLrqJJ0kqRDnmWA_yl-Q.eqWaiWQ32Vk5qqIgcEp2aCwwX6SDgk1cNYIrE5s-kTY&dib_tag=se&keywords=AMD%2BRadeon%2BRX9060%2BXT%2B16G&qid=1761698370&sprefix=amd%2Bradeon%2Brx9060%2Bxt%2B16g%2Caps%2C172&sr=8-8&th=1)",Neutral
AMD,"Gotcha, thanks for confirming :)",Positive
AMD,Yes,Positive
AMD,They're literally designed with it in mind.,Neutral
AMD,a $35 dual tower air cooler like a thermalright ps120 is more than enough for any amd cpu,Neutral
AMD,"A peerless assassin can cool any am5 cpu well  Gone are the days of the shitty overheating intel 13 and 14th gen i7's and i9's that needed high-end motherboard and AIO just to survive. Just your average b650/b850 mobo and air cooler will keep the temps way down. And good air coolers are very affordable nowadays, like the peerless assassin for instance",Neutral
AMD,"Lol, BeQuiet! Blackrock Pro5 user here for my 9800X3D. Cool temps through and through. AIOs are silly.",Positive
AMD,"I do custom watercooling because... It's fun and it looks cool. But air cooling is way more affordable, reliable, and almost as good (and sometimes better) than AIOs and even custom loops. Just dust it out every 3-6 months and you're golden.",Positive
AMD,Yes.  AIO is such a waste and dumb for 98% of cases,Negative
AMD,"Yes, in a lot of ways it’s the golden age of air coolers. There are a ton of relatively affordable air coolers that trade blows with noctuas best.",Positive
AMD,"With how efficient modern air coolers are, main difference between an air cooler and aio is pretty much purely cosmetic. An air cooler will work perfectly fine.",Positive
AMD,It's a 120w TDP CPU. And they're not getting hotter. The ryzen 9 7900x was 170w TDP.  A DeepCool AK400 would likely be enough.,Neutral
AMD,Arguably air cooling and heat sinks are so much more efficient now that water cooling isn’t nearly as needed,Positive
AMD,The marketing is working really well it seems,Positive
AMD,"I'm air cooling a 9950X3D with a D15S and have thermal headroom to spare, even with PBO. You're good. These chips running hot is pure myth. They're incredibly efficient.",Positive
AMD,"Im on a like $40 air cooler with a 9800x3d, its completely fine.",Positive
AMD,I had an AIO burst and leak. Corsair was super shitty about replacing parts.  Never water cooled again. I have a phantom spirit with two noctuas slapped on. Idle 43C.,Negative
AMD,"9950x3d with a Phantom spirit and two noctua fans. With a 5090 dumping heat in the case too.  The thing's never gone anywhere near to 90 C, even when I was doing some prime95 stress testing.",Negative
AMD,"No amount of cooling will ever be ""enough"", I have the same CPU with an AIO and it keeps boosting until it trips a temperature limit (saw it go 200W+). For normal use and air cooler should be fine. But if you're getting a high end CPU, why not get a good cooler as well? Good AIOs can be gotten for less than 100",Negative
AMD,"Yeah, a good air cooler can still handle the 9900X just fine, especially something like a Noctua NH-D15 or DeepCool Assassin IV. As long as your case has solid airflow, you’ll stay safe without the hassle of liquid cooling.",Positive
AMD,>Is air cooling still enough  Always was,Neutral
AMD,Boy do I got news for you.  There’s been some advancements so yes.  The Royal Pretor 130 is the current top dog,Neutral
AMD,"I'm easily cooling a 9900X with an ID Cooling Frozn A720, and I'm blasting 190 watts peak through it with PBO on. The worst it gets is like a very brief peak of like 90C on occasion. Very far from chip killing.",Neutral
AMD,My be quiet 5 keeps my 9800x3d at 40°c to 50°c at 40% to 80% utilitization,Neutral
AMD,It looks like a liquid loop with insufficient mass actually works _less_ effectively than air cooling. At least from what I gleaned poking around about a 9800 build.,Neutral
AMD,"4080super and 9800x3d here, love my peerless assassin 120",Positive
AMD,"then how can i fix my r9 5900x with noctua nh u 12a reaching 76c in bf6 on full hd reso with rx7800 xt?  I.ve fiddled with fan curves and still not enough , changed thermal paste , updated driver, bios..  Any clues or improvements?",Negative
AMD,"I bought the same CPU as you did, and I'm getting a Peerless Assassin SE 120 it's great bang for the buck",Positive
AMD,"I have a 9950x with mild overclocking and a Noctua NH-D15. It was sufficient but louder under load (gaming) and couldn't keep my CPU cool under heavy artifical loads (Prime95). One challenge is the video card (5080) exhausts heat up and into the Noctua intake fan, making it harder to cool during gaming.    I switched to a Artic Cooler Liquid Freezer 360. It runs cooler and will not throttle even under Prime95 loads. It is quieter under heavy loads and doesn't speed up/slow down the fans nearly as much as the Noctua. I am much happier using the AIO than the Noctua and would recommend this to others with similar hardware.",Neutral
AMD,A peerless assassin 120 been keeping my 9800x3d cooled no issues.,Positive
AMD,Yes,Positive
AMD,Short answer? Yes. Long answer? Yeeeeeeeeeeeeeeeeeeeeeeeeees.,Neutral
AMD,Yes,Positive
AMD,Yes. I've seen multiple videos that show air cooling is every bit as efficient as water cooling. Water cooling looks cool but you don't need it.,Neutral
AMD,I just built a Ryzen 9 9950X3D system with a Noctua NH-D15.  Runs at lower temps than my AIO cooled 7900X3D,Neutral
AMD,"Build ducting so your CPU cooler has separate intake and exhaust ducts with at least 2 fans. One blowing in and the other blowing out. This allows low fan RPM and quiet operation.  Same for your GPU. Pipe in cool outside air. Use a 3d printer or heat bend thin sheet plastic with a heat gun.  Water cooling is for people who don't understand how to move air in a case properly. Move the air in one direction, in from the bottom or front, out in the top or the rear.",Neutral
AMD,My 16 year old Noctua DH-14 handles a Ryzen 9 5900X like a champ.,Neutral
AMD,Yes.  /thread,Neutral
AMD,I run a Noctua NH-D15 on my 13700K and I stay under 82° C,Neutral
AMD,9900x isn't high end. It's mid range. X3d is the high end.,Neutral
AMD,"Air cooling is great once it is installed even now, especially with silent enough fans and proper airflow. But the heatsinks got so large and heavy that installing them is not fun at all. I went to the shop to install it the last time and will do it the next time, if they screw up something they are gonna fix it themselves right then and there.",Negative
AMD,"Yep.  Running a Noctua U12S from years ago on a 200W 5900xt with an OC/UV. No issues.   However, case and fans are more important.  Air coolers need an  uninterrupted path of air. Modern fishtanks don't work right. And cases without proper airflow struggle. A shit case can work with a giant 360mm AIO. Not with an air cooler.",Neutral
AMD,"Yes, a Thermalright Phantom Spirit is cheap and will work just fine. And of course Noctua NH-D15 at twice the price. They're about the same performance.  That's all assuming you're not going to do any real overclocking that require more power.",Positive
AMD,"My last PC had a small but notable decrease in temperature when I switched from a cheaper AIO liquid cooler to a Noctua 2×120mm air cooler, with a modest overclock.",Neutral
AMD,My thermal right PA120 is doing a great job on my 9800x3d. Most of the time playing it sits at 60C and at 100 load it goes to about 70C range which it rarely does except when prerendering a game for example. I also have exceptionally bad airflow into my case.,Positive
AMD,Average air coolers are on par with average AIO's  Nowadays just comes down to what you want for your build aesthetically.,Neutral
AMD,"Fine as in “not thermal throttle”? If that’s the expectation most reputable brand’s dual tower air cooler will do.   However, AIO, especially 280/360, have significant advantages when it comes to noise to performance under sustained heavy load. If you don’t mind your PC sounds like a jetliner taking off, or you wear headphones anyway, you typically don’t need AIO.  But some of the thermalright 360 AIO are surprisingly price competitive even up against their own air cooler, can’t speak for its longevity but performance wise they are definitely worth considering.  Just avoid 120 AIO as they sometimes perform worse than air cooler, unless you are building a SFF build and short of space for air cooler.",Neutral
AMD,I mean sure you can make anything work but honestly a 360 AIO is just so much less headache and noise. Plus you never really have to worry about cooling and they tend to take up less room in the case all things considered.,Negative
AMD,"Been running an air-cooled 9800X3D since Christmas, so all through the seasons now I guess, and other than the usual hyperfixation on every little thing when it was a new build, I've never once even *thought* about CPU temp, let alone actually worried about it.  Cooler is a little fancy, a noctua nh u12s chromax black, but its not _outrageous_ imo.",Negative
AMD,yes it is: [https://www.techpowerup.com/review/amd-ryzen-9-9900x/25.html](https://www.techpowerup.com/review/amd-ryzen-9-9900x/25.html),Neutral
AMD,"Well, I'm using a 9900X in my Fractal Terra with a NH-L12S cooler, so I hope this is enough, lol. So far, no issues.",Positive
AMD,"I got a Deepcool AK400 Zero Dark with my 9800x3d, and it is doing just fine.",Positive
AMD,My 9800x3d barely gets over 70C with a Noctua in single fan mode. Never even took the 2nd fan out of the box.,Negative
AMD,"It depends on what you do, but yes... air cooling can absolutely be viable on the 9900X or even the 9950X.",Neutral
AMD,Yes - my 9950x runs almost as fast ( just -.2 GHz slower on all core workload) with my peerless assassin than it did with a MORA IV 400 and optimus sig v3 water block.,Positive
AMD,"Yes I have the 9900x All Core at 4.9hz &  Phantom Spirit SE, & max temp is literally 65c-7c while gaming and streaming.",Neutral
AMD,"9800x3d running deepcool ak620 dual tower but i only have one center fan installed. Runs perfectly, no issues.",Positive
AMD,"the peerless assassin 140 keeps things pretty cool on my 5800xt, so far. No OC except to the 4.9ghz it clocks to natively. No adjustment to fan curves or other.  The phantom spirit is a slight upgrade to the peerless assassin. totally worth their money.",Positive
AMD,"LMAO, buddy, most of the time unless your water cooled setup is custom and insane, air cooling will always be better...",Positive
AMD,100% fine. Cpu temp's being hotter is really not something to be concerned about its all a mind game at this point between years ago and newer tech today. There is a performance aspect too it but in the end 99.99% of cpu's that have issues down the road will be because voltage/bios/setting issues or physical parts like the vrm's failing and killing the cpu then riding the thermal limit of the cpu,Neutral
AMD,"I only have an AIO cooler to give me access to components on the motherboard. I got man hands. 🙌   Otherwise air coolers are totally fine for most use cases.  edit: Eh, rough crowd. I get no respect.",Positive
AMD,Yes.,Positive
AMD,"It's enough, but you'll have to use the beefy / dual tower ones. Personally I don't like the look of them hence I favor AIO for higher-end chips. But if all you care is performance or you like the look of massive cooler on your motherboard then go for it.",Negative
AMD,Yes,Positive
AMD,Yes,Positive
AMD,14900k would like a word,Neutral
AMD,Is it enough just for gaming or is it enough for something that's much harder on a chip like video transcoding?,Neutral
AMD,>any amd cpu  On AM5 anyway. AMD does make 500W CPUs (with 128+ cores).  :P,Neutral
AMD,That's exactly what I'm running ATM and temp currently sitting at 40~C. High-end gaming around 60~,Neutral
AMD,"Any AM5 cpu, maybe",Neutral
AMD,Even a Threadripper Pro 9980x. Wow the dual tower is so good,Positive
AMD,"What GPU and what blower style does you have? I feel like that matters a lot for air/water cooling decisions.  I've used air cooler for as long as I can remember (over two decades at this point) but recently had to upgrade to water cooling for the CPU as I upgraded the GPU, thermals in my whole box started going crazy and once water cooled, everything went back to normal levels, and GPU runs cooler too :)",Positive
AMD,"You can lose a bit on multicore if you don't have liquid, but its like 5%   Its not worth it   Aircool anyways    Its more than enough",Negative
AMD,Agreee with the Peerless Assassin,Neutral
AMD,"Phantom spirit is the newer upgraded version, almost identical but slightly better temps at about same price",Positive
AMD,The PA has been the top tier sinflce I can remember.,Neutral
AMD,stupid sexy AIOs. Really just an aesthetic choice imo,Negative
AMD,"The AIO mafia can pry my NH-D15 out of my cold, dead, hands.",Negative
AMD,Same chip in an NR200 (ITX) and close enough HSF which was ‘just for the build’.   It has been some time. AIO is still in the box and has not been a priority.,Neutral
AMD,What temps are you running?,Neutral
AMD,"Plus custom loops are more silent and more maintainable in the long-run.  These days custom loops don't even cost that much more as Bykski, Freezemod and Icemancooler brought prices down to a competitive level. Should only cost around ~$200 USD for a custom CPU loop with one rad :D",Neutral
AMD,AIO is just air cooling with an extra step.,Neutral
AMD,"Liquid cooling is often quieter under load, so there's that if we're not only talking about cooling performance.  I'm personally running peerless with my 7800x3d, but it gets loud at full speed. Curves can only do so much in the end. But for the money, it's definitely in the top range.",Neutral
AMD,"I think what else you have in the chassi matters more for water cooling of the CPU, than the CPU/motherboard itself. If you have something that has the traditional blower style of having both intake and outtake inside the chassi (like a modern GPU), then water cooling the CPU can make quite the difference for both the GPU and CPU.  Literally had this issue myself when I upgraded my GPU this summer, CPU temperatures (and GPU) ended up way too high until I got a AIO :)",Neutral
AMD,"What wattage/TDP setting in PBO do you have it set at?   If i set my 9700X at 105W TDP (142W) with simple PBO and no tweaking, it shoots up to 80C almost instantly under load. Seems excessive. I keep it set to stock 65W TDP (88W) with PBO.and undervolt, and temps are good under load (55C). I'm just surprised how hot it gets so fast at 105W TDP. Doesn't seem usable unless I thermal limit it. I have a Phantom Spirit 120 and good case cooling.",Neutral
AMD,"Because it is an unnecessary complication which has a much much shorter lifespan than air coolers. My experience with dozens, (if not hundreds) of personal and server compuers tells me not to rely on liquid cooling for a 24/7 on machine.",Negative
AMD,***Bang!***,Neutral
AMD,"Well yeah. Any cooler with insufficient mass isnt going to work as well. Mass affects heat transfer. Thats why people dont use tiny heat sinks for most cpus anymore.  More mass + more surface area means a cooler is likely to handle spikes in heat better, which is pretty much the only thing you need to worry about with when cooling and x3d cpu.",Negative
AMD,Im looking for a similar setup (case/cooler) - how is the noise lvl?,Neutral
AMD,"It's not that we like the look, it's that we don't look at it once it's installed, so we don't care.",Negative
AMD,14900k isn't an amd cpu. This entire post is asking about amd cpus not intel.,Negative
AMD,14900k cant have a word because it killed itself,Negative
AMD,"Your brain has lost its connection, please reconnect!",Negative
AMD,I’m pretty sure yall missed the sarcasm in his post.  (Downvotes engaged),Neutral
AMD,"A dual tower, dual fan air CPU cooler will generally have the best performance-to-cost value",Positive
AMD,Air is fine for all tasks.,Positive
AMD,"Air is fine for synthetic benchmarks that push all cores to 100%, it's impossible to have a real workload that runs the chip harder.",Neutral
AMD,"I have a 9800x3d with exactly that cooler. If video transcoding is as heavy as you say it is, thermal throttling is likely going to be an issue.",Neutral
AMD,"> video transcoding  You'd ideally do that on a GPU today, magnitude difference in performance.",Neutral
AMD,"I have a 7950x3d with a thermalright rk120se, and a 3080  the only fans that ever ramp up are my gpu, and I probably just need to repaste it since it's been used consistently for 5 years now  and I keep my case on the ground *gasp in horror*",Neutral
AMD,You only lose performance if you thermal throttle which most modern dual tower aircoolers don't on a 9900x,Neutral
AMD,Most good air coolers are better than a lot of crap liquid coolers so this statement really needs some clarification.    I would also argue that a good air cooler doesn't throttle any am5 cpu.,Positive
AMD,"Absolute hogwash. I'm air cooling a 9950X3D, and very much get all the performance across all 16 cores in the workloads I do. I'm hitting up to 5.9GHz across most cores with PBO.",Neutral
AMD,I’m pretty sure the only you really needed an AIO for was a 13900k or 14900k and that’s because of how inefficient they were with their power draw. I don’t think even Intels current gen Core Ultra 285k needs it. An AIO saves space inside the tower though so it can make it potentially easier to work on.,Negative
AMD,"This is a problem with your setup , not with the air cooling. I'm running the exact same processor, and the thing never goes over 75, and that's on heavy load.  At this point, I think i've done close to two dozen builds with that processor using air cooling benched all of them, and not a single one of them have had any problems.  That processor does not, nor has it ever needed a aio or custom loop.  One of the greatest strengths of the processor is , the fact that it requires such cheap cooling solutions.",Negative
AMD,"Got one paired with a 5800x3d and can confirm Also cooled two other systems with an ak620 and  7800x3d and 9800x3d without any problems. So yes, air cooling is sufficient enough.",Positive
AMD,"Haven't exceeded 60C, I also have a Fractal Torrent full size case and my home doesn't deviate from 20C.",Neutral
AMD,"Unless the pump(s) and radiator fans are in another room, there are still sources of noise to be hunted down",Neutral
AMD,Air cooling is just water cooling with extremely less humidity involved.,Neutral
AMD,All cooling on earth is just radiative cooling with extra steps.  :-),Neutral
AMD,"Ya, AIOs can be a bit quieter, and have a different aesthetic look.  IMO they’re probably not the smartest choice, but I get them anyway. 🤷",Neutral
AMD,"> Liquid cooling is often quieter under load, so there's that if we're not only talking about cooling performance.  Which is great, but when you're gaming with headphones on who really cares about that?  Idk, I get the quiet thing. There's a reason why I built my server and HTPC to be as quiet as possible. And if my gaming rig was also a workstation that actually needed that power then I would probably do the same. But for just a straight-up gaming rig? Crank those fans curves and make tons of noise, your components will thank you later.",Neutral
AMD,Then there’s me who got a shitty AIO that is louder than the peerless assassin I sold,Negative
AMD,"Thermal limit is 95c and they're designed to actually sit at 95c 24/7.   being worried about temps sitting in the 80s and set lower thermal limits, is not normal.",Neutral
AMD,"Silent during idle, audible during gaming, but not to a worrying degree. Gets the hottest when I do AI art-creation. I do have the 9900X limited to 65W TDP, but if you look at experts' research videos on youtube, they found that you barely lose any prowess, but get the max temperature down a lot (my 9900X never gets hotter than 69-72°).  And I could further optimize it, haven't yet done all I could. I think it's a nice, quiet system.",Neutral
AMD,"I need to remember this   > A dual tower, dual fan air CPU cooler will generally have the best performance-to-cost value > a $35 dual tower air cooler like a thermalright ps120 is more than enough for any amd cpu",Positive
AMD,"...which is a different answer, since I was asking about that specific one.",Neutral
AMD,"That's exactly the reason I asked, although I have used a NH-D15 on a 5700X for transcoding - but that's a 65W chip in comparison.",Neutral
AMD,"No, for offline transcoding - for storage purposes.  GPUs are fine for on-the-fly.  Even after the RTX quality upgrade with the 2000 series it's still worse looking than CPU transcoding.  I'm not talking about watching it as it is happening or Twitch or whatever.",Negative
AMD,"Hah, yeah, I still have a 5950X and used to have 3090ti, temps were all good with air cooling everything. But as mentioned, upgraded the GPU this summer, and *had to* move to AIO as the new GPU runs *very* warm and made both the CPU and GPU temperatures way too hot. AIO solved this problem immediately :)",Positive
AMD,My 9800x3d will throttle on a phantom spirit se stock   It doesn't throttle with curve optimizer though   Edit: wtf is with the massive downvotes   I lose 10% cinebench score pbo + stock vs pbo + co + 200 with ps120   I throttle unless I use co,Negative
AMD,What wattage does this pull at full power (like a handbrake CPU encode)?,Neutral
AMD,With curve optimize I assume,Neutral
AMD,"I still think you probably *should* liquid cool a 285k, but you don't *need* to like you basically were forced to on the 13th/14th gen i9s. Some of the beefy thermalright and noctua air coolers definitely can handle the 285k. Although if you're doing really heavy multicore work for hours at a time, it will probably push into the 90s on air cooling.",Neutral
AMD,That much is true. However not all pumps are made equal.   Most D5 pumps are better built and have a lower noise output than AIO pumps. Even a $50 Freezemod D5 pump will run significantly more quieter than any premium AIO pump on the market right now.,Neutral
AMD,Yeah I have coil whine in both my GPU and power supply lol,Neutral
AMD,"Yeah, but 13 fans going 30% will be quieter than 3 fans going maximum overdive",Neutral
AMD,"I have an old gtx660ti 2 pipe heatsink and as an experiment I decided to hold a dual jet lighter against it (you know them lighters which make nice blue cones of flame).  The water from the combustion in the flames condenses onto the heat spreader about 5mm away from the edge of the flames. No amount of time holding them flames to the surface will cause the water to evaporate off again.  I can immediately take the flame off and put my hand to the point I'd been trying to heat for minutes and it does feel like it's warmer than air temperature but it's by no means ""warm"".   And that's without any fans on the heatsink.  Air coolers are just evaporation coolers really.  What matters is keep the air in your case cool. Which is why I have a positive pressure fine nylon mesh filtered 4 in, 1 out (to encourage exhaust out of the case on the peerless assassin 120 I use) mesh case. Dust finds it hard to get in, and it's easy to blow out, all the extra fans intaking make sure the case is always at room temperature and the exhaust encourages the airflow to go through the cooler even without the CPU fans.",Neutral
AMD,"> Which is great, but when you're gaming with headphones on who really cares about that?  People who don't use headphones",Negative
AMD,"I use open back headphones exclusively, even when producing. I don't mind the noise so much, but i could see how the noise from air cooling a 9950x3d would be a little loud for some",Neutral
AMD,Open back headphones exist,Neutral
AMD,"The GPU is probably the loudest fan in a gaming system anyway, so having a whisper quiet CPU while under load is kind of pointless unless you also liquid cool the GPU.",Neutral
AMD,"No it’s not. He said air is fine for all tasks, which would include video transcoding.",Neutral
AMD,"I still use NVENC for storage/rendering purposes of videos too, but most of the material I deal with is in 4K so maybe it's a bit harder to notice the CPU vs GPU difference compared to what you're handling. Otherwise for archival I'll use whatever format I received it in + smaller versions (again NVENC).",Neutral
AMD,"Thats strange, my 7800x3D will hit all core 4.85 GHz on a phantom spirit SE and top out at like 83 without curve optimizer. Its mid to high 70s with a -10 offset. I thought the 9000 series was supposed to be more power efficient and run cooler",Negative
AMD,9900x doesn't have the 3d part and has a different CCD layout.,Neutral
AMD,"You either didn't install the cooler correctly, or you did a shit job of applying thermal paste.  A 9800X3D will not throttle with a PS120.",Negative
AMD,Strange... I have a 9800X3D.   I cool it just fine with a Phantom Spirit 120 SE and the fans it came with.   Temps never go above 65c when gaming and never had to worry about using Curve Optimizer. And this is with a +200Mhz core offset.  Same goes for my friends 9950X3D with the same cooler.,Positive
AMD,My 9800x3d hasn't gone past 64C on a thermal right phantom spirit with no custom fan curves,Neutral
AMD,You're getting dunked on because one bot failed basic reading comprehension then people blindly followed lmao.,Negative
AMD,"Yup, fun times after paying the Noctua tax lol",Positive
AMD,"I don't even have my fans on a curve, basically 30% until something reaches 80° which at that temperature, something has gone really, really wrong.",Negative
AMD,"Hell, some systems are loud enough at full load that you hear them through open backed headphones.",Negative
AMD,and people who play games that are heavy loads but aren't loud.,Neutral
AMD,> the noise from air cooling a 9950x3d  Never in a million years would anyone convince me to stick an air cooler on a 9950x3D - Liquid Freezer III Pro 360 or go home.,Negative
AMD,You would be correct if I was asking about tasks - I meant that specific Thermalright cooler to the person I asked the question.,Neutral
AMD,"That's probably true regarding the 4K, I agree.  Typically CPU rendering ends up with a smaller file compared to NVENC, with higher quality to boot.  I keep falling back to Handbrake over alternatives, perhaps because I have custom presets I've made that I like.",Neutral
AMD,"It's not more efficient. The arrangement of 3d cache makes cooling more effective and allows higher max temps, but that is offset by using more power (for perf). Similar cooling scenario in the end.",Neutral
AMD,"I get 10% higher cinebench scores with pbo, curve optimizer, and +200 vs pbo + stock   With pbo+co+200, I hit 5425mhz on all cores at like 83c",Neutral
AMD,he said “any” amd cpu btw,Neutral
AMD,"The title says modern high end cpus, then like 9900x   That can include everything 2 chiplet basically",Neutral
AMD,"While I agree that the PS is usually more than enough for a 9800x3d, there are of course many other factors.  - Room temperature   - airflow and number of fans in the case  - A founder edition Nvidia 5000 series GPU blows its hot air up to the CPU instead of out the back. That could make a huge difference",Neutral
AMD,"I only see limitations in cinebench, never in gaming.   If I use curve optimizer, pbo, and +200, I can get 5425 on all cores with 9800x3d   But if I just use pbo, I end up losing 10% performance vs pbo, co, and +200",Negative
AMD,"> Temps never go above 65c when gaming  I mean, good for you I guess? When I bench CPUs I'm using Cinebench and Prime95.  I know that I will never achieve that kind of wattage when gaming, but I at least want to validate it to make sure it's not throttling under *any* conditions. Otherwise I wouldn't consider the cooler adequate.",Neutral
AMD,Try cinebench,Neutral
AMD,Yeah I don't get it. I have a fairly level headed opinion here.  Bots! Bots everywhere!,Negative
AMD,"I have mine on a curve but they rarely ramp up very high at all. I'm also on a custom loop with triple 360s and the loudest noise it makes is the occasional PSU fan firing up. I'm sure turning the front fans up a hair would fix it entirely, I just can't be bothered",Negative
AMD,"To which they already said, is more than enough for any amd cpu.",Neutral
AMD,"Yeah, 7800X3Ds were extra efficient because they kinda had to be, since the thermal path from cores through the cache chips to get to the heat spreader and eventually the heatsink wasn't great, therefore had to run somewhat conservative clock speeds and voltages compared to the non-X3D 7xxx models (and was also locked so you couldn't even attempt overclocking). 9xxxX3D has better thermal contact from cores through to heatsink so can run higher speeds and therfore worse out of the box efficiency with the higher power, but it can get more performance out of it.",Neutral
AMD,Damn that's pretty good,Positive
AMD,"""I'm planning to upgrade from my current AM4 socket CPU PC to a new one on the AM5 socket with the AMD Ryzen 9 9900X. """,Neutral
AMD,"To add onto this, the comment that started the chain said ‘any AMD CPU’, they didn’t specifically say ‘non X3D AMD CPU’.",Neutral
AMD,"Agreed, but these are fundamentals.  You may as well say air coolers air fine for all AMD CPUs as long as they have access to air for the cooling.  Even with the hottest of FEs exhausting straight at my dual tower I have no problems in my case. Best advice is to consider these factors and cooling method when buying the case, and vice-versa.",Positive
AMD,Just the 5080 and 5090.  5070 FE doesn't blow air upwards.,Neutral
AMD,> limitations in cinebench  And rightfully so... its a synthetic benchmark that will stress however many cores are available.   Temps on non-synthetic are always going to be lower.,Neutral
AMD,I do that too and I never reach anything even remotely close to throttling temperatures with my 7800x3D,Negative
AMD,"With ps120 and gelid phase change pad, co-30, pbo mobo, and +200 is 10% higher score than stock+pbo with 9800x3d for me   Final oc r23 of 24422",Neutral
AMD,"Max my 9800X3d ever saw in Prime95 was 85c with the PS120SE.   So you either have a low-binned chip, or something else is wrong like fan curve or something in the BIOS could be tweaked to better improve temps. Ambient temps also play an important factor. I keep my house sitting at 72F.   My max fan usage is 80% when temps hit above 75c. Between 20-50c, fans are 40% and 50% from 50-75c. Fans are barely audible.",Neutral
AMD,Nah I use real world work loads,Neutral
AMD,"Yeah I have a 420 and 280, as I love 140mm fans. My old EVGA power supply was the loudest fan but it was 10 years old so I replaced it with a fresh 10 year warranty EVGA unit and this one has a much quieter fan as it's 140mm as well lol",Positive
AMD,"No, they said air is fine for all tasks instead of the specific thermalright ps120 I was asking about.  It can be fun to argue sometimes, I realize.",Neutral
AMD,R23 score if 24422 which is top 1% for the chip   Gets loud with default fan curves though,Neutral
AMD,“a $35 dual tower air cooler like a thermalright ps120 is *more than enough for* **any amd cpu**”,Neutral
AMD,Thank you,Positive
AMD,"gaming is mostly single threaded, with one heavy render thread, and many lighter threads  cinebench is actually a benchmark to replicate the maxon rendering engine, which is used for cpu rendering of stuff professionally  It's a benchmark, but it replicates a specific real world workload",Neutral
AMD,Cinebench is actually a real world workload turned into a benchmark   Its rendering using the maxon rendering engine,Neutral
AMD,"I have one of those Super Flower ""High Density"" PSUs, so it's all packed in there, which isn't great for airflow, but it fits in smaller spaces. Whole thing's only 130mm long",Negative
AMD,"> a $35 dual tower air cooler like a thermalright ps120 is more than enough for any amd cpu  This is the comment you are referring to, is it not? They specifically, word for word, said  ""is more than enough for any amd cpu""",Neutral
AMD,"No problem, though I am baffled at the upvote/downvote counts on this comment chain. You're at like -20 right now, and even though I'm backing what you're saying I have +15? lol. Reddit is so bizarre. Glad IDGAF about the karma system, would be miserable if so.",Negative
AMD,"> specific real world workload  Yes, an all-core rendering workload.   Its not entirely representative of CPU performance for everybody. Especially not for gaming.   Its mostly used as a test to check for stability - checking for any abnormal behavior after an overclock/undervolt etc; and temperature - testing if your fan curves are set the way you want them for the temps you're looking for in an all-core scenario.  Something as simple as a change in temp of 1-2 degrees and your Cinebench score can swing a few hundred points, upwards of 1000 or more depending on how heatsoaked the PC is.",Neutral
AMD,"Last one I used was r20 years and years ago, what would you recommend for 2025?",Neutral
AMD,"...which is why I asked for task clarification and the person replied.  If you are new here, sadly the majority of the community mistakenly thinks gaming is the end-all be-all of anything.  Gaming is a freaking joke compared to some tasks.      I'm glad you've had a chance to catch up.",Negative
AMD,Thats bizarre lol,Neutral
AMD,"""Its mostly used as a test to check for stability""   It's a piss poor test for stability. You want linpack for stability testing. Linpack can find instability in 30 seconds that passes 24 hours of cinebench.",Negative
AMD,Cinebench is still fine for benchmarking,Positive
AMD,"compatibility looks fine. You may wanna bump the PSU up in wattage if you think you'll upgrade the GPU past the 9060 XT, you could probably get a cheaper SSD with comparable specs, if you have aceess to microcenter you should look at their bundles.  those are all nitpicks though. if you ordered it rn it'd be fine",Positive
AMD,How much is that total and where are you located?,Neutral
AMD,Ok thank you!,Positive
AMD,This costs around $1350 and I am located in Eastern Europe,Neutral
AMD,"Alright.  Overall it looks good. If you can save a little by going with another RX 9060 XT 16 GB then do that. Also consider spending a little more on maybe an 750 or 850W Gold rated PSU, that leaves more room for potential upgrades.  Might also be able to save a little on another SSD like WD SN7100, those Samsung PRO SSDs tend to be a little on the expensive side with no real benefit.",Positive
AMD,Noted.   Bumped the PSU to 750W (Corsair CX750).  The only available WD SN7100 is 4TB at the moment so alternatively there is a Kingston KC3000 that would save me around $35 over the Samsung,Neutral
AMD,9800X3D if it’s 99% gaming.,Neutral
AMD,Are you sure you need a CPU upgrade and not a GPU upgrade? 13900K is still a very relevant CPU.,Neutral
AMD,"For your use case: pretty much zero difference between both of them, as the extra cores of the 9950X3D wont be utilized for gaming. Go with the 9800X3D",Neutral
AMD,If you are just gaming then definitely the 9800X3D.  The 9950X3D is more for a workstation PC that moonlghts as a gaming PC.,Neutral
AMD,9800x3D is exactly what you need. The 9950x3D is a productivity CPU and offers no benefits for gaming.,Neutral
AMD,Gaming 9800x3D is the champion. Any kind of cpu intensive rendering or processing. 9950x3D outperforms. At least from all the reading and reviews I did before I bought the 9800,Neutral
AMD,9950x3d would be completely wasted on you   9800x3d,Neutral
AMD,"Don't pick the 9950x3d, I got one, and it's terrible,  The core parking bugs out constantly, I spent 2 months trying to find out what's wrong with my pc,  Only to find out it's the cpu, You need to do the strangest fixes to make it work,  And to top it off, It turns itself into a 9800x3d when you game,  So, there is no performance boost by having a 9950x3d,  And people say that procces lasso will change that buts not working for many people Inc me,  So please don't buy it, you will have issues",Negative
AMD,"If all you do is game, you choose the cpu that's better value for gaming, lol. Which is the 9800x3d, what are you confused about",Neutral
AMD,"I went through this same dilemma. If you are going to do any video, photo editing and such then go with 9950X3D, but if you are only gaming go with 9800X3D. I went with the 980p and am really happy.",Positive
AMD,"Can we ban this question, it’s asked every single day",Neutral
AMD,If its primarily for gaming go with the 9800x3d. You dont gain anything gaming wise from the 9950x3d and it costs significantly more so you would be paying for pretty much nothing.,Negative
AMD,Would the 9950 work better if you have an ultra wide up with a game in the middle and chrome windows open on both sides of the game window? Wondering if the extra cores would take on that load.,Neutral
AMD,"I started streaming with a 3800x cpu. 8c/16t. Discord open, OBS cpu encoding, chrome open, gaming and all that. Background tasks take up less than 1 thread.   I had a 7950x3d and went up to a 9800x3d. Still an amazing streaming and gaming setup.   The 9950x3d boosts like 200mhz higher than the 9800x3d but it can be complicated with so many cores and some apps and games not choosing the right ones. I had the threat scheduling program I cant think of the name, but the 9800x3d has none of those issues and is much cheaper and basically the same performance.   So theres no real worry you'll run out of threads on it especially when you take streaming out of it.",Positive
AMD,"If price isn't a factor then The question is can you use those extra eight cores, if you can't then might as well save some money if you can then get the 9950 X3D as it will perform about the same in gaming plus or minus a few percent as it is the same architecture.",Neutral
AMD,Go for 9950x3d so you can stream at the same time.,Neutral
AMD,"3.16 Bios listed as Validated!""",Neutral
AMD,"You have solids parts, idk why you need an upgrade.   Is it the prebuilt PC’s AIO fault? Is the Thermal paste dry & turned into, not paste?   Prebuilt’s tend to be built like shit, so maybe check the parts in your case. Fan & coolers alike.",Negative
AMD,"They are exactly the same for gaming, so 9800X3D is the answer. The 9950X3D makes sense if you do other things than gaming that actually benefit from such a powerful 16 core CPU. For gaming it would be a waste of money.",Neutral
AMD,"9800x3d: less heat, easier to cool.   9950x3d: the feel of having the best cpu for am5 (until the next am5 cpu comes along)  Most games do not load my 9950x3d for more than 10-20%. But there are exceptions. You will see the benefit of 9950x3d only in some games. The rational choice would be to get 9800x3d or some other x3d for cheaper now and then switch it to the next gen am5 cpu later.",Positive
AMD,9800X3D.,Neutral
AMD,9800x3d,Neutral
AMD,"If money is not problem, what about some time?  Wait for January and get the 9850X3D - 400mhz more plus likely another 200 OC.  They are effectively the golden silicon samples of the 9800X3D that can clock higher",Neutral
AMD,"As others have said, 9800X3D is for gaming and will perform on par with the 9950X3D with a less substantial price value. However, if you want to do other things other than gaming, such as video editing or heavy workload tasks, then get the 9950X3D. Either way, you won't have an edge on either processor in terms of gaming in whichever you choose.",Neutral
AMD,5.5k for sure if not higher.,Neutral
AMD,Get the 9950x3d so you never look back and wonder😁,Neutral
AMD,"If money not an issue get the 9950 , like they said they ran benchmark and they similar but for me it's better to have it and not use it then to need it and not have it",Neutral
AMD,I would recommend waiting for the 9950x3d2.,Neutral
AMD,Anyone think I will regret not going the Intel route? Like the ultra 9?,Neutral
AMD,"If not streaming, the 9800x3d",Neutral
AMD,"As a 7950X3D (workstation that I also game on) and 7800X3D owner (racing simulator) I can confidently say the 9800X3D is what you should choose for a 99% gaming PC.  The extra 8 cores of the 9950X3D get parked during gaming, so it's effectively a 9800X3D when gaming anyway.",Neutral
AMD,7800X3D - 9070XT / 5070Ti  9800X3D - 5080/5090  7800X3D Tray for 300€ and in 3-4 years go for Zen6/Zen7? 10-16 Cores X3D   They confirmed Zen6 for AM5 so why would you pay 461€ for the 9800X3D whos 15% faster then the  7800X3D?,Neutral
AMD,9800x3d would be a downgrade from 13900k.   I went from 13900k to 9950x3d when I had to rma my 13900k for degradation.,Neutral
AMD,"Yeah in all benchmarks 9800x3d and the 9950x3d are equal in terms of gaming. Only get the 9950 if your video editing, streaming or running several things in the back ground etc etc.",Neutral
AMD,"I’m using a 4090 I’ve had nothing but issues lately with the cpu, running at max temps, having to use intel XTU to drop down my cores to run any new game. For the last Warzone I had to go from 55 to 54 battlefield is essentially unplayable without continuing to drop the cores. Nothing else is wrong and I’ve read of people having the same exact issue.",Negative
AMD,Is the 9950x3d bad for gaming since its for workstations?,Negative
AMD,Until the rumored 9950X3D2 comes out:  [https://www.tweaktown.com/news/108480/amd-ryzen-9-9950x3d2-processor-wont-be-launching-until-q1-2026-probably-a-ces-2026-tease/index.html](https://www.tweaktown.com/news/108480/amd-ryzen-9-9950x3d2-processor-wont-be-launching-until-q1-2026-probably-a-ces-2026-tease/index.html),Neutral
AMD,technically shaders would load quite a bit faster if your cooler can handle it.,Positive
AMD,Running the latest BIOS? People have told me the issues with dual CCD Ryzens are solved but I keep hearing reports like this,Neutral
AMD,I have this processor and I have no problems,Negative
AMD,"no issue with my 7950x3d, which it should share the same principle with the 9950x3d.  Did u do a fresh install of Windows?",Neutral
AMD,And people are too lazy to spend 2 minutes finding the answer - https://gamersnexus.net/cpus/amd-ryzen-9-9950x3d-cpu-review-benchmarks-vs-9800x3d-285k-9950x-more,Neutral
AMD,"Unlikely to make any difference, apart from maybe a few % so not worth it.",Negative
AMD,"not at all.   9950X3D is a 16 core CPU.   Most games use only 4-6 cores. Very few use 8 cores, and only a literal handful could even manage to use more than that - if that was the case, then you'd already know if you needed a 9950.   Running 2 browser instances doesnt even register as a blip for even 1 core to be fully utilized.   Running an ultrawide display, is more about GPU capabilities than CPU. And with the way you're saying you want to use it as.... its never going to matter.",Negative
AMD,Should really make no meaningful difference since you'll almost certainly use the GPU (eg. NVENC) for encoding so the CPU barely has to do anything in relation to streaming.,Negative
AMD,"You shouldnt be using the CPU to render/encode your streams - is slow an inefficient.   GPUs have their own dedicated encoder that takes frames from the frame buffer that have already been rendered.   Most streamers, the ones that are actually gaming... are still streaming and gaming from their 1 PC and it does it very well with using the GPU to do the work.   Its fast, highly efficient, and unless the streaming platform you're streaming on even allows you to have a bitrate thats in the 10,000-12,000+ to run higher resolutions where the bitrate would actually matter.... CPU rendering is useless. The only streamers that even have such bitrates available to stream at, at the ones who are pulling in tens thousands of viewers each stream, whom are on a consistent schedule that justifies the platform in unlocking that ability. And even most of those aren't game streamers, they're talking heads with a mostly static image coming from their $800+ DSLR face cam.   The average streamer, even if you run a podcast that has 1-5k viewers every stream... are usually locked to 6000 and below. I think on Twitch you're limited to a bitrate of 3500 and only until you meet certain thresholds will they even allow you to hit 6000.    The only time anyone should be using their CPU to encode for streaming, is when you're running a dual-PC streaming setup so that 1 PC runs the stream, alerts, overlays and transitions, recording and transcoding - so that the gaming PC's resources are not taking away from the performance of the gaming PC. And even then in a dual-PC setup - its just better to be letting the GPU on the Stream PC to be doing all that work.",Neutral
AMD,"Hmmm, let's  think.  The Ultra 9 is on a platform that has no future, it's ""dead"", there is no opportunity to drop in the next gen Intel cpu.  The Ultra trails both your current cpu in many games and it trails many of the AMD cpu's.   The Ultra is obliterated by the AMD x3d chips.    The one advantage the Ultra has is in productivity apps.  The 9950x3d goes toe to toe with the Ultra 9 in productivity apps:  a little faster in some, a little slower in others.  The Ultra cannot compare with the 9950x3d in gaming.  The vcache side of 9950x3d is actually a 9800x3d. The 9950x3d also obliterates the Ultra in gaming, because, well it is a 9800x3d when gaming.  Both the 9950x3d and the 9800x3d are on the AM5 platform.  AMD will run its next generation of cpu's, Zen6, on the AM5 platform.  Reliable sources report that Zen7 will also be AM5.  If you go with a chip setting on the AM5 platform (e.g. 9950x3d), you have the opportunity to simply drop in a Zen6 then a Zen7 cpu and have a modern and relevant machine for another 5+ years.  Intel only dreams of this!  The price delta between the 9950x3d and the 9800x3d is about $200.  If you have any doubts, get the 9950x3d.   Better to have it and not need it, than need it and not have it.    I was considering a 9800x3d vs a 9950x3d, I went with the 9950x3d.   Precisely because I want to know I have the flexibility to do more than game.  From a price to performance ratio for gaming, the 9800x3d is the winner.  If spending an extra $200 for the best, the fastest consumer cpu on the market , is woth the peace of mind?  No brainer,  go with the 9950x3d!",Negative
AMD,"If you want a cpu for 99% gaming, then get the best cpu for gaming which would be the 9800x3d. I was in the same boat as you last year when I switched to AMD. I was always an Intel person. The learning the motherboard were weird to me, the ram was different. It seemed like a leap. You won’t regret it dude/dudette. It’s a solid choice.",Positive
AMD,Ultra 9 is slower than the i9-13900KF in gaming.,Negative
AMD,"Nope. I've always been Intel, but I'm switching over to AMD when I get my next paycheck.",Neutral
AMD,"There is nothing in your use case that would make the Ultra 9 a better choice than the 9800x3d. Same for the 9950x3d, the 9800x3d is the clear best choice and the second best choice would 7800x3d",Neutral
AMD,Its a dead socket.   So no upgrade path when Intel releases their next-gen CPU and Socket. You'd be forced to also replace the motherboard if you go with Intel. At least you can keep the same RAM sticks.,Negative
AMD,"Despite my issues with my 9950x3d  Its a 9800x3d when gaming, and you WANT a 9800x3d because it's the best cpu on the planet  Without all the darn issues the dual ccd Cpu's have,  I am an Intel guy and I am telling you  9800x3d is simply the best 👌   Intel for productivity  Amd for gaming   9950x3d if you want to suffer like me And please don't do that",Positive
AMD,"They are not ""parked"". They are only not used for the game process. Anything else can use them while gaming.",Neutral
AMD,I run a 5900X with a 5080 but use an ultrawide 1440 monitor. It's not as simple as this -> that.,Neutral
AMD,How exactly is that a downgrade when the 9800X3D is better for gaming?,Neutral
AMD,"Do these things in the background include multiple overlays for IRacing, music, and other software for numerous peripherals like pedals, wheelsbases etc?",Neutral
AMD,"I'd keep fiddling with it over getting a new CPU unless you really don't care about the money.   If you're going to switch to an 8-core CPU anyway, why not try dropping your 13900 to 8 P-cores, no hyper threading, and a bit of an undervolt. Your cooler is also relevant here but I'm assuming you have something decent?  I made the same changes to my 13700 and a client's 14900 and it's much better now.",Neutral
AMD,"Sounds like your CPU may have been degraded enough if you're having issues with it. 13 and 14th gen CPUs have 2 different issues - 1 is Intel letting motherboard manufacturers pump so much voltage to the CPU to run higher clockspeeds that they degrade really fast. So there were many BIOS updates to correct it, and also add the ""Enforce Intel Limits"" option to actually keep these CPUs operating how they were actually designed. The other problem was manufacturing defect - something about corrosion within the die so you'd have a permanently gimped CPU that no amount of undervolting and capping clockspeeds would ever help.   Might be time to RMA the CPU and get a fresh one from Intel - if yours is still within the warranty period. I would try this option first before trying to replace the entire platform. A brand new fresh CPU for free.... or spending $600-700 for a 9800X3D?  Like any corporation, they'll want to try to deny your claim, but if you pushback they're likely to replace that CPU.",Negative
AMD,"With the 4090 and some background apps, the 9950x3d might actually be worth the money if you don't care about the increased cost.     I hear you might need to manually assign the 3d cores to games though for maximum efficiency though, and I don't think it'll be any better in a meaningful way outside of not overdoing the load on your CPU with background using up assets",Neutral
AMD,"Used to run the same, except a 14900k, plagued with issues and had 2 degraded CPU's replaced with warranty in the span of 7 months, until I had enough and switched to an AMD R9 9950X, smooth sailing since.  Fuck Intel, CPU crashed mid windows update, corrupted everything",Negative
AMD,"Nope, it's roughly the same performance as the 9800x3d in games. The main issue is that the 9950x3d is way more expensive so there's no point spending that much money for bonus productivity that will not be used.",Negative
AMD,"No.   They perform identical in games.   If you're just using your PC primarily for gaming, there is absolutely no reason why you'd need a 16-core/32-thread CPU that doesnt game any better than the 8-core/16-thread 9800X3D.   There's also a nearly $200 price difference between them.   9800X3D retails for $459.   9950X3D retails for $649.    Again, just for gaming.... you really want to spend almost $200 more for a CPU that performs the exact same?  That extra $200 could get you into a whole new tier of GPU. It can mean the difference between a 5070Ti and a 5080. Or a 5070 and 5070Ti.",Neutral
AMD,They're the same for gaming. Here's some numbers if you scroll down: https://gamersnexus.net/cpus/amd-ryzen-9-9950x3d-cpu-review-benchmarks-vs-9800x3d-285k-9950x-more,Neutral
AMD,The rumored 9850X3D would be a better pick 99% gaming.,Positive
AMD,"Yep, I have tried everything,  I got it working in the end,  But it took me months, and im not bad with pc's too, It's 100% software issues  They just built the software in a retarded way, Its working though windows systems and Xbox game bar + mode  And if gamebar bugs out you need to change registry settings to fix it,  Widows bugs out regularly, changes constantly, Its just waiting for failure at this point,  I wish we had more control over when core parking would activate directly from amd software So windows won't screw with it",Negative
AMD,"Thats great, but have you checked the core parking in several games?",Positive
AMD,"I am really thankful that you want to help me out, But i luckily got it working in the end,  Reinstalling windows was a routine in my 2 months of trouble shooting,  I lived inside bios,  Ripped out components, Replaced all cables, keyboard , mouse, and monitor   Windows 24h2 and 25h2 with and without updates, Stock windows and custom debloated versions,  Tried every single driver from motherboard,  windows, and directly from the manufacturers,  Every single hack, tweak, and fixes found on YouTube, forums, and reddit,  In the end, it was a combination of faults in the bios, Windows being windows, 2 drivers that secretly wouldn't install, and the software AMD itself that's only  fixed by registry settings,  And after all that, it's sometimes still buggy only to be fixed by a pc restart  Luckily,the last one won't happen very often ...so far",Positive
AMD,"The driver works to tell the OS to park the cores unless they're actually needed.  You can easily see this behavior using either task manager or AMD Ryzen Master if you have a second monitor to display it on your while gaming.  The OS will unpark them and allow other processes to use them if they need it, but it tries to keep them idle if possible.",Neutral
AMD,"It’s an 8 core CPU. The 13900 is 8 Performance cores + 16 efficiency cores. It has 32 threads vs the 16 threads on the 9800x3d  Sure the 9800x3d will get a fps boost from the 3d cache when gaming but outside of gaming the 13900k is faster per core and overall. Just look at single threaded and multi threaded core each r23 tests that put the cpus to the test. For multi threaded the 13900k scores in the 38,000 range and 9800x3d scores around 23,000    For single core it’s 2100 for the 9800x3d and 2300 for the 13900k  So technically the 13900k is a faster more capable chip due to having more cores and a good single threaded performance. The only place the 9800x3d will be an improved is in gaming fps thanks to the 3d cache. This is fine if all you do is game and want higher FPS but beyond that, it’s a slower chip in every other way than the 13900k.  The 9950x3d is faster than the 13900k in every way, not just gaming FPS. I’d go that route if it’s in your budget (and I did from a 13900k). It’s a far better chip.   https://en.overclocking.com/test-amd-ryzen-7-9800x3d/",Neutral
AMD,Yeah multiple midi inputs/controllers while doing a collab in the background over discord. Plus trying to make edits as fast as possible in ableton with all those things going on I'll catch a lot of stutter I dont want anymore. Plus it should handle my version of premire pro like a champ. Gives the strength I need to keep the render process feeding towards my 3090,Neutral
AMD,It was a pre built cyber power I honestly don’t know what cooler is in it but I haven’t had any issues with it before. I’ve gotten kinda fed up with the whole trouble shooting it over the last couple of weekends lol,Neutral
AMD,"""some background apps"" amount to very little justification for having a 16core cpu. I think people overestimate what ""multitasking"" really means and think that you can only run a handful of apps before needing a CPU upgrade. RAM matters more to multi-tasking than CPU cores do - unless you actually use your PC to earn a living with - in which case, in a professional setting - yes, the 9950X3D is the BARE MINIMUM you'll want to use.   As a PC used 99% for gaming? No.   I have a 9800X3D and use my PC almost exclusively for gaming - occasionally i'll render or re-encode a video or two but im not running a game at the same time - I like being able to utilize my GPU at full-tilt of around 150fps when I'm re-encoding an 80GB 4K movie down to 10GB so I can watch some movies while I travel. I run multiple browser tabs, discord, a torrent client and sometimes Jellyfin.  The only other stuff running in the background is the typical list of windows processes that everyone has running.   Discord and my browser are chillin on my 2nd monitor and they're basically idle while Im gaming on BF6 or playing Expedition 33. Maybe I have a podcast playing... but that doesnt even fully utilize 1 core.   Unless OP is running a VM or two and/or is actually multi-tasking by letting a video or blender rendering, or some CFD program... a 9950X3D is not going to benefit him at all.  OP's use-case is similar to mine. PC mostly used for gaming with only a handful of typical programs running in conjunction with the primary focus being on a game.  The adage of ""unused RAM is wasted RAM"" would fit perfectly if you swapped ""RAM"" with ""cores"". Unused cores is wasted cores. If OP is only going to be using no more than 6-8 cores... the extra 8 cores are just going to be sitting idle barely registering as a blip.   Its also nearly $200 more expensive.",Neutral
AMD,I'm waiting for 9600x3d,Neutral
AMD,"I am not sure about that. It may cost close to $1000 and it probably only benefit very selected games that needs more compute than 8 cores can offer. In addition, there will still be a lot of context switching costs between the 2 CCDs.  That being said, I found myself to be a potential beneficiary of Ryzen 9 because I like keeping multiple low demand gacha games idling in the background (so I do not need to keep relaunching them). Although these games are both low demand and idling, they typically still fully occupy at least 1 cpu core",Neutral
AMD,"-update Bios and chipset drivers  -windows power settings - balanced   -Update gamebar and make sure it recognizes your game when you launch   To verify, You can pull up resource monitor, then go to gamebar settings and select “recognize this as a game” then when you close and relaunch the program the cpu tab will show half the cores parked.",Neutral
AMD,"I'm well aware of that, but OP stated they will use it 99% for gaming so they should really pick a CPU that does best what they do 99% of the time. That's why I would consider the 9800X3D to be an upgrade. If they did 50/50 gaming and productivity you could definitely argue differently.  I do gaming and quite a bit of other stuff that can benefit from a lot of cores so I would personally get the 9950X3D if I were to upgrade now.",Positive
AMD,"Well the 13900 is a very hot chip so unless you have a 360mm AIO, you're going to have issues running it at full capacity. That's why limiting the cores helps. It's reducing the heat generation so the chip doesn't throttle.  Regardless, if you just feel like replacing the CPU, board, and ram is easier, no worries!",Neutral
AMD,"I suppose fair enough, but the 4090 is full on top tier GPU of its generation and if anything will ever bottleneck a 3d chip at max settings, its one if the two most likely    Also seems like it'll fetch a better resale value if they ever want to upgrade in the far future since it'll be able to handle rendering, and easier to hold if they want to maintain top tier performance keeping it far into the future. $200 is also ram miney right now",Positive
AMD,Read my post closer.,Neutral
AMD,"Thanks for trying to help,  But like I said before  I already fixed the problem,  It was difficult to find even for people that knows what they are doing,  If it was this easy, it would not have taken me 2 months to fix,  I don't mean to disrespect or be mean to you, But I am just trying to tell people how much problems can come from the dual ccd Cpu's   I don't want people to need to become full on TI certified just to fix the stupid issues that's not even the fault of the customers  Just to play games.",Negative
AMD,At anything less than 53 cores I crash to bios in the middle of a game. Anything more and it runs but not for long.,Negative
AMD,"Meaningless because 9800X3D edges out the 9950X3D by slim margins, they typically trade blows even if the 9950 has a slightly higher clock speed on the cache CCD.   Typically good performing CPUs always hold their value much better: Look at the used prices of 5700X3D & 5800X3D - they're hitting over $300 and brand new stock that still exists is spiking due to high demand and no more production.   Its kind of a moot point when you're looking at it from a purely financial incentive.",Neutral
AMD,I guess I replied to the wrong person,Negative
AMD,For sure I feel you. I didn’t see that you had fixed it. This CPU should come with a guide or something so no one misses one of these important steps. I was surprised I had to seek the information out through other sources,Negative
AMD,"Sorry, 53 cores?  Regardless, it sounds more like you have a bad part rather than a situation where you need an upgrade.  If you have your bios up to date and set to Intel default limits, you can set P-cores to 8, E-cores to 0, and disable hyper-threading.  If your PC is still shutting off or crashing, it could be the cooler, the CPU (Intel 13th/14th gen failures, or even the SSD (because it's crashing to bios).  More info:  https://community.intel.com/t5/Mobile-and-Desktop-Processors/July-2024-Update-on-Instability-Reports-on-Intel-Core-13th-and/m-p/1617113#M74792  https://www.pcworld.com/article/2901830/finally-the-real-reason-why-ssds-are-disappearing-in-windows-11.html",Negative
AMD,"The 13th and 14th of intel CPUs are known to have some issues, just update your bios and you'll be fine.",Neutral
AMD,"From what i understand the 9800x3d wins by having less latency between cores, hence why it performs better with a lower clock. Most games also don't utilize more than 8 cores, so the 9950x3d also doesn't typically have a way to take a lead and ever so slightly underperforms as the rest of them don't get usage in game even if you make sure all 3d are utilized    The difference is also small, though, and if you ever want to upgrade you'll have a wider market to sell too as a gamer won't think twice about a 9950x3d over a 9800x3d, and animators/researchers would also probably be happy with a older model.     I'm not going fo say I didn't have doubts or I think i know better than you, but why worry about an extra $200 with a GPU that you couldn't catch for less than 1.5k if it let's you handle everything the more competitive model can, then heavier more professional loads today with potential to help later if theres ever a bottleneck?",Neutral
AMD,"Its all good mate,  I totally agree with you, Its so retarded that you have to happen to know   That you need to use Xbox game mode, gamebar and  NOT change the power scheme off balanced at minimum,  At maximum you need to Update and Change multiple settings in bios,  Install the processors Gpu driver and also before the chipset driver as otherwise it will secretly not install 2 drivers without telling you,  And become a bug fixer when at somepoint the  gamebar will screw up by directly change registry settings or build a Bat file ...  And thats not even all the things I could list,  Its completely bonkers",Negative
AMD,"Because why should OP spend $200 more money for a CPU that will go under utilized, when the 9800X3D will suit his use case? Especially when he said 99% of the time the pc will be used for gaming...  You're going into this under the impression that PC hardware is somehow an investment... like a car, its a depreciating asset. It MIGHT hold more value, for a bit longer. But nobody buys hardware thinking how much money they are going to get back when they sell the hardware later. Or.... in the chance that MAYBE OP's use-case might change... which it would just be easier to upgrade to the latest and greatest at whatever time in the future that he needs something that suits his needs better.   Like I said elsewhere, that $200 can be spent elsewhere thats more meaningful... like going with a higher tier GPU, or more storage space. Or not spending it at all.",Neutral
AMD,What is the problem with the old one?,Neutral
AMD,startup loop.,Neutral
AMD,"Looks good! You're paying extra for some of the components, but that's inevitable if you're focusing aesthetics, especially on a white build. HAVN is very new to making cases, but I've heard good things about that one. Are you going with an AIO for the cooler?",Positive
AMD,"I would drop to a 7800X3D and A620 mobo if you don't care about overclocking. A 9070XT doesn't need a 9800X3D.  swap the SSD to a 990 Evo Plus.  drop to 2x16gb ram unless you have a specific need.  Swap to 5070Ti, you'll be way happier in the long run. Though the savings above should get you to 5080 in budget.  Rest is aesthetic preference.",Neutral
AMD,Looks good. What cooler are you using?,Positive
AMD,You need some cheese.. chedder..,Neutral
AMD,"It's not bad, but you can likely do it for cheaper than you currently are without compromising on performance or build quality.",Positive
AMD,"Water cooler: NZXT Kraken Elite 280 RGB 2024 - AIO CPU Liquid Cooler - 280mm Radiator - F280 RGB Core Fan - Customizable 2.72"" IPS LCD - NZXT Turbine Pump - AMD® AM5, AM4 - Intel® LGA 1851/1700, 1200/115X - White  Thanks",Neutral
AMD,"I'm doing a bit of other work outside of just gaming. I plan to get back into some 3d modeling this winter, along with doing some intense data modeling with Python. I think I might need the additional overhead for that.",Neutral
AMD,"Ah, I didn't post that — going AIO -> NZXT Kraken Elite 280 RGB 2024 - AIO CPU Liquid Cooler - 280mm Radiator - F280 RGB Core Fan - Customizable 2.72"" IPS LCD - NZXT Turbine Pump - AMD® AM5, AM4 - Intel® LGA 1851/1700, 1200/115X - White",Neutral
AMD,all components already bought -> going with the white build because my girlfriend isnt going to like a glowing black box sitting in the office lol,Neutral
AMD,It might come in handy but 32gb is enough for that. I get by on 16gb doing similar work.,Neutral
AMD,"Yeah, this is a very powerful, expensive build. You could probably build this for cheaper. But, if you're happy with the price, go for it.",Positive
AMD,"Yeah, I get that — with the work I do on my Mac, I'm normally hitting the 306 GB limit and having to deal with SWAP memory. I feel like the 64 GB is a good top end for my future workloads, especially considering Windows is generally not as well optimized as Mac. If anything, I can return it by Jan 31st with the holiday return policy and it's \*easy\* enough to swap out.",Neutral
AMD,Can you elaborate a bit? What do you mean by building it cheaper? I felt like these were moderately priced (considering tariffs and all that).,Neutral
AMD,"It's hard to say when you don't list the prices you paid. But, here's a general breakdown.  * Asus motherboards are often overpriced compared to options from MSI or Gigabyte. * There are cheaper white cases * Corsair RAM is more expensive than options from Patriot or Silicon Power and don't perform any faster. * That fancy cooler with a screen won't perform any better than a $35 Thermalright Phantom Spirit. * Lian Li PSUs are expensive compared to options from Montech like the Century II. *",Negative
AMD,appreciate you breaking it down for me like that — I'll consider that,Positive
AMD,"Not €300, which gets you a 7600+AM5 mobo   You may as well sell your existing CPU, mobo & RAM to fund some DDR5 and get on AM5",Neutral
AMD,"$250 USD would be the max imo.  I got it during the $150-$200 era, but that was short lived.  $250 is a fair price.  But given it's the 2nd fastest CPU on am4's longlived life, the price of it has gone up a lot. People don't want to spend $450 on upgrading to am5. So some are willing to spend $300 for a 5700x3d.",Neutral
AMD,"How desperate are you to stay on AM4 and not just upgrade to AM5? Because the 5700x3d is absurdly expensive now.  MAYBE around 200 euros is it worth it. You get ryzen 7600 performance and don't have to change all your parts but costs slightly more than a ryzen 7600. 290 is just too much, that's basically a cpu+mobo.  Also we don't even know what mobo you have now, it may not even handle a 5700x3d fully if it's some A motherboard. Sure it'll work, not at 100% though.",Negative
AMD,"DDR5 prices are skyrocketing at the moment. The math has changed. Now a jump to AM5 isn't as cheap as previously. The 5700x3d in gaming is still an excellent choice. If you don't have the money to go AM5, you won't probably find much better.",Neutral
AMD,About $220.,Neutral
AMD,Nothing because upgrading to am5 is better.,Positive
AMD,200-250 at most,Neutral
AMD,$200,Neutral
AMD,"235 euro , got one from ebay nowhere else was available except ebay or aliexpress",Neutral
AMD,"Ideally not a lot more than 200€, since it is only as fast as a 7600(x), which costs about 170€. The only benefit is that it saves you from buying a new motherboard and ram, so paying more isn’t that much of an issue. But it shouldn’t bring you too close to am5, which imo it does at 290€.",Neutral
AMD,250 at most,Neutral
AMD,"£120 is what i paid, but I'd say £150 is the maximum. Any more and you can sell whatever you already have (if you're doing the legendary 5600x to 5700x3d upgrade) and use all the money to go am5",Neutral
AMD,When they were on sale in the 200s CAD on AliExpress. Now it's not worth it. Just spend the equivalent on a 7700x.,Negative
AMD,It would be better to save another $100 on top of this cost and upgrade to AM5. You can get a cpu + mobo for that price. The extra hundred is because of the cost of ddr5 ram at the moment. So for 400 or so you can upgrade to the am5 platform. If you're wanting to stay am4 I'd stay away from the x3D chips unless you find an insane deal on it.,Neutral
AMD,Absolutely nothing. Rule 11,Negative
AMD,It was around $180 USD on Aliexpress nearly 2 years ago.,Neutral
AMD,At a certain point it becomes cheaper to move onto an AM5 platform.,Neutral
AMD,If AMD somehow could produce a 6800 X3D or w/e for AM4 they could probably charge like 400 euro at least for it.,Neutral
AMD,"I bought mine for 200 a year ago. It's not a bad CPU but i feel that if you are aiming for 100-140 fps it doesn't have more than a couple more years in it, so it wouldn't make too much sense buying it now and for more. Unless your budget is really inflexible or your GPU wouldn't really enable high FPS anyway i'd save up for am5.",Neutral
AMD,"Not too much, considering I should ditch AM4 and hop on the 7800x3d for $280. I'd have to buy RAM and a board but at least you aren't stuck on a EOL platform.",Neutral
AMD,Buy a 5800XT for 1/3 the price instead.,Neutral
AMD,"No, that's way too much. At that price, you might as well just upgrade to AM5. The whole point of the 5700x3d was because you can get 7600 performance with the benefits of 3d v-cache for around $200 or less, while staying on your existing AM4 platform.",Negative
AMD,$150,Neutral
AMD,I got a 5700x3d for a friend for 180€ on ali a year ago. Do their prices really are that high now ?,Negative
AMD,"I paid £200 a little while ago, second hand. Personally I'm so freaking glad I made the switch. The uplift paired with my 9070XT has been amazing.   I really don't think I'd be willing to spend anything more than that.",Positive
AMD,I paid that in poland for 7800x3d,Neutral
AMD,I wouldn't pay more than what a 7600 goes for these days.,Negative
AMD,I just bought one (in a bundle) for essentially $150 around a week ago.,Neutral
AMD,I got one off AliExpress for $141 like a WEEK before teriffs hit the states. Haven't seen them anywhere close to that since,Negative
AMD,You can get a 5900x for 150-175 used. That's the way to go at this point unless you want AM5,Neutral
AMD,"I‘m in the same position as you are right now. Considering buying a 5700x3d/5800x3d or 5800xt.  All lot of people in the comments seem to be a bit off the EU pricing on components. You‘re not getting an AM5 system for the price of a 5700x3d.  The cheapest solid system I can get in Germany is running me at 420€ (w/o shipping and more importantly w/o cpu cooler) and that System only has a 7500f. 7600x would be +40€ and 7600x3d would be +145€. Because my system is still running fine, inchoate to wait until I can get a good deal on a used 5X00x3d cpu on eBay or somewhere else.   As to your question: is it overpriced? Absolutely. Ist it worth it? Depends on how much you would gain (you didn’t write what your upgrading FROM) and if you could spend more on an upgrade? I‘d say if you’re not into high fps 4K Gaming you could stretch an 5X00x3d until AM6",Neutral
AMD,Max the Ryzen 7600 / 7500F money +- 50€.,Neutral
AMD,"Yes, this is overpriced. The Ryzen 5700X3D is no longer produced. For 300€ you can already get an AM5 CPU + MB + RAM.",Negative
AMD,$100-150,Neutral
AMD,75 max,Neutral
AMD,This is the way! 7600x performs very similar to 5800x3d.  5700x3d makes zero sense at that price. Get decent b650 board for 120-150 and 7600x cpu. Ram is extra but you will be future cpu upgrade ready if/when needed.,Negative
AMD,"I have MSI B450-A Pro Max (7B86-022R) and 32 GB DDR4 Ram and 5060 ti. The problem is I’m a complete PC noob, I don’t trust my self that I can change my motherboard alone.",Neutral
AMD,"Both 5800 X3D and 5700 X3D have been absurdly expensive since release with the exception of a few flash sales in my country. Everyone wants to milk the last performance they can from their AM4 platforms.  I just bought a 5800 X3D for 3000 SEK (\~315 USD), after hunting for one for quite some time, they usually go for 3500 SEK or so. I wouldnt have done it if I didn't want to play BF6 with decent FPS and on top of that I got a high-end AM4 motherboard (X570 MSI MEG ACE) with high performing DD4 ram, so I got the opportunity to milk every last frame from my AM4 platform.",Negative
AMD,my 5800x3d just died.... looked at the priceses and was like hell no im going am5 plus my 4090 needs a bit more cpu power...  (I was really hoping to upgrade my living room cpu to it :( ),Negative
AMD,"Changing all my parts is too expensive lol, I haven’t been in the pc space for a while but damn ram alone is double what it used to be.",Negative
AMD,Forgot to add: an 5800xt might a great option. Still can be had new from vendors for about 170€ 👍,Positive
AMD,"I came from a 5 5600, just bought a 5060 ti but now I’m CPU bottlenecked in CPU heave games like BF6.",Negative
AMD,Nah 3d cache is massive in certain games. Can't compare,Neutral
AMD,Switching MB is not hard. Watch some youtube videos. You need proper Philips (?) screw driver and two hands.,Neutral
AMD,Which CPU do you currently have?,Neutral
AMD,"With YouTube, there are literally *thousands* of videos that'll show you how to do it step-by-step. It's really not difficult. Just take your time.   Spending real money on an AM4 platform if you don't need to is a waste of money.",Negative
AMD,everyone's a noob at the start. Trust me it's not hard,Negative
AMD,"Swapping the CPU is the ""hardest"" part when changing mainboard + CPU + RAM. If you would do that anyways on AM4, you should be able to change a motherboard with the help of a video guide",Neutral
AMD,"Dunno about europe, but the 5700x3d were $180-200 for months in mid 2024. $180 was an amazing deal and people were idiots to ""save"" like $40-60 and go for a 5700x/5800x instead but in their mind it was still ""30% higher cost for only 10% performance"". Stupid way to evaluate CPUs, based on % price difference versus gaming performance.",Negative
AMD,"300€ is too expensive anyway, don't feed the scalpers. If you are adamant on staying on am4, you might get a 5800 x or xt instead.",Negative
AMD,"I guess it depends where you are. Took a look at newegg and amazon, an AM5 build with an entry level board is $365 with 32gb 6000 c30.  Depending on what you're replacing, say, 5600 + B450 + 32gb ddr4 should at least sell for $230, so you're spending $135 for the upgrade.  A 5700x3d is 240-260, minus 80-100 by selling the 5600, $160 for the upgrade.",Neutral
AMD,Same as me. I‘m still happy with my 5600x but I will upgrade to a 5700 or 5800x3d if I find one around the 250€ mark. Don’t know how long it will take but unless ddr5 starts to drop or the b850 boards come down in price an AM5 upgrade will be double the cost.,Neutral
AMD,"Does appear in CS2 that the 5700x3d is better. Though I think I’d still go the route of 7600x and trying to tweak it + memory timings, it’ll be close, and atleast you’re on a newer platform.",Neutral
AMD,"Some games yes, modern titles like BF6 performance is very close. Not that 9800x3d is close to 7600x but 5800x3d is close to 7600x. Older arch is not as great, cache helps but not in all games.  EDIT: here is some “proof”  https://www.techspot.com/review/3021-ryzen-5800x3d-cpu-gpu-scaling/  Only 4 games, but it paints a picture. Warhammer is cpu heavy game.",Neutral
AMD,"And maybe a new Windows license if yours is tied to you motherboard (e.g., prebuilts)",Neutral
AMD,"5 5600, in games like BF6 big CPU bottleneck",Neutral
AMD,"The cheapest I ever saw one was 2999 SEK new (315 USD once again), and that was from flash sales that sold out in hours. Granted that was when it was the best gaming CPU availble. Normal price was around 3999 SEK for most of the time.   But yeah, the used market for 5800 X3D have resulted in bidding wars everytime I saw one. AMD probably had a somewhat bad percentage of CPUs that actually passed the bar for being a 5800 X3D combined with a insane amount of people that had a good AM4 motherboard wanting to upgrade.  And maybe they fucked up the spread somewhat, trying to oversell in the US compared to Sweden. Since the shortage have been insane here.",Negative
AMD,"I currently have a 5800x, my pcs pretty good for am4 if I want a significant improvement I’m gonna have to do a whole new build",Positive
AMD,For something like world of warcraft the 3d cache is upwards of 100 fps,Neutral
AMD,"BF6 is a game where AM5 offers a pretty big uplift.   If that is your main game, I'd save for AM5.",Positive
AMD,"It's very, very rarely worth buying a new chip with the same architecture.   Just go AM5 at this point and sell your board/ram/cpu as a bundle to someone",Negative
AMD,"not worth it from a 5600 at 290 euros. MAYBE if you had like a 5070 Ti and playing at 1080p/1440p, but the gains for some $300 GPU won't be worth it at all.",Negative
AMD,"[https://se.pcpartpicker.com/product/3ZKscf/amd-ryzen-7-5700x3d-3-ghz-8-core-processor-100-100001503wof?history\_days=730](https://se.pcpartpicker.com/product/3ZKscf/amd-ryzen-7-5700x3d-3-ghz-8-core-processor-100-100001503wof?history_days=730)  2449 KR, still not a great deal though relative to the US since that's $260. We just have cheaper tech prices historically. Of course those days could be long gone. Hell I think it was like 2 years ago that you could get a 7800x3d+mobo+ram for $500 at microcenter, and nowadays the same bundle is probably $550 if they do ever offer it.",Neutral
AMD,I got mine from INET for 2000 SEK like a year ago. 5700x3d. It was from their christmas calendar If I remember correct,Neutral
AMD,Yeah then don't change anything and save that money for a future am5 upgrade.,Neutral
AMD,For 290 euro though… way overpriced. 7800x3d just makes more sense for a bit more and selling the old parts.  If it was cheaper I would agree to just go that route.,Neutral
AMD,"You mean “generation”, and I agree. As to architecture for gaming PCs, be it Intel or AMD, it’s all x64 these days :)",Neutral
AMD,"5700 X3D is not the same, people want the best just because it is the best. But yeah, that is definitely a decent price that is not availble anymore :D",Neutral
AMD,"Oh agreed, i just wouldn't compare a non x3d to it because of price. In some games sure 7600x is good, but others I'd rather save up more and get the 7800x3d",Neutral
AMD,Looks like a good build but I'd personally go for a more powerful power supply as if you ever upgrade the GPU it's going to need more. Maybe lower the motherboard to a 650 rather than 850 model and spend the difference on an 850w power supply.,Positive
AMD,"To answer your questions:  * Yes a more powerful GPU would be better, but it's not required. The 9060 XT and it's equivalent 5060 Ti are okay for 1440p gaming, just don't expect max settings, and be ready to use FSR/DLSS upscaling. 9070/5070, 9070XT/5070Ti would be ideal of course.  * I also use a B650 mobo and it's fine for what it is. Get a fancier mobo if you want better IO like 10gbit/20gbit/40gbit USB 3.2 Gen 2x2 or USB4/Thunderbolt.  * The 9700X could be worth it for just 20 euro more, if you plan on rendering/encoding video or anything else that likes higher core count CPUs.  * 750W is more future proofed if you, in the future, want to upgrade to an RX 10070 XT or RTX 6070 Ti or similar. 850W would be even better.  As a bonus tip - please consider the Odyssey G50D instead of the G51C. The G5 G50D is IPS and 180Hz. The one you picked is VA, which has worse motion clarity and viewing angles.",Neutral
AMD,"Thank, I'll have a look at the 850W power supplies.",Positive
AMD,P.s. Yes if your budget can stretch further try and get a 9070 or even 9070xt,Positive
AMD,"Thanks for all your answers!  I don't really need the additional IOs so maybe I can spare some budget by choosing a B650 mobo and invest in a 850W power supply instead.  I don't plan on anything like video encoding, so I guess I can keep the 7600X.  Thanks for the bonus tip, I was not aware of the difference between VA and IPS, I will have a look.",Positive
AMD,Good luck. The Samsung Odyssey monitors are quality too. You should have a lot of fun with it. Also I'm sure you know but just in case don't pay full price for windows 11. You can buy a key for about £15 from various places like Groupon.,Positive
AMD,"Before buying a 850W PSU consider if you'll be upgrading in the next 5 years. If not, and you upgrade in 5+ years, it's probably better to get a new PSU then anyways, as they tend to perform worse as they age. I wouldn't trust a 5+ year old 850W PSU to handle a system with a 300W+ GPU (who knows, maybe the RTX 7070 Ti will be a 450W part and the minimum will be 300 for lesser GPUs of 2030) and everything else on top of that.",Negative
AMD,"AMD just has the better gaming chips atm. Intel also has/had serious problems with the high end chips killing themselves over time.   For tasks where you need high core counts Intel might still be the better choice, for gaming AMD is usually better overall and at set price points.    But it depends on your needs and budget",Neutral
AMD,If you are just gaming and browsing the internet.  AMD fo sho.,Neutral
AMD,x3d AMD baby,Neutral
AMD,"Largely, yes. There are still situations where an Intel unit is a better choice, but those are somewhat rare. If your intended usage is ""general use/gaming"" then yes - AMD is almost certain to be the better choice for your needs.   As to why, Intel basically has been on a case of the ""fuck-its"" for a WHILE now. They were making money hand-over-fist while Apple was using their units, and AMD didn't really have much with which to compete. So they would just do an almost yearly, ""here's our latest release. It's slightly better than the last release."" But they lost their edge for innovation, and didn't put enough time/money/effort into making sure that their fabs were cutting edge. Then Apple dropped them and they lost a HUGE customer. Shortly after (or maybe almost simultaneously), AMD released Ryzen which was their first legit competitor in quite a while. It still wasn't *quite* as good as Intel's best at the time, but it was getting better. And because AMD was fabless, they could use the latest fab technology and not have to implement it themselves. So while Intel was more-or-less stuck on their endless refinement of the 14nm process in 2021, AMD was releasing Ryzen 5000 on 7nm. Intel did somewhat come back with their 12th gen CPUs, but then stumbled again as their panic releases took them in continually bad areas. AMD then released their 3D V-cache series and then for gaming they basically left Intel in the dust.",Neutral
AMD,"Yes, and you have a much longer windows of upgrades. Intels doesn't support their sockets as long as AMD.   But, avoid AsRock boards if you ever plan to put an X3D chip in it.",Neutral
AMD,"There is practically no reason to go for Intel unless it is heavily discounted, or if you’re doing extremely heavy rendering/productivity.   AMD wins in basically every category right now though, and they lose in productivity/rendering by such a small margin, only the really maximalist professionals would pick Intel.  If you’re going for a gaming pc AMD clears. The X3D chipset is really impressive, but its price is also imposing. You will get a great experience with the AM5 chipset but X3D is definitely the go to if you can afford it in your budget.",Negative
AMD,[https://cdn.mos.cms.futurecdn.net/ck86DgAJZmSd2VC8TuvXJJ-1200-80.png.webp](https://cdn.mos.cms.futurecdn.net/ck86DgAJZmSd2VC8TuvXJJ-1200-80.png.webp),Neutral
AMD,Yes. Intel is dead.,Neutral
AMD,"Yeah  Only if you get a 14600kf for dirty cheap, otherwise, go AMD",Neutral
AMD,with rumors that the next few generations of CPUs being compatible with am5 mobo it's gonna pay it self off easily. and the x3D chips are great but at premium costs. either way you're saving money down the line when you're ready to upgrade,Positive
AMD,"Performance, cost, value, power-efficiency, and upgradeability.  Depends on your exact needs though. There are some reasons to buy Intel CPU too. It’s not as bad as 2015 when there was basically zero reason to buy AMD.",Negative
AMD,"Yes unless there's some amazing intel sale which I dont know of any currently, but the 14600k was $150 like a month ago and included BF6 so you could not beat that deal.  AMD is fastest for gaming and has actual upgrade path. Intel is perhaps best if you need the very best multicore performance for something.",Positive
AMD,Yes,Positive
AMD,"AMD is in a strong position lately because Intel has just dropped the ball for years in terms of reliability and efficiency. Yeah, their best parts will edge out a 9950X and 9900X and do it a little more cheaply, but Intel has an ungodly stink on their brand and their current socket is basically EOL. AM5 is going to be here at least through Zen 6, and Zen in general is only going from strength to strength.",Neutral
AMD,"In the last 6 months, Nvidia has been having driver issues and has been pushing a business focus on AI for about a year now. While their GPU processors are good, AMD has caught up and the AMD overall board designs and drivers are competitive or even better value/performance if you're not spending at the very highest level.",Neutral
AMD,"Was for me, Went AMD in the first time in forever.",Neutral
AMD,Yup. AMD cpus are more powerful and use less watts than Intel cpus.  Just built a 9950x3d machine. Love it.,Positive
AMD,Price to performance theyre the way to go. Am4 socket chips are still decent and pretty cheap.,Positive
AMD,"Intel might appear to have better performance, but it's reliability is so poor with the chips it's made in the last few years, that it isn't worth it.",Negative
AMD,"AMDs CPUs outclass intels by a noticeable margin except in relatively niche circumstances.  You'll want to aim for a socket am5 cpu. If you can swing it. A 7800x3d or 9800x3d are both really good. With the latter being the newer version of the former.    AMDs ryzen cpus are more energy efficient, better performance, and because of all that, better value.  There are some niche uses where an intel chip is a better choice... Such as video transcoding for home server/media stuff. But that's about it.",Positive
AMD,"Yes. The 7500f, 9600X, 7800X3D, and 9800X3D are the best price-to-performance options on the market right now, and AM5 has incredible upgrade pathing.  Some of the Intel 12-14th gen chips still have good price-to-performance, but they have zero upgrade path potential beyond the 14900K.",Positive
AMD,"I just built my first pc in like 9 years, went for a 7800X3D and I hope it will serve me atleast 9 years like my last pc",Positive
AMD,Yes.,Positive
AMD,"Yeah, they are basically the go to for everything at this point. Intel really fell off hard. They had a huge lead and fucked it up hard.",Negative
AMD,I built my first amd rig since the athlon II days this year; can't complain.,Positive
AMD,"YES - AMD all the way.  Better performance per watt, better value for your dollar.  x3d variation for gaming.",Positive
AMD,Superior in every way outside of some specific productivity uses.  Intel does command a slightly higher price to effectiveness ratio because of how much they've lost this gen.,Negative
AMD,It's for all. Especially temp and power usage. Intel just got left in the dust.,Neutral
AMD,"AMD, in general, is just as good as intel for everything.  Their X3D CPUs are better for gaming than intel CPUs, but come at a premium.  The non-X3D CPUs are just on par.  That said, AMD's AM4 socket had a long life and AM5 will have at least one more generation of CPU before the switch to AM6.  Intel's current socket is done and will be replaced next year.  Most people recommend AM5 because of the potential to upgrade the CPU in the future is perceived to be higher.  If you are not someone who upgrades CPUs, it isn't really an issue.  Intel's core Ultra 2s are price/performance competitive (for the most part) with their AMD counterparts (excluding X3D for gaming).  Also, intel got a black eye with the way they handled the voltage spike degradation issue in the 13th and 14th gen K series CPU.  It isn't like AMD didn't have a somewhat similar issue with 7800X3Ds the year before (they did, but they addressed it a bit quicker and a bit more openly than intel did).  Also, on the upgrade front, recently there have been a few threads where people have asked about upgrading their AM4 CPUs and people are telling them to switch to AM5 for upgrade-ability, which kind of defeats the purpose of buying AM4 for upgrade-ability five or six years ago.  So, once AM6 comes out, the same people that are telling you to buy AM5 for ""upgrade-ability"" are going to tell you to just switch to AM6 when you ask them what AM5 CPU you should upgrade to 😉  My take is that if you are someone who has to have the latest and greatest and are likely to upgrade your CPU in the next year or two, go with AMD and AM5.  If all you do is gaming and you can afford the X3D premium price, go with AMD.  Otherwise, go with whatever you fancy.  By the time you upgrade, you'll be replacing the motherboard and memory (DDR6!) anyway.  Disclaimer: I have owned stock in Intel (I got out before the s#!t show started) in the past and currently own stock in AMD.  So I'd really prefer you to buy AMD.",Positive
AMD,"Setting aside any other functionality of the chip, because right now AMD is very competitive and in a lot of cases just better than Intel, let me give you the main reason why you should ONLY build your computer with AMD, and NEVER buy Intel if you can help it, with the ONLY exception being if someone is giving you an Intel motherboard and CPU for less than $100...   **Motherboards and CPUs are expensive.**  Even my used x870 and 7600X, on sale, cost me over $300: and you don't want to have to spend this kind of money every couple years just to upgrade the computer.   **Intel**, for at least a couple decades, has been doing anti-consumer crap where they purposefully make a new socket every couple years, forcing you to buy a new motherboard if you want to upgrade your CPU, with methods as absurd as arbitrarily changing the pin locations on their sockets so that even though the socket is functionally the same the two CPUs won't work in it.  There was even an absurd situation less than 10 years ago where you could upgrade your CPU by just using tape or bits of metal to arbitrarily cover or bridge connections to make the motherboard think it was a different CPU, because apparently they were cutting performance based on that.   **AMD**, meanwhile, just released another round of CPUs for the AM4 socket, which already held the record for the longest supported socket: everything from the ryzen 1000 series to the 5000 series works on that socket, meaning if you have a computer with a Ryzen 1500, you can upgrade it right now to a Ryzen 5950 X3D, assuming the power control supports that wattage.  Note that they came out with the AM5 socket a couple years ago, meaning ***AMD is still making new CPUs for a socket they aren't even pushing anymore.  Even the first AM4 socket computers can upgraded to support Windows 11 without buying a new Motherboard, CPU, and RAM.***  The AM5 socket is charted to be similar, with the 11000 and 13000 series slated for it.   Once again, **motherboards and CPUs are expensive.**  You don't want a CPU socket you can't upgrade.  Even if Ryzen processors weren't performing better than Intel processors, even if there was some known problem with them (like there is with Intel right now), if your motherboard has an AM5 socket, you could always go buy a newer one in a couple years.  Personally, I have a plan to replace this older Ryzen 5 with a 9800X3D or even a 11800X3D when the price gets dropped by newer CPUs coming out... And I won't have to replace the motherboard to do that, because I got a mid-range X870 with full PCIe5 support, meaning it was MADE to handle those CPUs to their full potential.   This is in contrast with my last few Intels I got on deals: the CPU no longer cuts it, and there's no option to upgrade, sometimes even within the *same generation.*  With my AMD?  No more problem.  If the CPU isn't cutting it (and it is, because the 7600X is excellent for the price point), I just go drop another couple hundred on a newer and better one.    This is just basic math: future me will be able to afford a newer CPU WAY sooner than a full-on motherboard, CPU, and potentially RAM.   Even if there's some reason to upgrade the motherboard, I can do that without buying a new CPU, because it's all the same socket.   **Side Note:**   I run Linux, which AMD has better support for in both CPUs and especially GPUs, so Fuck Nvidia, I'm recommending AMD for GPUs too.   **TLDR:** Don't make the mistake of boxing yourself in like I've done in the past, go with the company that is making sockets that last, and spend a little more on the motherboard now so you aren't buying a new one in a couple years.  YOU WILL SPEND LESS MONEY TO GET MORE.",Neutral
AMD,"If it's for gaming and you have $$$ and pair it with RTX 5080/5090, AMD is the go to with their 9800x3D/7800x3D.    If you have a more mainstream card you can go take the 8 core Ryzen 7700 Trail new sealed at around 150$/€ on Ali and it's the best bang for bucks.    If you do productivity then you look for benchmarks of the programs you use, here Intel's CPUs are better in some scenarios.",Positive
AMD,"Anyone got any info on how this tracks for a build for an after effects and blender workflow (not gaming)? Intel chips have always been drilled for after effects stuff, but I’m open minded. I priced a build with an i9 ultra, is this stupid or would I be able to do the same with an AMD chip?",Neutral
AMD,In my anecdotal experience its been great. Runs a little hotter and ramps up like a mofo (intentional design choice) but the performance for the price point is great. Only thing is its not great with ray tracing but I dont care too much about that.,Positive
AMD,"I think it really depends on the person. This will probably get me downvoted but for 90% of people if they are gaming and browing the web it does not matter to a large degree.   So a goto chip for most people is whatever you can afford and want to put into a machine. If you want to build a intel machine if you put in a decent intel cpu and decent gpu. You will still enjoy your games just fine.   Now if you are the gamer who has the frames clock going in the corner, and is dropping in a 5090 and researching the tightest ram timings, and getting the best of best nvme drives then sure get get get your amd 9800x3d or what have you and be happy.   But really just make smart choices for your budget and you're going to end happy playing games you want to play.",Neutral
AMD,"But a lot of other stuff Has changed. Now you can by a perfectly compatible motherboard and CPU and by now you have to jump through hoops to upgrade the BIOS before the CPU boot. You may find that motherboard qualified RAM May have to go through an initial memory training, maybe more than once. And 10 years ago did you have to worry about Intel or AMD publishing specifications you relied on that were actually Factory overclock with some CPUs not even lasting as long as the Factory warranty.",Neutral
AMD,"Yes, just don't go X3D with an ASRock motherboard.",Neutral
AMD,"For gaming, AMD dominates everything but the *very* low end, where the Intel i3-12100F is somehow still around for <$90 nearly four years after it launched.",Neutral
AMD,"Intel is in rough shape. Their current gen performance wise is at best even with last gen and sometimes even worse. They did a imo stupid rebrand with none of the names making sense anymore. They had a ton of CPU's killing themselves with too much voltage or a manufacturing defect. They somehow managed to lose the server market to AMD which is something no one thought would be possible. They had to be bailed out by Nvidia and the US government because things were looking that bleak for them.   And then for gaming AMD's 3D's CPU's just stomp whatever Intel comes up. Intel has yet come up with answer to it despite having almost 4 years to do so. AMD also uses way less power and has better upgrabilitiy. Sort of a no brainer to do. Better performance, less power, and a better upgrade path.  It is basically backwards compared to ten years ago. AMD for the high end and Intel if you are on a budget.",Negative
AMD,Cpu? Amd is great choice. Gpu choice is mixed.,Positive
AMD,No because amd just stopped supporting rx 5000 and 6000 series,Negative
AMD,"For my cost-conscious build a while back, I went with an AMD CPU (5800X) and a nVidia GPU (4070).  I still think those are in a nice spot for performance and cost.",Positive
AMD,AMD is always the go to. Fuck Intel and their Evil Empire bullshit.,Negative
AMD,9800x3d,Neutral
AMD,"ryzen X3D are the best for gaming by a huge margin.  Outside of gaming the 265K is good, and so is ryzen, to the point you should go for preference and price/perf ratio",Positive
AMD,"Intel are still good, especially for workstation purposes, there’s just not much to be excited about and that blunder with 13th and 14th gen didn’t help",Negative
AMD,Yes AMD all the way.  Stay away from Raptor lake 13X00 and 14X00 Intel CPUs (esp higher end models),Neutral
AMD,"For gaming, AMD's X3D chips are the move right now, better performance per dollar and way more efficient.",Positive
AMD,"As long as your purpose is gaming, its not even up for question. Amd also has good server processors but thats a diferent line of products. Intel consumer cpus on the other hand are somewhere in between, jack of all trades master of none. Its pretty much that simple. Buying intel right now means that you either dont have much understanding about technology, or you are just a loyal fanboy who doesnt mind buying inferior products for extra price.",Neutral
AMD,"I always buy the best value for my money, not the brand. Compare performance of the hardware you can afford and chose the best one.",Positive
AMD,"Yes, people here are going to push you towards AMD.  They all hate Intel now for some reason.",Negative
AMD,"Yes, AMD and Intel have swapped places in the last 10 years.      The x3D perform better in most games.  They're not too expensive.  AMD keep their sockets longer giving more upgrade paths and they're running cooler.",Positive
AMD,"Yes. Have you noticed how the i3/i5/i7/i9 line is no longer in production? *That's* how badly Intel dropped the ball in recent years. Genuinely looked like they'd get bought out until nVidia threw them a lifeline.  That said, the Ultra 3/Ultra 5/Ultra 7/Ultra 9 line seems ok. Noone's bought one to find out since they're all overpriced but in theory they're respectable.",Negative
AMD,"It's because Intel CPU's were blowing up and the AMD x3d chips are the best for gaming by a significant margin.  If you need something that is a beast for productivity, Intel can be back on the table. But something like the 9950x3d is close to the best of both worlds (gaming and productivity) - but very expensive.  If you only really cared about productivity, Intel might still be the way to go with higher RAM speeds, etc.",Positive
AMD,"AMD basically blows intel out of the water. Nvidia is still superior in high end GPUs, but AMDs offerings are plenty fine for 95% of the population.  I personally perfer AMD for everything. My last build is all AMD and had very few problems if any there were mainly related to drivers in 2020.",Positive
AMD,"Intel is still my go to. I bought Arrow Lake, very satisfied with it. Can recommend",Positive
AMD,"Stock vs. stock yes.   Tuned 14900k vs. tuned 9800x3d? No, the 14900k wins most of the time.",Neutral
AMD,"There is just genuinely no way to justify buying intel unless you're doing something like video editing, rendering, etc. for a gaming machine AMD wins in every single category that matters: price, stability, efficiency, futureproofing.",Positive
AMD,"If you don't plan on doing any AI related development work or plan on making use of virtualisation software then hell yea I love my AMD chip, it's an absolute beast but obviously my opinion is based on which AMD cpu you end up going with. It's less of a problem nowadays, I'm a couple models behind currently.",Positive
AMD,I've used AMD CPUs for years now.  Still won't touch AMD GPUs after bad experiences with ATi and AMD. It's Nvidia or nothing there still...,Negative
AMD,Any higher end chip from either will give you more performance than needed for gaming. So with that said I personally use intel because its more familiar to me.,Neutral
AMD,In 10 years we wont know who will be the best. /s,Neutral
AMD,"AMD has a lead with their 3d cache technology which significantly helps frame rate in low resolution gaming.  Intel's 14900K was (initially) the king of last generation.  Trading blows in gaming benchmarks with the 7800x3d, while having double the productivity/rendering.  Then some people discovered that the CPUs were failing.  The cpus were running at too high voltage and damaging themselves (degrading)  By all reasonable accounts, it was about 5% of the chips, which is *way* too many, but not as ultimately devastating as the techtoobers would have you believe.    Intel released microcode which limits voltage spike potential, and offer 5yr warranty on these parts.  They did the right thing, but due to the nature of the problem people don't really know if it was solved for good.  Many people suspect they will continue to degrade although we haven't heard any recent reports to this affect, so probably fixed.  The 9800x3d introduced a lead over the 7800x3d (and 14900K) and simultaneously Intel released a new generation of CPU that is great for productivity, but worse than their 14th gen at gaming.  In the span of about 1.5 years they went from top dog to ""budget king"", much like AMD has been since it's inception.  Because of the fab situation, people generally don't see how Intel is going to catch up.  That said, a 14900K or 14700K on deep sale is going to be the best all around gaming/productivity value.  And the core ultra series (also heavily discounted) is a top performing productivity processor at bargain price.  Unfortunately, Intel just couldn't keep up with TMSC fabs and 3d cache technology used by AMD. (Intel's ultra series outperform AMD's best in productivity, so the 3d cache gaming advantage is really what put them ahead in most redditor's minds.)",Positive
AMD,"AMD's X3D chips are an insane value for gaming performance and tasks with a similar workload. For most other things intel is quite competitive and for professional workloads can actually be the better value outside of the top end chips.  As long as the pricing makes sense, no one would fault you for building around a 285k or 265k, for example.",Positive
AMD,"The whole time my daily rig was on AMD, I had XEONs doing dev, finally said F it and I’m back to Intel, 13900K 5.8 GHz boost is pretty boss",Positive
AMD,"No, Intel has some great products,  really depends on price and use.",Positive
AMD,"Intel has two niches where they are better. One is creative work due to Intel having codec support for video workflows. The other is low wattage computing with their N series chips. Outside of that AMD is generally better performance per dollar.   As for fear of Intel burning themselves out, that’s a fear with the 13 and 14 series chips, haven’t seen anything about their core ultra chips. So if you see a cheap used 13/14 series beware. Nowadays the worry is asrock motherboards burning out your AM5 cpus.",Positive
AMD,"Right now, there no huge difference, AMD performs better single core, Intel performs better multi core.  Some will say that Intel 13th and 14th Gen i7 and i9 has that overheating problem that would fry the CPU over time and that's the reason to buy AMD, but not anymore, as it was fixed by Intel with a BIOS update.  So right now, just remember to update your BIOS and you shouldn't have a relevant performance difference between equivalent processors from Intel and AMD, so choose by which one has the best price to performance  And remember that LGA1700, the Intel equivalent for the AM5 socket supports both DDR4 and DDR5, so that's important if you're on a budget",Neutral
AMD,"Buy whatever is affordable. You wont notice a difference.  People pretending they go so much more out of the 9800X3D vs the 9700X are coping hard.  And people claiming intel are just bad forever likely don't understand that Intel can be cheaper in some cases. And still perform just as good.  Benchmarks show that, with acception to highest end CPUs, per watt and per penny, Intel is just as good.",Neutral
AMD,"For sure, if you want to buy another chip every couple years. IDC what anyone says, they die fast.  I could still be using my i7 6700k right now if I had to. Intel chips live forever. Of course that's unless your a 13th or 14th gen lol  Let the amd shills start the downvoting",Neutral
AMD,"Every time I open this sub it’s like I stumbled into an AMD tent revival. “Ryzen is the second coming! Praise be to 3D V-Cache!”  Let’s get real. AMD’s chips are *great* no question but half the people screaming about “Intel being dead” couldn’t tell a P-core from a power bill. They’re parroting benchmark thumbnails and pretending it’s gospel.  The truth? Both brands have been leapfrogging for *years*. AMD wins efficiency and heat right now, sure. Intel still slaps harder in raw single-core speed and productivity loads. Depends on what you’re doing and how much you like watching Cinebench bars go up.  Everyone acting like AMD cured global latency is the same crowd that’ll be crying when AM5 boards get replaced in two years and suddenly “value” means buying new RAM, new socket, new everything again.  Build what fits your rig, not what Reddit’s cheering section tells you. The only real “fanboy win” is when your FPS counter doesn’t drop faster than AMD’s stock price during a GPU shortage.",Positive
AMD,People keep exploding on me for saying that the 7800x3d is a gimmick lol the 7700x is almost identical for 100$ less. A lot of people don’t understand that at 1440p and 4k your GPU is the bottleneck. I have a 7600x w/5070ti and my cpu doesn’t go over 30% when I game 4k ultra. AMD’s marketing is clutch bro🤣,Neutral
AMD,"I have a 7800X3D in my gaming rig but bought a Intel 265K for my studio (audio engineer) because while it might suck for gaming, it was a great buy for audio work. I think I found it for around $230 or so at the time which was great.",Positive
AMD,"The 9950x3d is the best consumer cpu on the market today.  It is both the best gaming cpu (equal to the 9800x3d).  It is the best productivity cpu as well (on par with the Core 9 Ultra).  Just saying, you can have it all in one package.",Positive
AMD,Gaming mostly. Thanks.,Positive
AMD,"Amd is better at higher core counts too. Just not cost competitive. But it tends to draw less power, which is nice.   I guess it depends on the use case.",Positive
AMD,"Intel fixed the issues with a microcode delivered through a BIOS update, so just update your BIOS and an Intel CPU should perform just as well as AMD",Neutral
AMD,"Unless you need absurdly high core counts, then you are back to AMD.",Neutral
AMD,Is amd bad for rendering?,Negative
AMD,What if I'm just designing and drawing and editing and typesetting and gaming and browsing the internet?  What then?,Neutral
AMD,What is Intel better at these days?,Neutral
AMD,So Intel is better for what? Production?,Neutral
AMD,Ryzen was released way before Apple dropped Intel.  The ryzen 1600X - 1800X competed with the Intel 8600K - 8700K.  I got a 8700K because the first ryzen had mediocre single thread performance and while amd beat Intel in multithread that generation they weren't allround better.  I'm not sure exactly when apple dropped Intel but AFAIK there have been 10th or even 11th gen MacBook.  At that time Intel was (about to go) up against zen 3.  Zen 3 (the 5000 series) was where amd went from a good alternative to outright better in many situations.  They're still not better in all though. The 13900k that I'm running is a fine chip and it could certainly handle itself against amd when it came out.  For gaming the x3D chips are best but they only have 8 cores and are mediocre workstations. They're not all rounders in that sense.,Neutral
AMD,"You also didn't really mention how the 13th and 14th gen intel chips had issues with degradation, basically burning themselves up in a way that slowly destroys their ability to run at high speeds, and by the time it starts causing crashes its too late to take preventative measures.   Sure they've released software updates to prevent some of those issues, but if someone doesn't know, they'll probably only find out when their chip is already a gonner",Negative
AMD,Do you have a mobo recommendation?,Neutral
AMD,"Ultra 7 is effectively $166 at microcenter with a mobo combo, 250 at new egg with a mobo combo.   14600k is still $160 right now at walmart.",Neutral
AMD,This is definitely a gaming pc first and foremost. Do games use multi core?,Neutral
AMD,Good to know. Gotta read up on some builds. Thanks.,Positive
AMD,"Oh I remember very well having to upgrade mobo and ram because I wanted a new CPU several times in my PC gaming past.  Thanks for the long response, appreciate it!",Positive
AMD,">Buying intel right now means that you either dont have much understanding about technology, or you are just a loyal fanboy who doesnt mind buying inferior products for extra price.  Or you want to do something that an AMD CPU can't do or does worse.  If someone wants to use google play games, choosing AMD would be a pretty dumb decision.  And just because you (generic you) don't use it, or arbitrarily classify them as not ""real"" games, doesn't mean other people don't have a use for it.  That is just one example of why someone would choose Intel over AMD that has nothing to do with not understanding technology or being a fanboy.",Negative
AMD,This is like saying Mitsubishi is still my go to over Toyota. Just insane to say.,Negative
AMD,"This guy has his fingers in his ears going ""la la la"" at all the things wrong with Intel right now",Negative
AMD,Because you are factually wrong. The 7700X is not almost identical. Literally just look at benchmarks between the two. The extra cache is extremely helpful.   You also are picking parts totally wrong.  You are only bottlenecked if you choose to be bottlenecked. Graphical settings are adjustable but there's not really anything you can do on the CPU side if you are lacking performance. It is a hard ceiling.  You should choose your GPU for the graphical performance you want and the CPU for the FPS you want. Just pretend the word bottleneck doesn't exist since it is completely irrelevant when choosing parts.,Negative
AMD,Intel quicksync also kicks serious ass if you are streaming content (e.g. plex server),Negative
AMD,The 7800x3d was killer for its price. Absolutely insane. I've got one and I don't think I've ever really pushed it.,Positive
AMD,"I'm interested in this. Not read anything about the Intel being better for audio latency and related drivers, so went AMD. Please fill me in.",Neutral
AMD,Isn't assigning cores/core parking still a problem?,Neutral
AMD,Can't beat the 7800x3d pricing atm,Neutral
AMD,An X3D makes sense depending on GPU and resolution. Other times a non-X3D makes more sense for performance-per-dollar. A real recommendation can't be made without knowing more.,Neutral
AMD,7800x3d then,Neutral
AMD,"If you can afford it the 9800X3D is an absolute beast. I have it combined with the 9700XT and I rarely had such a fast, quiet and stable gaming machine. And that is coming from Intel 12th+13th gen and 3080/3090 FE.  But feel free to go Nvidia for the GPU if you want.  You do not need to spend a ton on the board. Buy the features you need.",Positive
AMD,Then just find the best value AMD X3D chip at your budget and don't worry about the rest of the market.,Positive
AMD,"This is true to an extent, but needs more context. At the top end, Intel has more cores than AMD, but not all cores are created equal. Even back in 12th gen, Intel started using big.LITTLE, so you do sometimes get more cores, but it's a lot of small, single threaded cores that often don't clock higher than 4.5GHz. With the Core Ultras, Intel has now moved to single threaded cores across the board, so core count is even less equal to throughput. For example, the 285K has 24 cores, which *sounds* like more than the 16 of something like a 9950X. However, that's 24 cores with 24 threads instead of 16 cores with 32 threads. Also, only 8 of those 24 are performance cores that clock at 5.7GHz. The other 16 core max at 4.6GHz. Meanwhile, the 9950X is 5.7GHz across 32 threads. For anything that can actually make use of all the threads, the 9950X will *destroy* a 285K. The saving grace for Intel is that only happens with very specific workloads. In many cases, even 8 cores and/or 16 threads is more than enough for any productivity task. It's just important to realize that core count doesn't tell the whole story anymore.",Neutral
AMD,Haven't AMD pushed more cores than Intel with the entirety of Ryzen? And aren't AMD are using fully fledged cores rather than Intel's mish-mash of power and efficiency cores to inflate their core counts.,Neutral
AMD,"No, they are excellent at that too",Negative
AMD,"The post was about CPUs, and in that regard AMD has been blowing intel out of the water for a few generations now.  Rendering GPU wise, no, AMD is not bad per se, but nvidia is better, specially at the profesional level. That said if you're not doing it full time for work or anything like that and it's more of a hobby, or you just messing around with different things, AMD does the job, generally at a lower price than the nvidia equivalent.",Neutral
AMD,"AMD CPUs are for the most part just as good when it comes to CPU heavy production tasks. AMD GPUs are also great for gaming and can do rendering, but nvidia GPUs are still better for most GPU production tasks because of the cuda cores.",Positive
AMD,"No, it's just that Intel jamming in efficiency cores makes them really good at brute forcing raw compute performance for the price tier",Positive
AMD,"The 9950X3D is currently at the top of consumer CPUs in the Blender benchmark.  The only CPUs better are Threadrippers, EPYCs, and Xeons costing thousands of $$.  But when you look lower, intel has the edge in price/performance.  I suggest you check benchmark results for your rendering software of choice.  There is no one blanket statement that says one brand is definitely better than the other.",Positive
AMD,Intel and AMD are much more evenly matched for workstation uses like video rendering (which I'm assuming is the kind of rendering you mean) than they are for gaming.,Neutral
AMD,"Not at all, but Intel has some advantages in Premiere thanks to Quick sync and iirc better multicore performance. Puget systems has productivity benchmarks that show how well GPUs and CPUs perform in rendering and the like.",Positive
AMD,AMD is king of rasterisation performance per watts.,Neutral
AMD,"No they're not bad, but I'd it's all you do th 265K is unbeatable value",Negative
AMD,No but generally speaking that's GPU dependent,Neutral
AMD,"Don't GPUs do the rendering? Generally speaking, go AMD for CPU and Nvidia for GPU.",Neutral
AMD,9950x3d,Neutral
AMD,Getting people to waste their money,Negative
AMD,NAS,Neutral
AMD,"Production, servers, older single core games, games that are not cache bound.",Neutral
AMD,room heater (i have intel),Neutral
AMD,"Single core performance, which is a non-zero consideration, especially for older games that only use a couple of cores.  I'd get a better framerate out of _Fallout New Vegas_ if I'd gone Intel.  I'd also be filled with regret the rest of the time, though.",Neutral
AMD,"Home NAS or anything else that requires idle-ish 24/7 operation, as idle power consumption is quite poor on AMD.",Negative
AMD,"Single core performance for software, higher core count options (that’s not server)  , media encoding , lower idle wattage ( on average)  It’s why we buy Intel",Neutral
AMD,"> Ryzen was released way before Apple dropped Intel.  Fair enough - my memory was such that it was around the same time. But it looks like Apple dumped Intel in 2020 and Ryzen came out in 2017. Still the point stands - Apple was a HUGE customer for Intel, and losing them meant that they had a lot less money to throw at R&D like they had done in the past (i.e. the last time AMD was kicking their ass, they supposedly spent more in R&D than AMD's entire market valuation and somewhat violently took the performance crown back from AMD with the Core2 series).   And I believe I covered that the initial Ryzen releases weren't as good as Intel's options at the time, but they were a VAST improvement over Bulldozer. The point I was making was that they could rapidly improve their designs because they could use other company's fabs and didn't have to deploy new process tech on their own, having spun off Glo-Fo years earlier.   > The 13900k that I'm running is a fine chip and it could certainly handle itself against amd when it came out.  Right. That's why I mentioned that Intel recovered somewhat with the 12th gen, the 13th gen would be included in that (if you somewhat ignore the degradation issue with Raptor Lake silicon). Their rapid slide, however, continued with the panic release of the 14th ""gen"", which was basically an overclock of the 13th gen silicon.   > For gaming the x3D chips are best but they only have 8 cores and are mediocre workstations. They're not all rounders in that sense.  ...Right. That would be why I mentioned that ""for gaming they basically left Intel in the dust."" And also why literally the second sentence I wrote in that initial comment was that ""there are still situations where an Intel unit is a better choice, but those are somewhat rare."" If you're building a workstation to primarily work on Adobe platforms, for instance, an Intel CPU is still the way you'd want to go (unless you decided to go with an Apple workstation).",Neutral
AMD,COFFEE LAKE GOAT 🐐,Neutral
AMD,I have an MSI B850 Tomahawk. Fantastic board but can be a bit pricey.,Neutral
AMD,"Been a while since I built mine, so it's an Asus TUF Gaming X670E-Plus, been pretty happy with it. Used non-qvl G.Skill sticks in it, but their ""twin"" was in the QVL, only difference was that mine was without RGB - Runs Expo Advanced without issues. I'm also running an undervolt on my Ryzen 5 7600x by combining curve optimizer and curve shaper, so speedwise I'm running something in between a 7600x and a 9500f - but at ecomode wattage. You ofc. don't need to tinker as much as me, but it's just to say that it's been stable for me.   If I was to build today I would probably pick Asus TUF Gaming B850-Plus Wifi. Nice feature set\*, Wifi7 etc and decent price, price may ofc. differ depending on your location in the world.  \*decent audio chip too, I specifically liked a similar chip about my board:   Realtek ALC1220P 7.1 Surround Sound High Definition Audio CODEC   \- Impedance sense for front and rear headphone outputs   \- Internal audio Amplifier to enhance the highest quality sound for headphone and speakers   \- Supports: Jack-detection, Multi-streaming, Front Panel Jack-retasking   \- High quality 120 dB SNR stereo playback output and 113 dB SNR recording input (Line-in)   \- Supports up to 32-Bit/192 kHz playback\*   **Audio Features**   \- Audio Shielding   \- Premium audio capacitors   \- Dedicated audio PCB layers   \- Audio cover   \- Unique de-pop circuit  yet another edit: Only thing I'll say about Asus boards is, do not install their ""Armoury Crate"" software - it's bloat and also caused annoying issues.",Positive
AMD,"Do you live near a microcenter?   From what I gather, avoid any ASRock especially on AM5.   I've had an Asus TUF AM4 and it has been flawless.  Plan on building an AM5 with one in the next few weeks too.",Neutral
AMD,"Games basically only need 8 cores and often they're using far less than that.  go ryzen, microcenter bundle if nearby one.",Neutral
AMD,"A few games utilize 8 cores, some utilize 6 cores, but the vast majority of games utilize 4 or less cores.   Any CPU with 8 or above should be plenty for the foreseeable future.",Neutral
AMD,"For games, the per-core performance and cache and memory performance are more important. Games use multiple cores but that’s not a major distinguisher between the current latest desktop CPUs.  But better than thinking about those factors is to look at reputable performance benchmarks. https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/17.html  No need to get attached to one CPU or brand. Be open to optimise your list once you’ve got the first draft and a better understanding.",Neutral
AMD,"There's a lot of things wrong with Intel but arrow lake is solid. The 265K is great price/perf, low-ish power draw (half a 13900K for the same-ish performance). They aren't killing themselves like last gen. The only thing truly better is 7800X3D and 9800X3D and those would be up to OP based on need (gaming-only, mostly.)",Positive
AMD,I haven‘t experienced issues with Intel in decades. But I have experienced issues with AMD in the 2010s. Thing is that Reddit is shilling AMD so anything Intel releases is heavily criticized.,Negative
AMD,Bottleneck-calculators have been a thing for a long time. Try it sometime,Neutral
AMD,"What? The only difference is the cache? It’s not like you’re adding 2 more cores to it. At 4k ultra your GPU IS taking the brunt of it. I have a 5070ti and 7700x RIGHT NOW and am not going over 30% cpu use at 4k ultra setting in many AAA titles. Bottleneck, Do you know what that means? It means there is an essential part of your PC slowing down another. In this case we are talking about the cpu bottlenecking the gpu. You don’t need the overpriced x3d to achieve this. I don’t see what you aren’t getting  Edit: whoops I meant 7600x. Still proves my point further with the 7700x not being your bottleneck.",Neutral
AMD,"But it's not needed really, I run my large Plex server on 9950x3d and transcode using my 4090 and it doesn't even break a sweat.",Neutral
AMD,I just upgraded my 3700x to a 7800x3d and its insane the difference. Im still using my 3090 and ive been running BF6 maxed out @ 1440p with a near constant 130+ FPS.,Positive
AMD,"It was just a much better bang for the buck considering no one else really wants these new Intel chips. I can run like up to 200 tracks with moderate DSP, which is crazy. I've also heard Intel handles audio better than AMD, but I have no idea if this is true.",Positive
AMD,"Not at all.  The core parking problems were with the 7950x3d.  They were solved before the 9950x3d and 9900x3d launched this year.  I've had no problems with my 9950x3d since April, when I got it.",Neutral
AMD,Microcenter has a bundle right now that gets you the 9800x3d and a b650 mobo for like 530. Pretty good deal.,Positive
AMD,Errr I wouldn’t say that no…  7800X3D or 9800/9900/9950X3D yes…  There are others I would research very carefully before saying yes to… the 5800x3D for example I wouldn’t recommend now as it’s on a dead platform and cost isn’t worth it. 7600X3D performance isn’t worth it either… 7900/7950X3D still have some performance limitations that may affect the value proposition…,Negative
AMD,"> At the top end, Intel has more cores than AMD  I was about to say how you were wrong, but then I looked it up. Intel now has a 288 core CPU so does, in fact, beat AMD's 192. But when I was talking absurdly high core counts, this was the range I was in.",Neutral
AMD,Was thinking of going for 7700 from my r5 3600,Neutral
AMD,"Intel video rendering is better than AMD video rendering because of quick sync, but AMD is better at 3d rendering.  This sub is mostly for gaming, but for things like transcoding/video rendering Intel comes out on top (e.g. NAS setup).",Positive
AMD,"And his wife? To shreds, you say.",Neutral
AMD,No not really compared to Nvidia and cuda,Neutral
AMD,"AMD doesn’t blow Intel out of the water in CPU rendering. They trade blows at the high end, and Intel tends to be ahead in the midrange, especially considering that QuickSync can be better than NVENC/VCN in some cases.",Positive
AMD,"i put a system together a few months ago, i was debating between the 9700x and 265k. not really a gamer, i only play WoW really. the 265k i got for $259 has been amazing.",Positive
AMD,"I don't really get why it's so important for you to affirm all the things you got right. I wasn't trying to put you down or anything just pointing out the discrepancy between the ryzen release and the apple debacle.  Apple was a big customer but when Intel was dropped fully the M series of arm chips was already displacing them. It was still a huge blow for sure but it was never the real reason Intel ended up struggling. It was a symptom of the real problem and not the cause.  The real problem wasn't entirely complacency either even if they sure were milking consumers. The biggest issue was a combination of factors - mostly the natural monopoly emerging with tsmc while at the same time a short sighted but genuine sunken cost fallacy prevented Intel from going EUV.   I mean genuine as in, Intel was so much ahead that when they needed it EUV wasn't ready so they spent a metric ton of money maxing out multi patterning for the right reasons. EUV wasn't ready and they had to do something else in the meantime.   When EUV finally was ready, tsmc started buying EUV while Intel had enough (and at the time still competitive) production capacity with multi patterning. They were basically still making the money back on those investments.   The problem was when multi patterning hit the wall EUV was still accelerating and they were a mile behind at that front. They should've realized that they had to bite the bullit on it earlier, but this is hard when it's so expensive and your products still seem to be doing well.   I think it took way too long and I still think Pat Gelsinger essentially made all the right decisions, but just painfully late (though maybe still not too late).   The simplified story MBA's like to spin that you're a genius if you go fab-less in my view is bullshit because intel-foundry is a must survive strategic asset and it wasn't doomed from the start (and still isn't).   The mindset that all that matters is margins and short term shareholder value is how the entire world ends up dependent on Taiwan. It's painfully undesirable but indeed slightly more profitable per unit for all companies involved.   That also becomes a self fulfilling prophecy though. ASML machines become progressively tuned and better yielding as you pump out volume.   A monopoly has the highest volume so will end up with the best tuned machines and the best yields.   Given that engineers are massively cheaper and work longer hours in Taiwan and that tsmc receives strong government support in Taiwan, combined with the volume advantages makes it hard for them not to win.   Spinning of Intel-design as fabless leaves Intel foundry with less volume and you can't survive or thrive as a foundry that way.   The Chinese understand this hence the ban on using foreign silicon in the future.   Normally it's a bad move to force your companies to use inferior products but foundries are weird. The product is inferior with low volume because you don't get to tune the machines enough. If you do guarantee high volume you do get to tune the machines and your product won't be so inferior anymore.   This weird dynamic (and the fact that you need to sell your early wafers while you're still tuning the machines to survive economically) is what makes foundry challenging and weird.",Negative
AMD,"I can understand that perspective, but choosing Intel because ""it's what I've used for 20 years"" is a boomer ass mentality and is the reason that Intel has gotten away with being stagnant since 2012 for so long. Alder lake was actually great but had stiff competition, coffee lake was a good but long overdue upgrade.",Negative
AMD,The issues with intel are vastly overblown. I used to be the biggest AMD fan boy you could find.  After so many issues with both AMD processors and video cards I switched to intel. I’ve been using intel since the I7-920 and will never switch back to AMD.,Negative
AMD,Bottleneck-calculators are absolute pure pseudoscience just like the flat earth theory.   Bottlenecking all depends on the game and graphical settings. There is not a blanket answer to this and any CPU can bottleneck any GPU under the right circumstances. Bottlenecking is not some calculable number and as I said previously not even worth something considering since it is the wrong way to try to pick parts.,Negative
AMD,Uhh yeah the only difference is the cache....It is like saying a base 4 cylinder mustang is the same as a GT500 because the only difference is the engine. It is a massive difference. I can link to benchmarks all day long showing the uplift that is to be had.  Sure at 4K Ultra with no upscaling you will be generally be GPU bound but that isn't how real people play games. If your card isn't reaching the FPS you want you will adjust the graphical settings until it does. You aren't just going sit there and play at 60FPS if your goal is 144FPS.   If you choose your parts for your goals then you shouldn't be held back by any part nor wasting money on things you don't need.,Neutral
AMD,"Because you have a GPU. I run plex on just the i7 (was an i3 which performed fine but I ended up with a spare i7 and wanted to try virtualization)  Also, a 4090 seems like overkill … shit wait is this a joke?",Negative
AMD,"How does that prove it’s not needed? Intel is far ahead of amd in streaming, giving an example of your overkill setup where the amd cpu does no streaming is interesting to say the least.",Neutral
AMD,Thats a lot of watts for Plex,Neutral
AMD,If only I have a microcentre in my country,Neutral
AMD,>*best value* AMD X3D chip *at your budget*,Positive
AMD,Yeah. I was specifically speaking of the top end *for consumers*.,Neutral
AMD,"I got a 9700x for my last build. Great price, only 65w and 8 fast cores. Good deal. 7700 is also a great cpu",Positive
AMD,"Honestly, unless you really need bleeding edge performance, I'd drop a 5800/X/XT in there and ride it out for a little longer.",Neutral
AMD,I got 7700 non X ! Very efficient and cool in general CPU and one of the best value for money at this moment. I bought it tray at 198 euros i think,Positive
AMD,"As all things… it depends. If you’re a hobbyist (like most people) then you wouldn’t notice the 3 second difference In amd and Nvidia. Of course if you’re a big studio with a rendering server doing path tracing for the most realistic lighting or some shit, then yes it does matter, but if you’re making furry or overwater porn, then you won’t notice the 3 extra seconds for the light to bounce off of the cum to be rendered. And if you do then you have a lot of problems that need to be optimized.",Neutral
AMD,"QuickSynx is a media engine which do not have any direct relationship with rendering . Intel probably do have good edge in the mid end as you said but for high end, I think Intel is indeed beaten the shit out to a degree that they simply stopped their prosumer HEDT line altogether and let threadripper become the only choice in this segment",Neutral
AMD,ahead with what kind of efficiency ?,Neutral
AMD,"WHOA don’t go making legitimate points now! When Reddit is trending a narrative and “prefers” something at the moment, you better follow suit or else",Negative
AMD,"I'm enjoying my 265k too. Was a hell of a deal. I do a little bit of gaming, but nothing competitive. And I'm doing it at 3440x1440, so I'm GPU bound anyways (have a 9070). Runs nice and cool with a Phantom Spirit too, even if it can pull 100 more watts than the 9700 lol",Positive
AMD,"Not sure who voiced anything about using intel out of nostalgia. My views are formed on real world performance and prices. I own a 7800X3D and 12700K personally, as well as a few forms of ARM64. I try to be as hardware agnostic as possible.  Alder Lake was...alright. the 12400 was a killer budget chip but sat pretty exactly equal with the 5600X. My 12700K had severe stuttering issues until I disabled e-cores. Arrow Lake has fixed the e-core problems to the point games run worse when you disable them.",Neutral
AMD,"Really? Even though AMD and Intel both have changed a ton since the i7-920 was released?  I don't have AMD or Intel brand loyalty. When I next upgrade (probably early 2027), I'll buy whichever of Zen 6 or Nova Lake delivers the most performance (with reliability) for my use case, and in the context of the platforms that they offer.",Neutral
AMD,Absolutely.,Neutral
AMD,"You are right. It does depend on the situation. It’s just raw numbers though. And it can be calculated. Just like in instances the extra boost clock frequency from the 7700x can outshine the 7800x3d. Anyways, I’ve gathered this info just from the specs on AMD website that they have for both cards. The 7700x (4.5) has a higher base frequency than the 7800x3d (4.2). 7700x also has a higher boost by .4. The 7800x3d is using more L3 cache (and a different version yes, 3d but all it does is allow more L3 cache not necessarily a different cache entirely) and it does make it better but not by much. They are both Zen4 architecture and am5 socket. It isn’t close to enough to justify doubling the price. A good card yes. Marketing gimmick yes. That is all I’m saying. I’m not saying it’s a bad chip. Just insanely overpriced and marketed to death",Neutral
AMD,In some scenarios the 7700x will outperform the 7800x3d because of the extra clock speeds. A lot of it is scenario bound also. You’re paying way too much for a small increase in performance dude and that’s why the market is so screwed,Negative
AMD,Someone that won’t read the specs for each will say what you’re saying. It gives a 5-10% increase in certain scenarios for the CPU ALONE. Not your entire PC. But if that’s worth more than twice the price for you then buy it please lol,Neutral
AMD,"I’ve gathered this info just from the specs on AMD website that they have for both cards. The 7700x (4.5) has a higher base frequency than the 7800x3d (4.2). 7700x also has a higher boost by .4. The 7800x3d is using more L3 cache (and a different version yes, 3d but all it does is allow more L3 cache not necessarily a different cache entirely) and it does make it better but not by much. They are both Zen4 architecture and am5 socket. It isn’t close to enough to justify doubling the price. A good card yes. Marketing gimmick yes. That is all I’m saying. I’m not saying it’s a bad chip. Just insanely overpriced and marketed to death",Neutral
AMD,No it's not. It's not like I bought the 4090 for Plex lol. My desktop just does double duty as my Plex server because I was able to fit 4x18TB ironwolf pros in there so there's really no point in having a separate NAS box.,Neutral
AMD,A lot of newbies will go out of their way to not build a dedicated server. They usually learn after a little while.  All semi serious servers use a dedicated quick sync setup. Nothing else even comes remotely close to the efficiency.,Neutral
AMD,"I only recently got Plex Pass and turned on GPU transcoding, before that the 9950x3d handled it just as well.",Neutral
AMD,"It's my desktop, just doing double duty.",Neutral
AMD,7950x3d have shown up cheaper than their 9000 series replacements… like i said research a bit to be sure and thanks for the downvote,Positive
AMD,"got a 9600x for my latest build with a 5080, works like a dream at 4K and was half the price of a 7800x3D lol",Positive
AMD,"True this, I have a 5950x, upgraded from a 5800x, both CPUs are still pretty good these days.",Positive
AMD,"Unironically helpful info, thank you",Positive
AMD,"Even if your a hobbyist depending on what you do amd I'd slower, bender especially and even small video renders I do I can tell my 3080ti was faster than my current 9070xt. Start stacking effects and yeah you see the difference.",Neutral
AMD,"I was thinking of Ryzen 9 and Core 9 as high end. Yeah, Intel doesn’t compete with Threadripper.  Media engine is used for some video rendering.",Neutral
AMD,"Reddit loves sensationalism. They took a small problem that very few people actually had and blew it up to make it sound as if everyone who bought Intel was not only stupid, but they also had issues with the 13-14th generation of processors. But remember, Reddit HATES Intel. This site led by the AMD fanboys over at r/pcmasterrace will say anything to make Intel look bad. These kids worship AMD and saying anything against AMD is almost sacrilege. I have had an I7-14700k with 0 issues for over a year now.  I've been building my own PCs since I was 19 years old. That was over 20 years ago. The first PC I bought and built myself was an AMD. (K6-3) And I was the biggest AMD fanboy you could find. You could get more bang for your buck. Intel at the time was vastly more expensive than AMD. So if you didn't have a ton of cash to blow on a new rig, AMD was a better choice. But then came the performance and driver issues that plagued AMD for years. Frequent blue screens, random crashes, coil whine and buggy video drivers. I remember having to find a 3rd party program to de-clock my video card because the sucker would overheat.  When I had some money I decided to splurge and get the Intel I7-920 and an Nvidia card, which was in 2008. Overnight all of my problems disappeared. No more blue screens and random crashes. No more buggy video drivers. I've since used all Asus, Intel and Nvidia parts and I've never had an issue with my PCs other than a failed network adapter on a MoBo that I bought open box. I don't care about the fact that AMD might have slightly better gaming performance. I'm not going to notice that 1% difference when running at max settings. All I want is my system to be stable and have no issues or errors and I've never had a stability issue on any Intel system that I have built since the I7-920.  I will never buy an AMD product ever again.",Negative
AMD,"Sure you can calculate it on a per basis situation but that is the total opposite of what a bottleneck calculator does.   The 3D CPU's have lower clock speeds than their non 3D counterparts due to heat and instability at higher speeds due to the extra cache. The extra cache does more than make up for the slight drop in clock speed though.    The proof is in the pudding. The performance is there and it is very real, there's no marketing gimmick. If you want or need that extra performance then it is all the worthwhile. Is it worthwhile to you? That all depends on what you are looking to do and get out of your PC. I can't blankly state that it is or isn't worth it.",Neutral
AMD,"In some scenarios/outside of gaming yes since the extra cache isn't always utilized. But we are talking about gaming here and the extra performance is real, the 7800x3D is about 15-25% (results vary site to site) faster than the 7700x.  https://www.techpowerup.com/review/amd-ryzen-7-7800x3d/19.html  https://www.tomshardware.com/reviews/amd-ryzen-7-7800x3d-cpu-review/4  But like I said before if it is worthwhile comes down to the individual and if they want/need that extra performance.",Neutral
AMD,"What makes you think I don't know the specs? I know the most of specs and honestly the finely detailed specs really don't matter. There is a broad layer of abstraction with CPU's. Can it do what you want it to do? The nitty gritty details don't matter at the end of the day. You don't need need to know the fetch execute decode cycle, how a ALU works, how a pipeline works, how registers work, how branch prediction works, etc. Speaking of specs do you know the 32 SSSE3 instructions?   Probably not and you probably shouldn't either.    This is about gaming and your FPS is the sum of your parts and choices. And as I said before you should choose the CPU for the FPS you want and the GPU for the graphical performance you want. A increase in CPU performance will ultimately be a increase in the maximum achievable FPS that your system can achieve.",Neutral
AMD,"Oh I see. I have a dedicated plexbox running on unraid. My i3-6100, then i3-9100, and now i7-13700 all transcoded up to 4 streams (most I tested) without issue. Might add my old 3060ti for a gaming pc virtualization for the kids.  Your approach does seem pretty expensive power-wise though. Is it an issue?",Neutral
AMD,You just like nerding out and throwing money at stuff. Your advice is terrible for normal people.,Negative
AMD,"Ahhhhh makes sense. I was doing that as well for a long, long time.",Neutral
AMD,I got my 9600x for less than half the 7800x3d and it’s a beast with both my old Rtx 3080 and now my 9070xt,Positive
AMD,Nice at 4k then 9600x is great you dont really benefit from getting a Uber high end cpu if you play at 4k,Positive
AMD,How much did the build cost you in the end? My 14 month old Dell laptop that cost me 1800 bucks took a shit after almost no use. Now I'm just gonna go the desktop route. But not planning on a mega pricey build. I have a monitor and keyboard-mouse. Don't need any of that. Just the tower of mediocre power.,Negative
AMD,People keep exploding on me for saying that the 7800x3d is a gimmick lol the 7700x is almost identical for 100$ less. A lot of people don’t understand that at 1440p and 4k your GPU is the bottleneck. I have a 7600x and a 5070ti and my cpu doesn’t go over 30% when I game 4k ultra. AMD’s marketing is clutch bro🤣,Neutral
AMD,"Oh I never said that it wasn’t slower, just that for most people the speed doesn’t matter, udna should fix this because NVIDIA is using a unified architecture and rdna (amd‘s graphics architecture) is specialized for pure rasterization/gaming instead of standalone rendering, it also doesn’t help that amd was late to making a communication api to directly communicate with the gpu like cuda.",Negative
AMD,"The difference is in seconds, it really isn't that relevant",Negative
AMD,"Reddit does love sensationalism, sure, but that also depends on the subreddit. This isn't one of them that does. While ofc there's fanboys for everything everywhere pretty much, I haven't seen what you've seen, not in this subreddit at least.  You had problems with AMD 17 years ago when AMD and Intel were *very* different companies than they are now. Very likely 99% of the staff isn't even the same. Certainly both AMD and Intel both have had execs come and go, the leadership of both has changed, the computing world has changed. Yet, you continue to write off AMD in 2025 despite that, which really surprises me. You say you were an AMD fanboy back then.. seems like maybe you've become an Intel fanboy instead. Better I think to not be a fanboy at all. I've owned both AMD and Intel, had good success with both. I've been on Intel since about 2010, but had the 7800X3D been out when I bought my 12700K in 2021, you can be sure I would have gone AMD again then, bc the X3D CPUs really *are* the best for gaming (and way more than 1% as you claim).. and bc gaming is my most-demanding use case. Likewise, when I upgrade in a year and a half, I'm certainly going to consider Intel, Nova Lake looks really interesting, but will it perform? I'll wait on independent benchmarks before deciding what I'll get. Logic, not fanboyism.   Idk, seems like AMD (Lisa Su) has the leadership to make good products, and has proven that by doing so. Intel on the other hand, thought it was a great idea to cancel the ""Royal Core"" project led by Jim Keller, thinking mediocre improvements like in the 2010s was the way to go, and has bungled every CPU line after Alder Lake (12th gen), so I've lost confidence in Intel's leadership, for now.   Do as you wish, it's your money, your choice, and maybe Intel will be the best to buy next-gen, but if I buy Intel next time, it'll be because it's the stable AND most-performant choice, and not bc I had a bad experience a human generation ago.",Negative
AMD,"I get you. The point besides Intel being reliable is that issues AMD chips have are just swept under the rug. The X3D failure topic never got overblown like the 13th/14th gen degradation. Both are catastrophic failures which do not affect everyone. There are also cases on other boards than just ASRock. But hey, nobody „loses trust“ in AMD because of that. That’s where you can spot the bias again.",Negative
AMD,"It goes to sleep when idling for a bit and wakes up when someone tries to stream from it. Sure the hardware is more powerful and therefore more energy consuming when it's awake, but it also barely has to try at all for any Plex workload, such that I can game at 4k/120 at max settings while multiple people are transcoding and it doesn't matter at all.  It really hasn't changed how I use the computer at all practically speaking, except my extended family bitch if I turn it off going on vacation or something not thinking and then no one has access lol.",Neutral
AMD,"Hello, your comment has been removed. Please note the following from our [subreddit rules](https://www.reddit.com/r/buildapc/wiki/rules):  **Rule 1 : Be respectful to others**  > Remember, there's a human being behind the other keyboard. Be considerate of others even if you disagree on something - treat others as you'd wish to be treated. Personal attacks and flame wars will not be tolerated.    ---  [^(Click here to message the moderators if you have any questions or concerns)](https://www\.reddit\.com/message/compose?to=%2Fr%2Fbuildapc&subject=Querying mod action for this comment&message=I'm writing to you about %5Bthis comment%5D%28https://www.reddit.com/r/buildapc/comments/1okautv/-/nmannoz/%29.%0D%0D---%0D%0D)",Neutral
AMD,"yeah brother its crazy good.  i was sceptical looking at the reviews that a 6 core zen 5 was so good compared to an 8 core x3D chip but man, the value for performance on this chip is actually incredible.",Positive
AMD,"1550 for 9600x, MSI PRO motherboard, 32GB 6000mhz DDR5 RAM from Crucial. Bequiet silent base 802 for the case, cheapest thermalright CPU cooler + 750w PSU for my home server and a PNY 5080 OC..     I already had a PSU from my old build gaming build as well as storage so that saved on costs overall     i had sold the 4090 for exactly £1560 so it cost me exactly £0 overall.",Neutral
AMD,Rocm should be the fix it's just also going to take these software to also update and support it. I don't doubt that once Rocm gets proper support we see a big improvement for amd cards that support it.,Negative
AMD,Again it depends what you're doing. If you're just rendering a video you trimmed then yeah it's nothing really. Start to stack different effects it adds up especially in the preview window.,Neutral
AMD,"In my humble opinion, the x3d chips are way more niche than they are. It’s has huge impact on cpu limited games at 1080p, minor uplift in 1% lows 1440p, and almost no difference in 4k  (For most games, yes I know space marines 2 and MS flight sim exist)",Neutral
AMD,Danke,Neutral
AMD,"Well it’s not the big ole fix, it helps yes and should see a increase in selective benchmarks like blender but it doesn’t matter how good rocm/cuda is if the architecture isn’t made to allow it if that makes sense. Like as I mentioned Nvidia uses a unified architecture meaning their server cards are the same as their desktop (or at least the architecture is the same) while amd uses a split architecture, their mi350x or whatever it’s actually called is their server gpu and is made for handling data, like render servers or ai servers, rdna which is the gaming cards are made for gaming, the drivers and architecture are oriented for it, not saying it can’t do other things, rocm does work and support rdna but rdna isn’t made for these things.    TLDR rocm should help and definitely make them more viable but until we get udna it won’t be a major increase.",Neutral
AMD,"9400f is getting up there so you would benefit from a new cpu, but to make it a worthwhile upgrade you’d need to commit to a new motherboard/cpu/ram bare minimum.",Neutral
AMD,it will be compatible what is your psu how many wattage is it?,Neutral
AMD,Yes your CPU is compatible with a GPU.  This includes most every GPU from the last two decades.  It's done on purpose so that people can just buy a GPU and not worry about compatibility.  That said after you get the new GPU you may want to monitor GPU usage.  If it's consistently below 96% then you will likely want a stronger CPU.,Neutral
AMD,"If you can swing it get yourself a 12400F and a B760 DDR4 motherboard. That way you can reuse your other components like memory, cooler and compliment your new GPU.",Positive
AMD,The rx580 is a 185w GPU and the rx9060xt is a 180w GPU.  If his PSU wasn't strong enough his current system wouldn't be running.,Neutral
AMD,"the rx580 requirement psu is 500w only while the rx9060xt 650W or 750W power supply is recommended for better stability, future upgrades its better than to be sorry so no issue will occur in the future too",Negative
AMD,"The 500w, 650w, and 750w are suggested capacities for an entire system.  At full tilt a rx9060xt 16gb still draws less power than a rx580.  If you put them into the same system the power draw while gaming would be near identical.  Heck with my r5-5600 my power draw from the wall doesn't even hit 300w while gaming.  I still run a 750w PSU though.  They are just too cheap nowadays for a decent one with a 10 year warranty.",Neutral
AMD,"Most reputable sites will have compatibility filters for your chosen CPU.  Same with coolers. You just have to decide air vs liquid AIO. After six years with an AIO, I've made the switch back to a decent air cooler.  Sorry I can't help you with your question regarding RAM. I bought a new rig two months ago, but didn't notice any insane RAM prices.",Neutral
AMD,"> Do I just look for any AMD AM5 Socket?  Pretty much,  you wnat some decent VRM for the Ryzen 9800X3D but most B650 boards will handle it fine.  > - same question as above but about finding coolers for it  Pretty much every cooler in the market now works on AM5. Just check the compatibility list.  > - from what I read, I might have picked the worst time to have to upgrade RAM, since it apparently just skyrocketed thanks to AI datacenters. Is this true?  Yepppers  > I noticed some rumors about AM6 socket coming out next year and DDR6 soon too?  It's not soon, there's at least one more generation on AM5, probably two if I were to bet.  >  oh and, would the new(er) AMD socket support older CPU from the old one?  Wut?  Furthermore You could probably keep the PSU. The power draw isn't that much higher.",Neutral
AMD,"The only part I can help with is the RAM price question. I bought a 2x64 AM5 kit ten days ago at NewEgg, and it's gone up by $80 US since. I'd get whatever the Ram is going to be, even if you need to wait on the rest.",Neutral
AMD,"Thanks for the extensive answer :) I will take a look at different mobo manufacturer and pick something, same for cooler.   I really hope I will not have to swap the PSU too, but I will check all total power once I finalize my new CPU, mobo and RAM",Positive
AMD,Yes! As long as it’s in good condition a 6700xt under 200 bucks is a good buy. I recently upgraded from a 6700xt but it was incredible. I ran it with a 240hz oled 1440p monitor and it never gave me issues. Definitely worth it.,Positive
AMD,xfx one is better imo should be same price or less,Positive
AMD,Sounds like a fair price.,Neutral
AMD,"Back then it was a solid 1440p card capable of handling 2160p. Now, in the fucking era of UE5, 6700 XT is more like a 1080p card, but with the help of upscaling and frame generation, you are still able to play a lot of 2160p titles close to 60 fps, although I don't recommend this card for such high resolution. Anyway, it is still decent for the budget.",Negative
AMD,"For that price definitely! I got the PowerColor Hellhound RX 6700 XT just last week used for €180. And I'm very happy with. Currently playing Expedition 33 at 1440p, having such a blast. Definitely recommend both the gpu and the game :)",Positive
AMD,"The 6700XT is definitely a solid GPU ! Among a few others. I recently did an AM5 build. Slapped the Trusty MSI Radeon Mech In to see what gives. Slightly overclocked it , very conservative with Adrenaline. Couldn’t justify spending $ on the new options out there until I benchmarked it in the new system.  My Score (Excellent)  DX12  Gaming Battlefield 6 1080p  My Score 2541  Average 2432  Best 2717",Positive
AMD,"Think about this logic. Can you play at 1080p with no issue with gameplay. FPS is solid , you can see the crosshairs and the enemy. Everything else is theatrical fluff. Not even paying attention to it.",Negative
AMD,"Yes, just make sure your CPU won't be a bottleneck.",Neutral
AMD,"Okay, awesome! I don't really know anything about the specs of my pc since my friend told me it was steal since it came with a acer monitor, mouse, keyboard, and speakers 🙃",Positive
AMD,6700 xt mech oc here.  Runs 1440 great,Positive
AMD,I have a 6700 XT and a 6800 that I'm trying to sell I didn't know that their value was still that decent.,Neutral
AMD,To be fair it was either really old games or low framerate to say it was incredible at 1440P. Only upgraded from my 6700XT but IMO it's very much a 1080P spec card these days. If you're playing pretty old games I guess you can get on frames at 1440P. solid 2080p card though.,Positive
AMD,"if it isn't in good condition, save up more money  you can get a B580 from Intel for about $50 more \[brand new\]",Neutral
AMD,Is there another gpu youd recommend?,Neutral
AMD,I love expedition 33! I played it briefly on my old xbox.,Positive
AMD,I actually found a AMD RX6800 16gb that I might get :),Neutral
AMD,"It would be good to know the internals of your computer prior to buying this. Only because you don’t want your cpu to be a bottleneck. Look into what’s actually in your computer, and either reply with what’s in it or look up if the parts are compatible.",Neutral
AMD,"I play games like life is strange, stardew valley, or BG3 😅 I recently got Life is strange double exposure on my pc and the graphics are just blah so that's my main reason for upgrading lol",Positive
AMD,"Bruh ain't no way that card is pushing 1440p at 240hz/FPS like that dude said without massive amounts of upscaling, or turning the game down to low everything. Least not anything modern. I have a more powerful card and even I can't push that in near anything (maybe in 1080p lol). Fortunately OP plays casual games which this card can definitely play; but yeah people are over selling this used GPU. Mind you it's a decent value card; just certainly not going to live up to what is somehow the top comment in this thread.",Negative
AMD,"I don’t know. I just upgraded recently and the only game I can think of that I couldn’t get at least 60fps constantly was expedition 33 on 2k res.   I mainly played shooters though, but everything I played was smooth 140+ fps it seemed. Maybe I’m delusional but damn I just feel like it did well in 1440p.",Negative
AMD,"The 6700xt is a bit faster than the b580.   Also, as far as ""good condition""... its a used GPU, not a used car. They tend to either work or they don't. It's not like you could say ""ahh the previous owner didn't change the oil often enough and it needs new tires"". As long as it works, the only poor condition it could really be in is dusty, which is pretty simple to fix.",Neutral
AMD,The 16gb is great.  It’s basically a big upgrade over a 12gb.,Positive
AMD,I only what was listed on marketplace!  Ryzen7 570x Asus prime b450m-A II 16 gb ram -> my friend took the 16gb and gave me his 32 gb instead tho 4 storage harddrives: 5tb HDD 240gb HDD 256 gb SSD 512 gb nvme GeForceGTX 1060 oc 3gb gpu CX650M power supply   monitor is an acer v226hql,Neutral
AMD,"can we please stop using the b word? it’s overused and has lost its true meaning. as long as they have a cpu from the last like 8 years, it’ll be a good purchase.",Negative
AMD,"Yeah fair enough, It's a decent GPU. I upgraded my monitor to 1440p UW from 1080p UW, so needed the upgrade. Had the 6700XT 4 years, zero issues.",Positive
AMD,"I have a 9070XT and granted it's 1440 UW, can't get a solid 140 in bf6. The 9070XT it's twice as good as my 6700XT was. I mostly play shooters so I want 100FPS+. With that said, I only have a 5700X3D. If I threw a 7800x3D at it I'd be getting close to 200 I think. So even at 1440 a decent CPU will help.",Positive
AMD,Assuming that’s the ryzen 7 5700x then yeah you’ll be fine. Good buy on the 6700xt id do it.,Positive
AMD,"Lmao. No, the 6700xt will be too much for an 8 year old cpu. Been there done it.",Negative
AMD,would you think upgrading my monitor is needed or not?,Neutral
AMD,"Yeah I think it’s your cpu but your cpu is still very capable, but I mean in the most modern games I’m not getting super high frames on 2k with a 6700xt but it did perform. Granted I didn’t play max settings on those games and I tweaked it to get that high frame rate. But i really can’t think of one game where it struggled to get 60fps on 2k seriously.",Neutral
AMD,"Awesome, thanks!",Positive
AMD,"1. If you're planning to use Plex, you're going to want an Nvidia or Intel GPU instead. Plex doesn't support AMD cards for hardware transcoding. 2. There is almost no chance you'll get 4 DIMMs working at 6000 MT/s. If you want that much RAM and you want it to be that fast, get a 2x32GB kit instead. 3. The 3D vcache variant of the CPU likely isn't going to do much for this use case. Going with the non-3d chips might save you some money or let you get a beefier CPU.",Negative
AMD,"1. I'm using Emby and UMS.  Emby charges if you want to hardware encode, UMS doesn't.  I got to do more research on support.   2. Thank you, I'll change that.    I got time, i looked at price history and stuff went up 2 weeks ago.  If the prices drop back to where they were for Black Friday, i can save $200ish.",Neutral
AMD,Can anyone explain like I’m 5  Why did they remove the USB-C functionality and what are the ramifications of this? I’ve never used my 7900XTX’s USB-C port.,Negative
AMD,I was looking at 6700's just recently for a new build. Does this mean it would be a bad idea?,Neutral
AMD,And I'm still with my RX 6600...,Neutral
AMD,Lol at me for buying a 1000€ 6900xt when it came out. No new game support after such a short time. Ridiculous,Negative
AMD,"Wow. thats what i get for supporting the ""underdog""(gpu) when the difference isnt to great. (performance per dollar within my budget and need).  And i dont understand why this isnt getting more traction than it currently is getting, this is setting a really terrible precidence. Im pissed",Negative
AMD,"This makes 0 sense and is such a trash move by AMD as 6800XT and 6900XT are still very capable cards and beloved by the community. These companies and their idiotic decisions in which they think will generate more profit can easily backfire, hope it does.",Negative
AMD,"What the fuck I bought my 6700xt in 2023, this is balls",Negative
AMD,"I can understand not perpetually supporting a card with new features, but it seems a little sneaky and very unnecessary to disable existing functionality such as with this USB port.",Negative
AMD,"Well, fuck, they ended support earlier than I thought for the 6000 cards. I bought the 6900xt in 2021 lol.   There goes any hope of fsr4 support for us.",Negative
AMD,Ah yes.. the same AMD who's first dedicated card comes in at *#29...* on the Steam Hardware Survey and who's LATEST GEN Cards arent even registering on the Steam Hardware Survey at all (I know about the bug but they arent even caring to fix that either).  I understand you cant maintain tech forever.. but this seems stupid if you are trying to retain/encourage consumers.  Look at Intel. Forcing people to get new Mobo/CPUs every 2 years drove their customers to the multi year platform of AM4.,Negative
AMD,"Excuse me, what the actual fuck, I upgraded to the Rx 6800 XT from the GTX 1060 6gb and they both loose driver support at the same time? Is this some kind of a Halloween joke? GPU have never really stabilized after the crypto rush and then got into the AI slop era immediately. Gaming is really in the worst place ever.",Negative
AMD,The consoles are still running 6000 series. This makes no sense.,Negative
AMD,"It's insane to me that AMD are cutting adrift graphics cards from 2 generations ago (6000 series).  On the CPU side they continue to support AM4, and word is the next generation of CPUs will still work on AM5.  How does this company not have a coherent company-wide philosophy for legacy product support?  This isn't even hardware.  It's software.  There is no technological reason for them to stop supporting these GPUs--only greed and laziness.",Negative
AMD,Disabling that USB port seems dicky? If it causes issues then it's on them to fix these and not keep it broken.,Negative
AMD,"This seems really premature for the 6000 series cards.   The 5000 series I can kind of get because they lack the ability to do RT and weren't *that* popular.   But 6000 series production only ended what, 3-4 years ago?   The rx 6700 and up are still very capable gpus.",Negative
AMD,"That's disappointing, looking that Nvidia just ended support to cards released between... 2014 and 2016.",Negative
AMD,"It's almost as if this is one of the reasons AMD is In a distant second place, Nvidia JUST dropped the 10 series which is almost a decade old.",Negative
AMD,"4-5 years is unacceptable for game driver support on the RX 6000 series. If Nvidia can give game support to the bloody GTX 1660 Ti & RTX 2060, why the hell can AMD not be bothered to give it to cards like the RX 6700 (Which is the bloody PS5 GPU) or RX 6900 XT. Reeks of laziness.",Negative
AMD,"I bought a Waterforce 6900XT last year because it was on sale, my 2080TI crapped out, already had a water block on it, and I wanted to take advantage of resizable BAR with my 5800X3D build.  Well this sucks.",Negative
AMD,"This is such a shit move by AMD    I guess no matter what GPUs we decide to buy, we still will get fucked by the corpos",Negative
AMD,"Seriously? wtf AMD   How to fuck with consumer trust in one simple step. I wonder when they will do the same for 7000 or 9000 series cause imagine buying such card for 3 or more years of usage for pottentially having been cut from features (or worse, drivers if they decide to fuck us even further)",Negative
AMD,"We were receiving new features? As far as I know even rocm still isn't supported on my card.   > new features, like the latest Battlefield 6 update    Oh, they call the bare minimum of maintenance a feature. Fuck amd why did I give them the benefit of the doubt after buying a fucking buggy rx5700xt before.",Negative
AMD,"Nah, I just got 6750xt and was talking about how well it runs 1440p, telling my friends to switch from green team. Not anymore",Neutral
AMD,Wow! And here I was floating my next GPU decision between AMD and nVidia. If AMD means I'll need to buy a new GPU every 4 years I'll just stick with the folks still updating my 7 year old 2080.,Neutral
AMD,"I get not supporting older cards. That happens, though this does seem a little premature. The 6000 series was released 4 years ago. And on the other side of that, we have the Nvidia 10 series that stopped receiving update earlier this year. They were released 9 years ago. Pretty big difference.   But disabling the USB C port is insane. What reason do they have for that? Just let people use it as a standard USB C port, or continue to use it for DP over USB C for VR if they already are.   Cant wait to see all the AMD fanboys in PCMR flock here to try and justify this one.",Negative
AMD,I love how all companies trying their best to make me quit this hobby.   My 6700xt will be the last gpu from amd. I may ditch my cpu too.   This gotta be one of the scammest things i have ever seen in my entire life.  Ill gladly to stay away from this company for good.,Negative
AMD,Makes me regret buying my 9070xt now...  This is absolutely insane. The card came out recently and can still run modern games. Huh?,Negative
AMD,So what does the practically mean? How long until games won’t run on 6000 series cards or they have issues with operating systems?,Neutral
AMD,This is really bad right? People were irritated that the 10 series was losing support earlier this year and the 6000 series is like half as old even the 5000 series is not particularly old.,Negative
AMD,"cool, not like I was planning on supporting AMD after my current card.",Negative
AMD,I.... just got my RX 6600 not even a year ago...,Neutral
AMD,"Because this offended the mods elsewhere for who knows what reason:  Not that I'm team this or that (I don't give a fuck about any of that, I go where the performance is for whatever I'm shelling out at the time) but this definitely pushes me more toward a certain other team.  Because, what the fuck is this? My card (6900 XT) will only just turn 5 in December and it's already being relegated? Meanwhile, Nvidia is still supporting the 900-series cards for the next three years, albeit only with quarterly updates — full-time support ended, what, this month? 11 years of support and this is getting relegated after 5?  Fuck off. Properly.",Negative
AMD,I have a 6800XT and I haven't seen any updates for game compatibilities in more than a year... so this changes basically nothing I guess?,Negative
AMD,Wondering how much I could get for my 6700xt and grab a 5070.,Neutral
AMD,"So this is from AMD's Duscord channel about the USB Type-C on the Radeon RX 7900 series GPU: We’d like to inform you that the release notes for AMD Software Adrenalin Edition 25.10 2 posted included misinformation that has since been corrected. There is no change to USB-C functionality on the RX 7900 series GPUs in the 25.10.2 driver. There was an incorrect line in the originally posted release notes that has been removed, and the release notes have been updated.   We apologize for any inconvenience.",Neutral
AMD,"We got Nvidia cranking the damn prices to oblivion and we got AMD dropping support for less than 5 year old GPUs that are still in tons of builds.  I can't believe I'd be saying this, but Intel might be the future of the consumer GPU market if they make the right business moves in the next few years and capitalize on this.",Negative
AMD,Fuck AMD.,Negative
AMD,I thought AMD aged like fine wine???,Positive
AMD,![gif](giphy|U79sZO1AFMp3DI0b0r),Neutral
AMD,"Ok what the actual fuck i have a 6700xt and can run any game pretty decently and most of them at 165fps stable, why they will stop receiving features? Those GPU are more than capable on 2025",Negative
AMD,I bought my 6900 XT as a birthday gift with my new job in 2022.  It's already getting dropped? Why??,Negative
AMD,What about the Steam Deck?,Neutral
AMD,"What does that mean for the USB port?  Can you still use VR headsets?   (I never owned one but, AFAIK, this is the use-case for it).  I would be glad that someone explains this to me.",Neutral
AMD,Haha good thing I didn't spend 300 dollars on a used 6000 series gpu then haha....,Positive
AMD,"Think I might just pay the nvidia tax next time I upgrade my GPU. Still won't do that until games become literally unplayable for me, but yeah, pretty bad look to axe driver support after a measly 4 years.",Negative
AMD,"""WHOOOOW WE FUCKING LOVE BEING STUPID NVIDIA DESERVES THEIR 100% MARKETSHARE BECAUSE WE ARE COMPLETELY AND UTTERLY BRAINDEAD FOR LITERALLY NO REASON GET FUCKED IDIOTS FSR 4 ONLY WORKS ON 9000-SERIES CARD WHILE DLSS 4 WORKS ON A GODDAMN 2060 BUT WHO CARES BECAUSE WE'RE STUPID""  Did I understand them right?",Negative
AMD,If prior cards are any history the Linux kernel will keep them supported for a few decades.,Neutral
AMD,Well that puts the nail in the coffin for me. Only Nvidia cards for me from now on. I have a 6800xt that works great and it's not that old.   Trash behavior.,Negative
AMD,"What bastards, what the fuck",Negative
AMD,"So not only do they essentially end support for 5, 6 series, this is a guaranteed nail that it will not receive FSR4. I got my 6600 in 2022. So 3 years and it originally dropped in 2020? I think. So 5 year support and instantly dropping it.  At least for me.. I'm not sure i want another amd product. Prices have been higher than msrp, drivers are pretty ass, still tons of bugs. They take months! MONTHS! To release stable drivers with bug fixes everyone's been asking for. The program itself is buggy, slow. It constantly kills itself and you have to restart it. Generally all of this is excusable for products under msrp and is within its product limits of hardware.  Yeah big yikes amd... not too sure about your future.",Negative
AMD,"Oh, you mean the peoples champion of value and virtue is not what it seems?  https://preview.redd.it/teug50mjmayf1.jpeg?width=947&format=pjpg&auto=webp&s=33f1600db1cfa55dc2253cadd2879d8c9aee1480",Neutral
AMD,"Me, still on a RX580...  Soooooo do I need to upgrade?",Neutral
AMD,I paid £600 for my 6950XT in June 2023 and I don't even get the BF6 update?  My shits lagging af from a memory leak and is unplayable - not sure if that's a driver issue or game issue but I'm quite annoyed over this. It wasn't cheap.,Negative
AMD,People shit on Nvidia for cutting support for 7 year old cards. This is even worse,Negative
AMD,"AMD really wants to move from their prior gen. As quickly as possible.   They underestimated the market when it came to AI and now realize they can’t ignore it.  The problem is they have been ignoring it for too long and now have prior generations of products that really don’t have a path to fit in with their current model.   Nvidia on the other hand was laying the ground work starting with 20 series and why they can STILL have some carryover support for new features and keep things going. Their whole line up has a single trajectory and don’t require different teams and feature sets because their products are scattered all over the place. It’s why some features DLSS4 went back to supporting 20 series.   AMD can’t do that because they never laid down that groundwork. They’re JUST NOW starting to work the their current gen 90xx cards, that means they need to back away from prior gens as soon as possible to focus on this and future generations.   5000, 6000 now, 7000 probably isn’t that far off either.",Neutral
AMD,"No way, only 4.5 years after release 6000 series lose driver support for new games.  This is akin to nVIDIA abandoning 3000 series cards, meanwhile Rx 6000 series are even 0.5 year newer!  People would not let NVidia live that down! There would be crusades.",Negative
AMD,"I have an RX 5600 XT, am I cooked chat?",Neutral
AMD,The fuck? My fiance is rocking a 6700. And that card is only what 4-5 years old? The fuck,Negative
AMD,"Wait, my 7900XTX has a USB-C port??",Neutral
AMD,"Not sure i understand why tf they would disable the usb-c features. I mean, what did it hurt them to just leave it as it was? Seems kinda like when they nerf a character in a game because they think its too OP. However, nerfing a product that youve paid for is bs. Also, what exactly is “maintenance mode”?",Negative
AMD,Oh okay so between still being on AM4 and using a 6700 I'm double fucked.   Anyone interested in some hairy feet pics? /s,Negative
AMD,Geezus AMD has never been good with GPU support... Kepler got like 13 years of game ready drivers,Negative
AMD,"At this point it's not even worth recommending AMD saying that they have 16GB compared to a 5070, since if they last 2 years of support with 12GB maybe for 2 years you can still play at 1440p, clowns",Negative
AMD,I thought my RX 6800 (non xt) was gonna carry me through for a long time. Like the RX 480 and 580.   Does that mean the shelf life for the 7000 and 9000 will last for another year and a half?  Gotta say I'm bummed to buy anything new.,Negative
AMD,I guess this kills any chances of an official FSR4 version for RDNA2 and RDNA3 GPUs. Booooo!,Neutral
AMD,Is this the AMD FineWine I keep hearing about? They keep making me regret going with them for my last gpu,Negative
AMD,"Yeah these fucking shitstatins can go fuck themselves that's for sure.  My laptop gtx 960m which released in 2015 and is based on a gtx 750 ti is still getting updates, but my 4 year old 6700xt isn't going to be getting shit now?  These fuckers truly never miss an opportunity to miss do they.",Negative
AMD,"AMD seriously reconsider, you'll hurt future GPU sales with this move. Can they open / community source it or something so people don't regret supporting them?",Negative
AMD,"Wtf?  Going green on my next GPU upgrade. They are overpriced by like 50% but at least they get longer support so I save more money on the long run.  AMD, after going through 3 of your GPUs for the past 10 years, you lost me fully with this one.",Negative
AMD,"So I have an odd question about the disabling of the usb-c port on the 7900 series. So does it fully not work to display on, say, a usb-c dock at all now?",Neutral
AMD,Does that mean my 7700XT *will* be getting new features?,Neutral
AMD,I just got a 7800xt last year. Makes me a little worried how much longer it may have support for😬,Negative
AMD,Makes me question if buying the 9060xt was the correct decision. If it’s gonna get support for it dropped in 3-4 years then it kinda sucks.,Negative
AMD,At this pace of support drop off how likely is it for the 90- series to become unsupported in the next 5 years?,Neutral
AMD,"Bought a 7900xtx during a gpu drought… so not only has my card lost features but the generation previous has lost support, signaling to me this card will have its support cut in what, A year or so?   Really looking like I should’ve bought team green.",Negative
AMD,"Ugh, and I’ve been so happy with my 6900. If they don’t reverse this decision, they won’t get my money next time I upgrade. Maybe I’ll give Intel a chance if they’re still making GPUs by then.",Positive
AMD,Are these companies allergic to money or what??  They've been hitting it out of the park lately and now they go and fuck it up.,Negative
AMD,FML just built a second PC for living room with a 6800XT. This stinks.,Neutral
AMD,Wow. I'm currently rocking a very capable RX 6800. This...honestly might push me to Nvidia for my next card. AMD are *crazy*.,Positive
AMD,guess I'm not switching to amd gpus after all,Neutral
AMD,AMD decided to make the GPU and CPU parts of the company polar opposite for some reason?,Negative
AMD,"I bought an expensive card specifically so I would have less to worry about in terms of upgrading. Do I just turn off updates and hope for the best, or lose my USB-C port?",Neutral
AMD,"That's really bad, like wtf amd?",Negative
AMD,"Man, duopoly sucks. I thought AMD would cultivate good will with their current position, but I guess they're just another corpo.",Negative
AMD,I'm wondering about my fate owning a 7000 series card in the next few years but man this sucks.,Negative
AMD,That's really anti-consumer.,Negative
AMD,Well that sucks. I'm glad I at least got a decent deal on my 6650xt.,Negative
AMD,From what I've heard Nvidia works right out the box no tweaks yess some have less v ram but the same performance as amd if not better and on the other hand with my personal experience both my cards the rx580 and the rx6800 have crashed my pc multiple times unless I tweak setting this is with 3 different PC setups so imo if this is all true fuck amd cards in the future and watch that end of their company fold just as they were doing pretty well maybe they are just going to focus on cpus and new architecture but to end support for these cards this soon while still selling them new seems like a huge scam,Negative
AMD,I'm going to give them the benefit of the doubt on this one before flipping out. The 6950 XT is slightly worse than the 7900xt in raster so it doesn't make sense to axe that line of gpus. 5000 series *maybe* but even that feels like a stretch. I hope they make a statement clarifying what this means.,Negative
AMD,"That's a shame, I got my sapphire 5700XT right before the pandemic hit (800$ was and still is a huge amount for me to spend) and it's been good to me ever since. Gonna have to stick with older/indie games for another couple years until I can afford to make a new build.",Positive
AMD,"shit, i was looking to get a rx 6600 to pair with my ryzen 5 5600x, guess ill be looking at Arc now",Negative
AMD,Fuck you AMD,Negative
AMD,Me with my 7800 XT  ![gif](giphy|55itGuoAJiZEEen9gg),Neutral
AMD,"Oh I bet this is how they weasel their way out of giving RDNA2 INT8 FSR4 despite modders having literally shown it works and reviewers like HUB testing it out. Dicks, my 6950 XT I bought two and a half years ago and it only released 3 and a half years ago ffs.",Negative
AMD,"Man, I love AMD for years until now. This disappointed me so much considering the fact I owned and still use 6800XT which is 5 years old. I understand that they want to stop support any gpu over 10 years old. Hell, 6800xt is match to match the 9060XT in most games to the point it isn't needed to ""upgrade"". This is a big middle finger to me as a 6800XT and a fan of AMD.  I guess my next upgrade would be NVIDIA.",Negative
AMD,They will still be supported and receive new features in Linux.,Neutral
AMD,"Don't get too comfortable, AMD",Neutral
AMD,"Shit, c'mon AMD do better. If they're going to be like this my next card I will be going back to NVIDIA. My GTX 960 on an old machine just got another update.",Negative
AMD,AMD fanboy : ''BuT aMd DoEsNt DrOp SuPpOrT fAsT''  They did the same shit with first generation ryzen processors and tried to do it to 3rd gen ryzen barely 3-4 years in until community backlash. VEGA gpus got cut 3-4 years in and got 1 update every 2 years after ...  AMD is the most anti-consumer company.,Negative
AMD,They're still going to fix bugs and security issues. It's not like nVidia adds new features to 5 year old hardware either.,Negative
AMD,"It's fine cause it's AMD, and AMD never does anything wrong whatsoever  Greedy capitalist pigs from NVIDIA on the other dropped support for the 10 years old 1080 TIs because Jensen is that mean of a guy",Negative
AMD,"Seems I dodged a bullet, I was torn between a rx 6800xt and a 3080 and went with the 3080 last minute a couple months ago.",Neutral
AMD,"Jesus, what a garbage generation of gpus... They have aged like milk, If I had bought a 3080 instead of a 6800XT I would not have bought a 4080super to replace It.",Negative
AMD,"Keeping up with pc hardware was already impossible, and now they just start crapping out like some damn cheap Motorola phone? Niiiice.   Really happy I chose AMD for that extra VRAM for all those future games that are going to need it :DDD",Positive
AMD,"bruh... i was planning to get a rx 9060xt as an upgrade from my rx 6600... reading this changed my mind, i will go with a rtx 5060ti or heck even a used rtx 3080/ti, i cant keep supporting AMD while they keep making such stupid, dumb decisions.",Negative
AMD,Can just 1 year go by without AMD's GPUs division shooting themselves in the foot with a poorly communicated decision that defies any sort of logic or reason? Just once please to see what it feels like.  What the hell are they doing over there? It's like they are refusing to even attempt to regain market share and like they're  allergic to good PR that lasts for more than a few months.,Negative
AMD,Fuck. I just bought a brand new 6600 like a sucker lmao. Every day I regret that decision more and more lol 💔,Negative
AMD,"Ah yes, good ol' planned obsolescence.",Positive
AMD,"Damn. I mean on one hand, I was planning on upgrading to the RX 9070 sometime the next few months anyway, but on the other it’d be nice to continue to get support for a 4 year old card that is still beloved by the community.",Positive
AMD,Ded card.,Neutral
AMD,Imma give Intel just a little longer until I switch my 6700xt.,Neutral
AMD,"Wait, what i I have bought 6900 so i can have it for like 2-5 years. What now can someone explain it to me like i; am 2 years old?",Neutral
AMD,"I don’t use a usb-c on my 7900xtx, what else does this mean?",Neutral
AMD,So my usb c on my midnight black edition rx 6800xt will be disabled?!,Negative
AMD,I wonder if any of this has any relation to Microsoft's dropping of Windows 10 support? The timing seems rather coincidental to just be a coincidence,Neutral
AMD,bUy AmD nOt NvIdIa,Neutral
AMD,And i will never buy amd again gpu wise if the comments are right this time. Lucky i just sold my 6600xt,Negative
AMD,bunch of fucking greedy criminals fillig landfils.. fuck em,Negative
AMD,"The RX6400 is honestly of my favourite cards ever. £125 back in 2022, and ran everything I needed it to really smoothly. And then FMF was introduced and it felt like I got a huge upgrade. Absolutely goated budget card",Positive
AMD,Legal! End of purchasing amd models for my PC.,Neutral
AMD,guess I won't be buying another AMD card anytime soon,Negative
AMD,Does this mean no drivers for future games for my 6900xt wtf,Negative
AMD,"This is exactly why when I sold my 7900XT for lack of FSR4, I moved to Nvidia. Can't deal with 2 scummy companies. Nvidia is more than enough.",Negative
AMD,"So does that mean AMD is going to reimburse people for locking out their devices for zero reason? If I buy a car and the manufacture is like well yeah you bought a v8 and all, but im going to do a ota update and lock out 2 of the cylinders because we don't want to support all 8 cylinders anymore as we are moving to only v6s.   Time to reach out to my shitty ass fuck fed rep...",Negative
AMD,The RX 9000 series will suffer the same fate in 4 years.  Curse you AMD!,Neutral
AMD,My 6950xt is cooked 🥲,Neutral
AMD,Lesson learn; bought an rx 6700 XT two years ago after quitting Nvidia.  Never again ! It's such a sh.. move to your customers.,Negative
AMD,Nvidia set to have 100% market share in that case.,Neutral
AMD,"Dropping support this fast suggests the company’s financials are completely fucked & they need to risk consumers never buying from them again, by trying to force an upgrade cycle. I’m guessing they’ve done the stupid thing & put shit tons of money recently in Ai without realising the US economy is about to explode & need enough liquidity to survive the collapse.   I love my 6950xt, I love my 6700, I will probably not buy AMD for a while because this kind of nonsense is why I stopped buying Intel products.",Negative
AMD,I just bought my 6700xt. I will never buy any product that they will release.,Negative
AMD,"Custom driver packages incoming, I'm sure.  Also, curious if this would give Linux a lead with its Mesa drivers.",Neutral
AMD,"It's understandable, they just want to avoid the ""forever-card"" situation, like with the 1080Ti.  It's not a problem that they can't make a faster GPU than the 7900 XTX, they'll just drop support for it in a year or two. That'll stop people from buying them instead of a 9070XT, 10060XT, or whatever the hell they make next.",Negative
AMD,There is a lot of hysterical over-reaction to this. The 5000 and 6000 series cards are still going to be supported. The worst that will happen is that they will miss out on very minor tweaks that are added to drivers for later cards - additions that will make no significant difference to most users.,Negative
AMD,"I had my 6950xt for almost 3 years now, i was always so fucking happy with how well that purchase turned out as i skiped a 3080 for this card.  Turns out fuck me i guess i should have gone for the 3080 for 100 bucks more, at least I would still have gaming driver updates.   I'm running this card until i can upgrade and then never toching another amd card ever again, fucking bastards.",Negative
AMD,"People can drama as much as they want to, but GPUs will be working as usual. The vast majority of users won't ever notice the slightest difference.",Neutral
AMD,"The USB-C port is a VirtualLink port that provides 27w of power to anything plugged into it, a DisplayPort (iirc 1.3 or 1.4?) connection, and USB 3.1 support for data transfer. This means you cannot use it for fast charging/Power Delivery, or acting as a regular USB-C 3.1 port once its disabled. It will only be able to drive a montor with DP if the monitor gets power from somewhere else since its now been disabled on the VirtualLink port.  Its a cool little port that could do loads of other things like support and power an external drive bay but unfortunately it died off because VR didn't hit as mainstream as companies liked and valve/htc never managed or bothered to make a Virtual Link adapter that would handle all of your headsets data, display, and power needs. iirc only the PSVR2 makes use of its intended purpose which came out 5 years after we first saw these on the RTX 20 series GPUs",Neutral
AMD,"First time I heard about them having a usbc port, I have an XTX spectral white and mine doesn't have it, nor did any of the ones I saw when looking up prices.",Negative
AMD,"AMD stated on their discord it was an error and that line doesn't show anymore in the change log, not sure if it's true or not because I cannot test it",Negative
AMD,Just speculating but most probably to prevent unofficial firmware updates in the future.,Neutral
AMD,"Yes. It won't receive support for new games.  From the driver announcement directly: ""New Game Support and Expanded Vulkan Extensions Support is available to Radeon RX 7000 and 9000 series graphics products.""  https://www.techpowerup.com/download/amd-radeon-graphics-drivers/",Neutral
AMD,"It will be fine. Just because they aren't adding new features doesn't mean that the existing ones won't be supported. Most ""new features"" are very minor tweaks that make very little real-world difference anyway.",Neutral
AMD,"I bought an used 6700xt just this week, chose it over similar priced Nvidia models cause of the 12gb vram, thought it'll age better. Shit luck I guess.",Negative
AMD,"I'm pretty sure Steam stats showed that as one of the most common cards out there. Probably why they're giving it the shiv, to get people to finally upgrade to newer cards.",Neutral
AMD,"I was on a RX 580 just a month ago, I wonder how many years of useless drivers I had downloaded lol",Negative
AMD,"copy/paste from another thread  *If you are lucky enough to live in a country with 5 year warranty OR if you bought your 5 or 6 series gpu card, maybe even 7900XTX with the usb-c within your country's warranty limit, go ahead and use it, this is NOT what you paid for!*",Negative
AMD,yup my 6800xt has been treating me very well,Positive
AMD,You definitely got your money out of it. The 9060XT is a pretty good upgrade.,Positive
AMD,I still have mine as a backup,Neutral
AMD,"Yea same, I love my 6900xt but this is quite a bummer to see. Great GPU with great performance.",Positive
AMD,"You will get game support i believe,thats maintenance, just no new features",Neutral
AMD,"If you are lucky enough to live in a country with 5 year warranty OR if you bought your 5 or 6 series gpu card, maybe even 7900XTX with the usb-c within your country's warranty limit, go ahead and use it, this is NOT what you paid for!  I am lucky enough to live in a country with 5year warranty, if AMD is ending driver support to my XTX within the next 4 years i for sure know the store would have to honour a warranty claim.",Negative
AMD,>And i dont understand why this isnt getting more traction than it currently is getting  Too busy calling NVidia the devil for cutting support on their 12 year old graphic cards.,Negative
AMD,Well AMD is known to do this. They don't support their GPUs for long compared to Nvidia.,Neutral
AMD,"Lol multi billion dollar company owned by the cousin of Nvidia.      ""Underdog""",Neutral
AMD,"They’re still being sold too, so you’d expect to have a decent amount of life out of them.  This is a bullshit move to show you’re only providing 3 years of proper driver support from their initial release date.",Negative
AMD,Remember how much shit Nvidia got for dropping 1000 series support earlier this year? Cards that are like 9 years old? Imagine if they had dropped 3000 series support.  I can't wait to not see a single tech influencer call this out to any similar degree even though it's way worse.,Negative
AMD,"Heck, the RX6600 was still THE budget card for a lot of markets. It was that or a 3050 for a lot of people if they wanted a new card, or maybe an A580/B570.",Positive
AMD,I bought my 6900xt in 2022 and it runs everything I play flawlessly,Positive
AMD,I had a 5700xt and was planning to build a new pc soon and honestly seeing all this is dissuasing me from doing so,Negative
AMD,"I'm pissed off there's nothing wrong with my RX6950xt ... This was my first AMD card. But if they're only supporting their hardware for 3 years then I'll go back to Nvidia, that's outrageous honestly.",Negative
AMD,"My TUF 6800XT actually has been a royal pain in my ass, RMA'd twice and likely dying as it stands so kinda torn between upgrading or just throwing my 2080 Super back in...",Negative
AMD,"Bruh my friends 6900xt goes toe to toe with my 3090. This is fuckin’ bullshit, that thing should still be playing games at LEAST on 1080p medium-high in another 4-5 years from now.   Hell the 1080 TI is almost 9 years old now and still compares to budget GPUs today.",Negative
AMD,they need more money,Neutral
AMD,My 6950xt still has warranty and still is a fucking beast.   This really sucks...,Negative
AMD,"The 6-50 XTs released in october 2022 (3y) and already not supported, while nVIDIA dropped the 1080Ti (a 2017 card) only this year?  Were the drivers too good or what?  GPUs in 10 years will get 1 update on release then become obsolete right after.",Negative
AMD,"Bro, I bought my 6900XT in January... 2025",Neutral
AMD,"I bought my 6800 this year lol, but it's ok, it's only for the new games that will be released from now on. And even if it has no ""support"" you will still be able to play them, people play games on rtx1050 still......",Positive
AMD,"Same here. Man everywhere I turn there is a corporation or some politician saying ""fuck you, what you going to do about it?""  Like this in particular doesn't strike me as the biggest deal so long as the card is still usable for years to come, but it's just yet another unwanted plunge in the behind when I just want a day off from being completely fucked.",Negative
AMD,"copy/paste from another thread      *If you are lucky enough to live in a country with 5 year warranty OR if you bought your 5 or 6 series gpu card, maybe even 7900XTX with the usb-c within your country's warranty limit, go ahead and use it, this is NOT what you paid for!*",Negative
AMD,Got my 6900xt in 2022. This is such a bullshit move by AMD. Wtf.,Negative
AMD,The USB C port on these graphics card causes bizarre display issues when used. Ask me how I know…,Negative
AMD,Yeah I was still hoping for it but... yeah...,Neutral
AMD,"You have like me (7900xt) the int8 FSR 4, which is a massive upgrade from FSR 3. Its easy to install with Optiscaler.",Positive
AMD,"Fsr 4 isn't even coming to the 7000 series. You can get it to work because the leaked code/software, but even then, on the 7000 is doesn't work as well as 3.1 because of the overhead.  But yeah, this situation is fucked man, they should've given way more support to these cards.",Negative
AMD,Lol exactly the same: My Rx 6700 XT is 1 year old and it is losing support same time as my 7 year old GTX 1060 6gb.   Also Bf1 randomly started crashing on my PC for no reason. Lovely stuff.,Negative
AMD,I literally just bought a 6800xt to replace my 2070.. might return it for a 7800xt.. just didn't want to spend £75 more for a card that doesn't out perform the 6800xt.,Negative
AMD,Another reason to accept Nvidia tax. At least they don't fuck you over after just 4 years like AMD,Negative
AMD,"Actually, the 1060 6GB gets support for new games like Battlefield 6 and Bloodlines 2 that the 6800 XT is NOT getting support for.",Neutral
AMD,I will never understand why people downgrade to AMD.,Negative
AMD,Radeon had always a driver support of a average 5 years.      Its dont mean that you GPU will become unusable. you will just dont get the new feature.,Neutral
AMD,"Last time they did this, they dropped all Vega-based GPUs, while still actively selling mobile CPUs with Vega iGPUs.  It's not like they're supporting the consoles either, the console manufacturer gets complete control over the chip there - it's Sony and Microsoft who make new optimizations for them.  EDIT: Oh, it's even worse. This drops RDNA2, so once again, an iGPU that they're putting in new mobile CPUs for next year. The ""new"" Ryzen 3s and Athlons will launch without GPU driver support from day one.",Negative
AMD,"Whilst I agree dropping support for a 2020/21 product is bad, the AM4 platform has been one of the most widely used in history and honestly don’t see them dropping support for another couple of years.",Negative
AMD,The only reason AM4 is still going is as an outlet to get rid of extra server dies. If it didn’t make business sense they probably wouldn’t do it,Neutral
AMD,Even the 6600 is still very popular as a budget card,Positive
AMD,"The RX 5700XT was very popular. It's what brought RTG back from the grave, and made money for the development of the RX 6000 series - with which AMD's popularity peaked. That was the single out of ""modern Radeon era"", where they were actually somewhat competitive with NVIDIA. The 7000 while decent, was overshadowed again by the Green Side.   Also, this is a bad look either way. Who's to say they won't drop the 7000 series by next year?",Positive
AMD,The 6x50's came out in 2022 and is already being dropped   Next time im paying nvidia tax,Negative
AMD,"I love my 5700xt but I've had it for 6 years now, it's served its purpose though I wish it could get support for a bit longer.",Positive
AMD,"I have a RX 5700 XT and it's still a very capable GPU despite the lack of RT.  They didn't cut support of the RX 6000 series because the hardware was deprecated, they dropped it for no good reason.",Positive
AMD,It's possible that this support has something to do with Windows 10 support as well. The timing seems rather coincidental,Neutral
AMD,Why? They've done it previously with TerraScale. I'd say 5 years is pretty good.,Positive
AMD,"that been the average time for driver support from Radeon since his existance.  that also dont mean that you will not be able to play futur game,  you will just not have the driver-level optimization that driver update offer.  I know i have been there with my 5700 XT( end of support 2021) before early 2023 where i brough my 7900 xt.  Driver support is a no-issue for 99% of game.",Neutral
AMD,"This doesn't mean new games won't start with that card, it means AMD isn't adding new features or updating the drivers to fix bugs for specific games. This doesn't instantly turn your card into ewaste.",Negative
AMD,You'll get security updates and bugfixes for the foreseeable future. The fact that they bothered to backport features from RDNA 3 and 4 at all is actually pretty uncommon. nVidia almost never does that kind of thing.,Neutral
AMD,"AMD is green team, no?",Neutral
AMD,"AMD is still supporting these cards. Bugfix and security updates are going to continue for years to come. If anything, 4+ years of feature updates like this is actually pretty good. People aren't reading the article.",Positive
AMD,"4-5 years is the average radeon driver support.  My First GPU purchase HD 7950 2013 -> end of support 2018  R9 390/5700xt  2016/2019 -> end of support 2021.     The USB-C  is still working as a Display Port, it his powering capability that are disable, cause its a huge source of issue since his existence.   Modern VR headset have thier own station now, the USB-C of GPU as been a one time thing that also nvidia as abandoned.  Most viable VR headset today are all going wireless so its even less of a issue.",Neutral
AMD,I mean… who is actually using a USB C port on the back of their GPU? Thats super niche and random.  Maybe it was causing issues? Not everything is some grand conspiracy.,Negative
AMD,"There's no concrete timeline, but games as recent as BF6 could technically start having issues.",Negative
AMD,"It just means that they won't be getting any new features in upcoming drivers. They're still supported, and driver updates will contenue for the foreseeable future.",Neutral
AMD,"4-5 years is the standard from Radeon since his existance.  And driver support are not impacting much. i know that alot of people are new to the PCmasterace but driver support is not a big deal at all.  here a exemple    [https://www.youtube.com/watch?v=YLlBCdcloJQ](https://www.youtube.com/watch?v=YLlBCdcloJQ)   The R9 390, card that got end of support in 2021. Game that don't rely too much on heavy modern tech are running completely fine.",Neutral
AMD,Guess I'll wait until mine dies to see what upgrade comes next.,Neutral
AMD,You're still going to get security updates though.,Neutral
AMD,So AMD historically has had much shorter driver support compared to Nvidia. I'm not really surprised.,Neutral
AMD,Fuck amd and nvidia they both suck and both do shtty things.,Negative
AMD,The Open-source Linux drivers will keep chugging along unaffected.,Positive
AMD,People praising one corporate over another until the one they trust fucks them over too. Choosing between two evils.,Negative
AMD,this was a very surprising thread to read on r/ amdmasterrace,Negative
AMD,"If its running the games you want, then I wouldn't upgrade, just my opinion.",Neutral
AMD,"The RX 580 is such a champ though, played so many recent-ish games on that.",Negative
AMD,RX470 here 😂 we don’t need to but it does mean our next upgrade is cheaper now 🤷‍♂️,Neutral
AMD,"If you're on a budget, get the 5060/ti . It's a sizeable upgrade from that gpu.",Positive
AMD,"the cards will still receive drivers, they will not receive feature updates or game specific updates. But will continue to function and be supported by new driver releases. Yall are pissing and screaming over nothing lmao.",Neutral
AMD,7000 series got screwed worse than any other generation... Took 2 years for FSR 3 then dropped them from future FSR versions 6 months later. AMD is a dumpster fire,Negative
AMD,I have a 5700 xt and I'm with you 💀,Neutral
AMD,lol,Neutral
AMD,No it means that next year they will stop updating it and in 2028 they'll stop everything like on that news the bear minimum lol,Negative
AMD,"If this is a pattern, its getting axed from all updates in 2 years.",Neutral
AMD,Did you even read this thread? People with AMD cards are pissed and have likely already lost them as future customers.  I got a 6800XT just 2 years ago and was expecting to keep using it for several years. Cutting support so soon is nonsensical.,Negative
AMD,The 6800XT was considered one of the best cards available for its generation lol,Positive
AMD,Wrong. Thats a great card and was the price performance king for months,Positive
AMD,it seems AMD cards doesn't age as well as Nvidia cards so maybe paying extra is worth it,Neutral
AMD,"Up till today it was a great generation, 6800XT was a much better buy than the 3080, i had one till the 9070XT launched, still performed great at 1440P.   They've got to reverse this decision, it makes no sense, the 6800 and above still perform well in modern titles.",Positive
AMD,"If it was Nvidia doing it, the sub would be bashing Nvidia. Since it's AMD, this is normal",Negative
AMD,That’s not what that phrase means.  Planned obsolescence would be AMD designing them to DIE at some point just outside of warranty. This is simply them not supporting the cards with newer features because they lack the hardware.,Negative
AMD,The card won't get new features or a game optimisation driver anymore just security and maintenance patches,Neutral
AMD,At least I got 4 years,Neutral
AMD,Don’t say that. It’s just RDNA 1/2/3 were bad products that have short lifespans. GCN cards were super long lifespans and RDNA4 is also promising. You should not buy a 6700XT in 2025 when FSR4 was released and praised.,Negative
AMD,"Ending driver support for 5 year old GPUs that are still plenty powerful enough to run new games is just shitty. Period. You absolutely will notice a difference with new games without updated drivers. 6700-6900 cards are still PLENTY powerful enough for new titles.  This is the equivalent of Nvidia ending support for the 3000 series, which would be completely insane   Even Nvidia doesn’t pull that crap. They give 8-10 years",Negative
AMD,I don't have one but I could see it being useful for a monitor that has USB-C in using the DP mode. Single small connector with no adapters.,Neutral
AMD,"Hijacking to get this information to the most people.  If you are lucky enough to live in a country with 5 year warranty OR if you bought your 5 or 6 series gpu card, maybe even 7900XTX with the usb-c within your country's warranty limit, go ahead and use it, this is NOT what you paid for!  I am lucky enough to live in a country with 5year warranty, if AMD is ending driver support to my XTX within the next 4 years i for sure know the store would have to honour a warranty claim.",Negative
AMD,Sooooooo non-issue for everyone except for maybe <1% of RX7900 users?,Neutral
AMD,huh. Always wondered why my 2070S FE had a usbc port but newer RTX gens didn't. That explanation makes sense.,Neutral
AMD,You would think this would lead to lawsuits since they are removing something people paid for.,Negative
AMD,kinda funny this happens right around the rumor of Valve's new VR headset being green lit,Neutral
AMD,Is it known if it also disables the USB-C port on my 6900XT?,Neutral
AMD,Why do it now that more headsets support it? It probably also works with the Bigscreen Beyonds and the Vive Focus Vision.,Neutral
AMD,It was ahead of its time when unfortunately. Really sad it died.,Negative
AMD,"you can't flash anything onto the GPUs via the USB-C port. it doesn't ""connect"" to GPU memory it passes through to the motherboard, and it would be useless when the GPU is powered off anyways. you'd still need a reprogrammer that physically connects to the eprom on the GPU PCB to flash unofficial BIOS if the on-board security doesn't reject it. These aren't like USB ports on TVs or your phone that let you flash new firmware.",Neutral
AMD,Is this not a bit soon? Like maybe I’m wrong here. I’m not really that well-versed in this world of software updating but aren’t Nvidia still supporting the 20 series which came out around the time of the 5000 series if I remember correctly?,Neutral
AMD,"You got down voted, but that link has put me over the limit for sure. On to the 7000 series",Negative
AMD,"The ""New game support"" is some bunk, because in a technical sense the 6000 hasn't worked for many new games which have Ray Tracing as a requirement vs a little feature you can turn on to increase the power draw on your PSU while also making the FPS drop.",Negative
AMD,Features not game support.,Neutral
AMD,What the fuck ? All the while the current gen consoles use RDNA 2. What is going on,Negative
AMD,"Was just discussing this with a friend. Its so recent too, I'm shocked. I was reading about Nvidia updates for their 1000 series bro thats like a decade plus old no? What is AMDs reasoning here  Like these RX cards are the best value for Vram in my opinion, anything 12 or 16 gb is pushing 1000 dollars for Nvidia, but only around 400-600 for a 16 gb 6000 RX Card   Make it make sense",Negative
AMD,What a stupid decision.. highest market share product no longer supported?? Seems like a stupid gut punch to your consumer.  How do you plan to maintain market share at all,Negative
AMD,"Well, it's time to save for a new GPU...",Neutral
AMD,"This actually does the opposite effect since the customer will think that the GPUs only gets 4 years of driver update now, they will switch to Nvidia for better support and long term usage. Not to mention the 2nd hand market value will drop even further when AMD cards are already selling cheaper on 2nd hand market. AMD actually shot themselves in the foot lol.",Negative
AMD,"Looking at the Steam Hardware Survey right now, the Top 10 AMD GPUs right now are:  1. ""AMD Radeon(TM) Graphics"" (2.2%) 2. ""AMD Radeon Graphics"" (1.95%) 3. Radeon RX 6600 (0.92%) 4. Radeon RX 7800 XT (0.72%) 5. Radeon RX 580 (0.66%) 6. Radeon RX 6700 XT (0.65%) 7. Radeon RX 580 2048SP (0.61%) 8. Radeon RX 5700 XT (0.61%) 9. Radeon RX 7900 XTX (0.51%) 10. Radeon RX 570 (0.41%)",Neutral
AMD,"If AMD gets away with this shit, what do we think Nvidia will do next? This is horrible, and people dont seem to get it",Negative
AMD,"literally just bought a 6800 few months ago, pissed off.",Negative
AMD,It genuinely seems like amd is fine treating their discrete gpus as an afterthought and don't really care about increasing marketshare at all.   They already bungled the whole fake msrp thing with the 9070xt earlier this year.   Might as well just pay the extra Nvidia tax of $50-100 for your card so you know it'll at least get more than 3-4 years of driver support.,Negative
AMD,This this this. I bought a bnew 6600 like a fucking idiot 😭,Negative
AMD,"The 6000 series is 5 years old next month. Not that I'm happy with this situation but it's not 3 years only.  Anyway, it's sad because my RX 6800 was probably one of the best purchases I have ever made price / performance and it's still going strong.",Negative
AMD,didn't you get the memo... come on everyone knows AMD is a small startup indie company. That is not only the underdog but is treated like swine by the big players. I mean Intel/Nvidia are totally out to get AMD and do everything in there power to miss treat and destroy AMD reputation every chance they get.   That is why Nvidia keeps support for there cards for so long. Not because there a good company or care about the consumers (They don't care about the consumers but that isn't the point.). They do it to make AMD look bad...   Poor AMD I hope they survive another ten years off there billions of dollars of profit. It must be hard. /s,Negative
AMD,"It's just that Nvidia still actively supports GTX 900 and 1000 cards. There's still gonna be a handful more gpu drivers supporting them.     But yeah, I doubt these threads will reach 1k upvotes or get any media coverage. The underdog can't possibly be held accountable for anything.",Neutral
AMD,"The double standards from the tech influencers is really disappointing. It feels like they’re too afraid to upset AMD fans to actually tell the truth.  It’s a real shame, because while AMD makes amazing CPUs, their GPU division feels like an afterthought, yet they get glazed to the high heavens. They’re quick to point out any issues or flaws with Nvidia, yet the AMD side barely gets a whisper in comparison.  What’s funny is that despite their rhetoric, most of them use Nvidia cards in their own PCs.",Negative
AMD,"From what I know, Nvidia still has not dropped the support. They will droplater next year. Unlike AMD they just gave a years notice.",Neutral
AMD,My decision between the RX 6600 XT and the A770 genuinely landed me with an Intel card that is being supported for longer. What a wacky world.,Neutral
AMD,"I was about to buy an Rx 6600 when the last unit sold seconds before I got it, And I ended up with an arc A750 because a 3050 6gb was almost 50 bucks more.",Neutral
AMD,I got mine last year in February...   ![gif](giphy|qQdL532ZANbjy),Neutral
AMD,mine too. this is a brain dead move.,Negative
AMD,my 2080Ti is from 2018 is still getting updates and can even use the newer transformer dlss model,Neutral
AMD,I got a 6700 XT last month lmao,Neutral
AMD,10 was is supported thru this month i think,Neutral
AMD,Well you will be able to play most games but I know some anticheat requirements are newer drivers and who knows if it will allow drivers that are not updated regularly.,Neutral
AMD,No New features. As in new versions of fsr or future technology. Your card will work fine with driver updates,Neutral
AMD,"there's no rtx1050, you might be thinking of the GTX 1050, which still had full support up to this month, and will keep getting security updates untill october 2028.",Neutral
AMD,">*people play games on rtx1050 still......*  GTX 10 series (including the 1050) is only no longer getting game-ready drivers after October 2025, so technically the GTX 1050 from 2016 was supported by NVIDIA longer than the RX 6800.",Neutral
AMD,"Among other things, they're capable of fast charging. That power draw off a GPU could explain that.",Neutral
AMD,The USB C port on my RX6800 drives a couple of daisy chained displays through a simple USB-C to DP adapter.,Neutral
AMD,"Yeah but I'm not going to buy another GPU when the 6900xt is perfectly fine lol...... not looking to upgrade until after the 9000 series.  Edit: oops, misread that reply",Negative
AMD,"Yeah, my 6800XT is 2 years old and already loosing support, I guess I will buy Nvidia in time to play the PC port of GTA 6",Negative
AMD,Have a peace of mind and go with Nvidia. Go with the 5000 series gpu and pretty sure the driver support will last a good number of years.,Positive
AMD,"Well...maybe the 7000 series will be next. Looks like AMD isn't going to support their GPUs for a longer period anymore. If you want to keep your GPU for more than three years, your only option might be to go for NVidia.  I always used AMD GPUs before but now that they are going in that direction, I might switch for long term peace of mind.",Negative
AMD,"Yeah, after this, I'd rather buy even Intel, at least that is affordable and they have to get competitive on the driver field. I play mostly 5+ year old games anyway but I will buy the RTX 6000 series whenever that comes out",Neutral
AMD,"Read the article, it literally says it's not getting the bf6 update, I wasn't expecting fsr 4 or later, but drivers should last at least 4 generations so people can upgrade every 5 gens",Negative
AMD,"My thinking was it's at least more logical for the 5700xt since it's six years old and can't play some newer forced RT games like doom dark ages (without linux workarounds).   As others pointed out the last 6000 series release was the 6750gre only two years ago, so that's way more of a kick in the dick.",Negative
AMD,"I know the 5700xt is still usable for 1080p, but there'd at least be some logic to discontinuing drivers for it since it's 6 years old now and games with forced RT like Doom dark ages and Indiana Jones are becoming more common.   Discontinuing support for the 6000 series, where the last new gpu only came out 2 years ago is a serious dick move.   Real shame they can't have Nvidia's track record of long support.  Glad I upgraded from a 6600xt to a rtx 5070 this year.",Negative
AMD,Given how piss poor games are at release it kinda turns them into e waste,Negative
AMD,With modern game optimization... well...,Neutral
AMD,Are you slow? Rtx20 is running the latest dlss4 transformer model.,Neutral
AMD,Amd is red team,Neutral
AMD,"You can literally see the reddish shift in the picture used in the article   My god, how can this be so hard? Here's the 9000-cards reveal, from the thumbnail to the video it's all red: https://youtu.be/DhQecal2kE8  Hell, the adrenalin driver uses red everywhere",Negative
AMD,To switch FROM green team,Neutral
AMD,Last week I installed the latest drivers to play Arc Raiders on my 2080. If I owned a RX6900 I would not have optimized drivers for that game. 4 years is HORRIBLE.    Also disabling the USBC port is an objective anti-consumer move. Why would I buy anything from a company that will take away my actual i/o?,Negative
AMD,"The difference is gpus were a lot less pricey back in 2013 and also the gpu performance increased a lot faster between then and 2018.   Like a rx 6800 is fairly on par with a 9060xt 16gb, a brand new gpu, for raster at least.",Positive
AMD,"I have a PSVR2 that hooks up to the PC via USB C, though I have the PSVR2 PC adapter, and I have no idea if it would even work hooked directly to the card. But Im sure there are some people out there using it, however niche it is.",Neutral
AMD,"Sure, so why were people angry over this happening to the 10 series from Nvidia then? Also something like a 6900 XT seems at least relatively competent for current games and could probably use with some driver optimisations if a game comes out that could need them.   Like I understand if this is the norm in the industry and Nvidia are just outside of that norm in supporting their cards for a longer period of time but it seems like AMD really does not need the bad press for their GPU division right now. Like how much harder would it have been to keep supporting at least the 6000 series since that was when the positive attention started being given to the Radeon division? Even if the support is relatively minor and they only occasionally go in and add Support for the latest games if they happen to need it I feel like that type of patch cadence could work quite well at least to not have users feel like they are being left in the dust.",Neutral
AMD,"Yeah it's just a shame a number of people don't realize this. This whole ""team red"" was just clever marketing. Nvidia does the same stuff. They are corporations trying to make money, not friends or teams. The only friend and team should be your wallet with whatever product has the value you desire at the time.  Still this driver support, or lack thereof, has certainly made me glad I sold my 7900 XTX.",Negative
AMD,"Seriously, though! I've been running BF6 on it, and it's doing so good! My only delay is that it takes a bit longer to load assets at the beginning of the round. Otherwise, it keeps up with everything just fine!",Positive
AMD,"There’s a reason they lost pretty much all of ATIs marketshare, which at time of acquisition was around 45%+.   They never had a vision or path, just played catch up and even did that poorly, to this day.   AMD fanboys are some of the loudest but most delusional group I’ve ever seen on the internet.",Negative
AMD,"then dont buy amd ever again, problem solved",Neutral
AMD,"The only good thing about this gpu is it's raw performance for raster and the 16gb of VRAM buffer, everything else is mediocre at best.  - Awful RT performance - FSR2/3 is absolute garbage - It had subpar performance on DX11 and OpenGL games for more than 1.5 years until AMD decided to improve drivers",Negative
AMD,6800xt is is still great mid range card apple to compete with 9070's and 5070's,Positive
AMD,It's lack of dedicated ray tracing hardware or AI upscalers was always going to make it age worse than the competition.,Negative
AMD,"I got it for 300$ a few months ago lol.....new, not used",Neutral
AMD,And you base this on what?,Neutral
AMD,Nvidia also stops updates.,Neutral
AMD,But them literally killing a port for no reason is making it obsolete. The guy is right 100%,Negative
AMD,"It is planned obsolescence, just by a different manner/technique. What you described is the traditional way, but if a manufacturer says that we will limit X features only to products A and B, C and D will be shut out, then C and D does become obsolete because if you want those features you will be forced to upgrade.",Neutral
AMD,"You can plan for the product to fail by design or action. In this case they planned to make the cards obsolete with an update. Pro Apple move, they have sold me the last chip I will ever buy from them.",Negative
AMD,In turn intentionally making them obsolete. It's the same outcome. He's right.,Negative
AMD,I wonder what else do I need to upgrade in case I will change to 7 or 9 series,Neutral
AMD,"Ah yeah, one of two times in the past 15 years AMD has competed with NVIDIA seriously and up to the flagship, and it's a bad product now. RX 6800 and up outperform the 9060 XT, no reason to drop support.  Also, RDNA 4 is a carbon copy of the initial release of RDNA 1, 5700 XT and 5700 | 9070 XT and 9070. 5600 XT and 5600 OEM-only | 9060 XT and 9060 OEM-only.  This is 5 year old hardware that's been sold new as recently as 2024 (which is still put into current gen mobile APUs of 2025) being delegated to security updates the same time NVIDIA is delegating Maxwell and Pascal from 2014 and 2016 to security updates.  If they're gonna drop game-ready support on their users after only a few years, then screw Radeon.",Negative
AMD,"And they'll keep being powerful enough to game modern games. For years and years. I've played enough games before and after of ""game ready"" and noticed nothing to came to the conclusion that is something that hardly justifies the constant GB downloads. Disabling the USB is a dick move, that part I must agree.",Negative
AMD,It was supposed to be a game changer for VR.,Neutral
AMD,I'm trying to get all my shit working from a single port. I've tried muxers and other tools and nothing has worked well.   Needless to say I'm bummed.,Negative
AMD,"It's sick for laptops, but that specific use case I feel is less crucial on a desktop card",Neutral
AMD,Are you certain that this is actually possible? This seems like such an edge case.,Negative
AMD,"If the warranty laws in your country actually work the way you think it does, then almost every budget Android phone could be returned to the store because driver and OS support ends within a few years. Somehow I doubt that’s the case.   Warranties in most of the world simply mean the product has to continue functioning. There is no requirement for a manufacturer to continue improving the product after selling it to you.",Neutral
AMD,"its a non-issue for most people but I'd still be pissed if Nvidia released an update that disabled the VL port that I paid for on my 2080Ti for seemingly no reason. this is not a good precedent if its permanent even if it doesn't affect you or most people.  If one day AMD goes ""study shows most people dont use more than two monitors, so we're releasing an update that disables all but 1 of your HDMI and 1 of your Display Ports"" would you still be fine with that since you're not personally affected? ""most people dont use 2< monitors anyways""",Negative
AMD,"Those <1% are still customers that are going to be left eating dust, it's still a bad move despite you not being affected.",Negative
AMD,"Yeah it only the ""FE"" model that has it, mine doesn't",Neutral
AMD,The more I know.  What are they for then?,Neutral
AMD,Nvidias latest drivers literally still support Maxwell which first released in 2014 with the 750Ti  They are gonna cut off support soon to only 2000 series and up from 2018,Negative
AMD,"Ya like am I just supposed to buy a new card because my 6700xt is 4 years old?!  What the fuck is ""maintenance mode""??   I've been an AMD exclusive PC gamer since 2019 (3x GPUs and 2x CPUs) but if they're gonna fuck my card for no reason I might have to consider Nvidia for my next upgrade.",Negative
AMD,"They're still supporting pascal, but that's ending pretty soon.",Neutral
AMD,"Does feel early considering the hd 7970 was supported from 2012 to 2022, while never highlighted for new games after like 2016, the updates seemed to add access… to me this points to a radical shift in gpu design coming up, trying to support as few structures as possible",Neutral
AMD,It doesn't matter too much tbh. I've used drivers from years ago and have no problems. It only matters if there is a bug.,Neutral
AMD,"Way too soon, the 980ti I bought in 2015 is only now losing support.",Negative
AMD,"Yeh. AMD basically left the 6000 series to die as soon as they left the shelves. They haven't gotten any new features in like 2 years now, with FSR3, as far as I remember.  And the 7000 felt like a rushed launch that only exists to jump into the AI train and keep investors hooked. Don't expect much from that one either.",Negative
AMD,"not really. 5000 has no RT at all, and 6000 was there first gen so its half baked. The cards will still work but they are quickly falling behind with modern standards,  also people need to learn to read. they are still getting drivers, just not game specific ones. they are both RDNA cards so they will still benefit from the drivers being released.",Negative
AMD,AMD GPU division is too poor to support 10+yr GPUs like Nvidia does in their driver support,Negative
AMD,"Yeah, I edited my comment to include the link. Some people can't be bothered to do even 30 seconds of double-checking, or just got offended AMD is dropping support for GPUs so fast.   The 7000 series can be good, but I'd be a bit hesitant about those as well. They don't have AI cores, which could give AMD an excuse to drop game support for them in a couple years.",Negative
AMD,"It's not getting new game support for two games that DO NOT have RT, Battlefield 6 and Vampire: The Masquerade – Bloodlines 2.   I'm the biggest critic of AMD's approach to RT and half-hearted ""enabling"" it on RDNA 2, but it can at least run it (you can get a good 60fps+ in Doom Dark Ages with some upscaling and lowered settings, even on the lowest RX 6600, for instance). It also supports other DX12 Ultimate features like Mesh Shaders. It's gotten game support for lots of ray tracing games up until now. There's absolutely no reason to drop support for RDNA 2.",Negative
AMD,"From the driver announcement directly: ""New Game Support and Expanded Vulkan Extensions Support is available to Radeon RX 7000 and 9000 series graphics products.""  https://www.techpowerup.com/download/amd-radeon-graphics-drivers/",Neutral
AMD,"Not just current gen consoles, there's handhelds being manufactured and sold *right now* with RDNA 2 iGPUs, that will presumably lose game-ready support.   The RDNA 2 based PS5 Pro is even getting FSR 4 somehow made to run on the shader cores, while RX 7000 series doesn't get it despite having better AI capability.",Negative
AMD,I mean they probably put some thought into this but I dont get why. Feels like it would just drive people to nvidia for free,Negative
AMD,They don't care they axed all 7000 users from future upscaling updates and theyre still selling the cards... at a time when upscaling is needed now more than ever. I knew my dumbass should of went with nvidia.,Negative
AMD,I just got mine this past Wednesday. Smh,Neutral
AMD,"Mines 3 years old now and the fans are starting to get a little noisy, i'm running more compute workloads than gaming loads now so i looked into getting a waterblock for it but no dice! The gigabyte aorus model doesn't have a compatible waterblock because it's a special board design!",Negative
AMD,"you bought a 2 generation soon to be 3 generation old card and are mad its not being actively supported? Kinda on you bro lmao. Also they are still getting drivers, you just wont get game specific patches for those 2 generations, which honestly are rarely needed anyways.",Negative
AMD,"Yeah looks like it given Nvidia only just dropped 10 series support for cards first released in 2017, the extra $50-100 buys you real value on longevity it seems.  AMD own goal on market share, just as they we’re getting more support from gamers",Neutral
AMD,"It’s not game support it’s feature support. Nvidia 20 series still gets latest DLSS support.   AMD slept while Nvidia laid out the ground for AI, upscaling and RT. Some of us saw this coming long ago because you can’t flip a switch and catch up YEARS of development, not in chip technology. AMD was too busy getting pats on the back from their “Raster  and native resolution only bros” meanwhile Nvidia saw the dead end road and the performance demand that software was going to require years ago. We’re seeing that now. GPUs hitting a wall and games requiring more and more raw power that isn’t there and can’t easily be made to be there.   AMD is just now getting off the ground with RDNA4/5, but that means they need to ditch prior gens because guess what, they can’t support what AMD is doing. They just have to cut their losses and stop trying to throw resources at developing new features for prior gen cards. Nvidia doesn’t have that problem because their prior gen cards support some of the feature they are putting out since the arch was being laid out from 20 series forward.   It was always worth spending $50-$100 more for those features and support. Always. Some of us have said that for years.",Neutral
AMD,Wonder how the fine wine folks are going to handle this,Neutral
AMD,"Ahh yes good point, it is longer than 3 years, I must have misremembered.  Looking it up, the first batch started coming out 5 years ago next month and the first batch of the line up a year  and a couple of months after that.  So it is a range on the series that is somewhere between almost 5 years - 3 years 10 months.  That’s also only the initial release dates btw.  But on top of that they were plagued with availability issued.  So there was definitely a delay on a lot of people getting them.  Which when you combine with the fact they’re still being sold, so there’s warranties and guarantees on those newly sold cards, it seems like a real mismatch for AMD to put them in maintenance mode.  They should have discontinued the cards before putting software support on maintenance.",Neutral
AMD,"BF6 wouldnt run at all on my driver from last year, it was the only one not causing stutters on my 6700xt. (Common issue)  So not getting updates for new games is going to mean I get to play my backlog!",Negative
AMD,I’ll be keeping my 7800XT for as long as she keeps chugging along.,Neutral
AMD,"You dont even understand what BF6 update mean.  Its just a optimization patch that you do not require to play. On a  generic game that everyone will be deserting in less that 2 week.  We already have FSR4 call int8 FSR4 with Optiscaler.  Everyone would like to have infinite Cash, infinite lifespan and infinite driver support.  That not how reality work.  I have used GPU out of driver support for years and years and that never impacted anything.  IF EA decide to block Old GPU, its EA fault, not AMD.  Radeon user are less that 5% of the market and its only going down, no matter what.  The radeon team have no incentive to give anything to customer that are not profitable.  People need to stop to thing that Radeon is in competition with Nvidia, its not. Nvidia is a 5 trillion dollars company that lets Radeon on life-support just to avoid Anti-trust lawsuit/monopoly law.",Negative
AMD,"Funnily enough, my parent's PC banck in the time had a Radeon HD 4350 (yeah, back in the 2000s) and the card was supported for around 7 yeras before support was dropped...and those cards were as entry level as you can go.  Seeing high end cards in the RX 6000 series going out of support after half that time is so wild.",Neutral
AMD,"How? Their logo is black/white, being green originally. Never has it been red.",Negative
AMD,that specific I/O is completely broken from what I've heard.,Negative
AMD,"Performance is not a metric in Driver support, its only time and time.",Neutral
AMD,"Nothing realy matter. bad press or not, Nobody purchase radeon gpu,  Radeon as lost marketshare even with the 9000 series launch.   We are less that 5% of the marketshare now. and as a radeon user since 2008,  Driver support is not something important.  Radeon is only losing marketshare, Period and will alway be no matter what.  All the praising from Tech reviewer of the RX 9000 series are made up. only a extreme minority of people buy them.  The only thing that matter is price. If a new game dont support my decade old GPU. i will simply be a patient gamer and wait my next GPU upgrade to play it.  In bonus instead of paying 100$ for the game, i will be paying 20.",Negative
AMD,Nice!,Positive
AMD,But bro…..it’s got 16GB VRAM. Haven’t you heard? VRAM fixes everything.,Neutral
AMD,"Nah, it was an outstanding card",Neutral
AMD,"Clueless. Also, for it's price it was an absolute monster of a card (still is and if AMD did not do this idiotic move and continued supporting it - it would literally go down as one THE GOAT Gpu's of all time.",Negative
AMD,"It's literally losing driver support for new releases, how does that make it a great card? There is no sugarcoating it, AMD made a bad decision on this one.",Negative
AMD,it has ray tracing cores.....,Neutral
AMD,"I could be wrong, but maybe it's based on the fucking thing we're commenting on? Buy hey what do I know.",Negative
AMD,Based on this year the 10 year old GTX 10 series got its driver support dumped. While 5 years old RX 6000 driver support gets dumped,Negative
AMD,Are you for real?,Neutral
AMD,Nvidia stopped support for the 1000 series after 8 years and people were mad about it. Now AMD stopped supporting GPUs after 5 years...,Negative
AMD,Yes but not many bends over to justify it like they do for AMD,Neutral
AMD,Is the expectation that every device will receive updates forever?,Neutral
AMD,Do not change to 7 series. RDNA4 have 12x better AI performance and RDNA3 will ends up more or less just like RDNA2.,Neutral
AMD,Except sometimes games just straight won’t work at all without current drivers. Plenty of examples of this. Some games literally won’t let you launch if you’re not current.  If Nvidia pulled the same crap people would have their pitchforks out.,Negative
AMD,Can you ELI5 why it would’ve been?,Neutral
AMD,"In my country, Norway? 100% sure.   The rest of the world, you may experience pushback for sure.      Its about what you can resonably expect when buying a product, i can still buy a 6800xt new in a normal shop in Norway, unless they spesifically state that its about to lose support meaning you can expect trouble playing new games then i dont see how they have a case.   In Norway im almost certain they would not get away with only stating it in the EULA for example.",Neutral
AMD,"Actually I'm just guessing that 1% may arguable have right to file a return if within warranty time. In Europe I'm pretty sure some shops would comply with it; you bough it with a functioning USB-C port and it's no longer working? Refund.  I generally like AMD better than Nvidia but fuck removing functionalities and, in this case, very much fuck AMD.",Negative
AMD,It's called a VirtualLink port.,Neutral
AMD,"To expand a bit to what the other person said, they're virtual-link ports that were meant for VR headsets. The plan was for the headset to just need one cable connection for data transfer and power which is the usb c port on those Gpus. Unfortunately a lot of the headset manufacturers decided to use separate cables for everything combined that with VR not taking off nearly as much as companies thought, the port is being sunset by AMD now. I believe Nvidia stopped even adding them after the 20xx series so it's hasn't been a thing on team green for a while now too.",Neutral
AMD,"Yeah, and like I totally get the criticism and defence here. Nvidia was probably not doing a huge amount of actual optimisation or fixing for these old GPUs but they still got the occasional fix and for some reason AMMD just doesn’t want to commit to that which seems odd to me.",Negative
AMD,"The 750ti I used for 7 years, sold and replaced with an rx6700, will (theoretically) outlive it. Lovely.  https://preview.redd.it/bj2s3ryzrcyf1.png?width=720&format=png&auto=webp&s=2ca86fbd880c65730f6e8b69c6fb8d7ab8625bdf",Neutral
AMD,They dropped Maxwell and Pascal recently   RIP my 1060 your days are numbered,Negative
AMD,"Hilarious given the 2000 series can play many titles on ultra, the performance growth since 2000 has been pretty negligible",Neutral
AMD,Yeah this is dog act from AMD. I have a 6800,Neutral
AMD,"Here's my thought, after 4 years hasn't AMD gotten as much performance as possible out of their architecture? I've never been on top of updates and I don't think I have ever had a problem playing a game. My current setup is 5600X on Asus B350-i Strix with a 6700XT that I managed to snag at MSRP a few years ago. To be fair the newest game i played on it was Elden Ring.",Neutral
AMD,"I would assume that maybe they’re planning on doing something with Redstone and focusing all of their work on this generation but maybe also the last generation since they got so much criticism for that. If the rumoured FSR 4 for RDNA 3 thing happens to pan out it does make sense that they would do this type of thing and focus their work on the generations that actually have the machine learning hardware to use a modern upscaler, but it’s still just a big shame.",Neutral
AMD,What? Nvidia drops cards the quickest lol,Neutral
AMD,"thanks for the heads up. mind if i pick your brain? whats your thoughts on the reason why they'd disable the USB C port on the 7000 series? i have a 7900xtx, i never used the port, but now i have FOMO lol",Neutral
AMD,AI cores won't be a thing used by games until 10th gen consoles have them.,Neutral
AMD,"Battlefield 6 plays really well on an RX 6700, it doesn't NEED any extra support to provide a good gaming experience.",Positive
AMD,You can still play new games. They just won't have game specific optimizations in the drivers.,Neutral
AMD,"Driving me back to Nvidia, have a 6750 since 2022 and was just eyeballing 9070xt but not anymore.",Neutral
AMD,"Lol. Personally won’t go nvidia cuz of the 12vhpwr thing. I probably wont get a tier of card that could burn, but I still wouldn’t go nvidia. It’d be for me to sleep better at night.",Negative
AMD,"Nvidia is currently distracted by AI, so AMD is probably less worried about them as a competitor in the gaming market.",Neutral
AMD,Return it ASAP,Neutral
AMD,Nvidia will no doubt follow suit now.,Neutral
AMD,"Hearing people talk about AMD over these last 2 years has been maddening. Don't get me wrong, they make good products. But if you want longevity and halfway decent features, there is no choice. I've been right there saying it too with you.",Negative
AMD,"Yeah, the “raster only, native only” people were insufferably smug and now they have to eat their own words.  It was obvious that physical limits on GPU tech were reaching their zenith. You can only improve things so far with native rendering, and process nodes are about as small as you can get. Graphics are already photo realistic so the easy gains are now longer there.  Now is the real time for raytracing and DLSS tech, and as you said Nvidia has been laying the proper groundwork since the 20 Series. The 30 Series saw a huge step forward, and the 40 and 50 Series have begun to perfect it, making games look and run more fantastic than ever.  I love my 4090, been using it for over 2 years and the technology it has is incredible. I don’t need to worry about it losing support next year because Nvidia actually plans ahead rather than playing tribal politics to score brownie points.  Honestly this debacle shows you why it’s important to properly plan out and develop these technologies with the future in mind. DLSS and RT are the future for graphics technology and Nvidia are miles ahead of the competition.",Neutral
AMD,Is this the one where you get massive fame stuttering as the game loads new textures?,Neutral
AMD,Ooh yeah I misread the reply.   I'm aware if the INT8 option. Just a shame they've dropped RDNA2 like this lol. I do understand it lacks the hardware but it's still a punch to the balls - i could have a 3080 back then but decided AMD because everything else i had was AMD lol.,Negative
AMD,"It's not about a particular game, I don't care for bf6 too, I'm also not saying that the GPU is instantly bricked, but that AMD is sending a clear message that says ""hey this GPU is not important to us, the experience might not be perfect and we don't care"" which looks really bad for a 5 year old GPU, nobody is expects fireworks from an older gen GPU, but they need to acknowledge the fact that a lot of their customers still use rdna 2 and the argument of Nvidia beating them by a huge margin is irrelevant when it comes to that, you sold a product, support it, easy",Negative
AMD,Radeon is red,Neutral
AMD,My point was things moved faster back then with gpu improvement so a 4 year old card was genuinely obsolete.  Now however even a lower end rx 6600 is still fine for 1080p medium on most modern games.,Neutral
AMD,Yeah! And it's good for Linux too lmao,Positive
AMD,"I still own mine, do you know that It struggles to keep up with recent games unless you abuse the hell out of upscaling? Have you used FSR?   OpenGL and DX11 performance was bad until AMD rewrote their drivers, I'm not talking out of my ass.",Negative
AMD,I think he means it *was* a great card up until this short sighted asinine decision.,Negative
AMD,No it doesn't. It computes ray intersections on the shader cores. Even RX7000 only has ray accelerators. Worse RT and only having a not great upscaler was always going to age the card.,Negative
AMD,It doesn't. It has ray *accelerators. T*hese are not the same thing as actual dedicated cores like what Nvidia has with their Tensor cores,Neutral
AMD,And that makes it unusable? A 6900xt will always be better than a 1080ti,Positive
AMD,">Nvidia stopped support for the 1000 series after 8 years and people were mad about it.   Yeah, and that's not reasonable.",Negative
AMD,It's also just a fact of life for GPUs to stop receiving updates. Just not for 2 generation old ones.,Neutral
AMD,I don't understand what your criticism is here tbh,Negative
AMD,"Not at all. Every device dies eventually, nothing last forever.  In my opinion, a good benchmark for ongoing support would be: if a gpu can play modern games ably, it should have support.  My 6950XT which is three years old plays games on ultrawide 1440p at high/ultra graphics at 100+ fps. I certainly don't need an upgrade anytime soon, so I won't buy a new gpu simply because of artificial divisions.",Neutral
AMD,Go back and read the OP again. A currently active feature is being disabled. That means it's not simply about no more updates.,Neutral
AMD,For me it was back when the Oculus Rift came out people had so many problems with the USB and power delivery.  The idea of having both video and power coming from the GPU seemed like the solution we have all been needing.,Neutral
AMD,I’m also Norwegian. I really don’t se komplett accepting this as a reason for return. If they do I’m returning my 7900XT and buying a 9070XT instead.,Negative
AMD,"It doesn't matter if it's not something you use. It's still a part of the product you paid for. It's still YOURS, and a CORPO is taking it from YOU.   There was no reason to remove it, it already exists and it costs to nothing to leave it alone.  A corporation stole from you, you should be pissed even if you don't use it.",Negative
AMD,"Your car manufacturer just disabled the motors that control the side mirrors from your steering wheel, how annoyed are you?",Neutral
AMD,"Well, shit, as long as schu2470 is fine with it, then I guess it is fine. /s",Negative
AMD,"I agree it’s an odd/bad choice to remove it, but yeah I also can’t imagine a scenario I would plug a USB into my GPU if we’re being realistic. Majority of cases and motherboards have many USB ports nowadays",Negative
AMD,"Thats very unlikely, its only for hardware failures, it was disabled via software, and warranties usually dont cover software",Negative
AMD,"To be fair, I think you can still use it on the 20 series which is different from what AMD is doing here.",Neutral
AMD,"AFAIK VR works better with nvidia anyways, one person i know ended up selling their 9070xt and going back to nvidia because of complications with VR and simracing",Neutral
AMD,It depends on which part of the GPU stack youre looking.  In the past 7 years. The top end GPU has improved on average performance 326%. (5090 vs 2080ti)  If youre talking just RT with new much improved transformer denoiser and upscaling that hits 2000 series hard. the difference is well over 400% or 4x better.  There is a much more than negligible performance increase its just that not all parts of the GPU stack improved at the same rate.  A 2060 vs 5060 is only about 80% better and doesn't come with a massive vram boost like a 5090.  But at the same time 60 class cards have been getting cheaper each gen. Not even including inflation since the 20 series much cheaper including it.  A 2060 launched at 349$ which is 450$ today while a 5060 is 299$,Positive
AMD,This is telling me to go nividia for my next gpu. So rediculous.,Negative
AMD,"I'm not expecting more performance out of my 6700xt, I'm just expecting it to continue to work the same as it did before they stopped supporting it. Unless I'm misunderstanding what ""maintenance mode"" means because I can't find anything about it.",Negative
AMD,"it is a real shame, just another thing to hurt their rep",Negative
AMD,May I direct you to: [https://www.reddit.com/r/pcmasterrace/comments/1ok68yj/comment/nm8e32a/](https://www.reddit.com/r/pcmasterrace/comments/1ok68yj/comment/nm8e32a/),Neutral
AMD,"I honestly have no idea. My initial thought was that there's a problem with it, especially since they directed users to a significantly older driver version (25.3.1 from March 6th 2025). Maybe they found something in newer drivers that could cause crashes or damage? But if that's the case, I would have expected at least a brief explanation in the note (i.e. ""Disabled USB-C power deliver on RX 7900 products, due to to the risk of damage to connected devices"" or something like that.)   I really hope it's not just a decision to disable the feature because they don't want to support it anymore. But unfortunately, that seems to be their motivation for a lot of decisions, lately, like dropping RX 5000 and 6000 game support. AMD has a history of this, they dropped game support for Vega APUs a while back while they were still being actively manufactured and sold. Nvidia, on the other hand, supports their GPUs for far longer. Their latest driver still supports GTX 900 series Maxwell cards, although that's expected to end this month along with GTX 10 series support, probably with one final driver update today or tomorrow.",Negative
AMD,"""...I never used the port"" - that's the reason, right there! It was designed in the hope/expectation that VR would really take off. It didn't and they won't bother ccontinuing to support a feature that nobody uses.",Negative
AMD,The PS5 Pro is about to use AI-based FSR 4 upscaling and frame generation.,Neutral
AMD,"But don't you see this is a precedent for the future? It doesn't matter if it runs well rn. BF6 is already well optimized, one of the few games released this year that are. But most games have been horrendous recently. The 6000 series is now effectively obsolete if you want to play newer games.",Negative
AMD,"I mean, they're not going to brick your card so ofc you can still use it to play new games. It just won't have driver support for those games.",Neutral
AMD,"This is what I was wondering. I’ve had the 6700xt for 2 years now I think, and was like oh shoot do I need to upgrade already 🙄",Neutral
AMD,"There’s no reason for them too, they win by maintaining the status quo on their end.  Their valuation expansion, revenue expansion is unprecedented, and their not struggling on adding improvements to their new product lines.  Sounds like AMD is stretched.",Neutral
AMD,"AMD has always had a VERY loud (but smaller) fan base. It's always been like that ever since I got into PCs back in 90s and it was no different when AMD acquired ATI in 2006. In fact, I'd say it's only gotten louder when it comes to Radeon GPUs, sadly they haven't had much to show for it besides just hating on Nvidia and yelling louder. All they did was enable AMD to become complacent.  Heck just look at this sub or any PC sub here lol (or any influenreviewers on youtube, where most people here get their info from). Still as delusional as ever. There's a difference between supporting a product/company while still being critical of their poor choices and decisions...don't think AMD Radeon division saw that much, even as their market share dwindled from near 50% at acquisition to measily 6% now.  One thing I've noticed is that say 8 out of 10 people buy Nvidia... most of the time the 8 people will install their Nvidia cards and move the F on while the 2 AMD buyers feel the need to get on Reddit and tell the world about how they are now Team Red and how much Nvidia sucks. That's bassically how it's been and why real world markets don't look like Reddit PC subs.",Negative
AMD,"Well said.  People love shiting on Jensen and his jackets but the guy's been there since day 1 and when it comes to looking ahead and making the right calls, he's been spot on most every time. Yah he's still a corpo and runs Nvidia like a greedy corporation but you can't deny the fact that not only does he know what he's talking about (as an engineer) he knows what the F he's doing.  AMD can't even figure out how much to sell their cards for lol.",Positive
AMD,"Yes, but it never goes away. Using an older driver doesnt create the issue though.",Negative
AMD,"and its will continue to be fine, driver support are unrelated to card capability.  Here a exemple R9 390  [https://www.youtube.com/watch?v=YLlBCdcloJQ](https://www.youtube.com/watch?v=YLlBCdcloJQ)   95% of game running completly fine following the card capability. only 2 game not working mainly cause of too modern tech implementation and horrible optimization.  And you can just not trying to play these game, or refund them if not working and wait your next GPU upgrade to play them. well patched and at -80% discount.",Positive
AMD,"I completely agree and I am familiar but you are missing my point, which is that AMD literally shot this 6800XT in the foot with their own decision on FSR4 and updates non-availability - which is MORONIC. Hence why i said it could have gone down as one of the GOATs.",Negative
AMD,not enough people give a shit about RT,Negative
AMD,"It still has HW RT.  It has transistors on the die that handle RT  The tensor cores don't deal with RT, it's the ""RT cores that do that""  Tensor cores deal with denoising like ray reconstruction, and DLSS",Neutral
AMD,"> In my opinion, a good benchmark for ongoing support would be: if a gpu can play modern games ably, it should have support.   This is a very vague bar.   >I certainly don't need an upgrade anytime soon  And this doesn't cause you to need one either.",Neutral
AMD,"That is true, for the folks who actually used PD on the USB port that is clearly a bad thing.",Negative
AMD,"7900xt have not lost anything as of yet, so no case. But if AMD is not getting enough pushback to change their decision on this moving forward, its only time before both you and me lose support, if that happens within 5 years we have a case indeed.  >I really don’t se komplett accepting this as a reason for return.  I dont see how they cant.",Negative
AMD,>There was no reason to remove it  Says who? They obviously won’t write in detail a reason to remove in a driver changelog,Negative
AMD,Don't give BMW any more ideas.  The seated heats are bad enough.,Negative
AMD,"The thing is, most people don't like the newer frame gen. Ray tracing performance improved tangibly but that's about it. I'm talking raw raster performance. I have a 4080 and I still play with RT off when able because the performance penalty is never worth it.  4k performance on the 2060 is only like 5% worse than the 5060. 1080p performance is a starker difference but isn't that much more significant. And comparing the 2080Ti, a 1000 card to a 5090, a card you can't even get for MSRP at 2k, is odd, when it's performance difference in raster is definitely not 326%.  Ultimately my point being that the 2080 Ti especially can play anything including Cyberpunk at ultra (at 1080-1440p) at over 100fps",Negative
AMD,thanks bro,Positive
AMD,Probably because it was designed for a standard that never got off the ground. The whole reason it’s there is for powering VR headsets but I think only a handful actually can do everything over USBC and don’t require additional hookups or power.,Neutral
AMD,"I hope i dont get jumped on for this, but i dont think AMD's GPU department are in the best position right now.      Regarding the port. i did a little reading between the past and now. apparently it will still have some functionality but the charging function will be disabled. ill link the post i read and you can see for yourself and judge the veracity of the article?  https://videocardz.com/newz/amd-disables-usb-c-power-on-radeon-rx-7900-moves-rdna2-rdna1-gpus-to-sub-branch-in-latest-driver#:\~:text=Someone%20at%20AMD%20apparently%20uploaded,Polaris%20to%20the%20legacy%20branch.",Negative
AMD,fair play,Neutral
AMD,"The PS5 Pro is what, 10% of the install base?  AAA games already get released half-baked.  Devs aren't going to learn how to use something like AI cores/NPUs until they're certain that the large majority of users have them.  Look at the pushback there's been over the handful of PC games that don't run on the GTX 1080TI because they were designed for hardware with ray tracing cores. Hell, look how long it took after the release of the RTX 20 series before we started getting AAA games that supported ray tracing at all.",Negative
AMD,"No, you don't have to. You don't ""need"" driver optimizations specifically for every game. The AMD driver will work fine more likely than not. I've gone years before without updating graphics drivers before and still could play what I wanted.",Neutral
AMD,"I do agree with that, It also has been confirmed that FSR4 can run on these gpus using the int8 version leaked dll, so it's even more stupid...",Negative
AMD,You're missing the forest for the trees. We're talking about how a card will age. RT is only going to be more common and get heavier. The moment the ps6 gen comes out with actual dedicated ray tracing hardware most big games will follow and Radeon 7000 and older will far terribly against the Nvidia or Intel counterparts.,Neutral
AMD,"It might be a vague bar, but at least it is an idea to build upon.  I won't need a new gpu unless I want those features, and then I will suddenly have the urge to upgrade. Which is exactly what all of this is about: they want people buying 9000 series gpus and failing that, 7000 series.",Neutral
AMD,"Ah I thought you meant the usb c port support. If they enter legacy mode within the next 4 years it’s a win for us no doubt. That being said it’s such a dick move from AMD that id be pushed back to Nvidia, where 6+ years older gpus are still somewhat supported.",Negative
AMD,"Next, they'll have crank windows and require a subscription to electronically raise and lower your windows.",Neutral
AMD,Its is 326%  Thats on the low end actually.   https://www.techpowerup.com/gpu-specs/geforce-rtx-2080-ti.c3305  Techpowerups performance data base uses 1080p for cards slower than the 3080.  And comparing 4k for a 2060 vs 5060 is rather dumb when both will be choked to death with vram constraints and perform the same. At 1080p the intended res the difference isn't massive still but its about 80% faster.  And cyberpunk did launch right after the 20 series launched in 2019? Or 2020. So it makes sense they'd kill that game without using RT.   using RT though. Even a 3090 at 4k doesn't give a good performance with dlss performance.   While a 5090 hits 4k 60 with rt ultra natively,Negative
AMD,"Speaking from personal experience (I have a 7900XT and a first gen oculus quest), I never used the port on my GPU, I used my case's front USB-C, and it worked fine for all of my PC VR shenanigans.",Positive
AMD,"Sure, but (a) it's still a feature that's being explicitly removed from the card with (as of yet) no explanation, and (b) USB-C power delivery is useful for many things besides VR, such as driving a small portable monitor.",Neutral
AMD,"Yes, that's what the driver release notes said, it's just affecting the charging functionality. Still, no explanation given is not the way to handle this.",Negative
AMD,"Pretty much every new game supports DLSS, XeSS, and/or FSR 4, which are all AI.",Neutral
AMD,"> It might be a vague bar, but at least it is an idea to build upon.   It's not though. Because it's that vague.   >I won't need a new gpu unless I want those features  And you have all the features your card was advertised with.",Neutral
AMD,"Yeah the usb c port is more of an edge case, i agree. That would be a weaker case, but losing the ability to play newer games, i really cant see how anyone could be refused a warranty case as long as its within the warranty and the consumer protection laws are on par with EU or better.  >That being said it’s such a dick move from AMD  Yeah, im actually fuming here.",Negative
AMD,ah cool big non issue then really i guess? well for most people im sure.,Positive
AMD,I mean I’m not saying removing it is a good thing. I’ve never used the one on my card but I’ve been told they weren’t particularly stable anyway. And also it seems like a pretty rare scenario that you’re running both a high end card like a 7900 XTX and also a portable monitor so if it’s the kind of thing that already isn’t working great I can see how they might just drop support rather than spend time and money keeping it up to date. Losing features is almost always bad but I also understand the idea of pulling support for something that only exists for a very niche section of users. The classic pre-proton “why isn’t this being released on Linux” problem of if you can fit everyone who’s doing the thing into a Best Western conference room it’s probably not cost effective to continue support and not supporting it while still keeping it active on the card is selling people a product that might not be able to do the things they assume it can do. It’s kind of a “no good options” situation.,Negative
AMD,i agree with that sentiment,Neutral
AMD,This conversation got started with you saying that the 7000 series don't have AI cores.  So whether upscalers are AI is not what we're talking about.,Negative
AMD,What is your goal? I am not understanding that part. Are you in support of this or against it?,Neutral
AMD,You said AI isn't used by games because consoles don't use AI. I said the PS5 Pro does use AI upscaling. You said the PS5 Pro is a small portion of total consoles and devs don't use AI. I pointed out that essentially every new game includes AI upscaling features.,Neutral
AMD,">What is your goal?  To have a conversation, primarily. What else could it be lol  >Are you in support of this or against it?   This isn't really a 'for it or against it' issue. I wouldn't complain if they extended support, but I don't understand complaining.",Neutral
AMD,Do you have a thermal limit set for the CPU in BIOS? is it downclocking because there's an 80ºC limit on the CPU?,Neutral
AMD,"why would your brother care about you building it for him? anyway, if you really wanna know what the best bang for your buck is put all the parts into pcpartpicker and compare the prices",Negative
AMD,"Not sure, maybe worried something won't work.",Negative
AMD,"Looks like a solid build. If you really got the 9700X for that little it's a no-brainer. For the regular price of the 9700X I would have looked at a tray version of the 7800X3D.  > Is the PSU enough for the build (9070XT more specifically), or would a 750W PSU be OK ?  For the build the 750W unit is more than enough. However it is worth noting that the price gap between 750W and 850W units is exceptionally small at the moment and it may be wise to just get the similarly priced 850W unit for more optionality if you choose to upgrade in the future.  > Is the B850 mobo really needed? Or is a B650 (E?) enough? I see contradictory information on that  A B650E mainboard is pretty much as futureproof as a B850 but a normal B650 does not support PCIe 5.0 GPUs. It's currently not an issue but if you could see yourself upgrade to an RTX xx80 class (or AMD equivalent card) in the future, you may hit the PCIe 4.0 bottleneck.  > For the 9070XT, I mostly see Saphire and XFX at lower prices > are these reliable brands? Or should I prefer Gigabyte or Asrock, for example?  Asus, Gigabyte & Co. often put in a lot less efforts into their AMD models than AMD exclusive board partners like Sapphire and XFX though I would definitely prefer Sapphire if given the chance (though I haven't heard negative things about XFX in years).  > Do I need to buy separately thermal paste?  Not necessarily. The paste that usually comes with the CPU cooler will suffice but it's not going to be great paste. Realistically it won't make more than a 3° C difference though   > Do I need to buy a GPU bracket/support/holder?  Depends on model and make but is worth exploring if your GPU sags.",Positive
AMD,"**Answers:**  **What do you think of the build? Is it solid or has it flaws?**   The CPU + GPU combo is rock solid. I’ve got the same setup and haven’t had a single issue running new titles on ultra settings. My 1080p monitor is the only bottleneck now 😅  **Is the PSU enough for the build (9070XT more specifically), or would a 750W PSU be OK?**   750W should technically be enough, check the GPU vendor specifications just to be safe.  **Is the B850 mobo really needed? Or is a B650 (E?) enough? I see contradictory information on that.**   I’m using the MSI B650 Plus and it works perfectly fine. Unless you need advanced PCIe 5.0 support or beefier VRMs for heavy overclocking, B650(E) should be enough.  **For the 9070XT, I mostly see Sapphire and XFX at lower prices. Are these reliable brands? Or should I prefer Gigabyte or ASRock, for example?**   I went with Sapphire—great cooling and no issues so far. If you're not into overclocking, Sapphire or XFX are solid picks. For more aggressive tuning, you might want to compare thermals and VRMs across brands.  **Do I need to buy thermal paste separately?**   I didn’t. My cooler came with pre-applied paste, and I run my CPU at 105W mode. If you’re planning to push beyond that, investing in a good thermal paste could help.  **Do I need a GPU bracket/support/holder?**   Yes—definitely recommended. The 9070XT is a heavy card, and a bracket helps prevent long-term sag or stress on the PCIe slot.  **Do I need anything else that won’t be provided with any of the components yet?**   If you’ve got a modular PSU, make sure you have all the necessary power cables. Also double-check for case screws, standoffs, and Wi-Fi antennas (if your mobo supports it).",Positive
AMD,"thats overkill for 1080p, but great anyway, you can even go 4k with that setup",Positive
AMD,"Thank you very much!  I was initially going for the 9600X, but seeing the price of the 9700X getting lower and lower, I just thought the extra € was worth it. It reached that price on Aliexpress (tray version) in SEP with the promo codes + shipping from a European warehouse.  Indeed, 750w and 850w PSUs seem very close in price. But I'll keep an eye on the 750w promos, in case I could save a few bucks there and invest them somewhere else in the build.  Just checked the price of B650E mobos and it seems like the price difference with the B850 is decreasing quite a bit. Looking at the prices, it seems the B850 would def be a good deal.  Noted for Sapphire and XFX!",Positive
AMD,"Thanks a lot!  Glad to read that this is a nice combo, and it works great!  Regarding the PSU, I checked the Sapphire version on their website and they're giving this info:  Power Consumption: 304W Typical Board Power   System Requirement:   * Minimum 750 Watt Power Supply * 2 x 8-pin Power Connector * PCI Express® based PC is required with one X16 lane graphics slot available on the motherboard  How can I confirm, say, the [MSI MAG A750GL PCIE5 II](https://www.msi.com/Power-Supply/MAG-A750GL-PCIE5-II), would be OK?   When I scroll down to the specs table, I see the +12V says 744W. Is that the one?  https://preview.redd.it/efvm02c95owf1.png?width=1303&format=png&auto=webp&s=42426270ef2bb5541c9ac9563dae0c5b7e8d2d2f  Regarding the cables to plug the GPU, which ones did you use ?   I've read about at least 2 issues where people would use the cables from the GPU box, and it was recommended to use the cables provided with the PSU instead.  Do I understand correctly that I must use 2 PCIe (8 PIN to 6+2 PIN) cables, and only use 3 6-PIN ends, and not use the 4th 6-PIN from the 2nd cable ?  And noted regarding the other things to keep in mind! 👌",Positive
AMD,"You don't need to look into that much detail, if a PSU is rated 750W then it will definitely work.   I went for a non modular PSU so i didn't had to worry about cables. So i don't have much idea, for me it was pretty much plug and play.   Probably somebody else can help you on the cable doubt.",Neutral
AMD,"OK, thanks a lot!",Positive
AMD,My Radeon 6650 XT is holding on for dear life,Neutral
AMD,"I hope it becomes common practice for developers to explain what kind of performance is expected with the system requirements they give in the “additional notes”, like some devs already do.   For some games, having the recommended hardware allows you to play at 1440p, high settings, at 60fps. In other games, “recommended” only allows for 1080p, medium settings at 30fps with upscaling.",Neutral
AMD,"Resident Evil Remakes all run very good and look awesome, I don't think we should be worried.",Positive
AMD,Capcom really meant it when they said they would no longer immediately support Windows 10 huh...,Negative
AMD,My Ryzen 5 3600 is still worthy!  https://preview.redd.it/3zuuf1jsr5yf1.png?width=585&format=png&auto=webp&s=5fdcb61e69a49ff1dfa3a0dd8b5a3e71c393a5fd,Neutral
AMD,"1660 vs 5500 is not close, that sounds like a VRAM limitation making it unable to run on the 1650S.",Neutral
AMD,pretty tame requirements,Neutral
AMD,Going to assume this is 1080p,Neutral
AMD,"Im betting the CPU sysreq is only due to Windows 11 minimum requirements, then again who knows",Neutral
AMD,If this doesn't run on w10 somehow then Capcom is fucking insane. Over 40% of PCs are still and will be w10 for at least another year,Negative
AMD,Lies.,Neutral
AMD,That means no Windows 10 anymore?,Negative
AMD,Just like monster hunter wilds. And we know how that turned out.,Neutral
AMD,If your pc is equivalent or close to a console it’ll run games that consoles will play,Neutral
AMD,"Its going to be just like every other release and long as you have a capable GPU you will be able to play it. I remember playing village on a rx 580 and a  i7 4790k medium settings averaging 100fps and 80 during cutscenes. Those requirements are only there as a standpoint, if they had any lower requirements then it would make their team look bad as it would insulate they aren't evolving.   S/N check youtube after it releases",Positive
AMD,"interesting I wonder how the 6600 will hold up. Re4 definitely tested it and it runs moderately well, some issues in night village and heavier sections but overall above 60fps at 1440p.  These are pretty low specs to be honest. Makes me think they either have optimized the fuck out of it or it's gonna be similar to RE2/3 where sections are small and you move on. I'm gonna guess maybe 60gb or 70gb like RE4. Cool to know this!",Positive
AMD,Resident evil games were never demanding but always look good. Apart from 5 with that god damn ugly colour grading,Negative
AMD,Much much lower than what I expected wow,Negative
AMD,Looks like another game that i can play without problems on high graphics.,Positive
AMD,its so fuckin werid to see a 8700 still on the RECOMENDED list.... i replaced that thing 2 years ago.,Neutral
AMD,re games are usually good pc port since 7. I always got high fps with them. But the real question will be how good their rt is. Cause it usually sucks but they partnered with nvidia this time so this will be very interesting,Positive
AMD,rx 580 gang raise up!,Neutral
AMD,Well it isn't 5090 to start so that's a +,Neutral
AMD,"They are using their own engine and not UE5, but that being said 2060/6600 as recommended cards in 2025, is pretty impressive",Neutral
AMD,guess i'll finally update to Windows 11,Neutral
AMD,I did not know there was another RE game coming out.,Neutral
AMD,I hope they will add dlaa/dlss and option to turn off sharpening,Neutral
AMD,"My 1050 TI crying inside my case,",Neutral
AMD,Thats rather minimum specs aimed for minimum fidelity (1080p/30fps/low-mid) is my quess.,Neutral
AMD,"nice to see. i upgraded my pc this month for all triple a games going forward, and this game was on my list to purchase.",Positive
AMD,"Ugh, even games are only supporting Windows 11. Wonder how much Microsoft pays them off to make it not compatible with Windows 10.",Negative
AMD,That's fairly mild all things considered.,Neutral
AMD,My 6700 xt prevails once again,Neutral
AMD,What if that's the specs for ultra 720p. I wish we got the context here,Neutral
AMD,No windows 10 ?? well i will take my 70 euros elsewhere then i guess.,Neutral
AMD,wait...windows 11 required :(,Neutral
AMD,5070 at 1080p maximum settings lol,Neutral
AMD,Past RE games have been very good optimized but this time they're working with Nvidia and adding Path Tracing so i know it will be a mess,Negative
AMD,YESSIRRRR I WILL PLAY THIS NO MATTER WHAT,Negative
AMD,"A 1660 at a minimum? A 2060 as recommended?   Huh. Seems pretty well optimised.   Then again, the RE engine is pretty good. Except for Dragons Dogma 2 anyway.",Positive
AMD,Now THAT is optimization!,Positive
AMD,"forgot to add: ""This game is expected to run at 720p / 30 fps (with Frame Generation enabled ) under the ""Medium"" graphics setting.""",Neutral
AMD,Same with my 6600xt/ryzen 7 5700G,Neutral
AMD,Is 6750XT ok for now? 1080p myb 1440p gaming,Neutral
AMD,i’m convinced my 6600 will last another decade,Neutral
AMD,RE4R did this. Maybe eventually they add more details.,Neutral
AMD,"Exactly. It would help a ton. But at least we will always have the option to wait for benchmarks, I guess.",Positive
AMD,"They might add it, but in most cases the recommended is for 1080p 60fps with uspcaling 😄",Neutral
AMD,Their RE Engine is optimized for these type of games. MH:W runs awful because the engine isn't supposed to be used for open world games.,Negative
AMD,Doesn't the game have ray tracing? Hope we get system requirements for that,Neutral
AMD,I mean RE engine Is still last gen with some tweaks here AND there  I dont expect Capcom games with the RE engine to be demanding yet bar exceptions (DD2),Neutral
AMD,"How is it not close? It's a 10% difference at best, usually even lower, also the last few resident evil games performed slightly better on AMD than usual so that could be the thing making up for the small gap between these 2 cards.     Also it's very likely they actually chose the GTX 1660 card as a reference for minimum settings first and then tried to find the closest AMD competitor after that which left them with no option other than 5500XT, I guess RX 590 is close too but that card is a generation older than both.",Neutral
AMD,"PC requirements are more often general guidelines than set in stone rules and the reason why you may see a wide disparity like this is because that may just be what the dev had on hand to test against. Pretty much anything should be taken with a grain of salt until we get some real benchmarks.  That being said, the listed requirements are quite reasonable so it should run just fine on any PC built within the past five years or so.",Neutral
AMD,could also be down to driver overhead. amd cards can perform better than faster nvidia gpus on weaker cpus,Neutral
AMD,Honestly thats such an annoyance for me with most System Requirements you see for games. You have no idea what experience you can aim for with them way too often.,Negative
AMD,"Tbf, RE engine has always excelled at the closed maps of resident evil. Monster hunter wilds AND dragons dogma were just too large for it.",Neutral
AMD,"Different genre, RE engine suck for open world game like MHW & Dragon Dogma(still dissapointed with both) but non of the RE games are having issue with RE Engine. well it created for Resident Evil 7 to begin with",Negative
AMD,"Salty MH fan letting us know that it ran like shit, meanwhile 7, 8, RE2R RE3R and RE4R all ran incredibly well. Perfect on my end even.",Positive
AMD,"The RE Engine isn't good at open worlds like Monster Hunter Wilds and Dragon's Dogma II.  But it excels at linear games. Just look at how well optimised and how well Resident Evil 2 Remake, Resident Evil 3 Remake, Resident Evil 4 Remake, Resident Evil VII and Resident Evil Village run.  They shoehorned MHWi and DD2 into the engine and it was a disaster. But RE9 will run absolutely fine and that's evident from the low system requirements and how superbly the other RE titles using it have run.",Neutral
AMD,"RE engine was not made for Open World games in mind, that's why ir runs so bad. Rise has smaller maps and more static maps and runs miles better... It also has Switch Graphics, but still, if It was more like World, it would probably make the Switch explode",Negative
AMD,"That’s not true, consoles have way less OS overhead and developers specifically optimize for consoles. On equivalent hardware consoles always beat PCs.",Neutral
AMD,* If you're willing to play at the settings and framerate considered acceptable to console users.  Ignorance is bliss on console and I don't mean that as an insult.,Neutral
AMD,These are for non RT ! Later they will announce with RT,Neutral
AMD,"I played re4 remake around minimum specs-ish with a cpu below the minimum, and that ran at a steady 50fps at around medium-high graphics settings with 1080p.  Its probably something similar here, since every modern RE ive played has run great.",Positive
AMD,Modern resident evil games have been very well optimized though,Positive
AMD,fyi MH:W runs so shitty because the RE Engine isn't meant to be used for open world games because it's designed for Resident Evil games. It'll absolutely run better than MH. Probably mroe so in line with RE4R.,Negative
AMD,"6750xt with an R5 3600 here, just squeaking by but still getting me 60fps at 1080p on everything with a few tweaks here and there. I can't wait to upgrade to AM5.",Positive
AMD,"It's on par with current consoles, so anything cross-platform will run fine on it as long as those are still being developed for.",Positive
AMD,"Brand new? Nah, get an rx 9060 xt 16gb instead.   Used? Maybe, if at a fair price.   Already have it? Yes, it can handle 1080p high settings and 1440p mid settings just fine.",Positive
AMD,"Even better, path tracing! Let's just hope for the best lol",Positive
AMD,"nvidia's driver overhead is way better as of late. Plus, look at the CPUs they recommend/require",Positive
AMD,And they all look great too!,Positive
AMD,oh no,Neutral
AMD,Hopefully the same stands for the upcoming title!,Neutral
AMD,"I know (at least from RE7), and I hope I'm wrong on this title.",Neutral
AMD,"Mabye it's because I play a lot of slightly older titles but I normally play 1440p to 4k with my PC, specs^, perfectly fine",Positive
AMD,its not nearly a problem as hardware unboxed makes it to be,Neutral
AMD,"Not sure buut think may also be cpu bottleneck, same gpu and a 5600 r i can play current tittles at 1440 60 fps no issues, and even 180fps same res on better optimised games like doom",Neutral
AMD,"I've been playing Detroit Become Human at 4k max settings at like 60 fps. Now that might not be fair though because there's not much gameplay, sort of just walking.",Negative
AMD,Something a bit like this is probably the best you can get between 900-1000 (pre tax because US) [https://pcpartpicker.com/list/zhvgv4](https://pcpartpicker.com/list/zhvgv4) would be a bit cheaper but recently ram has gone up a substantial amount.,Neutral
AMD,And your country is? 7600x is a cpu.,Neutral
AMD,You can build a pc with no case if that money will go further towards a gpu. Obviously save for a case soon after…,Neutral
AMD,9060 xt is better,Positive
AMD,"Check Newegg deals, found one a few months back with a B850 and a 9600x for $350, but the B850 came with a free 500gb SSD and the 9600x came with a free 2x16gb 6400mhz ram set. Paired it with a 5070 for my old lady, whole build was $1100.",Neutral
AMD,Used market.,Neutral
AMD,Try the NVIDIA RTX 3060 if budget fits better frames decent price. Worth checking!,Positive
AMD,"i meant the Radeon 7600 XT 16 gig, oops",Neutral
AMD,what is even the issue? ur trying to find a gpu? whats the budget and the rest of the build. ur build might be able to change to fit a better gpu.,Neutral
AMD,"something around 7600 XT price, my budget is abt 900-1000",Neutral
AMD,list the rest of the specs of the build ur planning on buying. i can also make u a list from scratch if u would like. what country r u in? and it would be easier to contact me via discord if u would like but not required just prefered as im constantly available there.,Neutral
AMD,"...  7600x is 10$ more. though you'd probably benefit from more cores, so 7700x for 100$ more.  mobo with no availability?  300$ ram?  lame storage.  5070 is 30$ extra :shrugs:  lame psu for that price.",Neutral
AMD,"I'm running a 7600x with my PNY 5060 Ti 16GB card and it plays everything I want at the highest settings perfectly. Plenty of horsepower in that combo, and it should be good for some time to come.",Positive
AMD,"Optimize the build if it's faulty. I am more than happy to find alternate solutions.  Also, I just want to make sure, not trying to be rude, did you read all the software I listed?  Is intel any better?",Neutral
AMD,"Intel is good, the Core Ultra 265K is a good processor",Positive
AMD,Is there a cheaper alternative? It just barely misses the mark.(By like 100$),Negative
AMD,well my next gpu won't be amd if they drop driver support like that,Negative
AMD,"In all fairness, driver support isn't dead for RX 5000 & 6000. They just aren't getting game optimisations at all & are getting them less frequently. Still shitty nonetheless considering a RTX 2060 is still getting game optimisations off of Nvidia in spite of it being nearly 7 years old. 4-5 years for game focused drivers on windows is utterly shite, especially for RX 6900 XT owners.",Negative
AMD,You gotta why wonder why amd self sabotages like this   Clutching defeat from the cusp of victory,Negative
AMD,Reminds me of that time when AMD cut all GPU driver updates for any GCN GPUs in 2021. Keep in mind this would have included the RX 400 / 500 and Vega series which were recent.,Neutral
AMD,As soon as they got the AI money. oh man...,Neutral
AMD,"Amd fans, how do you explain this: 2015 tech, gcn 3 r9 380 vs maxwell 2 gtx 960, fast forward an entire decade, the first is long gone, whereas maxwell is still getting the latest driver as of november 2025? 581.57",Neutral
AMD,the AMD fanboy cope is strong around here it seems,Neutral
AMD,"Being poor sucks... Would love to pay the 900$ they are asking for the 9070xt, but I can't afford it",Negative
AMD,"AMD loves to drop support for GPU hardware. They've been this way for as long as AMD owned ATI. Meanwhile, Nvidia continues to support their hardware for years with drivers.  And this is why I still buy Nvidia.  Supporting your products with updates is actually valuable.",Negative
AMD,Like if they rly going to drop suport for 2-3y cards never again i will buy amd,Negative
AMD,Dropping full support for cards that are still in production is truly devious work by AMD.,Negative
AMD,I hope hardware unboxed looking at this matter and backlash their ass.   This company deserves to go bankrupt for doing this thing only.,Negative
AMD,"How do we feel about this?    I feel extremely put out as someone who spent a lot on a 6900xt. It's still a powerhouse card so very disappointed. As someone who is fully team red on this build I'm done with their GPUs for the foreseeable. I've a 750ti that still gets updates and my brother has a 1080 that also still gets update. 12 and 10 years respectively.   I understand I'll still get bug fixes but this is a middle finger for my ""flagship"" card to no longer get day 1 optimization. I get it's down a lot to architecture but it shows poor planning on their part. Fuck you AMD",Negative
AMD,Wait 6000 series is being dropped? Ah that sucks. I only got my 6750xt like 2-3 years ago. I'm keeping this card for another year then I'll have to evaluate,Negative
AMD,Rare anti-AMD meme.  ![gif](giphy|9lusxBBUsTz8Fk029b|downsized),Neutral
AMD,"AMD dropping Windows driver support\*  Meanwhile, on Linux, Valve developers and other open source contributors keep improving even decade-old hardware: [https://www.phoronix.com/news/AMDGPU-More-GCN-1.0-SI](https://www.phoronix.com/news/AMDGPU-More-GCN-1.0-SI)",Neutral
AMD,AMDead,Neutral
AMD,Looks like i made the right choice getting rid of my 6900xt and went crawling back to nvidia.  AMD already had awful stability and performance with their drivers. It’s only going to get worse.   Shit all over nvidia all you want for their business practices buy atleast they support their customers in the long term with a solid reliable product.,Negative
AMD,As someone who was considering getting back on team red - this really puts me off,Negative
AMD,"even when they're still supported, i had many instances where the newer driver crashes my games constantly or gives an anomaly. AMD drivers are a total hit or miss nonetheless.",Negative
AMD,"GPU = Nvidia, CPU = AMD, perfect combo, always.",Positive
AMD,This made me not want to buy AMD GPUs anymore.,Negative
AMD,"As someone who is a big AMD fan, and bought a 6950 XT day one, and recently bought a 7900XTX for my wife’s first ever new PC: this is *fucked*. They need a reality check, because this is going to suicide their brand, how can they even imagine dropping optimization support for a 3 and 2 year old cards. I dropped $1200 on the 6950 in 2022 and $850 on the 7900 XTC just weeks ago - it’s enough to consider returning the latter.",Negative
AMD,"People have been trying to explain to PC enthusiasts that high-end AMD cards are not worth the compromise and that the drivers are garbage.      I guess now “Team Red” gets to learn the hard way.      This fucking sucks though, because it looked like they were rebuilding a competitive brand with the 9070XT. Just to shit the bed shortly after.",Negative
AMD,That's AMD for you.,Neutral
AMD,they apparently put their money in nvidia stocks and are hoping they hit 6 trillion in value by shooting themselves in the foot lol,Neutral
AMD,Fine I will wait for my bouns at work and by eiither 5060ti 16gb or 5070 from Nvidia. I don't want to. But if AMD is going to do this to there GPUs then they learned nothing from why people buy there CPUs.,Neutral
AMD,AMD Drivers on Linux even for cards as old as R600 are still getting updates and features,Neutral
AMD,"Speaking of AMD drivers, has anyone been having issues with theirs? Mine has been crashing on random applications after this last update and was wondering if anyone's got a fix.",Negative
AMD,"I picked up an used 6700xt just this week, atleast i could have paid less, if the news was out last week.",Neutral
AMD,"How is this legal? I bought my 6750xt in 2024 brand new and the 6750xt itself was released in 2022, 3 years ago and already dropping game optimization? I know what to buy next when I need a new GPU.",Negative
AMD,In today's news Amd becomes Greedy and ~~Nvidia~~ 12v-2x6     Connectors well they Still do whatever they want  https://preview.redd.it/2k9ww5pmhbyf1.jpeg?width=3072&format=pjpg&auto=webp&s=cf5f8e7742a56a577dd1b00a3e048e7a540c3a1e,Neutral
AMD,Time to sell my 6800 lol,Neutral
AMD,the cards will work,Neutral
AMD,"I live in Europe, not if this is even allowed after that short amount of time.  Some 6000 cards are only 2 years old.",Negative
AMD,Might bruise a heel.,Neutral
AMD,"I didnt read it as like they're dropping support, I read it as no new stuff, which I kinda assumed was the case anyways",Neutral
AMD,Missing the last frame where the McPoyle brother falls and it turns out he was only a couple feet off the ground and lands easily on his feet unharmed. Everything is fine people don't buy into the drama.,Neutral
AMD,I guess upgrading last week was a good idea.,Positive
AMD,"""Buhh buhh the 6xxx series is no longer getting updates""...  You mean... optimization for RT/PT features those cards can barely use anyway?..   It'll be fine..  this is just bullcrap rage bait.",Negative
AMD,Not surprising with the focus on UDNA/RDNA5/Console moving forward.,Neutral
AMD,overreactionary as hell. Reddit in a nutshell,Negative
AMD,"They're still getting driver support, just no day 1 game optimisation drivers.",Neutral
AMD,"There are architectural reasons for splitting the support between RDNA2 and RDNA3.  One case for example, RDNA1 and RDNA2 have one FPU per shader where as RDNA3 and RDNA4 have 2 FPUs per shader.",Neutral
AMD,They are not dropping driver support. Stop spreading lies.  They are no longer actively optimizing for those cards. the exact same thing every GPU company ever has done with older GPU's since forever.,Negative
AMD,"If PCMR posters could read, they'd be very upset with you right now.  Please make this into a semi-informational YouTube video and maybe you'll get the point across.",Neutral
AMD,"“RDNA 1 and RDNA 2 graphics cards will continue to receive driver updates for critical security and bug fixes. To focus on optimizing and delivering new and improved technologies for the latest GPUs, AMD Software Adrenalin Edition 25.10.2 is placing Radeon RX 5000 and RX 6000 series graphics cards (RDNA 1 and RDNA 2) into maintenance mode. Future driver updates with targeted game optimizations will focus on RDNA 3 and RDNA 4 GPUs.”  Except it’s literally stated that they will NOT be getting day one updates for games in the future and will only receive security and bug fixes in the future.",Neutral
AMD,Didn't they also claim no more game optimisation and it was only getting security updates?,Neutral
AMD,The 10 series is nearly  a decade old. It's not fair to compare when the 6000 series is only 4.9 years old.,Negative
AMD,Yeah. Because people should always go to discord for updates? People are rightfully upset about what AMD themselves said in official press release. For all i know that discord could be for amd fanboys.,Negative
AMD,"You are right, but companies could also still provide better support long-term. They don't have to, but they also don't have to force the upgrade cycle.   People are more or less pushed to change phones every 2-4 years. Cars every 5-10 years. Appliances somewhere in that range too.  And for what? Just because a company wants you to consume more.",Neutral
AMD,"Sure buddy, keep drinking the AMD Kool aid rook.   The veterans know it's over.   We lived thru Vega and Polaris. We know the shit games AMD be playing.  AMD hasn't made a good GPU since 2012.",Negative
AMD,I’m wearing my tinfoil hat and saying that all of the posts about AMD “killing support” are a coordinated effort,Neutral
AMD,">The AMD cards are just not getting new features.  Yeah, we know, moving it to a legacy branch doesn't means it's getting no support, but less support, which is ridiculous, hence deserving of backlash. They're still dropping support for the cards, just not entirely.  I've been saying this for years that AMD has worse driver support than NVIDIA. It's not just that they drop products faster and push older ones to legacy branches, they also have worse driver feature support, even when they're still on the main branch.  Every time AMD releases a new radeon software feature, it's always locked to the newest cards, or the second newest. RSR, SAM, AFMF, RIS 2, Anti-Lag+, etc  I remember when RSR came out and NVIDIA responded with NIS for feature parity, and it worked on every GPU they still updated. AMD then claimed Anti-Lag+ couldn’t run on older cards, only to prove themselves wrong by adding support through Anti-Lag 2 in games using those same GPUs.  The reason why is because in games, AMD needs as many compatible cards as possible to convince developers to integrate their features. On the driver side, there’s no such pressure, so they use feature restrictions as a way to push upgrades.  That’s why I switched to NVIDIA. They’re just better when it comes to long-term driver and driver feature support. Getting things like RTX HDR, Video Super Resolution, DLDSR, and more on older generations is nice.",Negative
AMD,Thanks for clarifying that. Jesus. What's wrong with people not clarifying this?,Negative
AMD,So I guess intel for the win? lol,Neutral
AMD,"They are not dropping support. OP is a liar.  They are stopping doing active optimisation work on those older GPU's, which is basically standard practice. except this time they publicly announced it, and now it's being deliberately misinterpreted to make a news story.",Negative
AMD,lets get real outside of reddit most people keep there cards for 4+ years. The fact AMD's dropping support for a card that is only practically 3 year's old and is still being sold in some forms.  That is just straight up stupid. Then again AMD regardless of what anyone thinks isn't known for there great business decisions.   Sorta makes me wonder if Intel hadn't blundered so badly... would AMD still be in the position its in.   I am thinking of the 754 and 939 chips that were legendary and morphed into bulldozer.,Negative
AMD,"""Driver support isn't dead, only driver support to make games work better or introduce new features"" is a distinction that isn't even worth drawing. It's like saying ""She's not dead, only her brain is dead."" Like yeah, that's the important part.",Negative
AMD,"They’re no longer getting optimizations for new titles and AMD is disabling features on some cards already - with more likely to follow, for all we know.   You spend $1200 on a flagship AMD card at the peak of the GPU bubble and AMD rewards you with a bag of shit. Hi it’s me, I’m holding the bag. Guess I play games released before October 2025, now.",Negative
AMD,"""Just"" aren't getting game optimizations...those optimizations are the whole point of keeping an older card in use. If it's not going to be optimized for newer games or getting features, you might as well get rid of it and get a new one.",Negative
AMD,"All I can rationalize is they think they need to do this to buttress the demand for their latest offerings, if 6000 and 7000 cards were still delivering performance for value, consumers won’t buy their new card.   Problem with that assumption is, this is reputation suicide and now nobody will buy their old or their new cards both.",Negative
AMD,"AMD didn't drop total driver support for GCN cards. My sister is currently using my RX 580 and it is only a single driver behind my RX 6700 XT. The current, official Windows drivers for RX 500 and Vega GPU and APUs are only 2 months old. Only a couple weeks older than the current drivers for my RX 6700 XT.",Neutral
AMD,It’s okay Linux just added the drivers to fix them :),Positive
AMD,Clear which customers they care about more now,Neutral
AMD,Even in AI they ass.   They just make 948558 APU and manufacturing new handleds and consoles every month for nothing. Just wasting resources and shifting employees to optimize their aliexpress handelds.,Negative
AMD,Fine wine smth,Positive
AMD,"that's cute, you think nvidia's actually still doing anything for the maxwell GPU's.  also, nvidia is dropping support for those cards next month, along with pascal (10 series) en volta ( titan V (december 2017) ) support.  So, titan V was supported by nvidia about the same amount of time that AMD supported the r9 380.",Positive
AMD,"it's the nvidia fanboys that's the issue here, deliberately misinterpreting what AMD said, claiming AMD is dropping driver support.",Negative
AMD,"thank god, barely any good games were released since 2020, so it doest hurt staying on 6600 xD",Positive
AMD,"cheesus, prices suck where you live. 9070XT's can be had for 640 euro here, without VAT and converted to USD that works out to around 611 dollars.  So they're literally 50% more expensive where you live then here.  Also, OP is a liar and AMD is not dropping driver support.",Negative
AMD,They are not dropping support.,Neutral
AMD,"Yep. 280x was my last AMD card, support was both terrible and short.",Negative
AMD,"They are not.    They are not dropping support.   They are shifting optimalisation resources towards newer architectures.  Which is what literally always happens when a GPU is 2 or 3 generations old, with either vendor.",Neutral
AMD,"OP is a liar, they are not dropping support.  They are just going to stop doing game specific optimisation work on those cards... which has been standard practice for ages already for GPU's 2 generations behind.",Negative
AMD,"I do hope they make a video about is, so it's clear to you all that there cards are in fact not losing support, and that shifting optimalisation resources towards newer architectures is standard industry practice.",Neutral
AMD,"Your 6900xt will still get driver updates, it just won't get new features.",Neutral
AMD,Got myself a 6600 few months ago (I know it’s not the best but for my current situation it works for me fine) and to see this is just gutting and feels like I’ve wasted money. Will definitely be going nvidia for my next graphics card,Negative
AMD,They always burn GPU customers.    Welcome to the club.,Neutral
AMD,It's not. OP is a liar.  AMD is going to stop doing game specific optimisations for those cards. which has been standard practice for older GPU's since forever.,Negative
AMD,Typical PCMR though.   top comment is a defending AMD and straight misinfo.,Neutral
AMD,"AMD is not dropping driver support for the RX 5000 and 6000 series.   Not on windows or Linux.  What they said was that they are no longer actively optimizing for those older GPUs, which is exactly what has happened to older GPU's since the existence of GPU's. This SHOULD have been a nothing story. but because it got (deliberately?) misinterpreted, now its everywhere.",Negative
AMD,"no, OP's just a liar.",Negative
AMD,"Fuck brand loyalty. There is never going to be a tech company that will have me get comfortable enough to resort to tribalism to where I’m crawling back to anyone.  That way of thinking is baffling to me.    Actually strike that, no company period.   Educate yourself with every major purchase you need to make.",Negative
AMD,Its not a team. They are both billion/trillion dollar companies. You buy the better product.,Neutral
AMD,Its not they do insane things like this all the time. Just people have dumb brand loyalty and think AMD is there friend.   Reality is AMD isn't anyone's friend nor is Intel/Nvidia. Its best to go with the better product (Nvidia)... but the zealots will tell you it will age like fine wine and give you a bunch of other garbage.   Then when that fails they will find some obscure reasoning to why your wrong.,Negative
AMD,"No joke, I have a 6950XT, it's still the sixth most powerful AMD GPU to this day. Dropping support at this point seems asinine.",Negative
AMD,They are dropping new feature support.  They are still getting day 1 driver support and maintenance updates.  Thats it.  Youre acting like they are beheading the cards and burning them in massive piles.,Negative
AMD,"dude the 6950xt has been such a pita for me, the rgb led strip will not respond to its native software or third party controllers so its just rainbow vomit constantly and i have insane driver issues and crashes with black ops cold war zombies like it only runs on 23.something and still occasionally crashes out",Negative
AMD,"Dropping active optimisation development for GPU's 2 generations old has been happening since forever.  But this time, because AMD's actively communicated it, suddenly it's a huge deal.",Neutral
AMD,Except that OP is a liar and AMD is not dropping driver support.,Negative
AMD,"There not ""Nvidia Connectors""     That connector is the future for all gpus amd has it on some 9070xt models that have already caught fire and no doubt intel down the road if arc get's another line of gpus.",Neutral
AMD,"Of course they will. But they won't receive features and optimization drivers, something insane for a 5-year old product. Meanwhile NVIDIA is still fully supporting all RTX 20 / GTX 16 and up without any problem.",Negative
AMD,RTX 20 older than RDNA 2 and their games still optimized.,Neutral
AMD,Too much effort because the comments will be the same I bet.,Neutral
AMD,"Yeah. Because people should always go to discord for updates? People are rightfully upset about what AMD themselves said in official press release. For all i know that discord could be for amd fanboys.  This is their official press release  “RDNA 1 and RDNA 2 graphics cards will continue to receive driver updates for critical security and bug fixes. To focus on optimizing and delivering new and improved technologies for the latest GPUs, AMD Software Adrenalin Edition 25.10.2 is placing Radeon RX 5000 and RX 6000 series graphics cards (RDNA 1 and RDNA 2) into maintenance mode. Future driver updates with targeted game optimizations will focus on RDNA 3 and RDNA 4 GPUs.”  Except it’s literally stated that they will NOT be getting day one updates for games in the future and will only receive security and bug fixes in the future.",Neutral
AMD,Best you’ll get out of my attention span is a sub 90 second YouTube short. Better talk fast and have subway surfer playing on the bottom.,Negative
AMD,The fact that this has 12 upvotes and the top post with absolutely incorrect information has nearly 200 is so Reddit.,Negative
AMD,Yes they did,Neutral
AMD,"Exactly. If anything, we should compare the 6000 series to Ampere (RTX 3000) which in fact ""did"" get a new transfomer model upscaler, and ray reconstruction ""officially"", not to mention that these things even made it to the generation BEFORE that. Hell, RDNA 1 didn't even have mesh shaders.  AMD already has a track record of this. GCN 3 official support was dropped 4 years earlier than Maxwell.",Neutral
AMD,My brother in Christ the 9070 XT is the second AMD GPU I have ever owned.  I stopped buying Nvidia in the wake of the 40 and 50 series.  And you accuse me of drinking Kool aid?,Neutral
AMD,"I saw it across 4 social media apps, across 6 different subreddits here, all heavily liked/upvoted/reshared, all within the span of an hour. Hmmm…  I guess nvidia doesn’t have the kind of resources to do something nefarious like that, they’re only worth.. * *check’s notes* *… 5 trillion dollars.",Neutral
AMD,"Because Nvidia dropped them ENTIRELY.  If you read my comment, they arent getting ANYTHING but security updates. Im literally comparing the difference because people THINK the 6000 series is getting the 10 series treatment.  By *comparison* AMD has only stopped giving the 6000 and below *new features.*",Negative
AMD,"Nothing is wrong with them and there's nothing to clarify.  AMD aren't going to release game optimized driver for the rx5000 and 6000, only security.   You've just taken at face value a lie.  The offical statement of AMD that goes directly AND completely the opposite direction of what that guy is saying: ""RDNA 1 and RDNA 2 graphics cards will continue to receive driver updates for critical security and bug fixes. To focus on optimizing and delivering new and improved technologies for the latest GPUs, AMD Software Adrenalin Edition 25.10.2 is placing Radeon RX 5000 and RX 6000 series graphics cards (RDNA 1 and RDNA 2) into maintenance mode. Future driver updates with targeted game optimizations will focus on RDNA 3 and RDNA 4 GPUs.""  The rx6000s was 2gen ago (like the rtx3000s), gtx1000 was 4gen and 9years ago.",Neutral
AMD,"Getting nothing but security updates is essentially dropping support, especially considering RDNA2 is only five years old. People should be mad at this. Turing is older than RDNA2 and is still getting game optimization and new features from Nvidia.",Negative
AMD,The 6900 XT is 5 years old. The 5000 series is over 6 years old at this point. I really don't think it's that unreasonable to drop support for them. Besides dropping support doesn't mean the GPUs will stop working. They'll still work perfectly fine for years to come.,Neutral
AMD,"Sorry but you're talking utter bullshit.  Those ""game specific"" driver updates bring only marginal performance increases, and if there is a actually issue, aka, a bug, they will still fix it.  This has been happening since forever, nothing's changed, it's also worked out without much issue in the past.",Negative
AMD,I don’t see it being a major issue with how it’s currently performing I don’t see my 6950xt being obsolete any time soon game optimisations or not,Negative
AMD,And then it loses the resell value because of it as well. Nobody is going to want a card that is not going to perform at its best. It’s a shame because 6900XT is a beast yet almost useless because of this now.,Negative
AMD,"This is about the 5000 and 6000, not the 7000 series.  And this has been standard practice for AGES already. but because AMD actively communicated it this time, now suddenly it's a huge issue.",Negative
AMD,"It’s not that: yes you’ll have the “latest driver” but outside of security updates the new drivers do little to nothing to update or improve the older card’s support to play new titles, or play them well by not bothering to get the most from the card hardware",Neutral
AMD,"That's not the point, that scummy company shouldn't have drop support for such relatively recent product.",Negative
AMD,"Being Brazilian, it's usually like that due dólar being 1:5,50. The shipping cost to the shops make it go up, them 2% tax over it, on that sum more 31% tax (ICMS + If).  Basically one 9070xt here is around 5500 brl, or 1kusd, but it's getting constantly discounts and can be found around 4700brl.",Neutral
AMD,don't waste your time let these guys continue to rage bait for the next couple days.,Negative
AMD,So they are dropping full support for in production graphics cards. Just like I said.  That remains devious work no matter how this thread tries to reframe it.,Negative
AMD,keep commenting the same thing you schizo,Neutral
AMD,This aged like milk,Neutral
AMD,"Just to be clear AMD is not dropping support, OP is a liar.   What AMD will do is shift their optimalisation resources towards newer architectures.   Nvidia does exactly the same thing, just silently. This literally always happens when a GPU is 2 or 3 generations old.",Negative
AMD,"Op is probably not lying sadly. After AMD said this about the Vega cards, Satisfactory moved to UE5 and I had all sorts of graphical corruption issues with it. To the point that the game displays a pop up warning that AMD cards of this vintage will have issues until further driver fixes are released of which there weren’t any the last time I checked (though I haven’t checked since finally having my desktops shipped in earlier this year).  Satisfactory was so totally playable on UE4. But then they listened to one group of idiots who asked them to move to UE5 because “lumen” and “nanite”. Game became borderline unplayable after the move because the whole screen will have flashing and static bursts as if the GPU is dying (it isn’t, only UE5 games has this issue on Vega cards). The solution given by AMD was to force the game to run in DirectX 11 mode, which performs exponentially worse, or turn off FSR, which causes massive frame rate issues that gives me motion sickness.  However AMD Linux support is awesome. The same GPU actually received RT emulation using compute cores. Making games that couldn’t run on it under windows like Doom Eternal run fine on it in Wine/Proton on Linux.",Negative
AMD,"People are idiots, it's hilarious.",Neutral
AMD,"this 100%,AMD is not your friend and neither is NVIDIA",Negative
AMD,I mean giving am5 extra generations is pretty nice.   That being said I own an Nvidia graphics card. I'm no one's fan,Neutral
AMD,ong 😭 its like politics,Neutral
AMD,"They are not dropping support, OP is a liar.",Negative
AMD,"It seems that what other new features are not supported by the cards physically.   I mean we all know that certain fuctionality have dedicated hardware to run. Probably the new features needed newer inbuild functions that both card bios and driver cant emulate or translate for the said feature.   But as long the hardware still run and the updates it still recieves dont brick it, we got nothing to worry about.   Edit:   And being not supported an issue, then why people still buy rx580 8gb and gtx 1070/1080ti in the used market? Hell even old components?   Sure it's cheat but it still works, it works.",Negative
AMD,"“RDNA 1 and RDNA 2 graphics cards will continue to receive driver updates for critical security and bug fixes. To focus on optimizing and delivering new and improved technologies for the latest GPUs, AMD Software Adrenalin Edition 25.10.2 is placing Radeon RX 5000 and RX 6000 series graphics cards (RDNA 1 and RDNA 2) into maintenance mode. Future driver updates with **targeted game optimizations** will focus on RDNA 3 and RDNA 4 GPUs.”",Neutral
AMD,edit: I just realized certain 9070xt models do also have the connector as well,Neutral
AMD,"still surpised on how they kept using this standard when we all know its problematic.  in other words, i wouldn't mind on keep using 8pin with its bulky connectors, as long as it doesn't burn my house down",Negative
AMD,They're,Neutral
AMD,"Considering that game devs dontneven optimize their games due to the crunch or the publishers says dont care if it's unoptimize, the frame gen will cover it up or something.   We rarely see such optimization being done nowadays that truly run smooth and clear at old gen pc without the shutter.",Negative
AMD,nvidia isn't doing any optimisations for 20 or 16 series either. They just didn't announce it.,Neutral
AMD,No they aren't.,Neutral
AMD,\*Sigh\*... Reminds me of that recent meme:  https://preview.redd.it/qn6xwpeolcyf1.jpeg?width=640&format=pjpg&auto=webp&s=985eec6946a627a2a01e0465ce3fbcc7435d9000,Neutral
AMD,"The irony of these people saying we can't read when it's RIGHT THERE in AMD's statement that they didn't read. It's wild how all you have to do is say something confidently enough and most people will believe you without questioning it, even after being presented with contradictory evidence.",Negative
AMD,"I'm legit going to uninstall this app, the rage bait is eating me alive.",Negative
AMD,Thing is 3080 from nvidia still getting updates to this day,Neutral
AMD,Is this brother _ 💀,Neutral
AMD,They were still manufacturing and selling the damned things as of the end of last year. A GPU losing support within a year is not something that's been happening since forever. You are talking utter bullshit.,Negative
AMD,"7000 isn't that far off either since it also lacks  the support they need going forward. They just don't want to spend time/money on supporting prior gens that aren't cross compatible with their current/future AI focused generations.  Nvidia doesn't have that problem because they've been laying th groundwork for AI features since 2018 with their 20 series, it's why they can push some of their current DLSS4 features all the way back to 7+ year old arch. AMD can't even push any of their FSR4 feautres to 1 generation old cards (7000 series), they'd have to develop and maintain a different approach or new ""comparable"" features that wouldn't be moved forward to current or future cards, no business sense in continuing to support such products apparently. Same reason they are disabling the USB-C port, underused now, no longer used in current/future products and requires continuted support....best they can do is just ditch it.  Simply put, AMD missed the bus years ago and now they're trying to catch it. Some of us were saying that they need to get on baord pronto but people were too busy knocking down AI, DLSS, RT. Apparently AMD and their fanboys thought native res and raster was where it was at, all while Nvidia saw the challenges and limits on continuing to increase the raw power needed to keep up with the ever growing demands of software/games.",Negative
AMD,"They dont even fix the issues in ""their new rdna 3"" gpus.   Instead of fixing the type c issue. Just disable it permanent for no deadline.   Idk whats the point of their new drivers atp  .",Negative
AMD,"You're putting WAY too much stock in game specific drivers. those improvements are marginal in nearly all cases.  Those cards have already been optimized in a general sense so there isn't much room for improvement anyway.  Not to mention that this has been standard practice for ages already, and those older cards are performing right around where you'd expect them to.",Negative
AMD,"AMD should have cut the older GCN GPUs like the 7000 series, maybe the 200 series (some were GCN 1.0 to 3.0). I guess thats the downside of architecture that was built on top of each other.",Negative
AMD,"Again, this has been happening for ages. The drivers on those older cards have already been optimized. game specific optimisations wont bring significant gains. in fact they are rarely bring significant gains even on current generation GPU's.  Nothing devious about it. just standard practice that has worked fine for ages.",Neutral
AMD,I will while people continue to spread the same lie.,Negative
AMD,"Going into maintenance mode and only pushing security updates might as well be dropping support. Nvidia does the same thing, but for much older cards. If the hardware can handle an Nvidia feature, it gets it. I can load up a rig with a seven year old 2060 right now and get access to the latest Nvidia features outside of frame gen, and Turing is older than RDNA2 is.",Neutral
AMD,"No, Op is a liar.   Vega had been moved to legacy support when the UE5 update of satisfactory came out.    This is not AMD moving the 5 and 6000 to legacy support. The 5 and 6000 series will keep getting all the normal driver updates.",Negative
AMD,"yeah, but AM4 5000 series getting new releases after this many years is kinda fucked, if you think about it  they either held back and sat on the high-bin chips and are just now selling them, or are repackaging leftovers with a slight change in clocks.",Negative
AMD,Well its seems via my downvotes you are right i am wrong?,Negative
AMD,same i would love to use another connector.      But making only one port saves on manufacturing cost's for power supply makers so i am not surprised they did this. Though seasonic now has to include safety features so yeah..  but who knows maybe its still the cheaper option for the power supply makers.,Positive
AMD,"No, those cards are still on the main driver, so they get all the features.",Neutral
AMD,"lmao, even the gtx 750 ti that is 13 years old, have the latest game ready drivers released in oct 2025 supporting battlefield 6.",Neutral
AMD,It makes me sad this will be real one day.,Negative
AMD,"If you haven't noticed, people will believe whatever they want to believe.  People will post about Microsoft changing Office icons to a new design and flip out about it but the top post here about still relatively recent GPUs put in maintenance mode software wise already is twisted into a ""nothingburger"" because good guy AMD can do no wrong here.",Negative
AMD,No. Neither is stopping adding new features on cards that cant really physically support them anymore! The cards are 5 YO and two generations behind.  Edit: I blocked them because they were being intentionally facetious when I was providing an example of what ACTUALLY dropping driver support meant and they kept arguing with me. It wasn't worth my time to keep going when they aren't reading or comprehending what I am saying.,Negative
AMD,"Theres a difference between actually dropping driver support and providing security and game ready updates. If you cant figure the difference after multiple people try to explain it to you, then you deserved to be blocked.",Negative
AMD,"Paid for downvotes, lmao. You cannot be serious.",Negative
AMD,"Reread his comments again.  He isn't comparing the GPUs. He's comparing the driver life cycles.  5 years in, or in the case of Nvidia, the very next generation usually, cards stop getting new features. As they are older and can't support the new features well or at all.",Neutral
AMD,2080*  Hell even the gtx 1660 is still getting updates,Negative
AMD,They are NOT losing support!  Stop spreading that misinformation like Nvidia is paying you to spread it.   The only thing that's happening is that AMD publicly said they will shift their optimalisation focus to the newer architectures. Nvidia does the exact same thing.,Negative
AMD,"You seem to be carrying water for amd. It's clear what AMD is doing and this has been done before. As others pointed out, nvidia still does day 1 support for older gpus such as the rtx 2000 series.  But AMD clearly said missing out on day 1 optimization. There will be delays to the rx 5000 and 6000 series on windows.  With that, we have already seen this before and it's a case of history repeating itself. And it's clearly artifical seeing as how even older gpus work just fine in linux and get support to play titles it otherwise can't play on windows.",Neutral
AMD,idk welcome to reddit ig,Neutral
AMD,They said it won't even get optimizations for Battlefield 6. You're downplaying it.,Negative
AMD,Why do you insist on deliberately misunderstanding what's going on here. Is Nvidia paying you people? (They've done that before)    As I already tried to explain but it seems I need to be more explicit:  Nvidia might still include those older GPU's in their day one drivers but they didn't invest any resources into optimizing for those older architectures. Which is how it's worked for decades already.   The only difference is that AMD is explicitly stating that.,Negative
AMD,Yep     Here is a fact and look someone proved its true     Mass downvotes hate pms because i am a nvidia shell fanboy etc     I am just like -\_-     Oh and a user pointed out one typo well i guess i should be punished now,Negative
AMD,Do you have it on any energy mode saving plans (either by Windows itself or external program?).,Neutral
AMD,"Had a similar issue with a Dell laptop. Problem was battery not being detected, which would lock the CPU to like 0.7GHz. Eventually the battery would pop up and everything would go back to normal, but it’s been a constant recurring issue.   In my situation, the main status light would be blinking orange (instead of solid white), and it refused to power on unless connected to the charger.",Negative
AMD,nope 😞,Neutral
AMD,sadly my laptop is charging rather properly. windows detects it to be at 60%.  Edit: nvm i just found out my charger failed,Negative
AMD,Whatever is cheaper. Pretty much the same CPU gaming wise. Otherwise 7700 if you need the cores for something.,Neutral
AMD,7700x,Neutral
AMD,"What kind of games and do you do any productivity, content creation or video editing?",Neutral
AMD,"I play games like siege, cod, battlefield 6, gta, cyberpunk, etc. No video editing or content creation.",Neutral
AMD,You do not need to disable CSM to enable UEFI boot.,Neutral
AMD,"Try enabling UEFI boot mode first, then convert your drive with MBR2GPT before turning off CSM.",Neutral
AMD,"If you're legacy booting, you need CSM enabled because that turns on legacy boot.  You're doing stuff backwards, and ""I've heard"" is not a reliable source of information.",Negative
AMD,"Yes I know that, but it said that when wanting to convert the SSD from MBR to GPT, turning off CSM in bios is necessary, or is it not required to turn CSM off?",Neutral
AMD,"Oooohhh, so I only turn off CSM after I have my SSD converted from MBR to GPT?",Neutral
AMD,"I'm not legacy booting, I have it set to UEFI only on both dialogue options inside my BIOS Boot options",Neutral
AMD,It's not.,Neutral
AMD,"If turning CSM off makes you not boot, you are probably legacy booting (or you have a non-UEFI video card, which would be very strange)  In BIOS, what is the boot device set to? If it isn't ""UEFI: Windows Boot Manager"", you are legacy booting.",Neutral
AMD,"Ohh, alrighty. So can I just use the MBR2GPT File to convert my SSD to GPT like, right now?",Neutral
AMD,"I have an RX 5700 XT from AMD and in BIOS, my boot options are set to UEFI only. idk why it's not working",Negative
AMD,Yes,Positive
AMD,"Alright, but what *is* the selected boot device?  ""UEFI only"" is **how** to boot, it is not **what** to boot.",Neutral
AMD,"Well then, thanks!",Positive
AMD,"So in the System Info, it said that the Biot mode was in Legacy, but I fixed it by just running the MBR2GPT.exe file and it then automatically switched over to showing the UEFI boot mode in the System Info",Neutral
AMD,"Small caveat is that the 750Ti is the only 700 series GPU still getting driver support, and Nvidia now dropped everything up to and including 1000 series aka Pascal.",Neutral
AMD,"GPUs won't get bricked, they will be working pretty much as usual. I can guarantee the vast majority of users won't ever notice the slightest difference. Disabling the USB is a dick move, tho.",Neutral
AMD,"This is completely false, AMD are simply not introducing new features to the drivers, and are instead focusing on bug fixing",Negative
AMD,"According to an Admin in AMD's Discord, RDNA 1 and 2 are indeed not getting new features which is to be expected but are still able to receive Day 1 New Game Support  https://preview.redd.it/ioo7ty2q1byf1.png?width=701&format=png&auto=webp&s=56bb595d9576e197d6175a972bca208308ef94fa",Neutral
AMD,There is no difference here its just because radeon software is embedded into the drivers the 750ti isn't receiving feature updates lol.       Although this is also just AMD's fault for having embedded driver software just build a driver base to add software over please I beg.,Negative
AMD,"Spreading misinformation is so easy these days. Amd said Rdna1/2 wont get new features, like fsr4 or redstone, but there'll still be driver updates, fixes, game support.",Negative
AMD,"Dropping support? Nvidia hasn't supported the 700 series in a long ass time. 0 driver updates have improved performance on a 700 series card in nearly a decade. If no new updates come out it doesn't stop working or get worse in any way. How is Nvidia still ""supporting"" those cards?",Negative
AMD,6600 is the budget goat what do you mean?,Neutral
AMD,this is not about the game drivers stop spreading miss information you useful idiot.,Negative
AMD,"Not providing day 1 support for 5000/6000 series is not the same as ""dropping support for 5000/6000 series"".   Stop spreading misinformation OP.",Negative
AMD,This is a misleading way to word that AMD has moved it to maintenance.,Negative
AMD,"wait isnt that card still good? i mean RTX 3060 Level???  why would they drop driver support,??!",Negative
AMD,"If NVIDIA were to release a statement saying that they wouldn't push any new graphics driver features for the RTX 30 series we would rightfully make fun of them and be angry.  The double standard that I'm seeing here is horrendous. The 6000 series is 5 years old, it's unacceptable that these cards are already in maintenance mode. All these cards are more than capable to play today's games, it's insane that we're losing game-ready drivers... And, possibly, FSR4.  Don't worship for companies, please. They're not your friends.",Negative
AMD,"The ancient texts speaks of the days of old, where AMD drivers were doo doo.",Neutral
AMD,what sort of pack of lies is this shit lol,Negative
AMD,"been running with it since 2016, just got a 5070 upgrade",Neutral
AMD,"AMD has dropped new feature support on the 5000 and 6000 series of cards, they will still be supported by drivers for a while yet.  Nvidia, on the other hand, is (probably) soon dropping support for all GPUs older than the 20 series with their next major versions of driver releases - that change affects over 7.5% of Steam users...",Neutral
AMD,"On the other hand... Nvidia GPU support on Linux it's a joke and Timur Kristóf (from Valve) has just pushed a commit to kernel 6.19 and its AMDGPU driver to fully support GCN 1.0 / 1.1 AMD GPUs, released in 2012. That's Radeon HD 7000 series...  If you still want the best support possible, switch to opensource software and be happy",Negative
AMD,"I mean, if you've been using older AMD GPUs way past they life cycle, you'd know that[3rd party](https://rdn-id.com/#compat) drivers exist.",Neutral
AMD,Bro how are you guys not more angry? According to the article they're still doing security and bug fixes but no more game optimization which is kinda the whole point of drivers isn't it? It's the reason I upgraded from my 290x back in the day cause it ran like shit on games where the driver wasn't supported but that card got updates for 9 years. If there will actually not be anymore optimization updates on such a recent card I'm not buying AMD again because they might just pull support at any time.,Negative
AMD,"750Ti is still under Maxwell, so there is that.  But even so, it is merely there, 0 optimisation or anything.  Nvidia likely supported Maxwell for so long because of the Switch, it uses Maxwell based GPU.",Neutral
AMD,AMD shills working overtime to defend this one. Imagine the outburst if nvidia moved 30 series cards to legacy support,Negative
AMD,"I have been a huge fan of Radeon with the better pricepoint but if this stands there is no way I will replace my 6800XT with another Radeon card. While nvidia is more expensive is most cases, I would rather actually be able to use the card for a few years before driver support is dropped. No reason my old 1080ti should have more support than a 6800XT.",Negative
AMD,Wait is that common practice from AMD? I was about to switch from NVIDIA to AMD Next month…  Maybe shouldn’t?,Neutral
AMD,For the 750 ti:   https://preview.redd.it/jdy0wok1qayf1.jpeg?width=304&format=pjpg&auto=webp&s=d4a6eee08f781607908a820005fd169931e7543f  Also crazy work from AMD considering you can go on amazon rn and buy brand new 6000 series cards direct from the mfr [https://www.amazon.com/ASRock-6600-8GB-Challenger-gddr6/dp/B09J8VCFWN](https://www.amazon.com/ASRock-6600-8GB-Challenger-gddr6/dp/B09J8VCFWN),Neutral
AMD,"This is just plain misinformation... They did not say they're dropping support, they said they were going into Maintenance mode, Nvidia does the exact same thing..  They're still doing Game Optimization updates.. you really think the 750 Ti is getting new features still? lol.  The USB-C part is bs tho.",Negative
AMD,This was my first card 🥲,Neutral
AMD,"I had GTX 650TI, new nvidia drivers basically were breaking card - it was overheating and doing weird stuff. I had to limit manually performance otherwise it was blue screen of death. Reverting drivers was solving issue.",Negative
AMD,"Not even sure what you mean, playing SWTOR on high with my R7 240 on Steam lol :)",Neutral
AMD,"FYI, NVidia has decades of innovations that they have no reason to release.",Neutral
AMD,"This is a joke of a post.  Nvidia never cared, and they never will. They will keep forcing their proprietary software and force you others into submission. Monopoly at its finest.  If you want an example look at the support provided for the Linux drivers.",Negative
AMD,does that mean I can finally afford a used RX6600?,Neutral
AMD,is there actual proof that latest game optimitations affect the 750ti or even the gtx 1000 series.  i merely think they bundle all driver branches into one package and thats all.,Neutral
AMD,Propaganda bot post,Neutral
AMD,They didn't drop full driver support. It's on the legacy branch meaning they just won't get drivers as soon as the newer cards. Half of y'all don't comprehend reading and it shows.,Negative
AMD,"Uh oh, a post criticizing AMD on my love amd hate nvidia sub. Surely people will agree that dropping support for cards not even 5 years old is asinine and nvidia still supporting their 9 year old gpus is a pretty baller thing from them, right guys? **Right?**",Negative
AMD,"What?  Wasn't it just a few months ago this subreddit was losing their mind that Nvidia was dropping support for the 10-series in... oh yeah, this month!  Funny enough doing a quick google shows reports that the 750ti is also losing support this month as well.  Lastly, AMD is not dropping driver support. But OPs image is 'technically' correct in its wording w/ ""full driver support"". It's going into 'maintenance support', meaning while the drivers will continue be released that support the card, they are no longer actively adding features to those cards or optimizing the drivers for new games on those cards.  Sounds like an attempt by OP to obscure the fact that this month is the month everyone cried about just a few months ago.",Negative
AMD,Wait up.  I've got a 6600.  Boooo AMD!!!,Positive
AMD,Oh yes because Nvidia is a saint of a company,Neutral
AMD,https://preview.redd.it/qy9ieikcscyf1.png?width=737&format=png&auto=webp&s=56ed4a5f0499676f285ff2525e8a3fa1505c31fd  Huh?,Neutral
AMD,misinformation at its finest,Neutral
AMD,"PCMR and spreading fake news, name a better duo xd",Negative
AMD,...what?,Neutral
AMD,reciving new features and reciving driver support is 2 diffrent things,Neutral
AMD,False news. I just updated my RX 6600 and it showed the following game support     * Battlefield™ 6 (DX12)    * Vampire: The Masquerade - Bloodlines 2 (DX12)  Under compatible products RX 6000 is listed. And driver release notes is same for RX5000 to RX9000 series.  # Radeon Product Compatibility  AMD Software: Adrenalin Edition 25.10.2 is compatible with the following AMD Radeon products.  || || |Radeon™ RX 9070 / 9060 Series Graphics| |AMD Radeon™ AI PRO R9700| |Radeon™ RX 7900/7800/7700/7650/7600 Series Graphics| |Radeon™ RX 6900/6800/6700/6600/6500/6400 Series Graphics| |Radeon™ RX 5700/5600/5500/5300 Series Graphics|,Neutral
AMD,"Ah, misinformation, my favorite kind of post.   Did you hear about how NVIDIA will gift you a Labubu if you buy a 5090?",Neutral
AMD,Tell me you're an idiot without telling me you're an idiot.,Negative
AMD,https://preview.redd.it/lr1aaamawayf1.jpeg?width=1080&format=pjpg&auto=webp&s=2666a3f38f9c92f4628a9a8c37a0e49217ce3667  What exactly are we freaking out about?,Neutral
AMD,"This is a good justification for paying the Nvidia premium: Nvidia GPUs come with longer software support, and thus will last longer than their AMD counterparts.",Positive
AMD,"I’ve actually been thinking of finally upgrading after 5 years. I have a 5700XT and I want something a little more. Maybe 12GB, but I’m still unsure.",Neutral
AMD,Hmmmm….. I have my 6600 as a backup if my 7800XT were to die.,Neutral
AMD,WDYM they dropped support for the 6600 ?????,Negative
AMD,"It's a mixed bag since Nvidia also stops production on all older series cards, forcing you to pay for the newest model.",Neutral
AMD,they're dropping the 6600? eh?,Neutral
AMD,"Oh boy, let me tell you the joys of my MSI GX70. GPU was the 8970m (GCN) and the APU was an A10-5750m. Same problem--about a year after this laptop released, AMD's new drivers did not support a pre-GCN/GCN configuration like this laptop. I'm still salty about this...",Negative
AMD,WTF,Negative
AMD,"It's ok, you can hate on Nvidia all you want.",Neutral
AMD,is this a thing in linux too?,Neutral
AMD,Got a rog g20cb from a neighbor with a gtx970 and was pleasantly surprised that drivers are still being supported.,Neutral
AMD,don't forget FSR4 is exclusive to the latest generation... DLSS4 can be used on previous RTX series (not confused with multi FG),Neutral
AMD,"No support has been dropped, but I'm sure nobody in this thread will actually do enough digging to check out the wild inisnuations by TH.",Negative
AMD,I was curious how old an Nvidia GPU could still be to run games. A rare win for Nvidia these days,Neutral
AMD,"Cool, but now my GTX 1080 is not getting support, so this is kinda BS.",Negative
AMD,"Those ""new game optimizations"" have never done jack shit, it's a random algorithm with zero understanding of how actually demanding the settings are.",Negative
AMD,Guess that explains their overall better value when in comes to pricing.,Positive
AMD,Oh no the gpus incapable of doing machine learning stuff will not be getting machine learning features. Shocking 😯. But seriously this means nothing since game drivers mostly improve stuff like fsr implementation and frame gen which none of those old gen cards are able to do anyways. The gpus will work fine,Negative
AMD,Still supported on Linux.,Neutral
AMD,Mesa supports those old AMD cards.,Neutral
AMD,BIG CAVEAT drake is a nonce,Neutral
AMD,"What a shame, I have an RX 6600. Well, never cared about the functionality they were adding anyways",Negative
AMD,amd pr people must be very stupid.   they can end rx5000 and rx6000 feature updates without making puclic announcement.,Negative
AMD,Consumerism at its finest here.   I chose X therefore it has to be superior to Y.,Neutral
AMD,And the 6800XT and 6900XT still hang in the upper midrange too.,Neutral
AMD,It is not true that older gpus get game optimizations. From my experience a 3 years old driver for Maxwell gpus provides near exact performance in newer titles,Negative
AMD,So they're not doing the first one? Or do you just choose memes templates at random? Or,Neutral
AMD,"As someone who has two 750 Tis, but only in outdated computers (A WinXP system and an upcoming WinVista system), this is certainly the news of all time.",Neutral
AMD,People use the proprietary AMD drivers?,Neutral
AMD,1080 support was dropped.,Negative
AMD,Meanwhile I was planning to get an RX6600 as an upgrade from my old GTX1050ti :(,Neutral
AMD,Melting cables though,Neutral
AMD,"As if AMD can make GPU drivers, they are still struggling on driver timeouts something NVIDIA has mostly fixed after the Windows vista days.  Cyberpunk 2077 RT crash issue has been around for months on RDNA3 and Dying light 2 RGB laser show issue stealthily got fixed after longer then a year.",Negative
AMD,Extremely rare Nvidia W  I feel sick after saying that,Negative
AMD,Why is blatant misinformation being upvoted lol.,Negative
AMD,"I'm pretty sure all the Nvidia drivers for cards older than RTX cards are now only receiving security updates, or will be very soon only receiving security updates, that is the same for these Radeon cards, they will still get security patches but not game optimizations, which I still think is crappy for an architecture that is still in widespread use and is the basis of both current gen game consoles, while the 1000 series and older Nividia cards are almost a decade old, and atleast were supported this long.",Negative
AMD,Actually you can't update drivers on most 700 series cards.,Neutral
AMD,I have a 6700xt. Are these old and crappy now? I ran old shit for more than a decade before and this seems to do fine with pretty much everything today.,Negative
AMD,"To be fair, NVIDIA is richer than most severeign nations these days.  How does AMD hold up?",Neutral
AMD,I had my 750ti until 2 years ago. It actually still performed really well LOL,Positive
AMD,Wtf 😒,Negative
AMD,"Thought they said they would still support security updates for the 6000 series? Just no game optimizations and they are put in ""maintenance mode"".",Neutral
AMD,Put a 750ti in my kids Minecraft computer and it seems great.,Positive
AMD,Gotta keep that 1080ti alive right... right?,Neutral
AMD,"This is the card im stuck with right now, and this is how i'm hearing about it.",Neutral
AMD,they shouldn't be selling 700 series cards anymore but here we are.,Neutral
AMD,"well... yes and no. aside from the most recent October update. i am pretty sure the last one was back in 2021. either way, Maxwell, Pascal, and Volta GPUs will no longer receive any more future updates. at least, as far as i know.",Neutral
AMD,For the amount of people that claim to have sex offenders this meme shows up way too often.,Negative
AMD,My first GPU I got a decade ago still has driver support and my current GPU doesn't 🥀,Neutral
AMD,maaan fuck misinformation,Negative
AMD,Amd GPU is cheaper and better until you get Error code 31 or 22 .,Positive
AMD,Am surprised people even remember that gpu,Negative
AMD,I'm going to scroll down now and expect to read a lot of comments about how this is actually fine.,Neutral
AMD,"Why is my GeForce GTX 780 Ti using driver version 475.14, while my sister’s RTX 5070 Ti uses 581.57? 🤣 No, all these updates depend on graphics card sales and usage. Similar to the update lifespan of Windows XP and Vista 13 yrs vs 6 yrs.",Neutral
AMD,I bought a RX 7600 because of driver support + AV1 (i play esports titles on potato settings w/ clipping software so i didnt care abt vram)  looks like that worked out (for now) 😭,Negative
AMD,"God its amazing the lack of reading comprehension in this sub at times.  It says they are in *maintenance* mode right in the link in the post. It's not getting new features, but it is getting bugfixes. It's still fully supported and in the exact same mode as the gtx 750 ti.  JFC",Negative
AMD,"RTX 3000 with no framegen, does that mean anything to you?  Nvidia propaganda:",Neutral
AMD,Difference is AMD’s GPU drivers are open source. If you switch to Linux you can get more out of old the hardware. On Linux you don’t have to do anything as the drivers comes with the kernel,Neutral
AMD,"Fuck, I have a 6600!",Negative
AMD,mesa winning,Neutral
AMD,Not adding new feature not equals EOS.,Neutral
AMD,"As a 6700xt owner, who plays games so slowly I haven’t got to any new ones, and my card now can’t run them that well even if I did try, I can’t say I’m too bothered to not get new features as long as it still works!",Negative
AMD,I own a 6600 and this is non news.,Neutral
AMD,NVIDIA is ending regular driver support for its older Maxwell (GTX 900 series) and Pascal (GTX 10 series) GPUs after October 2025.,Neutral
AMD,"They're not ""dropping"" support, the 6000 series and older will just not receive new features, all the current features will continue to work and normal driver updates will probably still increase performance in some way,    That's just like Nvidia every generation they release, the older card won't get the new features.   In fact this means the 7 and 9000 series will still get all the new features (for now) meaning they fully support 2 generations, something Nvidia does not.   The usb-c port is bullshit tho",Neutral
AMD,The rx 6600 is not in production.,Neutral
AMD,"Sure, but also they cut support of my 970 years ago and the 1000 series is about to be dropped to.",Negative
AMD,Literally rage bait.,Neutral
AMD,me when I lie,Negative
AMD,Nvidia dropped SLI support pretty quickly,Negative
AMD,"Nvidia says they keep supporting this, but how much optimization are they really doing for older cards anyway? No matter the vendor, if your card is 3+ yo, you are rarely getting any optimization that is double digits (if ever).",Negative
AMD,Someone forgets the physx fiasco too soon 😂,Neutral
AMD,Dropping what? The drivers are still going to get updated,Neutral
AMD,I call bullshit on this. Been burned too many times by Nvidia using forced obsolescence. This cannot be real.,Negative
AMD,Also Nvidia: dropping PhysX support on flagman cards.,Neutral
AMD,"Guys, people don't realize that absolutely nothing will change for them. They just won't be getting new features that haven't come out yet, but they'll still be able to use their gpus as normal.",Negative
AMD,"Like so many people have said, they're not dropping support..   Also, I'd imagine one of the main reasons AMD would ever decide to stop doing something that Nvidea comparatively still does, is down to relative limitations in budget and personnel.  Look at how short ROCM support lasted on 6000 series AMD cards.. if you don't have the capacity, resources, or money that your competitor does, then how can you possibly spread your focus as wide as they can?",Neutral
AMD,"Yall need to learn how to read.  Your 5700xts, 6600s, 6950xts, etc, are still getting support. Theyre just not going to get the high end drivers for the newest games anymore.  Yall will still be able to hunt monsters, rock and stone, and stare at waifus all day long. Youre just not going to get the really big, new game specific driver updates for the new triple A launches.  https://preview.redd.it/djnhtxe4wayf1.png?width=1008&format=png&auto=webp&s=ea61ab5dfb1a9ea261c9ed5c4366e115a08b457d",Neutral
AMD,Still in a better state than Nvidia drivers.,Positive
AMD,"as pointed out, this post is both fake news and propaganda  get blocked  .  edit (can't reply 'cause blocked OP):  .  based on other's comments:  Development of new features as ceased. Development of new game drivers has not.  The 750Ti hasn't had updates in a decade. The 10xx series won't get any more updates after next month.",Negative
AMD,"This isn't a fair representation, honestly, both AMD and NVidia have a track record for bumpy waters with their drivers for the last few years. Multiple systems, so I use both brands on-and-off and I have to say that I have been unimpressed with both of them. I run into issues and headaches with both of them at basically an equilateral rate. They're both more annoying than they were 5 years ago at an equal rate.  You can thank general enshittification for this, and more directorial authority being given to marketing teams rather than engineers. It's kind of a huge problem for Big Tech in general right now, all across the board. Unless all of us unanimously stop buying anything new until these companies de-head-from-inside-ass, it's only going to get worse.  I prefer AMD from a ""they actually have something to prove and a name to live up to,"" but I gotta say, their native recording sucks balls. NVidia is bad enough despite ease of use, but AMD is straight gargling balls wit' it.",Negative
AMD,False or misinterpreted news. I just updated my RX 6600 and it showed the following game support  * Battlefield™ 6 (DX12) * Vampire: The Masquerade - Bloodlines 2 (DX12)  Under compatible products RX 6000 is listed. And driver release notes is same for RX5000 to RX9000 series.  #,Neutral
AMD,"Nvidia is fully supporting GTX 750 Ti? They are not even fully supporting their RTX 3000 series... Is there some hidden Nvidia driver that adds FG support for these cards?  And no, these patch notes were not saying that they are stopping support, they were just saying that there are 2 branches of drivers for RDNA cards, my guess is that it's just preparation for FSR4 and Redstone launch where 1 branch will support these and other will not. It looks like I cannot even find this line anymore, they must have removed it because morons like tomshardware were creating drama out of nothing.",Negative
AMD,"Not only a 12 year old card, a potato tier one that's outperformed by almost any modern integrated GPU.  And it still gets driver updates.  WTF AMD.",Negative
AMD,"I hate on nvidia for it since they make older cards worse with new drivers, I was playing doom eternal one day and the next after a driver update i couldnt even play the game anymore since once it loaded to the home screen it was all just a blank black screen, only realised once it was too late since it was because my vulkan support got basically cut in half by nvidia trying to ruin my TITAN X Pascal",Negative
AMD,I honestly don't think drivers matter that much if the game devs don't optimize there product for most older cards am I wrong?   I've gone back to games that I couldn't run a few years back and suddenly its stable af mafia 1 remastered is a good example my 2060 could not handle it now I'm at a steady 60fps at 1440p. Am I just dumb?,Negative
AMD,My RX 480 got a driver update also.,Neutral
AMD,Fuck yeah 750ti for the W,Negative
AMD,I never update drivers. I have a 3070.had it for the past 4.5 years.,Neutral
AMD,"Buy nvidia if you don't upgrade often, by that I mean 5+ years",Neutral
AMD,This is crazy AMD is doing this!,Negative
AMD,I fully plan to run the new Anno 117 on my 1080 to   Yes I have a high end mobo with a 9800x3rd tho,Neutral
AMD,"Yeah, but it's AMD so it's fiiiine. This is PCMR. AMD AMD AMD AMD AMD AMD AMD  Also, did you switch to Linux?  LINUX  LOONIX  LOSDINFJDSNFDS  MICROSOFT SPYING  WINDWOS01111111  BYSDPADR NRO  WEFWFHIUEWHFEW  DO YOU NOW OWN YOUR COMPUTER  I SWITCHED TO LINUX  IT IS SIMPLE  ENTER THESE0923192139219323921COMMANDS  EYS USE PROROTIN  YES  loonix",Neutral
AMD,Crazy work OP trying to glaze on Nvidia by spreading misinformation,Negative
AMD,Not true and why would you even hate Nvidia for that?,Negative
AMD,drivers never been the strong point of amd,Neutral
AMD,"never forget, spider-man 2 can be played on an gtx 750 Ti but not in a RX 580",Negative
AMD,750ti was based on the same architechture as 900-series so it’s the latest 700 series card,Neutral
AMD,"Is the 745 still getting drivers?  Pretty sure it's Maxwell as well (just checked, last driver is from October 14, 2025).",Neutral
AMD,I have a 1080ti on my media server... what does that mean for me?  As long as my software doesnt need to check driver version Im okay right?,Neutral
AMD,*will drop support when 590 drivers launch we're still on 580,Neutral
AMD,My Titan X Pascal got the recent October 15th Game Ready drivers yesterday.,Neutral
AMD,You still get driver updates for the 1080ti tho,Neutral
AMD,Pascal is nearly 10 years old. And let’s say production is 8 years old. Still better than less than 3 years old.,Positive
AMD,">GPUs won't get bricked, they will be working pretty much as usual.   Didn't stop Reddit losing it's mind when Nvidia announced the last drivers for Maxwell and pascal",Neutral
AMD,What do you mean by disabling the USB?,Neutral
AMD,"USB PD getting disabled was done months ago, no one noticed",Negative
AMD,"Sure, maybe that’s the case but that raises the question of why people were angry when Nvidia were doing it for much older hardware. Maybe it’s just that more people have that hardware considering it’s a very small part of the market, but I do find this attitude of excusing AMD doing something while not giving credit for Nvidia doing the opposite kind of odd.   DLSS transformer model coming to every single 20 series or newer gpu while fsr 4 is, maybe for now, exclusive to the 9000 series. And now this situation.   I get that we all want to see AMD be viable competition for Nvidia, but we shouldn’t excuse them doing bad things just because we desperately need competition in the industry",Negative
AMD,"Sure but you’re not going to get future optimizations for new games either, like the Battlefield 6 driver update.",Negative
AMD,"I have a GeForce 250GTS and a Radeon 5850 around, and both do still work. Both ftom 2009. Both used in office PC's now, tho.",Neutral
AMD,"Yeah that's the part that irks me, and I don't even have a 7X card. Real dick move.",Negative
AMD,"Depends on what they call ""new features""   Newly releasing games often get specific optimizations/tweaks in new drivers   And if this affects those, that is definitely a problem. But I'm not quite sure if that's encompassed by this",Negative
AMD,Many newer and updated titles require minimum driver versions even if they'd work just fine. Which really sucks for both amd and Nvidia users.,Negative
AMD,"Yes, but over time newer games will run worse, even though it would seem like they should be fine, and eventually some won't run at all. That's what happened to my laptop Radeon GPU back in the day. It couldn't run a remaster of Quake 1 of all things",Negative
AMD,"People think this will make their graphics card explode, their card will continue to function like normal, just dont expect fancy new features",Negative
AMD,"Would new features also include ""Day 1 Game Support"" for games that suffer from bugs and need a driver update to iron out the issues?",Neutral
AMD,If by new features you mean game optimization which is the point of drivers then yeah. I wouldn't be so flippant about what's in practice end of life support for a card that's still pretty damn recent and very popular. Unless the article is wrong about game optimization updates being done I'd say this is a pretty damn big deal.,Neutral
AMD,They are removing USB-C from my card. This isn't innocent.,Negative
AMD,"Oh good. That's more in line with expectations. AMD has always been the pro-consumer driver people, except when someone is actively working against them.",Positive
AMD,"new features aka game optimizations. I wrote ""full driver support"" meaning now there is only partial support",Neutral
AMD,That's not really that bad. Kinda too soon for that.,Negative
AMD,I mean that seems idk fair almost. What you really need are the new game support sure you want the new FSR but like there are still a ton of game that's have the old version of FSR.    If this is a technical thing I can see it. If this is money this amd dicks. AMD has been cooling its heels on consumer GPUs for a hot min and by very chill with being in a duopoly and making some money rather then all the money.,Neutral
AMD,"Meme aside, what AMD is pulling here is like if Nvidia put the 30 series on maintenance mode. People would riot.  Yeah the meme is a little facetious, but Nvidia did literally cut 10 series feature support only this year. 20 series is still getting feature updates.",Negative
AMD,Fixing bugs,Neutral
AMD,I don't see how you can compare products released back 12 years ago to products that were still on sale as close as 2 years ago.,Negative
AMD,second this. had one myself last year.,Neutral
AMD,I love mine :),Positive
AMD,RX580 is the budget goat,Neutral
AMD,"Bought one early this year, goat card!",Positive
AMD,intel arc B580 enters the chat,Neutral
AMD,1080 ti is the goat,Neutral
AMD,Is Miss Information cute 👀,Neutral
AMD,It is about the game drivers  https://videocardz.com/newz/amd-confirms-focus-shifts-to-rdna3-and-rdna4-rx-6000-and-rx-5000-lose-day-1-game-optimizations,Neutral
AMD,But me want karma! 🥺,Negative
AMD,You can see that it's working looking at this thread lol,Positive
AMD,"They're not even dropping Day 1 support. According to another post in this thread from AMD's Discord the only thing changing is they're no longer getting new features. The cards will still update as usual.  Regardless, OP is still misinformed at best and is outright lying at worst.",Negative
AMD,"No, it's an accurate way to say that Rx 6000 series is now a legacy product on the level of the GTX 750ti/GTX 900 series released in *checks notes* 2014. Or the entire GTX 10 series released in 2015/2016.  AMD isn't supporting cards made in 2020/2021 (sold new well into 2025 mind you), Nvidia is still fully supporting the 1600 and 20 series released in 2018/2019.   So full yikes and absolute BS. 5 years of driver support? What is this 2003?",Neutral
AMD,They're not. OP is misinformed. They're no longer introducing new features to the card. It's still going to get driver updates and work as it does now.,Neutral
AMD,The non xt is below a 3060 the xt is above.,Neutral
AMD,They're not dropping driver support.  It's just not getting new features.,Neutral
AMD,Yeah the comments here are insane. I don't know why you'd be so emotionally attached to a company. The truth is this isn't acceptable from AMD the same as it wouldn't be from Nvidia and frankly it makes me reconsider buying a 9060xt.,Negative
AMD,"The 6600 LE was released less than 2 years ago and the 6750 GRE barely 2. And they still sell the 6600... Remember how crazy people went over the announcement that Nvidia would drop new features sometime before the end of the year (e.g. day 1 game optimizations like AMD is dropping RN) for maxwell and pascal , 11 and 8 years old respectively, and how pissed people were. And now they try to justify doing the same thing with 2 year old cards too.",Negative
AMD,"But it's ok when AMD does it, anon.  /s",Neutral
AMD,"Yeah, I get not liking Nvidia and how much of a stranglehold on the market they have, but defending bad practices of a multi billion dollar company because they're the ""underdog"" isn't helping anything.",Negative
AMD,Yeah  insane and pathetic,Negative
AMD,Officer Dee Dee Mega Doo Doo,Neutral
AMD,Were?,Neutral
AMD,"AMD dropped support for the RX 6000 series still in production (releasing up to 2023)  dropped support for navi (released til 20' sold till 22/23') at same time (today).  Dropped support for vega (to legacy) in 23, last model release 2019.  Dropped support for polaris (to legacy) (was still available to buy at that time) in 2023 (releasing new models up until 2020)  not a great track record for that stuff.  btw the kicker is the competitor for the 750 ti, the r7 260x was dropped from full driver support nearly half a decade ago.  also they sell laptops with igpus they dont support 😂",Negative
AMD,"They likely haven't done Maxwell-specific optimizations in a long time, but they have done bug fixes for games and applications that straight-up don't work.  In that sense, AMD is putting the RX 6000 series in the same state, but the difference is that Maxwell is a decade old, whereas RX 6000 series cards like the RX 6600 and RX 6700 XT were popular choices well into 2024 because they were overproduced during the crypto boom.  The number of people who bought a GTX 750 Ti new in the past year is vanishingly small, but the number of people who bought a new RX 6600 in the past year is arguably significant.",Neutral
AMD,I brought out my old 980 a year ago and it worked jsut about fine on any game I tested without required RT,Positive
AMD,"its okay dude. theres nothing wrong with moving a $1100 gpu released 3 yrs ago to legacy support, right!?  /s",Negative
AMD,You can still use your card...,Neutral
AMD,"They simply moved it into maintenance mode, it is still functioning just fine and has drivers available. Both Nvidia and AMD have done this for ever.   OP is just trying to make it seem like a big deal.   I have used AMD and Nvidia gpus for decades. If you want to go AMD now is not a bad time.",Positive
AMD,"It's really not as big of an issue as people are saying. The new games just won't get feature support and drivers, but you'll be able to play them just fine.",Neutral
AMD,"Yes it is.   They did the same shit with VEGA cards (the one in-between the rx XXX cards and the current navi cards), their ryzen cpus (and tried with more recent one like 3rd gen ryzen but got so much backlash they had to go back on it for PR purpose)...  Their mobile gpu get 1 driver per 6 months even when they're on the current architecture too.",Negative
AMD,"Seems like they only dropped support on Windows, the AMD drivers on Linux still support really old cards such as the Radeon HD 7000 series (2012)",Neutral
AMD,"I went full AMD, with an AM5 build and a 6600XT. It works perfectly fine.",Positive
AMD,Life is easier with Nvidia.,Neutral
AMD,"They did it even when they were ATI. I'm still angry for when they deleted drivers from their website for some „legacy” cards.    I had ATI Mobility X1400, they removed laptop (Mobility) drivers and kept only desktop version (they were not compatible back then) with notice that laptop owners should contact laptop manufacturers. I noticed this as I was stuck with OEM driver (from CD, as on Fujitu's website they had only the same old version of driver) for few months (because of HDD malfunction lead to loss of data... I had backup of documents, but not of updated drivers). After few months they restored some of the drivers (I don't know if there was some kind of consumer backlash back then), but I was so angry (after using AIT cards since circa 2001) that I selected Nvidia for next laptop.",Neutral
AMD,"AMD dropped support for the RX 6000 series still in production (releasing up to 2023)  dropped support for navi (released  til 20' sold till 22/23')  at same time (today).  Dropped support for vega (to legacy) in 23, last model release 2019.      Dropped support for polaris (to legacy) (was still available to buy at that time) in 2023 (releasing new models up until 2020)  not a great track record for that stuff.   btw the kicker is the competitor for the 750 ti, the r7 260x was dropped from full driver support nearly half a decade ago.  >Wait is that common practice from AMD? I was about to switch from NVIDIA to AMD Next month…   Maybe shouldn’t?  Well for the higher end card's drivers are fine so if your getting like a 9060xt/9070/9070 xt I wouldn't really worry though unless your planning on keeping your GPU 6+ years.",Negative
AMD,"Nvidia actually does this but more subtly. Look at the last 3 gens from launch vs now, only thing that has been added is dlss 4 and universal frame gen. Those AMD cards will keep getting game drivers and fixes but no new features like ml frame gen and stuff.",Neutral
AMD,drivers for RX 5700 are still current up to yesterday as well.  AMD just says new features are not coming to legacy cards.  They're not dropping support.  Frame Gen isnt coming to the 750 ti either.,Neutral
AMD,why support 750ti and end support for 1080 and gtx 1050 / series ????? 1030,Neutral
AMD,also:  https://preview.redd.it/ovbbln9iqayf1.jpeg?width=244&format=pjpg&auto=webp&s=9ff8455f6c44aceeac68330866bfefa349b27dd0,Neutral
AMD,You're compared 2 different things though (deliberately?)  AMD is not dropping support for RDN 1 and 2. they are stopping actively optimizing for them.   Nvidia stopped optimizing for the 750TI AGES ago.,Neutral
AMD,AMD is not gonna release **new** **features** for this cards but you will still get driver updates... Why are you trying to compare with 750? did you see it getting DLLS or any new features lately?,Negative
AMD,"Amusingly, the last time the 750 Ti got any significant performance bump was probably FSR's launch lmao",Neutral
AMD,And that somehow makes the Radeon division care more? Because not making game ready drivers for cards at most 5 years old that still outperform the latest hardware (ex. 6800 XT v.s. 9060 XT) seems like not caring on a whole 'nother level.,Negative
AMD,Surely people would read the source and realize OP is talking shit because they also didnt read the source.,Negative
AMD,Intel CPUs and Userbenchmark,Neutral
AMD,"Why'd you cut out the rest of the sentence that explains why people are freaking out? I love how disingenuous your screenshot is where it even looks like they're gonna be adding new features 😄, good one dude.  https://preview.redd.it/x2ord7c84byf1.png?width=1080&format=png&auto=webp&s=31d0cf96b9edc8fb8760eb8b8fa92525d742fb91",Negative
AMD,They didn't. They're just not bringing new features to them. They're still going to get security and driver updates and will continue working just fine as they always have. OP is spreading misinformation.,Neutral
AMD,">*Nvidia also stops production on all older series cards,*  I'd rather be forced to buy used or buy a newer card than buy a new card that loses new game support the same year I buy it, which, by the way, the RX 6000 series has been around for 5 years, as opposed to Pascal which is only in a day from now officially losing game-ready driver support from NVIDIA, and I can guarantee you there'll be more outrage about Pascal losing driver support.",Negative
AMD,"No. They are still getting support and updates, just no new features",Neutral
AMD,"No, AMD is supported under Mesa on Linux and BSD. In fact, AMD stopped doing proprietary drivers and they're working with Mesa.",Neutral
AMD,"not old or crappy, still a very good gpu, just no more special optimizations and new feautures aka reduced drivers",Positive
AMD,"Pascal & Maxwell are still getting 3 more years of quarterly security updates after October 2025. It's game-ready drivers that NVIDIA is dropping. AMD is still doing the same as NVIDIA, but for 2020-2022 hardware (that's still sold in current gen laptops and PC handhelds with current gen APUs) as opposed to 2016-2017 hardware, not to mention Maxwell from 2014.",Neutral
AMD,"You are fine, I promise you.",Positive
AMD,the gtx 970 is still supported on latest oct 14 release?,Neutral
AMD,"Was it ever even a fiasco outside Reddit and this sub? You realize they didn't drop support for physx alltogether right?  Literally affected only a handful of old games that implementend and enabled 32bit Physx by default and since AMD never utilized physx to begin with, nothing was lost/gained since turning off physx setting just brought you down to playing an old game just as it would on AMD........without Physx.  So a fiasco? not even close. Fact is, practically no one cared.",Negative
AMD,Burned too many times by Nvidia using forced obsolescence? What are you talking about?,Negative
AMD,"I’m glad, physx has always been trash even when it first came out",Negative
AMD,Cope,Neutral
AMD,Getting new game support is what people are complaining about. Imagine buying a brand new card that is still being sold today and it doesn't get support for the latest games.,Negative
AMD,Why do you think people should not be pissed because an 6800XT or 6900XT are not going to get new game specific updates when they literally still chew through those as 4-5 year old cards lmao,Negative
AMD,"Majority of updates ARE for new game optimization though. I get why they might stop feature support, which is few and far between as it is now.... but makes no sense why drivers shouldn't be optimized for future games....because the devs for sure aren't going to optimize for small fraction of market, why would they when they can put their resources towards the other 90% of cards/users.  It's not just one middle finger, it's a double handed 2 finger salute.",Negative
AMD,Where is it fake news? This came directly from AMD patch notes on the latest driver updates?,Neutral
AMD,">*both AMD and NVidia have a track record for bumpy waters with their drivers for the last few years.*  NVIDIA had a rough start with their 50 series and is getting flack & shame for dropping game-ready drivers for Pascal from 2016 and Maxwell from 2015.  Meanwhile, AMD's Radeon 7000 series had high idle power usage & other issues at launch (something of which has historically given AMD a reputation for bad GPU drivers from the RX 5000 series, Vega, Polaris & earlier), AMD has dropped new game support for Polaris and Vega from 2017 & 2018 in late 2023 (Vega of which has had iGPUs in mobile CPUs up to the Ryzen 7030 series **released in 2023**), they've dropped new game support now for RDNA & RDNA 2 (released in 2018 and 2020 respectively), with several RDNA 2 dGPUs still in active production and RDNA 2 actively being released in the latest mobile CPUs. The 750 TI has had driver support up to October 2025, while its competitor, the R7 370 from 2015, lost support back in 2021.  And people were more upset about the ""death"" of Pascal 10 years later.  You can give NVIDIA a lot of shit (and deservedly, with shit like their melting power connectors, missing ROPs, and, as mentioned before, shoddy RTX 50 drivers at launch) but driver support isn't something to shit on NVIDIA for when AMD's doing the same thing but ending support half a decade sooner.",Negative
AMD,They literally said they are putting the card on end of life driver support cycle now ... Some of those cards are barely 4 years old (especially the laptop models which came out in like 2022-2023...).,Negative
AMD,"It's not that they ""aren't even fully supporting their RTX 3000 series"", they ARE, it's just that frame generation is very intense on the AI cores. Even the RTX 20 series has DLSS 4 Upscaling support and it can use it pretty well... Even the RTX 2060 laptop. Compare that to AMD who just says ""we're working on it, trust me"" whenever somebody brings up FSR4 on RDNA 3 cards(which has been proven to work).",Neutral
AMD,"AMD is still updating the 6600. It's just in maintenance mode, meaning it's not getting fancy new features. OP is a liar.",Negative
AMD,"It gets drivers updates that don't do shit, Nvidia officially dropped updates for these cards back in 2021 and they even stopped security updates over year ago. 700 series is officially not supported, they only increase driver number. And adding to that 900 and 1000 series just recieved their last actual driver update this month.",Negative
AMD,"And it was Maxwell, which they've continued to use in mobile and lower-end desktop parts for quite a while, likely simplifying support.",Neutral
AMD,"Well, kinda. Nvidia introduced it as Maxwell and the 900 series was called Maxwell 2.0",Neutral
AMD,750ti really hype up the 900 series.,Neutral
AMD,Driver support is expected to end for all 900 and 1000 series (and by extension the 750 since it’s basically a 900 card architecture wise) sometime in the next week or two when the new drivers release. So while the OP is technically correct it’s only correct for a very short amount of time.,Neutral
AMD,yes,Positive
AMD,"Yes, i have one on my kids pc, still gets updates",Neutral
AMD,"Nothing.   Security updates will still happen, but games that come out from now on may not perform quite as well as they should if they use features newer than what the driver was optimized for, same for games that implement such features in an update. Then again, Pascal had that happen with mesh shaders when Alan Wake 2 came out as well, but thats just the architecture not having Dx12 Ultimate support anyway.   If all you do is watching video then.....Gawd, what a waste of electricity, its a monster card and you have it do work suited to the most mediocre of Intel iGPUs. But other than that youll be fine.",Negative
AMD,As someone who will go years without updating a driver until a specific game cries about it? absolutely nothing.,Negative
AMD,"They really haven't. I installed my my old RX 580 8GB in my sister's computer because her 1060 3GB was giving her issues playing newer games. At the time, I was able to download and run official drivers that were only 1 version behind my RX 6700 XT. I just checked, that is still the case. The RX 500 series is officially supported by Adrenaline 25.8.1, released on  August 4th, 2025. My RX 6700 XT is up to date and running 25.9.1, released August 25th. I also checked Vega 56, and Ryzen 3000 APU drivers, both are still only 1 version behind my RX 6700 XT.   I will be fair here and point out that the RX 9000 series is on 25.10.2, which released yesterday.",Neutral
AMD,"""much later"" feel disingenuous when a lot of Vega and Pascal cards came out in the same span of a year.",Neutral
AMD,"Officially they dropped it, but I'm still getting updates for my RX 470 on my old desktop.",Neutral
AMD,You get *one* more driver update for the 1080 Ti.,Neutral
AMD,They announced them but still haven't stop support. I believe maxwell is currently on the latest drivers now.,Neutral
AMD,"well, you see, when Nvidia does something, it's the spawn of satan, and when AMD does something worst, it's just the natural thing, sometimes even a good one.  something something know the work rules meme.",Negative
AMD,since when arent redditors overexaggerating?,Neutral
AMD,"Yeah, but that was Nvidia, AMD is very consumer friendly, so we can forgive this  /s",Negative
AMD,"While trying to fix 9000 cards, amd broke something on older cards. Nothing new, they will fix things in next version. People just can stay on previous versions and wait, effortless fix.",Negative
AMD,"""Folks who have a Radeon RX 7900 series card—that's the XT or XTX—that includes a USB Type-C port on the back may be dismayed to know that the new driver disables that port's ability to power external devices. It also disables the port's ability to act as a USB port at all. Instead, it is simply an oddly-shaped DisplayPort connection now."" I guess someone bought a 7900 for that, and now it won't work any longer. A nasty trick in my book, no matter how little people was using that USB.",Negative
AMD,Some of these cards had a special USB-C port meant for VR that didn't get widespread adoption. They're disabling that port. It didn't really have much use unless you had a PSVR2 so 🤷,Negative
AMD,"last driver it was supported was all the way back in 25.3.1, shows how many people use that port",Neutral
AMD,"Welp, all the people who bought the Rx 6600 in 2023 when the Arc A580 was out gotta feel like idiots. The A580 was the better performer and people bought the Rx 6600 because ""AMD has been around longer and has better drivers"".  Hahahahahahaha.  Nope, 2025 rolls around and Intel fully supports their alchemist gpus and AMD won't even bother supporting their Rx 6600.   It's hilarious, but also extremely anti-consumer and bullshit.  You're gonna roll into 2026 and the Rx 6600 won't have driver updates for the latest games and the a580 will. Hell, the Rtx 20 series from 2018 will also have full driver support. Think about that.  AMD can't support a GPU from October 2021, the Intel competitor sold new starting in 2023 at lower prices alongside the rx6600 will be fully supported, and gpus of similar performance from way back in 2018/19 like the Rtx 2070 will be fully supported.  Stop buying AMD gpus people! They don't give a shit about their customers.",Negative
AMD,It would be crazy if a driver update took away graphics apis.  Patch notes: - removed DirectX12 - downgraded Vulkan - OpenGL untouched (as always),Negative
AMD,"I mean, what else would there be for the next several years?  If it supports DX12 Ultimate and Vulkan 1.3, then what else is there to add?",Neutral
AMD,High chance yeah,Neutral
AMD,Yes  CHECK THIS: [https://videocardz.com/newz/amd-confirms-focus-shifts-to-rdna3-and-rdna4-rx-6000-and-rx-5000-lose-day-1-game-optimizations](https://videocardz.com/newz/amd-confirms-focus-shifts-to-rdna3-and-rdna4-rx-6000-and-rx-5000-lose-day-1-game-optimizations),Neutral
AMD,Game optimization drivers are literally GPU drivers rewriting game code to optimize the game for developers. It should have never been necessary anyway,Negative
AMD,They removed it in April update and you havent noticed... LOL,Neutral
AMD,It's that bad. Let's not sugarcoat it. If Nvidia did the same thing we'd rightfully be shitting on them to no end. Amd doesn't deserve the leniency,Negative
AMD,I hope so 20 series to get feature pack for next few years,Positive
AMD,even 16 series!,Neutral
AMD,AMD is still fixing bugs on RDNA1/2. Hell they still do for Vega.,Negative
AMD,"You realize op is the one who brought up the 750ti, right? Also, they're still on sale right now.",Neutral
AMD,I had an rx570 during covid got me through it,Positive
AMD,RX 5700 XT is too if you don't mind the lack of some features,Neutral
AMD,same price as 3060 12gb,Neutral
AMD,"We're talking about budget GOATs, the 1080 TI at an MSRP of $700 as NVIDIA's flagship GeForce card in 2017 was never ""budget"" being sold under NVIDIA.",Neutral
AMD,"Idk, but she's certainly spreading alright",Neutral
AMD,>outright lying at worst.  Good time to farm karna I guess,Positive
AMD,"> The cards will still update as usual.  After AMD's track record with Vega, I don't think it's unreasonable for people to be upset. Vega was ""still supported"" for awhile, but it very clearly was 3rd rate support at best.   Like every time AMD has moved an arch to back burner it's been anything but smooth sailing and it's well on its way to being completely neglected even in just bugfixes.",Negative
AMD,"Lmao. You guys would be shitting on Nvidia like no tomorrow if they announced the 3000 series stopped being supported for new features, Pathetic really how this sub is defending this trash. I was going to upgrade my 6650xt on a 9060xt but I'm paying the extra for Nvidia at least I know I won't get shit on after 5 years",Negative
AMD,"None of those cards are actually getting new features just game drivers, the last big support patch was dlss 4 to all rtx cards and fluid frames or whatever it is called. A dev in AMDs discord confirmed that all cards will continue getting game drivers and bug fixes, so that puts them in the exact same spot as the old Nvidia cards",Neutral
AMD,"""AMD isn't supporting cards made in 2020/2021"" ????   They are though. Those cards are going to get bug fixes and security patches.  If you think Nvidia is not doing the exact same thing anything older than Ada, you are naive.",Negative
AMD,They are still getting drivers just not new features lol... They also get day 1 patches for games.,Neutral
AMD,"No, that card is only not getting feature updates, just like the gtx10 and 16 series users never got dlss when it came out in 2019. By your logic nvidia never supported the GTX 1660 super cards at all because that card also came out in 2019.",Negative
AMD,"I'd less call it misinformed and more *lying*. In the first third of the article, it contradicts OP's statement.",Neutral
AMD,thank's god and thank you for correcting me too   i'm considering buying it used :p,Positive
AMD,"I'm here with a 6700 XT, I'm not happy at all. The ""yeah, but it's not like it will stop functioning"" in particular is extremely dumb and irritating. Imagine defending a multibillion dollar company that gatekeeps software features to their new hardware.  Well, thankfully I play mostly on Linux, where the Mesa drivers will continue like usual for quite a while.",Negative
AMD,"English 101 calls to you, young grasshopper. Heed its call.",Neutral
AMD,"i wasnt aware of the igpu's thing LOL, just came here to complain about polaris support.  thankfully i upgraded, though it was a secondhand 1080ti, so im still screwed for driver support. a much better card regardless",Negative
AMD,> any game I tested without required RT  Yeah that's what no new features means. They could've added RT in software like they did on the 1080Ti (which Nvidia also never updated again).  [Indiana Jones can run on a Vega 64 with drivers that added support for software RT](https://www.youtube.com/watch?v=cT6qbcKT7YY),Neutral
AMD,"So do AMD cards, what's your point?",Neutral
AMD,"It is definitely still going to be usable, but why would I choose a card that stops getting new optimizations so early over one that continues to get them for over a decade",Neutral
AMD,"RTX 20 series is still receiving brand new DLSS features tho and it's a whole generation older than RDNA2.         I understand the lack of dedicated hardware makes it impossible for feature parity but at least they should compensate with some extra features, they should definitely keep trying to improve standard FSR that RDNA2 and 1 rely on, features like AFMF should be available on RDNA1, it's the least they can do.",Neutral
AMD,You are plainly lying. Nvidia hasn't done this as of recent. Rtx 20 series is older than rdna 2 and is still getting new features and optimized drivers,Neutral
AMD,">new games just won't get feature support and drivers  Doesn't matter if it doesn't matter. This just ***looks*** bad, *especially* for a card still being sold new.  Only technical-orientated consumers can look past that statement. Your average gamer is going to read that and write AMD off for the next 20 years.",Negative
AMD,Some games won't let you play them without new drivers. Had to update mine for Battlefield 6.,Neutral
AMD,"Again, they're not dropping support. Cards will continue to get driver updates and security updates. They're just not introducing new features. They'll still work just as they are now.",Neutral
AMD,True unless you look at the interent too much and then connector paranoia,Neutral
AMD,yeah life is really easy with 5060 ti 8gb,Positive
AMD,my 750ti generate artifact and blak smoke :DDDDDDD,Neutral
AMD,as in game optimizations(aka some new games wont work). happy I upgraded from my 5700 xt 2 weeks ago.,Positive
AMD,"All of these end this month. 750 and 750Ti are Maxwell 1, 900 series is Maxwell 2, 1000 series is Pascal. They all end at the same time.",Neutral
AMD,Optimization and no new features is a massive deals. You clowns defending this would be shitting on Nvidia I'd they did this on 3000 series,Negative
AMD,NVIDIA problably isin't even optimizing for the 20 series gpus. They just make sure games don't crash or do anything weird.,Negative
AMD,"750Ti is Maxwell, not Kepler. Support for Maxwell and Pascal ends at the end of this month (or at least so says NVidia).",Neutral
AMD,"https://preview.redd.it/4m75wtyhmbyf1.png?width=1118&format=png&auto=webp&s=90c1c97054520c92b0488cfd88416134f7d42347  Which is also false ... The driver that released yesterday added BF6 and Vampire The Masquerade 2 support. And AMD will continue to release drivers for both RDNA 1 and 2, they just won't release any new features. Do you understand the difference between releasing game ready drivers and features?",Neutral
AMD,"The GPUs will still work, this is nothing unusal.   Same thing with happend last time when Nvidia announced it, everyone pretends GPUs are getting bricked.",Neutral
AMD,Oh.,Neutral
AMD,>working just fine as they always have  Minus the USB that they disabled,Positive
AMD,"Until that new game comes out and runs like dogshit because they aren't releasing a game optimized driver for it. ""Fine""",Neutral
AMD,They logic bombed the NForce 980a motherboard so windows 10 will attempt to install two different GeForce drivers side by side if you use a GeForce 600 or newer GPU on it.,Neutral
AMD,![gif](giphy|xTiTnDAP0RiCo9k85W|downsized),Neutral
AMD,"Both RDNA 1 and 2 received an update as of yesterday including support for BF6 and Vampire The Masquerade 2. The article is wrong as well, it's misquoting what AMD told them.",Neutral
AMD,"Because those games will run fine without them, so you have nothing to complain about.",Neutral
AMD,"I can't say that what AMD is doing is conscionable or justified, but there could be the reasoning of ""it is working better than we could have hoped at the start, do we want to update it or leave it alone so we don't fuck it up with an update?"" and I can sort of understand that, although my frustration more or less comes from that meaning that feature updates on capable hardware potentially not being a thing.  It's hard when tech has become so much more frustrating over the last 5-10 years and you just want to appreciate the little things, but everyone is fucking up and giving us less than what we're actually paying for.  I'm just... tired, man. The Big Enshittification has consumed most of my will to live.",Negative
AMD,"RTX 3000 series is not supporting FG because Nvidia doesn't want to add it, it runs Intels AI based FG without any issues. Nvidia is not fully supporting basically anything that is not their current generation.",Negative
AMD,"nvidia will still be supporting RTX cards, which i believe were released in 2018? so 7 years.  Still pretty good.",Positive
AMD,"Its more like a ""backup"" PC.  The 1080ti *was* in my main rig, but I've since upgraded and I thought I should do ""something"" with it.  Mostly just a plex server, but every now and then I feel like seeing how well modern games perform on dated hardware.  For funsies.  Its paired with an FX8370, hell of a bottleneck.  Edit: autocorrect changed funsies to funniest",Neutral
AMD,"it's not much worse than the 9950x in my server tbh, granted that spends most its life at idle unless someone's watching something on Jellyfin that has to transcode.   both the server (with no dgpu) and my desktop have maxed out at 1400w full bore before.",Negative
AMD,I just replaced my rx580 with an intel b570 and am sending the 580 to a friend latest drivers were not long ago which J was suprised by,Neutral
AMD,7000 series is on 25.10.1,Neutral
AMD,"Afaik they are not stable though  The newest driver aren't even stable on 30 series, iirc the last ""stable"" driver for the 30 series was  566.36  Seems only 40 and 50 series are ""mostly"" stable on the newest since 566.36, but even on my 4070 it's been hit or miss with each update and I have to roll back sometimes for a bit.",Negative
AMD,"Yeah but that's not the point I'm making:   Nvidia announce stopping driver support, Reddit loses their minds   AMD cut support in half the time. Reddit jumps through hoops to downplay it",Negative
AMD,They dropped support for Studio drivers. Game ready drivers aren’t affected yet.,Negative
AMD,"Meanwhile, DLSS 4 transformer model and RR gets official support on Turing, while the ""newer"" RDNA 2 probably doesn't get jack shit lol.",Negative
AMD,"Wait, that port was there for VR, so your VR headset could be connected with a single cable.  And now, I guess that no longer works. Probably still works on the RTX 2000 series GPUs, though.",Neutral
AMD,"What is even the rationale for that? Were they finding themselves unable to support some new updates to USB-C hardware, and figured it would be a less disappointing experience to just disable the feature rather than get a bad hardware interaction?",Negative
AMD,"That's really annoying. My Webcam has a USB-C connector, and the only USB-C port on my rig IS on the 7900 XTX, lol.  Tbh, I've had a lot of issues with my drivers crashing on turning on the webcam, so perhaps the port is just a bit shit on those cards and that was the reason why I was having the crashes. Oh well.",Negative
AMD,"Hi, it is a completely usable port that can be converted to display port or HDMI output with a usb-c to display port/HDMI adapter.  Saying this is a ""useless port"" is completely and utterly false. It's a widely adopted port with significant usefulness and capability.   You see it used all the time for devices such as a MacBook which only has USB-C out. Phones as well, it's used with Samsung phones for Dex, and other devices makers use it too. You also see it used with PCs for USB-C enabled portable monitors, adapting to HDMI/display port, and as you said for VR.   On a GPU like the 7900xt, it gives you a whole extra display out you'll no longer have.   This is a MASSIVE deal and absolutely bullshit. They should get sued to Oblivion for this as it is a feature the card was advertised with that is being removed for no good reason costing users an entire display out they'll no longer have.  It's completely bullshit. The Rtx 20 series had USB-C out and the 7000 series with USB-C out was marketed directly as an upgrade for 2070/2080 owners, while those cards still support it.  F AMD. I hope they get sued and every purchaser who bought their gpus gets a refund. That's fraud to advertise a key feature then remove it.",Neutral
AMD,"Ah, thanks for the explanation. That’s shitty of them. At least let the people who do use that keep using it even if you don’t provide support for them. Guess those people will just have to use an older driver version",Negative
AMD,"That ""useless port"" is literally the only way to connect a PC to an apple monitor. I can guarantee you if they turn off that port there are going to be a ton of people with no display output where the display has no other input options.",Negative
AMD,mostly reference models my AIB XFX 7900XTX doesn't have a USB-C port.,Neutral
AMD,As artist with a pen tablet a usb-c port capable of video is sort of great to have. If I bought that card I'd be pissed off beyond belief. (if I didn't have a cpu with an apu  that is),Negative
AMD,"Wtf do you mean ""useless"" ? That port is completely required if you want ""one wire setup"" (display, usb, ethernet, audio, all in one)",Negative
AMD,I’m sure there’s like one guy who’s using it to run an Apple 6k display or whatever. And even then it’s just pushing DP over USB-C so lossless conversion with a cable is totally possible.,Neutral
AMD,And now everyone gets to feign outrage since someone pointed it out,Negative
AMD,I mean it did disable certain USB C features,Neutral
AMD,But how about my Mantle support?,Neutral
AMD,Theyre signaling that rdna 2 wont get fsr4 etc.  i assume,Neutral
AMD,"nvm I was just on the AMD discord and one of the admins there confirm that RDNA 1 and 2 will not get ""features"" but will still receive day 1 game support updates.",Neutral
AMD,damn that'll suck but as long as I can keep using my RX 6600 without it being gatekept out of launching new games (like Alan Wake 2 and FF7 Rebirth with older cards) I'm chilling.,Neutral
AMD,https://preview.redd.it/kqp1c0d71byf1.png?width=705&format=png&auto=webp&s=7aaa661d465d9ef8aed40a9393a56ebad59b33a8,Neutral
AMD,"again, they aren't killing off support, they're simply focusing less on features and more on bug fixes",Neutral
AMD,It should have never been necessary but it is.,Neutral
AMD,Incorrect. Why would you make some random shit up like that. Daddy AMD promise you an ass blast or something?,Negative
AMD,"No... This happens a lot with products. Not adding new capabilities is pretty common for older components. They have limitations to them, and not to mention cost. If the card isn't selling to individuals or retail, then full support is going to die.",Negative
AMD,"You have roughly around 4 Years, because that was when the last round of GPU cut offs happened, I suspect 30 Series will also be cut then as well.",Neutral
AMD,"I was replying to the dude's ""how is Nvidia still supporting the 750 ti?""",Neutral
AMD,"750 ti still on sale? If used market or new old stock means ""still on sale"", then yeah, technically the Gt 210 and R7 240 is still on sale. You could even get riva tnt2 on sale.       Nvidia supported Maxwell (which includes 750 ti) for over a decade. Security patches will even continue for another 3 years. On the other hand, GCN 3.0 was dropped 4 years ago.   Ampere, hell even Turing got official support for RR and DLSS 4 transformer model upscaling. I highly doubt RDNA 1/2 will get FSR 4 at this point. Dropping game optimization support for a product that was released as close as 2 years ago (6750 GRE) is quite frankly, disgusting and anti-consumer.       This is not the way for AMD to regain gaming market share, but hell, i don't even think they care at this point when they're enjoying all that AI cash.",Neutral
AMD,I owned a RX 5700XT before I grabbed RX6800 for $340 last year,Neutral
AMD,"It... already lacks support for new features? No official support for new FG, Reflex, RT, and DLSS...?  You can force it yourself for some of these, but you can do that on AMD hardware too...?",Negative
AMD,They won't get day one patches,Neutral
AMD,"Were / are. Also, nice edit. Also, also - i have a distinct feeling that there are a few libraries between us (not in your advantage, though). Oh and by the way, English is not my primary language. But you do you, young linguistical genius.",Positive
AMD,Try finger. But whole.,Neutral
AMD,You might want to do some research. The 750 Ti is a bit of an outlier for Nvidia. All cards up to Pascals 1000 series have lost driver support.,Negative
AMD,"Last I checked, Nvidia is only supporting the 750 Ti out of every other GPU in that family. They’ve also dropped support of new optimizations, new features, and even new bug fixes (which AMD is still going to be supporting on the 6600) for all GPUs older than the 2000 series cards.  EDIT: As always, most posts with short, clickbait titles, are purposefully misleading and will never tell you the full story.",Negative
AMD,I get what you're saying but the average gamer doesn't even read tech news lol,Neutral
AMD,"If this is the case, then its absolutely fine. I don’t expect new features to come to my GPU, other than drivers etc. so tha upcoming games run smooth.",Positive
AMD,"They disabled it after the 25.3.x driver version and nobody noticed until now, about 7 months later.",Negative
AMD,"The driver support is still there, just no new features.",Neutral
AMD,"You are deliberately acting as if you can't see my point or that I am not already famililiar that the games will still ""run fine"".   Read again what I wrote.",Neutral
AMD,Let's see how XeSS frame gen performs on Alchemist cards. Specifically the A770 as it's pretty much just the Intel 3060.,Neutral
AMD,Nvidia -    GTX 1080 - MAY 2016   GTX 1070 - JUN 2016   GTX 1060 - JUL 2016   TITAN X - AUG 2016   GTX 1060 3G - AUG 2016   GTX 1050 2G & 1050 TI - OCT 2016   GTX 1080 TI - MAR 2017   TITAN XP - APR 2017   GTX 1070 TI - NOV 2017    AMD -   RX 550/550X/560/570/580- APR 2017   Vega 56/64 - AUG 2017,Neutral
AMD,Is my luck just insane or have I been dodging just the right applications that's affected by Nvidia driver issues?,Negative
AMD,Last time I tested maxwell was either 550 or 560 something so that is unfortunate,Neutral
AMD,"I won't speak to 10 and prior.   But 20 and 30 series on new\* (\* to denote not immediate download as soon as drivers drop). Is fine.    It was actually the other way around that newer cards were getting hit with instability.   I won't speak to your specific scenario with your 4070(I skipped 40 series). But, I would wager that it is something more unique to your system.",Neutral
AMD,"Disagree  I have 4060 and 566 was the most stable driver, currently on the Last month's driver, this one's pretty stable too, had issues with drivers of April-July, games were unplayable due to gsync issues",Neutral
AMD,"I mean, really, have 40/50 series drivers \*really\* been ""stable""?",Neutral
AMD,oh I thought you were making the opposite point my apologies...,Neutral
AMD,"It's just AMD fanboy copium. They somehow think AMD is their lord and savior, and Nvidia is some dark evil villain, when in reality Lisa and Jensen are just laughing with their AI money.",Negative
AMD,And how many more people own an Nvidia card? 9 to 1?,Neutral
AMD,"It's even weirder, I've seen that people have used fsr4 with the Optiscaler mod on 6000 and 7000 series cards so it definitely seems possible to port over.   Amd just doesn't either want to take the time to port it officially, or they want to ""encourage"" people to move on to the 9000 series.",Negative
AMD,USB a to c adapter is $5 unless your camera takes more power than usual.,Neutral
AMD,"I had the same issue, could be a adrelin software problem where the mic recording switches from your headphones to the in built webcam mic. the solution which worked for me was to disable the webcam mic in windows, the crashes are far less now",Negative
AMD,"For an equivalent, this would be like you buying a graphics card with a 2x display port and 1x HDMI output and AMD being like ""we don't wanna pay HDMI licensing fees so we are no longer supporting HDMI. You still have 2x display port, so shuddup"".  You'd be pissed, and rightfully so.",Negative
AMD,"The port will still work as display output, USB signaling and the 12v power output for virtual link are what's being disabled.  Still a dick move, but most people likely aren't even using the port for anything but a display if they're even using it.",Negative
AMD,"Actually, I'm posting this from my 2070S with an usb-c to display port cable connected to my monitor.",Neutral
AMD,"Maybe try reading before crashing out. The port still allows display output, it's power delivery that was turned off. And it was turned off a long time ago and nobody cared.",Negative
AMD,"I mean, you can still use the card for PSVR2 (AFAIK), the port was just a special port so that you don't have to have a splitter cable with separate power like with the valve index.",Neutral
AMD,"AMD didn't remove the video output of that port, only the USB functionality and 12V power. You can still use the port to connect to monitors with the new driver.",Neutral
AMD,> I mean it did disable certain USB C features  ...in March. Its practically November and nobody noticed until now.,Neutral
AMD,And it's not like they didn't do stupid shit before that  https://slashdot.org/story/192519,Negative
AMD,How many people noticed?,Neutral
AMD,"Fun fact, Mantle was donated to the Kronos Group and became Vulkan.",Positive
AMD,"With the way everyone is melting down in these posts, it would be nice if this information could get out there. Do we want Nvidia to have an even larger market share? Redditors are a vocal minority, but it's still bad press, especially with the people saying, ""stop buying AMD GPUs people!""  Tiktokers and Youtube Shorts are going to be talking about this too; they parrot what they see on Reddit and I'm seeing this topic all over the place on Reddit.",Negative
AMD,AKA game optimizations,Neutral
AMD,"They literally say in the patch notes that if you want to use USB-C PD you have to roll back to 25.3.1 driver, so it means that they removed it in April/May update and you haven't even noticed.",Neutral
AMD,Holy stupidity,Neutral
AMD,Bold move cotton.,Neutral
AMD,"Ah ok, the 1000 series lost support pretty recently thought right? I just hope this doesn't become a standard",Negative
AMD,Mate pascal stopped being supported this year. Rdna2 is less than 5 years old and is from the same gen as the 3000 series.,Negative
AMD,Or update their drivers.,Neutral
AMD,"Oh they wont read it, but they can still see headlines in their feed.",Neutral
AMD,"Yeah AMD has just badly worried it, basically these older cards aren’t going to get FSR4 I guess?   The leak awhile back doesn’t mean much now. Maybe they just decided to not pursue FSR4 on RDNA2. They did tech disable power on the USBC port of some reference cards but seems like that was done in March and just not published until now?",Negative
AMD,Doesn't make it less shitty to disable a feature that a product was originally marketed with and sold at a premium for.,Negative
AMD,"Nope, understood you the first time.  You're acting like ""game specific updates"" make a world of difference, they don't. And if there is a actual issue, aka, a bug, they will still fix them.  fucking storm in a teacup",Negative
AMD,Most of the time I have been fine with the driver itself but Nvidia borks the install through the Nvidia App and I have to DDU to be crash free,Neutral
AMD,"Most of driver issues you see on reddit are not necessarily super common. Many people including myself have not had ANY issues with their 5000 series card. Of course there are going to be issues with some users using Nvidia/AMD/Intel GPUs, but there are more people having no issues at all than people with issues.",Neutral
AMD,Definitely,Neutral
AMD,lol apologizing is considered bad in this sub and deserving of hateful votes against you  no wonder accountability is so rare now,Negative
AMD,So because AMD have less GPUs they can give their few customers a worse experience and it's fine?   They'll buy Nvidia next time,Negative
AMD,"the FSR4 version being used on 6000/7000 series cards on linux is different than what is being used on the 9000 series cards.  forget the exact details, but essentially there's something that's 16 bit in the main FSR4 version that requires the newer card and *could* technically run on RDNA3 cards but it was so bad that it wasn't worth it.  and it's been replaced with an 8 bit version or something.  so it's not quite the same thing, but it's still an improvement over FSR3 so we'll take it.  on cachyOS it's just a matter of adding launch arguments in steam to enable FSR4.  dunno what the process is on windows.",Negative
AMD,"""Just like your phone, you should change your graphic card yearly"" - someone in the sales department, probably.",Neutral
AMD,"Tbh I'll just get a new webcam - it was a crappy £10 amazon basics one anyway like 2 years ago, might be time to invest in something remotely decent!",Negative
AMD,"Thanks for the advice, but I'm afraid it's likely not relevant to me as my mic recording is already set to be separate from the webcam (and the crashes happen when enabling video recording in-app with no change to sound input device).",Negative
AMD,"I'm so tired and so back and forth in the past 3 minutes.  So there is a USB-C port on AMD cards that will artificially turned off because ""haha losers""?  But it's actually just random VR priority things that they're artificially turning off just because?  So for 99% of people with a USB C port on a graphics card nothing is changing whatsoever?  I'm confusing myself trying to summarize what I just read,.",Negative
AMD,"It might be a small or unused feature, but the point is that the feature used to be there and now it's not",Neutral
AMD,All three people that used the port,Neutral
AMD,If only 1 did thats alredy bad,Negative
AMD,"if no new features means no new supported shader features then it's still a huge L by AMD but that's par for the course for them. RDNA 2 is too new for game developers to break compatibility with, so any potential optimizations that new shader APIs provide will be sat on until RDNA 2 usage falls because their drivers won't them.  Also introducing more fragmentation of the drivers as linux will continue doing its with with mesa, sony and ms will likely keep up their own work so games can run the best they can on the hardware, while Windows is fucked because AMD can't provide feature support for GPUs they're still selling in laptops",Negative
AMD,Buy Intel then,Neutral
AMD,I'm not sure who you think I am. I can assure you we have never met.,Negative
AMD,"The difference is that the 1000 series were released in 2016 while the Rx 6000 came in 2020/2021  Nvidia has been supporting these cards for a decade while AMD is thinking of putting them in maintenance mode after 5 years. If they do the same thing like they did with the RX500 series, they may stop full driver support in the next year or 2.",Neutral
AMD,"It absolutely makes a difference, besides stop with the dishonesty as if AMDs exclusion of 6xxx cards from FSR4 was not scummy as hell, especially it leaked that it could have existed for the entire RDNA2, once we managed to actually implement via standalone add-ons, they never even addressed it.  Shilling for a company is laughable behavior, and if you cant see what planned obsolescence is and how they fuck their customers up - then there is no point in discussing anything with you.",Negative
AMD,"Vega has as ""up to date"" driver as Pascal has. AMD is still releasing driver for these cards 2-3 times a year.",Neutral
AMD,"Pascal drivers got dropped already.    Anyways, no, I didn't prove your point. I contested your point that their release dates were far apart successfully then you stuck your head in the sand and pretended I said something I didn't while calling me a shill.",Negative
AMD,The nvidia app was the main issue for me for some reason. Currently using the latest drivers on my 3070ti without the nvidia app and it's working fine.,Neutral
AMD,"You asked why Reddit didn't make as much as a fuss. It's because far more people own Nvidia cards, so far more people are affected. That's why.",Neutral
AMD,I had an rx 6600xt until recently and fsr2/3 is definitely way inferior to dlss.  I'd rather turn down settings than turn it on because it wasn't great.   Even if they release something that wasn't quite fsr4 but better like you said it'd be a large improvement over the old fsr blur.,Negative
AMD,It's not even like it was 10-15 years ago where you got pretty substantial gains each gen on performance.     Now it's like a *maybe* 20ish percent improvement lol.,Neutral
AMD,"Functionally yes.  The ports have some special add-ons that make them vr friendly (power delivery and USB connectivity). These two items are being turned off for...reasons I guess.  The dp Alt mode that USB c also supports will remain active, so it may as well now be a display port plug with a weird connector.  Still shitty to turn off even some of the functionality considering it's built on to the card already.",Neutral
AMD,"There is a USB C port.  It currently works as a DisplayPort out, a USB port, and supports USB power delivery.  The latter two functions are being disabled and it will only work as a DisplayPort out.  I can't imagine why they would do this unless it was always problematic or something.",Negative
AMD,"I still use my RTX 2070 that has a USB C port, and I use the port if I need or would prefer a dedicated USB bus lol  My previous 3DS katsukity capture card basically required that because it was not stable at all on the motherboard USB ports, but I've upgraded to a DIY Loopy for my N3DS since that works like a charm regardless.  I assume the AMD ones work the same way where it's just a USB port rather than a specialized single use case port?",Neutral
AMD,So you haven't noticed.,Neutral
AMD,"You know, it's okay to admit you're wrong.",Neutral
AMD,aka quarterly LEGACY updates,Neutral
AMD,They disabled the ability to draw power from it or use it for data but you can still use it for monitor output (it’s literally just display port). I just don’t see an existence where you have a GPU that retailed for $1000 and still goes for hundreds in your rig but you somehow don’t have a high end motherboard with solid USBC support. If that’s a place someone finds themselves in they’ve spent their money very wrong and need to go back to the lab.,Negative
AMD,"Cool and these legacy drivers ""without game optimization"" still allow Vega 64 to keep up with GTX 1080 that ""gets all optimizations"" just like these 2 cards did when they actually were getting updates. It's almost like Nvidia is only releasing their optimizations for newest generations.",Neutral
AMD,"https://www.tomshardware.com/pc-components/gpus/nvidia-confirms-end-of-game-ready-driver-support-for-maxwell-and-pascal-gpus-affected-products-will-get-optimized-drivers-through-october-2025  That is literally the last release they are getting.    Vega was released less than a year from several Pascal cards and barely more than 1 from the first, a year before the RTX 2xxx series cards. Support for drivers for pascal are already confirmed as being dropped. You are accusing me of spreading information while doing so yourself.    You know what *is* typical shill behavior, though? Making up arguments for someone else, exaggerating differences between products, and relentlessly calling a neutral party a shill.   Literally all I said was that they weren't released far apart and only said that Pascal support was dropped after finding out it had been myself.",Neutral
AMD,"My issue was specifically a problem with the 3DS capture card.  I should have given more context, but the Katsukity/non-standard 3DS capture mods were notoriously bad with motherboard USB ports for some strange reason, but it's believed to be due to multiple devices sharing the bus and the mod not liking it.  It didn't matter how low or high end your motherboard was.  They were extremely sensitive.  Using the USB C on my GPU was my workaround but I've also worked around it with a PCI USB bus.  If someone reading this by some oddball chance has a Katsukity mod that seems ""broken"" and not connecting even with a yellow triangle on device manager, get a PCIe USB bus and 99.9% it'll work again.  The USB C based capture mods by Loopy do not have this issue, as I've given my old modded 3DS to a friend since I've installed the Loopy on my main 3DS.",Negative
AMD,I mean there are tons of case options out there. What's the form factor of your motherboard?,Neutral
AMD,Sorry I'm new to PC building but Google says it has ATX form factor I'm getting the B850 if that matters,Neutral
AMD,"The Phanteks G370A is a good, cheap ATX option that comes with 3 fans.",Positive
AMD,"PC 2 is by far the better deal for uses 1 and 3, neither PC would do particularly well with 2 but 1 would be less bad.",Negative
AMD,RDNA 2 GPUs were released as recently as 2 years ago with the 6750 GRE 10 and 12gb. The 6X50 refresh gpus are at the most 3.5 years old. That's despicable as far as support time.,Neutral
AMD,Deprecating and no longer providing new game optimization updates for rdna2? A 6900xt that cost over $1000 got 5 years of updates... Expensive AIB like 6900xt formula editions got 4 years of support.     What a joke. Why would anyone buy AMD cards going forward when they lose primary support and become legacy so quickly and still cost insane amounts?,Negative
AMD,Changenotes and full discussion on this release already done:   [https://www.reddit.com/r/Amd/comments/1oj6zgl/amd\_software\_adrenalin\_edition\_25102\_release\_notes/](https://www.reddit.com/r/Amd/comments/1oj6zgl/amd_software_adrenalin_edition_25102_release_notes/),Neutral
AMD,"Can someone explain what new game support actually is?  My understanding is that game engines are written according to API standards. That game shaders are calling pre-existing directx or vulkan functions, and that the implementation of those functions is already optimized in the graphics driver.  So what exactly do AMD do when they release a driver with new game support?",Neutral
AMD,Makes BF6 crash with device hung drx,Negative
AMD,"Installed it today and every time I restart/turn on my pc I get a gpu driver crash message and kernel error in the logs….tried to reinstall, yet still persists.   Anyone else? PC has been perfectly stable up until this point. Weirdly games run fine, it’s just on startup I can hear/see the drivers crash and then when adrenaline loads up it tells me it crashed….quite strange",Negative
AMD,Back to awful driver issues again,Negative
AMD,My fps is halved in Overwatch 2 on my RX 7600. Nice...,Positive
AMD,"Lo peor de todo este driver no funciona en RX 6800XT no tiene optimizaciones, y encima te dice que no es compatible o sea no hay driver que se pueda instalar ! Y no veo que lo quieran arreglar ese instalador.",Neutral
AMD,My 6950xt is 3 years old! Ffs,Neutral
AMD,So bugged tried twice with ddu and just had horrible performance and insane stuttering. Rolled back to 20.9.2 and instantly better. Please do more testing before releasing these.,Negative
AMD,So rx 6600xt is being updated anymore?,Neutral
AMD,Things like the steamdeck are RDNA 2 as well,Neutral
AMD,"AMD is insane for thinking this is okay, I only bought my RX 6800 last year when it hit $340, we need Omega drivers to make a comeback.",Negative
AMD,It has not even been five years since they launched the 6900XT for $999 🤦‍♂️  Nvidia can support the ancient GTX 10XX series for some nine years with day one drivers but AMD can't even support the 60XX series for five years. Certainly helps drive the gpu business into the ground. When Lisa Su became CEO AMD had 25-30% of the dgpu market and these days  more like 5% if they are lucky.,Negative
AMD,"They’re not retiring RDNA 2; it seems they’re simply branching it differently. Since RDNA 2 lacks ML extensions like RDNA 3 and 4, that’s likely the reason.",Neutral
AMD,The download link is broken if you get to download from the release notes it will work I have 7900xtx and 6700xt build and I got both working flawlessly,Neutral
AMD,Amd saw that people prefer staying on old drivers.,Neutral
AMD,why you say no update.....NAD was the only one that improved overtime....even rx580 wa usable for long time.....  and you can use FSR4 and frame gen with all gpu.....,Neutral
AMD,Not because of this but because AMD let's issues just continue like driver timeouts. I'll never ever buy another AMD card. Sucks I will pay more for nVidia but I've never had problems like I've had owning an AMD card.,Negative
AMD,Usually minor optimizations and glitch fixes like textures appearing / loading funky,Neutral
AMD,"It's being updated but stuff like day 1 support for games would be delayed and they would focus on RDNA 3 AND 4 first, but it would be still be updated.",Neutral
AMD,"Just to clarify, Linux drivers are a separate ecosystem from Windows. RDNA2 cards get feature updates and optimizations through AMDGPU/Radv/Mesa (although recently radv and mesa combined) independent of Windows ‘Game Ready’/maintenance mode. Even the Steam Deck will keep getting Vulkan/driver improvements. So ‘maintenance mode’ on Windows doesn’t mean Linux support is frozen, the open source stack evolves on a different schedule and often keeps older cards performant for years. i.e the HD 7000 (from 2012) series is still receiving game patches on linux.",Neutral
AMD,lol same here. Bought an RX 6800 a year ago. Shame really. Looks like I’ll have to upgrade much quicker than I anticipated fs,Negative
AMD,"Omega drivers, wow that takes me back. I had completely forgotten about it",Positive
AMD,Branching GPU Architures is bad?? From a developer perspective that is normal.,Negative
AMD,"It's clearly stated  on the driver description of the latest drivers :   ""New Game Support and Expanded Vulkan Extensions Support is available to Radeon™ RX 7000 and 9000 series graphics products.""",Neutral
AMD,"Again, it no longer includes new game updates. That is retirement for a gpu. Sure, the drivers can be updated. Doesn't change the fact those drivers you just updated will not include new stuff for RDNA2",Neutral
AMD,The only time I experienced a driver timeout was due to a problem with faulty RAM.,Neutral
AMD,"Sounds like you have a bad card or something wrong in your system. Been running my 6800XT since release day, never had a driver timeout naturally at all. The only time I did was when I did an unstable overclock.",Negative
AMD,Do you have a ram issue?,Neutral
AMD,"Optimizations I can understand because there are architectural differences in the hardware from each vendor. There are cases where things take different amounts of time or resources depending on the hardware. And each vendor wants the best FPS number in the day one reviews for AAA games.  But how do glitches occur if the software is written according to the API standards, and how is it AMD's responsibility to fix them if the software is glitchy because it didn't follow the standards?  Or are game engines and gfx drivers much less [deterministic](https://wippler.dev/posts/determinism-in-software---the-good,-the-bad,-and-the-ugly) than I'm assuming?",Neutral
AMD,Switching to Nobara this weekend. Thank you for this info,Positive
AMD,"![gif](giphy|9rjKLsynBodhiIovdD)  me with my 3 year old rx 6800.  GD it didn't even reach 5 years and they are preparing to put it into long term storage?  Will not make the same mistake again, one bitten, twice shy.",Negative
AMD,"Nvidia relased DLSS4 on the 2060, an almost 8 years old card, so yeah, branching an architecture (placing it outside new games optimizations) that is still on the market (APUs) is not just bad, is totally unacceptable.",Negative
AMD,"These extensions likely require hardware support, so this is no surprise.",Negative
AMD,What I've read is day 1 optimizations. It doesn't mean they won't receive it.,Neutral
AMD,I have them too if I wouldn't manually offset the frequency. These cards boost way higher than they should which can result in crashes. My card is rated for 2.97Ghz and pushes up to 3.4Ghz which is unstable. Why AMD decides it's ok to push the frequency higher than what it actually should use as a maximum is highly questionable,Negative
AMD,day 1 optimizations,Neutral
AMD,"I'm not talking about the extensions, ""New Game Support"" is the problem.",Negative
AMD,̷I̷n̷c̷o̷m̷p̷a̷t̷i̷b̷l̷e̷ ̷w̷i̷t̷h̷ ̷R̷D̷N̷A̷2̷?̷ ̷I̷t̷ ̷t̷e̷l̷l̷s̷ ̷m̷e̷ ̷t̷h̷a̷t̷ ̷t̷h̷e̷ ̷A̷M̷D̷ ̷g̷r̷a̷p̷h̷i̷c̷s̷ ̷h̷a̷r̷d̷w̷a̷r̷e̷ ̷i̷s̷ ̷n̷o̷t̷ ̷s̷u̷p̷p̷o̷r̷t̷e̷d̷.̷   They have already fixed the driver.  It can be downloaded from here: [Drivers and Support for Processors and Graphics](https://www.amd.com/en/support/download/drivers.html),Neutral
AMD,Work graph wizardry support on 9000 series??? Nice,Positive
AMD,>Stutter may be observed while playing games with some VR headsets at 80Hz or 90Hz refresh rate on some AMD Radeon™ Graphics Products such as the Radeon™ RX 7000 series. Users experiencing this issue are recommended to change the refresh rate as a temporary workaround.   Fucking finally,Negative
AMD,"Wait wait, they disabled the usb-c power for 7900 series? Does this break the psvr2 running from it? :O",Neutral
AMD,VK_EXT_shader_float8 support ... litteraly Vulkan FSR4 support coming soon.,Neutral
AMD,Updated link for RDNA 1 and 2  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html,Neutral
AMD,"I discovered the cause of a bug that has been plaguing me for years, I thought it was an issue with Windows, but I just recently did a fresh install and discovered it is actually because of ReLive.     The problem is that if you are recording, or have the Instant Replay  feature turned on, when you change the volume and that Windows volume slider appears, it makes the game drop fps/stutter  severely.     Here is a video showing it, look at the fps numbers and graph next to it: [https://streamable.com/ui0vqj](https://streamable.com/ui0vqj)  Relive disabled (sorry for the recording quality, had to do it with my phone): [https://streamable.com/4a7e0h](https://streamable.com/4a7e0h)  There are some games where there is not a fps drop, but it shows some erratic behavior. For instance, on Little Nightmares 2 Enhanced Edition, it causes the game to ignore vsync/refresh rate of the monitor. I have my monitor set to 120hz, and here is what happens on LN2:  [https://streamable.com/jv4f5p](https://streamable.com/jv4f5p)  I also tested with 2 other recording software, Windows Game DVR and Steam Recording, and the issue doesn't happen with those: [https://streamable.com/rsesak](https://streamable.com/rsesak)     u/AMD_Vik Can you please look into to this? My system: 5800X,  2x8GB@3200Mhz, RX 6750XT, Windows 11 up to date, nvme. Thank you in advance!",Negative
AMD,"Error 182 on RDNA 2 cards, also, there's no data on .inf about RDNA1/2 cards.  Wrong .exe on the web maybe?",Negative
AMD,>USB-C power charging has been disabled for Radeon™ RX 7900 series graphics products.  Users requiring this feature are recommended to use AMD Software: Adrenalin Edition version 25.3.1.  don't tell me they fixed the VR issue just to kill the VirtualLink port. a step forward and 2 backwards.,Negative
AMD,First version since 25.4.1 that doesn't freeze my 7840HS/780M. Hardware acceleration still broken though.,Negative
AMD,> New Game Support and Expanded Vulkan Extensions Support is available to Radeon™ RX 7000 and 9000 series graphics products.  Are the RX 5000 and RX 6000 series moving toward a legacy release like GCN?,Neutral
AMD,This driver fixes AMD GPU driver crashes in 3DMark CPU Profile.,Negative
AMD,"Stupid question for a PC noob: what exactly does it mean when a new Driver adds support for a new game? I’ve been playing Battlefield 6 for a few weeks now, but now my Drivers support it? What does that look like?",Neutral
AMD,AFMF new frame blending option works really nice. Definitely improved smoothness using blended frames in emulated games.,Positive
AMD,Battlefield 6 now crashes with DX12 error,Negative
AMD,Half of my steam games refuse to launch since installing this update. I had to ddu and roll back to last month's version,Negative
AMD,"Hey, The new Driver is terrible on Battlefield 6, i lost over 100fps, if someone has that problem as well, just clean the driver with DDU and install the ""25.9.2"" one, now the game feels like it was yesterday for me. Edit: 7800 XT here.",Negative
AMD,Anyone who updated the drivers have noticed any difference in Battlefield 6 between the Preview Driver and this one?,Neutral
AMD,"Still nothing about the pink artifacts in Chromium? It’s been four months since AMD’s Matt said they were investigating this issue. At least it was working fine on 25.4.1, but now I’m forced to update drivers because of Battlefield. I don’t want those pink squares, ffs! I can’t replace everything that uses Chromium because of AMD… At least say something about it! Is our GPU broken? A lot of people are complaining about it! Just say it’s broken so we can replace it, ffs.",Negative
AMD,Where arc raiders support,Neutral
AMD,Noise suppression still broken since 25.9.2,Negative
AMD,"I still have an issue with this release, maybe it's related to Windows 11 25H2 idk  I'm on Windows 11 25H2, 5700x and 7900XT  I can't record or use replay, of course they're enabled, I get the first notification ""recording"" or ""saving replay"" but it's actually not saving anything when I click on my hotkeys again.  Even starting a recording manually with ""Desktop Recording"" enabled does absolutely nothing, it's not saving anything. I see an .mp4 appearing in the folder, but as soon as I stop the recording the MP4 file simply disappears.  I have tons of free storage, so that's not the issue, I'm using the default Radeon relive folder too, I didn't change a thing, tried DDU too but nothing.  Anyone else having this problem?",Negative
AMD,Is 25.10.2 newer than my current 25.10.30? lol im noob,Neutral
AMD,"I downloaded it and tried to install it on an RX 6800XT, but it says the installer is not compatible.",Negative
AMD,"For anyone encountering frame pacing issues on BF6, try turning off AMD Anti-Lag. Before I had terrible frame pacing doing literally anything, and every time I scoped in the FPS would drop to like 50 fps. Brand new 7800x3D system and fresh W11 debloated install, all drivers updated and all temps within normal spec. It was driving me insane since this was the only game with problems. Also future frame rendering helps even further, but test to see if you like the slight decrease in responsiveness.",Negative
AMD,NIce going to install after I get off my meeting.,Positive
AMD,"Looks like we got a new AFMF setting, Fast Motion Response. Can be set to Repeat Frame or Blended Frame. [https://i.imgur.com/BCEN27U.png](https://i.imgur.com/BCEN27U.png)",Neutral
AMD,AMD please fix eyefinity pro utility!,Neutral
AMD,Am I the only one seeing corrupted artifacts in Vtm Bloodline 2?,Neutral
AMD,"Installed these, now battlefield 6 shows up as Elder scrolls online in the Adrenalin game tab.. like wtf",Negative
AMD,"Wonder if this solves the driver crash, timeout problem started with 9.1.  That was the worst bug I encountered on an AMD Gpu. Crashed the whole system. Only way out was to hard reset the system.",Negative
AMD,AMD Software Installer Detected AMD Graphics Hardware in Your System Configuration That Is Not Supported - installing on 6800XT and there is no other driver to install? Why?,Negative
AMD,Not working on my RX 6600XT getting the error 182,Negative
AMD,"Error 182 on my 9900x B650E system. I repair-installed 25.9.1, and that worked fine, but it didn't help installing this new 25.10.2.",Negative
AMD,"I was able to get by error 182 by using the auto-detect installer from the AMD website... weird that that works compared to downloading the direct, and distinct driver for the newest version.  Haven't ran battlefield 6 yet, but will attempt to install next.  Update: Seems to work now! Will report more if that changes.",Negative
AMD,"If I have 9070xt, should I upgrade to this version?",Neutral
AMD,NOISE SUPPRESSION NO LONGER WANTS TO BE ENABLED. TF AMD. Even after many ddus and reinstalls. I need amd noise suppression for streaming. Stop breaking crap in your trash application. Especially important stuff.   16gb 9060xt Ryzen 5 5600xt,Negative
AMD,My external mouse doesn’t work when I install this driver on my gaming laptop 7900m gpu driver,Negative
AMD,"For some reason, I can’t install the driver only version. No matter what option I choose, it always installs the full package. I tried using DDU, and when that didn’t help, I also tried the AMD Cleanup Tool, but nothing solved the problem.",Negative
AMD,"Man this update is a goat my game graphics improved by alot ,I am running on xtx 7900 and this is the best driver for me without any issues",Positive
AMD,"When choosing the Minimal Installation, it is installing Complete version with all AI crap.",Negative
AMD,You have put in your post:  # Package Contents  * AMD Software: Adrenalin Edition 25.10.2 Driver Version [25.20.21.01](http://25.20.21.01) for Windows® 10 and Windows® 11 (Windows Driver Store Version 32.0.22021.1009)  On AMD's official page it says nothing about Windows 10.  Under Compatability it only says W11    From AMD's Download page:  ===================================   Systems with RDNA series graphics products:  * [AMD Software: Adrenalin Edition 25.10.2 Driver for Windows® 11 (64-bit)](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe)  <---W11 only   # Compatible Operating Systems  AMD Software: Adrenalin Edition 25.10.2 is designed to support the following Microsoft® Windows® platforms. Operating System support may vary depending on your specific AMD Radeon product.  * Windows 11 version 21H2 and later     AMD dropping W10 support???,Neutral
AMD,This stupidly poor update caused issues with my windows install. stuck with attempting repairs till I can get into safe mode and try to fix this shit. Companies releasing absolutely abysmal version updates is a bane on my existence at this point.,Negative
AMD,>USB-C power charging has been disabled for Radeon™ RX 7900 series graphics products.  Users requiring this feature are recommended to use AMD Software: Adrenalin Edition version 25.3.1.  This feels like a uniquely shitty thing to do with no information about why it's happening. I specifically used my USB-C port to play PSVR 2 on PC without the PlayStation Adapter. Guess it's about time I grab the adapter just in case I need to update my drivers in the future.,Negative
AMD,"Texture flickering or corruption may appear while playing Battlefield™ 6 with AMD Record and Stream on some AMD Graphics Products.  Finally I know what was causing my problems, disabling this fixed them. UI was constantly glitching with square artifacts.",Negative
AMD,"Anyone with past AMD driver issues try the newest release yet? Particularly, RX 6700 XT owners. 24.12.1 is still the last stable driver for me.  Here are some posts with the issues I speak of. https://www.reddit.com/r/AMDHelp/comments/1khzck4/version_2551_update_rx6800_crashes_often/   https://www.reddit.com/r/Amd/comments/1k60kzh/amd_ryzen_chipset_driver_release_notes_70409545/   https://www.reddit.com/r/AMDHelp/comments/1k2a66p/blue_screen_with_the_new_2531/",Neutral
AMD,google play games for pc no longer work with this driver. Impossible to start a game. Had to rollback my driver to 25.9.1,Negative
AMD,"On my 9070XT this driver was causing a crash and reboot if I locked my PC then left it alone for a while (20 mins or so) and then came back - the reboot would happen as I moved my mouse to wake my monitor.  If I locked it and immediately went back in it was fine, and games and things all seemed to be running perfectly.  Gone back to 25.9.2 for now",Neutral
AMD,"Ok, anyone know the official way to complain about the removal of the usb-c virtuallink port for no reason, killing psvr2 on one cable? I need to vent to them, hopefully enough do to reverse this stupid decision.",Negative
AMD,Is anyone's discord freezing with this 25.10.2 update? I had to disable hardware acceleration but now discord feels so laggy,Negative
AMD,"crashes my 6900XT in BF6, anybody else have similar issues?",Negative
AMD,"BF6 already feels smoother! Before I was still getting 120 fps ( locked I don't want or need the extra frames, can't hit a consistent 130 anyway) but it didn't FEEL like I was getting anything more than 100.  Now it actually feels and looks like 120 fps with the added bonus of reduced to almost non-existent stuttering and micro-freezing. I think I gained a few extra frames as well but still can't hit a solid 130 so I'll stay locked to 120. Frame timings have definitely been improved!  Super happy with the update so far, will continue to monitor and update you all!  Edit 1: Ran a game of Conquest on Mirak Valley and two rounds of RedSec, I could INSTANTLY feel and see the improved frame timing and smoothness overall. As stated above, also noticed little to no stuttering or micro-freezing that I was getting on 25.9.1. Will continue to update!  Edit 2: Had my first crash, tried to alt tab back into BF6 RedSec in the waiting room and my whole PC locked up ( could still hear my friends on discord though). Swapped over to borderless and haven't had any issues.",Positive
AMD,Thanks for the update that does the exact opposite! Yay!!,Positive
AMD,No Arc Raiders game ready drivers?,Neutral
AMD,"Noise cancellation still broken, it seems. :(",Negative
AMD,"Gotta love AMD support for 6000 series. Error 182. Would love to try this on BG3 with Vulkan, but you know, AMD.",Positive
AMD,"Hello, why is my 6900xt not supported???",Negative
AMD,"New issue I have after this update, pc screen goes black randomly while pc is still on and wont go back. Google says this is signs of a driver update, so Im going back an update to see if this fixes it.  Edit: After downloading drivers 25.9.2 so far no more black screens, looks like the new update did cause that for me.",Negative
AMD,"Still no ROCM Windows support?  I thought that since there was a preview, there would be support in the next non-beta release.  Also the VR issue is still not fixed.",Negative
AMD,They remembered,Neutral
AMD,"We will have a V2 soon I think, unable to install on a 680M laptop.",Neutral
AMD,"Quem estiver com problema com rdna 1/2 baixem pelas notas de lançamento:  [https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html)  ou direto por aqui:  [https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe)  O arquivo lá na frente da página tá só ""rdna3"", pelas notas de lançamento está ""rdna3-combined"", funcionou aqui 6700XT.",Neutral
AMD,"glad to see work graph support, they're likely a large part of the future of rendering",Positive
AMD,"For anyone getting a hardware configuration issue OR error 182.  There is a bug on some of the download pages. Usually I go to my GPU page (e.g RX 5700) and download straight from there. It's downloading a broken version, different file name and smaller size.  Instead either click on ""Driver Details"" and the release notes - then download it from there  OR  [https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html)  Download from there \^  Broken version just has RDNA3.exe at the end  Working version has RDNA3-Combined.exe  Worked for me.  EDIT:  Driver is garbage. Just gonna wait for the next version. Can't believe this is listed as WHQL Recommended...  I've crashed twice today on Battlefield 6 since installing it.  Haven't ever had a crash, even on the beta everything was fine. Reverting back to 25.9.2.",Negative
AMD,No Arc Raiders? 😢,Neutral
AMD,">USB-C power charging has been disabled for Radeon™ RX 7900 series graphics products.  Users requiring this feature are recommended to use AMD Software: Adrenalin Edition version 25.3.1.  What does this mean, because i can still quick charge my phone with the GPU USB Port with the new driver on an 7900XT?",Neutral
AMD,"I literally just installed 25.9.1 only to see a notification for this version afterwards lol. I do plan to start playing REDSEC with some buddies, but might see how it performs on 25.9.1 first as I don't usually jump on drivers/updates the moment they're released in case there are undisclosed bugs.",Neutral
AMD,Atualização AMD! Espero que em breve venha o FSR Redstone. 👏👏👏,Neutral
AMD,Any of you 7000 series folks updating or gonna wait a bit? The drivers always mess with BO6 for me for the first few weeks. Might be a BO6 thing but I play a few games daily and I'd hate to deal with crashes after 2 games,Negative
AMD,So I've had an AMD gpu for a couple months now. Is it normal that the update manager in Adrenaline does nothing? The last two updates I've had to get from the website because the software says I'm already on the latest.,Negative
AMD,I think this driver fixed bf6 crash.,Negative
AMD,Howcome I cant see any updates in Adrenaline for my 7800XT ?,Neutral
AMD,"Cannot install on RX 5700 XT !!!  It says it cannot detect any hardware. I then tried updating through adrenaline app and it installed something and now it shows that i have 25.10.2 released 24.9.2005, so do i have latest drivers or not? wtf  Also, my Windows 11 just updated today and after update, my monitor started flickering like crazy until i removed all drivers with DDU and installed new one (two months old drivers, because i couldnt install these). That was before i have tried new 25.10.2 drivers.  So somebody here messed up something. How can windows update mess up with my drivers?",Negative
AMD,hopefully one day i see improvements on dx11. sim racing titles like le mans ultimate and assetto corsa competizione noticeably performing worse than nvidia counterparts and valorant stuttering like crazy during shader compilation.,Negative
AMD,alt r for fortnite crashes the game instantly but the gpu driver doesnt.,Negative
AMD,"For RDNA2 owners who playing BF6, I am using modded Driver with two old dx12 .dll files from 23.9.1 not get any single crash, I got crashed with official drivers. Hope this help. FSR4 int8  doesnt work in this game.",Negative
AMD,"whatever this update was, it broke Hades 2 w/ enhanced sync on my 9060xt. VRR flicker and shaky, works with enhanced sync off but wow that was obnoxious to troubleshoot.",Negative
AMD,Starts to stutter and low fps on any game like it's losing focus. Alt tab works for a few seconds and back to low fps on this version,Negative
AMD,"* Stutter may be observed While playing Baldur’s Gate 3 on some AMD Radeon Graphics™ Products such as the Radeon™ RX 9000 and 7000 series.  I was losing my mind over this a few days ago, BG3 on Vulkan was unplayable on my 9070 XT. Stuttered every few seconds and I couldn't find anything online about it lol... sooo glad it got fixed and I don't have to play on DX11",Negative
AMD,"9070xt hellhound w GTA V enhanced blokuje na 60fps , po przywróceniu sterownika 25.9.2 300fps",Neutral
AMD,I wonder how long my laptop ryzen 7 5825u will be supported for especially given how intel has discontinued major updates for their processor graphics as new as 2022 I’m concerned if AMD may follow suit,Neutral
AMD,"Every time i upgrade drivers my CPU Temperature disappears from Adrenalin, i have no way to add it again until i do a fresh install of the drivers. Is it just me or its a common bug?",Negative
AMD,Welp time to move to Linux since I cannot afford to move out of my RX 6700 XT. Good job AMD!,Positive
AMD,Just got white screen with 29.10.2 on second day watching YouTube on 8745HS.  Likely going back to 25.9.1.,Neutral
AMD,I have three problems with this new driver: 1. My desktop freezes. 2. Adrenaline opens on its own even when I don't want it to. 3. VR is detected as a second monitor.,Negative
AMD,"Amd installer updated automatically today for me. Do i need to set again to not auto update or does it remember my settings. Also is this normal, on previous drivers amd installer wasn’t auto updating itself (not the driver)?",Neutral
AMD,Is it a recommended method to update the drivers directly through the Adrenalin app? Or is it always better to download the new drivers from their website (specifically from the release notes page)?,Neutral
AMD,can i use this version for 6700xt?,Neutral
AMD,Today I downloaded the driver again from the RX 6800 XT section; it's 912 MB and it says it's not compatible. It still doesn't work!Today,Negative
AMD,Almost at the end of the year and cyberpunk pathtracing crashes still aren't fixed and we haven't seen fsr redstone launch. But hey at least they finally fixed the vr issues after like a year.   Somehow this is a multi-billion dollar company 🥴,Negative
AMD,"Thank you AMD, you just lost a ""20yrs of loyalty customer"" by sunsetting proper RDNA 2 support. While the potential for game optimisations might have been exhausted, the future WDDM and Vulkan updates are important and should have continued for a couple more years instead of freezing on some random branch that has plenty of issues still that won't be properly addressed.",Negative
AMD,"BULLSHIT DRIVER... everything feels like 60hz and supper stuttery... everything was smooth and fine before... good job AMD, not! :\[",Negative
AMD,Driver broke external mouse support on my 7900m laptop.,Neutral
AMD,"Anyone get an error with AMDRSServ.exe? I got an error when turning my computer on saying ""\[file path\]/CNext/AMDRSServ.exe cannot be found"". I did sfc /scannow (idk what this command is actually called) in command prompt and that seemed to fix the issue as the end message did state there were some corrupted files that were repaired. Restarted and no issue upon signing in. Hoping this isn't a temp fix because I thought my computer bricked out at first and my heart sank in my chest lol",Negative
AMD,Did I miss an Adrenalin 25.10.1 release?  I checked for updates recently and the last available release was 25.9.2.,Neutral
AMD,cve has a meaning and I think a few of them are directly related to the vram memory leak problem.,Neutral
AMD,should i hype about this ?,Neutral
AMD,"They are unstable in that new bf6 battle royale, 1min and crash with some driver error, 25.10.1 complety stable",Negative
AMD,"This driver messed up the performance in Battlefield 6 on my 9070XT,so I had to go back to 25.9.2  Edit: it’s not the driver,it’s either the game or EA app servers",Negative
AMD,"Damn, look at all the security vulnerabilities.",Negative
AMD,BF6 keeps timing out for me with these new drivers,Neutral
AMD,"Guys, it's worth updating from version 25.10.1 (I installed this version for bf6)? I have absolutely no issues with this driver, except for the fact that it is a beta driver",Negative
AMD,Anyone else missing AMD Chat?,Neutral
AMD,Radeon software doesn't even give a fuck to update itself and you can't manually check for update manually within it any longer. Also can no longer update to non WHQL releases. What a piss change to some useless garbage AMD updater that doesn't even do its job - just more bloatware.,Negative
AMD,Is that mean that AMD is trying to give up on RDNA 2?,Neutral
AMD,"Definitely the worst RDNA4 driver yet. Massive frame drops in some games (Days Gone, Battlefield) and driver crashes + restart as soon as you reactivate Windows + monitor after idling for a while.",Negative
AMD,"Cool, another update and Cyberpunk is still broken.  Another update I'm not installing.",Negative
AMD,This driver literally makes Stalker 2 unplayable had to revert back to 25.9.1 it's running fine now,Negative
AMD,Wait this is finally the driver update for battlefield 6? A month old already lmao,Neutral
AMD,Why people on rx6000 are so eager to use this version? I don't see any mentions to rdna 2 that makes me wanna update.,Neutral
AMD,The driver only version installs radeon software minimal. Another disaster amd driver.,Neutral
AMD,"doesn't get installed on my RX 6650 XT, Good job AMD  giving me more reasons to go Nvidia",Positive
AMD,">**New Game Support**  >Battlefield™ 6 (DX12)  This why latest drivers and game support is useless, with this drivers Battlefield 6 is extremely unstable.  Rolling back to 25.8.1 and I get stable 60FPS with no crashes.",Negative
AMD,"No FSR4 for RDNA2? Guess Nvidia's RTX 2000 series is still a beast, thanks to DLSS.",Positive
AMD,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
AMD,[AMD.com](http://AMD.com) is down for me  Edited to say it's a me thing,Negative
AMD,YEP same here on 6750XT  error code 182 ''not supported gpu'',Negative
AMD,"You need to download it from the link provided in the release notes: [https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe)  instead of the one provided on the driver download page: [https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-6000-series/amd-radeon-rx-6800m.html](https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-6000-series/amd-radeon-rx-6800m.html)  because the link is: [https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3.exe](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3.exe)  If you are an RDNA 1/2 user like me, you need the EXE with ‘-combined’. I don't know how they could have made such a mistake.",Neutral
AMD,RDNA 1/2 is totally missing in the inf file,Neutral
AMD,Yep. Same on a 6800XT...,Neutral
AMD,"Literally just tried downloading the driver a minute ago for rx 6700 xt, and even forced refresh of the page, still Error 182 for me. File shows as: ""whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3.exe""",Negative
AMD,Same here ...,Neutral
AMD,Up,Neutral
AMD,Same with rx 6900 xt,Neutral
AMD,Nop. Dont work for RX 6800 XT Not compatible .,Neutral
AMD,"Oh, good thing I saw this. I was about to run DDU.",Positive
AMD,"Baixem pelas notas de lançamento:  [https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html)  ou direto por aqui:  [https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe)  O arquivo lá na frente da página tá só ""rdna3"", pelas notas de lançamento está ""rdna3-combined"", funcionou aqui 6700XT.",Neutral
AMD,"yeah same here on a 6800 xt, downloaded it tried installing even updated with the evil new windows update to see if this was the issue but still to no avail, the combined one i did find but it's way more unstable than the previous driver even for the ""supported new games"" they crash and it's not worth the little extra smoothness since it just crashes the whole driver specially with battlefield 6.",Negative
AMD,Driver 912 mb is the right version from rx 6800 xt ?,Neutral
AMD,"mnao meu pc depois desse drive ta todo buagdo nem os jogos quer abrir  , nao consigo nem ver a placa mais sumiu",Neutral
AMD,o meu ta dando erro 182 ate com os drives antigos,Neutral
AMD,SAME !!!!!!!!!!!,Neutral
AMD,What does this mean?,Neutral
AMD,"I sadly don't have one to check, but my phone still quick charges with the new driver so.... maybe? Maybe not.   Maybe it means some version of USB-PD being disabled?",Negative
AMD,Was there ever any news about USB-C power not working properly or causing issues? Would have been nice if AMD explained why USB-C charging was removed and why such an old driver would be recommended.,Negative
AMD,One can hope. I'm literally putting off Doom and Indiana Jones until I can run them with FSR 4,Neutral
AMD,"not working, i get:   [https://www.amd.com/de/resources/support-articles/faqs/Download-Incomplete.html](https://www.amd.com/de/resources/support-articles/faqs/Download-Incomplete.html)  loadet the original , and its still the old one, no support rnda1 rx5700  \-> The new Link now works, thanks <-",Neutral
AMD,Works on my GA402RJ to update both 680M and 6700S.,Neutral
AMD,that link is broken. Maybe try linking to the page and not directly to the download.  Videocardz has the correct mirror download for the combined driver.   [https://videocardz.com/driver/amd-radeon-software-adrenalin-25-10-2](https://videocardz.com/driver/amd-radeon-software-adrenalin-25-10-2),Negative
AMD,"Thanks! Love how the wrong one is linked to on the direct 6800 XT page, but this one works fine.",Positive
AMD,"Thanks for the heads up. I'm wondering, would it be possible for you to capture this stutter in a GPU trace using something like [UI for ETW](https://github.com/randomascii/UIforETW)?  I have a rough guide outlined in a [much earlier post](https://www.reddit.com/r/Amd/comments/11bhohu/refresh_rate_mismatch_in_232x/ja7t7h1/)  If you can capture a trace, compress to .zip, and fire it over to me via https://send.vis.ee or a method of your choosing, we'll be able to investigate; as far as I'm aware, ReLive shouldn't be causing any issues with overlays like that.  Reach out if you need a hand with anything,  Cheers",Neutral
AMD,"There are some old bugs that have been swept under the rug, such as the gradual performance degradation in F.E.A.R. — you can't revisit the classics.",Negative
AMD,"it's fixed , need to redownload",Neutral
AMD,Does this break the psvr2 support through the port? I was using that for it and it was perfect :O,Positive
AMD,Can you explain what you mean by hardware acceleration is broken?,Negative
AMD,That'd be unfortunate considering that the 6000 series is only a few years old.,Negative
AMD,it would go down as the shortest lived GPU in recent memories if so,Neutral
AMD,It looks like most of the extensions and their descriptions seem to either support FSR4 (exclusive to rx 9000 and soon to be rx 7000) for Linux or support  Dense Geometry Format (exclusive to future GPUs and possibly rx 9000 series) for Linux. So it makes sense that rx 6000 and earlier doesn’t have this support.,Neutral
AMD,"Eventually, yes",Neutral
AMD,they probably just don't have the hardware required for some of those extensions (av1 seems to be one of them that isn't available pre 7000),Negative
AMD,No. Just AMD messing around with the drivers.,Neutral
AMD,"Nothing really, maybe some stability or crash fixes. Sometimes, it could mean better perf.",Negative
AMD,"Sometimes bug fixes and performance increases, usually nothing. A game doesn't need to be supported by a driver version to work.",Neutral
AMD,For example on 25.9.1 driver I wasn't able to use FSR4 on Battlefield 6. For some reason... Less crashes and probably better performance are other perks,Neutral
AMD,Doesn’t mean anything. It’s just marketing. At this point Intel drivers are better than both Nvidia and AMD. They have to try since they’re the underdog.,Neutral
AMD,"Yes, I have the same situation. BF 6 crashes without an error. When I try to launch COD Bo6, it gives me an error and doesn't start. My hardware is a 7800x3d and a 7900 gre. I've tried everything, but I had to roll back to 25.10.1, where everything is stable and working.",Negative
AMD,"Confirmed also crashed here within the first match I played after driver install with a Powercolor 7800 XT but with no error, plus occasional artifacts in the loadout menu before that. DDUing directly back to 25.9.1 that I was on before 100% fixed it.",Neutral
AMD,"Yes, yes, yes! Are we going back to the days of problematic drivers?",Positive
AMD,Haven't noticed any difference between the two drivers.,Neutral
AMD,Big difference. Crashed 2 times in the first 5 hours of having 25.10.2. Haven't crashed on battlefield from beta up until installing that shit driver.,Negative
AMD,i think its smoother on bf6 zonewar but audio is mega delayed,Neutral
AMD,My frames droped from 140 to 50 on 7090 XTX,Neutral
AMD,IDK it feels like the game is running on a slower refresh rate but FPS says otherwise. Spawnselection is noticeably stuttering. 9070xt,Negative
AMD,"Ah so that’s why I see them on Steam and Spotify. I think I haven’t seen them on Firefox yet, I thought my GPU was in the beginning stage of failing.",Neutral
AMD,"Clean install with drivers only fixes all of the problems I was having, sucks that I cannot use adrenaline software but everything runs much much smoother without it now.",Negative
AMD,I also have this issue but it only happens sometimes in YT. Such a weird issue.,Negative
AMD,"Every time I ever saw pixel corruption like that it was because of RAM instability. And unfortunately RAM is fickle and hardware vendors take shortcuts.  It's not uncommon for RAM to be perfectly stable at X voltage and Y temperate, and very unstable just a little bit outside of that. It's not uncommon for MB makers to supply more voltage than they claim, because it generally helps with stability which reduces the cost of their returns and warranties. And it's not uncommon for instability to appear in only one specific place when your intuition tells you it should show up elsewhere too.  So my advice is test your RAM in cold and warm scenarios. And try reducing the voltage.",Negative
AMD,Try disabling Hardware Acceleration,Neutral
AMD,"Hold ya horses now mate! Can you believe this guy? Bro wants to have up-to-date drivers with big game releases lol. They have just released support for a month old BF6, so...",Negative
AMD,"Fr, this is crazy. I wouldn’t assume too much a performance difference with how well amd had aged but it would guarantee fsr 4 support…..",Neutral
AMD,"Arc raiders ran pretty decently on my 9070XT. I wasn’t watching my fps (which is a good thing imo) and I don’t have to go back into settings to fix things like except get FSR4 working. While yea I would like a day one driver, it’s not like the game was running bad. If I remember posted benchmarks it was even above a 5080 in perf.",Positive
AMD,This update broke mine,Neutral
AMD,Hatte gehofft es läuft endlich wieder. Sehr schade...,Neutral
AMD,Yes I had it. You must set a location to Store the Capture. There is not always an Error about it,Neutral
AMD,Been having this problem for almost a year now. I gave up tbh lol.,Negative
AMD,"Windows update may have forced a driver, overwriting what you had. I had this happen and it caused a few issues.   Use DDU to remove and download 25.10.2, or you may be able to install overtop.",Negative
AMD,why you on pro drivers? and yes it is the latest driver,Neutral
AMD,"It's based on branch 25.20.x, so yes",Neutral
AMD,nah,Neutral
AMD,"not new driver problem, this happens randomly",Negative
AMD,"me2 and it's super bad performance like 60hz@40-60fps (display is about 160-170, before it was 180-220)",Negative
AMD,Absolutely,Neutral
AMD,Always use latest drivers.,Neutral
AMD,"If you have some bug in games or something that the 25.10.2 solved then yes, if you're ok with the driver you're using... for what reason? Don't worry, I'm rocking mi 9060xt with the 25.9.1 driver, it's sooo good that I don't think I'm changing to another driver soon jaja.",Positive
AMD,mano a minha rx eu nao consigo nem instalar a antiga versao de tao bugado que ficou,Neutral
AMD,po mano minha rx sumiu depois da atulizaçao ja to a 2 dias tentando arrumar,Neutral
AMD,"Aah, had been thinking it was an issue with a google update then realised I'd just updated my drivers and checked in here. Can confirm this issue too.",Negative
AMD,Same GPU same issue. Going back to 25.9.2 fixed the issue,Neutral
AMD,BF6 runs well for me on the RX 6650 XT,Positive
AMD,Man fsr 4 better work out of the box…..,Neutral
AMD,"Yeah the new update broke it for me, was working perfectly fine before. Typical amd, break and dont fix stuff",Negative
AMD,"Tbh most of the new vulkan extensions won't work on rdna2 except for the vp9 decode..and hopefully, fifo latest present mode",Negative
AMD,There's a different download link for older cards.,Neutral
AMD,I'm having similar issue in the game Ark survival ascended,Neutral
AMD,"Para que eu deveria atualizar? Estou na versão 25.9.1 com a 6800xt. Está estável e com boa performance. Não vi nenhuma melhoria direcionada as placas da família 6000. Se não está quebrado, não conserte!",Neutral
AMD,"This is working, however the previous one when searching for 6600 drivers and getting it from the page also had rdna3combined. Still didn't work. However 6600 drivers compared to this is the file size is almost double. 903mb from 6600 page and the one you linked 1.5gb.",Neutral
AMD,"7900 XTX, always using latest drivers",Neutral
AMD,"Yes, it’s normal, the panel on Adrenaline only shows updates after a week or so of the update release date on a good day",Neutral
AMD,They messed up driver names. You'll find the proper ones somewhere in the comments.,Neutral
AMD,"The big DX11 revamp was in 2021 (2022?). DX11 runs excellent. In the games you mentioned, AMD might be slower due to other reasons, not DX11 overhead.",Positive
AMD,wouldn't using that FSR4 mod on RDNA 2 get you banned in BF6 due to its anti cheat?,Negative
AMD,"Whenever I boot up sometimes it works, sometimes it doesn't.",Neutral
AMD,good luck gaming on linux,Positive
AMD,25.10.1 was the Battlefield 6 Preview driver released earlier in the month. Had to manually install it from the AMD site.,Neutral
AMD,"Some updates don't come in adrenalin software, you need to download from the website.",Neutral
AMD,No this company is clown shoes,Negative
AMD,'25.10.1' isn't completely stable for everyone. Device hubga device removed CTDs been happening for weeks and .1 dropped on 10/10.,Negative
AMD,nothing changed for vega/polaris path.,Neutral
AMD,what you mean?,Neutral
AMD,skip driver upgrade if everything is working fine.,Neutral
AMD,"It's a new branch of drivers, so yes, absolutely.",Positive
AMD,Drivers are available for RDNA 1-2. They just uploaded files incorrectly.,Neutral
AMD,What issues did you have? Mine is running the same as before. The driver did require a restart too if you didn’t do that?,Neutral
AMD,Not the shader cache repopulating?,Neutral
AMD,Game runs without problems without official support. Most of the time official support is not a requirement for your card to render something. I'm running bf6 with version 25.6.1 since beta without any issues on a card released in 2020.,Positive
AMD,??? the 25.10.1 optional driver was released on launch day for BF6,Neutral
AMD,"dont talk, just go",Neutral
AMD,It works for me,Positive
AMD,Up for me,Neutral
AMD,![gif](giphy|XD4qHZpkyUFfq)  oh god the quality control is so bad,Negative
AMD,Same here with RX 6800 XT AND WINDOWS 11 25H2,Neutral
AMD,"Same here, with RX 5600 XT, the issue is I uninstalled the drivers first. And I did endless tests :/ to solve the problem.",Negative
AMD,Same error :((,Negative
AMD,Is it a recommended method to update the drivers directly through the Adrenalin app? Or is it always better to download the new drivers from their website (specifically from the release notes page)?,Neutral
AMD,Yes there is no inf for rx 6000 series,Neutral
AMD,Thanks,Positive
AMD,"for me is ""whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe""",Neutral
AMD,"for me it says it is downloading rnda3, guess it will wait for tmrw",Positive
AMD,"Just a quick question, the last driver that was stable for me with my RX 6700XT is 24.12.1, have the  25.??.? drivers been working for you?",Neutral
AMD,"I did run to it, was updating my nieces RX 6400 and was like wtf when I got the error. DDU didn't help so I tried the auto detect and it pushes me to the pro drivers so some reason. I simply assume it was some conflict do to running Win11 on an ""unsupported"" system so I ended up rolling back to 25.9.1 which installed issue free.  Good to know its just AMD having a Microsoft windows update moment but lucky I have to need to go back and update her drivers again. She plays basically just plays fortnite so yeah see ya in 2026....lol     \*oh and this was as of 4:30pm EST  \*\*Just checked Windows 10 on 7900 XT and its fine.",Neutral
AMD,More performance at the beginning but a new excuse for developers to not optimize their games,Negative
AMD,Both those games run absolutely fine on the system you've got in your flair.,Positive
AMD,"Holly crapt, 1.56 gb ?? So we install the 900 mb wrong driver ?",Negative
AMD,"Here are the captures from the games I had installed: [https://send.vis.ee/download/9f799176b79cd224/#su81YterFXzSOqZQXVvBXg](https://send.vis.ee/download/9f799176b79cd224/#su81YterFXzSOqZQXVvBXg)   One is Hellblade Senua's Sacrifice and the other is for Little Nightmares 2. The game I showed above was The Legend of Heroes: Trails into Reverie. I didn't have it installed, but if you want that specific game I can download it again.   Tell me if you have any problems or need anything else.",Neutral
AMD,Most reported bugs will be ignored. What's the point?,Neutral
AMD,Any more details? FEAR 2 still runs at crazy high FPS for me.,Positive
AMD,"I've played through FEAR 1 + DLCs relatively recently. You need mods for acceptable perf in game with any platform, aside from that, I'm not aware of any gradual perf decline.",Neutral
AMD,"All the FEAR games run fine.  FEAR 1 has its classic issues with HiD, that needs a mod to fix. Unrelated to GPU drivers.",Neutral
AMD,"I cant install on RDNA 2, the file contains RDNA 3 in the file name",Negative
AMD,No it's not.,Neutral
AMD,"seems so, im not sure what this statement means,  i need to test my HMD once i arrive home. but if they disable power delivery over the VirtualLink port then GG PSVR2. forcing you to use that dumb convert box they sell for Nvidia cards.",Negative
AMD,I mean if you experienced the bug that they just fixed in this driver then working “perfect” isn’t really all that perfect is it?  There’s been a lot of issues with virtual link which is probably why they gave up and magically fixed the vr issues at the same time.,Negative
AMD,HW video encoding/decoding leads to driver timeout or even system crash. Had to disable hardware acceleration in browser to watch youtube etc. Applications such as Apple Music that use hardware acceleration but do not provide an option to disable it are thus unusable.,Negative
AMD,"> whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe  They updated the link. Now RDNA1/2 and RDNA3/4 drivers are branched, which is not good news.",Negative
AMD,This is literal ASS.  ![gif](giphy|IVhivwuUT16HH7NRdP|downsized),Negative
AMD,I don't want to believe it either. But Team Blue ended standard support for Windows drivers on Iris Xe with similar wording last month. RDNA1/2 and its release date are similar.,Negative
AMD,That's Intel Ice Lake still. Maybe Radeon 7.  Maybe ATI X800 series.,Neutral
AMD,Hopefully it fixes bf6 crashing that's a big issue rn,Negative
AMD,Unless you use specific DX12/Vulkan extension that is introduced by particular driver version ;),Neutral
AMD,Intel drivers are by no means better. They seem to be since they add performance in more titles (compared to Nvidia/AMD) but that's more due to them being terrible at first and Intel working to fix those issues.,Negative
AMD,does your overlay still work?,Neutral
AMD,"Thanks! Unsure if I'll update my drivers or just wait for now, as the Preview Driver is working well for me.",Positive
AMD,"The spawn selection screen is also stuttering for me, it lasted on the 1.1 update.  I've seen people mentioning that it's tied to FSR. If you turn it off the stutter goes away.",Negative
AMD,Does this actually help with the Chromium-based app artifacts?,Neutral
AMD,Nvidia had the drivers ready for the tech test 2 that ran for only 3 days lol. Also AMD prioritizing support for a flop that is vampire bloodlines 2 over arc is icing on the cake,Neutral
AMD,Not necessarily but I feel like fsr4 support is already there? Drivers back in September already made games with true fsr3.1 automatically enable fsr4.,Neutral
AMD,"But I didn't change it, I'm using the default one:   `C:\Users\%USER%\Videos\Radeon ReLive`",Neutral
AMD,"I'm pretty sure it worked recently, maybe updating to Windows 11 25H2 broke it but I'm not sure as I said. That's a bummer because I've always been telling my friends that ReLive is a pleasure to use when choosing a new GPU :|",Negative
AMD,Can i just download 25.10.2 and be done with it? I'm scared to try doing ddu since im noob lol,Neutral
AMD,What is pro drivers. Just built my pc this year. Should i just download this 25.10.2 drivers and be done with it?,Neutral
AMD,"I don't know either, I also have 10.30 installed. It was updated last through Andrenalin as I recall.",Neutral
AMD,should i use DDU when installing a new version of Adrenalin?,Neutral
AMD,should i use DDU when installing a new version of Adrenalin?,Neutral
AMD,Oof that sucks dude 😩,Negative
AMD,It did during the server slam. No reason why that would be turned off on a driver level.,Negative
AMD,BF6 has worked this whole time before official support,Neutral
AMD,"Anything is welcome news. Hopefully it fixes it, it doesn't specifically say 6000 but, one can hope. Otherwise, dx11 forever until I get a better gpu and cpu.",Positive
AMD,Found it!,Neutral
AMD,"Bom, eu gosto de manter sempre atualizado independente de ter problemas ou não, mas tudo bem em não querer atualizar também, vai de cada um, se não tiver problemas não precisa.",Neutral
AMD,"Also, past drivers for my 9900x have been in the 900 MB range, just like this uninstallable new 25.10.2. I'm not moving up to a 1.6 GB driver when they've clearly made a mistake.",Negative
AMD,This my first experience with bad AMD drivers too. I guess they aren't immune to that just like nvidia. Been an awful experience playing Arc Raiders with this version lol,Negative
AMD,"it just doesnt make sense. in basically all games with new apis they are neck and neck, but in games with older apis especially ones that are cpu heavy it seems to slow down. at least from all the research ive done it points to dx11 and its dependence on driver to be the cause. if you have a different thought id love to hear it.",Negative
AMD,"I just swapped the fsr upscaler dll file, it work very well in Marvel rivals. I haven’t got banned in BF6 since I swapped the dll file. Do not use Optiscaler in online games, you should be fine.",Positive
AMD,"heh, already did on CachyOS bud - not really an issue for me.",Neutral
AMD,"I checked the AMD website earlier on Monday.  I did not see a 25.10.1 release.  The most recent release at the time was still 25.9 2, which is what I had installed.",Neutral
AMD,"The last point on the ""Fixed Issues"" list:  CVE-2023-4969 (RDNA only), CVE-2024-21969 (RDNA only), CVE-2024-36323, CVE-2024-36325, CVE-2024-36333, CVE-2025-61964, CVE-2025-61965, CVE-2025-61966, CVE-2025-61967, CVE-2025-61968     These are CVEs for security vulnerabilities.",Neutral
AMD,"No, they're moving it to a separate branch. That's the first step in making them legacy and no longer receiving new game optimization.",Neutral
AMD,\>What issues did you have?  Extreme stuttering basically moving around causes the whole game to drop massive frames all the time no matter what you do old driver was fully uninstalled and DDU'd and i did the double restart as well i even tried reinstalling which gave me the same issue 25.9.1 doesn't do this i will just wait till next driver,Negative
AMD,"mano minha rx 7600 sumiu do gerenciador de dispositivos , mais ela esta dando imagem mais nao da pra jogar acho que so formatando pra concertar",Neutral
AMD,"If you haven't found a fix yet, I found one for mine user @aker666 described it in this thread perfectly  go to your driver download page   click driver detail  click release notes  scroll down until you see:  Systems with RDNA series graphics products:      AMD Software: Adrenalin Edition 25.10.2 Driver for Windows® 11 (64-bit)  and click on the link    the file is larger than usual, I think it was 1.6 GB, but it did the job",Neutral
AMD,"opa vc consegui arrumar mano , aminha rx aki sumiu so ta com a dedicada ta foda fazer ela funfa",Neutral
AMD,"In my case, doing it through the application caused application errors (I suppose Windows 11 being Windows 11...) and driver timeouts. Until one day I discovered that if I downloaded the full installer, disconnected the laptop from the internet (airplane mode) and without a network cable, used the installer to do a clean install and restored the user settings... I've never had a driver timeout while gaming again.       If it works correctly for you when you do it from the application itself, I see no reason to stop doing so.",Negative
AMD,Just saw your comment and tried downloading it again from (forced a refresh with CTRL F5):   [https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-6000-series/amd-radeon-rx-6700-xt.html](https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-6000-series/amd-radeon-rx-6700-xt.html)   And clicked the on Win 11 download link... not combined :c,Neutral
AMD,"Apparently if you get it from a different link, it has the combined version that is twice as large download, but it seems to work. [IllustratorDeep7780](https://www.reddit.com/user/IllustratorDeep7780/) updated his comment again, and it now has the link: [AMD Software: Adrenalin Edition 25.10.2 Release Notes](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html)",Neutral
AMD,Bean soup theory much?  I know what I'm lookig at and on my 4k OLED tv it's not it. Feel free to game however you like,Negative
AMD,"Yeah, esp. Indi with path tracing on. And FSR4 will not be enough to make it playable at 4k on 9070XT, the FSR Redstone is necessary.",Neutral
AMD,Captures received - appreciate you,Neutral
AMD,"I find it amusing that you'd take the time to complain to me in such a generic manner without helping me help you in any way whatsoever.  The example provided elsewhere tells me you've never played FEAR on a modern system. Good on you for checking out a cult classic, read the guidance over on PCGW to get set up properly, then enjoy the game.  Get back to me with actual data and I'll see what I can do.",Negative
AMD,"You need to download it from the link provided in the release notes: [https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3-combined.exe)  instead of the one provided on the driver download page: [https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-6000-series/amd-radeon-rx-6800m.html](https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-6000-series/amd-radeon-rx-6800m.html)  because the link is: [https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3.exe](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.10.2-win10-win11-oct-rdna3.exe)  If you are an RDNA 1/2 user like me, you need the EXE with ‘-combined’.",Neutral
AMD,"I don't know what it means at this point, because even with this driver I am able to QC devices and my headsets all work.",Neutral
AMD,"Is that similar to the Polaris/Vega drivers, which are updated very infrequently and do not receive new features?",Neutral
AMD,"Maybe it's just due to the AI hardware? RDNA2 has no AI features, so maybe they just don't bundle the AI NPU Driver?",Negative
AMD,Best case: FSR 4 int8 is coming soon and they want to only provide the necessary version for everybody.,Positive
AMD,They're literally not branched.,Neutral
AMD,What’s your error? Mine throws a device_hung error at random but BF6 is the only game that does it,Negative
AMD,"I haven't crashed on Battlefield 6 ONCE even since the beta. Downloaded 25.10.2 (which took them way too long to release might I add) and it caused me not 1, but 2 crashes in 5 hours lol. Garbage driver by the looks of it.",Negative
AMD,*stares in ice lake gpu support*,Neutral
AMD,I don't use the AMD overlay I'm a rivatuner + capframe X guy and that is working fine for me.,Positive
AMD,Oh thanks Imma try it,Positive
AMD,"It was happening on everything. Discord full screen streaming, wowup when I used to use it, Opera GX, etc… all purple/pink artifacts gone once I got rid of the adrenaline software and did driver only install. Wish I could tell AMD somehow that it’s their software causing issues and that they need to rebuild it from the ground up to stop all of this nonsense but I doubt they will ever see my post :(",Negative
AMD,"NVidia also has BSOD left and right, game ready drivers are useless.  **25.8.1** runs both **Battlefield 6** and **Arc Raiders** at stable 60FPS, while this so called game ready drivers are extremely unstable on **Battlefield 6**.",Negative
AMD,"If you're not comfortable using DDU then yes, just install 25.10.2 overtop. It should be fine.",Neutral
AMD,pro drivers are for work stations-work station gpus and stuff but they still work fine but if you just game on the pc download the standard drivers aka 25.10.2-25.9.1 etc. not the 25.10.30.,Neutral
AMD,Only if you experience issues that could be caused by old driver remains,Negative
AMD,"No, thats not *usually* necessary",Neutral
AMD,"bota azar eu ia fazer live no tiktok , mais se pa vou ficar um tempo sem pc",Neutral
AMD,I don’t think it did. Since apparently it was an older build of the game. I was playing at 4k and I’m pretty sure it was fsr 3 even though it said fsr 4 was on. The grass looked like crap even on fsr quality at med/high settings. Bf6 vegetation/shimmering looked more stable than arc raiders.  I also did a quick test and I toggle fsr 4 and both with and without fsr 4 it looked the same.,Negative
AMD,"There are a lot of quirks present in drivers to make stuff performant.  The devs might be choosing a really poor rendering path for AMD for example and no workaround can boost performance.  If you're CPU bound in DX11, use DXVK or DXVK GPLAsync.  If it's an UE 4/5 game, run in DX12 instead of DX11 and you'll be less CPU bound once again.  If you want to further lessen CPU boundness, consider playing on Linux (in DX12 with VKD3D) if possible.",Negative
AMD,interesting thanks for the update.,Positive
AMD,"I know, sometimes they don't appear if you search specifically for your gpu, you need to search for the driver's name in google and download from amd website, the software will automatically install the correct driver for your gpu model. This happened with my 6700xt, if I search for it drivers in amd website, the latest one is 25.9.2     EDIT: they fixed right now, it's showing 25.10.2 in the website for my 6700xt",Neutral
AMD,save up?!? polaris and vega are 8 yrs old. skip the saving .. just go buy something new and enjoy lol,Neutral
AMD,">in short never buy amd no matter what, unless they sell the 7900 xtx for 5 bucks, i've had amd for many many years and every single day i regret it until i finally saved up enough to get a 4080  that you? lol!! 2 yrs ago u been saving for nvidia already... hope u can soon finish saving up.   also i thought you had a 1060 3gb... so where does all the 'had amd for many many years' talk come from?!  embarassing stuff.",Negative
AMD,hey yo. tell me as soon as you buy the 5080 btw.  cuz i wanna congratulate you...,Neutral
AMD,This might be the reason that web browser is enabled by default on this version. Did anybody else realise it as well? Also animations & effects is disabled for some reason.,Neutral
AMD,">That's the first step in making them legacy and no longer receiving new game optimization.   From the release notes, looks like RDNA2 and RDNA1 have **already** lost new game support. Whatever they had last month, that's it for new game-specific fixes and optimization.  >New Game Support and Expanded Vulkan Extensions Support is available to Radeon™ RX 7000 and 9000 series graphics products.",Negative
AMD,"Updated as Combined file for Rdna 1 and 2, download it. If it doesn't work, go back to version 25.9.1 and it will be installed this time.",Neutral
AMD,this world is really conplicated,Negative
AMD,from here [AMD Software: Adrenalin Edition 25.10.2 Release Notes](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html),Neutral
AMD,"yep, i saw, tho i will wait for tmrw and see, not in a rush",Neutral
AMD,Yea I did it now it open by it self with the 6700xt. Got laptop ai 300 fix hardware acceleration when watching YouTube and working flawless in my 7900xtx finally fix warzone lag in big map .,Positive
AMD,"Appreciate you taking bug reports via reddit, that's awesome!",Neutral
AMD,"All the bugs I've cataloged and reported over the past two years are still in exactly the same place. What do you want me to say?   F.E.A.R. is an old game. I tested it on my laptop, and it runs at 120-150 fps@1080P, but inevitably, the performance degrades within minutes until it reaches 50-40 fps. No, it's not thermal throttling, running at 11-15W and 60°C. It's a driver issue.",Negative
AMD,"Okay, thanks.",Positive
AMD,"Basically, yes. When Vulkan 1.5 is released in the future, RDNA1/2 will not be updated on Windows and will remain at 1.4. You will have to switch to Linux to get it.",Neutral
AMD,"This is a bit of a stretch. Considering that the RX 6800 and 6900 series are equipped with 16GB of VRAM, they can handle many basic AI tasks even without an NPU.",Neutral
AMD,"Really hoping for official int8 fsr4 support on rdna 2, but I feel like it's not gonna happen sadly. They don't seem that interested in bringing new features to older cards.",Negative
AMD,Device hung and device removed are the most common ones. Hopefully both are resolved with this but I'm not banking on it as it's like on Dice not Amd to resolve it as it impacts nvidia gpus too,Negative
AMD,Bf6 is so buggy it's not funny i guess it's a combination of the game and the driver,Negative
AMD,"I get like 3 crashes everytime I play BF6 for long periods of time, with hard restart. The AMD software literally never loads up or disappears after every crash. :(",Negative
AMD,"Had a 5080 since launch, no BSOD. Quit the cope. Bf6 stable 60fps lmao is that what you spend $1000 on for just 60fps.",Negative
AMD,"It was just a placeholder for this message, there's my real username in there",Negative
AMD,"I had 25.10.30 installed, its just what my pc installed automatically 😅 seemed to work perfectly fine,    25.10.2 broke noise suppression for me",Positive
AMD,For me toggling FSR4 on made the game run very sluggish for some reason. Also don't think it was working properly as the visual quality wasn't looking better than FSR3,Negative
AMD,"unfortunately none of these solutions will work because ive tried them. dxvk doesnt work properly (i tried multiple versions) in these games and either tanks performance or just has bugs which arent worth it. acc's dx12 implementation is basically non existant. linux unfortunately isnt an option for me.    idk its just really weird to me that in most games its nice and dandy, but in these its not. usually such a disparity would mean a driver issue, but idk anymore at this point.",Negative
AMD,How did u find this comment?,Neutral
AMD,pena  que da erro 182 toda vez,Neutral
AMD,thank you!,Positive
AMD,"The point still stands that you'd rather take the time to antagonise me instead of getting your issues addressed.  You may not be familiar with my involvement here, but I generally put in the time, care and effort to drive legitimate, reported bugs to resolution.  I'm ready when you are, but If you'd rather not cooperate, then that's on you.  As for that being a driver issue, I'm not convinced. I've played the hell out of FEAR1 with no such behaviour, though this was on RDNA2 and separately on Cezanne.",Negative
AMD,they are still missing ai cores which even the 7000 cards have (not many and they are weak but they are there),Neutral
AMD,"Fixed device removed problem by changing from pax to nato and one primary weapon, as stupid as that sounds but it actually worked at least on 1070ti which I’m using now while waiting my RX9060XT 16gb from RMA.",Negative
AMD,"Unfortunately it’s not fixed, hopefully EA can fix it soon. The CTD’s are pretty annoying.",Negative
AMD,I've had this AMD card for like 5 years now. Honestly mate I just think the majority of problems are AMD related. I know sometimes it might only affect one game so people blame it on that game - but then NVIDIA users have no issue with that game. I've had a whole wide range of issues regarding AMD drivers over those years and I'm pretty sure I'm gonna hop back to the other side when I get a bit more money.,Negative
AMD,"I don't know what lying gets you \[[1](https://www.neowin.net/news/nvidia-drivers-confirmed-to-cause-bsod-on-windows-pcs-without-certain-cpu-instructions/)\]\[[2](https://www.nvidia.com/en-us/geforce/forums/geforce-graphics-cards/5/556316/bsod-with-the-latest-versions-of-the-nvidia-driver/)\]\[[3](https://www.tomshardware.com/pc-components/gpus/nvidia-confirms-it-is-investigating-rtx-50-series-bsod-and-black-screen-troubles-no-timeline-for-a-fix)\], is quite clear on everyone who is copping.  Plus I didn't spend any $1000, the RX 6950 XT cost me 600€ and I getting 60FPS at 4K max settings, thank you for your unwarranted concern.",Neutral
AMD,ACC is UE 4.26. You can launch it in DX12 by adding -dx12 as a launch command. Let me know if it works and if it becomes less CPU bound.,Neutral
AMD,"I wasn't criticizing you, but rather the effectiveness of the bug reporting process and the apparent lack of fixes over the years that I've observed. Here's the same Cezanne that you said you tested. In five minutes, without leaving the spot, the performance degrades from 120 fps to 58 fps.     All drivers updated to the latest available version.  https://i.redd.it/k443c4kkq4yf1.gif",Negative
AMD,This makes sense from a marketing perspective. But ending standard support in 5 years is still too short for dGPU.,Negative
AMD,"Thanks for the reply man, saved me logging in to only get more annoyed myself! Arc raiders for now it is!",Positive
AMD,If I go nvidia it's 5090 or 6090 only so yea too end onlys,Neutral
AMD,i know... as i said i tried most of this stuff. it exists but it might as well not. the performance hit is even worse than just running dx11.,Negative
AMD,"are you making use of the recommended mods like large address aware? The last time I played through, I remember having to also use a reworked directInput module as the game would respond horribly to mouse input otherwise.  Looking at it now, EchoPatch seems like a recent development (or at least one only recently recommended), though seems to offer a comprehensive solution across many problem areas experienced when running on contemporary operating systems.  E: One other thing comes to mind, are you running this from Steam or GOG? I've only ever ran this through Steam (requiring the above mods). Looks like F.E.A.R. Platinum is in the GOG Preservation Program (thinking i should pick this edition up anyway).",Neutral
AMD,"He's not doubting that it occurs but there's known problems with FPS drops over time on modern systems that happens that happens regardless of vender so a driver can't fix it. Also assuming it's a driver issue without actually testing another vendor or at least older versions of drivers is a bit silly.  Oh and don't post screenshots as gif, it's disgusting (waste of bandwidth and hurts the eyes)...",Negative
AMD,"They aren't ending support, but you should expect some features not arriving to RDNA 1 and RDNA 2. (I wouldn't expect RDNA 2 to have FSR 4 officially)",Neutral
AMD,"If performance drops in DX12, then you're GPU bound and not CPU bound. Then your options remain upscaling and/or Frame Gen.",Neutral
AMD,"It's the Platinum version, from GOG. By installing the echo patch, performance increases to 200fps+, and degradation takes much longer to occur, but drops still happen, going down to 30fps.",Neutral
AMD,This Reddit only allows posting images in .gif format.,Neutral
AMD,"This also applies to Polaris and Vega if you think they are still supported. They do get Vulkan 1.4 on Linux, but end up with Vulkan 1.3 on Windows. RDNA1/2 suffered the same fate.",Neutral
AMD,"but how then 9070 xt and 5070 ti are basically equal performance in other gpu bound games, but in these even a 5070 does better....",Neutral
AMD,"That's neat, I'll need to pick up that GoG version to check in with. I've only ever used the Steam version on win10 and 11.  The Cezanne system I have has been dutifully running Fedora for a bit, will need to see if I have a spare m.2 NVMe disk to throw windows on. Can you tell me your windows build and in-game gfx settings?  e: Interestingly, there's an old [GoG forum post](https://www.gog.com/forum/fear_series/fear_fps_fix) from 2017 detailing the exact same issue you've described, with a link to a fix.",Neutral
AMD,Make sure you do the 4GB patch that Vik suggested. I'm playing through FEAR on my 7800XT and it runs perfectly after doing all the suggested pathes/tweaks from PCGamingWiki. https://www.pcgamingwiki.com/wiki/F.E.A.R.#4_GB_patch,Positive
AMD,"https://i.imgur.com/UxevhaL.png  Can't you upload to an external site? I don't use the app so they don't even give you the opportunity to upload an image to Reddit so I wouldn't know about those restrictions. That said, if you are posting a screenshot of a game, wouldn't you also be in browser?",Negative
AMD,There are games and games. Plenty of games where AMD GPUs punch above equivalent tier Nvidia GPUs. It's not all 1:1.  7900 XTX matching 5080 in DOOM The Dark Ages that uses RT for example.,Neutral
AMD,"The slide also states a Zen 6 release in 2026 for the ""Venice"" Zen 6 EPYC.  And a planned release of openSIL Firmware for Medusa Zen 6 in the first half of 2027.",Neutral
AMD,"I'm happy about openCIL, I think transparency and collaborating with others is the key to reducing attack vectors. Alternatives to UEFI/BIOS like coreboot should benefit from this!",Positive
AMD,Zen 6 in 2027 = my 9800X3D will be king of the gaming hill for a good while yet,Positive
AMD,Nada do redstone?,Neutral
AMD,First half 2027 probably means real late 2027 for the 3D chips as well.,Neutral
AMD,It doesn't say anything about the release date of Zen 6.,Neutral
AMD,9850x3d enters chat. lol,Neutral
AMD,"well, Intel is dead in the water so AMD is in no rush to release a half baked 3% performance increase.",Negative
AMD,I don't think opensil firmware release date tells you much about the actual CPU release date.,Neutral
AMD,Not with that attitude,Negative
AMD,"For Venice the plan according to the slides is to have an openSIL release within 3 months of the hardware release, and the stated goal of AMD is to shorten this time gradually until openSIL is released at the same time as the hardware, so I'd assume that for all Zen 6 hardware, the time between the hardware release and the openSIL release is at most 3 months.  While that doesn't really tell us anything about 3D chips, it would put the Medusa release sometime Q4 2026 to Q2 2027.",Neutral
AMD,I still need to check if this kills psvr2 on my 7900xt or not - kind of an ass move to kill one of the unique features of that card.,Negative
AMD,\>AMD disables USB-C power on Radeon RX 7900  Isn't this a breach of contract or 'not as advertised' in some markets?,Negative
AMD,They're definitely not backporting fsr 4 to rdna 2 at this point,Neutral
AMD,Killing support for rdna2 this quick is criminal while nvidia is still supporting rtx 2000 series which is rdna1 counterpart... I am glad I got my 6800xt for barely 300$ from a gpu mining farm instead of buying at full price. Can't even imagine how I would feel,Negative
AMD,My 6950 doesn't feel old enough to be pushed out already.  Makes me seriously consider getting nvidia for my replacement with this type of crap.,Negative
AMD,"branching off RDNA2, a four year old gpu arch, while nvidia just got around to dropping pascal/maxwell, is...  not cool guys.",Negative
AMD,The USB-C port is the reason I bought a reference 7900 XT! I have a mini-ITX case with a handle and the USB-C port lets me do a quick one cable + power connection on any of my home desks or televisions. I now need to check if any of my unpowered hubs still work.,Neutral
AMD,"the USB part is so baffling when you realize that the PSVR2 has an advantage on the reference cards due to requiring only one cable.   and WORSE still. its a catch 20/20 situation, this driver fixes a grave VR issue present for almost a year. but at the same time kills VirtualLink support.",Negative
AMD,"This is some bs, they need to reverse this decision, AMD is getting a little too greedy, pulling stunts even nVidia haven't done yet,. There's no reason not to add FSR 4 support for older cards, and no one, unless it's tied to something breaking, should be allowed to disable features after release.",Negative
AMD,Man was gonna eventually upgrade to a 6000/7000 series card in the future from a 5700 xt. I fell like there’s no point now.,Negative
AMD,It's sad that amd often kills support or degrades the support faster than nvidia  I plan to upgrade my gpu soon ( my cpu upgrade is a 7800x3d already here ) but I will absolutely look more on the nvidia side.  6000 series isn't old yet and them already putting it on some weird 2nd branch isn't looking great.  I'd rather pay 50 or 100 more and don't have a badly supported gpu in a few years.,Negative
AMD,"Jesus christ.   GPUs like the 6800 XT are only 5 years old & being considered 'Legacy' & so thus getting degraded drivers. Meanwhile Turing GPUs from 2018 are still getting frequent full driver support (and so is the 11 year old GTX 980 for the time being). People wonder why people go for Nvidia for GPUs, but things like the official driver support are a very good reason why people go Nvidia, as they give frequent GPU drivers for older GPU series whilst AMD starts to kill them off after 5-6 years.",Negative
AMD,"It's pretty funny that nobody double-checked if the functionality was still there before complaining.  > We’d like to inform you that the release notes for AMD Software Adrenalin Edition 25.10 2 posted included misinformation that has since been corrected. There is no change to USB-C functionality on the RX 7900 series GPUs in the 25.10.2 driver. There was an incorrect line in the originally posted release notes that has been removed, and the release notes have been updated.  tl:dr: the USB-C functionality was not altered",Negative
AMD,"Damn AMD doing everything they can to fuck over their own products as usual. I literally have a 7900xtx reference, definitely not buying AMD again now. Even NVIDIA still supports it on 20 series cards",Negative
AMD,I hope that the sub-branch only means no AI stuff and they will keep supporting the GPU for at least another 3 years I just bought a Rx 6750xt 2 years ago damn...,Neutral
AMD,GTX1080 got like 9 years of support. 6800xt got 5years? Sorry amd this is my last card. Now with leaked unofficial support FSR4 that amd just decided not to implement. Im out boys,Negative
AMD,"im not so sure about getting a 9000 series now, like are you kidding? 5 years of support only? 20 series of rtx is 7 years old and theyre still getting support and new tech, absolutely pathetic",Negative
AMD,"okay, i just tested and my HMD works, so does my [mini USB-C/Display doohickey](https://www.shophagibis.com/products/usb-c-docking-station-mini-pc-monitor), so VirtualLink is still working, which confuses me even more, so either they didn't disabled power delivery or they meant a complete different thing.  u/TsukikoChan for now seems that they haven't left us in the dark.  https://i.redd.it/hv1qy5xuyayf1.gif",Neutral
AMD,"Yeah...I have an RX 6950XT, thankfully did not pay anywhere near full price for it. If they're actually gutting driver support for a five year old architecture, one that is powering the current generation of consoles no less? My next card will not be AMD.   I would've begrudgingly accepted it had they done this in 2027-28. Wouldn't have been happy, but could've lived with it. This is just an atrociously short time frame, especially given how in recent UE5 games the RX 6950XT hangs around the RTX 5070 tier without heavy ray tracing.",Negative
AMD,"Very bad behaviour, now i'll think twice before buying a card that eventually will be dropped in a couple of years. Meanwile nvidia relased dlss4 even on 2018 hardware, at least this happened before i bought the 9070xt, i'd rather pay 100€ more for the competitor insted of buying a card that will eventually be abbandoned in less than 3 years.",Negative
AMD,"This is some mayor BS. My 6900xt is 3 years old and no longer supported. My next card will be Nvidia for sure. Thanks Amd, you just lost a customer.",Negative
AMD,y tho,Neutral
AMD,"I foresee the same thing happening in about two years, when they launch UDNA. Due to the complete change in architecture, it's possible they will stop supporting RDNA.",Neutral
AMD,My RX 6900 XT is more powerful than 80% of RDNA 3 cards…why no support!?,Negative
AMD,That's really sad that RDNA2 has moved to maintenance mode 5 years after being released.  My RX6800 is still a 1440p beast,Negative
AMD,I’m an old AMD fanboy but man it gets hard to stay one when they do this repeatedly of removing support and minimizing function and features of old cards over time. This seems to be a common theme of AMD.,Negative
AMD,Crap I was going to buy a 7900xt reference for my psvr2!!!,Negative
AMD,"I will upgrade my 6600 XT soon but this support drop makes me want to go for 5070 TI now, 9070 XT was for sure the choice, not anymore.",Neutral
AMD,"Thank you for making my move from my 6800XT (that could have gone down as one of the greatest cards if you supported it with FSR4) towards Nvidia upgrade so easy.  I hope your expectations for people to upgrade every other generation with planned obsolescence to completely bite you back in the ass.   You will NEVER see my money again, that is certain.  P.S. Fire the clown that came up with this strategy. Good luck.",Positive
AMD,I just bought the RX 6800XT and it's already legacy? DAMN I'm going to throw it out the window,Negative
AMD,AMD just lost sooo many customers over dropping 6000 series support this soon... Huge mistake.,Negative
AMD,"RDNA2 being killed off so fast just sucks. I wanted to buy another RX 6800 or a 7900xt for my LLM + gaming rig, but i dont think i'll be doing that. Intel B50 is probably the move then.",Negative
AMD,What a disgrace! Dropping driver support on not even 5 year old GPUs! My next GPU won’t be AMD anymore.,Negative
AMD,With my RX 6950 XT having only 3 years of support I guess my next GPU will be one from Nvidia,Neutral
AMD,"So... my very first AMD discrete card... is not supported after I have just 2 years after I bought it... just ""legacy drivers"" Next time I will buy either an intel card, or one of those chinese brands like Huawei, or even nVidia.  This is scummy and abusive.  Bad on you AMD, this are the last build and laptop I will ever buy from you.",Negative
AMD,I have a portable monitor that is powered by the usb-c port. Does this mean it will no longer work?,Neutral
AMD,"Wow, this feels like a slap in the face.  Knew it would happen eventually but not quite yet.",Negative
AMD,RDNA2 user here to pile up this is absolute bullshit.,Negative
AMD,"Yep, never again, buying an AMD GPU was my biggest mistake when buying my latest PC",Negative
AMD,"To be honest if I’ll need to go to a new card, I’ll most likely go with team green simply because I need to learn a hard lesson at this point about AMD.",Neutral
AMD,"Whaaat, my rx 7900 doesn't have a usb, omg that feature would have been so nice to have... Too bad they disable it anyway",Negative
AMD,"Joke's on them, mine doesn't have USB-C power.",Negative
AMD,"The new driver 25.10.2 For RX 6800 XT (Weight 912mb) dont work, say error, not compatible !",Negative
AMD,Wait so the drivers not working yesterday on RX 6000 series was not a bug? Wtf... I got my card like 2 years ago and it's already going to become obsolete??? xDDD Even my old rtx 2060 is getting updates still,Negative
AMD,"AMD dropping support for their GPUs much faster than Nvidia is just one of the reasons that explain their tiny market share. So much for ""FineWine"".",Negative
AMD,Why would you even need that,Negative
AMD,I hate it when all my shit becomes obsolete. I have to do a complete new build now to modernize,Negative
AMD,"The article looks biased written.   It basically says AMD bad, Nvidia good.   However, disabling a feature is not cool",Negative
AMD,"No reason to do this, this is a gigantic anti consumer move, coupled with cutting support for rdna 2 so soon, meanwhile maxwell is only just getting cut after 11 years by nvidia ? Amd are basically giving double middle fingers to gamers.",Negative
AMD,Lol I wish every card had a usb c port and they go and disable it? 🤦‍♂️,Neutral
AMD,"This is so sad, and frankly baffling given that the consoles all run RDNA2 so games are basically being built with it as the standard!  On top of that it was the last generation where Radeon was actually competitive (better by some metrics) than GeForce and they've decided that needs memoryholing?",Negative
AMD,i need more intel arc gpus,Neutral
AMD,Fake news but that never stops any of the garbage clickbait circus of websites who will never correct the headline because it will drive them views.   And predictably all the trolls come out from under their rocks to word vomit more fake outrage based on their uninformed ignorance.  Hope karma pays all of them back with bankruptcy,Negative
AMD,"I’d love to hear an update on this, my thought went exactly to this use when I saw they killed the power delivery for the usb c port",Neutral
AMD,"Hopefully not, apparently the headset's power label designates 7.2 watts or less, and that's less than 1.5 amps for even just USB-A.  Plugable makes a USB-C hub (3 device ports + 1 host port) called the TBT4-HUB3C which would probably resolve any potential power discrepancy (provides ~20w for attached devices), albeit at a fluctuating cost of $100-200 depending on the year.",Neutral
AMD,I hope not. I've got an AMD card specifically for VR.,Neutral
AMD,Please keep us updated,Neutral
AMD,"I wish monitors and desktop GPUs would embrace USB-C for video already.  It's not like it's a hypothetical new standard, it's been on every laptop for I think a decade now.",Neutral
AMD,"Might be, but AMD in their patch notes clearly clarified which version of the drivers customers can stay on if they want to keep the  feature so thats probably enough legally, as I don't know if they are required to provide driver updates that keep all the features the same.",Neutral
AMD,"Could be purchase / sale under false Facts and a few other things atleast in germany , if you own one contact your customer protection agency to ask.",Neutral
AMD,They're cutting driver support for the 6000 series cards too. So much for the good guy underdog. I guess F people who bought a 6800xt a couple years ago. Nvidia is still supporting cards almost 10 years out.   Nvidia it is from now on.,Negative
AMD,Definitely not..,Neutral
AMD,"they already did and accidentally released it, I tried it on steam deck and it works, when static - even 347p upscaled to 800p looks almost like native(although somewhat softer) but the problem is it's too heavy for steam deck.  on desktop RDNA 2 GPUs it must be running fine I'd assume, all you need is fakenvapi+optiscaler+leaked fsr 4 int8 dlls, throw them into the dir with your dlls-supporting game and set optiscaler to FSR 4 and the game to dlss  if setting via ini file(not the insert button menu) then set to fsr31 and then set the fsr4override to true in fsr3 section",Neutral
AMD,We'll see. i still think they will.,Neutral
AMD,Very stupid move imo,Negative
AMD,Except they already did... The code is there already,Neutral
AMD,"Meanwhile their primary Linux drivers see work to fully support GCN 1.0/1.1, so an architecture from 2012.  How is such a contrast possible? I guess since it’s an open driver and it’s not AMD working on that xD",Neutral
AMD,6950xt came out in May 2022.. man what the fuck.,Negative
AMD,I just bought the RX 6800XT and it's already legacy? lol... :(,Neutral
AMD,I'll buy green from now on. Have a 6800xt so I guess F me even though it's still pretty fast.,Neutral
AMD,This has been my biggest reservation about buying AMD based graphics cards. Historically they have been pretty quick about reducing or ending driver support. nVidia tends to support their cards for a ridiculously long time. My 8800GT and 980ti felt like they got drivers forever.,Negative
AMD,"This is one reason I stopped using AMD GPU's, after they dropped support for my HD 6950 only 5 years after release. I see they're doing it again with RNDA2. And now I'd say it's worse since GPU advancements are a lot slower, and a 5 years old GPU is more than capable to play new releases.",Negative
AMD,"ye while iam happy with the amd driver , i will pay for my next gpu rather 100 more than to go amd again for the gpu.  this is WAY too fast.     i was eying with a 9070XT but i guess not anymore.",Negative
AMD,My 6900xt cost about $2300 NZD about 4 years ago. lol.,Neutral
AMD,"Literally, most of RX 6000 cards are still go to in low income countries and are still capable.  Seriously my next GPU is more then likely NVIDIA or Intel, if I am gonna get ass fucked at least let it be lubed.",Negative
AMD,"Got identical combo as you, 5800X3D and a 6950XT. Next GPU definitely won't be Radeon with this BS going on.",Negative
AMD,Being pushed to a sub branch does not mean loosing support. There may be genuine considerations based on how RDNA 3+ are designed which would benefit from the split in drivers.,Neutral
AMD,"The dumbest thing is that AMD gives away that drivers for older cards go into maintenance mode, while nvidia has everything unified so people think game ready drivers improve performance for old cards.",Negative
AMD,yep was my first amd gpu in like 15 years ( had before that also some amd and even ATI cards ) will be my last AMD gpu ill shell again rather 50-100 more out for nvidia and dont get shafted after 5 years.,Neutral
AMD,I was using a Dual DVI-D adapter to USB C when I first bought the card to connect an old 27” Korean LCD panel.  I bought the reference specifically for this reason. So while I don’t currently use USB C I was glad I had it for future needs.  This is very frustrating.,Negative
AMD,Catch 22…,Neutral
AMD,What is the VR issue?,Neutral
AMD,FSR 4 is dependant on hardware in the newer cards. So yes there is a reason.,Neutral
AMD,"Just wait for backlash and see how AMD responds, If they arent gonna fix this go for Nvidia.",Negative
AMD,Did they even say why they did this? I don’t know why you’d disable it if it’s working.,Negative
AMD,Thats part of being the underdog multi billion dollar company can't afford to support there products for years like the competition.,Negative
AMD,What makes you think so? It's actually opposite.,Neutral
AMD,Dude... Almost 6 years since it was released. It's pretty old already lol.,Neutral
AMD,It’s a little ridiculous considering the 6800 XT is faster than most 4000 series cards like the 4070 but let’s put it  out to pasture. Still plenty of power.,Neutral
AMD,"I have a 980ti.    What driver support is that card actually for real getting, other than being ""included"" in the driver package? No bug fixes, no additional performance improvements or game support.    Frankly I don't understand why AMD doesn't just let their driver packages get big. It avoids silly self owns like this.    A 1gb download nowadays isn't really that big of a deal for plenty of people.",Negative
AMD,"Part of it is that, while weaker, Turing has most of the hardware features of the 30, 40, and 50 series cards. RDNA4 is the first AMD generation to be more or less on feature parity with Turing. AMD dragging their feet for so long on ML and RT acceleration is going to make RDNA 1-3 age rather poorly in terms of feature backporting.",Neutral
AMD,Degraded drivers??,Neutral
AMD,It really is just like with Apple and Android. Apple provides system updates for phones that are almost a decade old at this point,Neutral
AMD,Degraded drivers? My 5700XT is still getting all the drivers. I've no complaints at all.,Neutral
AMD,"Who said that the GTX 980 isn't considered legacy, it might be that Nvidia didn't have a public branch naming.  RDNA 1 and 2 are still supported, where did they say they won't support it?  It's just a different branch that is all.",Neutral
AMD,"Hm. Well, good to know it's all intact",Positive
AMD,Can you please tell me what does this means? I also bought a 6750xt less than 2 years ago. We won’t get new drivers? No support for new games?  This is unbelievable,Negative
AMD,"I bought 9060 this year and it doesnt scare me, but for someone buying in two years or three years... damn... Well I bought becouse features measured to nvidia and price was more appealing to me by far, But I guess I better pay more fo longevity... the price now makes sense lol, nvidia still king",Positive
AMD,"I've said that yesterday already. Power Delivery seems to work, they must have meant something else. But people just assume things and no one tests it. Altough most of the people complaining also claim to have an affected card. So i don#t get why they don't test it and instead ask and spread misinformation.",Negative
AMD,"Thank you for checking ❤️ that's good to hear, we finally have a good drivet then with the refresh fix and psvr2 works. It's probably one of the PD profiles they got rid of.",Positive
AMD,"abandoned after less than 3 years from its exit of the discrete market and still on the market on APUs relased in 2025, so bad.",Negative
AMD,I mean I understand not having fsr 4 on RDNA 2 or 1 but man the driver should be supported,Neutral
AMD,"no shit I bought my 6950xt literally like 3 years ago to the date, i bought it early november 2022, and now its no longer supported, i know that its not going to be useless but what is that bullshit, I remember before buying this card I had a GTX1060 6gb and that was still receiving updates by the time i upgraded and i bought that in 2016, i think i might need to go back to nvidia after all or just drop gaming as a whole lol",Negative
AMD,I got my 6800xt in 2023 definitely gonna consider a different option when the time comes,Neutral
AMD,same,Neutral
AMD,"It works, just tested with a external USB-c screen I used with my steam deck and it works fine.  They probably meant another feature being axed. Or haven’t disable it for this version contrary to what the patch notes suggest, only them can clarify",Positive
AMD,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
AMD,The thing I don’t understand is why would they even do this? It would take effort to actively remove a feature like that so there must be some motive behind it,Negative
AMD,"I won't be able to check it until next week - I haven't played vr in a while but was meaning to get back into it. Currently on 25.9.1, so i'll check if it works there, then i'll try 25.10.2 and see if it still works or not (jumping back if not). AMD really screwed over people with this release.",Negative
AMD,Can't you just... throw in a usb-c power injector? Something like this. [https://www.coolgear.com/product/compact-95w-usb-usb-3-2-gen-2-type-c-pd-injector-dfp-w-10gbps-speeds-for-seamless-legacy-host-to-pd-device-connectivity](https://www.coolgear.com/product/compact-95w-usb-usb-3-2-gen-2-type-c-pd-injector-dfp-w-10gbps-speeds-for-seamless-legacy-host-to-pd-device-connectivity),Neutral
AMD,"That's a pretty insane price for a basic usb hub. You can usually get a branded one all singing and dancing for like £20, or a Chinese one for £5.",Negative
AMD,Lmao who buys a Radeon for VR yikes,Neutral
AMD,"https://www.reddit.com/r/Amd/comments/1ojxjuq/comment/nm8xzdp   https://www.reddit.com/r/Amd/comments/1ojxjuq/comment/nm8ze1z   Two peeps already tested other HMDs, I'll test my psvr2 when I'm able at the start of the week",Neutral
AMD,"Eh, I don’t think someone could sell you a car with heated seats that they disabled through a software update and be legally okay by saying “just don’t update”. I’m certainly not a lawyer but that just seems litigation worthy.",Negative
AMD,Praying the Rdn.id modded driver dev will make some bypass for you guys 🙏,Neutral
AMD,"It’s actually crazy, AMD literally had the “fine wine” reputation but now their “wine” ages to 4 years and gets promptly “shattered”? What a shame, they never fail to shit the bed when Nvidia gives them the absolute lowest bar to do better than.",Negative
AMD,"I think the funniest situation are those budget builder in China who got their 6750 GRE about 2 years ago.  I mean, yeah it is discounted GPU sure, but 2 years of driver support is still quite funny.",Neutral
AMD,I got my 6800 non-XT a couple years ago 😬,Neutral
AMD,"He means officially support, as in you have a toggle in the driver and it does all the replacing for you like NVIDIA does with its DLSS swapping.",Neutral
AMD,"It's better than FSR 3.1 on my 6700 xt, albeit at a performance cost when comparing the same input resolution. FSR 4 performance is like FSR 3 quality in terms of fps for me, maybe a bit better FPS wise. Although image quality wise it's still a lot better than FSR 3 so I still prefer is.   The thing is that I dont want to swap DLL just to get the best experience for my GPU lol. I also dont want to risk trying it with games that have anticheat.",Positive
AMD,What makes you think they will. I hope you're right,Neutral
AMD,Linux not effected by the driver downgrade then?,Neutral
AMD,And so did the 6750 XT and 6650 XT (6x50 refresh). 3.5 years of total support is ***embarrassing*** as fuck.  [https://www.techpowerup.com/294707/amd-launches-radeon-rx-6950-xt-rx-6750-xt-and-rx-6650-xt-new-game-bundle](https://www.techpowerup.com/294707/amd-launches-radeon-rx-6950-xt-rx-6750-xt-and-rx-6650-xt-new-game-bundle)  And it's only 3.5 years ***if*** you bought it launch.,Neutral
AMD,"Apparently, it seems so. I might finally have to dual boot linux to actually get longer driver support. lol",Neutral
AMD,"> I'll buy green from now on  It seems so.  A couple years back, I swapped my RTX 3070 for a 6800 XT and up until this point, I had zero regrets.  Been fully satisfied with the card.  Was eyeballing possibly upgrading to 9070 XT over the the holidays.  But following this news, I'm going to have to just begrudgingly pay the additional premium to swap back to NVIDIA, so at least I know I can get as many years as I want out of the card without them pulling the rug out from under me by EOL'ing driver updates for perfectly functioning cards.    AMD, why do you keep doing this to yourselves??",Neutral
AMD,funny thing is there's people saying they didn't expect this from amd lmao,Negative
AMD,"Same man (about eyeing the 9070 xt), this 6700XT I bought a few years ago was my first AMD gpu but if support is dropped this quickly I may not go AMD for my next purchase. I finally thought there was hope for us with int8 fsr4 and now this happens, making me think it's not going to be an official release ever, which really sucks because some games will just ban you for swapping DLLs",Negative
AMD,The RX 6750 GRE was released in 2023! The card barely lasted 2 years.,Neutral
AMD,"You can see nvidia version values too, lol.",Neutral
AMD,>so people think game ready drivers improve performance for old cards.  ***All*** games and ***all*** GPUs benefit from bugfixes. Nothing has changed in game development that suddenly makes older GPUs immune to new-game problems.  They just simply lose support over time.,Neutral
AMD,"It's not just performance, sometimes it's visual issues that get fixed with future driver updates. Dropping support could also mean dropping support for new future technology that could run on the cards otherwise.  Like Transformer DLSS upscaling on RTX 20 series. Which RTX 20 did get, despite being ~~6 years old.",Negative
AMD,No they just see the catch really clearly lol,Neutral
AMD,WTF have you been for 6+ months?,Negative
AMD,"No, as FSR4(int8) leaked, and proved it works well on older architectures, not as well as ""proper"" FSR4, but way better than RDNA 3.1. So there's literally no reason, the cat is out of the bag already.",Neutral
AMD,"No, also shilling does not work, you should know how big companies operate.",Neutral
AMD,">AMD hasn’t explained why it made this change, but since no RX 9000 cards include a USB-C port, it may have been removed to streamline the already large driver package.  >For comparison, NVIDIA’s drivers are around 850 MB and still support GPUs back to the GTX 700 series (Maxwell). NVIDIA still continues to support its similar USB-C standard, VirtualLink, which is no longer added to cards after the RTX 20 series. The Virtual Link standard was effectively cancelled around 2020.   They didn't give a reason, but probably to streamline the drivers more.",Neutral
AMD,That's just bullcrap honestly.,Negative
AMD,"LOL, still pretending AMD is some underdog and can't support an architecture for two more years. This isn't 2016 anymore, they're swimming in money and RDNA development basically gets bankrolled by Valve, Sony and Microsoft. What are you even talking about?",Negative
AMD,"Yeah people always go ""AMD finewine!"" and then ignore if you actually have to hold onto a GPU for a really long time nvidia will give you *years* more support.  I still remember when my friend had to go out of his way to find the single beta crimson driver that would work on the HD6000 series that was required to make games *that came out in 2012* not crash.  And then when that died and he had to go back to his 5950 which didn't even have the crimson beta driver we just had to give up on all of those games until I bought a new GPU and could give him my gtx 670.  meanwhile nvidia kept support for GTX 400 cards for several years past that point.",Neutral
AMD,I fail to see why that's my problem,Negative
AMD,"6 years isnt old , specially if you consider the mid to top end gpus still running games to date at 1440p high to max settings at 90fps +     after your logic is and what you say is any gpu beyond 5 years is EOL and should be replaced if you bought AMD which is terrible.",Negative
AMD,"N22 and N23 launched in 2021. N32 didn't come out until near the end of 2022.  Even then, Navi 22 was discounted and sold far into 2023. Navi 23 (6600/6650/XT) was even sold far into 2024 in the sub 200 dollar market.",Neutral
AMD,"Steam Deck uses RDNA 2, the newly released ROG Xbox Ally uses RDNA 2.  And they are currently releasing rebrands of Zen2/3 APUs using RDNA 2. Dropping support for a product that is still seeing new releases really sucks for the customer.  https://www.reddit.com/r/Amd/comments/1oh9yia/amd_again_reshuffles_mobile_lineup_with_ryzen_10/",Negative
AMD,"Of course it's not getting any performance updates specific to your GPU, but the general fixes that are not architecture specific still apply to your GPU.",Neutral
AMD,> What driver support is that card actually for real getting  More support than my Vega based GPUs ever got for longer if nothing else.,Positive
AMD,"Radeon group is notorious for massive fumbles in PR, and ass backwards decisions like this. They somehow manage to clutch defeat from the jaws of victory every time. I have no clue why they would do this either, a 1.5GB driver package seems insignificant to me. I would say someone is getting fired over this, but at this point I don't even know it might even be intentional.",Negative
AMD,Sad but true. Turing was way ahead of its time.,Negative
AMD,I think it means less strict standards for driver quality or reliability,Neutral
AMD,"Apple guarantees 5 years of OS updates. Samsung and Google guarantee 7 years.   Apple usually gets to 7 as well, despite a lower guarantee. Just don't buy shitty Chinese androids.",Neutral
AMD,"Yeah, the iPhone 6S received iOS 15.8.5 security update in September and that phone was released in 2015 while my OnePlus 8 Pro released in 2020 got its final update last year. It's actually insane.",Positive
AMD,That's no longer the case. Most Android OEMs have caught up to the importance of long term software support.,Neutral
AMD,"I mean... Android now promise longer support than Apple (which started promising anything just because Android phones got the ""promise"").   iOS have tied system apps to system, which mean even small bugs in apps like phone dialer can be system breaking and they need full system update to be fixed. On Android there its much less likely for system app bug to cause such big system wide problem and if such bug exist simple app update is enough to get it fixed.",Neutral
AMD,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
AMD,Only security patch and maybe driver fix but no more optimization for newer games,Negative
AMD,"I hadn't got around to checking myself, won't be near it until sunday/monday. I'm thankful you two checked and confirmed, thank you ❤️",Positive
AMD,Oh it gets worse for some people. The RX 6750 GRE launched in the October of 2023. A literal two year old dGPU relegated to security and bug fixes only.,Negative
AMD,"We should not ""understand"" the lack of FSR4 on RDNA2 cards though lol.  That was a clown move as well by AMD, and this new shit is just icing on the cake.  I was planning to buy 9070XT for my girlfriend, but hello to 5070Ti now, absolute donkeys.",Negative
AMD,"And it seems AMD removed option to use modded FSR4 in RDN2 in this latest driver. All this hype about AI cores, machine learning, Tops , Flops whatever... and 6900xt plays modded FRS4 perfectly (at 10% more fps cost tho but is is worth it). Now we are not going to get jack shit officially.",Negative
AMD,"This is the same driver where they apparently fixed the framerate instability in VR. Maybe the problem only came up when was using USB PD and USB data on the same port?  So they just disabled the user from being able to do that.  That'd still be dumb though. Plus, a lot of ITX builds use that port to hook up portable monitors.",Negative
AMD,"I’m not sure either, unless it was causing some sort of stability issue I truly can’t understand why you’d disable such a random feature.",Negative
AMD,Did you notice that AMD fixed the refresh rate bug in the patch notes. Haven't tested my Index yet but it was finally marked as fixed. I've been stuck on 24.12.1 until now.,Neutral
AMD,"The thing is this was an advertised feature they took away, and just doesn’t make sense. There are work arounds but that just shouldn’t be necessary.",Negative
AMD,"It’s much more than a basic hub. It actually deals with the DisplayPort signals across the USB-C ports, whereas most hubs only use their single USB-C port for power passthrough.  In this particular case it means you could plug in up to a 20W USB-C display.  I actually use it as a ""docking station"", hooked up to an HDMI adapter and one of those ""branded"" USB-C adapter (3 USB-3 ports, a card reader, and I think a second HDMI port). Works just fine on my Macbook, iPad, Steam Deck, pretty much everything but my Switch because Nintendo's gonna Nintendo.",Positive
AMD,BMW attempted something similar if you weren’t aware. They were trying to lock heated seats among other features on a subscription based model. Last I heard they rolled it back due to customer backlash.,Neutral
AMD,"The ""fine wine"" reputation was more of a meme than anything else. Realistically, you do not want your chips to undergo a ""fine wine"" process in the first place. That only means that your launch drivers were absolutely crippling the shit out of your GPU's performance, which reflects on the driver team.",Negative
AMD,I'd be silly if they don't considering it already works on linux ootb,Neutral
AMD,Because it is already running and tested,Neutral
AMD,"Nope. Linux uses their own AMD drivers, which are fully open and community maintained. Just works without you having to do *anything* and tends to perform better than the AMD Windows drivers... Or the old AMD-made linux drivers that AMD dropped support for because the Open ones were better in every way.  You do have a couple tradeoffs though... You don't get support for new things quite as immediately, and if you're on a slower moving distro, you might not get it for a while. Stuff like FSR 4 took a couple extra months to work on Linux, for instance. Also, because the HDMI forum is... Well, I don't have nice things to say about them, but they blocked HDMI 2.1 from working on Linux, so on linux AMD card HDMI ports are limited to HDMI 2.0 and don't get VRR support among other things, and AMD is legally not allowed to do anything to fix that.",Neutral
AMD,Rip my 6650XT,Neutral
AMD,"Yeah, highly considering selling my 6750 XT for a 5070 Super when it comes out",Neutral
AMD,Hey sometimes you go through life and you don't realise the correct way to say a phrase until some smartarse points it out online. Just goes to show you can't take life for granite.,Negative
AMD,Did something happen? Have I missed something?,Neutral
AMD,"There's little reason to use it though. On linux, the emulated FP8 version is just as fast as the int8",Neutral
AMD,Maybe it works if integrated in a game.   Maybe a new driver isn't needed then.,Neutral
AMD,I am not shilling though? I literally use Nvidia GPU now.,Neutral
AMD,What confuses me is why they recommend a driver from March if a user needs the feature?  The driver size is now 1.5gb?! It ballooned in size. 25.9.2 is less than 900mb.,Negative
AMD,In my world it's so that we get rdna4 lol,Neutral
AMD,"Yeah reality is ""bullcrap"" because some people don't want to believe in it.  Nice way of looking at the world.",Negative
AMD,"Agreed, but it's not just that. I'm not paying over 1k every two years or so because AMD decide to EOL my fucking card.   On the other hand, they can't even guarantee a set of drivers every month for their latest hardware.",Negative
AMD,"Majority of wine isn't suitable for aging, so yeah, I think it fits the description perfectly, heh.",Neutral
AMD,You’re talking about GPUs that’re literally 15 years old. Go a couple generations ahead to Tahiti and Hawaii and you’ll find that AMD did *much* more for those products than Nvidia has ever did for Kepler.,Neutral
AMD,It's 2 generations behind. It doesn't matter if the performance is still good for the current workloads.,Positive
AMD,"nope, its a full branch",Neutral
AMD,"Like they were good quality to begin with  I won't forget them breaking RDNA 2 cards two times this year in span of 3 months, loved getting BSOD when alt tabbing",Positive
AMD,And who quantifies that the poster is using an intel / Nvidia setup so he doesn't even have first hand experience running a RDNA 2 GPU.  AMD just split the driver branch there is zero official mention from them about RDNA 1 and 2 going into legacy status.,Negative
AMD,Samsung kills tablets in 8 months though,Neutral
AMD,The problem there is essentially Apple did it for years already so everyone knows their word is good. Samsung only recently changed their policies so word hasn’t spread. Google benefits from being early to the long support time party among android manufacturers.,Neutral
AMD,"yet all userland jailbreaks/exploits still work on 15.8.5, Safari is outdated too.",Negative
AMD,"iOS have tied system apps to system, which mean even small bugs in apps like phone dialer can be system breaking and they need full system update to be fixed. On Android there its much less likely for system app bug to cause such big system wide problem and if such bug exist simple app update is enough to get it fixed.",Negative
AMD,Rdna 2 don't have AI core and it run very slow with the int8 model,Neutral
AMD,"If i were to hazard a guess, one of the CVEs is probably to do with secure comms or usb standards and amd probably thought it easier to take off the feature rather than get it working under a cve. Another hazarding guess, it probably doesn't follow a standard they said they are, they got caught, and just got rid of it rather than address it.",Negative
AMD,It does make sense. AMD doesn't want you to use their cards for too long. They want you to upgrade every gen.,Neutral
AMD,"Oh, I see. You need that connectivity for the psvr2 to function, so a cheap usbc hub might not do, though passing usbc through a switch and converting to hdmi and so on are quite different things. While usbc to hdmi adaptors are really cheap to get a proper expansion hub is very expensive. I don't have psvr2 but it's not clear to me that that's what required if it's just to add power to the usbc port.",Neutral
AMD,at that point why not just buy the adapter for $60?,Neutral
AMD,"No, that’s different. If I fully disclosed that I’ve locked a physical function behind a payment and someone still buys the product then they’re the idiot. That’d be like if AMD charged $50 to access the USB-C functionality in the first place, but this is the equivalent of BMW offering the heated seats initially and then stripping them via software later (effectively having sold a product with a function and then removed it after the sale).",Negative
AMD,Silly like dropping support for GPUs released under half a decade ago…I could see it at this point.,Negative
AMD,INT8 version 'leaked' and can be run on older AMD cards while providing much better visual fidelity with very little performance drop compared to FSR 3.,Positive
AMD,The RDNA 3/4 version of the driver is 900mb.  The combined version with RDNA 1 and 2 support is the 1.5GB one.,Neutral
AMD,The main driver download includes both branches and their separate installers.,Neutral
AMD,Adrenalin 25.10.2 (WHQL Recommended) is 902 MB.  Nvidia 581.57 is 896 MB.  These are very normal sizes for how many features they've added in recent years and how many thousands of games they support with custom code. They're really not growing that fast considering how fast flash storage has gotten cheap. It's like 50 MB/year or something?,Neutral
AMD,"Pardon my ignorance,   Does the driver just run on the CPU?  Or does some of it also run on the GPU?",Neutral
AMD,iam honestly checking for a 5070 or smth now instead of a 9070,Neutral
AMD,"Yeah good luck most people won't buy AMD regardless for a good reason   My 6750XT has been filled with issues, my RX 550 died 2 years in",Negative
AMD,Companies 1000x smaller can still support products longer.  Yes. It is bullcrap and not reality.,Neutral
AMD,"This is what the AMD Stans can't seem to understand. As someone who's used both Radeon and GeForce stuff, I can tell you... NVIDIA even when they remove support, they just get it right from the beginning. With GeForce you get 99% of the performance day 1. Rather than waiting months for the full performance of my card, NVIDIA gives it basically right away.  The whole FineWine argument makes no sense to me, these cards have a limited shelf life, if the first 6 months you're getting gimped performance, how is that a good thing? These cards are only relevant for maybe 24 months and their support is maybe 6 years of support if you're lucky with AMD and more like 8 years with NVIDIA. If I have to wait 4-6 months to get the full performance of my card, then any money you saved buying AMD is basically mute imo. The extra $50-100 is worth getting the full performance day 1 rather than waiting months for it. Especially if the issue is making the device completely unusable.   Then take into account black screen issues, game issues, multi-monitor power usage and inconsistent driver updates where like you said AMD may not ship a driver every month. I just never understood the FineWine thing... Now I will be fair and say NVIDIA's drivers honestly haven't been the best with Blackwell, but they've been just bad relative to how they were with Ampere, Pascal, Turing etc. They're still a lot better than AMD's worst driver moments the past couple of years and they're no where near Intel's driver problems for GFX. So NVIDIA is still the better choice in terms of driver support and I will give AMD credit, they held back RDNA4 to work on drivers more and it helped them a lot.  Either way, I'm tired of this duopoly and the GPU industry in general, too many people not willing to call out the BS from any of these companies and all of them are just screwing consumers. I honestly wished Intel was going to do better, but they've lagged for three generations now and they're looking like they'll abandon dGPU altogether.",Negative
AMD,I’ll gladly pay Nvidia $2000 for performance that laps everything else if it means I won’t need to worry about driver support being gone after a mere half decade. Nvidia JUST ended support for Pascal (as in they’re doing the same thing for a May 2016 GPU arch that AMD is now doing for a November 2020 arch).,Negative
AMD,"I'm also talking about stuff that happened almost 10 years ago.  But it is part of the reasons my friends and I have avoided AMD GPUs.  We had consistently had bad experiences with it, including driver related problems like this.  And here we are literally talking about AMD removing a feature from a product that released less than 3 years ago that nvidia is still supporting on products it released 7 years ago, and hasn't even included on anything newer than that.",Negative
AMD,"Vega/Radeon VII, RDNA1, and Vega APUs staring at you like Samuel L Jackson rn.",Neutral
AMD,"2 gens doesn't mean stuff , you could say the 7000 series are also 1 gen behind the 9000 series yet in most cases it's only a 5-10% difference.  These gpus are only 5 years old don't defend a multi billion dollar company it's not your friend dude.",Negative
AMD,typically when you split the driver branch its because you're putting one of them into maitenance,Neutral
AMD,I mean we know that android phones could run the newest android versions even with like 7-9 yo phones (rip custom roms 😭),Neutral
AMD,Very much the opposite. Apple used to degrade performance deliberately on only slightly older phones. Get sued for it.,Negative
AMD,Still better than fsr 3 by a lot. Fsr 4 peformance > fsr 3 quality,Positive
AMD,Nah that has been disproven.,Neutral
AMD,> amd probably thought it easier to take off the feature rather than get it working under a cve  You're probably right. That's exactly what Intel did with undervolting (laptop) CPUs a while back. It made me pretty upset given I couldn't rollback my BIOS far enough to get around it.,Negative
AMD,"Hopefully it's all a moot point and it works for OP anyway, due to needing less than 10 watts.  You're right though, I'm pretty sure there's inline power enhancement adapters for USB-C which would likely cost way less.",Positive
AMD,"Again, this is all on conjecture that's probably a moot point (7.2w should be easy even if PD is disabled), but probably because the adapter is hot garbage, at least so far as I've read/heard.  4K @ 120hz is a hard sell even under the best of circumstances, and being able to bypass any kind of active conversion in display/data pipes is definitely preferable.",Negative
AMD,"Interesting. Do you have some articles I could look at? I assume that FSR is powered by some kind of deep learning model and that this is a quantized version of that model.  I have not been using AMD for graphics for a little bit to be honest, so I don't really know what the latest developments are.",Neutral
AMD,Good. Punish these clowns with our wallets. I am upgrading my current 6800XT with the Nvidia 6xxx line-up and I threw my plans for AMD 10XXX cards in the garbage.,Negative
AMD,True words,Neutral
AMD,Again you are using performance to judge if it's old or not.  Nvidia gpus get full feature and driver optimization for 5-6 years on average. So amd is not doing anything crazy here.,Negative
AMD,It was when battery got down to 80% of capacity. they made that into a function,Neutral
AMD,Yep. I tried it for the first time on Outer Worlds 2 and it looks so much better I thought my brain was making shit up.,Positive
AMD,"Wait, that was the fix for plundervolt?!",Neutral
AMD,"It splits the signal, not conversion. Your GPU is doing the same thing over it's USB port.   They both maintain DP 1.4. One does it over USB-C DP alt and the other does it over a DP cable.   The VR2 adapter does what it should do, it's been reliable. The software was buggy in the beginning, but that wasn't exclusive to the adapter. What issues did you hear about?  The hub you linked requires thunderbolt host or USB4 for all the good features. It's an expensive path if you don't have a native port. Add in cards are pricey and didn't work well for me.  The [SUNIX UPA2015](https://www.sunix.com/en/product_detail.php?cid=1&kid=2&gid=11&pid=2146) is an option as well and cost ~$40",Neutral
AMD,"It's not really anything that is special magic be it DLSS, XeSS, or FSR4. It runs better on cards that have dedicated units for the precision/calculations being run. It's still just basically math and for this type stuff usually lower precision.",Neutral
AMD,"i have tested it myself and its absolutely a gamechanger, performance is not great (fsr 3.1.5 quality = fsr4int 8 perfomance) but image quality? man not even a competition fsr4 performance looks better than both xess ultra quality and fsr 3.1.5 quality.",Negative
AMD,r/steamdeckhq posted an article not too long ago. Here's the link:  [https://steamdeckhq.com/news/we-can-now-use-fsr-4-on-the-steam-deck-for-better-visuals-with-a-power-tradeoff/](https://steamdeckhq.com/news/we-can-now-use-fsr-4-on-the-steam-deck-for-better-visuals-with-a-power-tradeoff/),Neutral
AMD,"> I wouldn't be surprised to see NVidia going the same route as AMD to cut costs now that the precedent has been set. I'd wager they're keeping a very close eye on consumer discord to see if they can follow suite.  I'm not sure what do you mean by this. AMD cutting driver support soon is nothing new. Ass stated, HD 6000 cards only had driver support for less than 5 years. That happened 10 years ago and nVidia hasn't followed suite.",Neutral
AMD,You are right.  I'm using an old as hell driver for my nvidia GPU because otherwise I run into issues.  And it probably only works for me because I'm too poor to play new games now.  If I ever get the chance to buy another GPU it probably won't be nvidia after this disaster and what they're doing with AI and their friendliness with fascism.  I just wish there were any good options.,Negative
AMD,Again 5 years isn't old.,Neutral
AMD,"Yes, because if you ran the chips at a low enough power, it caused errors that could expose sensitive information.",Neutral
AMD,Yes I know how running AI models works. You don't need to explain. I am asking for something specifically on FSR4,Neutral
AMD,I’m surprised you so confidently said it *requires* hardware that RDNA 3/2 do not have when [this](https://www.techspot.com/article/3040-amd-fsr4-upscaling-older-radeon-gpu/) article (and many like it) blew up this very subreddit not even a month ago now. There are even articles about it running on the Steam Deck.,Neutral
AMD,"I didn't see those articles until I made that comment. Technically what I said isn't wrong. This is a different variant than the official FSR4 as the official FSR4 is FP8. This variant is Int8 and apparently has less performance. It's a different quantization. So someone has had to convert it to make it work, which is some what clever but it's not exactly the same thing.",Neutral
AMD,"I know this is about the 7500X3D, but product timeline like this one makes me believe the earliest we could see the 9600X3D, or even 9500X3D, would be 2028—heck, 2030 even because AM5 can continue AM4's absurdity.",Neutral
AMD,Affordable… 200 usd or more lol,Neutral
AMD,"They're in about 2 year life cycles, so late 2027 for the 9600x3D is my guess.",Neutral
AMD,$200 with performance that potentially beating out top end intel part. Seems like a good deal to me..,Positive
AMD,You cant afford 200 dollars?,Negative
AMD,I don't even know what is what anymore.   Maybe this is exactly what they want. Maybe they have such an overabundance of Zen 2 and 3 supply that they don't know what to do with them. Maybe this is their way to get rid of all that without them coming off as outdated.   It's purposefully confusing and misleading.   I don't like it.,Negative
AMD,"to make this more clear:     Ryzen 7 170 = Ryzen 7 7735HS     Ryzen 7 160 = Ryzen 7 7735U     Ryzen 5 150 = Ryzen 5 7535HS     Ryzen 5 130 = Ryzen 5 7535U     Ryzen 3 110 = Ryzen 3 7335U     Ryzen 5 40 = Ryzen 5 7520U     Ryzen 3 30 = Ryzen 3 7320U     Athlon Gold 20 = Athlon Gold 7220U     Athlon Silver 10 = Athlon Silver 7120U     Specifications are 100% the same. So, in this case, AMD ""just"" changed the names of it's 2023's mobile portfolio. Which, as well, was already a rebranding of older CPUs.     Source: [3DCenter.org](https://www.3dcenter.org/news/news-des-2526-oktober-2025)",Neutral
AMD,This is so brain dead. 😵,Negative
AMD,"The Ryzen 30 is wild.  I get that AMD shuffling the names every year (7000 & 8000 series in 2023-2024 where the 3rd digit indicated the architecture, 7520U was Zen 2 from 2019 -> Ryzen AI Pro Max 395+ bullshit where there's 3 words that mean absolutely nothing -> Ryzen 30, Ryzen 40, Ryzen 170, Ryzen 110) is \*\*meant\*\* to confuse consumers, but...  Well...  Not really much more to say, is there?",Negative
AMD,"AMD is a bunch of clowns, they just changed the stupid names to RYZEN AI, etc.  They're ridiculous.   They need to get rid of the amateurs who make these changes every 6 months, it's not possible.",Negative
AMD,"Uhh... That third digit wasn't always there. Looking at handheld PCs with AMD chips, I see 5800U, 6800U and then 7840U.",Neutral
AMD,Must’ve let some of that Rebrandeon marketing team over to the Ryzen team.,Neutral
AMD,"Someone there needs a nice slap. I still don't know the Ryzen AI variants until someone mentioned just yesterday that one of them, allegedly, has the GPU power of a 4070.",Negative
AMD,Wtf. If it wasn't already confusing enough,Negative
AMD,1. **Halo series:** AMD Ryzen AI Max 300 (Strix Halo) 2. **Premium series:** AMD Ryzen AI 9 300 (Strix Point) 3. **Advanced series:** AMD Ryzen AI 7/5 300 (Krackan Point) 4. **Mainstream series:** AMD Ryzen 200 (Hawk Point) 5. **Entry-level series:** AMD Ryzen 100 (Rembrandt) 6. **The PC should run:** AMD Ryzen 10 (Mendocino)  [ComputerBase.de explained it best.](https://www.computerbase.de/news/prozessoren/amd-folgt-intel-alte-ryzen-prozessoren-bekommen-nun-auch-neue-namen.94804/),Neutral
AMD,Sigh... the brainrot is getting worse.,Negative
AMD,It does feel AMD is a technically strong company with a terrible marketing department.,Negative
AMD,"I want to remind everyone that this naming bullshit is done by requests from laptop manufacturers, not on AMD's marketing whim. Case in point is that they don't engage it with desktop lineups.",Negative
AMD,This is honestly disgusting... Reducing brand value.,Negative
AMD,What a shame with AMD rebranding scheme,Negative
AMD,"When intel did that 8 years ago, everyone cursed them - included me.",Negative
AMD,both would be a novelty on the desktop,Neutral
AMD,"well, they wanna move probably to xxx naming scheme  they have strix point with 300, like the hx 370  they have strix halo with 300, like hx 395  10 will be zen 2  100 will be zen 3  200 will be zen 4  300 zen 5  wonder what they will do with zen 6 - probably 400 - but they are kinda all over the place with the naming",Neutral
AMD,"So it's OEMs who request rebrandings in order to sell old APUs as new to non-tech savvy consumers? TBH it makes sense, as desktop SKUs, which are sold directly to consumers, have a far more consistent naming.",Neutral
AMD,what are the chances 10000 series is ryzen ai desktop 100 or something,Neutral
AMD,whatever... better than whatever the fuck they were previously doing.  i guess they do this to prevent new old stock. much clearer what generation a chip is at least.  simple ask. stick with it. dont add bullshit. 2xx is Zen 4 3xx Zen 5 and so on. no jumping numbers or having two generations in one number. all i want is a sane product line up naming scheme that they stick to.,Negative
AMD,"Wow, laptop sales are this bad that they’re rebranding every two years so we just buy whatever.",Negative
AMD,Should be illegal to confuse consumers using rebrands. Should be extremely clear branding and models,Neutral
AMD,"Didn't expect there are some zen3+ leftover. mendocino chip should become  sempron  and rembrandt as athlon (for desktop).    Oh btw, where's the new zen3? /s",Neutral
AMD,"the naming scheme is god awful, as expected, and leaving aside the ""morality"" of selling 6 year old CPUs... Why do they do it?  Maybe they are reusing the """"old"""" TSMC 7""nm"" production, instead of manufacturing these CPUs on the latest node? I really don't get it.",Negative
AMD,"Throw those plans in the trash, change for the better for the love of silicon... There is no justification for this.",Negative
AMD,are they still producing zen 2 and zen 3+ APUs or what?,Neutral
AMD,Igpu updated for have latest driver support. Vega is really on dead way,Negative
AMD,this is largely driven by OEMs who want the cheapest chips while it not being obvious that it's previous generation silicon,Neutral
AMD,"While different naming schemes can be hard to make quick evaluations and ads an extra step, all it means is consumers should be educated on the basics and not be afraid to pull up a chart of the performance metrics that matter most to them, benchmarks, battery, etc. They should already be evaluating the totality of a notebook so looking at real-world testing and scores in relevant things they want to have the laptop or mini or desktop etc do is valid regardless.   It's hard for consumers to navigate, but the change in numbering scheme is enough for consumers to be aware its meant as a low power option.",Neutral
AMD,AMD's Marketing team should be reshuffled ... out the door,Neutral
AMD,"Eh, I would say this new system is less confusing than the old naming scheme. 0x0 is Zen 2 1xx is Zen 3 2xx is Zen 4 3xx is Zen 5  First digit is gen, next two bigger number = better.  A bit simple, but not bad.  If they keep this system up (using 4xx for Zen 6 and 5xx for Zen 7) then I think most people will be fine with this scheme.  The problem is that there is no trust that AMD will continue this naming scheme, especially since their actual customers for these chips (the large laptop OEMs) seem to want confusing naming on stuff.",Neutral
AMD,Oh my... Dual-core processors in 2026? Not even smartphones use dual-cores.   Ryzen 5 40 should be Athlon if AMD had learned anything.,Negative
AMD,shameless shit from AMD,Negative
AMD,It's like the person responsible for this has multiple personalities. One year it's clear and understandable and the other year they turn this shit.,Negative
AMD,"How is it confusing? You have two laptops, one Ryzen 170 and the other Ryzen 370. People will assume that the 370 is newer/better.",Neutral
AMD,"I'd assume AMD will phase out the 4-digit numbering scheme, and for a few years, the 2/3-digit scheme will make some sense before they start introducing a new one.",Neutral
AMD,"Honestly it works.   Im fairly tech savy (look im in a tech subreddit) but i couldnt, if my life depended on it, give you advice on amd laptop cpu skews.    And i think thats entirely their goal.",Negative
AMD,"Ah, you are missing something really important here - what the actual customers want.  The customer for laptop chips is not the end consumer like you or I, but large laptop OEMs .  And those companies want regular rebadged names for the cheaper older chips, as they believe newer names equals more sales.  Hard to blame AMD for making their direct customers happy.    Note how for stuff that has more direct consumer sales like desktop chips they don't do the rebadge game (granted the desktop chips do have some questionable naming decisions, but the same chips keep the same name), likewise for the professional stuff.",Neutral
AMD,"What's sad is confusing the customer seems to be the norm in tech. Try figuring out the naming convention of any laptop brand, you probably won't be able to. HP for example has hundreds of models that change by location, store, and just over time. Why do they do that? So you have no choice but to buy whatever they offer you.  It backfires with tech-savy consumers though. I bought a Ryzen system specifically because AMD had an understandable naming convention at the time and Intel didn't.",Negative
AMD,"There were SKUs that had a third and even a fourth digit in previous generations like the 5825U (mind you that is a Barcelo part, which was basically a small refresh of Cezanne). But AMD really popularized it and introduced it with the 7000 series to provide ""clarity"" as to what architecture something is. Why did they do that? Because in that same 5000 series lineup they had a mix of Zen2 and Zen3 parts. Like the 5300U which was Lucienne architecture, which was Zen2, but the 5800U was Zen3 Cezanne.  But it's just stupid because with the 6000 series, everything was Zen 3+, so whether you bought a 6800U or a 6600U it was all Zen3+. It didn't matter what you picked and tbh that's how they should have carried on, only make the new stuff the new generation naming.  In the 7000 series though they went back to the 5000 series naming sort of because they decided to mix old architectures again with the new stuff. For example, the 7735HS a Zen 3 chip and the 7745HX a Zen4 chip, that one third digit is supposed to notify you that you're buy Zen4, therefore it was as an indicator as to what Ryzen generation you were buying, but it just confused consumers even more because some people thought a 7640HS was worse than a 7735HS, but in single core the 7640HS beat the 7735HS and in multi-thread both were about even.  In the end, AMD made a shit solution to a shit solution, all of their own making and now they've come up with ANOTHER shit solution to try and ""fix it"".",Neutral
AMD,tangentially related but what exactly is nvidia going to do in 4 years when they need to release a card called the rtx 9070 when the rx 9070 already exists,Neutral
AMD,"That would be the Ryzen Ai 9 395, which yes, has a  iGPU that is pretty close to a mobile RTX 4070.",Neutral
AMD,"Yeah, that was another own goal from AMD marketing. They were claiming 4070 performance (may even have been desktop 4070, but not sure anymore so I won't pushed that) then when it came out it turned out to be somewhere between mobile 4060 and mobile 4070. Plus the devices with that SKU are so expensive that you are better off buying an actual gaming laptop with a 4060 or 4070.",Negative
AMD,"Mendocino should only exist on Chromebooks.    Kraken should be mainstream, and Strix-point premium. Let the old stuff die. But AMD doesn't want to evolve as a brand.",Negative
AMD,"AMD's been doing this sort of thing for years. But this is the worst variant of it. I mean the fact the 'Ryzen 7 170' even ""exists"" as a name is absolutely shameless. But even before that the 7735HS (which is the former name of the 170) was bad because many people thought it was Zen4 like the 7745HX was because of the 7000 series moniker and the fact it too is a Ryzen 7. Nobody will call out AMD really because they're the ""good guys"" and because they command market share in DIY now days.",Negative
AMD,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
AMD,There was info circulating about a year ago that AMD dropped Vega dGPU and iGPU driver support that turned up to be misinformation. Here are the list of the latest driver release:   Mendocino drivers: [https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-7000-series/amd-ryzen-5-7520u.html](https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-7000-series/amd-ryzen-5-7520u.html)  Rembrandt drivers: [https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-6000-series/amd-ryzen-7-6800u.html](https://www.amd.com/en/support/downloads/previous-drivers.html/processors/ryzen/ryzen-6000-series/amd-ryzen-7-6800u.html)  RX Vega 64: [https://www.amd.com/en/support/downloads/previous-drivers.html/graphics/radeon-rx/radeon-rx-vega-series/radeon-rx-vega-64.html](https://www.amd.com/en/support/downloads/previous-drivers.html/graphics/radeon-rx/radeon-rx-vega-series/radeon-rx-vega-64.html),Neutral
AMD,> it's previous generation silicon  Some of it is way worse than even that. There are _Zen 2_ chips (previous-previous-previous?) implicated here. That is some shamelessly re-badged garbage on AMD's part. These chips are barely even new enough to run Windows 11.,Negative
AMD,"No the real problem is they keep changing it every other generation. Maybe the new one makes more sense than the one before, but then again the one before that was even better. As a customer it is impossible for me to keep up, so the likelihood of me buying something expecting the latest CPUs and ending up with something from half a decade ago is disturbingly high. There is also no guarantee they won't change it again in a few years.",Negative
AMD,"For what it's worth, availability of the Athlon 7120U was **extremely** limited. I should know as I was trying to look for a laptop with that chip that didn't have 4GB of RAM with it, which effectively limited me to some really obscure HP slop box.   It doesn't make it excusable per se, but you'd have to stumble upon one of these completely at random or entirely intentionally to end up with one.",Negative
AMD,"The problem's that the moment these come onto the market isn't the moment every single older laptop gets wiped off the market, especially considering these are practically the same chips being re-released time and time again by AMD. Here's an example:  One customer looks at 5 budget laptops and their processors. #1 has a Ryzen 5 7520U, #2 has a Ryzen 3 7440U, #3 has a Ryzen 5 220, #4 has an older, discounted Ryzen 5 5600U, #5 has a Ryzen 5 40. Place your bets on the best one now.  Where do you even begin to compare these? Well, the Ryzen 5 7520U has a higher number than the Ryzen 3 7440U and the Ryzen 5 5600U, so it should be better, right? Wrong. Both the 7520U and 7440U are 4-cores, 8-threads, but the Ryzen 5 7520U has a 15W TDP, 4MB of L3 cache, is Zen 2-based (from 2019), and has a Radeon 610M iGPU. Meanwhile, the 7440U has a 28W TDP, 8MB of L3 cache, is Zen 4-based (3 years newer), and has a Radeon 740M iGPU. It's going to be an order of magnitude faster than the chip that's a higher number than it. As for the Ryzen 5 5600U? It's a 6-core, 12-thread Zen 3 chip with a 15W TDP, 16MB of L3 cache, with a much inferior Vega 7 iGPU.  Okay... so the Ryzen 7520U is a no-go, and the Ryzen 7440U and Ryzen 5600U are kind of up for debate if you have a dGPU in these laptops (though absolutely pick the 7440U if the iGPU is the only form of graphics onboard). Well what's up with the Ryzen 5 40? What generation does that fall under? If the third digit of the 7000 series was architecture, and the first digit of the 5000 series was architecture, is the Ryzen 5 40... Zen 4 based? Well no, you're not supposed to know that, you're supposed to just believe it's a new chip. Well we know it's a Zen 2 4-core 8-thread with 4MB of L3 cache from the article, but this potential customer doesn't. They'll find it out if they research it, but by this point they're probably getting sick of researching CPUs when the vast majority of people know barely anything more than an Intel i7 or Ryzen 7 being good, and an RTX graphics also being good. They most likely don't know anything about cores, architectures, TDP, or L3 cache, and probably just want a working laptop.  What about the, uh, Ryzen 5 220? That seems... probably a fair bit worse than the Ryzen 3 7440U and Ryzen 5 5600U going off the name and potentially being connected in naming scheme to the Ryzen 5 40, so it's worse, right? Nope. The Ryzen 5 220 is a 6-core, 12-thread, Zen 4-based, 28W chip with 16MB of L3 cache, and Radeon 740M graphics. A clear choice to pick that gets the best of both worlds with regards to the cores & cache of the older chip, and the newer architecture and iGPU of the newer chip.  Outside of this hypothetical scenario now, I want to add an honourable mention to compare to the Ryzen 5 220. I'd like to bring up the Ryzen AI 5 330, which is a Zen 5-based 4-core 8-thread 28W chip with 8MB of L3 cache, and Radeon 820M graphics. It's got less cores and less L3 cache than the Ryzen 5 220 and most likely will be worse in benchmarks [(and as a matter of fact, it is)](https://www.cpubenchmark.net/compare/6616vs6917/AMD-Ryzen-5-220-vs-AMD-Ryzen-AI-5-330), but despite this is a tier above (X20 v. X30) the 220, and very few will catch that the newer chip is worse than the older one. Even chips within this new naming scheme are inconsistent.  When these chips come out, the laptop market isn't going to have done away with the older chips for no good reason. [These chips are still out there](https://www.amazon.com/Lenovo-ThinkPad-Notebook-Keyboard-Professional/dp/B0FT3CR5VK), and customers looking for an AMD chip are gonna regret looking for one if they have to compare this kind of shit. This naming scheme's only a year old with the Ryzen AI 300 series, and the previous one (where the architecture was defined by the third digit) lasted two generations from 2023 to 2024. Pre-2023 laptops are perfectly usable for the average person and they're still going to be selling those laptops today, and god help anyone looking for an AMD laptop that has to deal with this naming BS. Intel's not much better with their 200V series but at least it looks like they're going to stay the course and keep using that naming scheme now that desktop and mobile are somewhat unified. AMD's just drunkenly driving off a cliff with their mobile naming scheme and landing on another cliff they'll drive off on in a year or two. Meanwhile, their desktop chips are very easy to understand in comparison. No random digit amount switching, no sneaky third digit signifying architectures, or any of that nonsense. The Ryzen 5 9600 is faster than the Ryzen 5 7600. Easy.",Neutral
AMD,"> How is it confusing?   I'm so confused as hell. There's also a 9000 series on laptops? Also, aren't the 300 series the ones with AI? But if they make 100 series, do the 100 series have AI too? But hey there's a 8945HS and that has AI. So does the 9000 series for laptop have AI too? What's the latest too, is it the 300 series or the 9000 series?  It's so disingenuous denying that their naming scheme isn't confusing.",Negative
AMD,"I didn't say that I don't like rebranding, I said that it's crap to do a rebrand every 6 months, everyone gets confused, it's not clear. However, I like AMD but they are very bad at it.  Like their rx9000 gpu range to stick to the rtx5000, except that the next generation would be rx10000 and people will be lost again 😅",Negative
AMD,By that logic everything nvidia does is extremely right.   That would be very unpopular on this sub I imagine.,Neutral
AMD,Bold to assume they will still produce gaming cards for us mere peasants,Neutral
AMD,"Most customers dgaf about ai and want a cheap laptop for office, and the better overview while browsing the Internet.  Kraken point is actually the mainstream offer with NPU.",Neutral
AMD,"Yes you received some security update but you don't have AFMF for example. I know it , I have this laptop",Neutral
AMD,TBF why would they not change it again with how everyone bitched about it?,Neutral
AMD,"The fact you need to decode a CPU name and need basically a whole manual for it is utterly ridiculous and it's 100% anti-consumer (made to be that way IMO). AMD continues their shit branding meme and this is why they make no headway in Laptop market, consumers find their branding confusing and when they try to learn it they just quit and buy Intel instead to avoid the headache and minutes to hours or research to just understand what architecture and generation they're buying. Mind you Intel aren't exactly saints either by renaming stuff like the i5-10400, but at least with the Core Ultra 278V or 275HX you know you're getting Intel's latest architectures.",Negative
AMD,"""Everyone gets confused""   That""s the whole idea, to confuse and mislead 😂",Negative
AMD,What do you mean? Nvidia is doing the best thing for it's customers.,Positive
AMD,"i have a great idea, instead of RTX, how about they focus on AI! now they can call their card the ATX 9070 ah shit nvm",Positive
AMD,"Zen2/Zen3 + Vega-based laptops are easily found for under $500, so this already exists. And they are faster octa-cores and more efficient than Mendocino.",Positive
AMD,When did AMD advertise or promise AFMF for Vega?,Neutral
AMD,"Have u read the tech specs for these CPUs. They are new configs with old tech. So it absolutely makes sense to have new names for them.  My customers who are interested in the hardware are able to Google the difference, and the others need help anyway.",Neutral
AMD,"PS: For 90%, it is done with the numbers 3, 5, 7 and 9.  It is for Intel and amd even the same meaning, which makes it actually really easy.",Positive
AMD,The ASUS Astral ROG ATX 9090 Housefire Edition,Neutral
AMD,"Correct me if i'm wrong, but the new 100 series has ddr5 support and a rdna3 igpu. This would imo be a real good mix between cpu and Gpu performance for the entry class laptop.  They should definitely cost around 400 bucks. If they are above 500 bucks, it's a rip-off.",Positive
AMD,"Indeed, I need to be more specific on my case. I have Asus G513AY with dgpu rx6800m (rdna2) and CPU 5900hx with igpu vega.   This laptop design is strange, because you have limitation due of igpu vega. If you want afmf from rdna2 you need to use only dgpu with a external monitor. It's why i like AMD rebrand zen 2 but with new igpu and not vega.",Neutral
AMD,"You can judge a CPU based on if it's a Ryzen 3, Ryzen 5, or Ryzen 7, but that's not really a good idea beyond a generalization. A Ryzen 7 1800X is much worse than a Ryzen 5 7600. An Intel i7-870 is much worse than an Intel i3-12100.  Going past the obvious, a Ryzen 9 9900X is worse for gaming than a Ryzen 7 9800X3D, and Ryzen 7 5700 is worse than a Ryzen 5 5600 (because the 5700 is a special chip - whereas usually chips like the 5600 are based off lower-clocked 5600Xs under the same architecture, the 5700 is based off the 5700G with PCIe 3.0 and much lower L3 cache. This continues with mobile hardware, from my comment above, with CPUs like the Ryzen 3 7440U and the Ryzen 5 7520U. One is two architectures newer, with double the L3 cache, double the TDP, with higher clocks, and a better iGPU. You'd think it to be the Ryzen 5 of the same ""generation"" (7000U) but it's not.  Desktop CPUs are easier (most of the time, there's still chips like the Ryzen 7 5700 though) but what I was saying up above is that AMD is making their laptop CPU naming schemes hard by switching them up every few generations, and it's almost certainly intentional to mislead consumers.",Negative
AMD,still using the “definitely not just 12vhpwr” connector of course,Neutral
AMD,Hasn't that always been the case? Intel came up with NUC for mini pcs way back in 2013.,Neutral
AMD,Smart choice moving to AMD chips for these.,Neutral
AMD,It's an Intel name like ultra book,Neutral
AMD,Minisforum sells an itx mobo with the 7945HX3D and 16x PCIE 5.0 slot for $500. Thats a really cool concept that I hope they duplicate with this chip. IMO its pretty ideal for an always on home server. I have an old home server with a 3950X and the 7945HX is about 30% faster yet consumes less than half the power. This will probably consume the same power and be 10% faster still.,Positive
AMD,Nice!    It is too bad Intel never had the chips to realize what most of us wanted out of a NUC.,Negative
AMD,And of course it's limited to just the 5070 configuration while the Ultra 9 gets the 5070ti and 5080,Neutral
AMD,"intel stopped producing nuc systems in 2023, and passed the rights for nuc to asus",Neutral
AMD,They use both intel and AMD for these. More intel models actually,Neutral
AMD,An AMD chip?,Neutral
AMD,Are any of the OEMs making these with 2x8 pin power connectors?,Neutral
AMD,Btw it has about 30% slower bandwidth than the rx 7900 XT which are starting to have some age and ship with 20gb memory,Negative
AMD,"Edit: I wrote something completely wrong.    I thought these were out of stock and they are not, at least at the time of editing this post.   I'm removing what I wrote earlier because of how incorrect it was.",Negative
AMD,So who's going to buy one to play games on it?  haha,Neutral
AMD,Anyone know if there's a significant difference between the creator version and the xfx version? Is it by chance slimmer?,Neutral
AMD,I wanna see benchmarks.  Make sure the AI aspect of it doesn't detract from the gaming aspect.,Neutral
AMD,"Wait, is this like a 7950 XTX, or like, a 7900 XTX XT?  What's the relative performance?",Neutral
AMD,"I don't understand why choose this card instead of 7900 XTX, for the money. Even if you need those extra 8GB memory, but for almost double the price?",Negative
AMD,Currently no.,Neutral
AMD,It’s also not a gaming card,Neutral
AMD,"I got 110-120 TPS on my RX 7900 XT using the  Qwen3-https://huggingface.co/unsloth/Qwen3-30B-A3B-GGUF in the Q4_K_S   Then I used lmstudio and put all of the model into the gpu using the ""offload to gpu"" fully",Neutral
AMD,"You can literally order one on newegg right now. What are you talking about?  Item|Price  :----|:----  [ASRock Creator Radeon AI Pro R9700 R9700 CT 32GB 256\-bit GDDR6 PCI Express 5\.0 x16 Graphics Card](https://www\.newegg\.com/asrock\-challenger\-rx9060xt\-cl\-8go\-radeon\-rx\-9060\-xt\-8gb\-graphics\-card\-double\-fans/p/N82E16814930143?item=N82E16814930143&utm\_campaign=snc\-reddit\-\_\-sr\-\_\-14\-930\-143\-\_\-10282025&utm\_medium=social&utm\_source=reddit)|$1,299.99",Neutral
AMD,"I am extremely tempted ngl. Was looking at a 5080 FE for being the fastest reasonable thing in 2 slots. Getting even close to its performance in raster with the 5090's VRAM capacity is enticing. Also, being a blower card means I can dig back out my designs for a custom ITX case where this card lays flat. This is close to the same price locally right now. About $150 more than the 5080.  It will be slightly slower than the 9070XT I'm sure, likely closer to the 9070 due to the lower clocks, but it's small and has 32GB of VRAM.",Positive
AMD,I'm considering it for my fileserver. I could run comfyui on it but also use it as a headless game streaming rig? It's enticing.,Positive
AMD,9070XT with twice the memory.,Neutral
AMD,This isn’t a gaming card,Neutral
AMD,The RDNA3 in the same category was the W7900 which cost $3500-$4000 so the Pro 9700 is actually a significant price drop.,Neutral
AMD,AI models and video editing,Neutral
AMD,Its 30% slower for AI models that fit within the 20gb limit on the 7900 XT,Negative
AMD,I don't know what happened and why I thought it was out of stock because when I looked it genuinely was not purchasable.    I sincerely appreciate you sending me this link because this link did work.   I ordered 2. Let's see if they come.,Negative
AMD,"Wouldn't it boost to the highest speed it can just like the 9070XT?  My 9070XT boosts well above the stated numbers from AMD...  If you do, please post some results for gaming!  I'd really like to see some numbers.",Positive
AMD,"Ah - that actually sounds amazing ngl, defo going to be looking at reviews!",Positive
AMD,"What advantage does the AI Pro card have over the 7900XTX, cause you can do AI, video editing on it, too?",Neutral
AMD,"You can do same on 7900XTX, I already tried it.",Neutral
AMD,"It's not for regular desktop use like the 7900XT where you would use just one card. It's for workstations where you can fit several, or even datacenters.",Neutral
AMD,Can you do a review? Either by video or text. Would be awesome.,Neutral
AMD,"It will, but bear in mind your 9070XT likely has a triple-slot cooler and a higher TDP than this card will. This is dual-slot, in a blower, and capped at 300W. 9070XTs regularly get near 340W. This will also likely get above its rated 2350mhz clock, and it's rated up to 2920mhz. But, Asrock rates their Taichi 9070XT at 2570mhz base and 3100mhz boost. That puts the 9070XT at a roughly 6% advantage if both are hitting their max clocks. Not much and I expect slight driver differences to exaggerate it a bit too. Based on my experience with the W7900DS (dual-slot N31 48GB) I expect this one to maintain around 280W under load and probably sit in the 2800-2900mhz range.",Neutral
AMD,"Don't get too carried away, they want you to pay AI fanatic money. Double the memory equals double the price you know.",Negative
AMD,RDNA4 ML performance is significantly higher than RDNA3.,Positive
AMD,"It does have ECC VRAM, for one. Also more VRAM, 32GB vs 24GB. Newer architecture that is better at raytracing and machine learning. And expanded support presumably, like with any of the Pro parts. Unless you mean the W7900 version of the 7900XTX, which features 48GB VRAM. But that one is also twice the price at least.",Positive
AMD,"There are few key differences.  a) First, PCIe 5.0 interface rather than 4.0. So technically double the bandwidth if you DO need to send any data to the card during your wokloads.  b) 32GB VRAM vs 24GB IS a major difference for the use case. You can barely fit 16-17GB model on a 24GB VRAM card once you also count in the context size. But 32GB? Easy.  c) Blower style cooler and 2-slot height, AMD is generally speaking expecting most target users to be buying **two** of these. It's going to be hard with 7900XTX.   d) a bit lower power consumption.  Honestly it's not a bad value proposition at it's price. Yeah, you are paying double for the same specs as 9070XT, just twice the VRAM. But as far as AI premium goes it's genuinely pretty reasonable compared to most other options.",Neutral
AMD,Still slower no matter where you put it,Negative
AMD,Given it's not far off the 5080 in price depending on where you are (+$135 over the FE near me) the price actually seems downright reasonable for a 32GB card. I'm mostly interested in this as a CFD card where Vram capacity is king. For $1300 this sure as hell beats the 5090's value and having twice the memory over a 5080 is very attractive. Biggest competitor in my book is either a pair of B50s or a B60 Dual.,Positive
AMD,"Yes, but it has lower memory bandwidth, 640GB/sec vs 960GB/sec, lower number of compute units.  In some areas it has advantage, in others it lacks...",Neutral
AMD,"You can use riser cables for 7900XTX if you cant put two next to each other. A really fast ssd reads at 14GB/sec, so not much advantage from PCIe 5.0. You dont load models that often, typically you load one and start using it. With 3 fans and better heatsink, gpu can use more power and boost clocks to give you most performance when you need it. Huge price increase for just 8GB extra, is not worth it. You could go with 48 or 64GB if you really need it, there is always going to be a model that you cant fit in vram.",Neutral
AMD,"My thoughts exactly, more V-ram for cheaper/same price = more better.  Now that real-time rendering in games is a debate between 140 fps or 130 fps (ignoring the dumpster-fire games that run at 30 fps wether you like it or not, despite looking like someone rubbed vaseline all over the camera, those failed tech demos don't count in my books) the V-ram has become the real selling point of the card. Not only does it give you more versatility in productivity scenarios, but it also makes games run more smoothly at the same time (and in some instances, makes the difference between a game even running or not at all).  Honestly would be nice if we could swap out our V-ram, or had unified ram and could use our M-board ram for the GPU as well.  Or better still, dedicate 2 ram slots to the CPU, and the other 2 ram slots (which are usually empty and useless since you lose performance running 4 sticks in most scenarios nowadays) are dedicated to the GPU as MOAR V-RAM.  That would actually be epic and massively increase the adaptability and versatility of each individual GPU, which would also increase competition too (meaning cheaper GPU's that are also significantly better).",Positive
AMD,Each compute unit is significantly faster on RDNA4 and far more versatile for ML workloads. Memory bandwidth comparisons are meaningless as they’re different architectures and RDNA4 is more memory efficient than RDNA3.,Positive
AMD,">A really fast ssd reads at 14GB/sec, so not much advantage from PCIe 5.0  It's 64GB/s vs 32GB/s (at x16 speeds). I am not saying you should stream data from your **SSD** into a GPU. More like RAM -> GPU. In some situations it will double your results. Admittedly for LLMs inference PCIe bandwidth is not THAT important (x4 4.0 so far seems to be sufficient) but it could make a difference in training.  >gpu can use more power and boost clocks to give you most performance when you need it  For LLMs it's primarily memory bandwidth that matters. Higher boost clocks mostly increase your electricity bill. It also means more heat and with a triple fan design bottom card just blows all the air back into your system.  >You can use riser cables for 7900XTX if you cant put two next to each other  You can if you have a massive case or are literally plugging video cards outside of it. On the other hand, consider a relatively cheap TRX50 board:  [https://www.gigabyte.com/Motherboard/TRX50-AERO-D-rev-10](https://www.gigabyte.com/Motherboard/TRX50-AERO-D-rev-10)  You can shove 2-3x R9700 without any fiddling.  >Huge price increase for just 8GB extra, is not worth it  If you are after 24GB cards then you shouldn't buy 7900XTX either to begin with. You go on ebay and hunt for 3090s. Arc Pro B60 is an option too if you find it at a reasonable price.  32GB is a different story. Right now this is the cheapest card offering this much. Whether you personally find it worth it or not is a different story but as far as workstation grade hardware goes... honestly it's not **that** much of a markup. Just look at how much you pay for a consumer grade 5090, let alone an RTX Blackwell.",Neutral
AMD,"I definitely agree on wishing vram was upgradable by the user. Perhaps in the future something like LPCAMM could provide that suitable interconnect. A 128-bit interface per module would mean that even the biggest modern busses only need to fit 4 of them.  As for unified memory, I don't think many people on these forums will like the road that takes us down. Like it or not, the future (at least near future) of that path is things like Lunar Lake, Apple Silicon, and Strix Halo. Those are all very impressive SoCs, but you may have to trade the ability to upgrade your GPU separately from the CPU to get your upgradable unified memory.  As for dedicating certain slots on the motherboard to each side, CPU and GPU, that gets you the worst of both worlds in some ways. Your memory is no longer unified, the full bandwidth is not directly accessible to either in most architectures, but you can upgrade it. If you are going to go down the route of integrating the CPU and GPU to use the same memory and connections, give both access to all of it in full bandwidth.",Neutral
AMD,"Isn't AI memory intensive? Doesnt bandwidth matter? You can have faster compute units, but they can be memory starved. Also 7900xtx has more compute units, this should make up for some of the speed gap.",Neutral
AMD,"I'm with you on this. At 32GB+ your options are slim right now. The B60 Dual is the only other thing I find particularly attractive that is somewhat near this, though that is 2x24GB per card rather than a single 32GB. The W7800 also offers 32GB on a 256-bit bus, but this card is substantially faster than that and currently cheaper.",Positive
AMD,"How have I not hears of LPCAMM until now!?  Yeah, in an ideal world you would have M-board that has 3 separate chip sockets, one for your CPU, one for your unified RAM, and one for your GPU chip. It would probably look something like 3 CPU sockets.  Which would also make cooling very easy and scalable, something our current M-board layout didn't account for. They did account for development add-on cards however so it was a necessary step and an integral part of computer design history. However now that we've optimised general use computers to CPU, GPU, and RAM (ignoring storage, keeping that scalable with addon slots is definitely the way) I do definitely feel that new layout standard is due.  Of course that's an ideal world where CPU, GPU, and RAM manufacturers all agree on the same socket, we'd probably need some kind of modular M-board that allows the use of whatever Intel, Nvidia, AMD, etc.... GPU, CPU, and RAM sockets each company invents, otherwise you'd get stuck in a closed ecosystem where you're at the whim of the manufacturer of wether you're allowed to have more RAM or a more powerful GPU or a more efficient CPU or not.  Though it's nice to dream of a more optimal computer layout (of course my personal take is to have the CPU and GPU on opposite sides of the M-board, that way they each get their own separate airflow, and also the backside of each socket/slot can get some extra passive cooling which would help with the higher and higher clock speeds our chips are now reaching).  It is the worst of both worlds, however it is highly scalable and if the GPU had it's own dedicated memory controller then it could work as an alternative where you have double the memory, but each chip only sees half of that memory. I imagine in high-demand dynamic workloads where the CPU wants more RAM one second, then the GPU wants more ram the next, that having separated RAM for each chip could become a benefit and act as a safety margin. Though I do agree that it's probably not the best idea and would require just as much of a redesign as unified RAM would.  Unless you just put RAM stick slots on the GPU (probably a really good use case for LPCAMM as that would allow you to maintain the same backplate clearance standard current GPU's are built to) then independent scalable RAM would just be a direct upgrade to our current layout as-is.  Though I definitely want discrete chips on an M-board with unified memory (and probably some extra silicone chip, hell you could probably split the traditional rasterisation GPU chip and the RT/AI/Tensor/etc... chip into separate sockets) cause that would mean having 3-4 AIO's on your board with nothing else and that would just look too cool imho.",Neutral
AMD,"RDNA4 has 4x the INT8 performance without sparsity enabled, 8x with it, so the gap is enormous. You are obsessed with bandwidth and ignoring things that matter more like the implementation of Matrix cores.  Plus, RDNA4 has 2x the PCIe bandwidth.",Neutral
AMD,I'm fairly sure capacity outranks bandwidth in most cases.,Neutral
AMD,"Then you can easily go with those mac systems with 128GB unified memory and not use a gpu anymore. (they are quite good at running LLMs, but mem speed is around 200GBs/sec)",Positive
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,Why Zen4 and not Zen5?  Do they have some unused Zen4 production lines?,Neutral
AMD,That's buck!,Neutral
AMD,i was hoping for an am4 5900x3d  aita?,Neutral
AMD,"CPUs that didn't make it as 7900X3D (one die with 3D V-Cache) or 7600X3D. The 7500X3D should have the same amount of cores with 3D V-Cache as the first two named CPUs, but with lower clock speed.  Also cheaper CPUs might favor certain markets where electronics are very high taxed and import costs. Countries like Brazil and Argentina in South America pay a lot for their PC parts compared to their average income.",Neutral
AMD,"AMD isn't making these. They get their chips from TSMC. Every so often they have chips that don't pass quality control - a defective core or graphics die, etc.  They save those and when they have enough, they launch them as a lower-tier product. They don't have a lot of them, that's why they're only released in limited numbers - Microcenter only, etc.",Negative
AMD,9500x3d would be pretty neat,Neutral
AMD,"Likely 7600x3Ds that couldn't make the cut   The difference between Zen 4 & zen 5 is negligible anyway, in performance and efficiency (If you match the power limits)",Neutral
AMD,They tend to dump the last usable x3D dies late as x500x3D.  I think they release them last cause they don't have a lot of them and they don't really fit in the performance numbering scheme well.,Negative
AMD,"Probably not a lot of ""failed"" Zen5 chips yet. They're accumulating stock for this before they launch ""failed"" SKUs so they'd have at least some stock and not just selling one at a time.",Neutral
AMD,All of the strange low availability chips are about using up leftover stock that didnt meat higher bins. Its not gonna be cutting edge shit,Negative
AMD,>CPUs that didn't make it as 7900X3D  That would mean 2 CCDs and one disabled?,Neutral
AMD,"Since we're only now getting 7500x3Ds, wee probably won't see those for 2 years.",Neutral
AMD,"I was hoping for 7700x3D, 5700x3d was at the perfect spot with the price/performance.",Positive
AMD,"Perhaps for desktop (I have no experience) but this is very untrue for server skus. 15% uplift on a per watt basis, and 40-50% uplift for the top skus zen 4 to zen 5.",Negative
AMD,Forgive my ignorance... 7600x3D is a real product?????  Damn... Wish I could get one.,Negative
AMD,"Zen 5 is bottlenecked by memory bandwidth, which is why the 9800x3D beats the non-Zen5 parts in productivity tasks.",Neutral
AMD,Possible.,Neutral
AMD,"Isn't zen 5 on the desktop ridiculously memory bottlenecked, especially on (albeit rare) avx-512 workloads? I guess the server parts benefit a lot from having quad-channel memory.",Neutral
AMD,You see it on the desktop space.   The zen 5 'x' CPUs gota 65W power limit. So they looked super efficient compared to zen 4 'x' CPUs   But when you gave zen 4 the same power limits (Or just used the non-x) it's almost as efficient,Positive
AMD,"The server parts have 12 channel memory. As for desktop being bottlenecked by dual channel, I would wager so.",Neutral
AMD,Let's see how many cards are actually in stock.   Because the Intel B60 was a super attractive price and you couldn't buy it for less than double.,Neutral
AMD,Would have been nice to get 48GB for a few more $$$  Whilst it's a good price point it does feel like this could / should have been launched last year spec wise.,Positive
AMD,"Congrats to AMD for showing up to the DIY (aka, consumer) AI party",Neutral
AMD,32gb vram noice. Too bad it's a blower design though.,Negative
AMD,Which card are they talking about that uses dual 8pin connectors?  All the cards I have seen and the specs from AMD shows it using the 12vhpwr connector.,Neutral
AMD,"I think there were a few of these floating around eBay a bit ago, they were snatched up quick.",Neutral
AMD,I wonder if it's going to have SPIR-V support,Neutral
AMD,"Think I'm going to buy one to pair with my 7900XTX to ""compare"" the two (and to use them both with LLMs).",Neutral
AMD,"Great to see it available for DIY market, hopefully there will be enough volume to keep prices down.   I see it has 2 X 8 pin connectors which is great, earlier I read it a 12 pin or something.",Positive
AMD,Does it support SR-iov/vGPU?,Neutral
AMD,600$ for 16gb of vram,Neutral
AMD,Is this a card for creators or for gaming? If gaming then which nvidia card does it compete with?,Neutral
AMD,But can  it run borderland 4 ?,Neutral
AMD,It also have 1/3 of the memory bandwidth of a RTX 5090,Positive
AMD,Why would u buy that instead of a 5080 and save a few bucks? I guess for gamers + ai-ers,Neutral
AMD,32GB is the most it can have. The use of GDDR6 prevents them from using more.,Neutral
AMD,"They already sold cards like the W7900 48GB, that one's just a lot more expensive and released at a time when AMD and ML was less viable than it is now.",Negative
AMD,"For it's intended use case it makes sense. People buying these are most likely to get 2. Mostly so you can have a nice and cozy 64GB VRAM setup for the price and power draw of a single 5090. Well, 2x 32GB it's not exactly 64GB but in some applications of AI it can be.  Weird thing is that these cards are already out, I have seen some available with next day delivery.",Positive
AMD,"This is a good thing. Exhausting heat out of the case vs into it. You can stack many of these, this way.",Positive
AMD,"Yeah no that’s standard for server cards.   In server rooms, server stations are organized into rows where the pathways between them are either hot paths or cold paths.   Cold air is vented in through the cold paths. Hot hair must be exhausted into the hot paths; not in the cold paths and especially not within the cases themselves. Hot air is vented out of the hot paths and either to the outside or an air-recycling HVAC.",Neutral
AMD,"Blower design is best and superior than the ugly trash triple fan designs from most AIBs.  They know their target market, and multi-GPU setups are a breeze when you have the right cooler design.",Positive
AMD,There are so many non blower cards on the market. You’ll be ok.,Neutral
AMD,What are you talking about? This is a workstation card and there's no 9800.,Negative
AMD,Don't your XTX has 30% more memory bandwidth its money down the drain if you ask me,Negative
AMD,"It’s for neither, it’s aimed towards productivity. Good for running local AI models in data centers.",Positive
AMD,"Yeah, and Arc Raiders. They dont charge you that much for complaints",Neutral
AMD,"Idk what you are getting downvote, but the answer is probably. ~~There is no video out, it's more of a GPGPU.~~~  ~~Maybe you could if they support virtual display like for VDI~~  Edit : I didn't noticed the DP on the thumbnail! So yeah, it should work.",Neutral
AMD,You use a GPU for playing video games? What a weird thing to do.    I'm pretty sure everyone uses them for compute.,Negative
AMD,"It's not that it uses GDDR6. It's the 256 bit bus.  Although if there are 8GB GDDR6 chips, they could in theory use them for 64GB VRAM",Neutral
AMD,32+32 = not 64?,Neutral
AMD,"They're also nice for workstations. This card dumps the vast majority of its heat out of the case itself, and does some work as an exhaust fan in general. Add to that the spacing tolerance of radial fans like this and you can pack them in like sardines. They're really hard to starve for air.",Positive
AMD,"That's not a server card.  Server cards don't have their own fans.  There's no need, since the fans are included with the server chassis.  All server expansion cards rely on server chassis airflow for proper cooling, and almost all of them do this without adding any additional fans.",Neutral
AMD,Most server cards I've seen are completely passive. Just a heatsink with open flow front to back and the super noisy chassis-mounted intake fans just blast air through everything.  Blowers are more like workstation cards IMO.,Neutral
AMD,"That's not standard for server cards, server cards are passively cooled. Blower is standard for workstations.",Neutral
AMD,"I don't like your tone, but whatever.   You are right, I mixed that up.",Negative
AMD,"Of course it can game. It's an RX 9070 XT under the hood, just with 2x VRAM capacity and normally-sized cooler.",Neutral
AMD,This card comes with 4x DP clearly visible in the image also confirmed by the specs on AMD's website.       [https://www.amd.com/en/products/graphics/workstations/radeon-ai-pro/ai-9000-series/amd-radeon-ai-pro-r9700.html](https://www.amd.com/en/products/graphics/workstations/radeon-ai-pro/ai-9000-series/amd-radeon-ai-pro-r9700.html),Neutral
AMD,"No doubt ""about"" it.",Neutral
AMD,"Not in the context of VRAM. You have two boxes 32cm long. Can you fit 45cm long figurine? Answer is no, it will just hit the side of one of the boxes. You can however fit two 32cm long figurines.  In ""AI"" world it works kinda similar. 2 cards of 32GB each can sometimes act as 2 devices with 32GB, can sometimes be used as 64GB with caveats OR actually behave as a 64GB VRAM pool.  For instance image generative AI has very dense, linear models. They have to fit in full. So for their purpose you have 2 GPUs, each with around 640GB/s bandwidth and 32GB memory each. You can produce twice as much AI slop by running 2 applications at once but you can't speed up producing a single one or use a really large model that exceeds 32GB.  In order to treat it as 64GB you would need to go over PCI Express bus instead and transfer data back and forth between cards. Which at x8/x8 configuration would be 32GB/s. So you can but it would absolutely slaughter your performance. Cards have to constantly communicate as each does the entire workflow.  But LLMs are different. You can split a 64GB model into 2 chunks of 32GB each. Card #1 does part of the workload, then it hands it to #2. They don't actually talk nearly as much. So for the biggest part 2 cards of 32GB = 64GB, they don't rely so much on PCI-Express.  And in the professional world there are also connections superior to PCIe. Nvidia H100 has like 3TB/s internal bandwidth but it also has 900GB/s NVLink. So if you need to stack 2 together - you can and performance penalty is not nearly as much as on consumer grade cards.  So yea, 2x32GB in PC world can sometimes mean it's still 32GB :)",Neutral
AMD,It's not passive cooling.  They won't cool passively.  They required airflow.  The fan is just part of the server chassis rather than part of the card.,Negative
AMD,"There is no such thing as a passively cooled server cards. There are server cards that come with fans, and then there are server cards that expects you to provide the fan. Both are very actively cooled.",Neutral
AMD,Only looked at the thumbnail and didn't notice but you are right!,Neutral
AMD,Autocorrect + morning is a bad combo lol,Negative
AMD,"Too bad open source AIs are all crap compared to the commercial ones like Gemini, Gemini 2.5 Pro Deep Think Deep Research is amazing, so this use case is kinda pointless it seems since any local LLM u can run is gonna be ass compared to the industry leaders",Negative
AMD,"I agree it's not technically passive, but as the GPU itself has no fan Nvidia and AMD themselves call it a passive form factor.   From Nvidia's own product page of the RTX Pro 6000 Server Edition: https://www.nvidia.com/en-us/data-center/rtx-pro-6000-blackwell-server-edition/  > Thermal Solution 	Passive",Neutral
AMD,"You need to tell your revelations to Nvidia, because they keep releasing passively cooled GPUs xD (https://resources.nvidia.com/en-us-l40s/l40s-datasheet-28413)",Neutral
AMD,"This 1200$ gpu wont be able to compete with multi billion dollar companies?! What’s next, are you gonna tell me water is wet?  Some people just like to experiment or do this as a hobby. Is that so hard to understand?",Negative
AMD,Local AI models are infinitely better than companies like Google when it comes to Home Assistant integration. Why? Because I keep my private life private. That's just one example.,Positive
AMD,"Yes, it doesn’t come with a fan. Yes, on the spec sheet it is advertised as passive since it doesn’t come with a fan. It’s still a 350W card and expects either the server case or for you to provide the fans.  No heat sink can cool 350W with zero airflow.",Neutral
AMD,"It's def better for that no doubt, but i rather not compromise on quality. Instead, avoid giving personal data when using Gemini. Instead of saying ""i take this medical pill what would happen if i increase the dose"" say ""what would happen if a person taking this medical pill increases the dose"" etc. etc.",Negative
AMD,Only person who said no airflow is you.,Neutral
AMD,"Yeah, that’s because that is what passive cooling is. Transferring heat to air with no requirement of air circulation. i.e. smartphones and Mac’s. If the case adds a fan to the heat sink, or if you add your own fan, it’s no longer passive cooling. It’s just advertised as passive cooling since it does not come with a heat sink fan. It still expects you or the case to provide the fans.",Neutral
AMD,Gleam is cancer,Neutral
AMD,"100% will go towards ""influencers"" and the like. Normal people rarely win these ""contests"".",Negative
AMD,Why hasn't any manufacturer done the obvious and released customizeable GPUs yet?,Neutral
AMD,"3 million entrances, good luck",Positive
AMD,"So they are giving away used/open box models in the hopes that the data they gather can be sold for more than the value of the cards?  Giveaways are really cool, but this gleam thing seems very weird.  I'd love to have one of the cards, but no.",Negative
AMD,"Why the themed GPUs now? 9 months after the game’s release and is not popular anymore? You gotta launch that shit when the game comes out or even before for the super fans.   Edit: like Asus launching Black Ops 7 9070 XTs. That makes sense with timing. I checked steam charts too and it spiked to around May levels this month. (35,000ish. Still down from 300K during launch month) Anyone know why the recent spike?",Negative
AMD,These social networking tasks required to get entries are a joke.,Negative
AMD,Not going to eaven Bother i KNOW i wount win ..... il just stick with what i have and be happy about it  99.99% sure it will go to a friend of theirs or family memeber ..... Nepatism at its finest,Negative
AMD,Would be nice if they were giving away a fix for driver timeouts...,Neutral
AMD,"Yes. Logged in with email. Collected the points and after finishing, it said: „log in to your social media accounts to verify you are a real person.“  Hell no! I won’t give gleam any of my social media logins.",Negative
AMD,"oh thank god, i thought everyone else liked this shit",Negative
AMD,I mean the Bigger issues is this gets botted easily🤷‍♀️,Neutral
AMD,ask sulky groovy toothbrush violet divide gold include political narrow   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
AMD,I won't an Athlon thunderbird like 24 years ago lmao AmA,Negative
AMD,I won an R2D2 xbox 360 from an online sweepstakes once. It can happen,Positive
AMD,"In all seriousness, do you really want one? I mean a free GPU is nice and all but these look absolutely horrible.",Positive
AMD,"There are some in china as far as I know. You can get engravings on the backplate, but not sure about custom printed images.",Neutral
AMD,impossible to win lmao and there's probably some people with multiple accounts too.,Negative
AMD,New patch not long ago. FFXIV collab.,Neutral
AMD,"""The game is not popular anymore"" No. There's plenty of valid criticisms you can point at Wilds, but you cannot deny that it is still pulling really good numbers, and many in the MH community still love the game despite it's valid issues",Negative
AMD,"The contest says some of them are open box, so it sounds like they are giving away cards used in marketing, maybe? Just brainstorming.",Neutral
AMD,"They're probably working with Capcom to make the game a early adopter of FSR's Redstone update.  If that's coming out this year, and Wilds is getting a big performance update before the end of the year. It would make sense.",Neutral
AMD,So that means you are not a real person!!  What are you!?!?,Negative
AMD,That is something I haven't even thought about.,Negative
AMD,If glem can't add a Honeypot in this day and age that's extremely sad and hilarious,Negative
AMD,i won a car from a yogurt contest  and when i asked the lawyer he told me only 5 people signed in   4 of which was me my mother my father and my sister,Negative
AMD,I won a 4070 super and got a free year of pizza hut within months of each other 😂 felt like my luck peaked,Positive
AMD,"I won a GTX 970 years ago in a contest like this. Almost missed out, because I didn't notice the email that informed me I won, until it was almost too late to claim. Just like hundreds of other contests, I've quickly entered, and then promptly forgot about, figuring I would never win any of them.",Negative
AMD,I won a 1080ti from a FB giveaway years ago. Only thing I've ever won.,Negative
AMD,Yeah I'm won a Noctua hoodie and a ton of merch in one of their Instagram giveaways,Negative
AMD,"I won a 3070 PC, Xbox Series X, and PS5 at the height of covid. Yes, I know I was very lucky. But these things are winnable, it's just not likely when there are millions of entries.",Neutral
AMD,Y'all keep making my luck even worse than normal. I guess I should never visit a casino at this rate.,Negative
AMD,"who gives a fuck how they look? who in their right mind doesnt want a free gpu? but yea, lets send the 10th free one to another useless youtube person who just makes yet another copy of a video. and since we spend more money on marketing, lets up the product price",Negative
AMD,Me I like monster hunter👍,Positive
AMD,lmao. nice,Positive
AMD,Lucky   Also you won debt unless you sold a Car also,Neutral
AMD,"I literally said ""a free GPU is nice and all"". Doesn't change the fact that ""common"" non-themed GPUs look much better than whatever this lazy attempt at a themed GPU is.",Negative
AMD,An international poll found that Monster Hunter is one of Capcom's top [5 ](https://www.reddit.com/r/residentevil/s/pC5bJbXqv8)franchises.  You gotta give the people what they want.,Neutral
AMD,"Correct, which is why you make a proper themed model like how they did for Cyberpunk and even Doom. Not whatever the fuck this is with a printed backplate and lazily colored shroud.",Negative
AMD,AMD had a Cyberpunk card?,Neutral
AMD,"No, I'm just talking about themed cards in general. It's a limited run of 5. Surely you can do better than this.",Negative
AMD,>cutting-edge Zen 4 architecture  Huh? I thought they already launched 9000 series embeded?,Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,So this could finally be zen 5 going into mainstream OEM prebuilt desktops,Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,thinitx dream z5/c RDNA 3.5 a Halo without npu?,Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,Just hope AMD doesn't axe the PCIe lanes like they did on the 8000G series.,Neutral
AMD,I guess it means we will finally have an APU that can compete with the ancient GTX1070.  Yippee...,Neutral
AMD,Hope AMD doesn't cut pcie line (and cutdown the igpu like 8300g and 8500g). Especially the krackan point 2.  Wonder if AMD has plan to release full-fat zenC at cheaper price than big.LITTLE style and full core.,Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,9700GE 880M FTW!,Neutral
AMD,"Oh, I bet you will be surprised. I mean, 8000G was based on Phoenix and the top Phoenix skus have 20 gen4 lanes. In Strix Point even the top sku have only 16 gen4 lanes. For the desktop parts 4 lanes will go to prom21 (Chipset), another 4 for the main nvme slot. So only 8 lanes for both the x16 slot and the general purpose usage, I don't know how the pins will be configured but I suspect that 4 lanes will go to the general purpose and only 4 will remain to the x16 cpu slot.",Neutral
AMD,"If a 9000G series APU ends up with that level of performance, it'll surely cost $350+.  I mean, the 8700G launched at $330 in 2024 with an iGPU that performed edit: worse than\* that of a GTX 1650 that was panned for being bad value at $150 back in 2019. While you can argue there's of course a good CPU along with it for much of that cost, you used to get $100 quad core APUs (ex. Ryzen 2200G) that competed with $80 GPUs released around the same time (RX 550, GT 1030) while being perfectly competent CPUs.",Negative
AMD,That depends on how fast memory it can support. Strix Point in the Xbox Ally X is already bandwidth-limited at 17W...,Neutral
AMD,"A GTX 1070 have much higher memory bandwidth that what you see on desktop DDR5, tough, and that will hamper performance.",Neutral
AMD,thinitx doesn't even have PCIe slot. A hd-plex H1 v3 would be perfect for a X600TM with one of these. A 9700ge could be the one for me.,Positive
AMD,"The 8700G performs around GTX 1050 level. I've one of those in a SFF setup for my wife that is nearly silent in desktop use at home (mail, browsing WWW, streaming, VPN, etc). That setup will never see a discrete GPU.",Neutral
AMD,Overclocking the iGPU n RAM u can reach the performance of a RX580.  The iGPU at 3200 n RAM at 9200.,Neutral
AMD,8700G / 8600G can already handle 7600 MT/s without issue. Much better memory support than chiplet Ryzens.,Positive
AMD,"Bandwidth isn't necessarily that big of an issue, especially when you take SRAM into account.  GTX1070 has like 2MB of L2 cache, compared to 8700G's 16MB L3 (shared with the CPU).  Besides, Pascal is an ancient uArch at this point whereas AMD has made some serious strides in memory compression with RDNA 4, at least according to this technical deep-dive from Ryan Smith, who previously wrote for AnandTech:  [https://www.servethehome.com/amd-rdna-4-gpu-architecture-at-hot-chips-2025/](https://www.servethehome.com/amd-rdna-4-gpu-architecture-at-hot-chips-2025/)  That's how the 9070XT manages to trade blows with 5070Ti, despite being on GDDR6.   Regardless, it would help if AMD add an X3D tile to their APUs, though I don't see it happening anytime soon!",Neutral
AMD,"Ah, my mistake.",Neutral
AMD,"They can handle 9000 MT/s with relative ease, but that doesn't mean Strix Point will have a better IMC. It's the same process node, and the 8700G is already limited by memory bandwidth even at 9000 MT/s with tuned subtimings.",Neutral
AMD,"As long it's at MSRP, it's a good option.",Positive
AMD,And largely against the non-x3d lmfao.,Neutral
AMD,Aren't they just showing that AMDs CPUs are better for gaming?,Neutral
AMD,"Whoever downvoted my comment, why not run your own benchmarks and compare your results with mine? You could make posts asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming. Or how about go to tech sites and tech tubers asking them to provide their benchmark scores. its disappointing  to know people prefer to stay in ignorance and prefer people getting scammed over knowing the truth, just face it reviews adjusted to satisfy the sponsor how do you think they make money.",Neutral
AMD,"I haven’t tried ARL processors yet, but based on my own tests, 14700K with DDR5-7200 is 30–40% faster in gaming compared to Zen 4/5 non-3D chips. In the five games I tested, 14700k matched 9800X3D in three, lost in one, and won in one. I have no idea how tech sites and tech YouTubers show non 3ds as being just as fast or only slightly behind when they are actually far behind.  14700k vs 9700x/7700x  [https://ibb.co/cSCZ6fdX](https://ibb.co/cSCZ6fdX)  [https://ibb.co/999WzW4T](https://ibb.co/999WzW4T)  [https://ibb.co/Gjw1nXX](https://ibb.co/Gjw1nXX)  14700k vs 9800x3d  [https://ibb.co/FbMNL40q](https://ibb.co/FbMNL40q)  [https://ibb.co/8nQXpYZ5](https://ibb.co/8nQXpYZ5)  [https://ibb.co/vvcpKsGY](https://ibb.co/vvcpKsGY)  [https://ibb.co/67Jn8tKr](https://ibb.co/67Jn8tKr)  RE4  9800x3d 179FPS  14700k 159FPS",Neutral
AMD,"i dont think proving you can do more work faster is going to turn around sells. more people exclusively game or game and do light work, than those who do heavy work at home on computers.  i dont relive there are enough youtubers and streamers out there to capture as there are general gamers",Negative
AMD,"I mean, yeah. A 9800X3D costs way more than 265K, while a 9700X is within 10 bucks of it.",Neutral
AMD,I assume they compared with CPUs in a similar price range,Neutral
AMD,I don't think these slides are as good as intel thinks they are. They are just an advertisement to buy x3d ryzen cpus lol.,Negative
AMD,I guess the point Intel is trying to make is that it's great for gaming and much better for content creation (aka work). It's also cheaper.  X3d CPU's are only better for gaming (and not in all games). Doesn't mean they are crap but you pay a premium for x3d.,Positive
AMD,"if you think you're 100% correct - go out and buy an x3d and run the tests, post a video with the results, and put an amazon affiliate link in the comments for a 14700k build that overall is better than an x3d.  there's nothing wrong with the 14700k, it's a beast. i think there's a few select games where it even beats the 7800x3d. but there's a reason that you can find hundreds of youtube channels with sub counts up and down the spectrum from 0 subs to 10M subs showing hardware side by sides with the x3d's beating the 14700k - and most of them don't have advertisers to please.  in my opinion, you're getting downvoted because you don't want to provide proof, you're out here saying it's tech tubers fault that you have no proof. you can also find a good amount of open source benchmark data at openbenchmarking.org, it's not pretty - but there's useful data there as well.  but hey - you're only hurting yourself by not breaking open the youtube techfluencer conspiracy around AMD processors. go buy an x3d, do all the testing, and use it as a springboard to free us from the lies of big tech. i'd love for you to be correct, but until you can provide proof, it honestly just looks like you have buyers remorse with your 14700k.",Neutral
AMD,">asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming.  [but it's not true?](https://youtu.be/IeBruhhigPI?t=13m3s) 1%low on 9700X is identical to 14700k/14900k, and better than on 14600k, AVG. FPS is slightly higher on 14700k/14900k, or is 9.7% higher average FPS on 14900k considered ""far ahead""? Plus, 14900K is not a CPU for gaming, it's a CPU for production workloads plus gaming, which makes it noticeably more expensive than 9700X, so realistically we should compare 9700X to 14600K/14700K.",Neutral
AMD,"Cherry picked results or just outright fake.  A 14700k isn't 30-40% faster compared to zen 4/5 non-3d chips.  Come up with a decent game sample first first before coming up with stupid conclusions.  Every reviewer I've found just happens to have different results than yours so everyone must be wrong except ""me"" right?  Even if the 14700k is slightly faster it still consumes a lot more power so theres that.  The numbers you can find from reviewers point to the 9700x being around 5 % slower, and you here are talking about 30-40% lul, if it was 30-40% faster it would be faster than a fking 9800x3d lmao.",Negative
AMD,I just built a 265K machine with 64GB 6800 RAM and it is no slouch. I haven't really played much on it yet as I've been working out of town a lot lately but my 4080S is now the limiting factor rather than the 10850K I had before.,Positive
AMD,RE4  9700x vs 14700k   [https://www.youtube.com/watch?v=FcDM07sgohY](https://www.youtube.com/watch?v=FcDM07sgohY)  [https://www.youtube.com/watch?v=-WO0HqajShY](https://www.youtube.com/watch?v=-WO0HqajShY),Neutral
AMD,"I did the same comparison, but it was 13900k vs 7800x3d and the Intel was much smoother. It's sad that you are getting down voted by bots who have never tried it and repeat tech tuber numbers. It's sad because ultimately the consumer is the loser when they are believing in lies.",Negative
AMD,"Now install windows 11, lol",Neutral
AMD,"> I haven’t tried ARL processors yet, but based on my own tests  Then whatever you're about to say has 0 relevance to the topic at hand.",Negative
AMD,"Clearly not as sometimes they use the 9950x3d, and sometimes the 9900x to compare to the ultra 9 285k.   One of those is the half the price of the other.  (And guess which one they used for 1080p gaming comparison)",Neutral
AMD,"And to add to that, which often gets omitted. Is that they are only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, intel in my mind actually makes more sense.",Neutral
AMD,"Why not tech sites provide built in benchmarks scores and let everyone veryfy them, and let us decide which processors better, why should we believe in charts blindly, why should I stay silent about what i found.",Neutral
AMD,"I sell PC hardware and get to test a wide range of components. I’ve built 7950X3D and 9800X3D systems for my customers, and when I tested five games back then, their performance was very close to the results I got with the 14700K. I’ll be able to test more games in the future if another customer asks for an X3D build. I never claimed the 14700K was better than the X3D in general I only said it scored higher in one game, Horizon Zero Dawn Remastered, and matched it in others. At first, I thought something was wrong with the build, so I made a post asking 4090 owners with X3D processors to benchmark HZD, and they confirmed my result. If I were to test 10 or 20 games, the 9800X3D might turn out to be about 5% faster, but I’m still not sure. Since I’m testing on 4090s, I can buy whatever processor I want, but based on the five games I tested, it doesn’t seem worth it. Also, the minimum frame rate was always a bit higher on the 14700K. X3Ds are generally good, but my main concern is that non-3D chips are 30–40% slower than the 14700K. Why does it feel like I’m the only one who knows this?  As for proof, I’ve provided built-in benchmark screenshots, phone-recorded videos (since OBS uses 15% of GPU resources), and OBS captures as well, which should give very close idea of processors performance. I’ve shown full system specs and game settings every time, and provided more evidence than any tech site or tech YouTuber ever has, yet some people are still in denial. I want just one tech site or YouTuber to show benchmark screenshots or disclose full system specs before testing. None of them do, and people keep trusting charts with zero evidence.",Positive
AMD,"Ive given my evidence by videos recorded by phone and obs,  scores screenshots prove 14700k is 30% faster than 9700x where is your evidence, talk is cheap, you either provide hard undeniable evidence or stay silent.",Neutral
AMD,"14700k vs 9700x  CP 2077 171FPS vs 134FPS 30%  spider man2 200FPS vs 145FPS 35%  SoTR 20%  RE4 160FPS vs 130FPS 25%  horizon zero dawn 172FPS vs 136FPS 30%  30% on average vs 9700x, 35%-40% vs 7700x  here is video recorded testing 5 games by OBS for 14700k vs 7700k  RE4 160FPS vs 110FPS  SM2 200FPS vs 145FPS  [https://www.youtube.com/watch?v=oeRGYwcqaDU](https://www.youtube.com/watch?v=oeRGYwcqaDU)  [https://www.youtube.com/watch?v=u6EPzOGR2vo](https://www.youtube.com/watch?v=u6EPzOGR2vo)",Neutral
AMD,"can you do a test for horizon zero dawn and CP built in benchmarks, also if you have RE4 can you go to the same place of the game I tested on and take screen shot showing what frames you are getting, I want all tests 1080P very high, if its not much trouble for you.",Neutral
AMD,"I only tested 5 games, and the two processors were very close. I’d probably need to test 10 or more titles to know for sure if the 9800X3D is actually faster. The fact that the 14700K came out ahead in Horizon Zero Dawn shows that there are games where Intel performs better.",Neutral
AMD,I did same performance on all processors.,Neutral
AMD,"It does, just like RPL being shown on corrupt tech sites as being as fast as Zen 5 in gaming, in the real world ARL could be much faster than they make it out to be.",Neutral
AMD,That sounds like an AMD Stan argument circa 2020,Neutral
AMD,Lmao   https://youtu.be/KjDe-l4-QQo?si=GsvzFon8HYcrzrMc  theres no conspiracy mate,Neutral
AMD,"You need to grow up, evidence from some rude slime on Reddit is irrelevant for me, I value data from techpowerup, hardware unboxed or gamers nexus x100000 times more than anything coming from you.  I tried making this conversation constructive, but it seems that you're not adequate enough to even try. Good luck.",Negative
AMD,Can you explain me why in the video you are running a 7700x on 6000 cl40 RAM?,Neutral
AMD,"I challenge all tech sites and tech tubers to publicly share their built-in benchmark scores so everyone can verify them. By the way, I posted these scores on all the popular tech tubers pages on X (twitter), but none of them replied. I wish at least one of them would debate these results and prove me wrong.",Neutral
AMD,"Are your results with or without APO? APO supports SOTR and Cyberpunk, so give it a try if you haven't yet to see if it makes any difference.   https://www.intel.com/content/www/us/en/support/articles/000095419/processors.html",Neutral
AMD,"I can do CP 2077 later. I have a 265k with a 9070, but only 6400 ram  (Mostly replying so I remember to do it)",Neutral
AMD,I only have Cyberpunk of the three.  [https://imgur.com/a/L3qMGva](https://imgur.com/a/L3qMGva)     I think the 4080S is holding me back too much in this benchmark.,Neutral
AMD,"Your tests on win10, win 11 24H2 improved performance. All of your tests are complete nonsense anyway",Neutral
AMD,I was just thinking the same. We have come full circle.  Time to buy some more intel stock,Neutral
AMD,It worked for them didn't it.  Why is it loads of people think AMD has dominated since original Zen CPU?,Neutral
AMD,Expand ?,Neutral
AMD,"I want elegato unedited video from start to finish i did that and on cp 2077 9800x3d matched 14700k, and it managed to beat it on horizon zero dawn, i want built in benchmark scores so everyone can easily double check the numbers, forget everything online buy the hardware and run built in benchmarks RPL i7/i9s on ddr5 7200 are equivalent to zen 4/5 x3ds much faster than non 3ds, if you can't afford it make posts asking people for thier scores in games.",Positive
AMD,"It is a common knowledge at this point, that HUB, LTT and GN are working for YouTube algo, not the costumers. YT algo promotes saying AMD is king, so their reviews are exactly that (Every single thumbnail or tittle with Intel being dead or destroyed). Let's pair Intel with mediocre ram and loose timings, let's power throttle them, put on some weird power profile, turn off whatever bios setting, focus on specific benchmarks that have very little to do with real usage etc. In worst case scenario they can always say that Intel won, but it's not enough or that Amd will crush them in 6 months or so. They've been doing it for years now.   Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong. It's really not about being right or win some CPU wars, it's more about not being screwed by that giant YT racket that once again works for sponsors and YT bucks, not the costumers.",Negative
AMD,"I spammed my score screenshots on every popular tech tuber’s page on X (Twitter), and none of them had the courage to face me. I wish one of them would come out of hiding and tell me what I did wrong in my tests. Show me the scores they’re getting. Reviews are sold to the highest bidder deal with it. Also, PCGH showed the 14600K beating the 9700X, so why isn’t that the case in other tech site charts?",Negative
AMD,"at first I tested 4080 1080P ultra on these processors on the same DDR5 7200 im using on the 14700k but it didn't make a difference tried the 2 profiles 7200 xmp and expo CL38, so when I got the 4090 I decided to use whatever available but that wouldn't make a difference, I could use DDR4 on 14700k and still win in every game against non 3ds,   here is the test on 14700k 5600 CL46 which got me around 185FPS vs 145FPS for zen4/5 non 3ds.  [https://www.youtube.com/watch?v=OjEB7igphdM](https://www.youtube.com/watch?v=OjEB7igphdM)  also here is 14700k +DDR4 3866 +4080 for RE4 still faster than 9700x+4090+DDR5 6000  143FPS vs 130FPS  [https://www.youtube.com/watch?v=lxZSMJnkYNA](https://www.youtube.com/watch?v=lxZSMJnkYNA)",Neutral
AMD,"Did you get any more FPS when you tried it? I’m getting good frames without it, so I don’t think it’s worth it. Also, in SoTR, even though the 9800X3D scored higher in the benchmark, there weren’t any extra frames compared to the 14700K during actual gameplay. I tested multiple locations, and the performance was the same, the higher score on the 9800X3D came mainly from the sky section of the benchmark.",Negative
AMD,did you do it,Neutral
AMD,I was at:  RT Ultra: 118.78 Ultra: 182.42 High: 203.76  https://imgur.com/a/edaNmHQ,Neutral
AMD,can you reset settings then choose ray tracing ultra preset.,Neutral
AMD,here is w10 vs w11 same scores in every single game.  [https://www.youtube.com/watch?v=vQB4RrNDzyM](https://www.youtube.com/watch?v=vQB4RrNDzyM)  [https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s](https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s),Neutral
AMD,because they exclusively exist in DIY build your pc enthusiast bubble,Neutral
AMD,Pricing was aggressive. A 12 core 3900x was 400 usd.,Neutral
AMD,">And to add to that, which often gets omitted. Is that Comet Lake is only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, Zen 2 in my mind actually makes more sense.",Neutral
AMD,literally multiple videos from him even with a 7800x3d and still 14900ks can only eek out wins in like 1-2 games like congrats you dropped 500$+ on the RAM alone and 720$ on hte motherboard to run that RAM just to lsoe to a 7800X3D,Neutral
AMD,">It is a common knowledge at this point,  It's not an argument.  Only objective way that you have to prove that those sources are spreading misinformation, is to prove it and share it with others - do it, make your own research and share it on Reddit and YouTube, if you're really correct and your data is accurate, it will hurt public reception of these sources(HUB, LTT, GN) and will damage their credibility.  >Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong.   [No true Scotsman - ](https://en.wikipedia.org/wiki/No_true_Scotsman)sorry dude, but your argument is logically incorrect, for pretty obvious reasons - it's unfalsifiable, it lacks evidence and it assumes the conclusion **- ""every dude"" argument is a way to sound like you have the backing of a consensus without having to provide a single piece of evidence for it -** as I said, real evidence in your case would be substantial evidence that proves that sources that you named in your comment are spreading misinformation, which shouldn't be hard to test and release in public.  I don't like to be a part of emotional conversations, I honestly hate those - if we want to speak facts, share facts with me, if you want to speak emotions - find another person to discuss them, I don't like emotions when it comes to constructive discussions.",Negative
AMD,"Okay, I did it",Positive
AMD,"No, I didn’t remember good",Positive
AMD,"I did. I ran the benchmark a couple more times and the results were similar. APO on, 200S Boost enabled, no background tasks running except Steam. I think it's a GPU bottleneck.",Neutral
AMD,Thanks for solidifying opinion that your benchmarks are fake,Positive
AMD,"Ddr5 7200 cost as much as 6000, i bought mine for 110$ from newegg, I should have got the 7600 for 135$ but I was a bit worried about compatibility, my z790 mb cost me 200$, and on my 14700k on a 4090 it matched 9800x3d in 3 games lost in re4 and won in HZD remastered, ill test more games in the future but since i won in one game to me 14700k+D5 7200=9800x3d, 14900k+ddr5 7600 could be faster, i dont care about any video online or chart, i only believe in built in benchmarks scores and what my eyes see.",Neutral
AMD,"if there isnt cpu bottleneck your score should be around 135fps, can you give it one last try with sharpness set to 0.",Neutral
AMD,"What’s fake about it? What scores are you getting? Prove me wrong with score screenshots, not with fanboyism false accusations. If you’re comparing score screenshots to OBS-recorded videos, you shouldn’t, because OBS uses about 15% of GPU resources, which reduces the scores slightly. I’ve been talking about this and calling out tech sites and tech tubers on Twitter and Reddit for over a month. Do you really think they’d stay silent all this time if I were wrong? If I were faking benchmark scores, they would have publicly shown their results and shut me up for good, especially since I’ve been accusing them of corruption on a daily basis. Just yesterday Tech power up is the first techsite to block me on X for sharing my benchmark screenshot on their page, cowards, shouldn't they discuss what did i do wrong in my benchmark, then again what are they gonna say AMD paid us too much money that we need 10 years in hard work to make that much.",Negative
AMD,Cam someone confirm or is this gas lighting?,Neutral
AMD,This is nice honestly. Points out some nice stuff about ARL besides the usual hate on the internet,Positive
AMD,"The Ultra 7 265K is absolutely a better deal than the non-X3D 9700X or 9900X. The Ultra 9 285K is competitive with non-X3D 9950X. If 3D-Vcache offerings from AMD didn't exist, Intel would likely be considered the go-to for this gen.",Positive
AMD,Intel comeback real?,Neutral
AMD,"How come the non-k variants of ultra 9,7, and 5 have a base power of 65 watt, but the K variants have a base power of 125 watts? The only difference I see between both variants, besides base power, is the 0.1-0.2MHz clock speed increase. Am I missing something here?",Neutral
AMD,3D v-cache has entered the chat.,Neutral
AMD,Take it as a grain of salt. Intel marketing LOL,Neutral
AMD,Does anyone have the Intel Ultra 9 285k and can test all of this ?,Neutral
AMD,Thats cool ...but lets talk about better pricing.,Positive
AMD,"I got the 7800x3d and the 265K, the 265K got higher fps. Also when i watch twitch streamer i Look at the fps if the streamer is useing a 9800X3D. In CS2 the 265k got 50-80 fps less then the 9800x3d  (500 vs 580 fps) in cs2 Benchmark the 265K is consuming about 100 watts",Neutral
AMD,Tech Jesus has entered chat :).,Neutral
AMD,"Intel still trying to sell their already abandoned platform, they should learn something good from AMD and provide support for three generations per socket (and yes, Zen 6 will use AM5, already confirmed by AMD).",Neutral
AMD,"Two things can be true at the same time. Arrow lake has legitimately improved since it launched last year, and Intel is portraying it in the best, most optimal light they can that is probably not representative of a broad spectrum of games reviewed by a third party.   It would be interesting to get a retest of arrow lake now, but I dunno if it is worth the time investment for some reviewer like TPU or HWUB to re-review a relatively poor platform that’s already halfway out the door.",Positive
AMD,"Arrow Lake performs quite well at general compute and power efficiency compared to prior generatons ans even against AMD.  Where they have had trouble is in specifically gaming apps, particularly against the X3D variants (which it should be said drastically perform non-X3D AMD parts in gaming). And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  This led to Intel being dismissed out of hand by gaming focused YouTube reviewers like HUB, GN and LTT despite them being perfectly capable parts outside of gaming, simply because they did not place well in gaming benchmarks in CPU limited scenarios (e. g. with a literal 5090).  HUBs own benchmarks, btw, show an utterly inconsequential difference between a 9700X part versus a 9800X3D part when paired with anything less than a 5080/4090 in gaming. Most people with most graphics cards wouldnt see the difference with their card between the various high end CPU parts unless theyre literally using a $1000+ GPU. Ironically the 9070 performance was identical with a 9700X and a 9800X3D.  So it's actually kind of the reverse, gaming reviewers painted Arrow Lake in the worst possible light.",Neutral
AMD,What do you mean by gaslighting in this case?,Neutral
AMD,"The answer depends on whether or not you're a ""next-gen AI gamer"" (apparently that means 1080p avg fps with a RTX 5090).",Neutral
AMD,I doubt they would give false numbers. Those are probably what you get with the setup they list in fine print. Also they pretty clearly show the 3dx parts are faster in gaming. But they have probably picked examples that show them in a good light.   There is nothing wrong per se in the arrow lake lineup. In a vacuum if AMD and intel launched their current lineup now they would both look pretty competitive. AMD zen5 was also a bit of a disappointment after all. The issue that destroyed the image of arrow lake is that the previous generation was often faster in gaming. That made every reviewer give it bad overall reviews.,Neutral
AMD,Wait till refresh comes out in December... Memory latency lower and performance boosted. Price also lower... Wish it could come to the old chips but it architecturally cant. :(,Negative
AMD,"generally they cherry pick, so maybe the demo they use is process heavy with minimal I/O (arrow's biggest performance hit is memory movement). if you manage to keep memory movement low core performance is actually really good on arrow lake. it just gets massacred by d2d and NGU latency, which they don't get a pass for but its unfortunate that decent core designs are held back severely by packaging.",Neutral
AMD,"there is just no way dude lol  even their comparisons make no sense, 265K = 9800X3D / 265KF = 9700X? lol? how would that even be possible",Negative
AMD,Honestly it's hard to get real conversation with people who fanboying Amd so hard. Why people can't be normal when we talk about Intel and Nvidia? *smh,Negative
AMD,"The base clock rates, my assumption how it boosts during heavily multi-threaded tasks, is like 60%-80% higher on both core types.",Neutral
AMD,Nova Lake bLLC about to ruin Amd X3D party.,Neutral
AMD,Look at the last slide. Intel's source is TPU's review data which if anything is a less than ideal config for arrow considering it's only DDR5-6000. I'm honestly surprised they didn't pull from one of the outlets that did 8000 1.4v XMP+200S boost coverage.,Negative
AMD,I always wondered if Intel marketing budget is higher than the R&D budget,Positive
AMD,Intel Arrow Lake is much cheaper than Amd Zen 5.,Neutral
AMD,Genuine question how often do you or even the average person keep their motherboards when upgrading the CPU? IMO the platform longevity argument is stupid,Negative
AMD,only an AMD fan would worry about replacing their shit CPUs under 3 years,Negative
AMD,Hardware unboxed isn't a reliable source.,Neutral
AMD,"As a real world example, I upgraded my primary system from a 14700k to a 265k recently. My other desktop is a ryzen 7900 (non x, 65 watt tdp)  Compiling my os takes 6 minutes on the 7900, 10 minutes on the 265k, 10 minutes on a 3950x, and 16 minutes on the 14700k.   That is not in windows, but my os which does not have thread director or custom scheduling for e cores. (So going to be worse than windows or Linux)   All the numbers except the 7900 are with the same ssd, psu, custom water loop.  The motherboard and cpu changed between Intel builds and the 3950x was the previous build before the 14700k.   The e core performance significantly improved in arrow lake.  It’s now keeping up with 16 core am4 parts in the worst possible scenario for it.  If the scheduler was smarter, I think it would be very close to the 7900.   In windows, it’s pretty close in games I play on frame rate to the old chip. A few are actually faster.",Neutral
AMD,"> And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  Yeah, 10% less performance when frequency matched is quite a lot.",Neutral
AMD,Telling people that its performance is better than it actually is?,Negative
AMD,The ones with similar pricing not performance,Neutral
AMD,"True, except AM5 supported all cpus 7000, 9000 and will support the newest one. So not exactly true per se, because you need a new motherboard every time and those on AM5 can upgrade cpu without buying a board. Imagine if someone was on 13400F and wanted to buy lets say 285K? Well, you need a new board. So it depends what “cheaper” is seen as.",Neutral
AMD,"Yep. We all see the patterns here. Gamers nexus, HUB, LTT and most big name they all keep mocking Intel, even when Intel did good job to bring improvements to their products like fixing Arrow Lake gaming performance but those ""tech reviewer"" decided to be blind, they don't even want to revisit Intel chip and benchmark it, they don't even want to educate their viewers.    At the same time they keep overhyping Amd products to the moon like they own the stock, it's obvious now who is the real reviewer and who is fraud reviewer using their own popularity and big headline to spread their propaganda for their own benefits. They are just as worse as shady company!",Negative
AMD,"Platform longevity isn't stupid lol. There's a reason why am4 is one of the best selling and one of the best platforms of all time. The fact that you can go from a 1700x to a 5800x3d on the same mobo for 85% more performance is crazy good value. You can say that all you want but there's a reason why in the diy market everyone is buying amd over intel. People know arrow lake is doa where as am5 actually has a future, which is why everyone is flocking to the 9800x3d in droves.  Zen 7 is rumored to be on am5 as well. Meaning that am6 would come out in 2030/2031. If that's true am5 lived for around 8/9 years.  That's insane value to get out of one socket.",Positive
AMD,Quite common for AM4 in my experience.,Neutral
AMD,"My brother went from 1700X to 3700X to 5800X3D on AM4, so 10 years next year? I plan on upgrading to 3D cpu probably 9600X3D once its out or maybe 7800X3D (something more affordable but still somewhat better). Kinda want better 1% lows tbh, but maybe I’ll just wait for next ones unsure.",Neutral
AMD,"How is it stupid? On one hand, intel sockets used only 3 gens from 12th-14th while on the other hand amd has not only used 7000-9000 but will also be using zen 6. I don't think you realize just how big of a hassle changing motherboards are",Negative
AMD,"I upgrade my motherboard usually because of cpu socket. Thinking of going AMD. I build my PCs for myself, my wife and my kid. Lots of floating parts in this house could use the platform longevity.",Neutral
AMD,I originally had a Ryzen 5 3600 and am now on a 5700x3d so it’s pretty relevant I’d say. Of course it’s anecdotal but I don’t think it’s too uncommon,Neutral
AMD,"I agree with you, it’s daft, no doubt about it. But you’ve got to factor in that people can be daft too. Most tend to skimp, only to end up buying another middling CPU the following year, and then again after that. And so the cycle trundles on.  When I buy PC parts, I usually go for the best available. Provided the price is reasonably sane. I picked up a 9900K seven years ago and it’s still holding its own. Is it on par with the 9800X3D? Of course not. But it’s still a very capable bit of kit. Next time I upgrade, I’ll go top shelf again.",Neutral
AMD,"They frequently revisit products and test old hardware, but naturally that's not going to be their focus. Channels with that focus like RandomGaminginHD fill that niche.",Neutral
AMD,Sooo they are in the YouTube space for the money not for the love of tech,Neutral
AMD,"So true. After all they are reviewer, it's their job to revisit something if it's worth to benchmark due to improvements.   Didn't they said they want to educate their viewers? If so then why being lazy? This is obvious they just care about making big headline which makes more money for them. It's BS!",Negative
AMD,"If only the Arc B770 can beat the RX 9070XT with performance and features, then I definitely stay on Intel's side, but RN I doubt it",Neutral
AMD,"Even accepting that as true, I don’t think it changes the main point I was making. Arrow Lake has probably improved versus a year ago. It’s probably not as good as intel is making out. A good third party reviewer would be needed to determine by how much, yet it’s probably not worth their money and time to test it.   So the truth is probably in between what Intel is saying here and what the state of things was 6 months ago.",Negative
AMD,Isn’t that just either lying or exaggerating?   I know those words don’t “go hard” and are “low key” boring.   But I think some internet buzzwords are just overused or badly applied.,Negative
AMD,"Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Negative
AMD,The 5800x 3d costs anything between 450 (time of launch) and 300. With that kind of money - you can literally buy a cpu + a mobo bundle. There is no value in the platform upgradability argument. You can literally buy a 13600k and a brand new mobo for cheaper than the cost of the 5800x 3d alone.,Negative
AMD,YouTube has created an entire industry not just for gaming but for everything else where all these people rely on the YouTube algorithm and have figured out how to exploit it and bitch and complain whenever YouTube tries to change things up.  We need a new platform based on YouTube but built by technology like ipfs and run by the internet community.,Neutral
AMD,"No other card in recent memory has been more overhyped than the 9070 XT , we should be getting a B770 before 2025 ends. Make sacrifices to the Silicon Gods and perhaps your wishes will come true.",Neutral
AMD,"The only things that have changed since launch is the bugged ppm driver on windows causing extremely low clocks under moderate load (games being the main example), and 200S boost making d2d/NGU OC a single click for people not willing to type in 32 in the NGU/D2D ratio boxes. The current performance on arrow is achievable on the launch microcode.",Neutral
AMD,Sure but charts seem about right to me,Negative
AMD,APO is game specific. I'm referring to what has changed overall.,Neutral
AMD,"I'm unable to claim my copy of Battlefield 6 from the recent Intel promotion even after redeeming the code.  I redeemed my promotional code last month which added the ""Intel Gamer Days"" bundle to my account that includes Battlefield 6. The instructions say to return October 3rd to claim the key. I've been attempting daily to claim the key but just keep getting the message ""We are temporarily out of codes and are working to reload our system. Please check back soon.""  The game launches tomorrow so this is pretty frustrating. Is anyone else having an issue claiming their BF6 key?  I put a ticket in with Intel support but haven't heard anything yet.",Negative
AMD,"bought the newegg bundle. I got a master key but my PC is running linux. The linux HST tool doesn't even have an extension, no .exe no nothing. I don't know how to run it.",Negative
AMD,"Hello there, it seems that after updating my Windows 11 I can't scan for new intel drivers by using the Intel Driver & Support Assistant. It scans for a while but then it gives me the message: ""Sorry, something went wrong while trying to scan""; it seems to affect the Intel Graphics Software as well as it keeps looking for updates without success. I've already tried to reinstall DSA and deleting any folders associated with the program, rebooting Windows and also uninstalling the Windows Update. Any thoughts?",Negative
AMD,"After updating to W11, these awful KILLER apps and processes have returned and reduced my upload speed to less than 1mb!!! Normally I get 30mbs up.   Dell PC, 32 gbs RAM, Intel i9. x64 W11 Home 24H2, OS 26100.6584, WIRED Ethernet direct to router/modem.  The Killer thread is closed and directs users to post in this thread.   The same thing happened in W10 on same machine but problem was fully solved by the KillerSoftwareUninstaller tool, which also disabled Killer permanently from coming back on every single update.  However, I tried the same tool after the update to W11 and it removes Killer, and everything works better for a day or so, then suddenly everything slows down and Killer is back.  PLEASE help me get this awful suite of useless Killer tools off of my machine forever!   Thank you!",Negative
AMD,"I have a i7-13700 that may be the root of my issue. Long story short, MATLAB is getting an Access Denied error when trying to link to an Imperx framegrabber using videoinput. Imperx FrameLink works fine. The function that it crashes at works fine when called from a different way (getAvailHW), and only crashes when trying to call it through videoinput. The exact same setup works just fine on another computer. Even when no cameralink cable is attached, it crashes on this one. BIOS is fully updated. The CPU passed the checking program Intel provided.   Looking up this issue, people had similar problems, and they linked it to the CPU issues with 13th and 14th gen. Intel released a chart for what settings you should have in BIOS, but it does not cover the 13700, only the K variant. What settings should I have in BIOS to rule out Intel?  Chart: https://community.intel.com/t5/image/serverpage/image-id/56057i81282C3BCB9162A9",Negative
AMD,"Is this normal activity for a 265K at Idle?  https://imgur.com/SSSOB9F      The multiplier and clock speed keeps jumping around all over.  There are no background processes running, nothing in task tray, closed all applications except HWINFO to record this.      I am running into an issue where I feel like either my CPU or my AIO is bad.  The build is new, completed yesterday.  Idle temps hang around 30-34C; which seem fine.  But as soon as tasks start happening, it jumps all over up to 80C.  I have a 360 AIO on it.  Gaming even, it hit 83C earlier.  It doesn't make sense to me.  I have built a few 265K rigs and not had this kind of fluctuation in temperature before.  Even running cinebench on previous builds, the CPU never went above mid 70s.  But this newer machine is hitting 80+ in games.  Previous 265K PCs have ran mid 50s in games.  Sometimes 65 depending on the load, but never 80+.",Negative
AMD,"Hi u/Mydst For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues. They'll be able to assist with your Battlefield 6 code availability and on how to resolve this issue.   You may also check these links:  [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Neutral
AMD,"Hi u/afyaff For this kind of inquiry, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry and they may help you on how to input the master key in your PC.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Neutral
AMD,"u/Adrian-The-Great  For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues .They'll be able to assist with your non-working BF6 code from the Newegg bundle promotion.     Additional Information:   [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Neutral
AMD,"u/Broad_Remote3101 This could be a compatibility issue or something with the installation process. Just to help figure this out - are you using a laptop or desktop? If it's a laptop, could you let me know the exact model? Also, what graphics card and processor do you have?  You might want to try following this article too and see if that does the trick. Let me know how it goes!  [Intel® Driver & Support Assistant (Intel® DSA) Results in “Sorry,...](https://www.intel.com/content/www/us/en/support/articles/000026895/software/software-applications.html)",Neutral
AMD,"u/musicmafia77 Based on what you're describing, this sounds like a classic case of Windows 11 being overly ""helpful"" with driver management. To get to the bottom of this and find a lasting solution, I'll need some specific details about your setup.   First, could you check Device Manager and tell me the exact model of your network adapter and current driver version? Since you mentioned you're using wired Ethernet, I'm curious if this is actually a Killer-branded Ethernet adapter. Also, when Killer inevitably returns, which specific processes are you seeing in Task Manager - things like ""Killer Control Center"" or ""Killer Network Service""?  Here's what's likely happening behind the scenes: Windows 11 maintains a driver store cache that can automatically reinstall what it considers ""preferred"" drivers, even after you've removed them. The system recognizes your Killer hardware ID and keeps pulling that branded driver through Windows Update, completely overriding your manual removal efforts. This explains why the KillerSoftwareUninstaller works temporarily but then everything reverts after a day or so.   To truly fix this, we'll probably need to perform a clean driver installation process that involves purging the entire driver store of Killer-related files, blocking Windows Update from auto-reinstalling them, and potentially replacing them with Dell provided drivers rather than Intel generic drivers (since Killer hardware is actually based on Intel chipsets). It's more comprehensive than just running the uninstaller, but it should give you that permanent solution you're looking for.",Neutral
AMD,"PL1 should be  65W , PL2 is 219 W you can cross reference this value on this specs https://www.intel.com/content/www/us/en/products/sku/230490/intel-core-i713700-processor-30m-cache-up-to-5-20-ghz/specifications.html   However following intel recommended settings once you load BIOS with microcode patch it should follow the default, you may check also with your motherboard manufacturer",Neutral
AMD,"Update your bios to the latest version and set it to intel default settings. If the issue is the same, request a replacement",Neutral
AMD,"Thank you so much for your help!  The ethernet adapter is Killer E2400 Gigabit Ethernet Controller.  I am not super tech savvy, but what you are describing makes sense and is consistent with what I've read about this problem. I will need simple step by step instructions to fix as you suggest. Thanks again!",Positive
AMD,"u/musicmafia77 After checking our records, I found that the unit you're inquiring about has been discontinued-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html). While direct support is limited to self-help resources and community insights from other users, I'd be happy to extend some steps that should help resolve your issue.     Based on your initial description, here's what I recommend:     First, please review this article-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html) and follow the troubleshooting steps provided here-[Clean Installation of Wireless Drivers](https://www.intel.com/content/www/us/en/support/articles/000022173/wireless.html). Once you've updated to a newer wireless driver, I'd suggest reaching out to Dell Technologies for assistance with downloading the appropriate OEM driver for your specific system.     If you prefer a more direct approach, you can:  * Visit Dell's website and download the wireless driver for your adapter by entering your service tag in their support portal * Alternatively, run Dell SupportAssist after completing a clean installation of the wireless driver     For additional troubleshooting, here's another helpful article that covers complete removal of Killer software issues, which may be relevant to your situation.-[How to Perform a Clean Installation to Solve Most Intel® Killer™...](https://www.intel.com/content/www/us/en/support/articles/000058906/wireless/wireless-software.html)     If you've tried these steps and the issue persists, I'd recommend coordinating directly with [Dell's support team](https://www.dell.com/support/home/en-us), as they're best equipped to assist with OEM-specific products and can provide more specialized guidance for your particular system configuration.",Neutral
AMD,"Most commenters are wrong or didn’t read all the articles. Nvidia wants some custom SKUs of server chips to pair with its data center GPUs, and Intel wanted a better solution for pc parts for the GPU tiles.   NVIDIA is NOT getting an x86 license, nor do they want one. In case you haven’t noticed, Nvidia is a $4T company largely on sales of data center GPUs, sales of which were 10x the gaming segment. Profit/earnings ratio of data center is even higher than the 10x.   In my opinion, the only reason for Intel to use RTX chiplets on pc parts is so they can have a few skus for mobile gaming. Lower end laptop chips would still use Xe cores.  This solves NONE of intel’s structural issues - they need, in this order  1. Major foundry customers  2. Regain server market share 3. Data center GPU offerings 4. Move all pc chiplets back to Intel nodes and start to fill up the fabs again",Neutral
AMD,An Intel + nVidia handheld would be something I would welcome just for DLSS4 alone.,Neutral
AMD,"I don’t think it will change that much overall.  It’s no secret that NVIDIA wanted a x86 license for decades now and now they basically got it. This will ""hurt"" AMD by offering competition to markets where AMD currently holds a monopoly like with Strix Halo, most handhelds and consoles.  I don’t think we’ll see big changes on the Intel side.  I don’t think this will affect neither the iGPUs nor ARC because why would it.",Negative
AMD,"this jointventure is NOT about consumer gpu and does not change anythig about gaming gpu of intel, amd or nvidia.  this is instantly obvioiusly from just reading the headlines of the news announcing this.   this tumbnail is just hub using this news to suggest jet another time that maybe amd might just be screwed. they do this because they are neutral in every way.",Negative
AMD,"i think it's bad for us, consumers",Negative
AMD,Was the team up really to crush AMD or Nvidia's answer to enter China?,Neutral
AMD,AMDware unboxed only cares about AMD anyway,Neutral
AMD,This hurts the arc division way more than this could ever hurt amd.,Negative
AMD,They will crush user's wallet,Neutral
AMD,you guys realize Nvidia buys all their stuff from TSMC the same company that makes all of AMD's stuff...So now intel will buy from them too since they cant compete? Because no amount of Intel+Nvidia humping is going to make a TSMC baby...you got to pay to play.,Negative
AMD,"Even assuming they made monster co-op AMD still has own ecosystem of GPU and CPU, along with consumer support. It's Radeon and Ryzen are still very strong consumer wise. So no, AMD is not screwed",Positive
AMD,Remember Kaby Lake G? No? This will also be forgotten soon.,Negative
AMD,Yes.,Positive
AMD,Intel partners with UMC of Taiwan the third largest foundry in the world.    What does it mean for UMC and Intel?   How will it affect transfer / exchange of technology between Taiwan and Intel?   How does this affect TSMC position with USAG?,Neutral
AMD,Foveros baby!,Neutral
AMD,AMDUnboxed on suicide watch.,Neutral
AMD,"For mobile/laptop performance, yes AMD is in the dust, maybe? Makes sense since AMD GPU division is mainly focused on the console partnership with Sony/Microsoft--leaves the mobile platform wide open.",Neutral
AMD,"Who knows, time will tell. But if AMD does find its back against the wall, this is when it does its best. This should be interesting to say the least.",Positive
AMD,"The last time NVIDIA made onboard graphics for a PC, they could hardly be considered graphics. The 3D performance was better than Intel, but features like CUDA, and a good handful of video decoding capabilities were missing. You were much better off getting ATi onboard graphics back then inside of an FX or AMD Bullerdozer chip.  In more recent times, this also applies to laptops with the low end MX chips. Many lack encode/proper decode support, and they are effectively just 3D accelerators. The Intel Onboard Graphics would be a much better choice overall. Whereas an AMD APU would give you the full suite of features.   I'm not entirely worried for AMD. It just means if Intel and NVIDIA manage to not screw up onboard video, AMD will just need to make sure Radeon can remain competitive. I know they have a lot of tricks up their sleeves still, like designing HBM memory onto the die (how Vega was supposed to be originally) and unified memory (soldered down, see the new AI branded chips).",Negative
AMD,Ohh noooerrrrrrrrr,Neutral
AMD,"Actually this may be good for AMD and I will tell you why: Open source drivers. Nvidia if they had a say in intels graphics division would make the intel GPU stuff proprietary and locked down and when support ends its gonna hurt. Meanwhile AMD has open source drivers and in the age that linux is gaining popularity will make it the preferred platform for anyone interested in the growing linux ecosystem. Nvidia will NEED to change how they handle things or intel may not be as attractive as they once were, intel has already suffered a massive knock on thier reputation and this may be the mortal wound that can work in AMD's favor.",Neutral
AMD,"I don't see how this means anything at all for AMD in desktop. This is borderline irrelevant for desktop gaming. Most users pick up an Intel or AMD CPU and an NVIDIA dGPU.  APUs don't really exist in desktop gaming.  In mobile? I think most people won't care much. Intel and AMD APUs both offer compelling and reasonable alternatives for everyday usage. Anyone hoping to game on a laptop as their primary device will choose a model with an NVIDIA dGPU.   On handheld/console, AMD already has the largest market saturation, except for Switch, where Nintendo/NVIDIA opted for an ARM solution.  I don't see this APU being much more than a side choice that offers at best a reasonable alternative to a high end AMD APU, perhaps in the relatively small handheld market, if the performance is significantly better than existing AMD and Intel APUs, due to the DLSS support in a mobile form factor where a dGPU is not a realistic option, and upscaling is necessary for reasonable performance in a limited power package.  In all other cases I dont see this being a compelling option.",Neutral
AMD,So either a 200w igpu with a 300w intel cpu or an intel cpu with a 5030 slapped on it?  I think amd will be fine,Neutral
AMD,welcome to the Nvidia and amd duopoly,Neutral
AMD,"I’m really curious what the power efficiency ends up looking like here - mobile performance/W has continued to be very good in recent Intel platforms, and Nvidia is the market leader in that regard among gpu IHVs. But until now, Nvidia chips being intrinsically dGPUs has completely neutered any potential mobile power efficiency or handheld capability - but not anymore if such a thing can be a chiplet/tile! Interesting days ahead…",Positive
AMD,"By that time amd will have already made an FSR4 capbable handheld, so no significant change",Neutral
AMD,"It won’t affect either cause on parts that don’t need a nvidia igpu why raise the price with it when arc igpu is really good as is, maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive",Neutral
AMD,a partnership doesnt mean they get free reign over license lol,Neutral
AMD,"AMD already has the next gen of consoles. Except for the Switch, which NVIDIA already has (with ARM, not with Intel).",Neutral
AMD,"None, It's a way for NVidia to have another product line and stop Intel from having a competitive product line.",Neutral
AMD,"Not to crush amd, but this essentially just kills arc and hurts the consumer market more. You won't be getting any of those 250 dollar 12gb gpus anymore.",Negative
AMD,Why would it?,Neutral
AMD,"It has nothing to do with the dgpu product stack. Nvidia makes very little from the diypc market. Way more comes from prebuilts, laptops and 90+% from data centers and other enterprise use. In a very real way, the arc cards are not a competitor to Nvidia.",Neutral
AMD,"Unfortunately no matter what given Intel's current state the division is going to suffer. Intel cannot afford to invest in products that will take a very long time to generate profit or may never generate profit. If even AMD cannot compete with Nvidia, then Intel will likely never be able too either",Negative
AMD,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Neutral
AMD,Past != Future,Neutral
AMD,"nvidia also used to make motherboard chipset, with mixed success.",Neutral
AMD,FSR 4 looks like the later versions of Dlss 2 did,Neutral
AMD,"This ""insignificant change"" we definitely want though.  Competition is good in a stagnant market...",Positive
AMD,"Other than Halo SKU which we are yet to establish any trend of, AMD notoriously drags their feet. RDNA3 will be with us for a while",Negative
AMD,Wont this be targetting the ML workstation space where you have the iGPU with access to >128GB of RAM like Strix Halo with DMA and/or NVLink banswidth?,Neutral
AMD,">maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive  Possibly an overlap like this would be a waste of money developing, and tbh I highly doubt customers would go for the Intel option rather than a Nvidia one, *even if* the Intel option is better performing, because of Nvidia's insane mind share.",Neutral
AMD,Never said that.,Neutral
AMD,Also a way for Nvidia to invest some of their funny money (which is essentially infinite) into a cheap company. Intel under $20 was ludicrous. Intel is still at book value at $29.,Neutral
AMD,Why do so many people think that this will kill ARC?,Negative
AMD,The market for Arc is the same as for Nvidia.,Neutral
AMD,Was that back in the MacBooks having the NVIDIA 9400m paired with a Core2Duo? Those were solid.,Positive
AMD,"No, but it is a reminder on shortcomings to watch out for. The Intel + AMD Partnership with the Intel NUCs several years ago made for some pretty compelling hardware. I still have a NUC8i7HVK in service which works like a champ.  If we are talking about current day NVIDIA with their Discrete Graphics (not the MX budget crap) then their GPUs are obviously smoking anything AMD and Intel. That doesn't resolve some of the Linux problems that still remain with NVIDIA, and that's not going to stop some of the BS NVIDIA does with their hardware around NVENC/NVDEC stream limits. But they have raised those limits, and the open NVIDIA drivers are getting better in time.",Positive
AMD,"Yup. Their nForce boards were great for overclocking, but rather buggy too.",Positive
AMD,"It's not a later version of dlss 2, it's between dlss 3 and 4. Just go watch some videos comparing both.   And btw redstone is coming probably at the end of this year which will improve ray tracing quality and frame gen performance.",Neutral
AMD,"No reputable reviewer thinks that. Most I've seen say it's better than DLSS 3, and very slightly behind DLSS 4.  Why lie about this?",Negative
AMD,They literally haven't said a word about it everyone's just speculating off of Moore's laws anti Intel videos lol consumer is the last thing they care for atm and might be server first cause that makes the most financial sense,Negative
AMD,If they do that in the case the two companies deal fall apart Intel is screwed with no graphics to compete with even the basic amd ones,Negative
AMD,"Well phrased, They now have a seat at the table of Intel to sway votes enough to dissuade them from making competitive products",Neutral
AMD,"Too many people are peeping that MLID podcast. Dude there is very confrontative about his theory that ARC is being ""effectively"" canceled because of some very early roadmap not paying out. Everything is a confirmation bias at this point.   I on the other hand don't get why would Intel forfeit it's competitive advantage with GPU accelerated AI, XeSS or QuickSync and made itself dependant on nVidia. Doesn't make sense especially now, when they have significant influx of cash from both Nvidia and government.",Negative
AMD,"Arc was already not doing well long before. Pouring millions into their gpu division every year with no returns. Their own road maps had their gpus coming out a year sooner but everything got delayed with them resizing the company. Like this deal is not exactly what killed arc, but arc was already struggling beforehand so this was most likely the nail in the coffin.",Negative
AMD,Nvidia does not have an A310 competitor.,Neutral
AMD,But I still remember how many motherboards with Nvidia chipsets I replaced way back.  They used to reliably overheat and need a reflow.  I think it was the HP DV6000 and DV9000 series.,Neutral
AMD,I have not lied,Neutral
AMD,"While workstations dont directly contribute a big chunk of the sales, I suspect they dont want to yield their foothold on the ML/Data Science guys.  If they start getting familiar with non-nVIDIA ecosystems, it could mean indirect losses when these guys start procuring non-nVIDIA dGPUs in servers when they do port their workstation experiments to ""larger"" hardware now that AI is all the rage.  B60 is gaining momentum on homelabs, and Strix Halo to some extent getting popular for those needing more memory for their larger datasets.",Neutral
AMD,"MLID has used clickbait trash thumbnails/titles way too many times, no one should watch that garbage channel to begin with.",Negative
AMD,"I think Intel priced in multiple generations of taking losses when they decided to enter the GPU market. Essentially impossible to walk in to a mature duopoly market and immediately start turning profit as a new player, even with Intel's resources.  Of course I think they also had much rosier projections for their other ventures when they started this journey, so who knows if the GPU losses are still tolerable.",Negative
AMD,Intel doesn't have a current gen A310 competitor either.,Neutral
AMD,Theres no way that nvidia is gonna let intel keep producing entry level gpus that directly compete with their own. I know their deals are in regards to apus and to compete with strix halo but nvidia is not gonna invest into a company with a competitor to their gpus. Like a b580 destroys the 5060 and 5050 while costing less or the same amount. There's no way nvidia invests 5 billion without some agreement that kills off arc as long as they are together.,Negative
AMD,Their nForce boards were excellent for overclocking and tinkering! That's for sure. I also remember them being a bit buggy lol.,Neutral
AMD,IIRC that wss the AMD Turion X2 + NVIDIA 7300Go laptops. Those were notorious for dying.   I also remember some budget laptops with the Turion X2 + ATi 3200 chopsrt graphics would die due to graphics desoldering off the board. The GPU would sit at the end of the heatsink and burn up.   The 9400M MacBooks I remember would get hot enough to start leaving powder from the chassis on my desk lol.,Neutral
AMD,"You did tho. It doesn't look like ""later versions of DLSS 2"". Every reputable source has said it's better than DLSS 3 and close to DLSS 4",Positive
AMD,"They're already at 2, going on 3, generations of losses though (Intel discrete GPU's basically have 0 marketshare, even worse than AMD who are also doing really badly) so even if that were true that isn't a reason to be optimistic.  As a company they also seem to be desperate to get income as well and so there is no reason to assume they're going to tolerate that situation for all that much longer.    I doubt they'll entirely kill off their GPU division, they can still do iGPU's which will be good enough and cheaper than paying NV a royalty, but the discrete GPU line is clearly in trouble.",Negative
AMD,A310 is cheap enough and good enough for it's purpose. If it ain't broke why fix it,Positive
AMD,"Wrong man. You just don’t understand how small the diypc market is compared to prebuilts, laptops, and enterprise- it’s a fraction of one percent.",Negative
AMD,The later versions of Dlss 2 look like Dlss 3,Neutral
AMD,Nvidia probably feels the same about their low end SKUs.,Neutral
AMD,"The DIYPC market largely drives the “prebuilt” market, with a 1-3 generation lag.  Many people who buy “prebuilts” have people they go to for advice (i.e. they talk to their friends/family members who DIY). When there’s a big swing in the DIY market, it typically ripples out to the prebuilt market over a few years/generations.",Neutral
AMD,"Lol, what an explanation hahahaha   And that's still not ""better than DLSS 3"" as most say",Negative
AMD,Yeah lol,Neutral
AMD,AMD Unboxed say anything about that? What about Linus Cheap Tips!,Neutral
AMD,"""Everyone I don't like is biased""-ass answer",Negative
AMD,"can someone explain how this might effect me, as a long term holder, as if I am a golden retriever?",Neutral
AMD,The MTL/ARL/LNL Dates are all wrong for CPU launch it's an OEM roadmap not Intel.,Negative
AMD,Medusa Ryzen... Sounds like something I'd rather avoid in real life.,Neutral
AMD,Panther Lake is slipping to Q2 2026 now?    It was originally supposed to launch in Q4 2025,Neutral
AMD,Just hodl until you get the biscuits,Neutral
AMD,"If leak is correct it's bad news.   Panther lake q2 2026, Nova lake q2 2027.    It means 18A will arrive later than TSMC 2N.    Nova lake will likely arrive after Ryzen successors.    Combine that with other performance rumors where there isn't a huge performance uplift. It's bad news.",Negative
AMD,"The dates aren't wrong. As you said, It's the OEM roadmap, and they follow a Q2 release cadence.",Neutral
AMD,Gate All around and Backside power was supposed to be in 2024...  Now it's q2 2026. At this point it's 4N7Y... Which is what we call TSMC cadence.,Neutral
AMD,"why even panther laker when it was only for mobile, cancelled it and just released nova lake next year",Negative
AMD,"This isn't Intel and AMD's road map, this is an ""undisclosed OEM's"" road map.   This doesnt tell us anything about PTL's launch. Just when this OEM will refresh their laptops to have PTL, and its normal for OEMs to launch refreshed models at certain times.   Dell, for example, will usually do Spring, so this may be them. Surface is notorious for being 3 or so quarters after CPU launch.  Look at where MTL is on this schedule, for example. Also around Q2, even though it had been on store shelves technically since the previous year's Q4.  Edit: Looking at the roadmap a second time, this OEM seems to always target a Q2 launch cycle: ADL, RPL, MTL, ARL, PTL, and NVL all follow it. LNL is the only outlier.",Neutral
AMD,I thought Arrow Lake refresh was in the cards for 2025.,Neutral
AMD,Wasn’t Panther Lake going to use TSMC for a portion of the volume?,Neutral
AMD,Yeah I forget to to mention wrong In the sense that it is not Intel CPU Launch OEM roadmap.,Negative
AMD,This entire thing is a mobile roadmap so why are you here?,Negative
AMD,"This ""undisclosed OEM"" is Lenovo by the looks of the font used. Also the timeline makes sense. For example Lunar Lake is shown to be in very late 2024. Lenovo had only one model of Lunar Lake available at that time - the Yoga Aura Edition.",Neutral
AMD,"Yeah if it's Surface roadmap, it's a nothing burger.",Neutral
AMD,Yeah some OEMs don't refresh their lineups until the new generation is already launched,Neutral
AMD,I think it was for the iGPU but I don’t remember. I just know the majority of PTL is 18A. I saw a rumor that they were using Intel 3 for the iGPU also so idk.,Neutral
AMD,I also think it's Lenovo. Q2 is when they refresh their ThinkPad line,Neutral
AMD,The 4 Xe3 core iGPU will use Intel 3. The 12 Xe3 core iGPU will use N3E.,Neutral
AMD,"Wonder if Intel will do stacking two bLLCs on a single CPU tile like rumoured as an option with AMD's Zen 6. I iknow this is basically both Zen 5 CCDs having each an 3D v-Cache and NVL seemingly going to have that option now. But two bLLCs meaning 288MBs of stacked cache + cache in compute die would be insane and crazy for gaming.  Regardless I'd love a P Core only RZL SKU with much stacked cache as possible, or heck just 48MB 12P Griffin Cove + 144MB L3 bLLC would be sweet.",Neutral
AMD,"I like how they just throw random words around to pad their ""article"".",Neutral
AMD,The only reason I don't have a dual CCD ryzen is the lack of X3D on the 2nd CCD.  I would 100% be in for a 16 or 24 (when they go 12 core CCD) core dual X3D.,Neutral
AMD,"""leaks""",Neutral
AMD,"When are people going to realize ""Leaks"" are marketing ploys to get eyes on product. Come on people.",Negative
AMD,"Considering the latency is not even expected to be any good with BLLC, AMD is seemingly quite a careful opponent for Intel indeed",Negative
AMD,Scared of their rumor?  Lets release our rumor!,Neutral
AMD,I don't think 2x bLLC could fit on the package. Sounds like nonsense to me.,Negative
AMD,it seems as if right now the best option for consumers is to just build a cheap 1700 or AM4 system and call it a day for the next 4-5 years or so. but that's just me,Neutral
AMD,"My god.  The only reason huge caches benefit gaming, is to benefit rushes to shipment by the publisher, so that game engineers either don't have to optimize, or they can use garbage like Unity, or both.  This is going to make so many mediocre developers better... except no it won't.  It'll just get the market flooded with more garbage.  Also... diminishing returns, and I doubt AMD will put this in Zen 5 -- seems like a trash rumor, to me.",Negative
AMD,"3D cache will eventually become full of assets and dip, if and onlyif IMC is bottlenecked by random memory access.  - the ability for cache to transfer data from memory, to IMC, to cache has a FPS hit (cache hit-rate). This is due to the bottleneck of a chiplet-based IOD: compromising memory latency with cache hit-rate. Eventually the cache fills and relying on IMC to feed the cores anyway.  You can easily test this by monitoring 1% lows and increasing memory latency (lowering DDR4 B-die clocks/DDR5 hynix and increasing timings) to see how Cache holds up when full of assets while memory is being bottlenecked-   What games need is 10-12 P-Cores and an IMC capable of high speeds. Within a monolithic die to reduce relying on cache hit-rate and IOD chiplet bottlenecks.",Neutral
AMD,This is all they needed to sell for gaming again and yet they gave us all the e cores no one asked for jfc.,Negative
AMD,bLLC is not stacked cache,Neutral
AMD,What do you consider random? The article was perfectly clear.,Neutral
AMD,Do you have a specific application in mind? I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.,Neutral
AMD,Some rumors of 16-core dual X3D chip actually surfaced yesterday...,Neutral
AMD,"They would have trouble fitting just 1 bLLC onto a package, 2 bLLC might only be viable for something like a server chip with a much larger socket.   It may be possible once Intel starts 3D stacking they're CPUs though, but we have to wait and see.",Neutral
AMD,Yeah. Definitely just you,Neutral
AMD,You could literally make that claim with any CPU performance increase.,Neutral
AMD,Speak for yourself. Skymont ecores have raptor lake IPC. We need more of those,Neutral
AMD,"What CPU do you have? If its Zen 4 or slower, your cores are same or weaker than Arrowlake E cores in IPC",Neutral
AMD,This is correct. Stacked cashe comes later. I'd say around 1 to 2 years after. And that comes with stacked cores as well if they go with that path.,Neutral
AMD,"Correlating asset pop in with memory latency is utterly nonsensical.   Edit: Lmao, blocked.  Also, to the comment below, the creator of that video clearly has no clue what there're talking about. There's no ""bottleneck between cache and memory"" with v-cache. The opposite, if anything. Just another ragebait youtuber who doesn't know how to run a test.",Neutral
AMD,"> I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.  It does, but i'd rather every core be equal in terms of per-core cache in a local sense. Thread scheduler can handle the CCD split (usually)  There are definitely workloads that benefit from X3D but either scale to many threads or are let down by poor OS scheduling.  It could even be a boon to gaming with well thought out core pinning of game threads.",Neutral
AMD,He has a fair argument with the horrible optimization of modern video games but any innovation in the CPU space is a good innovation at this fucking point,Negative
AMD,Id love a 200 e core cpu with lots of pcie lanes for my home server. But I do not want mixed cores.,Neutral
AMD,Yea but I’m not paying for e cores that are barely faster than my current skylake cores. I just want p cores only.,Negative
AMD,"IPC is just 1 factor  Zen 4 clocks much higher, and got a far superior low latency caching subsystem   No one sane thinks that Skymont can actually beat Zen 4 in most workloads Only if you downclock Zen 4... and guve it slow ram to increase latency",Neutral
AMD,"Now pay attention to the memory bottleneck of chiplet IMC architectures on x3d chips:P https://youtu.be/Q-1W-VxWgsw?si=JVokm7iLScb7xynU&t=700  This simple 1% lows test indicates how texture-pop can impact frametime, directly. Since assets are first stored in memory, then fed to cache, any bottleneck between Cache - to memory will cause delays when loading asset thus 1% FPS... measured temporally(in this case).",Neutral
AMD,The bad optimisation critics aremostly people trying to sound smart by rambling things they dont understand.  Game engines are very optimized these days.   Its funny they mostly pick on Unreal Engine 5 as the prime example... That is probably the most complete game engine out there. So naturally its hard to make all that stuff work well togetter... They have litterally some of the best coders and researchers in the business ... Id love to see you do a better job.,Negative
AMD,Mixed cores are awesome if the scheduler would do what i want and there lies the problem and i dont think even AI can solve that yet.,Negative
AMD,"I said IPC of Zen 4 is the SAME and weaker than Zen 4 is weaker than Skymont in IPC. No more no less. As for clocks, the core scales to max 5.2ghz (default 4.6ghz), certainly overall slower than Zen 4. IPC is a comparison about throughput at a given clockspeed. Its a give to compare you will need to match the clocks.  Funny you you should mention latencies, considering that core to core (even accross clusters), latencies of Skymont are better than Lion Cove? So if latency is the issue, it has nothing to do with Skymont and everything to do with the trash uncore they have based on the Meteorlake SOC. Even workstation Crestmont on a different package performs better. Furthermore skymont IPC is measured based on Arrowlake. Arrowlake is worse latency than Raptorlake and Zen already. Latencies have been accounted for.",Negative
AMD,"I play Warthunder almost daily. It’s ridiculously fucking optimized.   I got cyberpunk when it came out, it was a shit show. Elden ring? Mediocre.  Escape from tarkov? Fucking laughable.  Call of duty? 500 terabytes. Besides this small list,  I’ll wait for battlefield 6 (which by the way also has a history of releasing unoptimized)  The one thing I will say is that developers do work hard after releases to optimize their games. I’m just so use to playing Warthunder which almost always works, everyday, perfectly, with no complaints. lol",Negative
AMD,"Do tell how a bigger cache makes games faster, if you are aware of some details.  And, of course, how doubling the already tripled cache will yield perf improvements worth the huge amount of extra money to make such chips.",Neutral
AMD,Any efforts of Intel to improve efficiency and even performance on old products is a pro-consumer move. Good to see.,Positive
AMD,It's not sorcery. Its just Intel doing the game developers work.,Neutral
AMD,"Is it updated? Last time I tried it, it didn't have an uninstaller and not many games were supported. In the ones I tried, there was no difference at all.",Negative
AMD,"It's very specific scenarios where this can unlock new performance, most games it won't do anything or even lower performance.     So they can't *really expand it* since it's an uncommon scenario. That's why we have so few games that benefit from it.",Negative
AMD,Never count out Intel. They have some very talented people over there.,Positive
AMD,"Regarding the cache optimizations you pointed out, I have a question. Can you test all APO supported games you own and capture the utilization of the E-cores ?  My understanding is that Intel is keeping 1 E-core per cluster active and parking the remaining 3 E-cores. So the lone E-core of each cluster has full access to the 4mb shared L2. For the i9 they can have 4 such solitary E-cores that have their own cache and hence do not touch the L3 cache. The 4 E-cores handle background task while the 8 P-cores with the full 36mb L3 cache deal with the games.  Please correct me if I am wrong.  Also, what cpu cooler do you use ?",Neutral
AMD,What about this post saying APO havent gotten anything in over a year.. https://www.reddit.com/r/intel/s/tTKCn1CnDZ,Neutral
AMD,Does anyone know if changing the Windows power plan affects performance? What Windows power plan do you recommend for the i7 14700kf? Balanced? High performance?,Neutral
AMD,I wonder why APO isn't just tied to game mode. That way it would work for all games from ADL to ARL.,Negative
AMD,"or just get rid of E cores. I know all the non engineers in here think they are great, but real world they are crap, this is just another in a long line of examples of why intel needs to ditch them.   anyway go ahead and downvoted me becuse being correct in this sub is looked down upon.",Negative
AMD,A few years ago you'd have to tweak the hell out of AMD CPUs to get comparable performance to Intel equivalents. Now the tables have turned.  In either case those who know how to tweak will get top performance not matter the platform.     Kudos,Negative
AMD,Let me teach you a trick.... there is a hack to disable e cores for every game using Intel compatibility tool.... you can try yourself to see if it works,Neutral
AMD,I will follow all!,Neutral
AMD,Congratulations on getting a CPU that didn’t fry itself.  Consider yourself one of the few lucky ones.,Positive
AMD,"It looks like Intel APO almost completely disables the efficiency cores for the game when you look at the screenshots I posted below with per core activity.  But this performance boost likely cannot just be gained by turning off the efficiency cores in the BIOS yourself I wager, because it would be too simple.",Negative
AMD,How is this doing the game developers work?,Neutral
AMD,"It's updated as far as I know, but not for every platform.  If you're on Arrow Lake you have the new games that were added, but if you're still on Raptor Lake then you only have the original titles.     This sucks because I own BG3, which is one of the new titles they added recently along with Cyberpunk 2077.",Negative
AMD,"They have added a few new games, but it appears you need to have Arrow Lake to be able to apply APO on them which is BS!  They need to have a better system than this.       The DTT driver should be downloadable from Intel, and not from the motherboard manufacturers who cannot be trusted to make the latest version available.",Negative
AMD,The intel software team is pure black magic when they allowed to work on crack.,Neutral
AMD,"Here is a screenshot showing CPU activity across all cores.  This is with HT disabled BTW, and the cooler is an Id FROZN A720.  It looks like the efficiency cores are not engaged in this particular game when APO is enabled.  And Metro Exodus is the only game that I own in the current lineup for the 14900K.  I know they've added some games for Arrow Lake (like Cyberpunk, Baldur's Gate 3) but those games still aren't available yet for the 14900K     [https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png](https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png)",Neutral
AMD,"Raptor Lake isn't getting new games for APO.  Which strikes me as greedy, since I'm sure it would work fine. Intel originally said that APO wouldn't work at all on 13th gen, which turned out to be completely false.",Negative
AMD,"When combined with Nova Lake's bLLC, it could give Intel a major edge over AMD's X3D.       And yeah, limiting it to certain games and certain CPUs is a real downer for the technology, so hopefully Intel can find a way to streamline and expand it when Nova Lake launches.",Positive
AMD,"AFAIK, it's recommended to use Balanced mode for proper use of the efficiency cores if you have a Raptor Lake or Arrow Lake CPU",Neutral
AMD,Balanced in full load will just do the same thing as High Performance.,Neutral
AMD,"It's capable of a few things, but the main mechanism from what I've seen is that it temporarily disables three out of four cores in each E-core cluster by forcing them into the lowest power state. The result is that the remaining active core has access to each cluster's full L2 cache (2MB on 12th-gen, 4MB on 13th/14th-gen). L2 cache isn't shared with P-cores (L3 is global), so this can really minimize E-core cache evictions before they're forced into slower memory, and games do love themselves some cache.  It's a genuinely clever way of maximizing available resources and I really wish they'd allow user control over its features, but it seems to be pretty tightly leashed to/by the team that developed it. It obviously wouldn't benefit every situation, such as particularly low/high thread occupancy situations, but it's pretty rough to have the option tied to quarterly updates for one or two specific games.",Neutral
AMD,Or... Just process lasso.,Neutral
AMD,Because it’s optimizations on how it can efficiently use the cpu.,Neutral
AMD,Must the App be downloaded? Or it's automatic after installing the PPM and DTT drivers?,Neutral
AMD,"All of this stuff is super situational. For when it’s supported for my 14th G, I love it.  To call it better than X3D though? That’s a bit much.",Positive
AMD,They are they came up with ML for their upscaller before the red team did and even better than FSR as a dump upscaler and they also did a good RT implementation. But the hardware still has raw horse power issues and they already use a big die for what it can do.  In the 80's or they shammed The DEC Alpha team on subnormal floating point operations with the 8087 FPU. Intel was doing way more precession without having to round so earlier on a consumer FPU while the Alpha CPU was a Mainframe one. Intel also gave it up to help estandardize the way CPUs handle floating point types.  Intel compilers were also black magic becuase they optimized so much C and C++ for Intel specifically but there was always controversy causeless AMD x86 couldn't take part of those optimizations,Neutral
AMD,Were there any background tasks running when the screenshot was captured ? I want to see how the E-cores are assigned for the background tasks under APO.,Neutral
AMD,In certain games. It's not universal at all. There would be scenarios where it gets an edge but if AMD is pushing as hard as rumors state they may still have the better CPUs overall.,Neutral
AMD,"Great explanation that makes sense!  It definitely has to do with the cache, that much is true.   A Raptor Lake CPU with 8 P cores and no efficiency cores, with 36MB of L3 would perform exceptionally well in games.   But the efficiency cores have their uses as well in highly parallel tasks.",Positive
AMD,"Yeah it's a pretty cool, yet simple idea. It would be really nice to give users more granular control natively.",Positive
AMD,Are the e cores clustered in order so we would be able to manually achieve this with process lasso?,Neutral
AMD,Why should game devs be automatically expected to specifically optimize their game for specific architectures of Intel's CPUs? It's not on them to do so.,Negative
AMD,You need to download the Intel Application Optimization app from the Windows store,Neutral
AMD,"I never said it was better than X3D, only that it is useful as a foil.       Remember that Nova Lake will have bLLC, which is the answer to X3D.  But in conjunction with APO, it may give Intel the edge and help them regain the gaming crown in the next cycle provided they can streamline and expand the technology.",Positive
AMD,"Actually, it is better than X3D in almost every GPU/CPU combo where the GPU throttles. Intel 14th gen almost universally wins 1% lows and frequently FPS - almost universally in 4k gaming, but as I said, when the GPU throttles at any resolution, which is almost always, Intel wins.",Positive
AMD,"Eh, they're very good but they benefited vastly from AMDs mistakes and the lessons learned from it. Remember AMD didn't have any ai hardware for their first 2 gens on RDNA and RDNA 3 didn't have enough for FSR4 either. It's not that intel were better at producing a ML upscaler. They just had the hardware for it from the get go so they could implement it quicker.  They did make Xess though which is miles ahead of FSR 3 even on AMD systems (on average. FSR3 is pretty good when modded and can be competitive) so they do have some extremely talented people there. I hope they fund them properly and don't go the AMD route of having Ryzen and Radeon not work together due to pride.",Positive
AMD,"Just  Steam, MSI Afterburner, Windows Security, Nvidia App and my sound card command center",Neutral
AMD,Where to download this APO,Neutral
AMD,Except its THE game devs job to optimize games for multiple cpus and gpus.,Negative
AMD,It’s literally their job to do so? wtf you talking about?,Negative
AMD,AMD also has Heterogeneous CPUs and even more so ARM cpus.  Devs should start to do this optimizations to schedule the stronger cores.,Negative
AMD,"Will do, I got a 265K. Performance is already great tbh.",Positive
AMD,I love Intel but with how the company is doing right now I’m in a “I’ll believe it when I see it” type of skepticism.,Neutral
AMD,"Sorry I am making unreasonable requests. Can you test the game with APO on, and a few background tasks ? Like a few browser tabs, a youtube video playing, discord etc ? I am interested in the E-core usage, particularly the spread of E-core usage.   Thanks.",Neutral
AMD,"You need to enable it in the BIOS, me ale sure the DTT driver is installed and then download the app from the Windows 11 store",Neutral
AMD,"Which they already do, and are often overworked and underappreciated for.   But sure, it's their fault they didn't optimize the game even more for a subset of already relatively high end CPUs....",Negative
AMD,"You are massively oversimplifying. A lot of games that support APO, like Metro Exodus EE OP used as an example, were released before Intel released CPUs with P and E cores. Expecting studios to rework and optimize old games for new CPUs is very naive.",Negative
AMD,"They should be focusing on optimizations that help the vast majority of all CPUs, especially older/lower end ones.   It's not their job to specifically help one companies latest CPU architectures because they couldn't figure out how to create a core that has both good area and performance.",Negative
AMD,That's not simply what APO does.,Neutral
AMD,"You should have the full list of current APO optimized titles, unlike myself which only has the original list LOL!",Neutral
AMD,"Everyone should be skeptical, sure, but I choose to be hopefully optimistic also.",Positive
AMD,"I booted it up again, this time with three Chrome tabs open including one that had an active YouTube video playing.  The efficiency cores were definitely being utilized for the background tasks I believe as I could see a small bit of load going from core to core.     [https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png](https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png)",Neutral
AMD,i am not talking about older games. i am talking about newer games.... really dude?,Neutral
AMD,Ok…all CPUs include Intel CPUs…I don’t get your thought process,Neutral
AMD,"According to Intel, the app is entirely optional. The DTT driver already includes APO, the app is just an interface to control it and disable APO, if needed.",Neutral
AMD,Thanks a lot.,Positive
AMD,> i am not talking about older games. i am talking about newer games.... really dude?  New games seldom have problems with P and E cores. That is why if you took at minute to look at games that APO support you would notice almost all were released before 2022 before release of Alder lake CPUs. Because older P and E unaware games struggle. Don't really dude me if you are this clueless.,Neutral
AMD,"APO includes specific optimizations made for specific Intel skus, not general optimizations that help pretty much all processors.",Neutral
AMD,That is not true whatsoever. Games do not properly utilize the p and e cores and unreal 5 is very known for this.,Negative
AMD,Ok…? If there is a specific problem then they need to optimize it for that hardware. Literally a devs job. Still don’t get your thought process,Negative
AMD,"Dude, Unreal 5 runs like crap on pretty much all hardware. It stutters on AMD and nVidia graphics. Unreal 5 doesn't count.",Negative
AMD,"There isn't a specific problem, it still works lol, just not as well as it could on a specific architecture.   They shouldn't be wasting their finite resources/time on optimizing specifically for an already relatively high end and well performing architecture, but rather attempting more generalized optimizations that help all users.   It really shouldn't be too hard to understand.",Neutral
AMD,"Very cool. Cheers Intel! As a 6900XT owner, I use Xess whenever it's available. The DP4A version got very solid once V1.3 hit.  Hopefully more devs implement it.",Positive
AMD,"I like how Intel still support DP4A model. My meteor lake chip gonna love this XeSS2 with FG and LL.  Meanwhile Amd give middle finger to their consumer who don't have radeon 9000 series, FSR4 doesn't works at all on their previous GPU including the flagship rx 7000 series.",Positive
AMD,Did the DP4A version also improve from 1.3 to 2.0?,Neutral
AMD,"I thought it was a mix of both shader model 6.4+ and DP4a, not either or",Neutral
AMD,Okay but why would I want to use that instead of NVIDIA DLSS?,Neutral
AMD,It’s the least you should get after not getting FSR4.,Neutral
AMD,You'd use it over FSR if that's available too?,Neutral
AMD,Except you NEVER got acces to the DP4a version tho. Intel cross vendor locked the DP4a path to their igpus and only allowed Shader Model 6.4 version for other gpus.,Neutral
AMD,"I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4.   On Linux they managed to emulate it using the FP16 instruction, which is available on RDNA3, but it seriously impacts performance and introduces 1,5ms of input latency, compared to the 0,5/0,8ms of latency on RDNA4 with hardware FP8.",Negative
AMD,And they expect people who bought previous RDNA to buy more RDNA,Negative
AMD,Not by a significant amount.,Neutral
AMD,DP4a only relevant for intel igpus as its crpss vendor locked by intel. Other gpus only get the Shader Model 6.4 version.,Neutral
AMD,for the games that dont support DLSS,Negative
AMD,"If you own a card that doesn't support dlss.   This is an extra option, it doesn't affect your choice to run dlss or not.",Neutral
AMD,10 series cards will benefit from this,Positive
AMD,It's great news for gpu that can't use dlss because fsr3 is a blurry shitfest.,Positive
AMD,"I mean it gives better image quality than FSR 3.1, so I know I definitely do.",Positive
AMD,Sure. The image quality is generally better. I'd only use FSR if I was trying to squeeze every last drop of performance out of my GPU since it's faster than Xess DP4a. But Xess will be tried first.,Positive
AMD,Do you have a link? Anything I have seen points to the GPU using Xess DP4A unless it's an ARC GPU. So it supports RDNA2 and newer on AMD and Turing and newer on Nvidia.  https://www.pcgamer.com/intel-xess-compatibility-nvidia-amd/  And I do literally run Xess on my 6900XT (currently using it for F7 Rebirth through Optiscaler). So if I'm not running Xess DP4a then what am I running?,Neutral
AMD,">I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4   Sure, but they can make FSR4 for non hardware accelerated like Intel right? FSR 3.1 is so bad even when compared to Intel XeSS 1.3 DP4A, but Amd refuse to improve it because they want to force people to buy radeon 9000 series.",Negative
AMD,"Amd really all in when it comes to copying Nvidia. Guess what? The more you buy, the more you save!",Neutral
AMD,DP4a is cross vendor.   XMX is Arc only.,Neutral
AMD,DLSS is the widely adopted upscaler in the current market. I'd be shocked if a major game in 2025 releases without DLSS at the very least.,Neutral
AMD,1080ti heard no bell,Neutral
AMD,"You run the Shader Model 6.4 version which i stated. There was a discussiona bout DP4a being cross vendor locked but for the life of me i cannot find it again. The article you linked is from 2021, in early 2022 intel cross vendor locked DP4a. The main article of the post clearly states Shader Model 6.4.",Neutral
AMD,where did amd touch you bud?,Neutral
AMD,"Yeah more and more games are adding DLSS support, so I'm not sure when I would need Xess",Neutral
AMD,"That's true but in edge cases where even the lightest Xess option is still too heavy, FSR may still deliver a better overall experience.   E.g. CP2077 on the steam Deck in Dog Town. When I was testing some stuff out (trying to see how close I could get my Deck to the Switch 2 build) I found Xess to be a unsuitable. Even on the lightest Xess mode, the performance hit was a bit too noticeable. FSR doesn't look as good as Xess but it runs better and you can get away with a bit more upscaling artifacting on the Decks relatively small screen. Just rendering at a lower res and letting the screen upscale was also a solid option in that scenario.  As I say though, that's an edge case where you really just need outright speed. If your GPU doesn't support DLSS or FSR4 then using Xess makes sense in the vast majority of cases.",Neutral
AMD,Are you sure you're not getting the DP4A/Shader Model 6.4 and Arc exclusive version mixed up? DP4a was added to Shader Model 6.4. https://github.com/microsoft/DirectXShaderCompiler/wiki/Shader-Model-6.4.  There is a separate path of Xess that is better than DP4A but that only works on Arc Cards.,Neutral
AMD,"It's good to see finally Intel working with game developer to bring optimization to Intel platform. This time not only optimization for Arc GPU, but also for Intel Core CPU like proper CPU scheduling for P and E cores to make sure the game runs on the right thread.  I think this also the reason why they never update APO game support again. Obviously optimizing their platform by working directly with game developer is better than doing it by themself.  I really hope we got optimization like this for more titles.",Positive
AMD,News like this will never been posted on Yahoo Finance or Stocktwits... Instead the news from Short-Sellers and Paid Bashers ...,Negative
AMD,"Im not impressed, my i9 12900k is bottlenecking my 4080 like crazy in bf6 beta :s  Im only getting 100-140 fps on my cpu while my gpu wants me to have 200. Any kind soul who can explain how i increase this?  My cpu is right out the box, i have not done any OC what so ever",Negative
AMD,13700k vs 9800x3d BF6 Beta menu cpu fps comparison (105 vs 370 fps) [https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8](https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8),Neutral
AMD,You sound like an intel stock holder lmao The markets don't care about gaming at all.  They only care about AI slop,Negative
AMD,"the game is just really latency sensitive, i dont think it will even change in the full release sadly  i kinda wonder what makes the game so demanding on CPUs, the mechanics and physics arent much better compared to older BF (like BF4) that barely use modern CPUs beyond 20 - 30%",Negative
AMD,"Dealing with same issue on a 285k + 5090. dipping to 125 fps curing certain scenarios specifically on the cairo map at point C. MY system should not struggle at all at 1440p to reach 144 fps, but the 285k is getting absolutely destroyed by bf6 and I have no idea what the issue is,",Negative
AMD,"It's not like game runs bad on Intel CPU which has P and E cores, i will be lying if i say that. It's about maximizing CPU performance. CPU demanding game will runs with much higher FPS when it knows which thread should be utilized.    This is why game with APO enable like R6S on i9-14900K at 1080p low settings, it got performance increase up to 100FPS which is massive.",Neutral
AMD,"Most games are optimized for consoles which are natively eight cores, and a lot are ported over to PC and so you basically need eight fast cores in whatever CPU you happen to have, and the rest don't really do much. That's why Intel HEDT and AMD's Threadripper aren't of much use in this regard.",Neutral
AMD,"No, that's because and isn't really better for gaming unless you get their x3d variants. Those have a different style of memory that is better for gaming in particular but can underperform when it comes to workload tasks. Idk why 🤷",Negative
AMD,"Yeah and in a game like bf, I only care about the 1% low staying at minimum my monitor refresh rate 🫠 But I have not spend that much time optimizing my settings so I think I can work around it",Neutral
AMD,"I found a ""fix""   Dynamic render scale, I set it to 75, so when shit really hits the fan, it just lowers the render scale for just enough time for me to not drop below my monitor refreshrate, and it happens so fast I don't see the change in gfx",Negative
AMD,"From what I understand, many games are generally fairly latency sensitive. The fast cache on the X3D chips basically helps with that.",Neutral
AMD,Thank you :),Positive
AMD,Wouldn't the 300 series actually be Arrow Lake Refresh?,Neutral
AMD,"Videocardz is wrong      This chip will likely be called ""Core Ultra 5 445k"" it's an 8+16 die with bLLC + 4LPe   (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")    Ultra 3 has 4P + 4E + 4LPe  Ultra 5 has the 1x 8+16 tile + 4LPE   Ultra 7 has 8+6 P-Cores and 16+8 E-Cores +4LPe   Ultra 9 had 2x 8+16 tiles + 4LPe tile   Ultra 300 series will be reserved for mobile only Panther Lake and/or Arrow Lake Refresh   Ultra 400 will almost certainly be Nova Lake.",Neutral
AMD,I think this competition will tight up alot in the next gen for both intel and AMD.  Intel has had a superior memory controller for awhile now and AMD will finally address this in Zen 6.  AMD has better cache setup with VCache which Intel will finally address in NL.  May the superior arch win!,Positive
AMD,This is pat's work,Neutral
AMD,This is normal Nova Lake or the bLLC version? I heard the bLLC version has separate SKUs.,Neutral
AMD,8 Big cores with loads of cache and then a couple of E cores with their own cache for background tasks would be great.,Positive
AMD,Is Intel finally making a comeback with their cpus? I hope so,Positive
AMD,Large L3 cache without reducing latency will be fun to watch.,Positive
AMD,the specs sure do shift a lot..,Neutral
AMD,"Now we are talking Intel! If they can hold on to these chips, not melting themselves, then it's gonna be a good fight",Positive
AMD,it's just a bunch of cores glued together - intel circa 2016 probably,Neutral
AMD,Not sure I can trust Intel again after the BS around Arrow Lake and the lack of ongoing LGA 1851 upgrade path.,Negative
AMD,Bout time,Neutral
AMD,"Like I commented before, hope (but really don't believe) this is NOT the product that is only 10% faster than arrowlake because liquidation awaits them otherwise",Negative
AMD,Will it be available in fall of this year or 26Q1?,Neutral
AMD,So it only took them like 5 years to come up with a luke-warm response. cool cool. It's especially bad as Intel actually introduced something similar (eDRAM LLC) into the x86 space many years ago.,Negative
AMD,Noob question:  Is this their 16th gen chips?,Neutral
AMD,Planning on upgrading from my 13700K to whatever the top end Nova Lake is,Neutral
AMD,Only Core i3 can have 65watt ? no more i5 at 65Watt ?,Neutral
AMD,"For some reason it seems to me that all these new different micro cores are cores from ten years ago at the level of Broadwell or Lynnfield, but made on thin nm to fit everything under one lid lmao",Neutral
AMD,Cool.  I have said for a while now that is want both the extra cache AND a few lesser cores to offload stuff to.  The only alternative that has this atm is the 9900x3d and the 9950x3d.  Next gen when im buying new stuff then perhaps intel will have an alternative to those then.,Positive
AMD,"The 52 cores is exciting, but I would probably use Linux as my main OS and run Windows in a VM. Plenty of cores and plenty of RAM and it would be a great machine.  A 52 core desktop beast with an Intel graphics card with 128 GB VRAM would be a killer machine. LLMs would run like lightening.",Positive
AMD,"Intel mocking the chiplet design, describing it as gluing processors together and then doing it themselves - this screams that energy once again.",Negative
AMD,So Nova Lake IS happening? This talk about skipping to next node had me anxious..,Negative
AMD,"So alder lake on cache steroids ?  If the heavier core count chips don’t have the LLc cache, and if this chip is faster at gaming than the heavier core count chips, how will Intel market it?   If they call it a core ultra 5 or 7, then that will make the core ultra 9 seem less of a halo product.  Will be interesting to see how they market it.",Neutral
AMD,"E core need to get lost for gaming, we need 9800x3d rival, purely for gaming !",Neutral
AMD,"""E-Cores"",  Ewww, Gross.",Neutral
AMD,"**Hi,**  I’m currently running a 9700K paired with a 5070 Ti. Do you think I should upgrade to Arrow Lake Refresh now, or hold out for Nova Lake?  The CPU’s clearly starting to bottleneck a bit, but it’s not super noticeable yet.  Is it worth waiting for Nova Lake, or better to just go with Arrow Lake Refresh when it drops?",Neutral
AMD,NVL will definitely be the 400 series. PTL is 300.,Neutral
AMD,"Naming can be changed, but the most important thing is, this isn't the upcoming arrow lake refresh  I hope arrow lake refresh fix the memory controller latency tho",Neutral
AMD,"Yes, I expect the 300 series to be Arrow Lake refresh and Panther Lake.",Neutral
AMD,"> (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")  Maybe they will bring back the ""C"" from i7-5775C.   Behold, the Core Ultra 7 465KFC",Neutral
AMD,I suspect it'll be like:  495K: 16+32.  485K: 12/14+24/32.  475K: 8+16.  465K: 8+12.  Etc.,Neutral
AMD,Seeing all those different cores on a chip does look cool. But I presume this idea will end up being butchered by Windows not being able to schedule things properly,Neutral
AMD,"So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake? Or in 2027? Either way Zen 6 vs Nova Lake will be interesting.",Positive
AMD,Taking the 'L' to their own CPUs would generally be considered a bad marketing move.,Negative
AMD,"We'll see how far Zen 6 takes us on memory. Strix has improvements, hopefully that wasn't all of it though.",Neutral
AMD,"If intel doesn't screw itself with stupid tdp, reliability issues, or bad relative performance",Negative
AMD,More like *despite* him.,Neutral
AMD,All the SKUs rumored so far are BLLC,Neutral
AMD,"They have graduated from background tasks. At this point, even LPE cores can handle apps like Microsoft teams by themselves with no issue never mind the E cores on the ring bus",Neutral
AMD,"If the scheduler can really support this. As soon as a game would hit the e cores, it could turn bad.",Negative
AMD,Why not 8 P cores with HT + even more cache and no e cores at all,Neutral
AMD,"A large L3 will help a ton with latency. It's why AMD has so much success with it using chiplets which have a ton of extra latency. Reduces all those higher latency trips out to RAM. Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile and if that's the case this L3 might be a really big hit for gaming, etc.",Positive
AMD,If latency slightly increases or stays the same due to adopting Pantherlake optimizations and the newer Foveros then that might be enough to narrow the gap,Neutral
AMD,I assume the cache is based on Lunar Lake's memory side cache which doesn't bring much if any noticeable latency over Arrowlake H,Neutral
AMD,"Also done with them. Changing socket every \~3 years was also bad, but this is disgusting.",Negative
AMD,"Not sure how it could be, given all the extra cores.  That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  I expect NVL to be a big power/thermals/multithreading win, than anything else.",Neutral
AMD,Nova lake? More like 26Q4.,Neutral
AMD,Only Pantherlake for mobile,Neutral
AMD,"It doesn’t matter so long as all major vendors will automatically ship with whatever Intel puts out. Lenovo, dell, and HP carry a grand total of *zero* desktop Zen 5 (granite ridge) machine across their consumer, gaming, and business line. You heard that right, *zero* among the global big three.",Neutral
AMD,"Since when they are ""mocking chiplet design""? Have you forgot Intel is the first when it comes to gluing chip together? Intel did this with Core 2 Quad. Meanwhile Amd copying that with ryzen but sadly people don't want to admit it.",Neutral
AMD,It is really happening. The only question is when? Or can they release it on the next year without delay?,Neutral
AMD,"Core Ultra 9 has 48 cores (and 4 LPE cores). It slaps you in the face as halo SKU (unless it becomes entry level threadripper target?).  Currently rumored:  (8+16E+BLLC, 8+16E)  8+16E+BLLC  8+12E+BLLC",Neutral
AMD,The i9 sku is rumored to be two 8+16 tiles. So slotting in an extra bLLC 8+16 tile instead of a normal 8+16 one for the top sku doesn't sound impossible.,Neutral
AMD,Probably same way AMD did the 5800X3D 8 core which was slower than the 5800X at productivity and had less cores than the 12 core 5900X and 16 core 5950X.,Negative
AMD,"E cores are the future, P cores days are numbered.",Neutral
AMD,ARL refresh is not worth it. Go AMD or wait for NVL if you want.,Negative
AMD,If you have to upgrade now for gaming get a 9800x3d from AMD otherwise wait and see what AMD Zen 6 and Nova Lake can do.,Neutral
AMD,"You just missed some pretty killer deals around Prime Day; that would have been the ideal time to go with Arrow Lake.  The 'refresh' will offer better performance, but I highly doubt the value difference will be significant.     You can definitely wait a bit.",Neutral
AMD,"I would like the naming with suffix ""C"" for cache at the end or maybe ""G"" for gaming.",Neutral
AMD,"W11 has (mostly) fixed the scheduling issues, in fact the scheduler was already pretty good by the time Raptor Lake launched. Sometimes it can mess things up but it's quite rare nowadays. The only time I have seen some small issues is with legacy single threaded software.",Neutral
AMD,"Why would it be butchered? the issues with big/little are long gone now - but I guess to your type none of that matters - getting upvotes for blind anti-MS zealotry is all that matters.  but given that this is an intel sub, i am surprised its not merely blind anti intel zealotry instead.",Negative
AMD,">So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake?  Yes. Intel has confirmed NVL will launch in 2026.",Neutral
AMD,The former ceo,Neutral
AMD,"Raichu has repeatedly talked about BLLC ""variants"", so I wouldn't assume that all skus have the large L3.   Plus, it seems extremely wasteful to have all the skus have BLLC.",Neutral
AMD,"Yeah, Skymont are more like space efficient P cores that are down clocked and don't have all of the features. IPC is really good.",Positive
AMD,This is wrong. Arrow Lake already uses E cores for gaming to no performance hit.   12-14th Gen sure. But not with the new E cores and no HT. You’re using dated info.,Negative
AMD,"A number of public tests have shown that, even with all the P-cores disabled, ARL is still pretty good in gaming.  It's not the E-cores.",Positive
AMD,No different to 12-14th gen then.,Neutral
AMD,"windows pretty much gave up on this and just allows you to set to performance mode, which always tries to grab a P core first.",Neutral
AMD,"Or that I forgot they don't have HT on the newer chips.   To be honest I was meaning 14th gen P and E cores with more cache not the new gen stuff. We know the IPC, memory controller and core speed is enough it's just the cache that makes it fall behind AMD in gaming.",Neutral
AMD,E core is 30% faster than hyper threading,Positive
AMD,HT is worse than E cores,Negative
AMD,>Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile  They aren't,Neutral
AMD,It will enlarge the ontile L3 cache most likely with a cache only tile with high performance die to die connect. No 3d stacking until 18A-PT is available for production which will likely be well after Nova Lake tapeout which should be done now or with in the next few months in order to ramp up production in early 2026 and be on the market by late 2026.,Neutral
AMD,">That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  If you are in the lead, maybe this argument is valid. However Intel is behind Apple in ST and behind AMD in gaming. They have plenty of room to improve.",Neutral
AMD,"> That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point  And yet we see that constantly.",Neutral
AMD,"Ze3vsZen2, Zen 4vsZen3, Zen 5 X3D vs Zen 4 X3D, Alderlake vs RocketLake, Raptorlake vs AlderLake, Lunarlake/Arrowlake-H (Mobile) vs Meteorlake U and H, Gracemont vs Tremont, Skymont vs Gracemont and Crestmont, Many Arm or Arm based designs, etc",Neutral
AMD,ARM chips regularly do this sometimes on a yearly basis.,Neutral
AMD,A lot of gamers have no clue that Intel is actually doing well with ARL for OEM laptops/dekstops as ARL is basically a laptop design warmed over for desktop chip usage.,Neutral
AMD,Core 2 quads and Ryzen chiplets are totally different ways of doing MCMs  C2Q didnt have an IO die and it pretty much worked like having a 2 socket motherboard      IO die is what enabled much lower latency and the ability to connect much more CPU dies together (EPYC 9005 got 16 CCDs),Neutral
AMD,https://www.pcgamer.com/intel-slide-criticizes-amd-for-using-glued-together-dies-in-epyc-processors/,Neutral
AMD,"> They stopped trying to sell capacity to external customers and they will try again with 14a. It's not like they had a lot of capacity to spare. It's not even enough for themselves  Intel claims they can double to 1.5x their EUV capacity in arizona and ireland, if customers asked for it, in 1-2 years.   They built out all they needed for their own internal 18A demand too.",Neutral
AMD,"It shouldn't become an entry level workstation part since the Ryzen 9 will have 24c/48t to go against it.   48 cores (plus the 4 LPe, but those shouldn't factor in too much) should handily beat 48 threads, but not by enough to reach Threadripper territory.",Neutral
AMD,Most likely the bLLC variant wil be single compute tile and single cache tile as this would be cheaper for now than putting bLLC cache in base tile like Amd did. They can do that when 18A-PT is avaliable which will probably be after Nova Lake goes into production with 18A or maybe 18A-P,Neutral
AMD,"Lmao, some of my games still runs on e cores",Neutral
AMD,People are still disabling e cores for more performance.,Neutral
AMD,"Varients because there are two tiles. One with BLLV, one without. Full chip had 1 BLLC chip and 1 without. No actual SKU (not tile) without BLLC in some way so far",Neutral
AMD,"They are nothing like P-core as designed with no hyperthreading, area/space efficienct and integrated into clusters of 4 e-cores with shared L2 cache",Neutral
AMD,fym no different they're 50% faster,Neutral
AMD,No. ARL architecture is definitely not optimized for latency but hiding memory access which favours multithreading instead of gaming. Hopefully Nova Lake will optimize all the latecy issues with ARL arch and decrease latency to L3 considerably aswell,Negative
AMD,"Yes, but in games, it's the P-cores that are meant to be utilized. Also, more than eight cores or 16 threads aren't really necessary for gaming. The 9950x3d isn't faster, than the 9900x3d.  My point is that the space taken up by the E-cores could instead be used for additional cache, not?",Neutral
AMD,I had heard this both ways and never really know who to trust until I see it. Anyhow if that's true the large L3 should be even more helpful for things like gaming as it will keep those higher latency hits down. AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help.,Neutral
AMD,"As a advice, Nova Lake will further increase memory latency.",Neutral
AMD,In which gen iteration?,Neutral
AMD,Editorialized. But its funny because said quote is attributed to AMD when Intel made Core2Duo (or Quad?),Neutral
AMD,"If its is with 10% of threadripper 9960x at half the price, it will definitely hurt the low end threadripper cpus a lot for productivity/creator types who would love a more reasonable priced high performance entry level workstaion. Low end workstation is probably bigger market than high end gaming desktops anyways.",Negative
AMD,"Another factor is that Apple has booked out all the N2 capacity for the better part of a year before AMD can touch it.  Nova Lake will have been out for a while, before AMD shows up",Neutral
AMD,Process lasso can let you just designate by task so the E cores can do background and side jobs. People use process lasso on the 7950x3d to make the X3D chiplet run games with those 8 cores and the other 8 core chiplet for everything else.,Neutral
AMD,What issue did you run into with a 9800x3d? It has one CCD and no small cores,Neutral
AMD,"Nowadays there are very few cases where disabling E cores is extremely beneficial, sometimes they are a detriment to performance but more often than not they are useful. Not to mention the E core design team is more competent than the P core team and are quickly catching up in performance.    The E cores in Alder Lake had relatively poor ST performance but in Arrow Lake they are much faster and closer to the P cores in raw performance. Every new generation the gap keeps shrinking and it will make the disabling of E cores less and less relevant.    Sometimes the Windows scheduler will wrongly assign an E core to a ST bound task, even if a P core is available, but it doesn't happen often enough to be a serious issue.",Neutral
AMD,"yes, but not because of windows scheduling - its because you can OC higher with them off - better temps and easier stability - some people disable HT for the same.",Neutral
AMD,They don't pay attention,Negative
AMD,"No. Intel needs to do what Amd did and add seperate bLLC just for gaming. As long as latency is reduced to the L3 cache including bLLC, gaming performance will definitely increase as 285K and 9950x have similar performance in both gaming and productivity with small lead going to Amd. But x3d totally dominates ARL beacsue of the obvious huge cache difference and lower overall latency to their cache implementation.",Neutral
AMD,That is more so bc the 9950x3d isn’t really a 16 core cpu it is two 8 cores,Neutral
AMD,"There was one article a while back that reported something like ""intel goes back to integrated mc"", not understanding that panther lake is a fundamentally different product than the tile based desktop products.",Neutral
AMD,How do you think they're doing 2 compute tiles if the memory controller is on one?,Neutral
AMD,">I had heard this both ways and never really know who to trust until I see it.  I'm curious, which leakers have said otherwise?   You could still have mem latency improvements without the mem controller being on the compute tile too, so the situation there can improve either way.   >AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help  Yup, and ARL has higher mem latency than AMD even, so obviously there is room to improve even without having to move the mem controller back.",Neutral
AMD,"Intel will definitely decrease latency with Nova lake and as long as they add a bigger L3 cache variant, gaming performance will increase further to challenge AMD",Positive
AMD,"If you go straight to the source, the quote they used is directly on Intel's slides. Intel literally wrote that, regardless of who said it first.   [https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg](https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg)",Neutral
AMD,"I mean that it won't be priced like an HEDT part. It'll definitely perform like current generation lower end Threadrippers, but so will the 11950x.",Neutral
AMD,"NVL high end desktop skus are rumored to be on N2 as well. Intel has confirmed NVL desktop will be on TSMC, and Intel has also confirmed some NVL compute tiles will be external.   It's very possible Zen 6 DT is out before NVL-S, and it's very likely that NVL-S and Zen 6 launch within a few months of each other, if that even.   Also, if AMD wants to pay for TSMC N2 capacity, TSMC will build it out. Just a couple days ago we had a [report](https://www.digitimes.com/news/a20250724PD213/tsmc-2nm-capacity-2028-3nm.html) about TSMC being very aggressive with N2 scale out, and the reasoning here is two fold- first of all, 2026 is technically the 2nd year of N2 HVM, and also because a *lot* of major SOCs, other than just Apple and AMD, will be on N2- demand for N2 is high, and build out will be too.",Neutral
AMD,AMD is one of the leading N2 early adopters. Both TSMC and AMD announced this.,Neutral
AMD,That didn't stop Intel with N3B,Neutral
AMD,The recent Warhammer space marine game is a game that benefited from disabling e cores. I am not sure the disparity between arrow and alder in that game tho.,Neutral
AMD,">Intel needs to do what Amd did and add seperate bLLC just for gaming.  Thats what I tried to say.  >My point is that the space taken up by the E-cores could instead be used for additional cache, not?  Or isn't that enough? AMD stacks the cache directly on top (9000 Series) or under (9000 series) the CPU cores, right?",Neutral
AMD,"Yes but IF Intel decides to do quad channel for AI nonsense at least for flagship model, Amd threadripper will be stomped over as I will definitely buy the Nova Lake part just for the price/performance compared to 9960x threadripper. The motherboard selection alone for Z890 is absolutely amazing and RDIMM prices aretwice as expensive for the same amount of memory. I expect with CAMM2 that max ram capaciity will go for 512GB and maybe 1TB for Nova Lake aswell. Even with 2 channel, cudimm will go above 10,000 MTS and maybe 12000 if we are lucky on CAMM2 modules by 2027 as CAMM2 definitely being used for DDR6   I do feel Intel has I/O advantage beyong gaming and that market is getting bigger than hardcore gamers all the time!! A decked out 256GB Arrow Lake z890 based computer is way cheaper than same memory sized threadripper 9960x.",Positive
AMD,"I don't recall Intel stating that NVL core chips would be at TSMC, only that some of the eventual package would be... which implies iGPU, to me, and that wouldn't be N2 (possibly N4, since Battlemage is N5, and N4 would be open and cheaper than N3 nodes).  Do you have a source, where they directly claim the NVL cores are coming from TSMC?",Neutral
AMD,"The game is a dumpster fire when it comes to CPU utilization, it hammers even top tier gaming chips like the 9800X3D. I'm not surprised if the E cores can bottleneck the performance even further, most likely some of the game's heaviest threads get (wrongly) assigned to the E cores.    The scheduling in W11 has been significantly improved over the last few years but it's not 100% perfect either. Legacy software in particular, which typically utilizes 1 core, often ends up running in an E core and as a result you can't take advantage of the higher ST performance a P core provides. In that case you need stuff like Process Lasso to fix the problem. It doesn't happen very often but when it does it's very annoying.",Negative
AMD,"Yup.   >. Then as you look forward, to our next-generation product for client after that, Nova Lake will actually have die both inside and outside for that process. So, you'll actually see compute tiles inside and outside.  Q4 2024 earnings call.",Neutral
AMD,"That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.  Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  That seems like something they couldn't have known in 2024.",Neutral
AMD,"18A is about as good clockspeed as N2, but density is still around N3 level. To make a new architecture, it makes sense for them to target N2 if AMD does the same which they have",Neutral
AMD,">That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.   20A, and only the lower end 6+8 dies are rumored to be fabbed on that node.   >Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Not competitive enough.   >Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  Low end client products like WLC, and even the lower end 4+8 NVL compute tile, is rumored to be on 18A still.",Neutral
AMD,"It doesn't make sense, for either of them, due to the cost of being an early adopter",Negative
AMD,Will it also introduce Lunar Lake successor?,Neutral
AMD,I wonder if this could become avaliable on LGA 1954 because that'd be a good alternative for budget builds potentially if the iGPU is potent enough.,Positive
AMD,Is anyone left?,Neutral
AMD,I would love to see a Nova Lake-A (strix halo like) APU with an 8+16 tile + 20Xe3 cores which is equal to 2560 FP32 lanes or 40 AMD CU's with 16-32mb of memory side cache to handle igpu bandwidth demands,Positive
AMD,"Wonder what node such a product, if it does exist, would use for the iGPU tile.   AFAIK, rumor is that the iGPU tile for the standard parts would be on 18A or an 18A variant. However, if this is a flagship part, I would assume they would be willing to pay the extra cost to use N2...   However that would also presumably involve porting the architecture over to another node, which I don't think Intel has the resources, or wants to go through the effort, to do so.   Also wonder if such a product will finally utilize the ADM cache that has been rumored since like, forever.   And this is a bit of a tangent, but I swear at this point in the leak cycle for a new Intel product, there's always usually a ton of different skus that are leaked that may or may not launch - right now we have NVL bLLC, NVL standard, and now this, and a bunch of them end up not launching (whether they were cut internally, or the leaks were just made up). So it would be pretty interesting to see if any of the more specialized rumored NVL skus end up actually launching.",Neutral
AMD,These designs are going to be the go-to for creator and mid-range gaming at some point in the future. Having it all integrated together should provide opportunities for optimization and form factors that are difficult with discrete cards,Positive
AMD,How long do I have to wait for something that is more powerful than an AMD 7840HS and still cheaper?,Neutral
AMD,Good to hear! I have a strix halo asus tablet the 390 one and its only got 32GB ram but its fast!  Would love to see an intel version…,Positive
AMD,"This is great move by Intel since they aren't making dGPU for laptop so they can get some marketshare by making strong Intel ecosystem. If Nova Lake AX released then people don't need to buy overpriced laptop with Nvidia dGPU anymore, not to mention with Nvidia shitty small vram.",Positive
AMD,"I'm still using Alder Lake, I suppose all these chips will be released in 2026 ?",Neutral
AMD,That's Panther Lake in a few months,Neutral
AMD,Lunar lake was always confirmed even by Intel as a 1 off design with soldered memory and many of its design choices.,Neutral
AMD,"No, the memory config wouldn't work.",Neutral
AMD,No not really.  It's pretty f'n bleak atm.,Negative
AMD,"That isn't that different from the AI MAX 395+ already released, which has 16 Zen 5 cores and 40CUs and some MALL cache (I think 20MB) to help with bandwidth.  For this to be a meaningful upgrade, you'd want more than 20 Xe3 cores, which means you need more bandwidth, which means you need probably LPDDR6 memory to get it high enough as otherwise the cache demands would be prohibitive and LPDDR6 memory isn't coming to client until probably 2027.",Neutral
AMD,It's N2.  Sadly Intel Inside has almost entirely become TSMC Inside,Negative
AMD,"Nova Lake-AX will not be released, and DLLC will not be released, and nova Lake will not be released.",Neutral
AMD,"Unless they can find some way to bring down the cost.....these aren't gonna get much traction, the same applies to Strix Point of course. While more efficient than your typical 2000$ laptop with a decent GPU.....the traditional laptop is still gonna outperform them based on pure specs.  You get more battery hour and less heat but people who use these kind of laptop are used to having it plugged 24/7. Feels more like experimental models but they have a long way to go before they see proper adoption. Particularly the cost.",Negative
AMD,"> mid-range gaming  Strix halo is only in $2000 systems. high end price, mid-range performance.  Good for stock margins i guess.",Positive
AMD,I'm sure that Xe2 in Lunar Lake is faster than the 7840HS iGPU on average if you run it with the max performance mode in the OEM software that comes with your laptop.,Positive
AMD,The 7850HS can't compete with the Xe2 igpu because it's bandwidth starved.   The 8mb of memory side cache insulating the 4mb of L2 from main memory allows the Xe2 Arc 140V igpu in LL clocked at 1950mhz to beat the RDNA 3.5 890m clocked at 2950mhz as it only has 4mb of L2 as a last level of cache before hitting memory.   TLDR: RDNA 3.5 is bandwidth starved on strix point due to lacking memory side cache,Neutral
AMD,PTL's not really a LNL successor.,Neutral
AMD,"Yeah, it was just to prove a point that ARM is overrated.",Negative
AMD,"I hear you, questioning my decision to return under Pat’s hire-back spending spree. Dodged this one… but this is hitting differently.",Negative
AMD,What about an 8 + 16 big LLC tile and 32Xe3 cores (4096 FP32 lanes or ~60 AMD CU's) + 64mb of memory side cache?   Or even better a mid range part with an 8+16 tile and 16-20Xe3 cores + 16-32mb of memory side cache,Neutral
AMD,Just read about the JEDEC spec for lpddr6 of 14400mt/s.  That is wild!,Neutral
AMD,Who said it's It's entirely TSMC ? they have been pretty open about majority of tiles Intel the CPU Tile will be shared with N2 ofc though,Neutral
AMD,"It should be Intel here, TSMC there.  Or a little bit of Intc, a little bit of TSMC.",Neutral
AMD,IDM 2.0: Where we proudly declare our fabs are world-class—while quietly handing the crown jewels to TSMC.,Neutral
AMD,Source?,Neutral
AMD,"The BOM is lower, so the question is where the markup is coming from.",Neutral
AMD,Consoles have big APUs and they have better value than anything. I know they're subsidized however.,Positive
AMD,A mid range Nova Lake-A SKU with a 6+12 tile and 16/20Xe3 cores with 16-32mb of memory side cache would be sick,Neutral
AMD,Thats the problem with AMD sticking an additional 8 cores instead of infinity cache to fix the bandwidth bottleneck.,Negative
AMD,>still cheaper,Neutral
AMD,"It's the closest that we will get to a Lunar Lake successor   Really, panther lake is a combined successor to both Arrow Lake-H and Lunar Lake",Neutral
AMD,"Low volume and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference.  Combine that with not moving a lot of them compared to the rest of the lineup, and each one of those units has to make up more of the costs from things like bad dies or general spin up costs for a new chip.",Negative
AMD,It already has 32MB infinity cache.,Neutral
AMD,The 7840HS is cheaper because it is older.,Neutral
AMD,"It competes in the U/H lanes, so what is today ""ARL""-U and ARL-H. But yes, in practice it should bring *most* of the LNL goodness over, but PTL-U is still not a true successor in the original ~10W envelope LNL was designed for.",Neutral
AMD,"> and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference   It's no bigger than the equivalent dGPU, and you save on memory, package, and platform costs. Half the point of these things is to use the integrated cost advantage to better compete with Nvidia dGPUs.",Negative
AMD,The new Ryzen AI 250/260 with 780M is still cheaper because it doesn't have copilot+ .,Neutral
AMD,I will see the performance envelope of PTL U and decide should dump my LNL or not,Neutral
AMD,"The low volume part of that quote is important. If you're making heaps of them, each can take up a little bit of the startup costs for that design. If you only make a few, those costs get concentrated on what you do make.  Low volume also means a single defect is promotionally more of your possible units, and hits the margins harder. You do get more total defects in a larger run, but at sufficiently high quantities of those defects, you start making lower bins to recover some of the defects. If you hardly make the volume to cover one widespread SKU, you aren't likely to have enough to make a cheaper one with any degree of availability. At some point you hit a threshold where it's not worth selling what would be a lower bin because there's not enough of them.",Neutral
AMD,Because putting a 40 TOPs NPU for the Copilot+ certification is costly.,Neutral
AMD,"It doesn't have a 40 TOPS NPU, it's the same 16TOPS one in the 8840HS it's literally just another rebrand. It's cheap because it's from 2023.",Neutral
AMD,Exactly. Hence your Xe2 lunar lake being faster is irrelevant to the question because it won't be cheaper regardless of age when they have a NPU that takes close to 1/3 of die space.,Negative
AMD,I think I didn't say anything that deviates from what you just said.,Neutral
AMD,"Maybe I misread or misunderstood, I thought you said the 260/250 had a 40TOPS NPU",Neutral
AMD,enjoy rhythm aware outgoing practice bike attempt library versed cake   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
AMD,Haha wow that is hilariously bad.   And here I thought ASUS newer bios update was bad due to some higher temps,Negative
AMD,"The new Gigabyte BIOS also has booting issues for me, and memory is even more unstable.  At least the UI is entirely in English now.",Negative
AMD,"who needs quality control, what can go wrong?",Negative
AMD,Gigabyte is trash,Negative
AMD,The Elon Musk method,Neutral
AMD,"> “Yeah just push it to production nothing’s gonna happen, no need to QA/QC this”  Seems to be the motto of a lot of the tech world",Negative
AMD,"Yup indeed.   I would even go as far as one full month   Hence, why I learned to avoid installing new Windows Update automatically",Neutral
AMD,Intel really needs to be able to compete with X3D or they're going to continue getting dominated in the enthusiast consumer market. I like Intel CPUs and was happy with my 12600K for awhile but X3D finally swayed me to switch over.,Positive
AMD,"Intel has had plans for big ass L4 cache for almost a decade now, just that it never made it past the design board.  Supposed to be marketed as Adamantium. But it got ZBB’d every time I suppose due to cost.  For Intel to implement Adamantium, regular manufacturing yield has to be good enough I.e cost is low so they can splurge on L4.  Of course now they are forced to go this way irrespective of cost. I’d love 16p + L4 CPU.",Negative
AMD,"Honestly, good. I've been using AMD for a while now but we need healthy competition in the CPU space for gaming otherwise AMD will see a clear opportunity to bring prices up",Positive
AMD,"Something interesting is that the extra cache isn't rumored to be on a base tile (like it is with Zen 5X3D), but rather directly in the regular compute tile itself.   On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.   I think Intel atp desperately needs a X3D competitor. Their market share and especially revenue share in the desktop segment as a whole has been ""cratering"" (compared to how they are doing vs AMD in their other segments) for a while now...",Neutral
AMD,Hasn’t this been on their roadmap for a while now? I’m pretty sure they said 2027 is when they’ll have their version of x3D on the market,Neutral
AMD,"These core count increases could be a godsend at the low end and in the midrange. If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming, Intel will have a massive advantage on price.  That being said, if leaked Zen 6 clocks (albeit they’re from MLID, so should be taken with a grain of salt) are accurate, Nova Lake could lose to vanilla Zen 6 in gaming by a solid 5-10% anyway.",Positive
AMD,"Funny how non of this news posted on reddit hardware sub or even allowed to be posted. Guest what? R amdhardware will always be amdhardware! It's painfully obvious that unbearable toxic landfills sub is extremely biased to Amd. Meanwhile all Intel ""bad rumors"" got posted there freely which is really BS!  I still remember i got banned from that trash sub for saying ""People need to touch grass and stop pretending like AMD is still underdog because they aren't"" and the Amd mods sure really mad after seeing my comment got 100+ upvotes for saying the truth, but that doesn't matter anymore because i also ban those trash sub!",Negative
AMD,Intel should be ahead of the curve on things not looking to compete on previously created tech,Neutral
AMD,"Very fine-tuned ARL-S almost reach 9800X3D performance. Extra cache could help to close the gap   Given people are willing to overpay for price-inflated 9800X3D, I wonder if it could work given buyers need an entirely new platform. 9800X3D users are fine for a pretty long time like 5800X3D users did",Positive
AMD,"Lol, requires a new socket. Intel is such trash.",Negative
AMD,Intel will simply always be better than amd,Positive
AMD,"AMD gains tremendously from X3D/v$ because the L3 cache runs at core speeds and thus is fairly low latency, Intel hasn't seen such low latency L3 caches since skylake, which also has much smaller sizes, so the benefits of this could be much less than what AMD sees.   Only one way to find out, but I advise some heavy skepticism on the topic of ""30% more gaming perf from 'intel's v$'""",Positive
AMD,"Either more cache or resurrecting the HEDT X-series... Doesn't matter, as long as there is an affordable high-end product line.",Neutral
AMD,"The 12600k was a fine chip, but AMD had the ace up Its sleeve. I upgraded from a 12600k to a 7950x3d and it was one of the best PC upgrades I ever made.",Positive
AMD,I mean 9800x3D and 14900K offers basically the same performance in the enthusiast segment. Going forward though it would be nice to have more cache so normal users doesn't have to do any sort of memory overclocking just to match 9000x3D in gaming.,Neutral
AMD,4070 ti won’t cut it man - upgrade!,Negative
AMD,Broadwell could have been so interesting had it planned out.,Positive
AMD,"I want a 32 Core/64 Thread 3.40 GHz Core i9-like CPU. Not Xeon like with Quad-Channel and stuff, just 40 PCIe 5.0 lanes and 32 Power-Cores instead of little.big design. 😬",Neutral
AMD,">Otherwise AMD will see a clear opportunity to bring prices up  AMD already did, as you can see zen 5 x3d is overpriced as hell especially the 8 core CPU. Zen 5 is overpriced compared to zen 4 which is already more expensive than zen 3. Not to mention they did shady business like keep doing rebranding old chip as the new series to fools people into thinking it was new architecture when it wasn't and sell it with higher price compared to chip on the same architecture in old gen.  Intel surely needed to kick Amd ass because Amd keep milking people with the same 6 and 8 cores CPU over and over with price increases too! Not to mention radeon is the same by following nvidia greedy strategy.  Edit: Some mad Amd crowd going to my history just to downvote every of my comments because they are salty as hell, i won't be surprised if there are from trash sub r/hardware. But truth to be told, your downvote won't change anything!!",Negative
AMD,"Even though it's not stacked, I believe it's still going to fix the last level cache latency issue MTL and ARL have.   Ryzen CPUs have lower L3 latency than Intel because each CCX gets their own independent L3, unlike Intel's shared L3. Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3, so possibly giving the existing cores/tiles their own independent L3, improving latency and bandwidth over shared L3.  But one thing intrigues me. If this cache level has lower latency than shared L3, wouldn't this more properly be called L2.5 or something below L3 rather than last level cache? Will NVL even still have shared L3 like the previous Intel CPUs? I know the rumor that it will have shared L2 per two cores, but we know nothing of the L3 configuration.",Neutral
AMD,"> On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.  It is already a non-issue since AMD moved the 3D V-Cache to underneath the compute tile.",Neutral
AMD,"Adamantaium was on the interposer, did they change plans?",Neutral
AMD,"Don't remember them saying anything like that, but by around that time their 18A packaging is supposed to be ready for 3D stacking.",Neutral
AMD,"Nova lake= skip of it's just as good as zen, you would be looking at 2 gens after that and then swap from AM5 to intel.",Positive
AMD,"> If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming  Doubt that since it'll probably lack hyperthreading and the E-Cores are slower, even 6C12T CPUs are starting to hit their limits in games in the last few years, faster cores won't help if there's much less resources to go around, it kinda feels like intel went backwards when they removed hyperthreading without increasing the P-Core count.",Negative
AMD,Intel managed to run Sandy Bridge's ring bus clock speeds at core clocks which resulted in 30 cycles of L3 latency.   Haswell disaggreated core and ring clocks allowing for additional power savings.   Arrow Lake's L3 latency is 80 cycles with a ring speed of 3.8ghz,Neutral
AMD,"I'd like to see the HEDT X-series come back too, but Intel would have to come up with something that would be competitive in that area.  It's not hard to see why Intel dropped the series when you take a look at the Sapphire Rapids Xeon-W lineup they would have likely been based off of.  I think AMD would also do well to offer something that's a step above the Ryzen lineup, rather than a leap above it like the current Threadrippers.",Neutral
AMD,Well it was a downgrade on system snappiness as intel have way higher random reads than amd.,Neutral
AMD,> I mean 9800x3D and 14900K offers basically the same performance  LMAO,Neutral
AMD,"Huh? 9800x3d is universally known to be like 20-25 percent faster, even in 1 percent lows.   https://www.techspot.com/review/2931-amd-ryzen-9800x3d-vs-intel-core-14900k/",Neutral
AMD,Maybe that is your experience.  Neverthelesss if you compare most gamers who switched to 9800x3D they report a significantly noticeable uplift in fps and 0.1 fps. Maybe a negligible few reported a decrease. And this has very likely nothing to do with the x3D CPU but other causes.,Neutral
AMD,"Ah you’re missing the final piece. As far as i’m aware this pretty much requires controlling the OS as well (or at least solid OS support). Consoles get their own custom operating system, Apple built a new version of MacOS for M chips. Intel and AMD though don’t control windows.",Neutral
AMD,UMA is such a hassle That's why I don't see it much except for calculation purposes (HPC/AI)...,Negative
AMD,"Application developers are supposed to try to avoid copies from GPU memory to CPU memory, instead letting it stay in the GPU memory as much as possible",Neutral
AMD,">so there is still the cost of useless copies between system RAM vs allocated GPU ram.    There is none, AMDGPU drivers have supported GTT memory since forever, so static allocation part is just to reduce burden for app developers but if you use GTT memory you can do zero-copy CPU+GPU hybrid processing.",Negative
AMD,"Intel needs something decent because AMD has taken a page out of intel (up to gen7) playbook, same cores no changes. Intel now provides more cores but it's the 100% core increase Vs AMD 50% and bLLC that should shake things up, hopefully they keep the temperature down as I don't want to have to replace case and get a 360mm rad just to not throttle, and not ever again do a 13th and 14th gen degradation show.   If all goes well going back to intel for a few years then AMD, brand loyalty is for suckers, buy what's best for performance and value. Hopefully intel i5 has 12P cores and i7 18-20P cores that would be nice to have",Neutral
AMD,"bLLC is just a big-ass L3$ and since Intel does equal L3 slices per coherent ring stop, it'll be 6\*12 or 12\*12 with each slice doubling or quadrupling. The rumor is 144MB so quadrupled per slice, probably 2x ways and 2x sets to keep L3 latency under control.",Neutral
AMD,"Intel and AMD have effectively the same client L3 strategy. It's only allocated local to one compute die. Intel just doesn't have any multi-compute die parts till NVL.   > Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3  8+16 is one tile, in regardless of how much cache they attach to it",Neutral
AMD,It is a massive issue for amd. You're voltage limited like crazy as electron migration kills the 3D cache really fucking fast. 1.3V is already dangerous voltage for the cache.,Negative
AMD,"I still think there's a slight impact (the 9800x3d only boosts up to 5.2GHz vs the 5.5GHz of the 9700x), but compared to Zen 4, the issue does seem to have been lessened, yes.   And even with Zen 4, the Fmax benefit from not using 3D V-cache using comparable skus was also only single digits anyways.",Neutral
AMD,"Adamantium was always rumored to be an additional L4 cache IIRC, and what Intel appears to be doing with NVL is just adding more L3 (even though ig Intel is calling their old L3 the new L4 cache? lol).   I don't think Intel can also build out Foveros-Direct at scale just yet, considering they are having problems launching it for just CLF too.",Neutral
AMD,Zen6 is the last AMD CPU using its socket anyway,Neutral
AMD,"I'm an e-core hater but arrow lake e-cores are really performant and make up for the loss of HT. arl/nvl 4+8 would wildly beat 6c12t adl/rpl.  HT was always a fallacy anyway. If you load up every thread, your best possible performance is ~60% of a core for a games main-thread.  I would much rather pin main-thread to best p-core in a dedicated fashion and let the other cores handle sub threads. Much better 1% lows if we optimize for arrow lake properly (still doesn't hold a candle to 9800X3D with HT disabled though).",Negative
AMD,"Yeah, I somewhat agree with this. I suppose it depends if Intel’s latency problem with their P+E core design is at all a fixable one - 4c/8t is still shockingly serviceable for gaming, but 4c/4t absolutely is not.",Neutral
AMD,It's the same ratio as 285K 8P+16E vs AMD 16P and we know that 285K is competitive despite no hyperthreading,Neutral
AMD,"Sooo a few months ago, I helped a buddy of mine troubleshoot a black screen issue on his newly built 9800X3D and RTX 5090 rig, a fairly common issue with Nvidia’s latest GPUs.  While working on his PC, I'd notice a series of odd and random hiccups. For example, double clicking a window to maximize it would cause micro freezes. His monitor runs at 240Hz, and the cursor moves very smoothly, but dragging a window around felt like it was refreshing at 60Hz. Launching League of Legends would take upwards of 10+ seconds, and loading the actual game would briefly drop his FPS to the low 20s before going back to normal. Waking the system from sleep had a noticeable 2-3 seconds delay before the (wired) keyboard would respond, which is strange, considering the keyboard input was what wake the system up in the first place.  Apparently, some of these things also happen to him on his old 5800X3D system, and he thoughts that these little quirks were normal.  I did my due diligence on his AMD setup: updated the BIOS and chipset drivers, enabled EXPO profile, made sure Game Bar was enabled, set the power mode to Balanced. Basically, all the little things you need to do to get the X3D chip to play nice and left.  But man... I do not want to ever be on an AMD system.",Neutral
AMD,did they measure responsiveness and timed the click to action? and was it significantly different? how much difference are we talking about?,Neutral
AMD,"can you explain exactly what you're talking about here? are you talking about a situation where the system needs to do random reads from an ssd? aka: boot time, initial game load time?",Neutral
AMD,"How was ""system snappiness"" measured?",Neutral
AMD,No.,Neutral
AMD,"Now both AMD and Intel chips are “disaggregated “ which means between cpu and other agents like memory controllers, pcie, and storage there is higher latency than the 12/13/14th gen parts. AMD has higher latency due to the larger distances involved on the package.  Also Intel is not really improving the CPU core much. There won’t be a compelling reason to upgrade from a 14700 until DDR6 comes out. At least not in desktop. Nova lake high cache parts will cost $600 or more so value/dollar will be low.",Negative
AMD,So? Major upgrade for everything else,Neutral
AMD,"I had an 12600 not k. I had the opposite experience, I upgraded to a 7800x3d and the snappiness was a night and day upgrade. I can recommend a x3d to anyone. Pair that cpu with Windows 11 IoT LTSC and you have a winner <3",Positive
AMD,"Meant to say ""Gaming Performance""  >Higher avg on X3D  >similar or same 1% lows on both platforms >Higher .1% lows on Intel.",Neutral
AMD,"any comment that starts with ""I mean..""  I never go any further, its like some weird reddit think where everyone with ignorant comments seems to start out with this,  at least often anyway.",Negative
AMD,"""Enthusiast Segment"" my good sir. All the benches you see are poorly configured or stock 14900K. With tuning it's a different story. Intel craptorlake scales with fast ram.",Neutral
AMD,"As a long time AMD user I know that Intel needs to be tuned to perform best. So when you tune the 14900K or even 285K you get like 20% performance uplift vs stock. X3D just performs great out of the box because of the huge L3 Cache. At the very least if you do not like microstutters or frame drops and want consistent gaming performance Intel 14th gen is superior vs current AMD's offering. Anyone with a specific board like Apex, Lightning, Tachyon, or even Gigabyte Refresh boards + i7/i9 13-14th gen with decent memory controller can achieve similar gaming experience. I'm speaking from experience since I also have a fully tuned 9950x3D/5090 on my testbench. For productivity task Intel feels much better to use as well. I feel like Intel is just better optimized for Windows and Productivity too.",Positive
AMD,"Actually Intel thermal is already better than Amd ever since Arrow Lake and Lunar Lake released. Even Core Ultra 7 258V is arround 10c cooler than Amd Z2E and Strix Point on the same watt.   On MSI Claw 8 AI+, Lunar Lake temp at 20w is just arround 62c while the Amd version is arround 70c. I wouldn't have a doubt Nova Lake and Panther Lake will also have good thermal because it will have 18A node with BPD and RibbonFET GAA which is more advance than traditional silicon when it comes to power delivery and efficiency.",Positive
AMD,Ah so bLLC on both tiles is a possible configuration? Any chance Intel actually goes for this?,Neutral
AMD,You can very simply get 9800x3D to 5.4 with little effort,Neutral
AMD,"No, they will do zen7 too",Neutral
AMD,I haven't seen any of those issues on AMD where the underlying cause wouldn't also cause those issues on Intel.,Negative
AMD,"7800x3d here and never had these issues, came from intel",Neutral
AMD,Difference between 85MBps and 140MBps in q1t1 random reads and writes.,Neutral
AMD,Lets just ignore the whitepaper WD and Intel did about this.,Neutral
AMD,So what configuration (tuning and ram settings) can a 14900k match a 9800x3d?,Neutral
AMD,That (Apple Silicon is good) and UMA are different stories I already know that Apple Silicon is good,Positive
AMD,On which benchmark(s) / metrics?,Neutral
AMD,"Haven't kept up with mobile since AMD 5000 and intel 10th gen, all I remember is intel needing XTU undervolting and then intel blocking XTU so using third party undervolt programs, AMD like I said 5000 never needed undervolting.   Desktop side, AMD 7000 is a hot mess, like seriously ridiculous. Seems that was sorted on 9000,    Let's wait and see on nova lake and zen6, like I said, brand loyalty is stupid, bought into AM5, so it's cheaper for me to go for zen6 with 244Mb of L3, but no am6-7 that will be intel.",Negative
AMD,"In theory, yes. For packaging reasons and market segmentation, probably not.",Neutral
AMD,"what's that on in terms of percentage, or seconds to person? was it noticeable?  i'm not techy enough, but is random reads and writes for clicking things and accessing data, and less so on copying and pasting a file?",Neutral
AMD,where is this whitepaper,Neutral
AMD,"You can send white papers all day but if most people buy these for gaming or productivity, AMD is winning in both categories.",Neutral
AMD,Stock clocks 5.7/4.4/5.0 HT Off With DDR5 7600-8000 on Windows 10 22H2 or Windows 11 23H2 is enough to match 9800x3D at 5.4ghz with 6000 c28/6200 c28,Neutral
AMD,So MLID leaked in his most recent video about this (and he ranks it with a blue color code - 'very high confidence')  What do you think about this?,Negative
AMD,[https://youtu.be/0dOjvdOOq04?t=283](https://youtu.be/0dOjvdOOq04?t=283) This explains it.  Gonna find the whitepapers link again.,Neutral
AMD,Based on what evidence?   I looked online for a few moments and found:  (1) Buildzoid from actually hardcore  overlocking did a 12 hour live stream where they couldn't even get a 8000 mhz overclock stable on the 14900k. No wonder why people haven't benched this lol  https://www.youtube.com/live/bCis9x_2IL0?si=ht3obVoBLcRFCyXI  (2) Plenty of benchmarks where an overclocked 9800x3d is about 10 percent faster than an overclocked 14900k with 7600 ddr5,Neutral
AMD,"Haven't you been listening? The conversation is strange (Confused) You first brought up the story of UMA, right?",Neutral
AMD,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Neutral
AMD,"That channel has a lot of videos and even on this specific video it would help if you point the specific time you're referring to.  Now regarding AI, I assume you are talking about token generation speed and not prompt processing or training (for which Macs are lagging due to weak GPU compute).  I happen to have expertise in optimizing AI algorithms (see https://www.reddit.com/user/Karyo_Ten/comments/1jin6g5/memorybound_vs_computebound_deep_learning_llms/ )  The short answer is that consumers' PCs have been stuck with dual-channel RAM for very very long and with DDR5 the memory bandwidth is just 80GB/s to 100GB/s with overclocked memory.  Weakest M4 starts at 250GB/s or so and M4 Pro 400GB/s and M4 Max 540GB/s.  Slowest GPUs have 250GB/s, midrange about 800GB/s and 3090~4090 have 1000~1100GB/s with 5090 having 1800GB/s bandwidth. Laptop GPUs probably have 500~800GB/s bandwidth  LLMs token generation scales linearly with memory bandwidth, compute doesn't matter on any CPU/GPU from the past 5 years.  So by virtue of their fast memory Macs are easily 3x to 8x faster than PC on LLMs.  The rest is quite different though which is why what benchmark is important.",Neutral
AMD,"I'm highly suspicious, for packaging reasons if nothing else. I'm not going to call it impossible, but that would put enormous strain on the X dimension. Think about it, it's something on the order of 3-4x what the socket was originally for.  And obviously goes without saying, but MLID ""very high confidence"" doesn't mean shit.",Negative
AMD,"Based on my own testing , the games I play and benchmark results. Bullzoid is not relevant to this conversation. You can keep downvoting me for speaking the truth it's okay. I can't blame you because youtube and mainstream techspot are 99% misinformation. If you believe 9800x3D is 30% faster then go buy it, nobody is stopping you. I have investments with AMD and you're doing me a favor by supporting them.",Neutral
AMD,Just to be clear compute and hardware support for things like tensor cores have a massive impact. HBM is king but on older cards like the mi100 (released five years ago) can be out paced by a mid range card like the 4070.  All I wanted to convey is llm and token generation is a complex topic with limitations and struggles beyond memory bandwidth.,Neutral
AMD,"I haven't downvoted you at all, what are you even talking about.   You want people to believe in some conspiracy theory where any other information is a lie, and only you provide the ""real truth"".",Negative
AMD,"Besides, UMA wasn't first developed by Apple. Even if Intel introduces it, the software side or the software framework… Moreover, the OS side has to deal with it, so it is necessary to consider it a little. That's what you said earlier",Neutral
AMD,"I take no side there. I'm a dev, I want my code to be the fastest on all platforms  I have: - M4 Max so I can optimize on ARM and MacOS - Ryzen 9950X so I can optimize with AVX512 - in the process of buying an Intel 265K so I can tune multithreaded code to heterogeneous architecture.  The problem of Intel and AMD is segmentation between consumer and pro.  If Intel and AMD want to be competitive on AI they need 8-channel DDR5 (for 350~400GB/s), except that it's either professional realm (Threadripper's are 8-channel and EPYC's are 12-channel) with $800~1000 motherboards and $1500 CPUs and $1000 of RAM.  Or they make custom designsvwith soldered LPDDR5 like the current Ryzen AI Max 395, but it's still a paltry 256GB/s.  Now consumer _need_ fast memory. Those NPUs are worthless if the data doesn't get fetched fast enough. So I expect the next-gen CPUs (Zen 6 and Nova Lake) to be quad-channel by default (~200GB/s with DDR5) so they are at least in the same ballpark as M4 chip (but still 2x slower than M4 Pro and Max).  I also expect more soldered LPDDR5 builds in the coming year.",Neutral
AMD,"Have you not seen HardwareUnboxed 9070 XT Finewine video? Tell me why should I trust someone like that? I'm just sayin if you want real information test it yourself. I just don't trust product influencers like GamersNexus, HUB , Jayz2cents and other mainstream channels. Id rather buy the product and test it myself. Im not forcing you to believe what I said, its for people that actually knows what Raptorlake is capable off. If you want to see a properly tuned 14900k check out facegamefps on youtube. This is a very capable platform.",Neutral
AMD,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Neutral
AMD,Is it safe to install an Arrow Lake CPU without a third-party contact frame over the long run?,Neutral
AMD,"Edit: solved. Windows is capable of disabling e-cores on boot. The setting to turn them back on in MSCONFIG, boot tab, cores dropdown. Whatever feature causes this, I do not know but would love to find out, and furthermore their is no legitimate purpose for this feature, and Intel should prevent Microsoft from doing this, in my opinion, if there is any legal means available. Unless it interferes with Intel’s revenue.  Poor performance, incredibly low benchmarks. Beginning to suspect bad CPU.  I have tried bone stock with cleared CMOS settings. I have loaded multiple overclocking profiles, clearing CMOS in between. I have tried undervolting, and adjusting Load Line. I have changed Lite Load settings. I have tried with and without XMP enabled. Changed windows power plans. Nothing gets my cinebench r23 score above 11k. CPU-Z benchmarks it as 43% as fast as the previous generation CPU. I never overclocked this cpu, and I changed to the updated bios with new microcode the day it was available. CPU Purchased from Intel, retail box.  Windows 11 Build 26100.4484  CPU: Intel I9 14900K, stock clock, 360 AIO liquid cooling  RAM: Patriot Viper Venom DDR5 32GB (2x16) 7200MT/S CL34  GPU: MSI Ventus NVIDIA RTX 4080S  Motherboard: MSI Z790-S Pro Wifi with most recent bios  Storage: WD Black 2TB NVME on CPU; Toshiba 20GB X300 Pro  PSU: NZXT C1000 PSU (2022)  Display: Samsung Odyssey G93SC  I am at my wits end with this thing. I first noticed games crashing to desktop a few months ago. I thought it was just poorly coded (Helldivers 2). I checked my FPS in the game, and while previously I had to frame limit it 10 144FPS, I was now getting close to 60-80FPS.  Is my CPU a toaster or is there something I'm missing?  CPU-Z output, and HWiNFO64 sensor readings:  [https://imgur.com/a/ROuZOKS](https://imgur.com/a/ROuZOKS)",Neutral
AMD,"I bought a 265kf CPU from Amazon. On the checkout page it clearly stated that my purchase qualified for the Intel Spring Gaming Bundle and that I would receive an email with a Master Key. Well, I didn't receive anything and I spent all day being transferred from one Amazon support staff to another to no avail.  The promotion is literally still active and if I try to buy the CPU again it shows the same offer.  But somehow no one on Amazon or Intel support can tell me why I didn't get the email.",Neutral
AMD,Does anyone know if the upcoming Bartlett Lake-S 12 p-core no e-core CPU will suffer from the same stability issues as Intel 13th gen and 14th gen CPUs?,Neutral
AMD,Any news to share regarding this link? https://www.reddit.com/r/intel/s/Bg4QnVzIdD,Neutral
AMD,"Bug report: Latest ARC driver 32.0.101.6972 causes crashing using Speed Sync  I get crashing in ALL games when attempting to use the new Speed Sync option on this driver.  Specs are:  Ryzen 9700x, Arc B580, 32GB DDR5 RAM, 2tb SSD.  Monitor is non VRR compatible (older gsync monitor ASUS PG348Q)  Hope this gets resolved as it sounds like a great feature.",Negative
AMD,"Hello,  I own a i9-14900K, mobo MSI MAG Z790 Tomahawk. I am wondering if it’s possible a specific program can cause my cores to be power limit exceeded, and is there any fix to that? Currently it drops the cpu speed to 0.8 GHz. This one program LeagueClient.exe for a game, League of Legends, has recently started to cause this problem, the only fix is to restart the computer. I have updated my bios, changed power limits within the bios, disabled c state, disabled EIST, disabled e-cores, tried to go into safe boot but the program won’t launch, reinstalled multiple times.",Negative
AMD,Has the degradation of the 13th and 14th gen CPU’s been fixed yet?,Neutral
AMD,"Been getting the following error A LOT while playing Expedition 33, and now after 18 hours I can't start the game without it crashing:  `LowLevelFatalError [File:C:\Hw5\Engine\Source\Runtime\RenderCore\Private\ShaderCodeArchive.cpp] [Line: 413] DecompressShaderWithOodleAndExtraLogging(): Could not decompress shader group with Oodle. Group Index: 760 Group IoStoreHash:52bcbf8ac813e7ee35697309 Group NumShaders: 29 Shader Index: 9301 Shader In-Group Index: 760 Shader Hash: 3BF30C4C9852D0D23B2DF59B4396FCC76BC3A80. The CPU (13th Gen Intel(R) Core(TM) i7-13700KF) may be unstable; for details see` [`http://www.radgametools.com/oodleintel.htm`](http://www.radgametools.com/oodleintel.htm)     Am I just screwed because I bought the wrong generation of Intel CPUs 2 years ago?",Negative
AMD,"I'm not sure if this matters, but when running `garuda-inxi` on my laptop, it tells me that my i3-8130U is Coffee Lake Gen 8, not Kaby Lake Gen 9.5. I've seen similar problems reported with users using CPU-Z, is this just a known bug or is there something else going on?  I'm going to be tweaking my CPU performance soon, so I'd like to make sure of the CPU capabilities/options first.",Negative
AMD,"I've been planing to build a PC from scratch for video editing and Ultra 7 265k and Arc B580 are good in my price bracket. Now, I'm worried with all the talk of Intel potentially going bankrupt or having layoffs, is it a good idea to still buy their products? Will we lose support with drivers and stuff like that?",Positive
AMD,My gaming laptop Intel core i7-12700h runs at a constant temperature 95 . Doesn't matter if I'm playing a high end game like cyberpunk or some indie game like hollow knight. Is this thing suppose to always run at this temp?,Negative
AMD,"con el nuevo microcodigo de intel , los juegos de ubisoft (ASSASINS CRREED ODYSSE) tienen tiempos  de carga excecivamente altos, diria que al menos 10 veces mas de lo normal, Al devolverr la bios al microcodigo anterior todo funciona bien, cuando lo van a parchar?",Neutral
AMD,is it worth the upgrade for ai preformance cus my 5070 ti wont work for some reason so now my cpu is my main accelertor and user so should i upgrade to ultra 9? and i  will remove 5070 ti from my flair soon so yeah .,Negative
AMD,"ok now this is something. my Intel HD Graphics Control Panel is no longer there? no idea how long its been gone but i clearly remember it being there at a point.  using a Lenovo G510(i7-4700MQ, HD Graphics 4600)   Windows 10 Home 22H2 (Build 19045.6216)  windows apps in settings shows the intel driver but not the control panel   drivers from [lenovo's website](https://pcsupport.lenovo.com/in/en/products/laptops-and-netbooks/lenovo-g-series-laptops/lenovo-g510-notebook/20238/downloads/ds103802-intel-vga-driver-for-windows-10-64-bit-lenovo-g410-g510?category=Display%20and%20Video%20Graphics) are dated 16jul 2015. version seems to be 10.18.15.4240   drivers from [intel's website](https://www.intel.com/content/www/us/en/download/18388/intel-graphics-driver-for-windows-10-15-40-4th-gen.html?wapkw=intel%20hd%20graphics%204600) are dated 9jan 2015. version is 15.40.7.64.4279  now whats funny is that my device manager shows that my driver is dated 8mar 2017 which is version 20.19.15.4624   i also have another driver that i can see in the update drivers menu(drivers already present on my device) along with this one which is dated 29sep 2016 version 20.19.15.4531  i have tried reinstalling the driver from the update driver menu using the 8mar 2017 version, no change at all.  my drivers dont seem to be DCH drivers.   intel also says that they [discontinued the ms store version of the control panel](https://www.intel.com/content/www/us/en/support/articles/000058733/graphics.html) anyway.  this is all that i could think of writing here. any other details required just ask.   any help would be good lol",Neutral
AMD,"my i5-14600KF is being throttled at low temps and refuses to go past 0.8ghz of clock speed. Nothing I do seems to get it to stop throttling. According to throttlestop i have a red EDP OTHER ongoing throttle under CORE and RING. My average CPU temp is 31 degrees C across all cores and im getting 0.69 Voltage to my CPU  CPU: Intel i5-14600KF stock settings no overclock   GPU: Intel Arc B580 ONIX Odyssey BAR resizing enabled   Motherboard Gigabyte Ultra Durable Z790 S WIFI DDR4   RAM: Corsair Vengeance DDR4 16 GB x 2 (32GB)   Storage: 1TB Corsair MP600 CORE XT SSD + 2 TB WD Black SN770   PSU: Cooler Master MWE Gold 850 V2  EDIT: NEW INFO ACQUIRED   When running in safe mode and when booting into BIOS settings my CPU acts normally and receives typical voltage. Something running on my computer is throttling my CPU as i boot into windows. If i open Task Manager quickly after booting, I see system interrupts consume a mild amount of CPU before quickly going away, rather than sticking around when their is an ongoing hardware issue. I have reason to believe a program, either maliciously or due to error, is fucking with my CPU. Also worth noting is that Intel Graphics Software reports my CPU utilization as far higher than task manager, anywhere between 2-50% higher.",Negative
AMD,"Currently in the planning/purchasing phase of a small NVR/Steam Cache server. Information on VROC on X299 is pretty limited. so far I've seen mixed information on the drives supported. Before I purchase x4 Intel P4510 drives, I was hoping someone on here has a similar configuration that works.  The mobo manual states that only Intel based drives are supported but doesn't clarify which intel drives. Also saw on the intel forum that X299 CPU raid is further limited to only Optane based NVME drives. This drive will not be booted to, and I dont want to do a windows based raid.  My planned specs are:  CPU: 10900X  Mobo: X299 Taichi CLX - One of the few that seems to support bifurcation & VROC  Drive: Intel P4510 1TB  VROC Key: VRoc Standard  Are the Intel P4510 supported for Vroc on the x299 platform?",Neutral
AMD,"My Intel I210 ethernet device has device id 1531 meaning unprogrammed. The freebsd ethernet driver does not work with 1531. It needs device id 1533 meaning programmed. (Can I use a different driver? No, it's an embedded system that only supports this driver.)  I was linked this:  https://www.intel.com/content/www/us/en/content-details/334026/intel-ethernet-controller-i210-i211-faq.html  but 1) have no idea how to do it 2) cannot access the .bin file and tool it requires  Does anyone have ELI5 steps for getting the device to show devid 1533? Where can I get the .bin?",Negative
AMD,"The RMA process for Intel is absolutely atrocious. I don't know if  anyone can give advice on this but here is what is happening:  Based in Germany, for geolocation info. I was one of the early adopters of the 13th gen processors, but as I don't follow tech news too strongly I didn't find out about the issues with these chips until autumn 2024 when the issues I was having with my PC escalated to a point I couldn't ignore any further. Identified the CPU as the likely culprit and started the RMA process.  Firstly, Intel would not offer any solution where I could continue to use my PC whilst they analysed my CPU. As I use my PC for work, having it out of action for weeks/months was simply not an option, so I was forced to pause the RMA ticket whilst I saved up for a new cpu way ahead of my expected timeline.  With that aside, I reopened my ticket and the requirements they lay out are near impossible to meet - they wanted a clear photo of the matrix on the front of the chip and the matrix on the pcb itself.  The front of the chip was simple enough but on this series Intel printed the matrix in dark grey on a dark green pcb. It's barely visible just with the naked eye and I've tried so many ways to take a picture of the matrix with my smartphone and nothing I do is getting a clear picture.  I don't understand how your average consumer can possibly meet this requirement - solutions online apparently suggest purchasing a special type of expensive scanner or a macro lens for your smart phone? Which is ridiculous to me.  There is no way they cannot verify my chip with the rest of the information I have been able to give them, as well as far as I am aware, the matrix on the side of the chip on the 13th gen is literally the same as the one on the front of the chip.  The whole experience is proving to be awful, time consuming, feels like it should be illegal and has completely put me off ever using Intel again, or recommending it to anyone I help spec builds for.",Negative
AMD,"Hi, I have a Lenovo Ideapad with an Intel CPU and Intel GPU.  I find Windows font rendering unreadably faint. I have an astigmatism, and a light sensitivity, so even with prescription sunglasses I can't use dark mode, or bright screens, respectively.  I've tried finding Intel Graphics Software app settings which might help. The contrast settings quickly get too bright for my eyes, so they don't fix this. The right gamma settings might help though, one profile to make medium shades darker to make text bolder, and one to make them lighter to make images bolder.  I tried the support site here, but it has buggy scrolling which triggers my migraines, and it doesn't work with Firefox's reader view:  https://www.intel.com/content/www/us/en/support/products/80939/graphics.html#211011  How can I find or create these profiles? The Intel Graphics Software app doesn't seem to have an option to create profiles.  P.S. I can use the Windows Display Calibrator; if I ignore the instructions, and turn it as dark as possible during gamme, I get readable text at the end; if I turn it as light as possible, I get clearer images. But I can't see  a way to save those and switch without going through the whole rigamarole again.",Neutral
AMD,"Can someone explain to me the differences between all the different names of the Intel CPUs, I’m new to laptops and am trying to learn. What’s the difference between 155H, 255H, and Meteor/Lunar/Arrow Lake and which one is “better”? All the different names get so confusing to me",Neutral
AMD,"I've been chasing down a crashing issue in a Unity program. Memtest86+ ran for 24 hours and cleared 18 passes so I was confident it's not RAM, and I started setting core affinity in an elimination pattern to see if it might be CPU related. I have discovered it crashes within minutes if I set affinity to Core 8, but it's rock solid on Cores 0-7 (haven't individually tested any of the cores past 8 yet but I have it on 9 now and it seems fine so far).  I have a 13900KF (not Overclocked) which is part of the batch that had the microcode issue. My BIOS was updated, but I am now suspicious of this core. Is it likely I should RMA this? Is there something else I should try first?  So far the system itself has been stable but this one Unity program crashes consistently, and I was having tab crashes in both Firefox and Chrome that also resolved when I removed Core 8 from their affinity...  The CPU passed the Intel CPU Diagnostic tool but because the crashes are so specific to core 8 it makes me suspicious.  [edit] RMAd the CPU and the system is rock solid stable now. WHEW!",Neutral
AMD,"i'm trying to get a specific driver for the storage but i can't findd a site with the driver, only setup exes (my old computer will not let it work and the new one needs the driver to install windows). is there a place where i could find the driver itself?",Negative
AMD,"Hi There. I been trying to get a replacement cpu for my 14700 and i chose option 2 to pay the 25 dollor fee (nonrefundable) etc. but my case now has a new number, a differnt worker. and he said that a credit card specilist is gonna called me and im supposed to give him my credit card infomation  edit 1: He also said the intel website isnt secured/safe for creditcard transaction",Negative
AMD,"Hi, I have Intel AX200 160Mhz Gig+ in 3 PC's (on board Gigabyte B550 x 2 & X570). My Router supports 160Mhz, it's enabled and was working getting me up to 900Mbps on my Gigabit plan. and in the last few months I've noticed the 160Mhz option in the intel wifi drivers has disappeared. I discovered this as I noticed the household PC's were struggling to get above 600Mbps most of the time. I double checked the router setting, nothing changed there 160Mhz still eneabled, only changes over the last couple of moths were 2 bios updates each for the motherboards and Intel wifi driver updates. Has anyone else had the 160Mhz option disapear? Any tips on how to get it and my full network performance back?",Neutral
AMD,"u/Progenitor3  yes, it is generally safe to install an Arrow Lake CPU without a third-party contact frame over the long run. CPUs are designed to function properly with the standard mounting mechanisms provided by the manufacturer. Third-party contact frames are optional and may offer additional stability or cooling benefits, but they are not necessary for the safe operation of the CPU. Always ensure proper installation according to the manufacturer's guidelines to maintain optimal performance and safety.",Neutral
AMD,"u/TerminalCancerMan  Intel cannot comment or interpret results from third party benchmark tools. Run [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html) to confirm if there are any issues with the CPU. You may try this, If the motherboard BIOS allows, disable Turbo and run the system to see if the instability continues.  If the instability ceases with Turbo disabled, it is likely that the processor need a replacement.",Neutral
AMD,"u/Progenitor3  Just fill in your info, and it’ll automatically create a ticket for you. Our team that handles those items will get in touch within 3–5 business days.   [Software Advantage Program](https://softwareoffer.intel.com/Support)",Neutral
AMD,Gotta wait for real-world tests to know for sure tho.,Neutral
AMD,"u/Special_Ad_7146 To stay on top of Intel news, **visit** our [Newsroom](https://newsroom.intel.com/).",Neutral
AMD,"u/Hippieman100  just checking can you confirm which software you're using? From what I’ve seen in the latest [ReleaseNotes\_101.6972.pdf](https://downloadmirror.intel.com/861295/ReleaseNotes_101.6972.pdf), there doesn’t seem to be a “Speed Sync” feature in the current version that comes with this driver. It does support V-Sync and Adaptive Sync though. Just wanted to clarify are you referring to the older Intel Arc Control software or the Intel Graphics Command Center? Just making sure we’re on the same page!",Neutral
AMD,"u/TheCupaCupa to better understand and isolate the issue, I kindly ask for some additional information. Please find the details requested below:   1. When the CPU drops to 0.8 GHz, do you notice any **error messages or warnings** in Windows or BIOS? 2. Does the issue happen **only with LeagueClient.exe**, or have you seen it with other programs too? 3. Have you checked **CPU temperatures and power draw** when the issue occurs? 4. Can you check **Event Viewer** for any critical errors or warnings around the time of the slowdown? 5. Are you using any **custom power plans** in Windows, or is it set to Balanced/High Performance? 6. Is **Intel Turbo Boost** enabled in BIOS? 7. **When did this issue first start happening?** Has it occurred before? 8. Have you made any software or hardware changes to the system recently?   Once I receive this information, I will be able to properly assess the situation and provide further assistance.",Neutral
AMD,"u/Alloyd11 Not all 13th and 14th generation processors show instability issues. Just to better assist you are you planning to buy or use one of these processors, or do you need help with your current system?  Let me know how I can support you!",Neutral
AMD,"u/amitsly “crashes” is a pretty broad term, and not every system issue points directly to the CPU. There are quite a few steps we go through to fully isolate the problem before concluding it’s a processor fault.  To help us assist you better, could you please share a bit more info about the crashes?  * When did the issue first start happening? * Have you made any recent changes to the system either hardware or software? * Is there any visible physical damage to the system? * What troubleshooting steps have you already tried? * Have you noticed any signs of overheating? * Have you tested the processor in another working system, or tried swapping it out to see if the issue follows the CPU?  The more details you can provide, the quicker we can get to the bottom of it!",Neutral
AMD,"u/Jay_377 its  likely comes from Intel’s naming convention. The i3-8130U is part of the 8th gen, but it falls under “Kaby Lake Refresh” (for mobile chips), not “Coffee Lake” (which is for desktops). Tools like `garuda-inxi` or `CPU-Z` might label it differently based on how they categorize architectures, not a bug, just naming differences.  [Intel® Core™ i3-8130U Processor](https://www.intel.com/content/www/us/en/products/sku/137977/intel-core-i38130u-processor-4m-cache-up-to-3-40-ghz/specifications.html)",Neutral
AMD,It is because Kaby Lake CPU's are 7th gen processors for laptops and 8th gen to 9th are Coffee Lake check rhis link for better understanding:https://www.intel.com/content/www/us/en/ark/products/codename/97787/products-formerly-coffee-lake.html,Neutral
AMD,"u/Reality_Bends33 Intel has been around for a long time and is known for making solid, reliable products, so you can feel confident about choosing them for your PC build. The Ultra 7 265k and Arc B580 are great picks for video editing, offering the performance you need without breaking the bank. While there’s been some discussion about Intel facing challenges, remember that big companies like Intel usually keep up with driver updates and support, even if they’re going through changes. The tech world is always evolving, and Intel is investing in new technologies to stay ahead. So, you’re likely to get the support you need for your products.",Positive
AMD,"u/unknownboy101 It’s normal for your processor to heat up during heavy tasks like gaming. Intel CPUs are built to manage heat by adjusting power and speed, so they stay safe and avoid damage. However, running at a constant **95°C** on your Intel Core i7-12700H even during light gaming is **not ideal** and could indicate a cooling issue. While Intel CPUs are designed to handle high temperatures and will throttle performance to avoid damage, consistently running near the thermal limit can shorten the lifespan of your components and affect performance. **Feel free to check out this article for more info or steps to try. Just a heads-up this is specifically meant for boxed-type processors. You can still take a look, but I strongly recommend reaching out to your laptop’s manufacturer to get help with the overall system configuration.**  [Overheating Symptoms and Troubleshooting for Intel® Boxed Processors](https://www.intel.com/content/www/us/en/support/articles/000005791/processors/intel-core-processors.html)  [Is It Bad If My Intel® Processor Frequently Approaches or Reaches Its...](https://www.intel.com/content/www/us/en/support/articles/000058679/processors.html)",Neutral
AMD,"u/Frost-sama96 Tenga en cuenta que solo puedo apoyarlo en inglés. He utilizado una herramienta de traducción web para traducir esta respuesta, por lo tanto, puede haber alguna traducción inexacta     **To help me dig a little deeper into the issue, could you share a few details?**  * What’s the **make and model** of your system? Is it a **laptop or desktop**? * Do you remember **when the issue first started** happening? * Which **BIOS version** are you referring to, the one that works fine? If you can share the exact version , that’d be super helpful.",Neutral
AMD,"u/Fluid-Analysis-2354 If your 5070 Ti isn't functioning and your CPU is now your main accelerator, upgrading to the Ultra 9 could be beneficial. The Ultra 9 offers enhanced AI performance, which can significantly improve your computing tasks. If AI capabilities are a priority for you, the upgrade is worth considering.",Positive
AMD,"u/BestSpaceBot , As with all good things, your product has reached the end of its interactive technical support life. However, you can find [Intel® Core™ i7-4700MQ Processor](https://www.intel.com/content/www/us/en/products/sku/75117/intel-core-i74700mq-processor-6m-cache-up-to-3-40-ghz/specifications.html) recommendations at [Intel Community forums](https://community.intel.com/) and additional information at the [Discontinued Products](https://www.intel.com/content/www/us/en/support/articles/000005733/graphics.html) other community members may still offer helpful insights or suggestions.. It is our pleasure to continue to serve you with the next generation of Intel innovation at [Intel.com](http://www.intel.com/). You may also visit this article for more details [Changes in Customer Support and Servicing Updates for Select Intel®...](https://www.intel.com/content/www/us/en/support/articles/000022396/processors.html)",Neutral
AMD,"u/ken10wil   To help figure out what’s going on with your CPU throttling issue, I’d like to ask a few quick questions that’ll help me dig deeper:  * When did you first start noticing the problem? * Have you made any changes to your system recently like installing new software, updating drivers, or swapping hardware? * Is there any visible damage to your PC or loose connections? * Have you updated your BIOS to the latest version for your Gigabyte Z790 board? * Did you try resetting the BIOS to default settings to see if that helps? * Have you tried reapplying thermal paste to the CPU? Just to rule out any cooling contact issues. * Are there any startup programs or services that might be messing with your CPU? * What background processes pop up right after boot? You can check Task Manager or use Reliability Monitor to trace anything unusual. * Can you run the [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html) and let me know if it passes or fails?  Let me know what you find happy to help you troubleshoot further once we have a bit more info!",Neutral
AMD,u/PM_pics_of_your_roof   Interesting thanks for pointing that out! Let me check this on my end and I’ll post an update here once I have accurate information.,Positive
AMD,u/UC20175 Let me check this on my end and I’ll post an update here once I have accurate information.,Neutral
AMD,"I don't think that there it is awful, I mean in the past where the issue blew out and many consumers are reporting. People at Intel can barely accommodate them but they still give the replacement needed for the consumer.   If i am right, that image is necessary for them to get your serial number and other stuff. Unless you have the box of the processor, or you did not purchase it as a tray then I think you'll be fine.",Neutral
AMD,"Hi, I think you'll need to connect with your laptop manufacturer or Microsoft to help you customize an ideal setting for your display. But could you share your full laptop model?",Neutral
AMD,I think this will help you https://www.intel.com/content/www/us/en/processors/processor-numbers.html,Neutral
AMD,"u/Tagracat  May I confirm the exact error message you're seeing during the crash? Also, have you had a chance to contact the developer of the software in question? Regarding the BIOS, could you double-check if you're using the patch addressing instability issues specifically version 0x12F provided by your motherboard manufacturer? Since the Intel Processor Diagnostic Tool passed, not all crashes may be CPU-related.   Looking forward to your response!",Neutral
AMD,"u/Maleficent_Apple4169   What specific storage device are we talking about here? Is it an NVMe SSD, RAID controller, or something else? And what's the make/model of the unit in question? Knowing the exact hardware might help me point you to a more specific solution.",Neutral
AMD,Guess checking with your motherboard manufacturer,Neutral
AMD,"u/ChiChiKiller Just to be clear, what you've described sounds like a possible **fraud or phishing attempt**. Please **do not share your credit card information** with anyone claiming to be from Intel unless you're communicating directly through our **official support channels**. Intel will never say that our website is not secure we take security very seriously, and this kind of messaging is often used by bad actors trying to steal personal information.  I’ve sent you a **private message in your inbox** please check it when you can. Also, could you kindly provide the details of your **initial ticket** and the **new case number** you mentioned through inbox? I’ll investigate this immediately on my end.  Don’t worry I’ll keep you updated as soon as I have more information. For your privacy, I’ll continue communicating with you through inbox so you don’t have to post sensitive details publicly.  Thanks again for bringing this to our attention!",Negative
AMD,"u/Amazing_Watercress_4  Kindly check your inbox, I’ve sent you a personal message. Thank you",Positive
AMD,"u/PeakHippocrazy  This is completely normal behavior for your Intel Core Ultra 5 235U processor. Modern Intel processors, especially those in laptops like your Dell Pro 14, are designed to automatically park (turn off) cores when they're not needed to save battery life and prevent overheating. Your processor has 10 cores total, and Intel's smart technology decides which ones to use based on what you're doing, it doesn't need all cores running just for web browsing or light tasks. The cores will automatically wake up when you run demanding software that actually needs them. This core parking feature is intentional and helps your laptop run cooler, quieter, and with better battery life. Unless you're experiencing actual performance problems during heavy tasks, there's nothing wrong with your system, it's just working efficiently as designed.  *",Neutral
AMD,I fixed the problem. It was windows msconfig disabling e-cores on boot. You should forbid them from doing this.,Negative
AMD,"I can't remember which (not at my pc), I think it's intel graphics command center. I get a list of v-sync options, off, on, smooth, smart and speed (from memory). Speed was not available in previous versions of the software/driver. Underneath there is an FPS limiter and a control for latency improvement (I can't remember the name) with off, on, and on + boost options available.",Neutral
AMD,"Just got back to my PC, it's actually neither Arc Control or Intel Graphics Command Center. I'm using Intel® Graphics Software (25.26.1602.2), should I be using something else?",Neutral
AMD,"Hello, thank you and I'll try my best.  1. No error messages or warnings in Windows, unsure how to check BIOS. 2. Yes, currently I have only found this happens when LeagueClient.exe is started. 3. Using HWInfo64, right when I open LeagueClient.exe, all P and E cores have ""Yes"" in the Current column for ""Power Limit Exceeded"". Core temperatures are: Current- 33C Minimum- 31C Max- 72C Average- 42C. CPU Package Power, minimum is 65.699 W, maximum is 129.230 W. Upon starting the program, the maximum value doesn't change. 4. For Event Viewer, no warnings come up when LeagueClient.exe is started. 5. I have tried setting it to Balanced, High Performance, but I'm mainly on Ultimate Performance. 6. Yes, Intel Turbo Boost is enabled. 7. The issue first started happening 2 days ago, 07/25/25. No it has not occurred before. 8. I have not made any new hardware changes to the system. I did have a windows update, 2025-07 Cumulative Update Preview for Windows 11 Version 24H2 for x64 based Systems (KB5062660) (261.000.4770) installed on 07/25/25, however I installed this update later on during the day after the problem had already started.",Neutral
AMD,"Well, I didn't immediately blame the CPU. The crash message specifically mentions the issue that point the finger to the CPU, plus the provided link a saying that's the root cause.  Anyhow, here are more details:  1. The issue first started happening about 2 hours into my Expedition 33 playthrough and has happened at least 20 times since (in about 18 hours of gameplay) 2. I have not made any changes whatsoever to software or hardware before it started happening. Last night, after the 20th crash, I did run an update for various drivers & the BIOS (including the 0x12F update) using Gigabyte's CC software. 3. No physical damage that I could observe through the PC's glass window 4. I have tried everything the internet had to offer about this specific issue regarding Expedition 33. This includes verifying game files, changing config and settings, reinstalling, lowering the CPU clock speed and more.  5. I didn't notice the temperature when the crashes happened but I'll be on the look out. 6. I don't have a way to swap out the CPU nor do I have a replacement CPU",Neutral
AMD,"Weird, mine isn't in that list.",Negative
AMD,"I5 14600k desktop  When i try Odysse ACC, \*game\* I notice the start up and loading in to the game was so slow, like 8 or 10 minutes to Load.    \--Versión 182011.06 MB2025/05/21SHA-256 ：493D40A2351EED41FCF60E51346B9065880E87F986C1FD1FB1A3008E8C68DA26  ""Update the Intel microcode to version 0x12F to further improve system conditions that may contribute to Vmin Shift instability in Intel 13th and 14th Gen desktop-powered systems.   Updating this BIOS will simultaneously update the corresponding Intel ME to version 16.1.32.2473v3. Please note after you update this BIOS, the ME version remains the updated one even if you roll back to an older BIOS later. ""-- With this BIOS frrom ASUS page, The game do that   But, when i rollback to>   Versión 181211.04 MB2025/03/28SHA-256 ：98528167115E0B51B83304212FB0C7F7DD2DBB86F1C21833454E856D885C7EA0  ""Improve system performance and stability      Updating this BIOS will simultaneously update the corresponding Intel ME to version 16.1.32.2473v3. Please note after you update this BIOS, the ME version remains the updated one even if you roll back to an older BIOS later.""  Intel microcode 0x12B t  The game Load A lot faster and run well.  I dont know what this happens, but is a error caused by New microcode",Negative
AMD,"\- I started noticing the problem between august the 19th and august the 20th.  \- I updated the system around the time I \*noticed\* the problem, but I am not sure if that is when the issue started. I reverted the machine to a state prior to the update and the problem still occurs. Any other driver updates or software installs happened after I had already noticed the problem, and were initiated trying to solve the problem  \- There is no visible damage to my pc or wires  \- my Bios is up to date, and aside from turning on resizable BAR months ago for my GPU, my bios settings are default. I reset to default and turned resizable BAR on again just to be sure, and the issue still occurred.  \- I have not, mostly because it would be a hassle and because the CPU has remained very cool. I have manually overclocked it as a temporary solution to the problem and even under these conditions it is averaging 35-40 degrees Celsius when idle and hovers around 50 degrees when under stress  \- While the issue did not occur in Safe Mode, I am not sure which program is causing the throttling. I have disabled all installed startup programs and still get throttled.  \- No unusual processes pop up right after boot, though the sum of all processes hits 15% CPU utilization, stays there for a while and then the CPU throttling begins (all happens in less than 30 seconds after boot). I could see what happens after disabling even the security related startup programs and see if there is any difference  \- I pass the PDT tests but I have abysmal performance on various benchmark tests, and the PDT takes a long time to complete. Overclocking leads to more expected results.",Neutral
AMD,"Thank you. I spent some more time looking and it appears VROC on x299 was discontinued sometime in the past two years? Seems intel pushed people towards RST. I guess the question still stands since VROC on X299 has moved into sustaining mode, so drives that maybe came out during that time should still work.      [https://www.intel.com/content/www/us/en/support/articles/000026106/memory-and-storage/datacenter-storage-solutions.html](https://www.intel.com/content/www/us/en/support/articles/000026106/memory-and-storage/datacenter-storage-solutions.html)",Neutral
AMD,What I've gathered from support so far is documents needed by section 2.14/2.15 of https://www.intel.com/content/www/us/en/content-details/334026/intel-ethernet-controller-i210-i211-faq.html are behind a premier account registration. I'll try registering an account.,Neutral
AMD,"I'm mostly referring to the fact they've designed the matrix to be printed near invisibly with it being absolutely miniscule and basically the same colour as the pcb, then made it a mandatory requirement for the RMA process.   It reeks of a company trying to dodge responsibility for faulty products by making the hoops so difficult to jump through theres no reasonable expectation the average consumer will be capable of complying.   Why do I need to go out and purchase a macro lens for my smart phone? Its putting me out of even more money than I already am.",Negative
AMD,Ideapad 1 15IAU7.,Neutral
AMD,"No error message per se... the program just closes and there is an event in the event viewer. I chatted with the dev and it looked like it pointed to a virtual memory issue, but it ONLY occurs when bound to core 8. (or all cores including 8)  turns out I was on BIOS 16.02, NOT 17.03 which specifically addresses 0x12F. I will update that next!",Neutral
AMD,it's a NVMe m2 SSD. i don't think it has a specific make/model because I built the computer myself,Neutral
AMD,"u/Hippieman100 **Intel Graphics Command Center** and **Intel Arc Control** have been the go-to software for many users, but they’re being phased out soon, and support will be limited moving forward.  Since system (not your PC) includes an **Intel Arc B580**, I highly recommend switching to **Intel Graphics Software** instead. This newer software is bundled with the graphics driver package, which you can find at the link I’ll provide-[Intel® Arc™ & Iris® Xe Graphics - Windows\*](https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html). Before making the switch, please take a moment to read through all the details and driver descriptions on that page.  Also, based on my checks, it looks like you're currently using **driver version 25.26.1602.2 on your PC**, which is outdated.  Let me know if you have any questions.",Neutral
AMD,"u/TheCupaCupa If the motherboard BIOS allows, disable Turbo and run the system to see if the issues continues.  If the instability ceases with Turbo disabled, please let me know.",Neutral
AMD,"u/amitsly For further analysis, please provide the crash dump or log files generated by the game. You can follow the guide and scroll down to the section titled **""Crashing/Freezing Issues/BSOD""-**[Need help? Reporting a bug or issue? - PLEASE READ THIS FIRST! - Intel Community](https://community.intel.com/t5/Intel-ARC-Graphics/Need-help-Reporting-a-bug-or-issue-with-Arc-GPU-PLEASE-READ-THIS/m-p/1494429#M5057) for instructions.  Once you’ve obtained the files, kindly notify me so I can send you a private message to collect the logs.  For isolation purposes, please try the following step and let me know the outcome:   **If your motherboard BIOS allows it, disable Turbo Boost and observe whether the system crashes continues.**",Neutral
AMD,Yeah that's weird try checking qith laptop manufacturer about it,Negative
AMD,"u/ken10wil  Thanks for the detailed info - this is really helpful for narrowing things down. What I'm seeing here points to a software issue rather than hardware failure. The sudden onset timeline, passing PDT tests, and the fact that safe mode works fine all suggest you're dealing with a software or driver problem.   Your CPU temps are totally normal too, so I can rule out thermal throttling. That 15% CPU usage spike right before throttling kicks in is actually a big clue - something's definitely triggering this behavior.     [Information about Temperature for Intel® Processors](https://www.intel.com/content/www/us/en/support/articles/000005597/processors.html)  [How to Know the Idle Temperature of Intel® Processor](https://www.intel.com/content/www/us/en/support/articles/000090343/processors.html)  [What Is Throttling and How Can It Be Resolved?](https://www.intel.com/content/www/us/en/support/articles/000088048/processors.html)  For overclocking: Note that the motherboard does have an impact on the ability to overclock. Some are better quality and more capable than others. Furthermore, the way the motherboard is set up also impacts the overclocking ability of a particular system (such as liquid cooler vs fan). Please note that if the system was overclocked, including voltage/frequency beyond the processor supported specifications, your processor voids warranty.   **Next steps to try:** • **Check Windows power settings** \- sometimes updates mess with power profiles. Set to ""High Performance"" and see if that helps •   **Look at that 15% CPU spike** \- open Task Manager right after boot and sort by CPU usage to catch what's eating those cycles •   **Try disabling Windows security temporarily** \- sometimes antivirus can cause weird throttling behavior •   **Check Event Viewer** \- look for any error messages around the time throttling starts • **Check Reliability Monitor** \- go to Control Panel > Security and Maintenance > Reliability Monitor to spot any anomaly issues or critical events around August 19-20thThe fact that overclocking fixes it temporarily suggests your CPU is being artificially limited by software, not hardware.   Since you've already tried the obvious stuff like BIOS reset and driver rollbacks, you're probably looking at a Windows service or background process gone rogue. Keep me posted on what you find with that CPU usage spike and reliability monitor - those are likely our smoking guns!",Neutral
AMD,"u/PM_pics_of_your_roof  You're absolutely correct about VROC on X299 being discontinued. Intel moved VROC for X299 platforms into sustaining mode within the past two years and has shifted focus toward Intel RST (Rapid Storage Technology) for consumer applications. While VROC is no longer actively developed, it remains supported in sustaining mode for existing users, which means NVMe drives that were released during VROC's active development period (roughly 2017-2022) should still function properly. However, newer drives may work but won't receive official validation or certification. For anyone building new systems, Intel recommends using RST instead of VROC, but existing X299 users can continue using VROC with supported drives from the qualified vendor list. ***Support is now limited to sustaining mode with no new features or drive certifications planned, as confirmed in the support article you referenced.***",Neutral
AMD,"u/UC20175  Per your inquiry:  The necessary tools and firmware files, but they are only accessible through Intel’s Resource and Design Center (RDC), which requires a Premier account.  1. Register for an Intel RDC Premier Account 2. Visit: Intel RDC Registration Guide [https://www.intel.com/content/www/us/en/support/articles/000058073/programs/resource-and-documentation-center.html](https://www.intel.com/content/www/us/en/support/articles/000058073/programs/resource-and-documentation-center.html) 3. Use a corporate email address for faster approval. 4. From the RDC, search following Content IDs:  EEPROM Access Tool (EAT) – Content ID: 572162  [https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=572162](https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=572162)  Production NVM Images for I210/I211 – Content ID: 513655  [https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=513655](https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=513655)",Neutral
AMD,I guess you are the only one who is having trouble on that part. I mean I haven't seen anyone get mad about it. So I don't really seem to find any fault I mean it is their preventive measure for fake processors that are out there. Just ask for some help to take a picture it's not that hard,Negative
AMD,"I think you should revert the Gpu setting to default. Then start changing some settings in the Microsoft setting. There are settings for fonts, brightness, and such. There is also some setting for the smoothness of the animation in the Windows",Neutral
AMD,I suggest getting a replacement if the issue is the same. But make it sure you update your bios first to the latest version and set it to intel default settings.,Neutral
AMD,"u/Tagracat I’m okay to proceed with a replacement if the recommendation has already been tried and you're still experiencing the same issue.  Just let me know if you're good to move forward, and I’ll send you a personal message to help facilitate a possible RMA.",Neutral
AMD,"u/Maleficent_Apple4169 NVMe drivers are typically **chipset/motherboard-based**, not SSD-specific. The driver you need depends on your motherboard's chipset, not the SSD brand.     Here are my recommendation;    Identify Your Motherboard Chipset:  Check motherboard manual/box for chipset info (Z690, B550, X570, etc.)    Universal NVMe Driver Sources:  Windows 10/11 usually has built-in NVMe support  Try Windows installation without additional drivers first  If needed, use Microsoft's generic NVMe drivers you may contact Microsoft for additional guidance.    Motherboard Manufacturer Support:  Visit your motherboard brand's website (ASUS, MSI, Gigabyte, etc.)  Download chipset/storage drivers for your specific board model    Most likely solution, download Intel RST drivers or AMD chipset drivers based on your motherboard's chipset  this will provide the NVMe controller drivers needed for Windows installation.",Neutral
AMD,"You have misunderstood, I am using intel graphics software already as I said, 25.26.1602.2 is the version number of that software. My driver's are up to date according to intel graphics software.",Neutral
AMD,"Hello,    great news, it solved itself. I opened the program this morning and everything was fine. If this happens again, I will try disabling Turbo and see what happens. Thank you for the help!",Positive
AMD,So what is the QVL list for x299 VROC? I can’t find that information. I can find QVL for C621,Neutral
AMD,"So I was able to download the files from the RDC. Currently when I run        eeupdate64e.efi /NIC=2 /DATA Dev_Start_I210_Copper_NOMNG_4Mb_A2.bin  It says ""Only /INVM* and /MAC commands are valid for this adapter. Unsupported."" This is for three 8086-1531 Intel(R) I210 Blank NVM Device. Presumably in order to program them with device id 1533, I need to run a different command or use a different tool?  Thank you for your help, I feel like it is very close to working.  Edit: I believe it works, using /INVMUPDATE",Neutral
AMD,"[This is literally what I am dealing with.](https://imgur.com/a/WBowyjO)  I'm not even worried about posting this image because please tell me how to get a better photo than this with a regular smart phone, and you can barely see there's anything there.",Negative
AMD,"Yes, I've turned off the system animations, and the blinking cursors.  There aren't settings to switch fonts, to replace thin/faint fonts with bold ones. The folks developing Windows like Segoe Ui, maybe they can read it. There are regedit hacks, but I'm not sure how to do them. There is also Winaero Tweaker, which can switch some fonts, but doesn't work on some older apps with unreadable text. There is MacType, which can bolden text without switching fonts, but doesn't work everywhere either.  There are settings to scale fonts, or scale everything, or reduce resolution. I've tried every combination of these. I've reduced resolution, since it does work everywhere. But I can't go below 1280x720, and I end up breaking some interfaces anyway. There is also the magnifier, but it gives me a migraine.  Here's the thing. If text is too thin/faint, making it 2x as bold will take less than 2x the screen space; making it 2x as big will take 4x the screen space.  I'm surprised how well tweaking gamma has worked. So far it has worked everywhere with dark text on light images, and it's worked very well.",Negative
AMD,I'm now on the new BIOS (with Intel default settings) and the crashing is still occurring on Core 8. Alas... I was really hoping that would fix it.  What are the next steps?,Negative
AMD,"u/Hippieman100 Ah, I see I overlooked that part! Looks like this is the installer version, my apologies for the confusion since we were discussing three different software options earlier.  Alrighty, since you're using the latest version now, how can I help? Are you running into any issues with the new application, or is there a specific feature that's not working as expected?",Neutral
AMD,"u/TheCupaCupa Great to hear it fixed itself! If it happens again, trying Turbo off sounds like a good plan.",Positive
AMD,"u/PM_pics_of_your_roof  You're encountering a common issue, Intel doesn't maintain a centralized QVL (Qualified Vendor List) specifically for X299 VROC at the platform level. Since X299 is a consumer chipset, the VROC compatibility lists were typically maintained by individual motherboard manufacturers rather than Intel directly. The C621 QVL you found is for Intel's server/workstation chipset, which has more formal validation processes. For X299 VROC compatibility, you'll need to check with your specific motherboard manufacturer (ASUS, MSI, Gigabyte, etc.) as they would have maintained their own compatibility lists during VROC's active period. However, since X299 VROC is now in sustaining mode, many manufacturers may no longer actively update these lists. Your best bet is to search for your specific motherboard model's support page or contact the manufacturer directly, though given the discontinued status, this information may be limited or archived.",Neutral
AMD,Relax 😅 and follow this:https://www.intel.com/content/www/us/en/support/articles/000005609/processors.html,Neutral
AMD,"Yeah switching fonts, I haven't tried it. But in windows settings you can change its size and such. Try looking at the Microsoft community or support changing it. But I am glad that adjusting the gamma helps.",Neutral
AMD,u/Tagracat  Kindly follow the article linked below to request a warranty replacement. You can use this Reddit thread as a reference for faster processing.  [How To Submit an Online Warranty Request](https://www.intel.com/content/www/us/en/support/articles/000098501/programs.html),Neutral
AMD,"Thank you for the reply. Sadly asrock doesn’t have a QVL for nvme drives in U.2 form factor or enterprise grade drives, just m.2. Let alone a QVL for VROC support. Looks like I’ll be the test subject for this.",Negative
AMD,"That is what I'm following, I provided support everything on that list, including decoding the matrix on the front of the chip but they don't proceed with the ticket until they get a picture of the code on the pcb which is nigh on impossible.  I wound up taking it into the office to use a high resolution scanner thats used to digitise paintings and just about got something readable. I remain firm in my stance that whilst wanting to verify a serial number isn't outrageous, making the process in which you verify said information virtually impossible is.   I simply don't understand how regular people can take such a detailed photo using smartphone technology. My phone is 4 years old and was absolutely not capable (Samsung S20+)",Negative
AMD,u/PM_pics_of_your_roof  **Best of luck with your configuration!** Your pioneering work might just pave the way for others looking to implement similar setups.,Positive
AMD,"u/Danderlyon  **just sent you a message, check your inbox when you get a chance!**",Neutral
AMD,"Just wanted to follow up for anyone seeing this or is in a similar boat. I can confirm that Intel DC P4510 drives will work in a VROC Raid using a standard key.      Not sure if intel support can confirm but it appears the ASROCK X299 Taichi CLX supports VROC Raid across multiple VMD Domains, which is supposed to be locked behind Xeon scalable CPU/Chipsets. I need to buy two more drives and try to add them to my array to confirm.",Neutral
