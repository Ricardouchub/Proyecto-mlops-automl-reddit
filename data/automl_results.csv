brand,text,sentiment
Intel,"3050 performance at half the power, can't argue with that.  I do wonder how long it will be before the low-end dGPU disappears entirely from laptops, suspect these new chips probably aint it due to price but it's presumably coming.",Neutral
Intel,"While Geekbench's OpenCL isnt super popular for GPU testing;  I still cant believe a mainstream integrated GPU is actually competing with laptop 3050Ti, and these results still aren't with final optimizations!  by the time Panther Lake comes out, Intel claiming that its as fast as laptop RTX 4050 might actually be true lol",Negative
Intel,Are these better than Ryzen igpu? How do they compare to RTX 4050 AND 5050 in non demanding game?,Neutral
Intel,"I'm excited sure but if the claims are true how can anyone here expected these to be reasonably priced and sell well? If it competes with a laptop dedicated GPU it will be priced 2-3 times that laptop. The 285H isn't even that popular and its expensive, hope the 385H is affordable AND the claims are true.",Neutral
Intel,"The score is slightly above a GTX 1650 Super which in turn is slightly above a GTX 1060 which is barely matched by current most performing 128 bits iGPU (Lunar Lake / Strix Point)  Even if we expect 50% improvement compared to 890M, that's still half the performance level of a full power mobile RTX 3060. (roughly half a PS5 too)",Neutral
Intel,This would still no where be close to M4/M5 in single core and GPU,Neutral
Intel,"Hello 6950! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"The problem with buying a device relying on Intel graphics drivers, is that you're getting a device relying on Intel graphics drivers.  I'd rather not.",Negative
Intel,"iGPUs have already killed off the MX series, suppose it’s a real possibility other low end dGPUs also get killed off",Negative
Intel,"More likely what is considered a low end dGPU will shift, just as a current low end dGPU would have been an high end model a decade ago.",Neutral
Intel,"iGPUs is why low end desktop parts dont exist anymore, its going to do the same for mobile parts.",Neutral
Intel,"It'll happen eventually but they have a few issues.  First is memory bandwidth, they'd need to put a wider bus on it to address which is costly.  Second is the memory itself, they need something with higher bandwidth (but that might be addressed with LPDDR6).  To address the bandwidth issue they often need to put on extra cache, the 12Xe3 core is basically maxing out the bandwidth of the 'standard' chip, and if they went to say 20Xe3 cores (which would be 4060+ level) then they would need a bigger memory bus and more cache (Strix Halo put 20mb of MALL cache to try and address, and a 256 bit bus) and that is more design work.  They'd also have an issue with the socket - the APU would be too big to fit into a standard socket and would need a custom one which means a custom motherboard which increases costs.  Another is heat - while more efficient overall, all the heat is in one spot so the computer needs to be thoughtful about moving that out.  A think that in 2028 big APU solutions can compete well against XX50 series and most XX60 cards.  It isn't happening before then however.",Negative
Intel,Apple's M5 already beats the 3050Ti though.,Positive
Intel,I mean the early Iris pro onboard GPU’s traded blows with gtx 650m at a lower power draw. It’s been done before. Still nice to see but nothing ground breaking,Neutral
Intel,We will have to wait for the actual laptops to release & see how they do in real games; performance in synthetic benchmarks does not always line up in real tests.,Neutral
Intel,PTL should have comparable or possibly even lower production costs than ARL. There's some room for optimism.,Positive
Intel,this GPU is also coming into the 355H btw (more specifically there's a Ultra X7 version of the 355H with the full 12-core GPU)     and iirc last time the 255H was pretty popular,Neutral
Intel,"Way we should think about it in my mind is that it's got 12Xe3 cores, and 1 Xe2 core = 2 RDNA 3.5 cores.  Strix Halo has 40 RDNA 3.5 cores (though with a bigger bus and enhanced cache) which is kind of like 20 Xe2 cores (maybe a bit more), and Strix Halo basically ties with a 4060.  If you assume a 5-10% improvement from the architecture going Xe2 to Xe3 to offset the bus and cache (which isn't *as* limiting with only 60% of the hardware), then you're probably at about 60% of a 4060 power, which is more or less where this looks to fall.  4060 mobile passmark = \~17,400  4050 mobile passmark = \~14,300 (81% 4060)  60% 4060 mobile passmark = \~10,500  3050 mobile passmark = \~10,100  That puts it pretty much at a 3050.  With a little sprinkle of cutting edge upscaling and a bit of frame gen it'll be just fine for light titles, but won't be a dedicated monster.  It's just a good solid chip for light-weight long-battery general purpose use that can play some titles on the side (or play older titles with confidence).  I suspect it'll review pretty well, since the reviewers tend to be excessively games-focused and this will appeal, but it's not a games machine - it's a surprisingly zippy long-life chip that's a good fit for all-day laptops.  EDIT: 8060S (Strix Halo) is \~18,000 for reference.  EDIT 2: 890M (strix point's top end) about 8,100, so the PTL 12Xe chip will smoke that.  I'm excited about it, it looks like a great chip!",Neutral
Intel,"It is not 2023 anymore. Intel Arc drivers work quite well, and Lunar Lake graphics are already pretty solid in the low power category. No need to keep repeating this nonsense when you haven't even used the devices.",Neutral
Intel,"Only sticking point seems to be cost I suppose, implementing a truly competitive iGPU looks like it will be expensive if Strix Halo is anything to go by.  I think the Intel/Nvidia partnership might at least come close to making it a reality to have a 50/60 tier integrated GPU in a laptop that isn't much more expensive than one with a dGPU currently... in theory it could/should be cheaper.",Neutral
Intel,Not really? Even optimistically speaking this won't be close to a 5050 laptop when accounting for bandwidth limitations in real world tasks that use the cpu and gpu at the same time.,Negative
Intel,"It's a good point. There really isn't any truly low end dGPU options out there now as far as I know, Nvidia abandoned the GT line and someone up there mentioned MX is dead etc.",Negative
Intel,I’m assuming the caveat to posts like this is “running a version of windows/linux”,Neutral
Intel,"and the 780M when fed enough power also comes within range, and the 890M also beats it.",Neutral
Intel,"It could beat a 5090, it would still be useless until the bootloader is open.",Negative
Intel,Wasn't the previous Intel igpu really good for games and efficient?,Positive
Intel,"Even if what you said was true, I know enough about Intel to know that even if it worked great, they'd find a way to fuck it up eventually. That's what they do.",Negative
Intel,">if Strix Halo is anything to go by.  I think Strix Halo skews how expensive a competitive iGPU could be. Strix Halo was very low volume and was also, imo, overbuilt. I think it's definitely possible to have a more cost competitive big-iGPU SoC",Negative
Intel,Prices for Strix Halo are rumored to fall in the new year; I'm curious to see if it ever reaches 1000-1500 dollar laptops like I've seen suggested it could. Seems like a ridiculous idea right now.,Negative
Intel,And in gaming.,Neutral
Intel,I mean...technically Asahi Linux exists for Macs. Though not the M5,Neutral
Intel,Then there's the 8050S & 8060S,Neutral
Intel,"How so? It is not even beating, at best barely even with 3050 35W (non-Ti).",Negative
Intel,I'm sure thats the first thing that comes to mind when people purchase a thin and light.,Neutral
Intel,"The Arc 140V like in Lunar Lake 268V? it's about 3050 Mobile (35W) sort of performance in real tests. There's a 140T in some chips too, I think it's less powerful than the V.",Neutral
Intel,Strix point is the better comparison,Neutral
Intel,"I hope so. I was a little surprised by the price but as you say, they clearly have a market for that thing and it isn't particularly price sensitive.",Neutral
Intel,they are not really comparable to the traditional APUs,Neutral
Intel,These are 256bit bus devices and have even fatter GPUs .....,Neutral
Intel,"Wait, is the 780/880m close to the 3050? I had heard that it was close to the 1050 at some point... As an 780m owner, it sounds like I can suddenly play way more games than I imagined?",Neutral
Intel,yes that one,Neutral
Intel,"Right, but Strix Point suffers from the same problems that PTL-X will: 128b bus chocking the memory bandwidth.  If iGPUs want to truly replace entry level dGPU, there needs to be wider bus variants",Negative
Intel,"Might seem so, but mobile 3050 is way slower than let's say 1660 laptop.  Still, you can play many games, just at lower res and settings. You can look up benchmarks/tests for handhelds with this gpu (or ryzen z1/z1 extreme)",Neutral
Intel,"No doubt , I meant in terms of availability",Neutral
Intel,Why can't they have a low core count CPU but still keep the full iGPU?,Neutral
Intel,"While this has pretty much been known for a while, you really do get the feeling when looking at these frequency and power stats that Intel hoped for more from 18A.  It's a good node and a good chipset, I think it's targeted and designed well and will do well in the market, but it doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.",Positive
Intel,"6 different top models with just 100mhz between them is so stupid, that should be 1 or MAYBE 2 models for that core configuration  But just hope the 356H is cheap then",Negative
Intel,Is this desktop or laptop?,Neutral
Intel,">16C (4P + 8E + 4LP)  I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  Two clusters make sense. What am I supposed to do with three?!  Unless I'm mistaken, AMD seems to be sticking with full fat Zen 5 cores in both Strix Halo and ""Fire Range,"" and frankly, it sounds like Intel is just trying to keep up in the core count race with its plethora of 'baby' cores.  Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.   I use my laptop for convenience and my desktop for compute heavy work.",Neutral
Intel,"16 cores, but only 4 of them will be fast. The rest will be slow cores.",Neutral
Intel,"Hello Geddagod! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"Nice. My a770 still slaying 4k ultra gaming in 2025. No stutters no freezing just responsive beautiful gaming. Ofcourse it’s paired with 14900k so my igpu technically has identical specs but I keep temps down this way. Also have an RTX 50 series. I love arc graphics. Bang for buck AT HIGHER RESOLUTIONS — that’s where it’s at. And I’ve had 4090 (sold), amd 9070(sold). Never would I pay what they are asking for a 5090. Just because I wasn’t 100 percent thrilled with price per FPS with the 4090. Once you have had all of them you will see Intel is the way to go with 4k gaming(unless you like paying thousands of dollars). Arc doesn’t have shadows and artifacts . RTX does. Bad. Blurs my picture even on best settings possible.",Positive
Intel,Upselling,Neutral
Intel,IMO it's probably not economical for them to do it.   Gaming laptops/handhelds are already niche enough as it is -- doesn't make sense for them to package both and for OEMs to carry both for just for a ~$1-200 delta between the units. Those who want the full GPU will just pay for it.,Negative
Intel,"You gotta remember these CPU’s don’t have multi-threading. AMD gets 16 threads out of their 8 core z1 and z2 extreme but with these core ultra’s you get one core and that’s it. A full on physical core is better than a virtual core, but the same amount of cores with multi-threading is much better than just a same core-count CPU without it. Intel are offering up to 16 cores which is 16 cores/16 threads, but generally should offer better multi-core performance vs an 8-core 16-thread. If anything it’s nice Intel have the ability to improve core count offerings without significantly raising the power requirements.   I for one veered away from the MSI Claw 8 AI+ as a standalone PC solution because it lacked the multi-core performance I needed with it only having 8 physical cores and no multi-threading. A 16-core offering is exactly what I need.",Neutral
Intel,Because nobody would buy it.,Negative
Intel,"yeah, sucks cause they said handhelds was a priority for them",Negative
Intel,"Not great, not terrible. In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.",Neutral
Intel,According to jaykihn0 it's a heat issue: BSPD and transistor density is flattening the VF curve on the high-end. The ST power curve is substantially improved at lower frequencies over Lunar Lake and Arrow Lake.,Neutral
Intel,"If anything, seems strictly worse than N3, even compared to older iterations. That's not great for a node that was supposed to go toe to toe with N2. Especially if the production cost is more in line with the latter.",Negative
Intel,"18a is initially launching with its lower power variant, which is why the only nova cpus that will use it for the compute tile are the laptop ones. The higher power variant comes later",Neutral
Intel,"Jaykihn, whose tweets these leaks are based on, says that the design characteristics cannot be extrapolated to the node itself.   So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.",Neutral
Intel,"> […] It doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.  Well, we already had rumors of Intel struggling to hit actual intended frequency-goals, no?  Though especially the actual performance-metrics of the node will be quite sobering I think.  The power-draw of these SKUs is supposed to be a TDP of *officially* 25W – In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4) and 65/80W (4+4+4 and 4+8+4) respectively.  So it must be seen, if these parts are any more efficient in daily usage, or just a side-grade to Lunar Lake.",Negative
Intel,"Panther Lake is pretty much a pipe cleaner, so it should still improve a bit.  Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD. If that is the case, it will only really be allowed to shine in high powered desktop and server chips, while power limited chips won't be able to reap as many of the benefits of 18A, since laptops tend to target the efficiency sweet spot, rather than clocks.  My understanding is that 18A will scale much better with extra power than N3 and N2. Once you pass the sweet spot, 10% extra performance on N2 might take 30% extra power, while 18A should continue to scale much more linearly for a good while longer.  So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones, especially in use cases where SRAM density is less of a concern. AFAIK Intel CPUs aren't as cache-dependent as AMD, so SRAM density might not be the highest priority, whereas slugging it out with AMD and trying to beat them on frequency might?  From what I've heard, Intel is still just about matching AMD in terms of IPC, while Arrow lake is very efficient, just not great for gaming. If Intel wants the CPU crown back, simply pushing more power could go a long way in clawing back the deficit, assuming 18A does indeed scale well with more power. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.",Neutral
Intel,> 6 different top models with just 100mhz between them is so stupid  They've always done that because of the OEMs who like a million segments.,Negative
Intel,Laptop.,Neutral
Intel,Since when is any user determining what cores they want to use and when?,Neutral
Intel,It makes sense to have three as the low-power cores prioritize keeping the power consumption low for battery life but to achieve this they aren't attached to the ring bus which hurts performance so 8 regular E cores are also used,Neutral
Intel,">I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  From a user's perspective, CPU's are a blackbox and the inner-working details are irrelevant. What matters is results. They want, typically, a balance of performance, battery life, heat / fan noise, and cost, placing greater emphasis on one of these categories over the others.  The idea is that P cores = performance optizied  E cores = Area optimized  LP-E Cores = power optimized.  The theory being to have different core types focus on different parts of PPA.  E cores and LP-E cores are the same microarchitecture. Just LP-E cores are off-ring so the ring (and rest of the cores) can be powered down in light-load scenarios.  Intel's biggest problem is that looking at each of the cores, the P cores perform their designated role the worst. 200% larger than E cores for \~10% more IPC and \~15% more clockspeed is a pretty rough trade off - and by the time you've loaded all P cores and need to load E cores too, that clockspeed difference diminishes hard.  Intel's better off (and all rumors point this way) to growing the E cores slightly and making that architecture the basis of the new P and E cores (like Zen vs Zen-C)",Neutral
Intel,"> Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.  You are the niche use case. The average/median user is using H-class laptops.   These are meant to beat Strix Point and its refresh, which it will do and at better economics for Intel than Arrow Lake H.",Neutral
Intel,"And with all the issues Windows already has with the scheduler, I fully expect it to never be able to properly utilize the different cores. I expect a similar situation as with previous versions, where it's sometimes necessary to disable the slow cores to eliminate stuttering in games, because they sometimes offload tasks to them which slows the whole game down to a crawl.",Negative
Intel,"> I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  > Two clusters make sense. What am I supposed to do with three?!  Ain't these Low-Power Efficiency-Cores aren't even usable by the user anyway in the first place (and only through and for Windows' scheduler to maintain)? As I understand it, LPE-cores are essentially placebo-cores for marketing.",Negative
Intel,Darkmont should really not be that far off Cougar Cove. Certainly not *slow*.,Negative
Intel,The E-Cores are plenty fast for most use-cases by now and there are 8 of them in there. Aren‘t they even ahead of the skylake IPC by now? 8 will be doing a great deal with a good clock frequency.  Only the 4 LP-Cores are kind of weak. But they aren‘t attached to the ring-bus and really only used for offloading low req Backgroundtasks and an idling machine. So that you can turn off the faster cores when they aren‘t necessary.  But for performant gaming and productivity it is basically a 12 core machine.,Positive
Intel,I hope there's a 4+0+4+12 for handhelds but they might not consider adding a whole new tile configuration packaging line worth it over just giving the OEM a discount.,Negative
Intel,I find that weird. Who buys a 20 core CPU for GPU performance equal to like an RX 580?,Negative
Intel,"Yeah as of now only MSI looks committed to using Intel for their handheld, and not only are they not that successful but they had also branched out to AMD for another line of their gaming handheld.",Neutral
Intel,Gaming handhelds are niche but not gaming laptops (relative to gaming desktops). Steam hardware survey reveals a large number of mobile gpus very high up in the charts.,Neutral
Intel,Both Apple's M series and LNL use decent iGPUs for their 4+4 parts.,Positive
Intel,"This is a minority use case, but is a very valid one. I run a home server (/r/homelab) and could use some extra GPU power for video encoding and some basic AI stuff. Though this is just a home server so I don’t want to put in a dedicated GPU due to the desire to not spend all of my discretionary income on my power bill. Right now I have a 9th gen i7 that is good enough for the basics of what I am doing, but I’ve been keeping my eye out for a replacement. I’m not alone in that need, but I do get this is a minority group. I’m sure there are other use cases though. Saying no one would buy it is wrong.",Neutral
Intel,"IIRC Arrow Lake's compute tile is fabbed on N3B, which is pretty much TSMCs earliest N3 node for mass fabrication.  Its other tiles are on a mix of mature 5nm- and 7nm-class nodes.",Neutral
Intel,"> In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.  Arrow Lake was the pipe cleaner for 20A which in turn was deemed unnecessary to move into production because everything was going so well that they could use that groundwork to get a head start and move straight to 18A. That's right from Intel's mouth. Now the goalposts shift once again.",Neutral
Intel,"> Not great, not terrible.  For what it's worth, I'm glad and somewhat relieved, that the never-ending charade of 18A (and ultimate sh!t-show it eventually again amounted to, after their blatant Vapor-ware 20A-stunt) finally is about to come to some lousy end, after years of constant shady re-schedules, even more bi-weekly Intel-lies and basically +2 Years of delays again.  So it will be a bit of … 'consolation' I guess, not having to constantly read that 18A-sh!t in every darn Intel-news.",Negative
Intel,"I'm curious to see the core's power curve itself. Or the core+ring, I forget what exactly Intel measures here for their software power counters.   Intel claims PTL has outright lower SoC power than LNL and ARL, so if the power savings are coming from better uncore design and not the node gains themselves....",Neutral
Intel,"> BSPD and transistor density is flattening the VF curve on the high-end   He doesn't make this claim. BSPD in particular was supposed to help the most at high voltage, and the density does not appear to meaningfully differ from N3, which it still regresses relative to.    Best case scenario, the node, for other reasons, can't hit as high voltages as N3, and the top of the curve is just capped. But you wouldn't expect thermals to be such an issue then.    Doesn't seem to be any evidence of the core-level VF curve benefiting from the node either.",Neutral
Intel,"Jaykihn attributes only density to the supposed heat issues, not BSPD. Which makes sense because the process guy and Stephen Robinson both talked about having to space out the signal and power wires when working with BSPD.  If they prioritized density, then it is possible that frequency could not reach the theoretical maximum due to crosstalk or heat issues.",Neutral
Intel,"18A has always been low density, but expected to compensate for it with very efficient frequency scaling past peak perf/W. It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Wait for the desktop/server chips before you call it. With fewer thermal constraints and much higher power budgets, they should be able to push well past the perf/W peak, where it should continue to scale well and for a long time before it starts hitting severely diminishing returns.  I'm not talking 300W monster CPUs, but the scaling between a 65W and 105W 18A CPU should be more significant than on current nodes, where you might gain like 10-15% additional performance from that 50% power bump. I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.",Neutral
Intel,> So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.  Why would they suddenly decide to underclock it to such a degree?,Neutral
Intel,"Tbh, LNL battery life (note: not the same thing as loaded efficiency) with -U/-P/-H perf levels and market reach would still be a very good thing. It's just a shame to see Intel finally straighten out their SoC architecture only to be hamstrung by a subpar node.",Neutral
Intel,">In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4)      Intel's existing 2+8 parts have a PL2 of 57W, so seems to me OEMs are permitted to target the same PL2 they've been targeting on U series.  >and 65/80W (4+4+4 and 4+8+4) respectively.  Which is a fairly large drop compared to existing H series. ARL-H PL2 is up to 115W, PTL-H PL2 will be up to 65/80W",Neutral
Intel,">So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones,  NVL-S being external adds serious question marks to this.   I wonder if post NVL-S they go back to using further iterations of 18A though for even desktop, if they feel confident enough that they can hit high enough Fmax (even throwing away power and density) on those compute tiles.   Maybe for RZL or Titan Lake in 28' (going off memory for codenames lol).   >especially in use cases where SRAM density is less of a concern.  On paper, Intel has caught up to TSMC (even N2) in SRAM density.   >AFAIK Intel CPUs aren't as cache-dependent as AMD,  Generally speaking Intel uses way higher capacity caches than AMD, AMD uses smaller but faster caches.   >. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.  This seems *extreme*. Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?",Neutral
Intel,"I actually suspect otherwise - speculation on my part, but I suspect that they were running into unexpected process issues when they pumped the frequency and/or voltage higher so they had to scale back performance.  It almost feels like a process bug that they discovered when making the actual chips.  They've also talked about kind of weak yields on 18A on their latest earnings call.  That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  If they have a 'double upgrade' opportunity in both fixing the discovered 18A issues and implementing the originally planned 18AP improvements then it could end up being a somewhat bigger jump.  Don't think it'll be an N2 beater though.  18AP *might* beat the best of N3 in some applications, we'll see.  For that you'd want to look ahead to 14A where the CFO mentioned on the earnings call (and he was somewhat realistic in other areas so it's more believable here) that 14A has been a positive surprise and they're actually pretty excited about it.  It's possible that if Intel gets more experience on High-NA versus TSMC that they might start having an edge later on.  Maybe.",Neutral
Intel,"> Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD   And yet we see the exact opposite here. The leaker claims these go to to 65W, even 80W TDP. There's plenty of power to hit any ST boost the silicon can handle, yet it *regresses* vs N3B ARL. And that's with a year to refine the core as well.    > If that is the case, it will only really be allowed to shine in high powered desktop and server chips   Server chips are low voltage. Even lower typically than mobile chips. High-V only matters for client.    > My understanding is that 18A will scale much better with extra power than N3 and N2   There is no reason to believe that at this time. Notice that Intel themselves are using N2 for their next flagship silicon, including desktop.   Edit: Also, for the ""pipe cleaner"" rhetoric, remember how they cancelled 20A claiming 18A was doing so well and ahead of schedule? Even more obviously a lie now.",Neutral
Intel,Having to roll the dice on the scheduler doesn't make things better.,Negative
Intel,"Since release of Windows 7, which allowed users to easily set which cores are used on per application basis in first party software (task manager).",Neutral
Intel,"It's been a thing for a while as a means to circumvent the Windows kernel's shitty scheduling, especially on processors with asymmetric core or chiplet arrangements. Tbf, Windows has slowly begun to catch up and run cores a little more efficiently, but some people still swear by apps like Project Lasso for manually controlling their process affinity.",Neutral
Intel,"Users aren't ""determining"" anything because they don't have to.   On smartphones, they've virtually zero control over their own hardware.   And on the x86 side, I don't think people are buying big.LITTLE hardware in droves, and even if they are, I doubt the multiple core clusters are their deciding factor.   And if I had a big.BABY CPU, I’d definitely be tempted to play around with it, and yes, I don’t see the big idea behind having three clusters, aside from marketing gimmicks and artificially inflated MT benchmark scores.  From what I've heard, Windows Scheduler has trouble dealing with just two clusters as it is. On Linux, the experience is, at best, tolerable.  At the very least, I've certain issues with Intel marketing these CPUs has having ""16 cores."" I know a lot of 'normies,' first hand, who have fallen prey to this deceptive marketing tactic.",Negative
Intel,"> E cores and LP-E cores are the same microarchitecture   In this case, they're even the same physical implementation. 100% identical to the compute cores. Also, for MTL/ARL they were neither power nor area optimized.",Neutral
Intel,The average user is using U series,Neutral
Intel,">, which it will do and at better economics for Intel than Arrow Lake H.  Doesn't seem like this will be the case till the end of 26' though. At least from the earnings call a few weeks ago.",Neutral
Intel,"The average user runs apps like Teams, entirely off of the LPE core of Lunarlake and Pantherlake",Neutral
Intel,"LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  4 Darkmont LPE, in therory, should be a significant improvement.",Negative
Intel,"They are definitely more than usable if it’s anything like LNL, which basically defaults to them.  If it’s more like ARL or MTL, it’s placebo except for S0 sleep.",Positive
Intel,"The LP cores in Meteor Lake/Arrow Lake were too weak to be usable as multithreaded boosters, this isn't the case with Lunar Lake and Panther Lake will be no different",Negative
Intel,"Windows are supposed to be using these LPE cores for lighter task such as web browsing or Word documentation or idling so that the they won't have to activate the rest of the cluster -> Reduced power draw and therefore good battery life. Meteor Lake originated with these but they were so slow they were pretty useless outside of idling. Lunar Lake also had 4 LPE cores functionally and we see how good those cores are, and these Panther Lake are supposed to be built on that but with additional 8 E cores for people that really value multi-core performance and to also solve the one major weakness of Lunar Lake, if it is even that major in the first place.",Neutral
Intel,>Aren‘t they even ahead of the skylake IPC by now  They're definitely ahead of Skylake now. Cougar Cove IPC vs Darkmont is gonna be like 10% better,Positive
Intel,> Only the 4 LP-Cores are kind of weak   Not for LNL or PTL. They're not crippled like the MTL/ARL cores. Should be full speed Darkmont. That's like 12th or even 13th gen big core.,Neutral
Intel,"The LPE cores being weak is probably the point anyway, or at least partially. It doesn't need to usurp an insane amount of wattage to get to full speed because it doesn't scale well, but it stays fast enough at lower wattage to default to that power profile -> Improved battery life.",Neutral
Intel,They are ahead of Raptor lake P cores and Zen 4 in IPC under 40W,Neutral
Intel,Honestly I wonder if the 4 e-core cluster on the compute tile is outright more power efficient than the 4 p-cores.    I wish some reviewers would do power efficiency testing with different core count configurations enabled on LNL and ARL. Just for curiosities sake.,Neutral
Intel,"At this point, I just hope they have a handheld chip with just 8 e cores (ensuring the CPU tile is as efficient as it gets) and a decent GPU. Maybe in time for an Intel + Nvidia chip, which could be a dream handheld chip.",Positive
Intel,Gaming laptop with iGPU should be niche. Most people just get Nvidia in their gaming laptops.,Neutral
Intel,Apple gets their margins in upselling memory and storage they don't need to worry about the added complexity of more SKUs for the sake of price laddering,Neutral
Intel,"Perhaps fully mature was an overstatement, but it wasn’t the first product on N3B, the node was already in HVM for over a year at that point IIRC.",Neutral
Intel,"We all know that's a lie and 20A was a disaster. Why repeat it now? 18A may not be perfect, but it exists and these chips are comming soon.",Negative
Intel,What delays? 2025 node in 2025?,Neutral
Intel,">BSPD in particular was supposed to help the most at high voltage  Says who?  BSPD improves signal integrity, lowers IR Drop, and helps improve density, at the expense of heat dissipation.   Idk how a technology that is known to increase heat density was supposed to *improve* fMax",Negative
Intel,"> but expected to compensate for it with very efficient frequency scaling past peak perf/W   Quite frankly, the only people ""expecting"" that seem to be people on the internet unwilling to admit Intel underdelivered with 18A. There's been no objective reason to believe that was even a target for the node. If anything, the exact opposite. This was supposed to be the node where Intel focused more on low voltage for data center and AI.    > where you rarely push past the best perf/W due to thermal and battery constraints   That is not the case. ST turbo in -H series laptops has always been high, nearly on par with the desktop chips. At the power budgets being discussed, there's plenty available to hit whatever the silicon is capable of, and Intel's never been shy about pushing the limits of thermals.    And again, when you adjust for the power/thermal envelope, you *still* see a clock speed regression vs even ARL-H. The best possible outcome for Intel is actually that they can't hit the same peak voltage but look ok at low to mid.    > Wait for the desktop/server chips before you call it   Server is typically low voltage, or mid voltage at most. It's lower down than even laptop chips. And what desktop? Intel's using N2 for their flagship chips for NVL, which should really have been an obvious indicator for how 18A fairs.    > I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.   Intel had numbers in their white paper. BSPD, all else equal, was maybe a couple percent at high V. It's not going to produce dramatically different scaling, and again, we have direct evidence to the contrary here.    In general, Intel's had a lot of process features over the years that look a lot better on slides than they end up doing in silicon.",Neutral
Intel,"> 18A has always been *Xyz* …  There's always some excuse for Intel's stuff the last years, to accidentally NOT perform as expected, no?  > It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Hear, hear. Isn't that all to convenient …  > Wait for the desktop/server chips before you call it.  … and when is that supposed to happen? Nova Lake will be TSMC's N2.  What *Desktop* CPU-line will be on Intel 18A anyway then?",Negative
Intel,Give reasons why you claim this is an 'underclock' that was done 'suddenly'.,Neutral
Intel,"Of course, Lunar was quite competitive, yet it was mainly so on performance due to Intel basically sugar-coating the living benchmark-bar out of it via the on-package LPDDR5X-8533 RAM. That OpM for sure *majorly* polished its efficiency-metrics by a mile …  Though looking back the recent years, that's how Intel always masked rather underwhelming progress on their process-technology – Hiding the actual architectural inefficiencies and shortcomings behind a invisible *wall of obfuscation* by only ever deliberately bundling it with other stuff like newer PCi-Express versions of 4.0/5.0 or newer, faster, crazy high-clocking RAM-generations.  So Lunar Lake while very strong, was mainly so due to being propped by OpM, and TSMC's processes of course.",Neutral
Intel,"The article claims it's ""TDP"", which would typically refer to PL1. Might be leakers getting their terminology mixed up, but I wouldn't necessarily assume a large drop in power limits.",Neutral
Intel,AFAIK Intel was at 165W in mobile back then …,Neutral
Intel,"> Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?  [6%](https://web.archive.org/web/20240901235614/https://www.anandtech.com/show/18894/intel-details-powervia-tech-backside-power-on-schedule-for-2024/2)",Neutral
Intel,"I wonder if the issues Intel's facing with 18A partially explain TSMC's very conservative approach to adopting ~~it~~ BSPD.  I image in reality, 18A (and AP's) fMax limits are almost entirely the reason for N2 in NVL-S",Neutral
Intel,"> That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  There's no doubt that given enough resources they can fix the node to the point where it's good enough for HVM in high performance chips. It's what they did with 10nm after all but it took them several years to go from poorly yielding Cannon Lake/Ice Lake to okay Tiger Lake and good Alder Lake.  If we take the talk about 18A ""margins"" from the recent analysts call to be a euphemism for something like node yields and a stand in for when it will be ""fixed"" then we could be talking 2027 before 18A gets to that point. A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.",Positive
Intel,"Yeah I know they talked about yield issues, but everything indicated that it was not a case of manufacturing defect density, but rather something else.   That would imply issues with hitting target frequencies, which as you say, they should be able to iron out in due time. That is as long as it isn't a fundamental issues, which I have a hard time believing, given how unconcerned they seemed about it.   I'm fairly confident it will be ironed out before Clearwater Forest hits mass production.",Neutral
Intel,"There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery. It offers better voltage control and less current leakage, leading to higher efficiency at any given voltage, but especially past the point where current leakage starts becoming more of an issue, i.e. at very high frequency.  A pipe cleaner running at low frequencies is to be expected. Wait for the desktop chips and you will see P cores hitting well over 6+ GHz advertised all core boost, possibly 6.5 GHz. I wouldn't be surprised to see some users hitting 7 GHz stable.",Positive
Intel,The modern CPU cores schedule all cores equally with E core priority because nowadays the difference between E core and P core is less than between AMD X3D CCD and higher clocking CCD,Neutral
Intel,"Windows Scheduler has come a long way but I mean come on, this is pretty pedantic. The commenter above is clearly not using a tool to assign cores to specific tasks they are just being annoying.",Negative
Intel,4 skymont already seem pretty good in LNL,Positive
Intel,> LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  Makes you think why Intel even went all the way to integrate those and waste precious die-space doing so …,Negative
Intel,They could be useful if they could check your email and stuff while the laptop is effectively sleeping.,Neutral
Intel,PLT's LP-E cores take after LNL. ARL/MTL's LP-E cores design is dead,Negative
Intel,I hope those are powerful enough to eventually act as the low-power booster these were once supposed to.,Neutral
Intel,> Meteor Lake originated with these but they were so slow they were pretty useless outside of idling  The last info I had on the back of my mind was that these couldn't even be associated by the user and were basically reserved for Windows itself. Is that still the case now?,Negative
Intel,"Yeah after being reminded that the LPE cores are also Darkmont like the E-Cores I looked up the layout in detail and the differences is simply the P and E cores are on a large Cluster with L3 Cache and a fast ring-bus, but the LPE cores are seperated from that Ring-bus and don't have any L3 Cache (just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster).     They are for sure also much more limited in power-budget and frequency, but they can be very fast in general. The main usage is being able to disable the large cluster in Idle or low cpu usage to heavily reduce power draw.     The communication in between the clusters is much slower. So anything that relies on synchronisation and communication between cores is only really fast (latency-wise) if it is kept within the same cluster.      The LPE cores therefore can be very fast actually, but only if the task stays within the cluster. Modern demanding games will therefore 99% soley run on the large cluster, because they would run slower if they spread across all of them. The small cluster could be used to offload background tasks to themselves (if the powerlimits aren't hit), which could at least improve 1% lows and fps stability.",Neutral
Intel,"It's not just more power efficient, but also much faster at low power which you kinda want since you'd be mostly GPU bottlenecked. The good thing is that they've made that part of their thread director tech.",Positive
Intel,Yes it is. Memory uses power and IPC of LPE should be past Zen 3,Neutral
Intel,Intel makes e-core only CPUs (N100 etc) but at this point they're based on several gen-old e-core design. They're good enough for what they are so they might not update them for a while still though.,Neutral
Intel,"even 4 e-cores is enough, dont forget we use to pair up i7-7700K with GTX1080/1080Ti.  4 e cores + even stronger iGPU probably a better combo for handheld.",Positive
Intel,Apple absolutely does price ladder their SoCs.,Neutral
Intel,"True. But the node itself may be kinda mid. It seemed much, much more complex than N3E, and it's not as if the node was any sort of highlight in the products Apple used it in.   Going back to A17 reviews and such, there were serious questions presented about N3B vs N4P.",Neutral
Intel,I'm fairly sure that's what u/ProfessionalPrincipa was also implying.,Neutral
Intel,> We all know that's a lie and 20A was a disaster. Why repeat it now?   Probably in response to all the people assuming there must be some kind of upside.,Negative
Intel,"It was supposed to be a 2024 node. And given the perf downgrade and admitted yield problems, seems more like they're only delivering the originally promised metrics in '26 or even '27.",Negative
Intel,"> What delays? 2025 node in 2025?  What del— *Seriously now!?* Where you living under a rock the last years by any chance? o.O  18A is neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: *'cause Yields again!*) will be 1H26 and most likely even end of first half of 2026.  So it's a 2H24-node, which Intel is only able to offer actual volume basically +1–2 years later. That's called *»delay«*.  **Edit:** And not to mention the actual massive performance- and metric-regression u/Exist50 pointed out already. 18A is basically 20A in disguise. Since 20A wasn't actually knifed, but just relabeled as ""18A"" instead.",Negative
Intel,> Says who?   Intel. It was part of their own published results about PowerVia on the Intel 4 test chip. The gains range from ~negligible at low-V to around 6% at high-V.    Also explains why TSMC is not adopting it in the same way.,Neutral
Intel,Nova Lake is full on N2?,Neutral
Intel,"It's literally a regression from the prior gen, for a node that was supposed to be ""leadership"", mind you.    I don't know why it's hard to acknowledge that 18A is simply underperforming.",Negative
Intel,"Nah, credit where credit is due. LNL made a ton of fundamental design changes that PTL should also benefit from. Yes, they also benefitted a lot from both having an actually decent node and on-package memory, and no, they're not on par with the likes of Apple or Qualcomm, but merely making ARL monolithic on N3 would not have delivered these gains.",Positive
Intel,They definitely have their terminology mixed up. There's no chance PTL-H has an 65W/80W PL1,Negative
Intel,Which was never the PL1 but rather the PL2.,Neutral
Intel,"Didn't TSMC delay their BSPD node too? Ik they changed it from appearing in N2P vs A16, but unsure if there was a timeline shift in that as well.   Honestly, what's going on with N2P/N2/A16 seems to be kinda weird, timeline wise.",Negative
Intel,2027 just seems like hitting baseline parametric yields. Being able to get to an actual improvement in terms of performance is a whole other thing. Not an easy road.,Negative
Intel,">A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.  How it can be 24 product in 27 when it was 25 node in 25 with products on shelfs in january 26, rofl?",Negative
Intel,Intel themselves claim they won't be hitting industry acceptable yield till 2027 for 18A.,Neutral
Intel,"> There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery.    No, it's not. The main long term advantage of BSPD is to reduce the pressure on the metal layers as they do not scale like logic does. Better PnP is another advantage, but secondary. No one cares much about high-V these days.    > A pipe cleaner running at low frequencies is to be expected   As a reminder, ARL-20A was supposed to be the pipe cleaner, and they claimed they cancelled it because 18A was doing so well! PTL was supposed to be the volume product, a year after 18A was nominally supposed to be ready.    > Wait for the desktop chips   What desktop chips? PTL-S was cancelled long ago, and for NVL, they're moving the good compute dies (including desktop) back to TSMC on N2. A decision which should demonstrate the node gap quite clearly...   Also, ST boost for -H chips is in the same ballpark as desktop ones. They're already at the high end of the curve.",Neutral
Intel,"They aren't the same though, hence the roll of the dice to see if you actually get the performance you paid for.",Neutral
Intel,Because they still improve battery life under very light loads.,Positive
Intel,Yeah I think you had to utilize tool such as Project Lasso. I tried to assess the LPE cores to HWINFO64 and it was rather painful to use so I understand why these cores are never used otherwise lol.,Negative
Intel,"Yeah. The efficiency and overall battery life will probably comes down to how Windows manages threads, which seems to be pretty decent now with Lunar Lake seeing as they got pretty good battery life with the same design philosophy.",Positive
Intel,"> just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster  It's also accessible by the large cluster, fyi.",Neutral
Intel,"For those games back in the day, yeah. But modern games can and do use more than 4 cores.   Depends how low end/power you want to go, of course. 4 cores + eGPU cuold be enough too depending on your games: [https://youtu.be/XCUKJ-AgGmY?t=278](https://youtu.be/XCUKJ-AgGmY?t=278)",Neutral
Intel,"The e-cores are unlikely to be hyperthteaded though. 8 e-cores gives you 8 threads just like a 7700K did, though none of them share cores. It feels like a sweet spot for a handheld in 2025 imho.   This is while AMD competitors use 8c/16t of full Zen. An 8 e-core solution would remain weaker but more than sufficient on the CPU side, while allowing enough power for what truly matters (a capable GPU). If paired with an Nvidia iGPU, that could be the perfect handheld chip, if their partnership produces one like that.",Positive
Intel,"Those ""people"" are either bots or trolls. No sane person thinks Intel is crushing it with 18A.",Negative
Intel,"Whats the name of the close the gap initiative?  5 nodes in 4 years... announced in 2021  2021+4=2025  You can even read 3rd party articles from 2021 talking about 18A in 2025.  https://www.tomshardware.com/news/intel-process-packaging-roadmap-2025   >Intel didn't include it in the roadmap, but it already has its next-gen angstrom-class process in development. 'Intel 18A' is already planned for ""early 2025"" with enhancements to the RibbonFET transistors.",Neutral
Intel,">neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: 'cause Yields again!) will be 1H26 and most likely even end of first half of 2026.  How it aint 25 node when there will be products on shelves in january 26?",Neutral
Intel,FMax at same power consumption is higher true. But the temperature is just too high. Both temperature and power consumption are important considerations,Neutral
Intel,"High end NVL is N2, low end is 18A. At least for compute dies.",Neutral
Intel,"That's what we know so far, yes. At least the performance-parts. The lower end is supposed to be 18A, I guess?",Neutral
Intel,"> Nah, credit where credit is due.  I said verbatim, that Lunar Lake was quite competitive, and I meant it, unironically. It took long enough.  > LNL made a ton of fundamental design changes that PTL should also benefit from.  I haven't disputed that —  The modular concept was surely ground-breaking for Intel, and urgently needed!  Though, it's kind of ironic, how Intel all by itself proved themselves liars, when it took them basically +6 years since 2017, for eventually coming up with only a mere chiplet-copycat and their first true disintegrated chiplet-esque design …  For a design, which *in their world-view*, Intel was working on their tiles-approach already since a decade, which is at least what they basically claimed when revealed by 2018 – I called that bullsh!t the moment I first heard it. Surprise, surprise, they again straight-up *lied* about it …  They most definitely did NOT have had worked on anything chiplets/tiles before, if it took them *this* long.  Just goes to show how arrogant Intel was back then, letting AMD cooking their Zen in complete silence since 2012/2013, for their later Ryzen. *Still boggles my mind, how Intel could let that happen* …  > Yes, they also benefitted a lot from both having an actually decent node and on-package memory […]  In any case, we can't really deny the fact, that Intel basically cheated on Lunar Lake using OpM, eventually creating a halo-product, which ironically was quite sought after, but yet expensive asf to manufacture.  If AMD would've been to cheat like that (using OpM) in a mobile SKU, while dealing some marked cards in such a underhanded manner, it would've been ROFL-stomped Lunar Lake …",Neutral
Intel,"Willing to believe that, though then I have to wonder why it would be so hard to cool at such a substantially reduced PL2. Also if/how they could pitch this as an HX replacement.",Neutral
Intel,"Yes, I think it was 45- or 65W-parts PL1-wise. The spread was still obscene and outright insane.",Negative
Intel,I'm excited for 18A-P. Intel subnode improvements have always seemed to bring decent uplifts (Intel 10SF being a notable example).,Positive
Intel,"The difference is no longer stark. As I said, its now smaller than between 2 CCDs in the most popular CPU line ever made in actual raw performance. ie it's not something entirely unique at the scale the differences are",Neutral
Intel,"How even, if these weren't even used with MTL!?",Neutral
Intel,"Yup, pretty much paper-cores for marketing-reasons alone basically.",Neutral
Intel,We already have games tested on Skymont E cores. They are very fast,Positive
Intel,"Yes we know modern games can use more cores, but having smaller amount of cores means saving power budget. Those power consumption saved are better off use to juice iGPU.   Lets not forget the E-Cores here are skymont, not skylake. Skymont is definitely more powerful cores. The exact same reason why i5-7600K is more power than i7-2600K.  given how iGPU for handheld are no way near GTX1080 yet. I think it is better to pump more juice for better iGPU.",Neutral
Intel,Go to the intel stock subreddit lol,Neutral
Intel,"You'd be surprised how many people are willing to take Intel at their word on it.  Just look at previous threads here. Every time Intel posts nonsense slides, lot of people come crawling out of the woodwork.",Negative
Intel,"They are doing about as much as the ""sane"" people expected from 18A.",Negative
Intel,"> 5 nodes in 4 years... announced in 2021  It was supposed to start with Intel 7 in '21 and end with 18A readiness in H2'24.   https://img.trendforce.com/blog/wp-content/uploads/2023/10/17144859/intel-4-years-5-nodes.jpg  And now in the way end of '25, we're getting something more like what they originally promised for 20A, *if that*, and now they're saying yields won't be ""industry standard"" until as late as 2027. It's a disaster by any objective standards. Their failure with 18A literally got the CEO fired...",Neutral
Intel,"> How it aint 25 node when there will be products on shelves in january 26?  It seems, you don't actually grasp the concept of the term »difference«. *2025 and 2026 is actually NOT the same!*",Negative
Intel,so the igpu is propably tsmc too?  do we know if thats because of poor performence or poor manufacturing capazity?,Negative
Intel,I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   I think they'll probably have ARL-R for HX,Neutral
Intel,"Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage. Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.",Neutral
Intel,"The LP-E cores were a failure in MTL/ARL's design.  They're very useful in LNL and that trend follows with PTL/NVL.  They're definitely not just ""placebo cores"" - LNL uses them more often then the P cores and they can be activated and used in all core workloads as well.",Neutral
Intel,"They were, just not as often as Intel would have liked.",Neutral
Intel,"For MTL and ARL, yes. For LNL and PTL though, I haven't test them out personally but seeing the battery results of LNL chips made me think that the LPE cores of those things seem to be legit.",Neutral
Intel,"Oh year, right, I think I missed (or forgot) that. Darkmont should be even better but I'm guessing not by that much.",Positive
Intel,Kinda making my point.,Neutral
Intel,"Bro xd  Node readiness and product readiness and product on shelves are 3 different date  Node must be ready before product, product must be ready before being on shelves.  Officially 18A was ready around Q2 25",Neutral
Intel,"> so the igpu is propably tsmc too?  18AP, iirc. Good *enough* (or at least, assumed to be when originally planned) not to be worth the extra cost of TSMC.  > do we know if thats because of poor performence or poor manufacturing capazity?  Poor performance. Intel can't afford to be constantly a node behind. It's just too big of a gap.",Negative
Intel,"> I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   But that's exactly it. They said 65W vs 80W was in relation to cooling challenges, but either is more than sufficient for max ST boost. Idk, maybe reading too much into too little information.    > I think they'll probably have ARL-R for HX   At one point it sounded like they wanted to pitch PTL as a partial replacement. Guess it's not good enough though.",Neutral
Intel,"> Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage.  Of course, I already knew that. Binned Desktop.  > Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.  Well, up until 2018/2019/2020 Intel actually didn't really uncapped any of them and those parts were hard-limited to only pull their max 45–65W TDP.  Only later on when they were trying to hold pace with AMD, Intel insanely increased the TDP to pull to insane numbers (90W, 135W, 165W) … Other than that and prior, you had to resort to modded BIOS.  So until even Apple burned their thick skin on some i9 (**Cough* i9-9980HK in the 16"" MacBook Pro 2019), you didn't had such insane TDPs to begin with anyway (except for BIOS-mods).",Neutral
Intel,"As said, I owned a MTL-machine back then, and back then you couldn't even associated these cores after all, since you couldn't pin anything on them – So yes, *back then*, these were placebo-cores and they were in fact pretty much 'on paper-cores for marketing-reason' and basically useless.  Seems Intel managed to improve their performance a lot and make them actually useful, which is good!",Negative
Intel,"Yes, MTL's LPE-cores were basically a dud, LNL was fairly workable and PTL will be hopefully potent.",Neutral
Intel,haha,Positive
Intel,> Officially 18A was ready around Q2 25  Which is still at least 4 quarters *past* its due-date actually …,Neutral
Intel,"That's not strictly true, the (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  https://www.ultrabookreview.com/27215-asus-rog-g703gx-review/#a5  >That comes with a few drawbacks, though, and one of the most important is the noise development. Asus provides three different power modes for this laptop, which you can use to juggle between performance, thermals, and noise:  Silent – CPU limited at 45 W and 4.2x multiplier, GPU limited at 140 W, fans only ramp up to about 40-43 dB in games; Balanced – CPU limited at 90 W and 4.2x multiplier, GPU limited at 140 W; Turbo – CPU limited at 200 W and 4.7x multiplier, GPU limited at 200 W, fans ramp up to 55-56 dB in games.  It was always up to the laptop OEM to configure the power limit and there was no hard cap enforced by Intel.",Neutral
Intel,"The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, negating the entire point of them.  In addition, MTL/ARL had the LP-E cores on the SoC tile which made them functionally useless for full nT load.  LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks *can* stay entirely within the LP island, *and* they're on the same compute tile, so they're also used in full nT workloads.",Neutral
Intel,"> That's not strictly true …  Yes it actually is.   > The (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  Dude, you're basically confirming exactly what I wrote, by literally picking a **i9 9980HK**-equipped notebook of 2019, which is even exactly the very infamous SKU I was talking about …  I was talking exactly on (among else) that very CPU, even Apple couldn't tame and had to undervolt (still without being able to prevent the later sh!t-storm) until eventually abandoning Intel altogether.  ---- All I'm saying is, all the years prior with only quad-core, Intel \*never\* (nor any OEMs for that matter) went above and beyond the official TDP – The only lone exception from this, were those super-bulky Schenker Desktop-replacements.  That only suddenly changed by 2017–2019, when Intel suddenly increased core-count in mobile swiftly from Quad- to Hexa- and ultimately Octa-cores, while pushing the TDP in quick succession  into insane territory.  *That was the time, Asus took about a year to manage applying liquid-metal en masse, for Intel-notebooks!*  You are right with your sample here, yet you only confirm what I said before. Only past quad-cores it was.",Neutral
Intel,"> The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  Do you happen to know, if that (ring-bus related) was the same principle on MTL?  > The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, **negating the entire point of them**.  Yeah, talking about throwing some completely untested sh!t onto the market, irregardless of the fact, if the product makes sense or not — A true classic, I'd say. Just Intel being Intel.  Since dropping those LPE-cores on MTL, would've actually made the mask and thus needed die-space *smaller*, which in turn would've actually *increased* their yields \*and\* in return profit-margins already …  But muh, benchmark bars and ""AMD has moar corez!!"", I guess.  > LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks can stay entirely within the LP island, and they're on the same compute tile, so they're also used in full nT workloads.  Is there any greater penalty (latency or cache-flush-wise) for moving threads off the LP-island to P-cores?  *Thanks for the insides so far though!* I can't really see through anymore to all these constant arch-changes.",Neutral
Intel,"I recently picked up an MSI Claw 7 AI+, and have been pleasantly surprised with the current state of their drivers. I would absolutely be interested in a Celestial GPU if they have anything targeting 9070~ or higher performance level. Here's hoping!",Positive
Intel,I'm glad they are doing these followup videos. Gotta give Intel credit where due - their cards are getting so much better over time.,Positive
Intel,The Lunar Lake handhelds would be really popular if they were just a little cheaper.,Positive
Intel,They said their high idle power is an architecture issue so they can't fix that,Negative
Intel,Fine wine,Positive
Intel,God forbid Intel supports Day 1 GPU drivers longer than 5 years,Negative
Intel,"true but thats not really that big of a deal, and can be fixed with some tweaks [https://www.reddit.com/r/IntelArc/comments/161w1z0/managed\_to\_bring\_idle\_power\_draw\_down\_to\_1w\_on/](https://www.reddit.com/r/IntelArc/comments/161w1z0/managed_to_bring_idle_power_draw_down_to_1w_on/)",Neutral
Intel,So just like AMD then.,Neutral
Intel,"They usually do tho? We dont know about their dGPUs yet because it hasn't been 5 years.  Their Xe Laptop gpus are 5years+ and are supported as of this month.  No idea why you're hating on intel GPUs mate, i'm used to everyone cheering the underdog not the otherway around.",Neutral
Intel,"I think that guy's a misinformed user that is saying that AMD 5000/6000 series won't get driver updates anymore lol, when AMD just said that those gens aren't slated for game optimizations that use their ""latest technologies"" (FSR4 at the time of writing) anymore. Those gens still will get driver updates but AMD gave up on trying to implement FSR4 on them, like the leaked scripts earlier this year had performance reduction from 20% to up to 30% FPS to use FSR4 for 6000 series cards since they don't have the hardware and just using brute force software for FSR4.   I'd agree that my 6700 XT never getting FSR4 sucks, but the misinfo circling around is that we'd never get anymore driver updates lol.",Negative
Intel,"so the A, B, C, (etc) Series is just branding? and they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name? … but Xe3 plus will be enough to call it C series.",Neutral
Intel,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",Positive
Intel,Everything about Celestial I've heard seems to be regarding mobile. Is there actually going to be dedicated discrete graphics?,Neutral
Intel,The Intel driver downloading a shader cache for games from Intel sounds very noteworthy.   I'm really skeptical tying GPU drivers to a manufacturer's online service.,Neutral
Intel,"They don't care. These will be mid tier chips at best. The RTX IGPU ones will be their flagships. These are still damn impressive though, just have to make that clear.",Negative
Intel,"The mega brains of this community really struggle with product naming lol, its so funny to watch supposed intelligent people straight faced say they can't work out what product out 6 to buy because of their product names, most of them are in different market segments so you can't even accidentally buy the wrong one.",Negative
Intel,"They may divide ABC depending on ISA/hardware capabilities used for example. Like AMD used ""RDNA 3.5"" on mobile as full RDNA 4 would pull more power due to more silicon dedicated to AI (including FSR 4 -\_-). RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4. So Xe 3B will be better than current but Xe 3C will do better at DXR and/or AI frame gen/whatever new tech they added.",Neutral
Intel,"This will help you to understand: https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake  This is not brand new architecture, it's refinement on battlemage to work more efficiently. Thinks of it as Xe2.5 but for some reason marketing is calling it Xe3.",Positive
Intel,">they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name?      No what he said is that the Arc B Series hasn't been around for all that long and the B580 has garnered a very positive reception so they felt in terms of branding and marketing moving in Panther Lakes Xe³ into the B series brand is a better marketing move than throwing away the B series branding and starting the C series branding now. In others words they want Panther Lake to ride on the momentum and positive vibes the B series branding has garnered.   Xe³ is a Xe³ based architecture like Xe³P is architecturally but there is more to selling a product than the engineering, keep in mind there's strong rumors that there will be a B770 launch, Intel also just released the B60 and later this year will provide a big software expansion to Bxx cards so the B series brand is still full of life. Xe³P will launch much later and at that point Intel feels there will have passed enough time to leave the B series branding behind.",Neutral
Intel,An alternative explanation could also be that Celestial dGPUs *are* launching as Xe3P and they would want the launch of C series iGPU and dGPU to be around the same time and the same architecture.,Neutral
Intel,">so the A, B, C, (etc) Series is just branding?  Wasnt it always so? Xe is the architectural names, ABC was just branding for customers.",Neutral
Intel,"No public announcements yet, but logically it would make all the sense for Intel to launch them, and do so in larger quantities so people in the market can get them at MSRP. Battlemage proved there is a lot more demand than there was supply, and it'd take too long to ramp it up once it became clear they were received well. With proof of a market test pass, they've now got a chance to do it right.  They are excellent value products that would allow Intel to get a chunk of that market, given they are available in sufficient quantities at intended price points.",Positive
Intel,Is shader caching on first use a real problem? Sounds like bored people needing something to cry about again.,Negative
Intel,>RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4  Do you have a source for this? I vaguely remember 3.5 wasn't really bringing particular changes,Neutral
Intel,That's exactly what everyone's new GPU generation is.  An entirely new architecture is extremely rare for any company.,Neutral
Intel,That's not what Peterson was talking about context wise when he addressed this in the video.,Negative
Intel,"They would be extremely stupid not to continue to release discrete graphics cards, even if they decide to release 2-3 gaming SKUs per new generation; entry/budget, mid-range, high-end, etc.  - they win. until they keep iterating up, maturing driver, increasing install user base, improving software, until the point they can target higher segments (enthusiast tier)  Gaming GPUs remain the flagship component that pushes the limits of silicon design, features like ray tracing, AI upscaling, and real time path racing all debut in gaming before trickling into pro and AI workloads. Every research report shows that demand for high-end GPUs is still strong. Steam HW survey shows the most popular GPUs are still mid-range gaming cards, showing that discrete GPUs remain central to the PC ecosystem.  And yes, gaming GPUs are now marketed as AI accelerators too, Nvidia and AMDs GPUs both highlight AI compute as much as gaming.  Furthermore, if you look at the Steam HW survey, nobody gives a CRAP about 4k/8k gaming, meanwhile 1440p is growing! Nvidia, the leader in GPUs has been marketing for 4k gaming since 2013 with the 780 Ti/GTX 980 era, then 2016 with 1080/1080 Ti, 2018 with 2080/2080 Ti, 2020 with 3080/3090, and marketing for 8k gaming since 2020 with 3090, 2022 with 4090, and this year with 5090. So from 2013 to 2025 Nvidia has been marketing their GPUs as ""4k ready"" and ""8k ready"" but reality is that majority of gamers don't blame games at those resolutions. 4k adoption is only ~2-3% of users, and 8k is virtually nonexistent. So all Intel has to do is really focus on 1080p Ultra and 1440p High/Ultra gaming, above entry‑level (B570, A580) and below enthusiast (future B770/B780?) and keep optimizing and improving ray tracing and XeSS.",Neutral
Intel,[AMD RDNA 3.5’s LLVM Changes - by Chester Lam](https://chipsandcheese.com/p/amd-rdna-3-5s-llvm-changes),Neutral
Intel,"There are more to that, if you look at RDNA 4 and blackwell for example they had changes to the SMs, enc/dec, etc.   Here it using basically the same cores, but changes are made to better utilize those cores. I think this sentence summarize it the best:  ""Intel classifies Xe3 GPUs as part of the Battlemage family because the capabilities the chip presents to software are similar to those of existing Xe2 products"".  Now I'm not saying it's a bad thing, Intel has a lot of inefficiencies with Battlemage, a lot of time you expect it perform better judging by the amount of hardware. By going this route the basically giving pretty good improvement only year after initial release with basically almost the same drivers. It's just not a new generation but something in the middle between a refresh and a new generation.",Neutral
Intel,"Not rare, but more like it only happens when there's a paradigm shift in graphics that necessitates it.   GCN -> RDNA or Pascal -> Ampere are the most recent ones.",Neutral
Intel,"its not extremely rare. It happens once every 7-10 years or so when they redesign it for significant changes. but yeah, most generations will be refinement.",Neutral
Intel,"In the end it is all matter of marketing, filter it for a moment and understand what it's. As he said ""leverage all the good work from battlemage"". As I read it and looking the architecture it's battlemage with some enhancement that will come to future product celestial. Something like RDNA 3.5. I think they messed up the naming - they want to present it as something new and different (it's in some aspects) but it's not the real next gen in their roadmap.",Neutral
Intel,By discrete iam assuming you mean client graphics SoCs? The margins are not enough on those - you make much more money on Pro cards/ HPC.,Negative
Intel,"No this is not the case. The decision is purely branding. Xe3 and Xe3P are part of the same generation architecture wise unlike Xe2 and Battlemage, he addressed this.",Neutral
Intel,It's this. IGPU's will be for gamers and DGPU for data center. They basically both have said this :) People just don't notice anything. What do folks think RTX IGPU's on intel chips are gonna be for. LIKE LOL!,Neutral
Intel,"Then why he said ""when we move to our next architecture...""? Meaning they are not the same (Xe3 and Xe3P), also look at the slide they're grouping Xe2 with Xe3 under battlemage.  What you say contradicts their statements, and the article. The decision is of course marketing but nowhere they say Xe3 and Xe3P share the same architecture. This is why I said before it should've been called Xe2.5 or something of that sort like with RDNA 3.5.",Neutral
Intel,Next architecture is just rearrangement of existing cores into more effective set up. No contradictions.,Neutral
Intel,"When we will see deep dive into X3P celestial we will see what have changed. Until then it's just speculation of the differences between Xe3 and Xe3P. My bet there will be more changes than mere ""rearrangement of existing cores"".",Neutral
Intel,"Love a good TAP interview, hopefully he'll do some podcasts too once the embargo lifts.",Positive
Intel,"I like how TAP said ""expect even bigger investments in graphics in the future"". I hope this means what I think it means, because Intel \*must\* continue heavy investment in their own graphics. Accelerated compute is the future.",Positive
Intel,MLID must have an aneurysm seeing the guy still employed at Intel,Negative
Intel,"Tom's an absolute gem. It's so frustrating to see the most passionate and effective communicators locked up behind embargoes and NDAs, so fingers crossed that this marks the next wave of tech exploration and deep dives. Always nice to see him pop up in a random GN or PC World video.  Credit where it's due, Lex (Alejandro) has also always been attentive and forthcoming in his Reddit interactions, even factoring in the usual embargo/NDA constraints. Hopefully he'll dust off his u/LexHoyos42 account; I'd love to see some mini-AMAs as Panther Lake trickles out.",Positive
Intel,Igpus not discrete gpus,Neutral
Intel,"I would not attempt to read much into claims like that. This is marketing, not an investment roadmap. I'm sure if you pressed them on it, the lawyers would have some words.   Remember, they've yet to even *talk* about dGPUs past BMG. And the big-iGPU NVL is reportedly cancelled (likely in favor of the Nvidia partnership).",Negative
Intel,Hahaha I can hear Tom saying “fucking Tom Peterson has been *lying* to you” as you say that,Negative
Intel,Problem is we don't know the real roadmap is for Intel. I really want them to put out a roadmap like they used to. But things in flux probably :s.,Negative
Intel,I think only way for intel to enter consumer GPU space is to start via iGPU at this point.  They just need to make bigger faster iGPU to make Nvidia RTX xx50/RTX xx60 redundant.,Neutral
Intel,I don't think partnership Nvidia iGPUs will be ready until TTL/HML circa 2029/2030.,Neutral
Intel,"Agreed, but then similarly I would also not attempt to conjecture on their roadmap outside of an investment roadmap... lack of a public roadmap update on graphics in a while is certainly saying something by not saying anything, but it's also not saying cancellations, etc...",Negative
Intel,There are just too many Toms in tech media. Tom of Moore's Law is Dead needs to thin out the competition.,Negative
Intel,I don't think even Intel knows what the real roadmap is anymore. They're so far behind on the previous ones that it's not surprising that they're not releasing new ones.,Negative
Intel,We are not competing with higher end GPUs. It seems they want to capture the budget segment and then move up the ladder.  But I don't like that iGPUs has more priority than dGPUs. Hope celestial comes into life in future,Negative
Intel,"Probably, but no sense offering an in-house big iGPU for just one gen. It needs to be a long-term commitment.",Neutral
Intel,"I mean, the layoffs tell a story. The dGPU group, and client graphics SoC in general, was gutted even before Lip Bu came on. To say it's likely a skeleton crew is probably doing it a favor.",Negative
Intel,You stole what I was going to say... take my upvote.,Negative
Intel,"thats how they are doing it.  iGPU is the one that killed Nvidia MX, or GT xx30 series. So their next target is the RTX xx50.",Negative
Intel,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  One thing seems clear, this solution will be cheaper than standard AI data center products.  Hopefully, Intel can get and maintain footholds in medium-sized AI research market",Neutral
Intel,No way that power usage is at idle. Even if there was only one performance state like Tesla GPUs they wouldn't consume that much power.,Negative
Intel,that's insane vram density,Neutral
Intel,"This thing only has a 1Gb Ethernet port? I don't know much about the use case for this thing, but that seems surprisingly low. Simply from the use case of uploading training data to this I feel like something faster is necessary.",Negative
Intel,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Is there a fork of chrome that runs on gpus,Neutral
Intel,"Now do it with pro dual version of b770s, 64gb each. Could be a far more economical inference solution than what AMD is providing",Positive
Intel,but is that faster than a single 5090?,Neutral
Intel,Is this enough VRAM for modern gaming?,Neutral
Intel,Nvidia: ill commit s------e,Negative
Intel,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  If they rely on PCI-E for interconnect, bandwidth will be anemic between cards for training purposes. Might be ok for inferencing, but for training, you need the bandwidth.  That's not unclear at all.  This thing is probably not useful for anything, but serving many users with small models for inferencing.",Neutral
Intel,"I doubt we'll see any UALink this year, but perhaps  on the 160GB  Crescent Island card next year.  Intel hasn't announced anything, but it seems obvious.",Neutral
Intel,"You think Intel is going to maintain these footholds any longer than they would need to?  This push of their dedicated GPUs into the professional space, is just a mere attempt to get rid of the unsold inventory of ARC-GPUs at the highest possible price-tag right from the beginning – Dumping those into the enterprise- and datacenter-space, in noble hope that someone clueless might bite upon the Intel-brand, and then dip into it at high costs *at non-existing support software-support* Intel pretends to deliver some time in the future.  Since as soon as the bulk of it was sold, Intel will cut the whole division and call it a day, leave anyone hanging dry.  That's what's planned with the whole professional-line of them – Buying this, is a lost cause as a business.",Negative
Intel,"I think they've just taken the 400W per card TDP and multiplied it by 16 to get the 6400W figure, so I think they're trying to say minimum power draw at full load, not at idle. ""Minimum power draw"" is definitely really odd phrasing for that number. Surely you'd think they'd say ""Total GPU TDP"" or something less confusing",Neutral
Intel,I don't think servers are supposed to stay idle for long.,Negative
Intel,"> the 768 GB model uses five 2,700 W units (totaling 10,800 W), while the 384 GB version includes four 2,400 W units (7,200 W)    And an insane power consumption.",Neutral
Intel,You basically always put in dedicated NICs in those kind of servers. The onboard ethernet is rarely used.,Neutral
Intel,Could that make it very cost effective for any particular use cases?,Neutral
Intel,"Even the fancy NVIDIA interconnects are a bottleneck for training performance, as are the memory bandwidth and even L2 bandwidth. One architects the algorithm around that. A tighter bottleneck is obviously not ideal, but it might still make sense depending on the price of the hardware.  As long as a copy of the full model and optimization state fits on the 24GB bank the PCI-E interconnect isn't really too limiting for training. Small models, like a 1B parameters voice model, will naturally fit. The larger the model you want to train, the more complex the architecture might be, but there are ways to train even models over 100B parameters on such hardware, like routing distributed fine grained experts. Not that I think it's a good idea at this point.",Neutral
Intel,It's VideoCardZ so they probably got information from a tweet or Reddit post and copy/pasted without doing any thinking whatsoever.,Neutral
Intel,"I assumed that would be the case, I just didn't see any room for expansion. It's pretty crowded with 16 full sized cards.",Neutral
Intel,"I don't really see any, unless you can find a workload that will fit the card and multiply that workload for 32 users, but as each chip is performance wise less than a 3090, it has to be a fairly light workload.",Neutral
Intel,"1B might work for a voice model, but useful LLM models ARE 100GB+. Smaller models are fun little toys.",Positive
Intel,At least its a human hallucination and not AI hallucination.,Neutral
Intel,Can also be bad translation.,Negative
Intel,"according to the datasheet theres two free pcie 4.0 x16 slots on one version and two pcie 5.0 x8 slots on the other one. Both say ""support high speed NIC"" so I assume there is at least enough room for that",Neutral
Intel,"I wonder if these might make good controllers for ai powered robots. Something highly specialised, like a fruit sorter or something. I don’t know. I’m just hoping Intel can find a buyer because it’s good for the market to have more competition",Neutral
Intel,"It's a thing where the next generation of this card would be a viable competitor for the generative AI crowd, but not this one, where it can't compete with a 5 year old 3090.  For robotics, you just want inference. There are much better options focused on low power, small form factor NPUs custom made exactly for that, and they can be paired with small SBCs like a Raspberry Pi.",Neutral
Intel,No word on OMM so I guess that's reserved for Xe3-P. Wonder what other changes that architecture XE3-P will have.  The new Asynchronous RT Unit functionality in Xe3 is very cool and now Intel also has Dynamic VGPR. Neat.,Positive
Intel,"It's interesting they lump it in with B-series, given the architecture is pretty distinctly different from Xe2 used in Battlemage. Note that BMG/Celestial names have been explicitly used for dGPUs, however. They're not synonymous with Xe2/Xe3.",Neutral
Intel,"It would be nice if Intel released a small, budget friendly, passive cooled, PCIe powered dGPU with multiple QuickSync units, made specifically for AV1 encode and H.265 4:2:2 chroma with 10-bit depth.   It's unfortunate that one has to cough up $2,000+ (at least around here) for an RTX5090 to have three NVENC units at their disposal, and the 5090 still can't compete with a passive-cooled Apple Macbook that draws a couple of watts at peak.   You know things are crazy when Apple is the only company that caters to your specific needs!",Neutral
Intel,"Hello Dangerman1337! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Very bleak chance that Intel will continue with discrete gpu line up. Pretty shit decision tho,Negative
Intel,"So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In&nbsp;card under this precisely defined term.  Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – *In favour of Nvidia-GPUs?*",Negative
Intel,Xe3P needs that feature parity with RDNA 5 and RTX 60 overall since its a 2027 product.,Neutral
Intel,"hopefully that means NVL-AX is not getting axed. at least, the 24 core die. heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch. super weird, not believing too much into it. curious about Druid, people seem optimistic, idk why lol.",Neutral
Intel,yeah super weird -- but that does bode well for the next dgpu being based on Xe3,Neutral
Intel,"Arc Pro B50 was pretty interesting to me, it was a modern 75W card that isn’t gimped.",Positive
Intel,With a board with lots of PCIe x16 slots you can stack A310's or A380s and get along fine. I do 3 transcodes of av1 great on a single one right now.,Positive
Intel,The reason they'll never make this is the same reason they don't put SR-IOV on consumer cards.,Negative
Intel,Can you do this work on a $600 Mac mini? Or do you need something more expensive.,Neutral
Intel,"Yeah prob, but by how much. Looking forward to the post CES reviews and Xe3 better be a major leap over Xe2.",Positive
Intel,They will if they have xe cores that gen. They'll only fully kill it if they abandon that core development entirely to exclusively use Nvidia graphics tiles,Negative
Intel,">Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  What in the world are you talking about. This roadmap says nothing of the sort.",Negative
Intel,"No, they are saying the GPU in Panther Lake is Xe3, but instead of classifying it as a new-gen architecture, they are saying it's just an extension of Xe2, so they've put it under the B series product family.",Neutral
Intel,100% but I fear this is another botched gen so it will at best be on par with current gen offerings.,Negative
Intel,"Intel buying chips from Nvidia won't be as cost efficient as them developing their own chips. Intel can't abandon GPU development in the short term or it'll be left behind in the long term (Nvidia can easily sell their Intel shares and pivot away from Intel leaving Intel with nothing).    If Intel will stick to making dGPUs and expand to competing in gaming with said GPUs they benefit greatly from the experience and talent that dGPU development yields, this makes dGPU development a no brainer. If AMD can afford to make dGPUs Intel definitely can as well.",Neutral
Intel,"> heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch  Would be very interesting if so, though I'd be surprised if projects get resurrected in this spending climate. Competitiveness might also be tight in '27.",Neutral
Intel,Celestial was based on Xe3p.,Neutral
Intel,"Then that can't just be a minor tweak, but perhaps it's just market segmentation.",Neutral
Intel,I don't think Mac minis support hardware AV1 _encoding_. Apple only recently supported hardware AV1 _decoding_ so their devices aren't the best suited to AV1 media,Negative
Intel,> They will if they have xe cores that gen  Why do you believe so? They made iGPUs without dGPUs before.,Neutral
Intel,"They won't Nvidia tiles are for specific market segments, they aren't a one size fits all solution.",Neutral
Intel,That's not what my colleague's say,Neutral
Intel,It wouldn't be a Helpdesk_Guy comment if it didn't include insane made up bullshit to disparage Intel.,Negative
Intel,"Exactly. *Good Lord are y'all shortsighted!* Are you falling for their shenanigans and typical tricks again?  Crucial is, what Intel does NOT mention – That is anything future dedicated graphics, as in *Add-In&nbsp;cards*.  They're purposefully declare PTL as de-facto dedicated GPU, yet any thing of future **d**GPUs is ABSENT.",Negative
Intel,"Congrats, you looked at the pictures. We all did. Now look at it *again* …  Now explain to me, why ARC alchemist is separated (since it's a line of dedicated graphics-cards, duh?!), while Battlemage gets PTL's X^e Graphics iGPU-tiles put alongside, despite such tiles classically does NOT count as a graphics-card but rather a iGPU (no matter how large).  The actual answer is, that this on the slide was surely NOT done slide by accident, but it figuratively puts PTL's X^e Graphics tiles of PTL's iGPU *on par* with their dedicated Battlemage-cards, which (at least in Intel's eyes) shall from now on signal a ""continuation"" of  Battlemage itself. Thus, there won't be any further dedicated graphics-cards like Celestial as classical Add-In cards, which still is way overdue anyway …  Since if not so, then WHY are PTL's iGPU-tiles labeled as ""Intel ARC B-Series""?! *You see the virtue signalling?*    You're also aware, that Intel recently dropped the Intel Iris&nbsp;Graphics theme and now calls all iGPUs ""ARC""?",Positive
Intel,">Intel can't abandon GPU development in the short term  Well Intel now uses the same architecture across both their iGPU and dGPU, so even if their dGPU products get killed, iGPU demand(indirectly, through Core Ultra processor demand) can fund continued GPU architecture dev. Intel still has the majority market share in the iGPU market, so they have the demand to justify continued R&D into GPU dev for iGPU alone.",Neutral
Intel,Xe3p is a significant architectural advancement says Tom Petersen.,Positive
Intel,"Care to elaborate where I was blatantly wrong and it didn't actually turned out mostly or even exactly as I predicted prior when popping the prospect of Xyz, often months to years in advance?  A single example would be sufficient here, mind you.",Negative
Intel,"They did not declare PTL as a de-facto dedicated GPU. By definition it's *not* a dedicated GPU and they never said it is.  They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs. You're trying to fill in missing info with a nonsense interpretation.   What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".",Negative
Intel,"So much buzzwords, yet it sounds like a stroke.  You need help.",Negative
Intel,"Alchemist had both desktop discrete and mobile discrete. Intel's roadmap isn't a 'graphics card only' chart, it's an IP roadmap. That's why Panther Lake's Xe3 iGPU shows up under Arc B-series, they are saying Xe3 is an extension of Xe2, where as Xe3P (Celestial) is the next Arc family.",Neutral
Intel,"> A single example would be sufficient here, mind you.  Sure, here you go:  > So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In card under this precisely defined term.  >Their non-show is basically code for: “We're canceling everything dedicated, and our eGPUs we consider now dGPUs.”  >So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – In favour of Nvidia-GPUs?",Neutral
Intel,"> They did not declare PTL as a de-facto dedicated GPU.  Except that's exactly what they did – look at the damn pictures my friend, Intel purposefully group it into the same category as Battlemage's dedicated GPUs and Add-In graphics-cards and thus, iGPU-tiles as on par as dGPU cards.  > By definition [e.g. Panther Lake's iGPU-tile] it's not a dedicated GPU and they never said it is.  \*Sigh I *know* this, well all do! Yet Intel wants to pretend, that PTL's on-die eGPU X^e Graphics tile is now basically declared, what *Intel* now considers a **d**GPU. The implication is strikingly obvious here.  > They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs.  Ex-act-ly! You really don't get it, don't you? They deliberately leave everything what *classically* would count as a dGPU open and without ANY path forward, meanwhile their X^e 3-tiles are now considered dGPUs by Intel.  > You're trying to fill in missing info with a nonsense interpretation.  No, I don't. I'm just one of those being able to decypher Intel's typically backhanded tricks and try to explain that they're virtue signaling the worst: ARC as a line of dedicated graphics-cards is dead and won't get a follow-up, instead, Intel now considers it replaced by iGPU X^e Graphics-tiles (like the one in Panther Lake).  > What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".  Their trickery here, that they basically announce the discontinuation of ARC graphics-cards through the back-door.",Negative
Intel,I wonder why they're using such naming. If Xe3 is just Xe2 enhanced then why not call it Xe2.5 or Xe2+. And if Xe3P is a new architecture why not call it the real Xe3?,Neutral
Intel,"Just wait and see. I sadly have a horrific on point streak to remain in the right with anything Intel, even if I call things months to years in advance.",Negative
Intel,"The implication here is incredibly obvious:  ""Intel Arc B-Series consistes of Battlemage discrete graphics *and* PTL iGPUs"".  That's the implication here. It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL",Neutral
Intel,Because it's most likely still the same foundation. Xe3 just have additional optimizations and HW blocks along the lines of OMM (confirmed) and whatever new tech Intel decides is worth pushing for higher end parts.    But perhaps it's more of a RDNA 3.5 situation. Some early Druid tech backported to XE3P.    Yeah I have zero clue just guessing xD,Neutral
Intel,"> It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL  Of course not, since it would instantly tank their reputation and will kill all further ARC-sales?!  Do you actually think, that Intel is that stupid to plain announce the death of ARC-series graphics-cards? Do you think they'd even sell a hundreds cards after such a announcement? Who would by a dead-end line of graphics-cards, which won't even get any further driver-support and still have disastrous drivers like Intel's ARC? NO-ONE!  Of course they virtue signal it through the backdoor here, to preserve the sell-off the of the rest of pile of shame of their B-series graphics-cards as long as possible (leaving users with none support afterwards), and *only then* pull the plug afterwards when they went through their inventory …  Since when are you following Intel? Intel has always done so, and only announced a sudden discontinuation once the inventory was sold (leaving customers in the open with no further support overnight), despite virtue signalling a continuation all the time (to preserve a proper sell-off).  ---- Wasn't this exactly the same story with **Optane** back then? Each and every question upon a future support was answered in the affirmative, only to pull the plug overnight again as usual …  I'm not another piracy of cons here, it's Intel pulling the same Optane-stunt with ARC – Happily selling off their cards, knowing darn well, that they will pull the plug, yet refuse to actually play with open cards again.",Negative
Intel,"> > It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL >  > Of course not  Great, we're all agreed that you're making shit up when you said  > Panther Lake now Intel classifies as a dGPU",Negative
Intel,Just letting it be known that the [original source](https://www.reddit.com/r/IntelArc/comments/1nomuhk/driver_built_xess_frame_generation_might_be_on/) is over at r/IntelArc,Neutral
Intel,"Intel ""ExtraSS"" frame extrapolation tech has been in development for quite a while. I really hope it really does come with B770. Will be exciting to see how the implementation differs from MFG.",Positive
Intel,Could be? Wasn’t one of the lead developers for Asahi Linux’s graphics stack hired by them for this specific purpose?,Neutral
Intel,I do have to wonder how much we are missing out by these features being proprietary rather than having graphics vendors work with other stake holders and each other to make open cross compatible upscaling and frame generation techniques. It's been great for nvidia but bad for the ecosystem as a whole for everything to be so fractured.,Negative
Intel,"Yet more evidence Intel is likely not scrapping arc, as if we couldn't guess they know the trajectory of past nVidia semicustom affairs.",Neutral
Intel,"Nice to see. Still pretty happy with my Arc card, good enough for game and resolutions I’m playing and the AV1 hardware encoder is just excellent.",Positive
Intel,Wonder if well be getting ray reconstruction andmaybe even a transformer model soon.,Neutral
Intel,we are witnessing the downfall of pc gaming in real time,Negative
Intel,Why should they? They are going to buy NVidia GPUs for everything now.,Neutral
Intel,"Hello brand_momentum! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,This could be awesome more completion The better,Positive
Intel,Doubt anyone would use it for practical application. FSR Frame Gen is awful to use.,Negative
Intel,"Multi-Post News Generation, with three articles interpolated per source.",Neutral
Intel,I'm excited for Intels new GPU,Positive
Intel,"Microsoft has been way too slow in the DX12U era, and Kronos is even slower. Microsoft's new APi that integrates AI upscalers isn't even out yet and it doesn't even cover Frame Gen or AI denoising. By the time MS moves to integrate those they'll be behind on something else.",Negative
Intel,"They aren't, though. FSR Frame Gen works on Nvidia Hardware",Neutral
Intel,Honestly we are gaining more from proprietary features than we lose from them being locked behind certain brands. Currently them being proprietary gives Nvidia an incentive to push the envelope and the head start they have isn't that big so they're incentivized to keep making new features   If everyone was required to share their new features then the incentive for innovation disappears,Neutral
Intel,Competition *,Neutral
Intel,Obligatory article quoting reddit post quoting another article quoting original reddit post.,Neutral
Intel,"Fake frames, fake articles! /s",Negative
Intel,Sometimes I prefer vendor solutions today than waiting for microsoft for years like Direct Storage,Neutral
Intel,"Yes but FSR support is all over the place. Look at what optiscaler is doing, we could have had an open standard for upscalers that FSR XESS and DLSS could have been built on top of meaning much wider support across games instead of every game needing specific implementation and leaving us with outdated upscalers that we need driver overrides and DLL swaps to get around.  Microsoft is only now working on a directX based upscaler API that solves this problem. We should have had something like that years ago like we did for RT.",Negative
Intel,"I mean, surely it depends on when they have to share it. If they can release with the feature exclusively first, but then have to share at or after release, then it can still help the current generation of their cards until it's time for the new gen.",Neutral
Intel,"It makes adoption slower though. Especially for smaller devs and the smaller graphics vendors. Even now with pretty large games we still have software lumen only. If RT was pushed as an open standard earlier we might actually have more games and better implementation across the whole market, not just nvidias pet projects.",Negative
Intel,"DirectStorage 1.1 (the actually usable version unlike 1.0) was supported by Windows 9 months after PS5 release. They werent that far behind on that, noone just was interested.",Neutral
Intel,"patents expire after 15 years, they will have to share it then.",Neutral
Intel,Nvidia controls 90% of the discrete GPU market on PC.   Anything proprietary that they make essentially becomes the standard going forward.,Neutral
Intel,"Huh?  1. RT in games is now done through DirectX APIs, which are vendor agnostic.   2. For GPU features, NVIDIA usually introduces them through a DirectX standard, they are just usually the first vendors to support it until others catch up.   3. RT is limited because current gen consoles are not good at RT. AMD took multiple years to adopt RT or AI acceleration cores.",Neutral
Intel,I actually have not seen an open source or even cross vendor solution with any reasonably quick uptake. Its a theoretical myth at the moment,Negative
Intel,the issue with RT isnt standard. Its that consoles/amdGPUs simply didnt support it for away too long.,Negative
Intel,"...It's down to 15 now? I thought it was 20.   Although. Software is more copyright than patent anyway, and copyright is, like, 70 years after the author dies, or 95 after publication for corporate works.",Neutral
Intel,"> RT in games is now done through DirectX APIs, which are vendor agnostic.  Now being the key word, Nvidia launched their RTX before DXR was even available.  AMD's slow adoption is absolutely one of the problems which might not have not been so delayed if AMD, Nvidia, console makers and Microsoft worked on RT together from the start.   This is all just musing really. Maybe nvidia did us all a favour by breaking the mold forcing everyone else to play catch up.",Neutral
Intel,"Depends on country/industry. Copyright also depends on country, but for most civilized word its authors death + 95 years (thanks disney). Copyright however has many exceptions to it, well, at least in the west. In Japan it does not and technically you can get sued for saying Zelda is fun because you broke copyright by saying Zelda without permission. Its why Nintendo is able to get switch reviews removed from youtube.",Neutral
Intel,The DirectX RT spec and SDK came out in 2018. The blame lies entirely on AMD. Imagine Nvidia waiting for AMD to agree,Negative
Intel,I would imagine intel has to at least get celestial out the door before cutting off their GPU line. it's been so long in development that it would be more of a waste of money to cancel it then finish it and see how it sells. at least getting it to the public means some kind of ROI and a solid position in the lower end of the GPU market that AMD and Nvidia are abandoning to the used market.,Negative
Intel,"Okay then, what's the Xe GPU roadmap looking like then?",Neutral
Intel,"Hopefully Intel still thinks it needs discrete GPUs to compete in DataCenters and such.  Also GPUs for the ""edge""",Neutral
Intel,I can't claim to know the future or fate of Arc but I think if Nvidia wanted to kill it they could just price it out of the market instead of buying a stake in a company with a broadly uncertain future,Negative
Intel,They barely lived on before the deal.,Negative
Intel,"It'll live on our hearts, yes.",Positive
Intel,Lol if you believe that I have a bridge to sell you in Brooklyn,Neutral
Intel,"You have to look at this from both sides.  This would give NVidia the ability to offer OEMs a tightly integrated on SOC gfx solution vs a separate discrete gfx package as it is now. This would lower costs for NVidia's customers as well as improve performance and Intel's cpu's get to ride that train.   Intel has a long track record of using integrated gfx so it's certainly right up their alley to take this on.  Could Xe gfx coexist with NVidia? Yep, I think so.  Xe could be the budget igfx option and iNVidia is the premium integrated solution. That's basically how it is now with Intel Arc/Battlemage & GTX.   On the DC side, Intel has absolutely no gfx solution at this time and won't have anything for at least two years.  Intel already sells NVidia when a gfx solutions are needed so tighter integrated products will benefit both sides in that DC space as well.",Positive
Intel,"What's weirdest to me about this deal is. They specifically mention NVIDIA chiplets being packaged with Intel CPUs.   Now, with Meteor Lake and Arrow Lake, the CPU and the GPU are different tiles packaged together. But for Lunar Lake, the GPU was included in the same tile as the CPU, called the compute tile, because that was more efficient.   Supposedly, Panther Lake is using a different tile setup, with the GPU being separate again, or with the memory controller being on the tile with the CPU, but.   Either way, it feels to me like relying on separate NVIDIA chiplets for the integrated graphics is a bit of a limitation to how the overall architecture of the SoC can be designed.   My understanding is that, if another company buys Intel, that messes up the cross licensing agreement, and Intel might lose x86-64. So, even if NVIDIA keeps buying Intel stock, the most they can get is 49%.   And NVIDIA is too big now for Intel to buy, so the other way around wouldn't work. And even if NVIDIA wants to keep Intel's fabs open, I doubt they want to run them either.",Neutral
Intel,Wonder if this investment is purely to keep intel going to avoid a monopoly issue in a few years.,Neutral
Intel,AMD is not going to release a next gen gaming gpu it just leaked and that’s what I’m worried about intel arc are ramping up.,Negative
Intel,"There's also a better then not chance this alliance would have gone the same way as most other nVidia semicustom things by the time Druid tapes out anyhow, and the leadership at Chipzilla likely know this.",Neutral
Intel,"While that might be true in the long term, it's not necessarily true for whatever quarter the execs are caring about at any particular moment.",Neutral
Intel,"Probably a chronologically changed version of what is known now.  No one with any savvy expects this alliance to last with nVidia's track record, so carrying it on makes sense in both being ready for the inevitable burnt bridge and possibly pushing it back.",Neutral
Intel,It's looking for 6 years of Xe3 in integrated graphics I'd guess.,Neutral
Intel,"Always selling out actually, they just can't produce that much",Negative
Intel,And I have another if you think nVidia is capable of keeping this deal running for that long...,Neutral
Intel,Intel will hopefully split their fab business from the rest of the company either way.,Neutral
Intel,"Im not sure what you are implying, but nvidia partnerships have historically not broken from their side as far as I know.   Apple gave them the boot, not the other way around.   AMD are the ones who rejected the Nvidia-AMD merger back in 2005.   Tesla kicked nvidia out as well, because they did not want to keep paying them to develop the tech.   If you are referring to AIB Partners, then that can hardly be called an actual partnership. That's more of a contracted labor and distribution agreement. Even then, XFX only got blacklisted because they were a premier partner who were seen as jumping ship when the going got a little rough with nvidia stalling between 2009-2010.",Neutral
Intel,> they just can't produce that much  because they are losing money on them,Negative
Intel,Trade bridges.,Neutral
Intel,Intel's arc is dead with or without nvidia deal.,Neutral
Intel,"Microsoft went with Nvidia for Xbox and it ended in a lawsuit.   Sony went with Nvidia once, and never again.  Apple ceased to bundle Nvidia GPUs because Nvidia sold them (and HP and Dell) huge quantities of faulty 8000M GPUs.",Negative
Intel,There is a reason why every nvidia partnership ends up breaking apart pretty quickly...,Negative
Intel,"Nvidia seems to make working with Nvidia too difficult for their partners to continue. It's the business version of someone emotionally bailing, and abusing you until you leave them.  They did that with smartphones, consoles, Linux, Apple, and likely countless other projects I didn't follow in as much detail.",Negative
Intel,"That really is up for debate. That they are losing money on them when you account for RNE and RnD amortized by unit, sure. That I have no doubt about.  But that is not the same thing as losing money on each card sold based solely on the bill of materials and distribution cost etc. Which is what actually matters when looking at if a product is ""sold at a loss"" or not.",Negative
Intel,"Hardly. Even after the bubble, Intel needs in house GPU, for both APUs and data center... which means they'll make a gaming version if the work is already mostly there.",Neutral
Intel,"In the case of Intel and Nvidia chipsets, it was Intel that decidedly ended the relationship, paying Nvidia off to exit the chipset business so that they could make integrated graphics.",Neutral
Intel,"Yeah, if nVidia has an Achilles' heel, it's their company culture's near total inability to do semicustom without fuckery, and it's the reason AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops.",Negative
Intel,Well yes. But you do need to amortize the tape out and dev. costs over the production life cycle of the product. And with low volumes I'm not even sure AMD is accomplishing that. AMD is saved due to the console business funding dGPU tech development.,Neutral
Intel,"They've already cancelled every data centre GPU after Ponte Vecchio (which was forced out by DOE contract) despite reassuring us Rialto Bridge and Falcon Shore were on track as little as months before the chop, and their next is apparently Jagure Shore in 2028.   I have 0 hope that will actually happen.",Negative
Intel,">AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops  And how much money is there in your ""everything else outside of ..."" vs Nvidia's entirety of consumer prebuilt desktop/laptop?",Neutral
Intel,">Well yes. But you do need to amortize the tape out and dev.   If they are selling above the manufacturing and distribution cost, they are amortizing those costs. Might not be much, but it beats not selling them.  They might still be ""losing"" money on them when all costs are included. But it would still make sense to make more of them and sell them, since each one sold still plugs that black hole of externalized costs. And would not be the reason for lack of availability.",Neutral
Intel,"*looks at effectively every current gaming system that isn't a PC, Intel claw, a phone, or the Switch*  A fair bit, and frankly, even if it's not that profitable, it defines more of the dynamics of this sector then you'd think if you're a PC type.",Neutral
Intel,"Not only is it not profitable for the chipmaker, the revenue isn't even anything to brag about looking at AMD's gaming section of their financial statements. 1.1 billion for Q2 2025, and that includes whatever miniscule sales there were for Radeon dGPUs.",Negative
Intel,This is the card Intel should have released instead of the gaming version(s). Took them 2 years,Negative
Intel,Far more than I expected them to come out at. Damn.,Negative
Intel,"Not worth it at $599, slightly worth it at $499  This has 1/2 the memory bandwidth of a 3090 and 70% the compute performance.",Negative
Intel,I’m getting one when it releases in Australia,Neutral
Intel,Thats great and all but when will there be stock? (Canada),Positive
Intel,Pytorch libs are getting better for intel and amd support but there are still a lot of 3090 cards on the used market and those don’t need hoops to jump through to get compute working.  If I was handed a budget this low by a boss I’d just ask them to buy a lot of 3090 cards.,Neutral
Intel,I'm disappointed.  My order was canceled,Negative
Intel,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"The gaming cards require significantly less robust driver and application support. Releasing them first makes sense when they still needed to work on said driver and application support, the timeline likely wouldn't have meaningfully changed.",Negative
Intel,b580 should've allowed SR-IOV upon release. That's all the card really needs to sell.,Neutral
Intel,"Even at $599 it's significantly cheaper than a used 3090, comes with a full warranty, and can be bulk purchased new.  If you just need the VRAM, which is what this is targetting, I think it can definitely make sense.",Positive
Intel,"This is worth it at any price point.  I am getting tired of gamers not understanding that SR-IOV support and 200W are very important to some people. If there were a 24GB AMD card out there for 200W and $600 I would buy that too regardless of SR-IOV support.  Neither Nvidia nor AMD have a card at this price point with this feature set. You are looking at $600 for 24GB of VRAM and SR-IOV support. AMD's SR-IOV offerings are in the ""thousands of dollars"" and Nvidia's cards in this segment do not meet the 200W and 24GB of VRAM targets. The closest thing is the P40 and it's a decade old, has way higher power consumption and is about to be end of lifed.  Please tell me the card that exists in the same segment as the B60. I will wait.",Negative
Intel,"This is $150 cheaper than NVIDIA's competing A2000, which has less VRAM.  3090s do not get certified drivers, which are a business requirement for certain workloads.",Neutral
Intel,The 3090 does not have ECC on it's VRAM nor certified drivers,Neutral
Intel,Totally not worth it.,Negative
Intel,it's worth it for enthusiast collectors who want to own a rare Arc GPU. Especially if Intel abandons dGPUs. I'm sure Raja Koduri will buy it.,Positive
Intel,"I'm more looking forward to the B50, but obviously local pricing is everything.",Positive
Intel,"Yeah, the website now says: ""This GPU is only available as part of a whole system. Contact us for a system quote.""",Neutral
Intel,"It has SR-IOV, certified drivers and other professional features...",Positive
Intel,"Graphics cards are in fact seperated into two broad categories, consumer and professional, used by all the main manufacturers. They have completely different approaches to their design, different use cases, different drivers, different memory etc.",Neutral
Intel,">The gaming cards require significantly less robust driver and application support  Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  It was like that since A770 release. When some games straight up don't work or have unplayable bugs, you are literally paying for own suffering.  That said, B50/B60 are good, though i wish B50 worked with ""Project Battlematrix"" that is Intels NVLink. B60 was also expected to be for 500$, not 600$, but it is what it is.",Negative
Intel,Both nvidia and amd only take a few months between releasing gaming and pro lines of new gpus. Intel managed to do the same with their first gen (only a few months). How come the second gen takes 1 year!?,Neutral
Intel,"...Do integrated graphics support SR-IOV? Do any of Intel's iGPUs support the ""pro"" version of their drivers,   Like, obviously iGPUs have less compute power and less bandwidth than anything dedicated, but. Part of me feels like iGPUs should actually have the features of the ""pro"" cards, given they aren't replaceable, with the actual pro cards providing performance.",Neutral
Intel,"Are used 3090s that expensive?  Here in Chile they go for about 550-600USD. Got one (luckily) at 450USD some weeks ago for AI/ML, without the box and such but it works fine.  Asked a friend from Spain and it seems there they go for about 600 EUR there.",Neutral
Intel,Do not underestimate the lack of CUDA.,Negative
Intel,"Bought my 3090 for MSRP 4.5 years ago and honestly would never have imagined that it would hold 50% of its value.  I want to get another one when they drop to $200 or so, so that I can finally be disappointed by SLI and make 14 year old me happy by finally clicking the bridge onto two GPUs and firing up a benchmark, but who knows if/when that day will come.",Neutral
Intel,A used 3090 is only $100 more.,Neutral
Intel,"It barely makes sense, at $600.  3090s are $700 now and the new 5070Ti Super (faster than a 3090) is expected to come out at $750 soon. And you’d have CUDA for either the Nvidia 3090 or 5070Ti Super.   I strongly believe that $600 is the price Intel set to ward off the scalpers. The niche of the B60 is pretty tiny at $600.   It’ll make a lot more sense at $500, which I suspect is the “real” price a month or two after it goes on sale.",Negative
Intel,Used 3090's are cheaper than this lol. Did you check the sold prices on Ebay at all? I can literally buy them at buy it now prices that are less than this.   94+ upvotes lol. 3090's sell for about the same price as 4070 supers as they have the same gaming performance.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1,Neutral
Intel,yeah it's got a good feature set for its price. people looking at it from a gaming product frame of reference rather than a professional product frame of reference are doing it wrong,Positive
Intel,"SR-IOV alone makes this a substantially different offering than a used 3090. If you know, you know.",Neutral
Intel,Atlas 300i duo,Neutral
Intel,"Also 3090s are $200 more, are used, consume twice as much power (400W card BTW), and do not have SR-IOV support.",Negative
Intel,"It's cheaper than Nvidia's RTX Pro 2000, but it's also slower, but have more VRAM.",Neutral
Intel,Businesses aren't buying these cards these are destined to hobbyists home labs. The extra money to buy a CUDA supporting card is trivial for businesses they aren't going to buy Intel.  Certified or not the 3090 works wonderfully with all AI software out there while Intel does not.,Neutral
Intel,Yeah it was very scummy imo,Negative
Intel,"> R-IOV, certified drivers and other professional features  Did Intel ship the SR-IOV part? Last I read (been a while) it was ""coming soon"".",Neutral
Intel,It was meant to be a joke. Not so funny I guess.,Negative
Intel,"B580 needed to get out very early to beat RTX 5060 and RX 9060 to the market. Nvidia's and AMD's cards outperform it in gaming raster performance thanks to having been in the market for much longer. They have the more efficient architecture and software for gaming. It wouldn't be great for marketing if Arc came out at the same time with similar pricing, yet got beaten in the FPS benchmarks.  But productivity cards could wait. Arc's always been very competitive in productivity tasks. Maybe because Intel has more experience in this department. So reliable driver is the focus here.",Positive
Intel,">Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  Good driver and application support is not possible to deliver without a significant install base of your product in the wild. PC configurations vary wildly and usually driver issues are about some bad luck combination of hardware, driver versions, and application versions.  The incumbency advantage in this market is immense. Devs will validate that their stuff works on all manner of configurations including an Nvidia GPU, so they get the perception of having bulletproof drivers.",Negative
Intel,AMD and Nvidia have been having extensive driver support for years  ARC is a much newer platform requiring more careful hand holding,Neutral
Intel,Why does this matter?,Neutral
Intel,"Some iGPU do support SR-IOV, I think 12th generation and newer. Motherboard, driver, and OS dependent.",Neutral
Intel,Was seeing one sold locally recently for $800-$900. They very much are still expensive here in the US.,Negative
Intel,"Yup 600 Euro is about (or slightly above) what used 3090s are sold for here (Sweden) as well. I would rather buy a significantly better card used than a worse one new, if sold at approximately the same price point.",Neutral
Intel,"Used site jawa.gg shows 3090s priced from $750 to $950 (and of course, some people asking $999 or $1200)  https://www.jawa.gg/shop/pc-parts-and-components/geforce-rtx-3090-KNRXB30U",Neutral
Intel,Used market is freaking insane. It is better to grab new or open box.,Negative
Intel,Over here used 3090s are sold for 500-600€.,Neutral
Intel,"Well sure if you specifically need cuda then this won't work, but there's also a large subset of people who just need vram for as cheap as they can get it. And for that purpose this Intel card is interesting at least.",Neutral
Intel,Dude that feeling is so worth it though. I did that same with 2 Titan Xps a couple months ago and the build just looks awesome. Great performance on pre 2018 / SLI supported games as well.,Positive
Intel,Just keep on digging the hole man.  A 5070Ti or 5070Ti Super or 5090Ti Super Extreme Titan do not have Support for SR-IOV. You do not know why people want these products.  > Muh niche  **Gamers** are the niche. Have you been living under a rock for the past 5 years?!,Negative
Intel,the super gpus are not expected to release soon?,Neutral
Intel,"Why have you posted this same garbage about a 5 year old used card so many times in this thread? It's a valid point, to some people I'm sure, but it's like you're weirdly obsessed with making it until someone hears you.",Negative
Intel,"....are you mistaking the listings of broken cards as new, working ones?  because when you take those out you'll see pretty quick that all of the 3090's here are selling for FAR above $600 lol.",Negative
Intel,Used 4070 Supers dont sell for nearly $700+ on the low dude.,Neutral
Intel,Yes running business of used cards is how its done.....,Neutral
Intel,no fee SR-IOV makes this a substantially different offering then AMD or Nvidia.,Neutral
Intel,Typically run at 250W though to be fair.,Neutral
Intel,"They are less not more, no idea why people are making up used prices when we can check ebay sold listings.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1",Negative
Intel,not on the Vram like professional cards,Neutral
Intel,Q4.,Neutral
Intel,I thought it was funny ¯\_(ツ)_/¯,Positive
Intel,The b580 is still good at $250 and it’s only recently in the past month the 5060 and 9060xt 8gb dropped near or at $250 in some instances.  It’s a good card at 1440p aswell and it has 12gb of the other 2 8gb cards. Only problem I have it is the windows 10 drivers being abysmal dog shit if you’d want to play any current game with stuttering and crashes.,Positive
Intel,"Yeah, apparently a year’s worth of it",Neutral
Intel,Because they're super late to the party.,Negative
Intel,"Wow, 800-900USD after 5 years is more than I would have expected.",Positive
Intel,"No one on hardware swap is willing to go above $600 for a 3090, trying to sell one now. This reminded me to just put it up on Ebay.",Negative
Intel,I have no idea why people make up stuff like this when we have ebay sold listing search.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  Most recent one sold for $439.00 including delivery.  Again why are people making up used prices?,Negative
Intel,For what exactly do they need vram without cuda ?,Neutral
Intel,"Look at my profile. I have 0 comments in gaming and hundreds of AI comments. What do you know about running inference locally? Why do you think I mentioned memory bandwidth? Gamers don’t care about that. I haven’t played a game in years.   This is not a good deal for AI running consumers at this price, and a mediocre option for enterprise.",Negative
Intel,Gamers are not niche. They are 20 billion a year business.,Neutral
Intel,"False, they’re releasing in december or jan. So about 3 months from now",Neutral
Intel,"> Typically run at 250W though to be fair.  At which point we have to derate the performance. You know, to be fair.",Neutral
Intel,"The 3090 Ti did, but the standard 3090 did not.",Neutral
Intel,"It's a really good general purpose card. Perhaps not the best card if you want to squeeze out the most fps out of games, but it performs really well in a wide range of tasks including gaming. And yes 12GB VRAM really comes in handy.  It can easily be found for under $250. B580 is still cheaper than 5060 and 9060 XT and even RTX 5050 and RX 7600, especially in the UK where you can find a B570 for only 20 pounds more than RTX 3050. That's way cheaper than RX 6600, it's a no brainer. But the prices could only go down thanks to economy of scale mostly. It's been produced for a while, that's why.  Also one of the big reasons it's still selling now is thanks to good publicity. Imagine if it came out as the same time as RX 9060 XT. HWU would tear a new hole regardless of how much of an improvement it is over the Alchemist cards and the impressive productivity performance. Those guys literally don't care about anything other than pushing a dozen more frames in a dozen personally chosen games. Gamer Nexus and Igor too to some extent. It would be bad publicity.",Positive
Intel,"If intel felt they could have released this safely earlier, they would have",Neutral
Intel,People do not understand AI.  People buy up used 3080s and use 4 of them in an AI Rig to run MoE models. 96GB of VRAM can run you basically anything that's not the full 500GB models like Kimi K2. The power consumption doesn't matter because they're making more money off of the inferencing than the cost of electricity and it's cheaper than cloud providers.,Neutral
Intel,"Where did you find that? FB? Im saving for a GPU here in chile too, was aiming for a 5060ti 16gb but the model I want is 630k in ... Idk the top result in solotodo",Neutral
Intel,3090s will be much less attractive when the 5080 super and 5070ti super launches.  If the 5070ti super is 750- 800 with 24gb for a new faster GPU with a warranty it hel used market will have to shift.  The 3090s have nvlink but I'm unsure how much that matters for most people.,Neutral
Intel,https://www.ebay.com/itm/156720971107  That's a 3080 not a 3090 lol.,Neutral
Intel,"rendering, working on big BIM / CAD models, medical imaging,...",Neutral
Intel,CUDA isnt the only backend used by AI frameworks,Neutral
Intel,"I use opencl for doing gpgpu simulations, this card would be great for it",Positive
Intel,You mean your 0 profile history because you have it private?,Neutral
Intel,"And AI isn't the only use for a workstation card either, jfc.  3090s and 5070 Ti Supers don't have SR-IOV  3090s and 5070 Ti Supers don't have ECC VRAM.  AutoDesk for example will tell you to go pound sand if you try to get support and you aren't running a workstation GPU with ECC VRAM. So for businesses running AutoDesk software in a professional capacity, or businesses needing SR-IOV, you could offer them a 3090 or a 5070 Ti Super to them for $100 and it still won't be an option for them, because for their needs they might as well be paperweights.",Negative
Intel,I thought we were talking about actual hardware that was available in the referenced country (US) and not a banned Chinese product that may or may not even exist and if it existed would cost nearly 10x as much as this product.,Negative
Intel,Spoken like a true gamer.,Positive
Intel,last leak on them is that they’re delayed. Haven’t seen any reports since then that they’re releasing in q4,Negative
Intel,You still get about 85% performance compared to stock settings.,Neutral
Intel,"Of course, but that’s the point. It took them an excessive amount of time to “feel” like they could have released this",Negative
Intel,What are they doing that's making them money? Or are theu selling the compute somehow?,Neutral
Intel,"People do not understand Ebay sold listing search showing them selling for way way less than the values you guys just pulled out of your assholes?  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  They sell for roughly the same price as 4070's as they have roughly the same performance, AI don't matter gaming is still king in this market.",Negative
Intel,"Sip, marketplace.  Si buscas por RTX 3090 vendidas, es una PNY 3090 a 450K, y todo perfect.  La 5060Ti es más lenta que la 3090 para juegos, la 5070 es un poco mejor o igual.",Positive
Intel,">rendering  Rendering what? Because if you're talking from a 3D art perspective, all our software is leaps and bounds better with CUDA or outright requires it. You can't even use Intel GPUs with Arnold or Redshift, so good luck selling your cards to a medium scale VFX house that heavily use those render engines",Neutral
Intel,"It's the only one that's always well supported and expected to work with decent performance.  We're months after RDNA4 was released and ROCm still doesn't have any winograd kernels for it, which means 3x3 convs, the most common kernel size, are 2 to 3 times slower than they could potentially be...  I've close to no experience with OneAPI and SYSCL, but I'd imagine AMD should be ahead considering they've been playing the game for way longer… And if ROCm is a nightmare then I can only expect the experience with Intel to be as bad, if not worse.",Neutral
Intel,>CUDA isnt the only backend used by AI frameworks    It is not the only backend for all AI frameworks; it is the only backend for the majority of AI frameworks.,Neutral
Intel,It is the only functional one.,Positive
Intel,"Oh good, I’m glad I set that private. You can literally google the username anyhow and see 1-2 example posts that underscores my point.",Positive
Intel,0 people are using autodesk with an intel arc gpu lmao,Negative
Intel,"You know nothing about the ai market, huh?  https://www.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/",Negative
Intel,"Oh no, im part of 60% of world populatioin. how terrible.",Negative
Intel,Which is 36% higher performance per watt.       ... to be fair.,Neutral
Intel,The reason they didn't is because it would have been worse to release it earlier,Negative
Intel,"Gamers Nexus has a documentary on the GPU smuggling business, where even the 3060 12GBs are being used: https://www.youtube.com/watch?v=1H3xQaf7BFI  The 4090s, 3090s and other cards? There's such a demand for those in China that people will buy them up and smuggle them.",Neutral
Intel,Software Development,Neutral
Intel,Why did you spam my replies about cherrypicked results. The cards are worth $800-$900 and the listings you linked literally prove htat besides a few outliers that are $600-$700 to tank the prices.,Negative
Intel,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",Neutral
Intel,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",Neutral
Intel,"Depends on what software you use, but your right that NVIDIA with Cuda is safe bet when is comes to rendering. I use Enscape and that works great in my AMD gpu",Positive
Intel,"Intel is a bit better for common models as they actually pay frameworks and model makers to optimize for it, unlike AMD.  I use flux and sd models and it’s much better on my arc than my Radeon 9600Xt",Positive
Intel,Which ones?,Neutral
Intel,Private profile = Complete troll.  You're not an exception to this rule.,Negative
Intel,I too like to promote Chinese propaganda which isn't available in the United States due to the whole AI cold war going on and the fact that Huawei products have been banned in the US for what? 15 years?,Negative
Intel,"Right man, my point is that it shouldn’t have been",Negative
Intel,EILI7? How are software engineers using GPU ~~imprints~~ inference on their home machines for software development? What am I missing?  E: a word.,Neutral
Intel,"Hell, one of them was just the cooler without the actual GPU.",Negative
Intel,"En ese claro entonces tal vez valga la pena esperar a la 5070 Super, que supuestamente tendrá 18GB de VRAM.  Sobre comprar usado, todo bien en general, pero siempre hay que tener ojo y ojalá siempre probado. Yo fui a la casa del vendedor para revisar y todo bien, pero igual puede ser peligroso, por eso revisar bien los perfiles.  He comprado hartas 3090s usadas por FB Marketplace.",Neutral
Intel,for example every local image and video generation system I've seen.,Neutral
Intel,"you’re wrong about cost, availability, performance, and just plain wrong overall lol",Negative
Intel,"The Chinese are busy making hardware, not propaganda. These are old cards that have been around for a while, I can assure you they are quite real.",Neutral
Intel,According to whom? With what evidence?   Intel spends more on r&d per year than nvidia and amd combined   Do you think they are just being lazy?,Negative
Intel,"The many hours of trial and error getting things to work, and building expertise with said labor would be my guess.  This isn’t home software engineers buying up all the inventory, its people getting around sanctions.",Neutral
Intel,Getting good LLMs crammed onto GPUs for coding is a popular topic on /r/locallama.,Positive
Intel,And the one directly under that was the actual PCB...,Neutral
Intel,Im running Comfy UI + Flux on my Strix Halo right now without issue 🤷‍♀️,Positive
Intel,Propaganda is 90% of chinas economic output though.,Negative
Intel,"> Neweggâs  I know this is mojibake, but this kinda sounds like a Lithuanian versions of Newegg lol",Neutral
Intel,The competing product is sometimes slower while also being twice the price.  If this wasn't a success then Nvidia would be  unbeatable,Negative
Intel,"What's the word on the B60? Even more VRAM (24GB), and double the memory bandwidth. I see it listed as ""released"" in various places, but can't figure out where to actually buy one.",Neutral
Intel,"My RTX a4000 doesn't support SR-IOV. I don't know about newer series, but at the time you had to buy the A5000($2500) or A6000 and then there are some crazy licence fees to use it.  For 350 i will buy it when it gets available just for this.",Negative
Intel,L1 techs had a great feature on these.,Positive
Intel,"Profitable product for Intel, wouldn't suprise me if Xe3P and onwards for dGPUs happens because stuff like this can do easy returns.",Positive
Intel,1:4 ratio of FP64 performance is a pleasant surprise,Positive
Intel,"Honest question here: what makes it a ""workstation gpu"" that does it differently than say like a low end 5060/AMD equivalent?   Iis it just outputting 1080p ""faster""?",Neutral
Intel,"Its also just a whole 95 cards sold. (Past month, I’m unsure if its been up longer)",Neutral
Intel,"It will never be in stock again. It’s good for AI, hosting pass through srvio to VMs without licensing and a number of other things outside of gaming.",Neutral
Intel,"Hello 79215185-1feb-44c6! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Let me know when it shows up on the steam hardware survey. That's the only barometer for success that true hardware enthusiasts care about.,Positive
Intel,what's up my Neweggâs,Neutral
Intel,My favorite game  Fallout Neweggas,Positive
Intel,It also sounds like an ikea lamp name or something,Neutral
Intel,Geriau nei Bilas Gatesas,Neutral
Intel,I gotta spell Neweggâs?!?,Negative
Intel,"The B50 appears to be an decent low-end workstation GPU, at least as long as the intended workloads don't effectively require CUDA in any way, shape, or fashion.    My one lingering question is what use cases actually *require* the certifications of a workstation-class GPU (which would rule out something like a relatively similar consumer-tier RTX 5060 Ti / 16GB) but wouldn't benefit from CUDA?  Then again I'm not exactly an expert in the field, so I could be completely off-base here.",Neutral
Intel,"I'm disappointed in Nvidia's inability to put out a stable driver since last December, I'm waiting to see if a competitor card will come out that meets my wants for an upgrade.",Negative
Intel,Intel might be using the B50 as a pipe cleaner for the B60's drivers to prepare it for a retail launch in Q1 2026    IF they're doing this then it's a sound strategy,Positive
Intel,"Double the memory bandwidth of trash is still trash.  Edit: Y'all can downvote me all you want, but 250GB/s is just slightly more than the 200GB/s of my low-profile 70W GTX1650 GDDR6 that I bought for €140 in 2019. Its absolutely pathetic and should be unacceptable for a new product in 2025, let alone a product of $350 !!!. Even double of this (\~500GB/s) of the B60 is less than a RTX3060. Pathetic products.",Negative
Intel,SR-IOV is the selling feature for me and why I have one ordered. Getting a Tesla P4 with nvidias vgpu licensing working is a pain in the ass and expensive.  I'll get it and sit on it until SR-IOV is released in case of scalpers/stock issues. If it doesn't pan out I'll either just sell it on or drop it into my home media server for the AV1 encoding/basic AI stuff.,Neutral
Intel,"Last time I checked GRID licensing can be faked out, but yes, only Quadro/Tesla and Turing/Pascal(IIRC) through driver mods can use Nvidia's vGPU.",Neutral
Intel,"The professional market is smaller than gaming and even more slanted towards Nvidia. This might be a nice side business, but can't remotely justify developing these cards.   Not even clear it's profitable either. The numbers here are negligible so far.",Negative
Intel,"Do people actually need and use FP64 at all anymore? I've got one or two original Titan cards that I haven't thrown out although I've never used them for this purpose either, because they apparently have very high FP64 numbers and if I recall correctly can operate in ECC mode as well.",Neutral
Intel,"iirc, SR-IOV and VDI support in the coming months, toggleable ECC support, and it is ISV certified",Neutral
Intel,"So I spec our PCs at work. We do anything from traditional office work, to intense engineering tasks. On our engineering computers we run MatLAB, Ansys, Solidworks, MathCAD, LTSpice, Xilinx, Altium and other such apps. Lots of programming, VMs, design work, simulation testing, number crunching, and on occasion AI work.   This means we spec systems like with RTX Pro 500, RTX Pro 2000, RTX A4000, A4500, A6000s. The reason we have these rather than cheaper GeForce cards is mostly 3 things. Power/form factor, Driver certification, pro GPU features.   So typically Nvidia keeps the top binned chips for their professional cards meaning the power efficiency to performance is top tier. So we can get high performance single slot or low profile cards, or get some serious GPU performance in relatively small laptops. Drivers usually are validated better than the GeForce drivers, so they include better bug testing, and the apps we use validate performance with the cards which helps us evaluate performance. They also have way more vram like the RTX A4000 has 20GB of vram while being just a supped up 4070. Then from a feature perspective they have better VM passthrough support, or you can enable the vram to run in ECC mode for error correction. Very important when running 24-48 hour simulations.",Neutral
Intel,Software support is a thing. CAD applications like solidworks and inventor don’t officially support the GeForce rtx or radeon rx line of gpus and they’re considered untested unsupported options. You can’t get any tech support if you’re using them. For a business that needs those apps you need a workstation gpu. They also come with ECC vram,Negative
Intel,ECC memory.,Neutral
Intel,"That kind of puts it into perspective.  Also, let my take a guess:  Newegg sells them well because of how dirt cheap they are, people buying actually expensive Pro cards will more likely do it directly via their system integrator.",Neutral
Intel,Oh when they get enough enterprise customers they will definitely charge licensing fees,Neutral
Intel,How many A2000s show on the hw survey? Because that's the Nvidia variant and it has been around for a long time..,Neutral
Intel,Lmao,Neutral
Intel,Komentarą skaitai per tapšnoklį ar vaizduoklį?,Neutral
Intel,I am no expert but don't these gpus have ECC vram. That's a enough to get labs/professionals to buy them.   You don't want the headache of lacking error correctiin in a professional environment.,Negative
Intel,"I seriously considered getting one for my homelab. I would really like some SR-IOV, and giving multiple VMs access to transcoding would be very useful. Ultimately decided against it because at the moment my CPU alone is powerful enough, I have other uses for the PCIe slot, and I would have to import one. But it's something I'm going to check in on whenever I'm browsing for new hardware from now on.",Positive
Intel,"I know in my field of work, solidworks certified hardware is one such application where certain features are gated behind workstation class cards.",Neutral
Intel,Vulcan does fine with inferencing.,Neutral
Intel,"all the professional graphics stuff is where this can matter, i.e. CAD, large display walls, traffic control, video studios etc",Neutral
Intel,people are buying B60's  https://www.reddit.com/r/LocalLLaMA/comments/1nesqlt/maxsun_intel_b60s/,Neutral
Intel,"Most Zen-1 parts had much worse single core performance than Kaby Lake,    People still cheered on the competition anyway despite it's shortcomongs",Negative
Intel,"GTX 1650 has only 4GB of RAM at 128 GB/s; RTX 3060 is only 360 GB/s, and only 12 GB--or maybe just 8 GB for some cards--of RAM. But thanks for playing.   Edit: relevant username. Up voting you for jebaiting the crap out of all of us.",Neutral
Intel,you really dont want to fuck around with software licensing as a business. vendors do inventory audits to ensure nobody's exceeding their license allocations. piracy would automatically invite a lawsuit.,Negative
Intel,Grid licensing can be faked if you depend on a sketchy github driver that only works on Turing GPUs. You certainly don't want to be doing that in a professional setting where licensing costs are not a massive expense anyways.,Negative
Intel,"I believe mobile is the main reason they continue developing ARC IP, highly integrated SoC are crucial for lower power consumption and performance per watt, as more and more mobile designs are becoming more integrated (see strix halo for example) Intel knows it has to continue developing graphics IP that is competitive with competition. As for discrete cards, this is a battle in the long run to win, but it will take serious investment, we can hope that they won't axe as part of cost cutting measure.",Positive
Intel,"The B50 (16Xe cores) is pretty cut down compared to the full G21 (20Xe)die, it has 2600mhz boost clocks instead of the 2850mhz on the gaming cards, it uses 14GB/s memory (19Gbps on gaming cards) and it has a 128bit bus with 8 memory chips (B580 has 192bit bus with 6 memory chips)  The only costly thing about is the 2 additional memory chips.   I'm not saying it's extremely profitable but it can't be too expensive to make since a portion of the volume is likely faulty G21 dies that can't make a B580 or B580.   If Intel can sell the B580 for $250 without too much pain, then the B50 is probably making a profit",Neutral
Intel,"Yes, to the point where I’m considering picking up a Titan V on eBay. It’s a must for scientific computing, single precision floats accumulate errors fast in iterative processes.",Positive
Intel,I recognize those as words...,Neutral
Intel,I think it was obvious I was being facetious.,Neutral
Intel,"Yeah, if ECC is a hard requirement for whatever reason then that would certainly rule out all the GeForce-branded RTX cards.    Of course this then begs the question of what kind of labs / professionals are so cash-strapped as to not be able to afford something like a [RTX PRO 2000 Blackwell](https://www.nvidia.com/en-us/products/workstations/professional-desktop-gpus/rtx-pro-2000/) instead, which fits the same niche as the B50 (i.e. low-profile 2-slot @ <75w with 16GB of VRAM) while being faster and having a far superior feature set.",Neutral
Intel,I’ve been working in a professional aechtiectural environment for 5 years and haven’t seen the need for ECC once.   Can you explain situations where it’s needed? I’ve always wondered.,Neutral
Intel,"SR-IOV for (license-free!) vGPU is IMHO the killer feature here, perhaps along with being able to get 16GB of VRAM per card relatively cheaply and without needing auxiliary power.  Both open up interesting server and workstation use cases that can't be had cheaply from the competition.",Positive
Intel,Have you tried GPU paravirtualization?,Neutral
Intel,"You can buy it off AIB Partners but you can't buy it at retail (i.e. microcenter, newegg] and it doesn't have an official MSRP yet.   The prices you see now are what AIB's want to charge in bulk orders.    If you want to know how much let's say 5 B60's cost you have to get a quote from a distributor",Neutral
Intel,They need GPU IP for two things: client and AI. Anything else is expendable.,Neutral
Intel,"Yes, my point was *if* they have the gaming cards, they can justify the professional line, but it's not nearly big enough to justify making a dGPU to begin with.",Neutral
Intel,"SR-IOV is Virtual GPU (SR-IOV is IO Virtualization used to split PCIe lanes into virtual functions so their physical function can be shared between VMs). No consumer cards support Virtual GPU right now besides Pascal/Turing with driver hacks. AMD's SR-IOV offerings are [very limited](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#virtualization-support), [And Nvidia has a bigger selection](https://docs.nvidia.com/vgpu/gpus-supported-by-vgpu.html) but their budget VGPU options are being phased out (P40).  I believe VDI is Microsoft's implementation. I believe I've done VDI on my RTX 2070 before (I have done seamless sharing between host and VM), but I don't know if it's possible with AMD. Someone please correct me if I'm wrong here, I'm more familiar with the Linux side / vGPU than VDI.  ECC is Error Correcting RAM. I generally don't understand the use case for ECC either, but it is ubiquitous in HPC. All server boards support ECC RAM.  In modern environments most of these features need 16GB of VRAM minimum, but if you ever wanted to try it on a consumer card, you could get an old RTX 20 series and try it out with some driver mods. Optionally, the P40 is still pretty cheap ($250 used) and doesn't need those hacks at the cost of drawing a lot of power, which Intel has solved with their Battlemage Pro platform (by far the cheapest VRAM/$/W you can get).",Neutral
Intel,>Autodesk tech support will tell users to piss off if they encountered software problems with a consumer GPU.   A380 and A770 is also on the certified gpu list. But otherwise that statement is correct.,Negative
Intel,Can you buy an rtx pro 2000?  If i had to guess what percentage of wafers are for b200 chips i would say 90%.  I don't think there are enough pro 2000s around. I don't think there are enough gpus around in most cases.,Negative
Intel,"> Of course this then begs the question of what kind of labs / professionals are so cash-strapped as to not be able to afford something like a RTX PRO 2000 Blackwell instead  It is interesting. IDK how common it is, but one of my university labs had computers donated from nvidia with nice quadro GPUs for their time.",Neutral
Intel,Pathetic 1:64 ratio of FP64 flops,Negative
Intel,RTX 3090 Ti and RTX 4090 have ecc. Not that they're cheap.,Neutral
Intel,"It's needed whenever the work you're doing matters and a single-bit error could cause significant harm.  Something like audio or video, a single-bit error probably isn't very noticeable.  Calculations, it absolutely depends on what you're calculating and which bit gets flipped; flipping the low-order bit in a number might not matter much and flipping the high-order bit could cause a big error.  Networking, it depends on whether the protocols you use have more error checks at a higher level (TCP does; UDP does not).  If in doubt, you want ECC, but market segmentation to mostly restrict ECC support to ""server"" chips and boards and charge more for ECC memory means you'll overpay for it.",Negative
Intel,"They're so good, I wish there was a single slot variant.   I want to put them in my MS-01s. The Sparkle A310, being the main candidate for deployment in those machines, only has 4GB and its maximum h264 encoding throughput actually drops below the iGPU (although its h265 and AV1 throughput slaps the 12900H/13900H). It's just a little too low to comfortably handle the Plex server usage I have, so the iGPU remains in service until a suitable competitor arrives.",Positive
Intel,"IIRC, that requires a Windows host right? That's a non starter for many people unfortunately",Negative
Intel,my guy sr-iov is a type of gpu paravirtualization.,Neutral
Intel,The Asrock b60s are $599,Neutral
Intel,AI doesn't even need a GPU; it can have its own accelerators - see Gaudi.,Positive
Intel,"One use case for ECC, is when the data is critical and can’t be lost.",Neutral
Intel,>  I generally don't understand the use case for ECC either  Its for when you don't want errors to just be ignored?   How is that hard to understand?,Negative
Intel,ECC should be in literally all memory.,Negative
Intel,"\>but their budget VGPU options are being phased out (P40).     I mean, the T4, L4 , and A16 exists...     I'm also not sure why low end workstation GPU needs SRIOV support.",Negative
Intel,I see them at least available in my stores. although mostly as backordres via remote warehouses but they seem readily available with some shipment time.,Positive
Intel,"but you can still do gpu paravirtualization without sr-iov using Mediated Passthrough, API Forwarding (RemoteFX) or Dedicated Device Assignment",Neutral
Intel,"Where can you get them? And are they for sale yet, or pre-orders, or...?",Neutral
Intel,"The problem with Gaudi (I know, I've written code and run training runs on it) is simply that the programming model is not oneAPI, or whatever oneAPI becomes. Yes, pytorch works, but people care a lot about software longevity and long term vision when buying $5mm+ of GPUs (and these are the purchases Intel cares about that can actually start to offset the cost of development).   The whole purpose behind Falcon Shores (and now Jaguar Shores, if it will even happen) is to put Gaudi performance (i.e. tensor cores) in an Xe-HPC package. Unifying graphics and compute packages is what NVIDIA was able to achieve but not yet AMD, and it's really great for encouraging ML development in oneAPI.  See this post to see where Intel would like to be: https://pytorch.org/blog/pytorch-2-8-brings-native-xccl-support-to-intel-gpus-case-studies-from-argonne-national-laboratory/ (they don't mention the ""XPU"" because it's Ponte Vecchio, which are iiuc worse than A100s).",Neutral
Intel,"Intel can't get people even in an AI shortage. No one wants to deal with an ASIC. That's why their AI solution is GPUs, starting with (hopefully) Jaguar Shores. So it's that or bust.",Negative
Intel,I spit my coffee reading that. Gaudi? The platform that nobody uses that Intel has to revise their sales estimates down each half quarter?,Negative
Intel,"Yup. For example you are doing a structural integrity physics simulation, and a single flipped bit can ruin your 1 week long run (and your liability insurer will reject your claim, a lot of them have standards requiring calculations to be done only on ECC for sensible reasons).",Negative
Intel,"Great example of why certain people shouldn't reply if they don't have knowledge in the area.  - Tesla T4 is $650 Used and has 16GB of VRAM. - Tesla L4 is $2000 Used and has 24GB of VRAM. - Tesla A16 is $3000 Used and has 64GB of VRAM.  Compared to:  - Arc Pro B50 is $350 new and comes with 16GB of VRAM. - Tesla P40 is $275 used and comes with 24GB of VRAM.  If all you care is vGPU / VDI for a small amount of hosts, then no, you're not getting a Tesla A16. What kind of joke suggestion is that?",Neutral
Intel,"Mediated pass though requires big ass license fees vGPU/MxGPU, and isn't FOSS other than Intel's currently broken support that they abandoned for SR-IOV support.  API forwarding only support limited host/guest setups, and even more limited API support. The only FOSS support is VirGL, which only support Linux host/guest and only OpenGL.  Obviously fixed pass though is an option, but even that isn't without issue. NVIDIA only recently removed the driver restriction, they could add it back at any time. Plus you are limited on virtual machine by the physical GPU count. It works with Intel GPUs and is FOSS with them.  SR-IOV on Intel fixes all of that. It works amazingly well with their iGPUs, has no license issues, and is fully FOSS.",Negative
Intel,Hey no need to be aggressive towards the other user. Your comments are very helpful and I appreciated them a lot but keep it constructive please!,Positive
Intel,"LMAO, I actually have quite a bit of knowledge in this area.  If all you care for is VDI for a small number of VMs, then you'd go GPU passthrough. vGPU / MxGPU often requires higher levels of hypervisor software tier (i.e. VMware vSphere Enterprise Plus), requiring more money. For KVM hosts, setting up vGPU is a lot more difficult and time consuming than just straight up GPU passthrough.  Only two groups of people would be interested in GPU virtualization / splitting:  * Enterprise, in which they wouldn't care about the used card prices.  * Enthusiasts, in which they wouldn't want to pay for vGPU prices anyway. So why bother catering to this crowd?",Neutral
Intel,"Full GPU passthrough is not a solution that many people would consider because it is clumsier than using sr-iov (or potentially VirtIO GPU Venus). Plus for each extra passthrough instance I would have to add in another GPU and this greatly increases power consumption, heat output and cooling requirements. The process is not all that much more complicated at least on Turing GPUs with a hacked driver on KVM guests at least. Plus for passthrough, you probably still need an NVIDIA card because last I checked AMD cards still had a random kernel panic issue after being passed through.  My assumption is that sr-iov on the b50 will allow users an affordable way to have multiple guests on one host GPU without increasing power draw and paying for expensive alternatives and expensive vGPU subscriptions.",Negative
Intel,"...first time I heard people prefer SRIOV over GPU passthrough because it's ""clumsier"" lol. I'm sure setting up mdev devices in KVM, finding the correct corresponding GPU instances, making them persistent through reboot, then edit virsh xml for each individual VM is a lot easier than just doing IOMMU passthrough. /s     Again, enthusiasts don't care about power consumption / heat output / cooling requirements for their lab environment. Enterprise that do care about them are very willing to pay extra cost to get a production ready driver. You're creating a hypothetical situation that simply does not exist in the real world.",Neutral
Intel,"It's good they launching this, this card adds some competition to the landscape, but before anyone buys it they should figure out if their drivers are lighter weight.  It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  The more ARC cards out there the more developers get familiar with them, the more XeSS v2 gets added to titles, the more the drivers get matured and the better future ARC cards will be.  I'd happily pick up an ARC card... once they've proved themselves in terms of driver maturity and overhead.",Positive
Intel,This should be a great upgrade from my A750 if B580 performance is anything to go by. I hope it's under $400.,Positive
Intel,"Unless it's super cheap, it's not gonna be sold well at all.  Even here with all the ""enthusiast"" and people are saying ""make sure you have this hardware combo, that driver, these settings,..."". The average buyer would just simply pay 50$ more for an nvidia card and not have to worry about all that.",Negative
Intel,I hope they fixed the drivers CPU overhead problem or that GPU's gonna need a 7800X3D or 9800X3D to feed it fully.,Neutral
Intel,Too late for me I already went with a 9060xt but hell I had dreamt of it!,Negative
Intel,"I wish they'd get the drivers past the point of frequently broken, but also they haven't produced enough cards for any previous launch to make any dent in the market regardless.  It's pretty much guaranteed the upcoming super refresh will make much more of a difference in terms of bang for your buck.",Negative
Intel,The problem with arc is you need the latest and greatest CPU to go with or you lose 1/4 of performance,Negative
Intel,Intel has the ball in it's court   If you released a New GPU..  that is pretty much a 5070.... add on 24gb of ram...  and price it at 399   u will   make  boatloads.  it will play pretty much any game at max settings at 1440p..  They must really be  hating on turning down sony though at making the SOC for the PS6 cause the margins too low..they really would need that money now lol,Neutral
Intel,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Depending on the price I might give it a shot.,Neutral
Intel,Love it going to get one if I can scratch some money together,Positive
Intel,Wouldn’t there be a risk that future drivers will not be supported and that it comes with US government back doors?,Negative
Intel,"LOL, preparing ""box"" packaging   I immediately thought of advanced silicon packaging like CoWoS or whatever",Neutral
Intel,Who is gonna tell them G31 would be celestial die since B580 was G21 and A770 was G10?,Negative
Intel,"Ah yes, finally the 4060ti 16gb/4070 killer, only 1.5 years too late! Ig at least this will force price drops on the rx9070",Positive
Intel,Aren't the driver overhead issues really only seen on older processors that are six cores or less?  Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course,Neutral
Intel,"> It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  Is it really though? Powerful CPUs are comparatively cheap, powerful GPUs are expensive. I know plenty of people who went with a 9700X/7700X or even 9800X3D but 'cheaped out' on the GPU because spending $1200 on a 4080 (at the time) was simply too much.",Neutral
Intel,"I agree with everything you said.  However, I myself will buy one just because I want more competition and so I am just going to give Intel a sale. Sure it doesn't move the needle much, sure Intel's probably not going to make any money out of it and I personally probably won't use it much, but I am just doing it out of principle. I sure am in the minority, but at this point I can't sit idle and allow this duopoly to continue without trying something.",Neutral
Intel,"It's a hardware problem, not a driver problem. The cards have insufficient DMA capabilities and uploads must be handled by the CPU, no driver will fix it, and as a consequence the B770 will have even worse overhead.",Negative
Intel,I have a B580 and the driver seems pretty stable to me at this point.,Positive
Intel,"The super refresh is only for 5070, 5070ti and 5080. I doubt the B770 will compete with the 5070 to begin with so those cards are more upmarket.",Neutral
Intel,my b580 has been stable,Positive
Intel,"I’ve had a B580 for 6 months and have experienced one game-specific issue with Tarkov. Everything else, old, new or emulated has worked fine.",Positive
Intel,"I’ve been using Arc on both windows and Linux since alchemist, it’s powering 3 rigs for gaming, transcoding, etc.   Initial few months was rough but drivers are absolutely serviceable and have been for a while, and continue to get better each release.  I play lots of different games on steam btw, very rarely do I have issues.",Positive
Intel,to our knowledge... i wonder what kind of uplift we'll see it have with next gen cpus,Neutral
Intel,It's BMG-G31.  https://videocardz.com/newz/intel-confirms-bgm-g31-battlemage-gpu-with-four-variants-in-mesa-update  https://videocardz.com/newz/intel-ships-battlemage-g31-gpus-to-vietnam-labs-behind-past-arc-limited-edition-cards,Neutral
Intel,"It isn't the number that determines the generation, it's the prefix.  A770 was ACM-G10 (alchemist G10), while the B580 is BMG G21 (Battlemage G21). The shipping manifests that have been floating around for the better part of a year have been for the BMG G31. Unless new leaks I'm not up to date with are discussing a G31 with a different prefix, everything points towards it being battlemage, not celestial.  Now I pray that Intel have found a way to mitigate the driver overhead. If not, the B770 will be utterly useless for gaming. Nvidia is bad in the overhead regard, but the B580 is damn near an order of magnitude worse.",Neutral
Intel,> Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course  Not even close.,Negative
Intel,"HUB showed the b580 lost like 20%+ perf between the 9800x3d and the 7600 or 5700x3d and actually fell behind the 4060, as the 4060 lost minimal performance on the weaker cpu vs using the 9800x3d. And the 7600x and 5700x3d can certainly power much stronger gpus like the rtx 5070 without bottleneck.  Edit: my bad, I didn't know it was for only a specific game, though still not a good result for b580 overall",Negative
Intel,Where do you find such information?,Neutral
Intel,Source: I made it the fk up,Negative
Intel,"I was going to say, I put my sister in a b580 and she has had no driver issues in 6 months.",Positive
Intel,"That's sort of my point, they'll probably still exert more price pressure across the stack than the b770, despite being a totally different segment.",Neutral
Intel,https://youtu.be/00GmwHIJuJY?si=z4wU05sJx2SeS7K1  How can people get on here and lie and literally no one questions them? The 7600 only lost anywhere near that performance on Spider-Man Remastered,Negative
Intel,"Inference, there was a blog post tracing and comparing what the Arc driver does with the Radeon driver. The radeon driver just sends a few pointers to buffers, the Arc driver sends large amounts of data. Assuming the driver programmers at Intel aren't idiots, it's because something is seriously wrong with the cards and DMA.",Neutral
Intel,"No, I inferred it from tracing what the driver does, and assuming the programmers aren't idiots.",Neutral
Intel,Try Mechwarrior 5: Clans on high and say there are no problems again.,Positive
Intel,"that game runs on my 3080 ti like ass, just like all early UE5 games...  even with RT turned off, to hit solid 4k60 I needed DLSS and if I wanted 90+ fps I need to use DLSS performance / ultra performance.",Negative
Intel,Steve from HUB and Steve from GB both lack the technical knowledge to look into the underlying issues,Negative
Intel,"It doesn't even run, it crashes left and right on an Arc.",Negative
Intel,"that I have no idea, on launch I did have crashing issues on my 3080 ti, but they did get resolved over time.  but if you are now seeing it still then welp  PGI is a small team that may not have gotten help / getting to it themeslves to make their game arc capable.",Negative
Intel,"Yeah, still doesn't work at the latest patch (and latest Arc driver) with anything other than low.",Negative
Intel,SR-IOV at that price. Who cares about anything else.,Neutral
Intel,Intel would be stupid to axe there Graphic card division if this proves to be successful.,Negative
Intel,"Single-slot variant or custom cooler please, my MS-A2 running proxmox is demanding this card.",Neutral
Intel,About 66% overall performance of a B580 it looks like. That's really nice for a 70W card.,Positive
Intel,"This is exciting, definitely looking forward to the b60 as well.",Positive
Intel,"Obligatory ""Intel is exiting the GPU business any moment now"".",Neutral
Intel,how hard are tehse to actually buy?,Neutral
Intel,"Buying one, this is impressive",Positive
Intel,"if compared by price to performance ratio,  ARC B50 is slower than RTX 5060 in terms of price and performance",Neutral
Intel,Its better than a 1.5 year old bottom of the range card....well done i guess.,Positive
Intel,Better than NVIDIA? lol .... oooookay,Positive
Intel,"Haven't seen the video, but I'm already buying one if that's the case",Neutral
Intel,Literally it could be a damn arc a310 or rx6400 and people would buy that card at $350 without licencing bs. For anything VDI related the B50 is huge.,Negative
Intel,Intel’s “MOAAAAAR CORES” in the GPU space???,Neutral
Intel,what is that? SR-IOV?,Neutral
Intel,Super interesting!  Wonder how well it would handle AI tasks like Frigate while another VM uses it for inference or a third doing video transcoding with Plex.,Positive
Intel,16 GB VRAM too,Neutral
Intel,They will eventually axe it.,Negative
Intel,Instead of axing it maybe spin it off like AMD did with Global Foundries?,Neutral
Intel,And if it isn’t successful?,Negative
Intel,They do have 3rd party vendors for ARC PRO Cards this time around so it most likely will happen.,Neutral
Intel,"The B60 is more exciting to me just for that 24GB VRAM. Still, at this price point the B50 is a pretty compelling buy tbh.",Positive
Intel,I think it would be really stupid for them to do so.,Negative
Intel,You can preorder from newegg now. They ship later this month.,Neutral
Intel,"One Swedish retailer I checked has them coming into stock next week (10 September) and open to orders, how much stock there will be however, I have no clue.",Neutral
Intel,Same. I put my preorder in. Plan to put it into one of my sff builds.,Neutral
Intel,Why is this impressive for $350 USD? How will this be useful for you? I’m not being sarcastic. I am genuinely curious.,Neutral
Intel,What did bottom of the range cards cost 1.5 years ago?  How much VRAM did they have?  Did they support SR-IOV?   Just think for a bit sometimes.,Neutral
Intel,It quite literally is. Watch the fucking video.,Negative
Intel,"Wendell confirmed as much in the comments, looking forward to his future testing of the card.",Positive
Intel,What does AMD have in this product segment?,Neutral
Intel,"Ingen av de kortene greier LLM, hvis du tror det.",Neutral
Intel,"Ingen av de kortene greier LLM, hvis du tror det.",Neutral
Intel,"I didn't know either so I looked it up.   ""SR-IOV (Single Root Input/Output Virtualization) is a PCI Express technology that allows a single physical device to appear as multiple separate virtual devices, significantly improving I/O performance in virtualized environments by giving virtual machines direct access to hardware. This bypasses the overhead of a software-based virtual switch, resulting in lower latency and higher throughput for demanding applications by dedicating virtual functions (VFs) to guest VMs.""",Neutral
Intel,Everyone (including Nvidia) is moving toward APUs with large GPUs onboard. Why would Intel kill their chance at competing in that market?  They've already withstood the most painful part of the transition. There's no point in stopping now.,Neutral
Intel,They are keeping their Fabs which is even more expensive to maintain why would they sell GPU not to mention their iGPUs are pretty Damm good nowdays not like meme in Intel HD4400 even though they could play any game /jk.,Neutral
Intel,Doubt they have the revenue to spin out successfully without significant investment from outside sources.,Negative
Intel,Sadly Intel has a recent history of making poor life choices.,Negative
Intel,"Maybe it's just me, but this reads as AI generated.",Neutral
Intel,"I dunno man, I was building a PC for work and the 3050 was the cheapest Nvidia card I can get and the 7600 is the cheapest from AMD. Huge price gap between the two, by about 100 USD. AMD really needs to buck up their APUs to render cheap GPUs surplus or have something cheaper than a 7600 to price match the 3050.",Neutral
Intel,"They're comparing it to an entry-level NVIDIA GPU, the A1000. Saying that Intel GPUs are ""better than NVIDIA"" as a universal statement is flat-out wrong. Let's see some competition to the RTX 5070 Ti, 5080, or 5090. NVIDIA has zero competition on mid-range and high-end GPUs.",Negative
Intel,Radeon Pro V710 and you can't even buy it retail.,Negative
Intel,thanks 🙏,Positive
Intel,Because intel shareholders are super short sighted.,Negative
Intel,this comment is the weirdest version of 'corporations are people' that i've encountered,Negative
Intel,Lmao seriously the formatting and the amount of bolded words just screams AI,Negative
Intel,It's AI generated in your mind,Neutral
Intel,"because AMD has a bad habit of leaving a bunch of their older cards in the channel and having them become the low end...  CPU and GPU, AM4 lives for so long because there are still piles of the stuff in the channel and just gets slow tiny discounts till its gone in full  its like their demand forecast is too optimistic or something but at this point I think its deliberate",Negative
Intel,Because this is not a gaming GPU and thus the A1000 is the correct card to compare with.,Neutral
Intel,Good luck using those super gpus to host multiple gpu accelerated vm with one card. Nvidia won't let you.,Negative
Intel,"Yes, compare an Arc Pro to a GeForce, totally the same market.",Neutral
Intel,"That seems more mid or high tier rather than these relatively low tier gpus, the b50 is a cut down b580...  Also the v710 seems like the kind of ""passive"" gpu that's ""passive"" as long as it's next to several 7,000 rpm fans.  So it would probably not work very well as a retail car because it's rack server focused.",Negative
Intel,"I think the really loud short sighted shareholders have quieted down a bit after it became clear they helped drive the company to where they are. Hell, they're probably not even shareholders anymore.",Negative
Intel,The weirdest version was Citizens United,Negative
Intel,Did you not figure out why they’re bolded?,Neutral
Intel,I'm aware but it's the only current gen GPU for graphics workloads that has virtualization support from AMD.,Neutral
Intel,"The current Chairman of the board, Frank Yeary, is one of these stupid short sighted people. He REALLY wants to sell the fabs, and is probably the reason Intel went through their latest round of layoffs (Lip-Bu Tan wanted to raise money from Wall Street, Yeary apparently sabotaged it).",Negative
Intel,"if corps are people, they should be allowed to vote, right ?",Neutral
Intel,"Not only that, but because they are people, they should also be able to fund gigantic super PACS to get a candidate into office. I love America!",Positive
Intel,"u/michaellarabel It would be super cool to have Molecular Dynamics benchmarks for these kind of cards, since you already use them for CPU testing and a few of them (e.g. GROMACS) support APIs from all three vendors (CUDA, ROCm, SyCL, + OpenCL)",Positive
Intel,"I would be so curious about a die shot and GPU architecture of the A19 vs 8 Elite. Because Apple went from very little rt not so long ago to leading the charts. Like how many RT-Acceleration units are used in the GPU and how other components play into that.  Definetly exciting times, I think that the major players, Qualcomm, Apple, and Mediatek, are not so far away from each other, the leader can change in a generation or two, which hasn't been the case often in recent history.  Especially interesting will be how AMD client will turn out in 2026, if they are still on the charts (we expect Zen 6 clients on N3) or if Qualcomm will top windows charts, and Apple overall all the charts.",Positive
Intel,"Growing the E core into a medium core is really important and something I was worried apple might miss out on, but this means they should be able to keep up in multicore as trends point to flooding CPUs with E cores.  They really ~~buried~~ deleted the lede by not even mentioning the E core improvements beyond the cache and undersold the GPU. I suppose this means we should expect a lot of bragging in the M5 keynote.",Neutral
Intel,Youtube link:  [https://www.youtube.com/watch?v=Y9SwluJ9qPI](https://www.youtube.com/watch?v=Y9SwluJ9qPI),Neutral
Intel,I cannot wait to see what's in store for 2026 Mac Studios and the M5 CPU. Especially if M5 Ultra makes its debut. AI workloads should see a significant performance boost by 3-4x? I wonder if M5 Ultra will offer 1000GB/s memory bandwidth.,Positive
Intel,6 wide e core holy shit,Neutral
Intel,"A major exciting aspect for me is the massive boost to Raytracing performance. The M4 Max is the closest anyone has ever come to matching Nvidia in 3D Raytraced Rendering, beating out even AMD. In Blender M4 Max performs somewhere in between an RTX 4070M and 4080M.  A 56% leap in RT performance would essentially put an M5 Max closer to a RTX 5090M than anyone before at a fraction of the power.",Positive
Intel,"Is there a link for English? If not, can you summarize how they tested sustained performance and how much is the improvement over previous generations?",Neutral
Intel,"The E core having more improvements than just 50% larger L2 is a nice surprise, but damn the efficiency and performance of it is insane. 29% and 22% more performance, at the same power draw is insane, clocking like 6.7% higher too. They used to be behind the others in performance with the E cores but had better efficiency but now they both have better performance and efficiency.  As for GPU, I always wanted them to focus on GPU performance next and they finally are doing it. Very nice, the expected 2x FP16 performance, which now matches the M4 which is insane(M5 will be even more insane). Gpu being 50-60% faster is a nice sight to see. For RT performance(I still find it not suited for mobile but M5 will be a separate matter) I’m surprised that the massive increase is just from 2nd gen dynamic caching, the architecture of the RT core is the same, just basically a more efficient scheduler which improves utilization and less waste.  For the phone, vapor chamber is nice, them being conservative on having a low temperature limit can both be a good and bad thing which is shown, the good thing is that it means the surface temperature is lower so the user won’t get burned holding the device, and the bad thing is that it can leave performance off the table which is shown. As that can probably handle like another extra watt of heat and performance. Battery life is very nice, the fact that it can match other phones with like over 1000mAh bigger battery is funny. As people always flexing over how they have like a 4000, 5000mAh+ battery, of course having a bigger capacity is better, but the fact that Apple is more efficient with it and can have the same battery life at a much smaller battery speaks volumes about it.",Positive
Intel,"The occupancy characteristics of A19 Pro are quite incredible. 67% occupancy for a RT workload.  Look at Chips and cheese's SER testing. 36-44% ray occupancy with SER in Cyberpunk 2077 RT Overdrive.     Assuming NVIDIA can get this working on 60 series an effective a 52-86% uplift. After OMM and SER this seems like the third ""low hanging"" RT fruit optimization. Anyone serious about a PT GPU architecture NEEDs dynamic caching like Apple. And no this is not RDNA 4's Dynamic VGPR, it's a much bigger deal. Register file directly in L1$ has unique benefits.",Positive
Intel,"Nice been waiting for this. P-core frontend improvements and branch, and a wider E-core with it's newer memory subsystem shows great YoY gains as usual. Though I am not surprised since it's been years leading up to this that Apple has steadily have been increasing power/freq to get the rest of it's performance gains, although IPC YoY is still class leading. The wider e-core takes the stage which is now commonly being focused in the industry (ex. Intel: Skymont etc). Excited for any outlet doing die analysis (I don't know if kurnal has done it yet).  Real generational GPU gains, instead of last year's YoY tick. Supposedly GPU size has not increased and that is impressive. Massive FP16 compute matching the M4, really shows their commitment to ML (as if naming 'tensor cores' wasn't obvious) and this will greatly help with prompt processing if you're into local models. Finally with a vapour chamber in the PRO models, performance overall really stretches it's legs and sustained is really respectable.  Also, since I'm skimming, I'm assuming A19 base like it's predecessor is a different SoC to the Pro. It is also really really refreshing to see the base A19 be better than the 18 Pro, little to no stagnation and a year at that. The base iPhone 17 looks like a reaaly reallly good option, more than ever, wished they didn't drop the Plus model. But man, I feel like waiting another year, hearing rumours about N2 and new packaging tech excites me.  That said, looking forward to QC, MT, and Samsung. SD8EG5 seems to be closing the gap, and that'll be very interesting tho those GB numbers don't tell things like power.",Positive
Intel,"""Generations ahead of other ARMs M cores"".   Uhm we are getting the Dimensity 9500 and 8 elite gen 5 next week    The C1 Pro has 20% IPC improvement IRRC, plus this is N3P   Let's not jump to conclusions before seeing the competition    I also wonder if QC made changes to the E cores",Neutral
Intel,>SLC (Last Level Cache in Apple's chips) has increased from 24MB to 32MB  Really tells you how pathetically stringent AMD has been with cache sizes on their apus (no die size excuse allowed here because they never use leasing nodes specially N4 was already old when Strix point debuted),Neutral
Intel,">A19 Pro E core is **generations ahead** of the M cores in competing ARM chips.  >A19 Pro E is 11.5% faster than the Oryon M(8 Elite) and A720M(D9400) while USING 40% less power (0.64 vs 1.07) in SPECint and 8% faster while USING 35% lower power in SPECfp.  >A720L in Xiaomi's X Ring is somewhat more competitive.  No, Apple's 202**6** E-cores are just **+3% (int perf)** and **+1% (fp perf)** vs Arm's 202**5** E-cores, though at **-22% (int)** and **-16% (fp)** less power.  Note: Geekwan's chart is wrong. The Xiaomi O1 does not use the A72**0.** It uses the upgraded A72**5** from the X925 generation. Not sure how Geekerwan got the name wrong, as they recently reviewed it.  Integer  |Core / Node|int perf|int %|int power|int perf / W|int perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|4.17|103%|0.64W|6.51|132%| |Xiaomi A725 / N3E|4.06|100%|0.82W|4.95|100%|  Floating point  |Core / Node|fp perf|fp %|fp power|fp perf / W|fp perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|6.15|101%|0.92 W|6.68|120%| |Xiaomi A725 / N3E|6.07|100%|1.09 W|5.57|100%|  I would not call this ""generations"" ahead.",Neutral
Intel,I always wonder how they plot the architecture map and figure out such as the depth of LDQ kind of things...Is it public somewhere? That kind of detail won't be able to get via regular benchmark right?,Neutral
Intel,A19 Pro GPU is now only 1.62GHz vs 1.68GHz in A18 Pro while having the same number of ALUs (768). Does that mean the increased performance is basically due to memory bandwidth increase?,Neutral
Intel,"At this point all the flagship phones are great. I have an iPhone 14 pro max and a Samsung Galaxy S23 Ultra. But I’ve had iPhones for years and i couldn’t switch if i wanted too i have way too much invested in ios (games, apps, music etc). But I ordered a 17 Pro Max 1tb thought about 2tb’s but have a 1tb now and still have 450gb’s free and i download everything and never delete anything. Im trading in the Samsung and giving iPhone to my mom. Still think of buying a cheap phone that gives me the $1100 trade in because it’s any condition and I hate trading in a phone thats practically new.",Positive
Intel,-21 freezer lol https://browser.geekbench.com/v6/cpu/14055289,Neutral
Intel,"The snapdragon 8 elite gen 5 still beats it though , and handily at that. Unless I am wrong",Positive
Intel,">Power however has gone up by 16% and 20% in respective tests leading to an overall P/W regression at peak.  That's really not good for the A19 **Pro**, sadly a mark against Apple's usual restraint. It's a ***significant*** *perf / W* downgrade in floating point. The 19 Pro perf / W is notably worse than **ALL** recent P-cores from Apple, Qualcomm, *and* Arm:  |SoC / SPEC|Fp Pts|Fp power|Fp Perf / W|Perf / W %| |:-|:-|:-|:-|:-| |A19 Pro P-core|17.37|10.07 W|1.70 Pts / W|84.2%| |A19 P-core|17.13|8.89 W|1.93 Pts / W|95.5%| |A18 Pro P-core|15.93|8.18 W|1.95 Pts / W|96.5%| |A18 P-core|15.61|8.11 W|1.92 Pts / W|95.0%| |A17 Pro P-core|12.92|6.40 W|2.02 Pts / W|100%| |8 Elite L|14.18|7.99 W|1.77 Pts / W|87.6%| |O1 X925|14.46|7.94 W|1.82 Pts / W|90.1%| |D9400 X925|14.18|8.46 W|1.68 Pts / W|83.2%|  These are *phones*. Apple, Arm, Qualcomm, etc. ought to keep max. power in check. This is *o*n par with MediaTek's X925, a bit worse than the 8 Elite, and much worse than Xiaomi's X925.  I would've loved to see efficiency (joules) measured, like AnandTech did. That would show us at least if ""race to idle"" can undo this high 1T power draw or not in terms of battery drain.",Negative
Intel,"All of these benchmarks are sort of difficult to compare when the other chips aren't made on the same node N3P. Apple buys first dibs on the wafers so they always have that advantage, it isn't always about the architecture itself.  It will be more interesting when there are Qualcomm chips out with their architecture on this N3P node, and the Mediatek chips with the usual off the shelf ARM cores on this node too to compare.",Neutral
Intel,"Those GPU stats are false. According to Tom's Guide, in 3D Mark Solar Bay Unlimited, the 17 Pro Max is only 10% faster than the s25 ultra https://www.tomsguide.com/phones/iphones/iphone-17-pro-max-review#section-iphone-17-pro-max-performance-cooling",Negative
Intel,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,">I would be so curious about a die shot and GPU architecture of the A19 vs 8 Elite. Because Apple went from very little rt not so long ago to leading the charts. Like how many RT-Acceleration units are used in the GPU and how other components play into that.  Me too. I'd imagine a marginal increase in size over A18 Pro. The P cores have mostly stayed the same. And the E cores despite major changes are unlikely to contribute to a major increase in area (if I'm right, individually they occupy around 0.6-0.7mm2 per core). The extra cache (around 2MB) should increase area slightly. SLC area as well should contribute to that increase.  I'd imagine the GPU with the new RT units, doubled FP16 units, new tensor cores, and general uarch improvements are the major contributor to any notable area increase.  Plus I still don't feel like the approaches these companies are taking are aligned very much in terms of GPU architectures. For eg, Apple's been very focussed on improving compute performance on their GPU. Qualcomm less so.  >Definetly exciting times, I think that the major players, Qualcomm, Apple, and Mediatek, are not so far away from each other,   True that. Qualcomm has a shot at taking the ST crown lead from Apple atleast in SPECfp. But Apple's done extremely well with this upgrade. The E core jump has made them close the nT gap with Qualcomm while using much lower power.  GPU is a case where technically Qualcomm could take raw perf crown. But Apple's RT dominance, Tensor cores and general compute lead might help them in the desktop space.  >Especially interesting will be how AMD client will turn out in 2026, if they are still on the charts (we expect Zen 6 clients on N3) or if Qualcomm will top windows charts, and Apple overall all the charts.  Its either Qualcomm or Apple. AMD is too far behind Oryon and Apple's uarchs. They consume similar or even more area for their core architecture while lagging in performance while using significantly more power. The x86 ecosystem and compatibility is the only reason they'd survive Oryon.",Neutral
Intel,Having so many SoC makers on arm competing against each other by one upping each other yearly is bearing fruit vs x86. i don't know how Intel and AMD long term can fare at this rate. Arm CPUs are still showing double digit gains yearly.,Neutral
Intel,Most of apples RT gains are from optmsiing how the GPU deals with divergence.  This is not dedicated RT silicon so much as making the GPU be able to maintain much higher throughput when there is lots of divergence.  RT operations have a shit tone of divergence.,Negative
Intel,I can't wait to see die shots and measurements for these chips. The A18 Pro and A18 die shots were really interesting to see what was compacted or lost for the base model chip. I have a feeling that there will be bigger differences for the A19 Pro and A19 with that giant SLC on the former. Die areas will also be interesting. Cache isn't cheap for area and I'd also love to see inside the new E-cores and GPU.,Positive
Intel,">Growing the E core into a medium core is really important and something I was worried apple might miss out on, but this means they should be able to keep up in multicore as trends point to flooding CPUs with E cores.  I wouldn't say its a medium core yet tbh. Apple's E cores are still sub 1mm2 in area usage. Compared to other ""M cores"" they are still relatively small. I imagine the A19 Pro E core is fairly larger but the A18 E core was around 0.6/0.7mm2 in size. I'd imagine its not grown a whole lot.  >They really ~~buried~~ deleted the lede by not even mentioning the E core improvements beyond the cache and undersold the GPU. I suppose this means we should expect a lot of bragging in the M5 keynote.  I'd imagine they're saving up these to mention Blender perf etc in M5 keynote.",Neutral
Intel,4 minutes ago. Damn. I should have waited. I'll post it!,Negative
Intel,"An M5 Ultra would offer 1.23 Tb/sec of bandwidth scaling from the A19 Pro.  M5 (128-bit, LPDDR5X-9600) -> 153.6 GB/s M5 Pro (256-bit, LPDDR5X-9600) -> 307.2 GB/s M5 Max (512-bit, LPDDR5X-9600) -> 614.4 GB/s M5 Ultra (1024-bit, LPDDR5X-9600) ->1228.8 GB/s",Neutral
Intel,"Haha. I mean they are actually pretty late to this tbh. Most ""E/M cores from other competitors"" are similar in size if not bigger. I'd imagine Apple's E core is stuck between a true M core like the A7xx and a true E core like A5xx in terms of area, although it probably leans toward the A7xx in that regard.",Neutral
Intel,What does 6 wide mean? What units?,Neutral
Intel,"[https://www.reddit.com/r/hardware/comments/1jcoklb/enable\_rt\_performance\_drop\_amd\_vs\_nvidia\_20202025/](https://www.reddit.com/r/hardware/comments/1jcoklb/enable_rt_performance_drop_amd_vs_nvidia_20202025/)  In gaming RDNA4 RT isn't that far behind Blackwell. Other than that raytraced rendering like in Blender AMD has been for a while far behind. It won't be until Blender 5.0 till we see any improvements to HIPRT. Though for the longest time since following HIP it's been rather mediocre and my expectations are low for next release, though their [PRs](https://projects.blender.org/blender/blender/pulls/145281) make it seem they've been doing some work. It's a low priority for AMD which is unfortunate.",Neutral
Intel,>	beating out even AMD  Was that one really surprising?,Neutral
Intel,Is that Metal vs Optix or Metal vs Cuda?,Neutral
Intel,"The video has a mostly accurate English captions option. CPU P core is up 10%, E core is up by 25%, GPU perf is up by 40% and sustained performance is up by 50%.",Positive
Intel,> just basically a more efficient scheduler which improves utilization and less waste.  When you take a look at GPUs doing RT task you see that they tend to be very poorly utilized.  GPUs are not designed for short running diverging workloads. But RT is exactly that. So you end up with a huge amount of divergence and or lots of wave like submissions of very small batches of work (so have a large scheduling overhead).     There is a HUGE amount of perfomance left on the table for this type of task for HW vendors that are able to reduce the impact on GPU utilization that divergence has.,Negative
Intel,Maybe Apple measures occupancy differently in their tools. I wouldn't be too sure comparing these two. But I'd definitely think a combination of SER and Dynamic Caching present in A19 should result in very good utilization compared to other uarchs.,Positive
Intel,"That is very impressive, RT tends to have very poor occupancy as it is a heavily branching workload!",Positive
Intel,>Supposedly GPU size has not increased and that is impressive.   Important to note that the supporting SLC which is a major reason for improvements om the GPU side has increased from 24Mb to 32Mb. Which would increase area a bit.,Positive
Intel,">The C1 Pro has 20% IPC improvement IRRC, plus this is N3P  N3P is 5% faster than N3E. By TSMC's own claim..  Also I can't find a source for a 20% IPC improvement. ARM's claim is 16% IPC improvement. And that is not without a power cost since ARM claims that at similar performance, power reduction is only 12%.  https://newsroom.arm.com/blog/arm-c1-cpu-cluster-on-device-ai-performance  >Let's not jump to conclusions before seeing the competition   I mean I agree. But I don't see how the C1 Pro is supposed to cross a 95% P/W disparity. (4.17 points using 0.64W vs 3.57 points using 1.07W) using D9400",Neutral
Intel,"Tbf, Apple had a 32Mb SLC back in the A15 Bionic. They reduced the size of that afterward to 24Mb. Its not like the size significantly mattered in GPU performance until now.",Neutral
Intel,A 30% lead in P/W is a generations ahead in this day and age. Considering the successor (C1 Pro) is stated by ARM to reduce power by just 12% at iso performance leaving Apple with a comfortable lead for a year. Also I specifically was a bit confused by their choice to compare the A725L instead of the M variant.,Neutral
Intel,There's a guy on twitter who does the microbenchmarking for them.,Neutral
Intel,"No. I'm positive memory bandwidth offers very little in terms of performance upgrades. If you recall, the A16 was essentially the same GPU architecture as the A15 but used LPDDR5 instead of LPDDR4X, yet there were practically zero performance improvements.  I don't think anyone has investigated the A19's GPU microarchitecture thoroughly. But the main improvements seem to come from the increase in SLC size (System Level Cache which serves the GPU) from 24Mb to 32Mb and the newly improved Dynamic Caching. Its very likely there are a lot more changes responsible for that 40%+ improvement that we don't know about.",Negative
Intel,With most GPU tasks you are never ALU limited.,Neutral
Intel,"8 elite gen 5 loses in ST perf, only matches in MT perf at same power, wins slightly in GPU perf, loses in GPU RT perf. It doesn't beat it handily by any metric.",Negative
Intel,">That's really not good for the A19 **Pro**, sadly a mark against Apple's usual restraint. It's a ***significant*** *perf / W* downgrade and the 19 Pro perf / W is notably worse than **ALL** recent P-cores from Apple, Qualcomm, *and* Arm  Technically we need to compare P/W at similar power or similar performance. Peak power P/W is not a very accurate measure to compare gen on gen. And I really doubt you'll be seeing 10W of ST power consumption on a phone. Geekerwan tests these things using Liquid Nitrogen. They are to test an SoC without restraints.   An A19 Pro P core does seem to have increased performance at the same power by about 5%. Only P/W at peak has reduced. Which isn't even an issue when these phones would never reach that peak in daily use.",Negative
Intel,Im having my doubts here. Howis the A19 p core much better than the A19 pro p-core? Aren’t they exactly the same p cores?,Neutral
Intel,">All of these benchmarks are sort of difficult to compare when the other chips aren't made on the same node N3P.   N3P is a mere 5% faster than N3E when comparing similar microarchitectures... This is straight from TSMC's marketing slides.  Comparitively, barring the P core (which did see an ok improvement), the E core and the GPU have seen 30%+ improvements. The node has nothing to do with it.  If Qualcomm loses to, matches or exceeds A19 Pro this year it would be because of their updated microarchitectures and have barely anything to do with minor single digit improvements offer by a sub node.",Neutral
Intel,in the end what matters is the sicion you can buy so you can compare them.,Neutral
Intel,Please tell us more about how we can never compare AMD vs Intel chips by your logic,Negative
Intel,"Tom's Guide tested basic Solar Bay. This is the older version of the benchmark with less raytraced surfaces.  Geekerwan tested the modern, updated version of Solar Bay referred to as Solar Bay Extreme. This new benchmark has a much higher raytraced load, with far more reflective and transparent surfaces and much more detailed scene with more geometry.  Please kindly read the benchmark title mentioned in the posts. Or atleast watch the videos. Before commenting.",Neutral
Intel,Bad bot,Negative
Intel,"I think AMD will have a hard time to win any efficiency crowns, but historically speaking they always had the peak ST performance on process node. Of course this is not as impressive because those are desktop chips which draw 100W+, and there have been historically gaps between desktop and laptop chips in ST for AMD (even though laptop technically had the TDP to sustain ST).  I just wouldn't call the race so early, but it does seem very likely, that AMD will be behind. I just dont think it is as bad as it seems. AMD was plagued by Zen5% and still on 4nm for client, they might hit heavy with client dedicated improvements and N3, but in the end we have to see, x86 client performance really seems to struggle rn (and whatever intel is doing...).",Negative
Intel,">Its either Qualcomm or Apple. AMD is too far behind Oryon and Apple's uarchs.  Considering Zen 5 is designed to clock way higher, I don't think it's that bad really.  Also... Let's not forget about ARM. A SoC with ARM CPU cores and an Nvidia/AMD GPU could absolutely ruin Qualcomm's day regardless of how better/worse their custom CPU cores are.",Negative
Intel,>Qualcomm has a shot at taking the ST crown lead from Apple atleast in SPECfp  No,Neutral
Intel,"IMO, Microsoft needs to push hard to switch to Windows-on-ARM, or else they risk an Android-like OS for Laptops swooping in and filling the gap left by those who do not want to go the Apple route. It feels like a crucial moment for both Windows and Intel/AMD, at least in the x86 product space. It retains PC Gaming at this point, but if that nutt is cracked via even half-decent, *compatible* emulation, then ... sayonara!",Neutral
Intel,"Yeah, the E core isn't quite there yet, but it's good to know they're moving in that direction with now stubborn they are with P cores, refusing to include more than what can be run at the equivalent of all core turbo.",Neutral
Intel,"Wow that will be insane and adding the new ""Tensor"" elements added to the GPU cores will make it a formidable AI workstation.  Especially when NVIDIA monopoly is only offering small VRAM GPU cards at absurd prices.",Positive
Intel,That would be amazing. Id love to see them put some hbm there too,Positive
Intel,It’s highly unlikely they will go past a 256 bit bus. You run out of pins and layers to route a bus that wide. Gets extremely expensive. Still neat bandwidth!,Neutral
Intel,The m5 ultra should be on par with a 4090?,Neutral
Intel,Skymont (Intel’s E core) is 8 wide (9 wide at first stage then bottlenecks to 8 I think iiuc),Neutral
Intel,"According to AnandTech, Apple's E cores were based on Apple's Swift core, back when they had a single core type  Previous die shots show Apple's E cores were close to Arm's A5xx in core area (larger, but far smaller than A7xx core only). But in terms of core+L2 Apple's E cores are similar to Arm's A7xx in core+L2 area  I'd argue it's the other way around, Apple's E cores are the true E cores  Whereas Arm's A7xx were stuck between being an M core and an E core  Now Arm has split their A7xx core to Cx Premium (M core) & Cx Pro (true E core)  Arm's A5xx/Cx Nano are a very odd core, almost no else makes a similar in-order core. Arm's A5xx/Cx Nano are more like Intel's LP E cores, instead of Apple's E cores",Neutral
Intel,The decoder,Neutral
Intel,"Amd is very far behind in rt.  You re linking gaming benchmarks, thats not rt thats mixed use.    Just look at path tracing results for a more representative comparison",Negative
Intel,"Hey, they made an effort with RDNA 4. I think that should surpass the M4 Max. I just can't find any proper scores for it.",Neutral
Intel,"Metal vs Optix.  https://youtu.be/0bZO1gbAc6Y?feature=shared  https://youtu.be/B528kGH_xww?feature=shared This is a more detailed video with individual comparisions and a lot more GPUs.  Its a lot more varied. In Lone Monk, it hangs with a desktop class 5070. In Classroom, it hangs neck to neck with a 4060Ti. In Barbershop, it falls behind a desktop 4060Ti. In scanlands, it falls behind a 4060.   If we consider Classroom as a baseline average, a theoretical 60% faster M5 Max, like the jump we saw in Solar Bay, would land hot on the heels of a desktop class 5070Ti, a 300W card. Competing with a 65W laptop GPU.  Edit; The Youtuber is using the binned 32C variant. A 40C variant would surpass the 5070ti.",Neutral
Intel,"Yeah I forgot what was the term before but I remember, it’s just like Nvidia’s Shader Execution Reordering introduced in Ada Lovelace.",Neutral
Intel,I have a query regarding RT workloads. Would offsetting RT performance to the CPU with the help of accelerators help? Or is that not the case and it would be even worser on CPUs.,Neutral
Intel,"Sure it might be different, but I doubt it. Occupancy is just threads used/total threads.  It's interesting how first gen dynamic caching + SER (apple equivalent) is hardly better than NVIDIA in terms of occupancy. Yet only 44%. So only slightly better than NVIDIA (upper end of range). Seems like first gen was more about laying the groundwork while second gen is really about pushing dynamic caching allocation granularity and efficiency. At least so it seems.  Oh for sure. That occupancy is incredibly high. \~1.5x uplift vs A18 Pro. Getting 70% occupancy in RT workload is really unheard of. Apple engineers did a fine job.  AMD might opt for this nextgen if they're serious, but it's a massive undertaking in terms of R&D, but could massively benefit PT and branch code, ideal for GPU work graphs.",Neutral
Intel,Agreed and you can see that by comparing with occupancy numbers for competitors.       Anyone who's serious about RT needs to copy whatever Apple is doing xD,Positive
Intel,"C1 Pro is two generations ahead of the A720 in the 9400. Also, Xiaomi demostrated a much more efficient implementation of the A720 cores in their O1 chip (4.06 points at 0.82 W).  Edit: actually, it seems like the O1 uses A725 cores. Perhaps that is what they are referring to in the video as ""A720 L""",Positive
Intel,It’s been a BIG bottleneck in AMD’s apus since Vega 11 back in 2018. Doubling from 8CU to 16CU in 860m vs 890m gets you only +30%.   AMD is just so damn stringent with area despite jacking up the price on Strix point massively on an old ass node.,Negative
Intel,"You ought to do the math first. Power is the denominator. 12% reduction in power is *substantial*.  Integer: A19 Pro E-core is 3% faster at 12% less power vs claimed C1-Pro.   |Core / Node|int perf|int %|int power|int perf / W|int perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|4.17|103%|0.64W|6.51|132%| |Xiaomi A725 / N3E|4.06|100%|0.82W|4.95|100%| |\-12% power|4.06|100%|0.72W|5.64|114%|  Floating point: A19 Pro E-core is 1% faster at 4% less power vs claimed C1-Pro.  |Core / Node|fp perf|fp %|fp power|fp perf / W|fp perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|6.15|101%|0.92 W|6.68|120%| |Xiaomi A725 / N3E|6.07|100%|1.09 W|5.57|100%| |\-12% power|6.07|100%|0.96 W|6.32|113%|  Hardly ""generations ahead"".",Neutral
Intel,Thanks for your reply.  CPU monkey reported 2GHz instead of 1.62GHz. So maybe that's where most of the gain comes from.  I suppose the tensor core like matmul units also boost performance for graphics and AI.,Positive
Intel,Maybe the improvement is from the new matmul units?,Neutral
Intel,"I am deeply suspicious of all of these power measurements. Separating p core power usage from other aspects of the soc is difficult on iOS. Which is not a criticism of your summary, but I’d be wary of drawing anything definitive about the efficiency of the p cores.",Negative
Intel,"It's not a knock on the design, it's how apple is configuring the CPU. It doesn't matter that performance at the same power is improved if the default clocks on the real product put it way past the sweet spot into diminishing returns so bad it regresses efficiency.   On one hand, the power inflation isn't causing problems if the 23% increased battery life per Wh is anything to go by, but on the other, what's the point of chasing peak performance like this if your boost/scheduling algorithms never allow that speed to make an impact on responsiveness?",Negative
Intel,"Geekerwan's results are ***average*** power, not peak power, IIRC. These are real, actual frequency bins that Apple has allowed.  These frequency bins *will* be hit by some workloads, but just not nearly as long as SPEC & active cooling will allow. It would be good to revisit Apple's boosting algorithms, but IIRC, they hit 100% of Apple's design frequency in normal usage.  It's not like users have a choice here; we can't say, ""Please throttle my A19 Pro to the same power draw as the A18 Pro."" Low power mode neuters much of the phone, so it's rarely used all the time.  //  I find avgerage power useful two reasons:  1. *How* performance vs power were balanced; here, performance took precedence while not keeping power stable.  2. It also shows, when nodes are not the same, where the node's improvements went. Here, an N3P core delivers notably worse perf / W versus an N3E core. TSMC claims [up to 5% to 10% less power](https://www.tomshardware.com/tech-industry/tsmcs-3nm-update-n3p-in-production-n3x-on-track) on N3P vs N3E.  I agree 10W is not common and SPEC is a severe test, but it's more the *pattern* that has emerged on Apple's fp power and whether it's worth it:  2023 - A17 Pro P-Core: 6.40W  2025 - A19 Pro P-Core: 10.07W  Apple has leaped +**57%** average fp power in two years. Seems like not a good compromise when you're eating more power per unit of performance.   That is, the A19 Pro on fp has skewed towards the flatter part of the perf / W curve.   >And I really doubt you'll be seeing 10W of ST power consumption on a phone. Geekerwan tests these things using Liquid Nitrogen. They are to test an SoC without restraints.  I agree it's rare, but why would Apple allow 10W? Were many workloads ***lacking*** fp performance that users prefer a bit less battery life for +9% fp perf vs the A18 Pro?  Of course, to most, battery life is more important, IMO, which is why core energy is most crucial, but missing here.  //  >An A19 Pro P core does seem to have increased performance at the same power by about 5%. Only P/W at peak has reduced. Which isn't even an issue when these phones would never reach that peak in daily use.  So the question becomes: do users want slightly better perf and 5% more power? On a phone, I'm of the opinion that power is paramount and should be forced to lower levels.",Neutral
Intel,">Aren’t they exactly the same p cores?  Definitely not. See the SPEC perf / GHz: A19 is nearly just the A18 Pro. Thus, this seems to be the final picture:  A19 Pro = new uarch, 32MB SLC, 12GB LPDDR5X-9600  A19 = binned OC of last year's A1**8** Pro, w/ faster LPDDR5X-8533, but smaller SLC (12MB)  New uArch clearly didn't pan out as expected in fp efficiency. A19 Pro may sit at a flatter part of the freq / power curve, A19 Pro may have more leakage, A19 Pro's [faster RAM](https://en.wikipedia.org/wiki/Apple_A19) may eat *a lot* power (Geekerwant tests mainboard power, not purely CPU power), etc.",Neutral
Intel,I'm sorry you had to take time away from ripping off others' content to correct my mistake,Negative
Intel,">I think AMD will have a hard time to win any efficiency crowns, but historically speaking they always had the peak ST performance on process node. Of course this is not as impressive because those are desktop chips which draw 100W+, and there have been historically gaps between desktop and laptop chips in ST for AMD (even though laptop technically had the TDP to sustain ST).  M2 and Zen 4 launched around the same period. The desktop chips score around 5-10% faster in Geekbench while using 20W per core power and 30-40W more for the I/O die. Taking the ST crown by a hair's width while using 5-10x more power isn't a win at all imo.",Neutral
Intel,">Considering Zen 5 is designed to clock way higher, I don't think it's that bad really.   I'd agree if they performed as well as their clocks suggest. Whats the point of clocking to 5.7Ghz if a mobile CPU clocked at 4.4Ghz leads you in absolute performance by 15% (Geekbench) while using a tenth of the total power.",Neutral
Intel,This snapdragon era is Microsoft's third time trying arm (windows rt and surface pro x). Hopefully third times the charm,Positive
Intel,Their E-core is faster than something like Tiger lake per clock. It’s about 60-70% as fast as the best desktop CPU from 2020 and probably as fast as a U-series chip from then under sustained loads.  The real takeaway for me is that the rumored A19 MacBook is going to dominate the $600 laptop market segment if it releases before Christmas.,Positive
Intel,"Weirdly enough, I feel like them not letting the E core balloon in size has helped them in the long run. Seems they're focussed on maximizing performance within a limited area. Their E cores have had exceptional jumps in performance since the A12 almost every generation being 25%+ in improvements.  I'd wager that E core dominance is the primary reason why the iPhones are able to match Android manufacturers using super large and dense batteries in terms of endurance.",Positive
Intel,Doesn't mac studio already 500gb vs nvidia workstations of 96gbs?,Neutral
Intel,NVIDIA currently fist pumping the air over their failed ARM acquisition rn,Negative
Intel,Its a bit unlikely. Maybe for a version of the M series dedicated for a Mac Pro. But one of the main reasons they can get away with this design is because its very scalable. All the way from A series to Mx Max series. Adding HBM would probably require a dedicated SoC redesign for a very niche product segment in Macs.,Neutral
Intel,The M4 Max already uses a 512 bit bus. Does it not?,Neutral
Intel,Its technically not a true 9 wide core. I think its 3+3+3.,Neutral
Intel,E-Core is relative. Skymont is more of a C-core (area optimized) than what we typically think of as an E-core (energy optimized).,Neutral
Intel,with a viable width ISA it is better to look at the typcile throughput not the peak throuput as you very rarely are able to decode 8 instructions per clock cycle.,Neutral
Intel,"With the 9070? I don’t think I’ve seen any results showing that either, however all I’ve looked at is the blender benchmark charts",Neutral
Intel,"the shader re-ordering is different. (apple also do this).  Even with shader re-ordering you have the issue that you're still issuing 1000s of very small jobs.    GPUs cant do lots of small jobs, they are designed to do the same task to 1000s of pixels all at once.     If  you instead give them 1000 tasks were each pixel does something differnt the GPU cant run that all at the same time... in addition the overhead for setup and teardown of each of these adds even more void space between them.   So apple are doing a hybrid approach, for large groups of function calls they do re-ordering (like NV) but for the functions were there is not enough work to justify a seperate dispatch they do function calling.    This is were dynamic coaching jumpstart in.      Typical when you compile your shader for the GPU the driver figures out the widest point within that shader (the point in time were it need the most FP64 units at once, and register count).  Using this it figures out how to spread the shader out over the GPU.  Eg a given shader might need at its peak 30 floating pointer registers.   But each GPU core (SM) might only have 100 registers so the driver can only run 3 copies of that shader per core/SM at any one time.   If you have a shader with lots of branching logic (like function calls to other embedded shaders) the driver typically needs to figure out the absolute max for registers and fp units etc.  (the worst permutation of function branches that could have been taken).  Often this results in a very large occupancy footprint that means only a very small number of instances of this shader can run at once on your GPU.  But in realty since most of these branches are optional when running it will never use all these resources.  The dynamic cache system apple has been building is all about enabling these reassures to be provided to shaders at runtime in a smarter way so that you can run these supper large shader blocks with high occupancy as the registers and local memory needed can be dynamically allocated to each thread depending on the branch it takes",Neutral
Intel,"While RT does have lots of branching logic (and CPUs are much better at dealign with this) you also want to shader the result when a ray intercepts, and this is stuff GPUs are rather good at (if enough rays hit that martial)   We have had CPU RT for a long time and so long as you constrain the martial space a little GPUs these days, even with all the efficiency  loss are still better at it.   (there are still high end films that opt for final render on cpu as it gives them more flexibility in the shaders they use) but for a game were it is all about fudging it GPUs are orders of magnitude faster, you just have so much more fp compute throughput on the GPU even if it is running at 20% utilization that is still way faster than any cpu.",Positive
Intel,"Its not just useful for RT but also for a lot of other situations, being able to just call out to functions on the GPU and not pay a HUGE divergence penalty (you still pay some) opens up GPU compute to a load more cases were we currently might not bother.",Positive
Intel,I'm a bit confused. You think lagging 15% behind in P/W the competition for an entire year is not being a generation behind?  ARM themselves have managed only a 15% jump this year. So it will essentially be 2 years before we get an E core that matches the A19 pro. And this is just considering the Xiomi's SoC. Mediatek's and Qualcomm's which dominate the majority of the market lag even further behind.,Negative
Intel,I'd advise against using CPUmonkey as a reliable source. They're known to make up numbers. (Reported M1 Pro/Max Cinebench scores 6 months before they launched based on predictions),Negative
Intel,"I agree to some degree; Geekerwan notes they are testing mainboard power, not core power (if you Google Translate their legend).  For me, I assume all the major power draws on the motherboard *are* contributing to the overall SPEC performance, too.   If the faster LPDDR5X-9600 in the A19 Pro eats more power, it's fair to include that power because ***all*** A19 Pros will ship with LPDDR5X-9600. That was Apple's choice to upgrade to this faster memory.    Now, you're very right: we can't purely blame the uArch at Apple. It may well be the DRAM or the boost algorithms (like we saw at the A18 Pro launch last year) and—at Apple specifically—even the marketing overlords.  It's also why I'm a big proponent of JEDEC speeds & timings & volts in desktop CPU tests, much to the chagrin of a few commenters.",Neutral
Intel,Why is separating p-core power usage from SOC power uniquely difficult on iOS?,Negative
Intel,I understand your reasonings. But its the only semblance of comparison we have to date between different SoC. I've learned not to look a gift horse in the mouth.,Neutral
Intel,">Apple has leaped +**57%** average fp power in two years. Seems like not a good compromise when you're eating more power per unit of performance.   >That is, the A19 Pro on fp has skewed towards the flatter part of the perf / W curve.   I mean this has been a trend long before the A17. Apple has been increasing peak power little by little since the A12 Bionic.  I remember reading Anandtech articles about it.",Neutral
Intel,Don't be a sour puss now because you didn't check your sources before commenting. Mistakes happen.  >ripping off others' content   Eh? Are you stupid? You're pissed that someone posted a hardware review ON A hardware sub? The entire sub exists to discuss hardware bozo.,Negative
Intel,"While the efficiency for this is BAD, I dont think its 20W per core.  When we look at results by [Phoronix](https://www.phoronix.com/review/amd-zen4-zen4c-scaling/2) we can see \~7-8W per core for this (not great numbers, because its a weird chip and different node), which is still very bad. AMD certainly has some power issues, but many of which, i.e. inefficient I/O dies, are not really dependent on the CPU uArch and could switch at any moment. They certainly have much more inefficient chips at the moment than both Apple and Qualcomm. For Zen 6 we expect a major update to the desktop chiplet architectur which could bring some much needed improvements in terms of I/O though.  They have reasonably fast cores, and I think they are not in a terrible position, even though it is far from good. I think what is interesting for AMD to look out for is that they keep moving fast, instead of intel who didnt move fast since like 14nm, and AMD has strong cores. Additionally AMD has (including from the datacenter) an enterprise need to make the CPUs more efficient.  So yeah, very bad CPUs efficiency wise. A bit behind, but not terribly on perf per node wise, efficiency on the desktop is an afterthought for AMD, clearly, but they are moving constantly and are improving. It might be AMD Laptop Zen 6 has again like 35W TDP, for 3000 Geekbench and be dead, but with some client oriented tweaks I see chances (maybe just from the patterns in the tea leaves in my mug)",Negative
Intel,"I originally saw this on Phoronix' forums, but I can't find the link to the comment so I'll send this one instead: [https://blog.hjc.im/wp-content/uploads/2025/05/SPECint2017.png](https://blog.hjc.im/wp-content/uploads/2025/05/SPECint2017.png)  Zen 5 is now behind, yes, but it isn't really that bad.",Neutral
Intel,"Per clock, the A19 Pro E core is competitive with Golden Cove. Atleast in SPECint.  M4 E core-> 3.53 points at 2.88Ghz (1.23 p/Ghz) or IPC i9 14900k-> 9.93 points at 6.00Ghz (1.65 p/Ghz) or IPC  A19 Pro has a 22% jump in IPC in SPECint (might be some variations due to time difference and lack of knowledge about the subtests ran, but still gives a good picture)  22% IPC jump over M4/A18 E core = 122% of 1.23 = 1.50 A19 P core = 1.50 p/Ghz  Golden Cove in i9 14900K has a mere 10% lead over A19 Pro in perf/clock in SPECint.  https://youtu.be/EbDPvcbilCs?feature=shared  Source: I9 14900K compared with M4 in this review.",Neutral
Intel,It does but the Macs are limited in other ways (memory speed among other things),Neutral
Intel,It wasn't for a lack of trying.,Negative
Intel,Yeah there was some rumors of a server version and thy have a patent for a multi level cache setup but they also patent and prototype plenty of things that never get released.   https://www.techpowerup.com/277760/apple-patents-multi-level-hybrid-memory-subsystem,Neutral
Intel,"Oh huh, it does but it’s 128 bit per channel. So memory on 4 sides of the die. Wild, don’t see that normally except in FPGA for data logging (or GPUs)",Neutral
Intel,"The difference seems a bit drastic in open data benchmarks.   https://youtu.be/B528kGH_xww?feature=shared  Testing individual scenes, the 9070xt and M4 Max seem neck and neck.  The M4 Max at best (in Lone Monk) is 5070 desktop class and at worst (in Scanlands) is 4060 desktop class. On average, I'd say in Blender, it is neck and neck with an RTX 4060Ti desktop card. I think a theoretical M5 Max should be on par with a 5070Ti if we see the same 60% bump in RT performance.",Neutral
Intel,Apparently Cinebench 2024 GPU is not compatible with RDNA4 cards lol. So I can't find any scores to compare.,Negative
Intel,"So does dynamic caching ensure that the total size will ""always"" be the same as whats being called? As in certain cases it is still possible that there can be wastage like for the example you said ""Eg a given shader might need at its peak 30 floating pointer registers. But each GPU core (SM) might only have 100 registers so the driver can only run 3 copies of that shader per core/SM at any one time."" on that, there would be 10 registers wasted doing nothing, if it cant find any else thats <10 registers to fit in that.",Neutral
Intel,Yeah you're right. Should've said branchy or low occupancy code.  Combining such a HW side change with change on SW side with GPU work graphs API could indeed open up many usecases and new possibilities that are well beyond the current way of doing things. I can't wait to see what Apple does when Work Graphs arrives in a future Metal release.,Positive
Intel,"Well when you put it like that (13-14% behind in efficiency) saying ""generations behind"" certainly sounds misleading.  All it would take for another arm vendor to beat that is jumping one node(let) earlier than Apple, which is certainly doable given the lower volumes, although traditionally it rarely happens. Or jumping earlier to anything that might give them an advantage really, e.g. lpddr6.",Negative
Intel,"It's not ""generation**s**"" behind as you originally wrote. It's being compared to cores from **a year ago** already, mate.  I fully expect MediaTek to adopt C1-Pro and Qualcomm for sure will also update.  Apple's E-cores are simply nowhere near as dominant as they used to be in the A55 era.  EDIT: before we speculate more using marketing for Arm and hard results for Apple, let's check back in a few months to see how C1-Pro  *actually* performs and how Qualcomm's smaller cores actually perform.",Neutral
Intel,Its a bit harder since we don't have any A19 Pro die shots yet. But Apple's E cores have always been sub 1mm2 compared to A7xx cores.,Neutral
Intel,Yeah. All fair points. I don’t disagree. It’s still fun to speculate!,Positive
Intel,"Because we don’t have the tools. On macOS Apple provides powermetrics but Apple states that the figures can’t necessarily be relied on. On some very specific tests you can narrow down power to a cpu core, kinda. Spec tests often stress other aspects like memory, so I would use the provided figures as a guide. Either as a “p core bad” or “p core good” conclusion.",Neutral
Intel,"Oh for sure. It’s not a criticism of either Geekerwan or yourself. They are doing a great job with the available options and I appreciate your summary. I just find it a little amusing when people dissect milliwatt differences as absolutely accurate. We just don’t have the tools, and people are keen to jump on the “p core doomed” bandwagon.",Positive
Intel,"The power increaseas are definitively (and definitely) accelerating, though, worse than it used to be.  [**Geekerwan P-core power**](https://youtu.be/iSCTlB1dhO0?t=422) **| SPECint2017 floating point**  *% is from the previous generation*  A14: 5.54W  A15: 5.54W (+0% power YoY)  A16: 6.06W (+9% power YoY)  A17 Pro: 6.74W (+11% power YoY)  A18 Pro: 8.18 W (+21% power YoY)  A19 Pro: 10.07 W (+23% power YoY)  //  AnandTech did the *great* work of measuring energy consumption / joules. That really proved that race to idle was working; more instanteous power under load, less overall energy   [**AnandTech P-core Power | SPECint2017 floating point**](https://web.archive.org/web/20240907062349/https://www.anandtech.com/Show/Index/16983?cPage=15&all=False&sort=0&page=2&slug=the-apple-a15-soc-performance-review-faster-more-efficient)  A14: 4.72W, 6,753 joules  A15: 4.77W, 6,043 joules (+1% power YoY, -11% energy YoY)  Average power went up, but energy consumption went down.",Negative
Intel,You copied and pasted someone else's data and now you're acting like a hero,Negative
Intel,"Still very bad used to work in their IO team, and the idle eatt performance is still very bad. Their idle core loading is also not as good as I would expect for their next generation, and its sad to see the future generation lose to even on M3 on benchmarks. The LP core was supposed to help in this situation, but I just can't see past the fact on why the gap is actually widening over time. I hope Intel can come in and close this gap.",Negative
Intel,"The 9950x lags by 8% behind the M4. M5 is another 10% on top of this lead. M series chips use around 8W of power in total to achieve this perf including memory and what not. 9950x is like 20W per core, another 40W for the I/O and another unknown amount for mempry.",Neutral
Intel,I was comparing overall score.  Geekerwan got 4.17 at 2.58GHz which is ~1.62pt/GHz which is a little higher than your speculation and a mere 1.8% IPC difference from that Golden Cove number.  The fact that it did this at 0.64w in such a tiny core is absolutely incredible. It once again raises the question about how much ISA matters. I don't see any x86 E/C-cores getting anywhere close to those numbers.  https://youtu.be/Y9SwluJ9qPI?t=260,Positive
Intel,one will think the massive vram capacity just override the disadvantages.,Neutral
Intel,"dynamic caching would let more copies of the shader run given that is knows the chances that every copy hits that point were it needs 30 registers is very low.   If that happens then one of those threads is then stalled but the other thing it can do is dynamicly at runtime convert cache, and thread local memroy to registers and vice versa.   So what will happen first is some data will be evicted from cache and those bits will be used as registers.   maybe that shader has a typical width of just 5 registers and only in some strange edge case goes all the way up to 30.   With a width of 5 it can run 20 copies on a GPU core that has a peak 100 registers.",Neutral
Intel,"Metal has supported GPU side dispatch for a long time, (long before work graphs were a thing in DX) Barries and fences in metal are used by the GPU to create a dependency graph between passes and this is resolved GPU side (not CPU side).   I don't see the explicit need for some extra feature here as we already have \*and have had for a long time since metal 2.1\*",Neutral
Intel,"Historically, the core only area for Apple's E cores have always been between Arm's A5xx & A7xx cores, but closer to Arm's A5xx  But for core+L2 (using sL2/4), Apple's E cores have always been similar to A7xx cores in mid config",Neutral
Intel,"I agree that the presence of inaccuracies is very likely. And I certainly don't think the P core is doomed for a 10% jump in what is essentially a a very minor node upgrade.  But considering the video does go into the P core's architecture where the only substantial changes were the size of the Reorder Buffer and a marginally better branch predictor, the performance numbers make sense.",Neutral
Intel,This is a combination of the meaningless smartphone benchmark game (95% of users would be perfectly fine with 6 of Apple's latest E-cores) and the need to have a powerful chip in laptops/desktops all sharing the same core design.,Neutral
Intel,"I'm confused. And legitimately concerned about your mental faculties.  Literally in your previous comment, you posted Tom's Guide data, data thats not ""yours"" to try and discredit my post. And checking your post history, you also posted a Mrwhosetheboss video to discuss battery life comparison in an another subReddit.   So are you a hypocrite since you're ""stealing"" data as well? I'm not stealing anyone's data. I'm correcting your stupidly incorrect conclusion with a source to back it up. Just like you attempted to lol.",Negative
Intel,"You see 7-8W increase going from 1 Core to 2 Core, which indicates that AMD has a huge base overhead but the core doesn't use 20W.   So its a problem with the SoC design not with the CPU design that it uses 20W for ST.  Its more like running a ST workload draws 20W SoC + 8W CPU stuff. So if they had better SoC design they could substantially boost efficiency even with the same cores.",Neutral
Intel,"Comparing the 9950X to the M4/M5 is a bit of a stretch... I'm not saying AMD is as good, but if they did a ""Lunar Lake"" with Zen 5C + on package memory they wouldn't really be that far off.  I want x86 to belong in the museum too, but sadly the ISA doesn't really matter (that much) and AMD isn't exactly incompetent... EPYC CPUs are still dominant and this is what they're truly targeting...",Neutral
Intel,"Oh I was referring to this statement in your previous comment.  >Their E-core is faster than something like Tiger lake per clock.  By ""per clock"" I assumed you mentioned IPC.",Neutral
Intel,"I wouldn't recommend comparing IPC from two different runs of SPEC2017. There could be numerous different changes or subtests we don't know about. For eg, compare IPC numbers here vs the test used in A18 review. There are some notable differences in scores and IPC.",Neutral
Intel,Oh it's a huge benefit (I have a M2 Ultra at work) but we still use Nvidia. The cuda ecosystem is far more mature and widely supported with better support for embedded and datacenter scale compute.,Positive
Intel,"I see, so dynamic caching can make it so a shader doesnt have to be 30 registers wide if it doesnt have to do 30 often so it doesnt have to reserve that much space and waste it(such as in conventional cases, if its 5 registers and 30 peak, it will still reserve 30 registers despite it being at 5, which then would waste 25 doing nothing)  Also SER happens first right?",Neutral
Intel,Impressive and thanks for enlightening me.,Positive
Intel,I don’t disagree. The performance figures seem good. The power figures may or may not be. I’m just nitpicking.  Edit: just noticed that they show the A19 having 99% P-core FP performance path 11% less power. That is weird and get’s to my point about power measurement strangeness.,Neutral
Intel,"The needlessly high clocks, agreed: Apple could've still improved perf with lower clocks.  >the need to have a powerful chip in laptops/desktops all sharing the same core design.  They previously kept this in check on the A16 and even A17 Pro, both sharing the same core designs as the M2 and M3 respectively. That doesn't seem too related, as every uArch should scale a few hundred MHz either down or up.",Neutral
Intel,I posted links to reviews. I didn't copy the entire data for my own post,Neutral
Intel,"ISA matters much as well, or mostly the implementation of it. X86 and AMD dont go well together, and AMD is the very definition of incompetency. Both deserve to be sunseted by now. The fact that there is no proper ARM support on consumer platform is the only reason why X86 on consumer still exists. For servers, an ARM server is more power efficient, and only those really legacy stuff requires X86. Companies would really appreciate the cost savings and the ARM ecosystem more than the clusterfk of X86.",Negative
Intel,"Yeah, I guess that wasn't clear. What I meant is that the IPC of the E-core is so much higher than Tiger Lake that it's performance at 2.6GHz is surprisingly close to desktop Tiger Lake at 5GHz.  I know it's not fair to compare with Intel 10nm (later Intel 7) with TSMC N3P as it's 2 major nodes apart, but this goes way beyond that because these chips are using only around 2.5w for just the E-cores. TSMC claimed 25-30% power savings from N7 to N5 and another 30-35% from N5 to N3.  Using these numbers, we get these 4 E-cores using somewhere around 6-7w PEAK, but this is all-core turbo. Intel's CPU at it's most efficient 12w configuration won't hit it's 4.4GHz turbo required to win in single-core performance and is going to hit closer to its 1.2GHz base clock in all-core sustained performance at which point the E-cores not only have their giant IPC advantage, but double the clockspeed too.  All this again to make the point that a macbook with this will be blazing fast for consumer workloads and might finally be the laptop to usher in multi-day battery life.",Neutral
Intel,"This is true, but there aren't a ton of alternatives short of taking a ton of time to compile/run everything myself. I don't think it's worth it for a hypothetical IPC comparison that it sees nobody else would even be that interested in seeing.",Negative
Intel,"Reordering of shaders has a cost, if for a given martial you just hit 10 rays you will not want to dispatch that shader with just 10 instances as the cost of dispatch and scdulaing will be higher than just inlining the evaluation, so you will merge together the low frequency hits into a single wave were you then use branching/fuction point calls.    You will also use this mixed martial uber shader to use up all the dregs that do not fit within a SMD group.      Eg you might have 104 rays hit a martial but that martial shader can only fit 96 threads into a SIMD group so has 8 remaining thread, you don't want to just dispatch these on there own as that will have very poor occupancy (with 88 threads worth of compute ideal) so you instead inline them within a uber shader along with a load of other overflow.",Neutral
Intel,"Are you perhaps blind? The youtube link to the review is at the top of the post? What is wrong with you friend? Feeling a bit under the weather?  The video is in Chinese and before I updated the post with a youtube link, the previous source was from Bilbili, a platform that doesn't even work in most countries.   So I summarised the important points in it for people who didn't understand Chinese and couldn't infer anything from the graphs. No one's gonna ignore the link that is at the top of the post and read my summary before seeing that a link is available.   You're fighting imaginary demons here. Go to your samsung subreddit and whine about Geekbench or something.",Negative
Intel,"But we were discussing CPU uArches, and on the question of ""Does AMDs single core suck up 31W"" the answer is ""NO"", the SoC power is 31W, and that is an issue that can be solved independently of the CPU uArch.  This just means that the massive inefficiencies are not inherint to the CPU uArch, but are a Problem of SoC design.  7W on a single core isnt super efficient, but its far lower than 20-31W. The question is if AMD is able to strip away a large part of those ~20W overhead, but that is not contingent on questions about CPU uArches.  And thats the point of the 1 Core vs 2 Core comparison. To demonstrate that it isnt the CPU that sucks up 20W, but the SoC, which can be solved much differently.",Negative
Intel,Enjoy your scratch-prone iPhone!,Positive
Intel,Your one brain cell is trying really hard. See if you can wake up its buddy.,Negative
Intel,Excellent bang for buck with drivers getting better and better.,Positive
Intel,They're good if it's a decent bit cheaper than the 9060 xt.  You also need a fast CPU since the b580 scales with how good/bad your CPU is.,Positive
Intel,"Are you buying brand new or used? If you are considering new, I would suggest going used instead as you can get a better card for the same amount of money as a new B580.",Neutral
Intel,I mean it's alright. It's got 12GB of VRAM which is cool I guess but XeSS is not that great compared to DLSS 4 and FSR 4 these days and the performance is slower than a 5060 or 9060 XT 8GB.,Negative
Intel,"I have a B580 linked up to a 4K TV. The only quirk I've really noticed is that changes in resolution, refresh rate or HDR on/off will occasionally cause the TV to hard reboot. I'm not sure if this would happen with a regular PC monitor, but this never happened with the previous GTX 750 Ti I had.  Otherwise performance in games is as expected from reviews. I don't play AAA games in 4K with it, but haven't had crashing issues that some people report.",Neutral
Intel,"with how bad GPU pricing is slated to be due to the memory shortages, I'm starting to consider a B580 so I can at least have a GPU that should be able to play BeamNG in 3440x1440. not keen on having to replace it within less than three years though, but maybe I'm severely overestimating how much GPU I actually need in a realistic gaming scenario. the used market here has been rather dodgy as of late.",Negative
Intel,"I'm happy with my B580. It runs games in 1440p with frame generation, was the best option for this price.  Come visit r/IntelArc to learn more.",Positive
Intel,"That's their problem. Intel cards are fine, but AMD cards are priced almost the same and they are more reliable.",Neutral
Intel,i never buy used hardware. it doesn't feel good,Negative
Intel,"don't know what op's monitor is. if op is going to connect the pc to a tv, tv usually has upscaling function and we do not need the upscaling algorithm in the gpu",Neutral
Intel,"B580 changed everything.... For the six months until AMD released 9060XT.  I hope they keep making GPUs tho. B580 is seriously impressive for the price, but they've got so much catching up to do.",Positive
Intel,"Agree, the only high ticket items i buy used are things i can physically fix myself such as cars and bikes",Neutral
Intel,"You are incredibly misinformed. TV upscalers are nothing like DLSS and FSR.  A TV's upscaler really doesn't work all that well in gaming one because they generally don't really make games running at 1080p look like 4K since that's what most TVs are nowadays and this is coming from someone with a 4K Samsung.  Also DLSS and FSR improve performance, reduce motion blur, give better anti aliasing than regular TAA all of which a TV upscaler doesn't do.   Also TV upscalers introduce extra processing lag from the upscaling which DLSS and FSR straight up lower latency because of the performance uplift.",Negative
Intel,"It's technically slower than even the 9060 XT 8GB which is usually around the same price give or take 20-30 bucks. I wouldn't call it ""seriously impressive for the price"" at all it's only saving grace is having 12GB of VRAM but it's got iffy drivers, cpu overhead and a worse upscaler.  It's not impressive at all. It's decent at best. It's cool that it has 12GB of VRAM and that might matter in a handful of games at 1080p at maxed out settings or matter more in 1440p but it really lacks reasonable performance to play the latest games in 1440p without relying on upscaling which it has been surpassed by in FSR4",Negative
Intel,"oh thank you so much for the info. i'd look into that. i'm using a samsung 65"" 4k tv and i usually output 1440p from the gpu via hdmi to the tv and let the tv do the upscaling thing. i'd try enabling the upscaling in gpu and output 4k to the tv and see if i could spot the difference",Positive
Intel,>around the same price give or take 20-30 bucks.  Not the case at all where I live lmao. More like a 100 euro difference between a piece of shit 8gb vram card and the B580.,Negative
Intel,Cheapest b580s are 250 here. Cheap 9060 XT 8GBs are like 270. The b580 is pointless imo.,Negative
Intel,"Cheapest B580s are 250 here, the cheapest 9060XT 8gbs are 340. Ergo, B580 is not pointless. Simple stuff.",Neutral
Intel,It's pretty pointless to pay 20 extra less bucks for a card that performs worse.,Negative
Intel,"Arc B580 is a good budget card and leagues better then a 3050, just note it will probably struggle on high settings on 1440p. On tomshardware it scored 42.6 FPS on average for 1440p Ultra Rast which included God of War Ragnarök in there game selection.   If you want to play high settings consistently without frame gen at 1440p, I would spend the extra $100 for the 9060XT 16GB. It scored 40% higher then the B580, and cost 40% more. So you're getting same value wise, for price to performance 1440p Ultra.  Also note, Intel GPU's are still new and have the most vary performance with time,  so I would actually imagine the B580 is a bit better then the recorded benchmark I'm refrencing from.    [https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html#section-rasterization-gpu-benchmarks-2025](https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html#section-rasterization-gpu-benchmarks-2025)",Positive
Intel,"The B580 will about 90% faster (assuming no CPU bottleneck). But, if you can afford it, the Radeon RX 9060 XT 16GB is about 160% faster and can usually be found for about $350 in the US.",Positive
Intel,"Depends on pricing, the B580 is a decent deal in the UK, you can get it here for £200 and a choice of a £60 game out of four options including BF6 so a no brainer for budget builders. Anymore than £200 or the dollar equivalent id just jump to a 5060 or better yet 9060XT 16gb. There will be some CPU bottleneck, but you can get alot of performance out of itself still.",Positive
Intel,More ram because this is a amazing build,Positive
Intel,"Get fast 32GB DDR4 RAM (or even more). The rest of the build is well balanced and upgrade meaning completely new system (except PSU and storage). The GPU still good, but produce less fps in new games compare to 5070Ti (own both - personal experience).",Positive
Intel,RAM 32GB (16 * 2),Neutral
Intel,"The 7600X doesn't come with a cooler, you have to buy one separately.  Air cooling would be fine for it.",Neutral
Intel,"First time building, so excuse if im asking stupid questions. So I need one cooler for the processor (the one you put on top of it) and thats it?",Neutral
Intel,"Yeah, that is all you would need.  The only other thing would be case fans, but the case you selected comes with those.",Neutral
Intel,Thanks very much,Positive
Intel,"If everything is running fine, maybe not.  150 FPS is usually plenty unless your 1% lows aren’t cutting it.    If you’re on 1080p you’re probably fine to chill where you are for the moment.  I just made a similar upgrade from a 6800, but I’m on 1440p UW with 2.5x the pixels.",Neutral
Intel,150fps is pretty high for arc raiders tbh. You might just be a little overwhelmed,Neutral
Intel,"I mean... If everything you play is running fine, then probably not. I usually play heavier games and the upgrade from 6700 XT to 9070 XT was night and day.",Neutral
Intel,I'm doing it,Neutral
Intel,It would over double your fps,Neutral
Intel,Give me your location in Arc Raiders I may come and steal some FPS from you if you get the 9070 XT. The others can have the pink items and stuff.,Neutral
Intel,"GPU costs expected to rise due to RAM/vram manufacturer shortages. Eventually will go back down, but no telling when.   Is AMD planning any future GPUs besides the refreshes that came out, or focusing on console and redstone?",Negative
Intel,I would. People in the industry are saying there is about to be a GPU shortage. It’s really good right now. I reckon in 12 months when you want to again you’ll be kicking yourself you didn’t do it sooner when it was below MSRP.,Positive
Intel,"My wife's PC runs a 6750xt and I have a 9060xt.   It's a decent upgrade for the technology, running FSR 3 and 4 on newer games. That being said I don't think it's a big enough jump to justify the cost unless you're really wanting to run newer games at higher resolutions. Best friend has the 9070xt and it's nice but the 6750 does fine for now especially at 1080p",Positive
Intel,What is the refresh rate on your monitor?,Neutral
Intel,I would say wait for next gen next year or then buy 9070 to a more reduced price(though the impact of ram prices are difficult to guess on),Neutral
Intel,Well if you do you can always run the 6700XT with /r/losslessscaling for maximum frame gen.  https://docs.google.com/spreadsheets/u/0/d/17MIWgCOcvIbezflIzTVX0yfMiPA_nQtHroeXB1eXEfI/htmlview#gid=1980287470,Neutral
Intel,Yes,Positive
Intel,"I have a 6800 XT I said when there's about a 50% jump give or take that's when I will upgrade the fact that you can now get it at an MSRP $629 not bad!   I don't give a rat's ass about Ray tracing so to me I'll be running everything at 4K Max except no Ray tracing, except for the games that are well not optimized well going to try to get the max out of everything.   Will be selling my 6800 XT it's a gigabyte it's beautiful, I know people don't like gigabyte but the gigabyte 6800 XT windforce OC edition to me has run flawlessly never an issue.  Then again I don't play much, so basically whoever does get it is getting it with a box and everything pretty much like new.   But I will see if the 9070 XT is worth the upgrade before I sell my card.   From everything I read with opti-scaler fsr4 and just overall being much faster I think I'll be okay for many many years.   I don't know how good redstone's going to be and how much it's going to be put into many games but let's hope there's some leak like last time where it goes Open source by mistake and then the community can take advantage of it!   I just pulled the trigger because I don't think price is going to come back down for a while once stock is gone.   The memory prices of GPU are going to skyrocket they already are so I figured let me get in before there's nothing left.   I've always believed that if you don't really care about Ray tracing AMD is a better buy.   To me Ray tracing has always ruined all of gaming since it came out with the 2000 series.   Before we were getting great quality now you could have a great card even if you have a top tier you throw Ray tracing on the hit is not worth it on top of a poorly optimized game makes it even worse.   I think a 50% increase overall probably even higher if you add fsr4 and the optiscaler definitely worth the purchase especially if you're going to sell your older card probably will wind up costing me about $350 bucks Max.",Positive
Intel,I'd save the money and wait. Upgrade when it's a WOW jump,Positive
Intel,"I did from 6800 XT to 9070 XT and it was worth it.  Also if you can upgrade, upgrade now. Next year a 9070 XT will be $3,000.",Positive
Intel,"Snap, did the same and also have 1440p UW :) great experience so far.",Positive
Intel,Overwhelmed? What do you mean?,Neutral
Intel,I'm also doing it (and sticking it in my pcie3 motherboard at that),Neutral
Intel,240hz,Neutral
Intel,it’s almost 2x faster,Positive
Intel,You’re overwhelmed on how much fps other GPUs can give.,Neutral
Intel,"That part was weird, but 150 fps is a high refresh rate. What's your cpu? If you don't have a day enough cpu the upgrade will underperform, but ideally approximately doubling performance",Neutral
Intel,Might be worth it. I run a 144hz 1440p monitor and a 9070xt will max that on Arc Raiders,Positive
Intel,Wait til 4x faster for that extra wow and money saved. 6700xt is still plenty,Neutral
Intel,"I didn't understand that part either lol. I get 150 FPS in game, but my monitor's refresh rate is 240hz.  Ryzen 7 5700X3D CPU",Neutral
Intel,that’s not how generational increases in performance are anymore  you’d have to wait like 8 years to see that level of raster for 600$,Negative
Intel,The graphics card upgrade should get you to the 240 fps mark in Arc Raiders with your setup,Positive
Intel,8 years sounds about right for an upgrade then,Positive
Intel,"If want to wish for an upgrade for 3 years, sure.",Neutral
Intel,"It all depend on your need, in my case i went for the b580. Got it for 350$ cad plus they gave me bf6 ( i only play battlefield ) worth 105$ cad tax in. Got over 150 fps with Fg on at bf6, otherwise i would have buy a 5060. No reason to buy 4060 as the 5060 is about 100$ over.",Neutral
Intel,"Personally Id go B580, higher vram and the drivers have gotten alot better but also be sure to enable resize bar in bios. Its recommended for any gpu but the b580 performance is much better with it on.",Positive
Intel,i'm using 225f + b580. work fine. intel + intel combo makes me feel safe,Positive
Intel,"By far, get a new GPU.",Neutral
Intel,"The 12400f is still a decent cpu for gaming, a 9070xt would be a good upgrade now, you can update the platform (cpu,ram,and mb) later and still carry the card over a year from now",Positive
Intel,9070xt if you're going to update your cpu later.,Neutral
Intel,GPU. That's nothing really wrong with your CPU and the next two Intel gens have a ton of problems. If you're using it mostly for gaming you'll get much more out of a 9070 XT than from a CPU upgrade depending on what resolution you will be playing at. At 1440p the a770 is fine but still relatively slow and its only real good thing is the 16GB of VRAM. The 9070 XT has twice the performance with better drivers and features. At 1440p you also won't have any CPU bottlenecking with your current one but you likely won't see much difference in performance if you keep your GPU and upgrade your CPU. That all being said this applies to only gaming and at 1440p or higher. If you do a lot of multitasking and content creation you may benefit from an i7 or i9 with a higher core count.,Neutral
Intel,Intel i5 what? There are a lot of i5's.,Neutral
Intel,"Need to know what motherboard you have, i5 is on every socket made in the last 10 years at a guess, so doesn't tell us anything. Odds are though if you're looking at a 9070XT, you'll need a whole new system. Just a guess that it will all be old.",Neutral
Intel,Thanks for this! First time upgrading so I wanted to be sure I made the right choice.,Positive
Intel,Thanks for great response! Will I still be limited to 1440p with the 9070xt and my current CPU?,Positive
Intel,"Sorry, 12400f",Neutral
Intel,"i5-12400f, sorry",Negative
Intel,Not at all. CPU bottlenecking is an issue with lower resolution(depending on the game) because your CPU is actually generating the frames while your GPU renders the graphics. At low resolution your GPU can easily render frames so the bottleneck is with the CPU. Conversely the GPU is a bottleneck at higher resolution. The higher the resolution the less frames the GPU has the ability to render so the CPU has less work.   So basically right now you're being bottlenecked by a slow GPU. Your CPU has decent boost clock speeds so you would only see issues in very CPU dependent games and only at lower resolution (1080p or less). The 9070 XT is a perfect 1440p card. In rasterization performance it should be able to play any 1440p game at Max settings and ray tracing 120+fps and it can play 4k with high frame rates using FSR.   In situations where you're trying to decide what to upgrade it really comes down to what you're trying to accomplish. If you're gaming at 1080p you have no need to upgrade anything. If you're 1080p gaming and using your PC for content creation or CPU intensive workloads upgrade your CPU. If you're gaming at 1440p and above upgrade your GPU. If you're gaming at high resolution and doing CPU intensive workloads then upgrade both.,Neutral
Intel,Then your GPU is definitely the weak link.,Negative
Intel,"This is really helpful, thank you for this! Upgrading for the first time and I really needed this info.",Positive
Intel,"I feel like most people would want to run fsr4 quality even on 1440p. The cpu would giga bottleneck in that scenario and even on 1440p native max no rt, my 12600k on ddr4 was holding back my 9070xt on cyberpunk 2077 (not the newest game I know). And on psycho raytracing and fsr4 quality, it also bottlenecked. The 12400f will likely be on ddr4 as well.  But regardless, it's huge upgrade and if OP plans to upgrade platform, there will be another upgrade that will be felt. No system is ever going to be perfectly balanced especially if you play a wide variety of games so it's a solid decision.",Neutral
Intel,"I kind of thought so, but I’m not too knowledgeable on this stuff and wanted to be sure I was making the right choice. After getting the 9070xt, the CPU will be next. Is there anything I should be worried about if I pair it with my current CPU?",Neutral
Intel,"No problem and good luck! Last bit of advice. I would switch to AMD rather than just upgrade to another Intel when you do upgrade your CPU. No matter what you're going to need to upgrade your Mobo and RAM anyway unless you just get an i7 or i9 of the same generation which I wouldn't recommend. If your main PC use is gaming you would benefit a lot more from a 9600X and 32GB of DDR5-6000 than say a 14600k. Not only have 13th and 14th Gen Intel chips had a ton of problems but they're power hungry, are expensive, and run hot so you'll need to get a really good air cooler or AIO. AMD also does long term support on their slots with a new AM4 budget CPU having been released last week even though we're two gens into AM5. My second PC that I use for work has a 12700k that I'm happy with but when I upgrade I'm going to get a 9900X3D or whatever is better at the time in AM5.",Positive
Intel,My 5700X3D is also on DDR4 and I don't hit many situations where I'm CPU limited but to be fair that is an X3D chip. I play cyberpunk on Ray tracing ultra with fsr4 quality and I benchmark at average 68-74 fps without frame gen.,Neutral
Intel,What's your power supply?,Neutral
Intel,"Again, I’m barely dipping my feet into these things, but is it worth looking into getting a 3D CPU for the 9070? I’ve seen it being discussed as ana amazing series of processors, but I don’t know if that’s overkill for what I plan on buying. I just want to get the most of 9070 and kind of build around that for now. Hopefully it lasts me a good amount of years.",Neutral
Intel,"I play a lot of games, but I don’t think they’re too demanding. Mostly things like Overwatch, Fallout, Monster Hunter, Expedition 33 and smaller games like Hades or Slay The Spire. Cyberpunk never interested me and I know that game asks for a lot from your PC.",Negative
Intel,"650w, upgrading to a 850w when I get the 9070xt. I think that’s enough",Neutral
Intel,3D chips main benefit is the extra L3 cache directly on the CPU die. It puts super fast memory right in the cpu that improves gameplay performance especially in CPU intensive games. If your goal is to have a CPU you won't need to upgrade for a long time then it's a good idea but they're expensive so it really depends on what you're willing to spend. The whole reason I'm still able to game on AM4 with such good performance is because I have a 3D chip. That being said you absolutely do not need it to get the most of your 9070 XT. A 9600X is a great CPU for gaming with high clocks and it's very reasonably priced especially if you get it in a bundle with Mobo and RAM. The 9800X3D is more than twice the price and is only necessary for people that are seeking the best possible performance you can get on a gaming PC and should probably be paired with a 5080 or 5090.,Positive
Intel,The most demanding of these is probably monster Hunter but yeah none of them are super demanding. I'll need to do some research to see if they're CPU limited games though.,Neutral
Intel,Looks good to me.,Positive
Intel,"Thanks again for the advice, really helps!",Positive
Intel,would have tried to get onto AM5 with this budget especially with how much cheaper the B570 is in relation to the B580 but overall good parts selection especially if you are someone who waits multiple CPU generations before upgrading,Positive
Intel,"You overspent on some parts, with spending smarter you can even build an am5 build with a better gpu for the same price, as am4 parts cost are starting to rise  [PCPartPicker Part List](https://uk.pcpartpicker.com/list/9r47zP)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 7500F 3.7 GHz 6-Core OEM/Tray Processor](https://uk.pcpartpicker.com/product/ms4Zxr/amd-ryzen-5-7500f-37-ghz-6-core-oemtray-processor-100-000000597) | £141.46 @ Amazon UK  **CPU Cooler** | [ID-COOLING SE-214-XT ARGB 68.2 CFM CPU Cooler](https://uk.pcpartpicker.com/product/Zr8bt6/id-cooling-se-214-xt-argb-682-cfm-cpu-cooler-se-214-xt-argb) | £16.98 @ Amazon UK  **Motherboard** | [Gigabyte B650 EAGLE AX ATX AM5 Motherboard](https://uk.pcpartpicker.com/product/CvcgXL/gigabyte-b650-eagle-ax-atx-am5-motherboard-b650-eagle-ax) | £119.99 @ Amazon UK  **Memory** | [Corsair Vengeance 32 GB (2 x 16 GB) DDR5-5600 CL36 Memory](https://uk.pcpartpicker.com/product/dcJgXL/corsair-vengeance-32-gb-2-x-16-gb-ddr5-5600-cl36-memory-cmk32gx5m2b5600z36) | £99.99 @ Currys PC World  **Storage** | [Samsung 990 EVO Plus 1 TB M.2-2280 PCIe 5.0 X2 NVME Solid State Drive](https://uk.pcpartpicker.com/product/XQFmP6/samsung-990-evo-plus-1-tb-m2-2280-pcie-50-x2-nvme-solid-state-drive-mz-v9s1t0bw) | £58.99 @ Amazon UK  **Video Card** | [Sapphire PULSE Radeon RX 9060 XT 16 GB Video Card](https://uk.pcpartpicker.com/product/w2WmP6/sapphire-pulse-radeon-rx-9060-xt-16-gb-video-card-11350-03-20g) | £299.00 @ Amazon UK  **Case** | [Montech XR ATX Mid Tower Case](https://uk.pcpartpicker.com/product/fc88TW/montech-xr-atx-mid-tower-case-xr-b) | £50.04 @ Amazon UK  **Power Supply** | [MSI MAG A650BN 650 W 80+ Bronze Certified ATX Power Supply](https://uk.pcpartpicker.com/product/8LNxFT/msi-mag-a-bn-650-w-80-bronze-certified-atx-power-supply-mag-a650bn) | £42.00 @ Amazon UK   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **£828.45**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-11-21 11:07 GMT+0000 |",Neutral
Intel,Decent 1080p build. For intel gpus make sure resizeable bar is enabled in bios for more frames,Positive
Intel,Yeah not picking up AM5 is the blunder here,Negative
Intel,"Unless you know you'll only play a couple of games and that's it, you need more storage. I wouldn't go lower than 2tb.",Neutral
Intel,Seems like a good AM4 build. Only thing id do is add more storage. Youll burn through 500gb before you even realise,Positive
Intel,i just don't understand why everyone build a pc here with a 3rd party cooler. isn't it the stock cooler enough? op is not going to use a high end powerful beast which generates a lot of heat. op is going to use an entry level cpu which should not be that hot. stock cooloer is enough. don't waste money,Negative
Intel,"it's a good build, the only thing that u can change Is the CPU maybe, but for tour budget that is perfect",Positive
Intel,Just get a console.. you will not have more then 60 fps on games with these parts,Negative
Intel,"Thanks very much mate, I've managed to cancel all but the PSU, case and cpu cooler but will take the hit on these. Will go for what you've recommended above. Much appreciated.",Positive
Intel,Double the performance?,Neutral
Intel,I dissagree,Neutral
Intel,Just say you dont know nothing about pcs,Negative
Intel,"Thats good to hear, have fun building!",Positive
Intel,"The best bang for buck is a 9600x with a 9060xt. Period. The 9600x is giving you near peak cpu performance, the 9060xt (16) is giving you 1440p (and even some limited 4K gaming) performance. Everything else is less important.",Positive
Intel,Agree (with the disagree) Other than The processor and mobo (I can see you've spent there)....I built a AM4 platform for £300. (16Gb ram also) . It games fine.  Why on earth is your ram so expensive.  Why on earth did you get an ARC?,Neutral
Intel,ok then tell me why,Neutral
Intel,Well I'm gonna have to disagree on that,Negative
Intel,"Its because ram prices are so expensive, and the b580 is actually good, but he just overspent on many parts so you cna get something better for the same price",Negative
Intel,"it's hard to find another gpu with 10+gb and fit everything in 850€ budget, about intel arc u have to consider the fact that now they are good gpu's at msrp",Neutral
Intel,"look at my comment, its a much better build for the same price, he just overspent on some parts",Neutral
Intel,"look at my build, i can post it again if you dont want to do that. (its even on am5)   [PCPartPicker Part List](https://uk.pcpartpicker.com/list/9r47zP)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 7500F 3.7 GHz 6-Core OEM/Tray Processor](https://uk.pcpartpicker.com/product/ms4Zxr/amd-ryzen-5-7500f-37-ghz-6-core-oemtray-processor-100-000000597) | £141.46 @ Amazon UK  **CPU Cooler** | [ID-COOLING SE-214-XT ARGB 68.2 CFM CPU Cooler](https://uk.pcpartpicker.com/product/Zr8bt6/id-cooling-se-214-xt-argb-682-cfm-cpu-cooler-se-214-xt-argb) | £16.98 @ Amazon UK  **Motherboard** | [Gigabyte B650 EAGLE AX ATX AM5 Motherboard](https://uk.pcpartpicker.com/product/CvcgXL/gigabyte-b650-eagle-ax-atx-am5-motherboard-b650-eagle-ax) | £119.99 @ Amazon UK  **Memory** | [Corsair Vengeance 32 GB (2 x 16 GB) DDR5-5600 CL36 Memory](https://uk.pcpartpicker.com/product/dcJgXL/corsair-vengeance-32-gb-2-x-16-gb-ddr5-5600-cl36-memory-cmk32gx5m2b5600z36) | £99.99 @ Currys PC World  **Storage** | [Samsung 990 EVO Plus 1 TB M.2-2280 PCIe 5.0 X2 NVME Solid State Drive](https://uk.pcpartpicker.com/product/XQFmP6/samsung-990-evo-plus-1-tb-m2-2280-pcie-50-x2-nvme-solid-state-drive-mz-v9s1t0bw) | £58.99 @ Amazon UK  **Video Card** | [Sapphire PULSE Radeon RX 9060 XT 16 GB Video Card](https://uk.pcpartpicker.com/product/w2WmP6/sapphire-pulse-radeon-rx-9060-xt-16-gb-video-card-11350-03-20g) | £299.00 @ Amazon UK  **Case** | [Montech XR ATX Mid Tower Case](https://uk.pcpartpicker.com/product/fc88TW/montech-xr-atx-mid-tower-case-xr-b) | £50.04 @ Amazon UK  **Power Supply** | [MSI MAG A650BN 650 W 80+ Bronze Certified ATX Power Supply](https://uk.pcpartpicker.com/product/8LNxFT/msi-mag-a-bn-650-w-80-bronze-certified-atx-power-supply-mag-a650bn) | £42.00 @ Amazon UK   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **£828.45**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-11-21 11:07 GMT+0000 |",Neutral
Intel,There has never been a single generational leap anywhere near what you're suggesting.  That's like saying a 5060 gives you double the performance of a 4070,Negative
Intel,yes this build Is Better and believe It or not i was searching for something like this at least he could build an am5 PC,Positive
Intel,With RAM prices I am just buying anything non-RAM related and waiting.,Negative
Intel,"With RAM prices as they are right now it's definitely not the time to be making system upgrades if you don't need to. Upgrade your AM4 system, and hold out for a year or two until RAM prices return to normal.",Negative
Intel,"Without knowing your monitor and what performance you’re looking to get out the PC, really hard to say whether or not it’s worth the upgrade. Your setup currently is more than plenty for RDR2 or competitive Fortnite, but if you’re wanting like 4k high refresh/fps or something, then yeah an upgrade would be necessary.   I’m still on AM4 (R5 5600X, RTX2080 8GB, 16GB DDR4 and it’s plenty capable, but I also only play on a 1080p/120HZ monitor and I can max out every last gen game. It only really struggles (only slightly) with newer poorly optimized unreal engine 5 games.",Neutral
Intel,"I personally wouldnt consider ARC, fornite or Lol to be games that desperately need DDR5’s latest snd greatest. AM4 parts seem like a steal right now (ignoring the crazy prices on new 5800X3D). Like the 5700X, the 5800X, and the 5800XT are not too hard to find at okay new prices. Grab one of those and maybe spring for a 5070 and you’ll be in fighting shape really easily.",Neutral
Intel,"Bro tbh dont bother unless you got money to burn. If anything you can still upgrade the cpu to a 5950x or get oled monitor. Staying one or two generations behind will still allow you to play anything 1440p. I have nearly identical setup you have but with a 5950x and haven’t even bothered to overclock yet because it plays the latest 100fps+. But like I said, if you got money to burn then go am5 but it’ll be marginally better for the increased costs.",Neutral
Intel,"For the games you’re playing, except if you play 4k, you have no need at all to update.it should be all smooth. You can check your task window while gaming and checking if your cpu bottleneck your gpu or not. I’m on am4 with 5800x and 3070, 1440p 144hz monitor and I have no issue playing all games in ultra",Neutral
Intel,Newegg has a deal with some decent ram for free with a like $180 mobo. I’ve been thinking about running that with a 9800x3d on my 3090 till 5090 and ram prices go down.,Positive
Intel,Aim for a 5700x3d. It may be the biggest upgrade without having to buy a completely new system,Positive
Intel,"Since you have access to a Microcenter, and RAM with those bundles is actually not horrible (32GB for $240, $65 less than the same kit at Newegg), then yeah, go get the 7600X3D and do it **soon**, bc I am sure that price will NOT last.",Positive
Intel,"An AM5 x3D CPU will give you way better FPS+smoother Frame times+better 1% lows. So it is definitely worth the money imo.     For the motherboard, any B850 board will be able to handle the 7600x3D. No reason to spend extra.",Positive
Intel,"You could just plug in a 5700x  , still be a a upgrade that could take you to AM6",Neutral
Intel,A 5700x3d isn’t that much better than what you have.,Negative
Intel,Go AM5,Neutral
Intel,I'd 1000% go AM5 in your case. Depends on what the money means for you though.,Positive
Intel,I was just going to comment: Why isn't everybody simply suspending purchases? Because it's just ridiculous pricing.,Negative
Intel,But where??? Or a 5800x3d,Neutral
Intel,"that CPU is $300 right now, for $550 i could get a 7600x3d - New Mobo - Overpriced Ram lol. Just the market is in such a weird spot idk what to do!",Negative
Intel,"No it won’t. Next consoles will be using Zen 6, and AM6 is expected to be Zen 8 around 2030. Zen 3 like the 5700X isn’t going to be viable that long unless you don’t want to play newer games coming out around when the new consoles do.",Negative
Intel,Highly disagree,Negative
Intel,It is,Neutral
Intel,"Are you ok spending the extra $250? If so, buy the bundle for sure since AM5 will be around for a while",Neutral
Intel,Depends i guess what rumors are true.    I really hope am6 is not 2030!  I want DDR6,Neutral
Intel,Why?,Neutral
Intel,I had this exact build and upgraded to a 5800x3d last year. There really wasn’t much of a difference in fps in most of the games I played. My 5800x3d stopped working this year so I rma’d it and went back to my old cpu. I barely even noticed a difference again so instead of putting it back in I sold it and got a 9800x3d instead.,Negative
Intel,"So do I. I do think that Intel will use it before AMD, if the rumors about Zen 7 being on AM5 are true.. but I suppose we’ll see (or not see) in a few years.",Neutral
Intel,Because a 5700X3D is on a different level compared to a 3700X,Neutral
Intel,It really depends on the games you play if they don’t use the extra cache it doesn’t really make a difference but in most games the extra cache makes a huge difference.,Neutral
Intel,I’m not sure about that. I tested both builds against a variety of games. I saw maybe a 10-15 fps improvement. And even after i upgraded to the 5800x3d it struggled to maintain fps above 165 while playing most games in 1440p.,Negative
Intel,Because you’re at 1440p so you’re GPU bound. High refresh rate 1080p you’d see a larger difference.,Neutral
Intel,At 1080p I was already going way over the refresh rate anyway with the older cpu. I have the same gpu as op.  I just got a 9800x3d so I’ll report back and let you know if there is a meaningful improvement in my fps.  If the limiting factor is indeed the video card then OP is definitely better off getting the next gen cpu so that he can upgrade his video card later to a pcie 5 card and not have to throttle down to pcie 4.  Furthermore my advice to op is to bundle the parts at micro center for a better deal. That is if you can pick them up in person.,Neutral
Intel,"A 5060 or similar from AMD is probably a good option. You could also see if there's a 5050 but I don't remember seeing those recently. The difference between 8 or 10GB of VRAM is not that important tbf. 8, 12 and 16 is something to consider.  I would check the used market to be honest. See if any 30 or higher 20 series card is available there.",Neutral
Intel,"For a new GPU, a $199 B570 is hard to beat. I hesitate to recommend any Arc card for someone that isn't particularly tech-savvy though (who's to say the whole Arc program doesn't implode over the next few years). I would much rather go with used Nvidia, if that's an option. Something like a 3060.",Negative
Intel,"B570 is a good choice imo. what CPU are they going for or do they already have? the Arc GPUs generally perform much better with stronger CPUs, however for those games it probably does not matter unless they are trying to hit super high frame rates.",Positive
Intel,B580,Neutral
Intel,"If you're OK with used, a 1080ti would do well. Can find them on ebay for $150 ish.  Homie will own a piece of history with the GOATed graphics card",Positive
Intel,I'd also look at the used market. The 3070 can be had for similar prices to the B570 & is more akin to a 5060 in performance. The 2080 Super is also an option.,Neutral
Intel,"So, the Arc GPUs are a good value, but something to keep in mind is that they really need re-sizeable BAR and PCIe4.0 to get their best performance.  If you are building an otherwise modern system that probably isn't an issue, but if you are trying to build this from second-hand parts, this will exclude older Intel and AMD based systems.  I think that means you need Ryzen 3000 and later chips, or Intel 10th generation and later CPUs.  Not a huge deal, but something to keep in mind.",Neutral
Intel,"B570 is the best 200 dollar card if you're buying new. You could get parts that are more performant for less though, the RTX 3070 and RX 6700XT/RX 6800 but these parts consume way more power than the B570, that's also without saying that the 3070 only has 8gb of vram",Positive
Intel,rtx 5060 or rx 9060xt 8 gigs are can be found for like 250-270 on sale and are basically 50% faster even despite having less vram. They are the much better choice if the budget can be stretched just a bit.,Positive
Intel,5060 or 9060,Neutral
Intel,"3060, 6700xt/6750xt as long as they are about $200 or less would fit your needs perfectly. I don't know too much about the used market atm though. Buy used, not new. You might see some circles talk about vram but you are on 1080p. You don't need to worry about vram too much. 8gb should be serviceable for you at that resolution.",Neutral
Intel,£299 for a 9060xt 16gb An old 4060 ti 16gb If you go for intel make sure you have a return policy,Neutral
Intel,6600xt -6650xt -6700xt all good choices. Wouldn't get anything older than an rx6000 series or gtx3000 series.,Positive
Intel,2080 super!,Positive
Intel,"Don't sleep on 600 class AMD cards. E.g., 5600xt 6600xt - check prices on eBay.   If it's 1080p competitive games then even a 5600xt will run great.   I get 70-80fps 1080p low on battlefield 6 even with my 5600xt.",Positive
Intel,The 5060 is an nvidia gpu. The 5600 is an am4 cpu.,Neutral
Intel,"I'm confident my old machine with a 2070 could run all of the above, a 3060 or 3070 ""should"" be perfectly serviceable.   I'd put a little extra effort into making sure whatever mainboard they pick can support windows 11's secure boot nonsense for a little bit of future proofing.",Positive
Intel,"You want to pay $75 for the privilege of having a new card that performs the same as the one you already have? Why? You know intellectually that this grass isn’t much greener on the other side, lean into that knowledge.   Deny yourself. Save. Buy a *better* card.",Negative
Intel,Stop using a performance overlay. It uses up resources.,Negative
Intel,"I'm sorry, you want to move to a b580 from a 4060?  That's hardly an upgrade. Yes, you get more VRAM, and while i acknowledge Intel's strides in their GPU software, they're still not at the level of comfortable compatibility like Nvidia and AMD. Maybe next gen.",Negative
Intel,"Honestly not worth imo. At that point it's just a hassle, I'd rather make a bigger jump in performance personally. Save for a 9060xt, or even a 9070",Negative
Intel,RX 9060 XT is going for $275 on sale edit: nevermind it's the 8GB one,Neutral
Intel,go 9060xt / 9070xt,Neutral
Intel,"9060XT 16GB would be my suggestion.  It's about 20%-30% faster and double the VRAM.  It should perform well in BF6, as it favors AMD - about 60% faster.  Benchmarks have the 9060xt at 71fps at 1440p high with no upscaling vs 41fps on the 4060.  If you use upscaling and medium you should be able to get over 100fps...  Hardware Unboxed has a full BF6 benchmark and shows 1440p Overkill with FSR Balanced hitting 100fps avg.",Positive
Intel,"Dude I'm on 1440p with a 2070 the 8gb vram is phasing out , you should save your cash and get a sizable upgrade in the future for now try to lower the usage",Neutral
Intel,I think that's not a big enough performance increase to upgrade.  Save for like a 5060 ti 16gb at least.,Negative
Intel,"Buy 5060 ti 16gb on sale, sell your 4060, and might just cost you some 100-120 usd",Neutral
Intel,"Intel isn't quite there yet with its driver support. Don't worry about upgrading if you aren't going to get a substantial improvement it's just wasting money. If your vram ever does fill up, turn down shadows or something.",Negative
Intel,"Does your CPU have an iGPU?   You might be able run it on the 4060 at 1080p, then use the iGPU with Lossless Scaling to upscale using FSR",Neutral
Intel,"RAM costs are about to hit GPUs. If you had any thought of upgrading for the next 12 months, possibly longer depending on whether the AI bubble bursts, do it now. Don't be like me during COVID-19, I waited when I should have bought and had no GPU in my new gaming PC for 12 months.",Negative
Intel,Id say try medium. You probably won't hit the vram limit or lose many frames. I've not seen much difference between low and medium in my performance going between them.  I used to run a 3070 at 4k and I did eventually move to a 16 gig card but at 1440p only red dead redemption 2 gave me constant vram warnings.  Also far cry 5 or 6 hd texture pack at 1440p wouldn't run at 8 gigs.  If you want an arc card the new b780 or whatever is supposed to come out in the next few months.  That looks to be a 4060ti level card for a rumored bargin price.  You want at least 40% gain in frames IMHO bother with an upgrade.,Neutral
Intel,thats a downgrade if you ask me plus i wouldnt go under 220 for the 4060,Negative
Intel,I don't mind current performance it's just the anxiety of seeing that vram max out more than anything.,Negative
Intel,"Yep, either one of those would be fine(definitely the 16GB 9060 XT).",Positive
Intel,Are we talking the 8 or 16 gb model?,Neutral
Intel,"Yep, got myself a 9060 XT 16GB this month and I'm glad I did.",Positive
Intel,As a 9060 XT 16GB owner I agree with this. And it's the cheapest option as well for it's performance uplift and higher vram as well.,Positive
Intel,I'm already low on everything. I don't know if I'm missing something or if BF6 is just that demanding,Negative
Intel,He's a friend I wouldn't mind the hit my mans still on a 1660 ti,Neutral
Intel,"Turn off the overlay, enjoy the game",Positive
Intel,If it isn’t maxing out you have no problems?,Neutral
Intel,nothing wrong with the VRAM being maxed out. it's not gonna break anything.  you'll truly know you're out if VRAM is when the framerate goes down below 30 to 20 FPS all of a sudden after playing for a while because all a fraction of the assets swapped from the GPU's VRAM to the system memory/RAM.  if your 4060's VRAM is maxed out but the performance is still fine then leave it be. if you are just looking for an excuse to upgrade then get a 9060 XT 16GB or a used RTX 4070.,Neutral
Intel,"Bought a 16gb card and my vram still maxed out at 1440, pretty sure your system just utilizes all you'll give it",Negative
Intel,Ended up getting this.... Fuck it. Night and day difference,Negative
Intel,"The 16 GB model can be found for as low as 330$, just got one for my gf a few weeks back.",Positive
Intel,"That may be the unfortunate reality of PC gaming right now. It's either a 6000 dollar rig running a AAA at only 100fps or a modest system barely scraping by at all. I don't believe 16gb is worth it as your only uplift regardless.   I strongly recommend checking out the used market soon because of black Friday, there's bound to be some older 80's cards and the AMD equivalents showing up for the same price as a newer card with half the compute power.",Negative
Intel,"It's usually a thing like this that push you over the edge. Getting a new card and helping out a friend, life's good.",Positive
Intel,"Nice, congrats man! But yeah going from 1070 to 9060 XT was a night and day difference for me as well!",Positive
Intel,Picked it up after work. Thank you microcenter,Positive
Intel,"Don't buy right this second if you can.  Wait until the next gen stuff goes back to normal price because technically, performance may be better going for the 5800x3d, but you won't be able to upgrade anymore.    Not too long ago, a b650 and 7800x3d and 32gb ddr5 was only around 600$ this would give you better performance than the 5800x3d, but there's other cpu options you can use specifically for the upgrade path like a 7700x for example.",Negative
Intel,"The 5800X3D is available on Ebay for much less than that. Nothing wrong with buying used. It's a few years old now even if you can still buy them new. And CPUs are usually only ever either working or not. There's very rarely an in-between unless they're one of Intels recent chips.   If you buy one and it works, it'll likely keep working for years.",Neutral
Intel,"It's not just ""right now"" unfortunately. AM4 X3D is no longer in production, so the remaining stocks are outrageously overpriced. Maybe you'll be able to find something closer to MSRP with upcoming Black Friday deals, but it might be time to bite the bullet and get into AM5, as bad as DDR5 has gotten",Negative
Intel,I'd keep an eye on eBay. I paid £210 for a 5700x3D and thankfully moved to 32gb 3600mhz ram before the price hike. I'd like to say I'm more than happy to skip AM5 entirely.   The price to upgrade even before the price hike wouldn't really net me a huge performance game.,Positive
Intel,"I don't think they make the 5800x3d anymore but if you can get a used one, I'd highly recommend it,.I have one it still feels like a beast.",Positive
Intel,Have you tried OC'ing the 5600x? I run it at 4.8Ghz (very stable) all core and it helped with 1% lows in BF6!  Peak temps never go beyond 71c either.,Positive
Intel,"Gaming at 1080P I assume if CPU is the bottleneck? Unless you're going to upgrade the GPU as well, I don't see the point (personally) upgrading anything else. A 4070 isn't weak, but i'd consider it a pretty decent pairing with the 5600X. I'm just not convinced the upgrade cost is going to be good value at all. Especially as if you do that upgrade, you'll also need to upgrade the GPU so that isn't now the bottleneck.",Neutral
Intel,Something like this would get you similar performance.  It's up to you which way to spend your money. https://pcpartpicker.com/list/YkmDDj,Positive
Intel,"I'm with you though I want a 5700X3D as I really CBA to upgrade to AM5, and have to reformat my PC!     But these prices are now 50% over 7500X3D... =/",Negative
Intel,"The window for AM4 upgrading is getting narrower.  X3D CPU's, seeing a lot of used out there, but expensive.  I would recommend the non x3d, if you don't want to spend the money now to go to AM5.  Ram keeps going up in price, and I think that trend will continue for a while.  Along with NVME ssd's.  Prices have almost doubled in 6 months.  If you can afford it right now go AM5.  Good luck!",Neutral
Intel,DDR5 is crazy expensive right now. If you are going to a new motherboard you might want to switch to a newer Intel CPU+Mobo that supports DDR4.,Negative
Intel,I see a used 5700x3d for $267 in eBay,Neutral
Intel,"Yeah get an am5 system. Just keep waiting for the price of ram to come down. Also if you are in the US try to hit a micro center, they have bundles that'll save about $100",Neutral
Intel,Higher res monitor,Neutral
Intel,"Where are you located?   I recently upgraded my 5600x to a 7600x3d with MicroCenter's AMD bundles.  I spent I believe $400 in total for Mobo, RAM, and CPU, and sold my old stuff for around $250 so in total I spent $150 to upgrade most of my PC.  This, however, was before the RAM prices skyrocketed so your situation may be different. But if you can get to a microcenter, the bundle is absolutely worth it, especially with how overpriced the 5700x3d and 5800x3d chips are.",Neutral
Intel,I’d go for am5 there’s only upside if you plan to continue gaming. You’ll have a crap load of options to choose from and I’m personally banking on it being like am4 and lasting awhile because I had multiple upgrades on am4 all on the same board and it’s still going in a 2nd pc. I lied a little bit tho there is a slight downside. Ram prices just got hit hard so there is that.,Positive
Intel,"Yea like some others said maybe just wait a bit to see what happens. I had a 5 5600x and got a 5 5700X3D on aliexpress for $130 with shipping but that was 10 months ago or 8 ish? I then checked with CPUID and ran thermals with HWInfo. I also -30 on all cores and it’s pretty good silicone as I’ve had zero issues and it never glitch when playing with certain cores. I then took the 5600x and paired it with my strix 2070 super 8gb OC in one of my other rigs and use it as the stream pc or move it around the house to use. To my knowledge it’s still a good chip (5600x). The fact OP you mention those prices just the other day I did a 9070XT, ryzen 5 9600x, T force ram 16gb ddr5, a cooler master 750watt gold supply, some dope looking case, a 1tb drive , the whole build was $1193 to get an AM5 system for my nephew",Neutral
Intel,"spending loads now when better deals are just around the corner, so wild",Positive
Intel,Yo we are pretty in much the same shoes but I have a 3700X ryzen that bottlenecks my 4070super.  Would rather not open another plethora of upgradable parts as my motherboard is almost maxed out. Just gotta find a 5800x3D,Neutral
Intel,"The only issue, right now, is DDR5 memory costs. Otherwise a 7600x trades blows with a 5700x3d, even in CPU bound games.",Negative
Intel,Normally I'd say go for am5 but with ram prices....eh might be better off just getting an x3d chip on am4. Get the 5700x3d.,Neutral
Intel,How much would a 5700x3d and 16GB more of DDR4 cost you?   I'm rocking a 5700x3d with 32GB of RAM and its awsome,Positive
Intel,"I was in this exact situation and chose to go from a 5800x to a used 5700x3d to get my 1% lows up (which it did significantly). I also managed to snag another 16gb of ram for cheap to make 32. After selling the old chip this would have costed me about 180 usd in total, which is much, much better than the prospects of going to AM5 right now.",Neutral
Intel,"It really depends on your budget, but I can say that a 7800x3d/9800x3d is definitely worth the money especially if you play a lot of BF6.   I would just shop around awhile for a sale and see if you can find the RAM at a decent price.",Positive
Intel,"If the rumors are true, with AM5 you'll get to upgrade not just to Zen 6, but Zen 7 after that. There's no guarantee, that's just what the rumor mills are claiming to have learned.  But also be aware that DDR5 prices are inflated right now. Check out the prices in your area first, it might prohibit you from switching to DDR5.",Neutral
Intel,AM5 would require also DDR5 RAM .. which is expensive now,Neutral
Intel,buy used CPU,Neutral
Intel,I got my 5700x3d on AliExpress. Just make sure to look for reputable sellers. Mine was $170ish,Neutral
Intel,"Either wait for price stabilization and/or next gen or buy used, whatever the used hardware you may choose. New AM4 isn't worth it and DDR5 RAM is insanely priced to buy new.  For context, I had 5900x and bought a used 7800x3d and a x670e board with 32gb of ram. I ended up upgrading to 64gb ram since I found a great used deal as well (from a store with 5 year warranty though). I had never bought a used PC in my life and I'm over 30, that's how bad the market is.  If you're buying used, act fast. Some markets haven't adjusted to the new DDR5 reality, but once they do, RIP prices.  Also, I can confirm that the jump from 5900x to 7800x3d in some games is insane. As far as having double FPS in assetto corsa (from 100 to 200+ in same circumstances). This is playing at 4k, lower resolutions will benefit more.",Negative
Intel,"Went from same cpu to 5700x3d it is better yes, and onlg upgrade if u het nuce deal for it IF u can even find it :D",Positive
Intel,"With the prices of ram currently, I personally wouldn't buy on a new socket right now. Stick with am4",Negative
Intel,The best financial option is buying the 5700x3d that’ll last you another 4-6 years that can trade blows with CPUs like the 7600x and even the 7700x. The additional cost of a 5800x3d for about 3-11% FPS increase in games and 7% increase in multi core productivity compared to the 5700x3D is not justified. No reason in trading a “Ferrari” for another “Ferrari”. Save your money.,Neutral
Intel,"I couldn't justify $250 for a used 5700X3D so I bought a new 5800XT for $150ish.     Massive upgrade from the 1600X I was running before, probably not so much if you're coming from a 5600X.",Positive
Intel,"5700x, 5800xt or just straight up AM5.",Neutral
Intel,"5800x3d will fully saturate a 4070 in most titles. Normally I'd say full upgrade but with the cost of DDR5 atm, get a 5800x3d from ebay.",Neutral
Intel,"You can pay $400 for the 5800x3D on eBay or $400 for just 32 GB of DDR5 then a new motherboard, and a new chip.  Value wise, the $5800x3D is a great option for waiting this out. It pairs really well with my 5070ti and helped me save a few hundred bucks.",Positive
Intel,u/PhatYeeter tried to send you chat. I have a couple AM5 bundles with CPU/RAM/Mobo I’m going to be posting on hardware swap tomorrow when I get home. Send me a chat if you are interested.,Neutral
Intel,5700x3Ds were like 120 on AliExpress. Check to see if that’s still around,Neutral
Intel,"The only next gen stuff that’s really elevated right now is RAM.  CPU, GPU, mobo, PSUs are all at around MSRP.   SSDs are creeping up but still not terrible yet.    RAM ain’t coming back to earth for a bit.  Especially not the Hynix A-die and M-die that we all love after OpenAI bought some huge chunk of SK Hynix production.",Neutral
Intel,I was looking for 5800X3Ds on eBay last night and the cheapest ones were realistically still $300+.,Neutral
Intel,"I also always recommend buying Used CPUs, they are rarely broken",Positive
Intel,I'll look into buying used. Bought my GPU refurbished and its been running fine.,Positive
Intel,I'm running most games at 1440p with DLSS when available,Neutral
Intel,"This is technically not true. I play at 4K and you would think that cpu doesn't matter that much. I had same exact cpu 5600x paired with a 5070ti. No matter what I did I was getting 70fps in high in BF6. Even using DLSS and didn't improve. I recently upgraded to AM5 with an 9800x3d and now on high native I get 90fps (20fps difference) and when using DLSS quality, I am getting 120fps.",Neutral
Intel,"now, mobo and cpu is 400. Ram is sold separately",Neutral
Intel,"I just bought a b850, 9800x3d at Newegg and did the PayPal 20% cash back. By the grace of God got the $150 32gb Fury deal at Walmart locally. I'll sell the free MSI cooler from Newegg on Marketplace. Sold my old 2080s on Marketplace and another item I never used.   All said and done I think I'm out 75 bucks. I wanted to upgrade (definitely a want, not a need) but I sure as shit wasn't going to pay retail. It was a lot of running around that's for sure.",Neutral
Intel,It's never the CPU....until it is. 😀,Neutral
Intel,".. unless they are Intel 13th or 14th gen. You are really “rolling the dice” if you buy one of those used, what with the degradation issues. How many marginal-but-working Raptor Lake CPUs entered the used market, we’ll never know.",Negative
Intel,I would think the GPU would struggle more than CPU at 1440p,Negative
Intel,Yes that's exactly what a CPU bound situation looks like. You literally just proved my point. You couldn't get more FPS because your CPU was holding you back.,Negative
Intel,"How much did it cost for that 20fps? I'm just saying I don't think it's a good value upgrade. Since to get the most out of if you need a new GPU too. 5600 @ 1080P can net 90-100FPS with a 5090, so the CPU itself can pump out some frames. 5070Ti isn't really a 4k card, 5090 would be the way to go if max FPS is the goal. A 5070Ti is also much better than a 4070 (approx 60% faster in BF6), so their gains will be even less, unless they upgrade GPU as suggested. At 4k a 4070 nets 34FPS with a 9800X3D. 1440P (overkill) nets 70fps, with a 9800X3D.  I'm not saying don't upgrade, I just don't see it as a good value option. Each to their own etc.  FPS is always changing depending on mode, but I tend to hover around 80-120FPS with my 5700X3D @ 1440P ultrawide, high settings and maybe quality FSR4, I don't recall exactly. I could turn up the FSR, but i'm happy enough with those frames for now. Certainly not worth going AM5 yet even though my 9070XT would get a boost.",Negative
Intel,"I just checked, the bundle is still $330, but RAM is now $240 instead of the $65 that it was before.  Still could be worth it if OP sells their old stuff for around $250. End up with a new mobo, RAM, and CPU for $320.",Neutral
Intel,Not bad.  There’s definitely still deals to be had.    Wish I’d known about the 20% cash back thing *before* I bought my CPU this morning.,Positive
Intel,"Wait, what PayPal 20%?  Edit: Found it. Thanks for mentioning it!",Positive
Intel,Last broken CPU I’ve seen or heard of in person was a Q6600,Negative
Intel,buying used is always “hit or miss” so thats why i always recommend doing extensive testing first.,Negative
Intel,Not with dlss. 1440p dlss quality is just 960p upscaled. It may look like 1440p after being upscaled back up to 1440p but the game is still technically internally running at 960p as far the CPU is concerned.  This is why past balanced at 1440p you start to be really CPU bound.,Neutral
Intel,"I am not saying its a great move to make. I am just saying there is a difference with the CPU you go to, even at resolutions such as 4K. Of course there is money attached to it, that is a decision OP has to make.",Neutral
Intel,"oh thought it was 400, my bad",Neutral
Intel,"Hey, to ease your mind, it's limited and I think you have to do the pay later thing.",Neutral
Intel,"Build is fine just needs RAM and a GPU, 5060/9060+",Positive
Intel,"Thank you so much, I will look into ram. 5060 is on sale right now in my area as well.",Positive
Intel,"I would hurry with getting ram, it is getting expensive.   Get yourself 32gb of ddr4 ram and a 5060 or 9060 like kingcarcas said.",Negative
Intel,Thanks! I am searching for ram right now.,Positive
Intel,I have the 9800X3D and 9070XT and its crazy because I get better results than people with a 5080 without an X3D CPU in many titles,Positive
Intel,Congrats on the good results.  People who discuss bottlenecks in terms of average FPS are doing themselves a disservice. Minimum frame times are what matter most for gameplay experience.,Positive
Intel,"I’m running a 10700kf and 32gb of ddr4 3200 with a 5070ti and the 1% lows kill me. The average frame rate is excellent, but the stutters are so annoying. It feels like anytime it’s pulling new textures or something from the system ram is lags for a split second.   My son and his friend also like to play Helldivers on max difficulty, and the ridiculous amount of enemies that load at one time tanks my frame rate down into the 30’s. His laptop will only be getting like 15fps but it doesn’t seem to bother him lol.   I’m waiting for tax refund season to start, that child tax credit has already been ear marked for a new cpu, mobo, and ram.",Negative
Intel,CPU upgrades are severely underrated in the hobby/community  Far too many people have been convinced the ONLY thing that matters is the GPU,Negative
Intel,"This is interesting as I currently have a 3080/13600k/DDR4 setup. I have a 5070ti waiting to be installed when I get time but I have been wanting to go to 9800x3d/DDR5 next and was wondering what the jump would be. With RAM prices as they are I probably won’t for 6-12 months anyway and by then maybe even a new x3D chip will be out.  My PC is also used as a sim racing rig where I have 3 x 1440p monitors set up. The sim is extremely CPU intensive and the x3D chips really sing with this application, so I think I’d get a good boost with that too.  It would have cost me the same to upgrade to a 9800x3D + DDR5 + new mobo as it did to get the 5070ti, but I thought the GPU should be next in line to replace my 3080 10gb.",Positive
Intel,So a 13600k is sort of a 7600x equivilent-ish gaming wise.   Cpu bottle neck is more than just 100% cpu usage. Individual cores can hit 100% even if total usage is at 50%. Memory speeds can slow fps down 5-20%. Pci lane control to the gpu/ssd can slow things down. Its why x3d adds so much speed. Just a little cache on the processor itself means you dont have to send information from the hard drive to the memory and back to the gpu.   Looks like your experience matches the charts almost exactly for a cpu intensive games like Warhammer. 90fps--> 130fps with a 5090 when going from a 7600x to a 9800x3d in 1440.   Us poors with a 9060xt dont have to worry about this kind of thing lmao 😂   https://youtu.be/aYYVz4q-Rt8?si=NMiF6OfCLW9MRgB2,Neutral
Intel,"9800x3d is waaaay better cpu than 13600k, so kinda expected :D",Positive
Intel,"DLSS is a key factor people don't take into account. 4k dlss4 perf is actually the same or similar to 1080p native. The 9800x3d in games like baldurs gate 3 is easily 40-50% faster(130 vs 210fps iirc).   By not utilising nanite and lumen it actually frees up a lot of GPU resources in Arc raiders. With all the ai and things to track games like tarkov, ark, msfs, assetto corsa with lots of cpu logic + lower GPU load + upscaling to lower it further leads to the cpu being the biggest bottleneck. This is why I dknt place much emphasis on avg 1440p/4k charts to determine cpu performance. I prefer looking at titles where the cpu(even a 9800x3d) can't push past 150 fps",Neutral
Intel,Upgrading to an x3D has given the best gaming experience in many years of PC gaming.,Positive
Intel,"X3D & DDR5 are well worth it - if you can afford a 5090 build to begin with, it sounds like you're able to push for those kinds of specs and a build that you're way less likely to need to touch in the future, as a result.",Positive
Intel,"Yeah. I went from 5800x3d + 5080 to 9800x3d + 5080. Noticeable uplift in performance as well. Games like Dying Light 2 and GTA Online Enhanced, the GPU is not constantly waiting for the CPU anymore.",Positive
Intel,"On high settings, arc raiders can make my 13700k go down to 100 fps, 100%usage. Gpu is at 70%. It’s still playable, starting to see its age. DDR4 4000mhz ram",Positive
Intel,"I made this same-ish switch with a 12600k ddr4 and 7900xt, playing on 1440p ultrawide too and it was a massive improvement, more more than I expected, super happy with it and well worth it",Positive
Intel,"Yessir I went from 13600k 4090 to 9800x3d 4090. The AMD CPU is just \*chefs kiss\*  I gained like 30fps or more in KCD2 at 3440x1440.  But anyway the real benefit for me is stability. Once I got my bios settings locked down and tightened up my ram timings, the system has been 100% rock solid for almost a year now.  I had all kinds of issues with WHEA errors and undervolts only being stable for a certain amount of time before I had to increase voltage on the 13600k. I kind of hated that CPU.  The 9800x3d is everything I ever wanted in a CPU, and more. Truly great IMO.",Positive
Intel,> wasn’t seeing a CPU bottleneck  Are you sure? Was your GPU always close to 100% usage?,Neutral
Intel,Get a 4k monitor at least,Neutral
Intel,"I just can’t believe this : I just canceled my order for the same setup, coming from a i9-10900k +4090.   Why I canceled : every single research I did indicated there won’t be a single uplift from spending the 1.xk € .   I researched all weekend before pulling the trigger on the cancellation. I would be gladly spend the money for 20% better lows and 10-15 avg fps . But according to every non bias info about this it says : the i9 won’t even be limiting the 6090 in 99% of the cases. As long as there are not strat games or high fps games (so we are only talking gpu bound titles like cyberpunk)    4K Gaming only   So what’s true now 😂",Negative
Intel,You can't go from 90 FPS to 130-140 FPS without being CPU bottlenecked. That's not how it works – you definitely were CPU limited.,Negative
Intel,Do you just hate having money?,Negative
Intel,"I experienced pretty much same results.   I had 12900k/ddr4/4090 and play at 32:9 5120x1440 super ultrawide.   Upgrading to 9800x3d/ddr5 actually net improved performance a bunch. Felt like the 4090 really came alive.   Replaced 4090 with 5090 and not surprisingly fps increased.   I play lots of cpu intensive games though, Last Caretaker, Icarus, Stellaris, Cities Skylines, Civilization, etc and of course brute force heavy modded Fallout 4 and Skyrim.",Positive
Intel,"It was the ram upgrade not the cpu, that's giving you the bang for buck ddr4 3600 to ddr5 6000 is basically double the perf",Neutral
Intel,"""Despite not being CPU bottlenecked""   Obviously, you were CPU bottlenecked lol  What gave you the impression that you were not CPU bottlenecked?",Neutral
Intel,Do you guys think there would be a noticeable improvement from a 7800x3d to a 9800x3d?,Neutral
Intel,"If you were running DDR5 on the 13600K with a decent speed, the increase would have been 10% at most like you expected.",Neutral
Intel,I have a 5080 and a 7600x3d and it works like a charm. I think the x3d chips are great.,Positive
Intel,What is the gpu usage before and after the upgrade?,Neutral
Intel,Double the ram speed and number of cores... why expect less?,Neutral
Intel,"I have the 7800x3d and holy shit all i did was go to am5 and upgrade my cpu and ram i was astonished at how much performace it gave me for my 5700xt, i finally just bought the 9070xt and i cpuldnt be happier.",Positive
Intel,"I don't understand the upgrade.  If you are doing ok on the processor then just wait it out.  Nova Lake drops summer of next year.  It will have 52 cores and probably spank the crap out of the 9800x3D many times over for about the same money.  Then AMD will have a response that will be equally as nuts.  Just wait till, the chip horsepower wars are just starting.",Negative
Intel,It would be enough to upgrade to 14600kf + 5090 for playing 4k :)  For 2k and 4k cpus doesn't actually matter:),Positive
Intel,Yea 9800x3d fucks lol I love mine.,Positive
Intel,I saw this exact result also and was surprised,Neutral
Intel,At which resolution?,Neutral
Intel,Exactly the 1% lows are massively improved and change the whole experience even if the the top end of the frame rate barely changes,Positive
Intel,"Yeah man, especially if you play competitive games. People are going to have a bias to their budget tho",Neutral
Intel,considering this dude literally has a 5090 I don't think this is exactly the best thread to be saying this in.,Negative
Intel,"Yeah. Massive uplift ok frame time stability and 0.1/1% lows with a faster CPU, and quite noticable if using upscaling",Positive
Intel,"I feel like once upon a time, all that mattered was the GPU? Or am I misremembering?",Neutral
Intel,"It will be a good jump. I've got a 3080 12gb. Went from a 12600k to a 9800x3d. Everything runs smoother, I also used to get a slight stutter which is gone now.",Positive
Intel,"That’s what I’ve noticed too. I think it’s exactly as you said, individual cores reaching 100, and the cache doing magic. Had a used 3080ti for 3 years before I upgraded to the 5090 lol. Used to be a poor student, now I’ve got a decent job. It sucks that I finally have the money to spend yet way less time to game compared to before.",Negative
Intel,"I think people also just misinterpret the relationship between a CPU and GPU. Regardless of GPU, a CPU can only produce a certain amount of frames in each game. Changing the resolution or settings really don't have as big of an impact on how many frames that is (might be a 5-10% drop going from 1080p to 4k, whereas a GPU is doing 4x the work actually rendering those frames). With that, one of them will always be bottlenecking the other. Either the CPU will always have a frame available when the GPU asks for it, or it won't. You want the CPU to always have a frame available for the GPU, rather than the GPU waiting for a frame from the CPU.   Even when I had my 6750xt paired with a 5600, some games would see no difference in FPS going from 1440p down to 1080p because the CPU was already maxed out and the GPU could handle it, so in those scenarios I'd just start upping the graphics settings until it lowered the FPS a couple percent to swing it back so the GPU was the bottleneck again (generally plays smoother this way, even if it's overall lower FPS). And games that are more GPU intensive, I'll start playing with the upscaling to get towards that CPU ceiling.",Neutral
Intel,>DLSS is a key factor people don't take into account. 4k dlss4 perf is actually the same or similar to 1080p native.  Do people actually buy 5090 to play at FHD? Because I seriously doubt that somehow.,Neutral
Intel,"What you’re saying is true, and I agree with that. It’s on me for not specifying the settings but I wasn’t using DLSS on Arc, It was a native, non-upscaled comparison .",Neutral
Intel,That’s my goal. I intend to spend once and keep my setup for a long time. Instead of having to upgrade things here and there every 1-2 years,Positive
Intel,I guess arc raiders is quite a CPU intensive game after all,Neutral
Intel,It was usually around 92-96% before the upgrade,Neutral
Intel,"At the highest performance levels it's more about the 1% lows than it is average framerate.    In your position, the 4090 is still a strong contender as it's arguably the second best consumer GPU on the market, so while the CPU upgrade would do you a solid you might just want to hold out for the next AMD CPU cycle.",Positive
Intel,"I held off from november until January to replace my 13600k with 9800x3d. I kept seeing stuff that said it wouldn't make much difference with my 4090 at the resolutions I play at, namely 3440x1440 and 5120x2160. But finally I decided I wanted it and I would have it in case tariffs or AI demand somehow messed with the prices.   there was a huge difference in a lot of games. EVERYTHING is smoother. I notice basically zero stuttering and frametime spikes compared to getting them constantly on 13600k.  Everything is just better. Lightning fast, more stable. I'll never go back",Neutral
Intel,I get my 5080 tomorrow and I have the i9 10900 K I expect an massive uplift with this card,Positive
Intel,"You are in the same place I was when researching to upgrade from the 13600K (which was an upgrade from a 9700K back in the days).  I also cancelled my order once because I thought it wouldn’t be worth it. But at the end of the day, I didn’t want to miss Black Friday and I convinced myself that I’d have to migrate to AM5 sooner or later anyways.  The results surprised me in a good way. Can’t say the same for you, but overall system smoothness and PC stability alone might be worth it.",Neutral
Intel,"You might want to un-cancel/re-order, especially before DDR5 prices get totally out of control.. you have a very narrow window there. I'd expect something like a 75% improvement or maybe more, coming from 10th gen.  People also tend to forget the existence of DLSS upscaling in benchmarks.. 4K DLSS Performance will give you fps inbetween 1080p and 1440p native results, usually, for instance.",Neutral
Intel,Seems like they enjoy having a bunch of money.,Neutral
Intel,"It wasn’t utilizing 100% in intensive games, but I’m guessing individual cores might’ve been at 100%",Neutral
Intel,"Probably not, except in benchmarks. Wait for the (hopefully named) 11800X3D in early 2027, Zen 6 is rumored to be a beast.",Positive
Intel,around 90-95 to constant 99%,Neutral
Intel,"I expected an upgrade, but not this big of one since I thought I was more GPU bottlenecked with high fidelity titles",Neutral
Intel,>I don't understand the upgrade  >It will have 52 cores and probably spank the crap out of the 9800x3D many times over for about the same money.  You don't understand gaming CPUs.,Negative
Intel,">It will have 52 cores  Must be very good at unzipping archives. But OP doesn't need that, OP wants gaming performance. Those Nova Lake CPUs will still be slower than 9800X3D in games.",Neutral
Intel,What? You don't need 52 cores for gaming. The extra L3 cache on AMD CPUs is more important. Going with AM5 is better than betting on Intel anyway.,Neutral
Intel,"> Nova Lake drops summer of next year  That timing is pretty unlikely. More likely is anywhere from early Q4 to sometime in Q1 2027. If it’s the former, they’ll have some time to compete against Zen 5 + Zen 5 Refresh at parity or hopefully better. If it’s the latter, they’ll probably get stomped by Zen 6, and it’ll be “Arrow Lake all over again”. Don’t get me wrong: I want Nova Lake to be awesome, it’s just that Intel’s execution has really been lacking after 12th gen, so I’m skeptical they can get it done, how it needs to be done to be successful.",Neutral
Intel,"that’s the advice I got, but I think you might be surprised by the results",Neutral
Intel,You forget about DLSS upscaling. It matters.,Neutral
Intel,I play in 4k,Neutral
Intel,"Both always mattered, but for most of the 2010s we saw very slow CPU improvements, as Intel had a defacto monopoly, and so saw fit to offer only very low gen-over-gen improvements.. and only 4 cores, at that, unless you paid big money. GPUs didn’t have that stagnation (Nvidia fighting hard against AMD/Radeon) and so, GPUs were where the improvment was at!  Fortunately, AMD found their footing again with Ryzen, that caused Intel to “wake up” with 12th gen, and.. well, they messed up big since then, and so AMD has been dominant these past couple of years.",Neutral
Intel,Just have to wait for DDR5 prices to come down now. Cant justify spending $400 AUD for 32gb.,Negative
Intel,My 5090 was an upgrade to a 1080ti   And my 9800x3d was an upgrade to an i5 3570k lol,Neutral
Intel,3080Ti is a great GPU even today,Positive
Intel,"Recently upgraded from gtx 1080/7700k to a 5090/9950X3D build. Was a bit pricey, but probably gonna keep this build for a long time to come as well.",Positive
Intel,Adult money changes you aye haha,Neutral
Intel,You gotta have the fastest FPS in all of CS2 or league of legends,Neutral
Intel,i think you misunderstood what i said,Negative
Intel,Try dlss 4 quality in the game. Its so damn good.,Positive
Intel,"Zen 6 is rumored to be a big upgrade over Zen 5 though, if the rumors are true. An upgrade about the same magnitude you already got, coming from your 13600K! Given that AMD is jumping several nodes and various other changes, I personally believe it. Ofc, we will have to wait for independent testing to know for sure.  I considered upgrading from my 12700K + DDR4 this year, but I've decided to wait for Zen 6, since the games I really care about that could actually use the performance (presumably), probably won't be out until 2027 at the earliest. The rest.. well, I'm fine. Plus, historically, I've aimed for a near 100% per core uplift, so this is in line with that.  EDIT: Ok, after reading the various comments in this thread, I admit I’m wavering on not upgrading now..",Positive
Intel,"honestly nvidia Frame gen has gotten so good, its been saving me from a cpu upgrade for now.",Positive
Intel,"Let me guess - you used Reflex. It often results in GPU clocks dropping, so those 92-96% weren't even from the actual power of yoru GPU. Objectively, 4-8% of usage can't contribute to 40% FPS uplift. Thus I'm quite certain it was a CPU bottleneck after all.  Either way, congrats on the upgrade. As of now, you have THE best config on the market!",Neutral
Intel,This. Exactly my experience this time.  I guess you just have to try it to see it.,Neutral
Intel,Ofc you will have : my question is tho : if you have the gpu and bring the cpu to the 9800 x3d if then there is an improvement . GPU upgrade will always be a big step,Positive
Intel,Actually I did 😂 or rather found actually a better deal on the package and saved a bit of money. Will update here after upgrading,Positive
Intel,"Ok thank you, I got some problems with my A1 and B1 ram slots. That's why I have to unplug my CPU anyway :D",Neutral
Intel,Take out one stick and it will turn off dual channel and look at the numbers. Just this one mode makes a huge difference.,Neutral
Intel,"I understand he has a Raptor Lake currently, and its just about as fast as anything out there.  The new ultra 275 chips aren't even as fast as the 14th gen.  Is i5 is like 10-15% slower than the fastest you can get today.    In 6 months there will be 40 and 50 core CPUs that will make anything available today look pretty slow.  Its a good time to hold out for 6 months.",Negative
Intel,"There’s no particular reason to expect Nova Lake P-cores to be slow, TSMC is making the compute chiplet after all, and on a better node than the existing Zen 5 CPUs. Intel may or may not botch this next generation, but the rumors out there rn are optimistic if not stellar. I concur with the opinion that it’ll be at least on par with the 9800X3D for gaming, be much better for multi-threading, but that when Zen 6 comes out 6 months later, that Zen 6 will stomp all over it.  Ofc rumors can be false, either way. We won’t know actual performance until the CPUs are imminently out and independent testers get their hands on them, and make reviews for us to read/watch.",Neutral
Intel,"How do you know that?  Single core 14900k is faster than the 7750x3D, but only by a hair.  You can't see the future, and neither company wants to be the slow one.",Negative
Intel,"AMD will have a response to Nova Lake, and their mid tier stuff in 6 months will make AM5 look sad.  That's how it always is.  We just got used to intel increasing the speeds less than 5% for the 6/7/8/9/10/11th gen processors then a giant leap to Gen13 Raptor Lake and now a step back with the core ultra 185/285.  Big improvements are coming soon.",Neutral
Intel,It's so bad that there is no way to buy two PC and compare it))),Negative
Intel,"According to techpowerup, there's no difference between x9600 and 14600of, which cost the same.🤷",Neutral
Intel,huh,Neutral
Intel,"Maybe I was unaware and bottlenecked myself!   I think I went Q6600, 2600k, 6700k and am now on 9900k eyeing off an x3d build.",Neutral
Intel,"I bet that felt amazing! The 3570K was an excellent CPU at the time, so was the 1080 Ti, I owned both, I still have them around. I couldn’t stand to wait as long as you did though, and upgraded to a 12700K in 2021.",Positive
Intel,Both games will be heavily CPU-bound with such setup. I bet switching from FHD to UHD won't change FPS at all.,Neutral
Intel,I guess if the CPU can't generate the frames you can still get some frames from somewhere.,Neutral
Intel,"Ah, that’s probably it",Neutral
Intel,i have seen crazy numbers on the cpu side on the comp scene in cod but idk - do you have cpu limits with your setup ? a decent 5080 can be close to a 4090 i hope i got an good one,Negative
Intel,Awesome! Congrats!!,Positive
Intel,Congrats dude! Keep us updated on the results!,Positive
Intel,"Yeah, it’s not fun when motherboards have problems. I hope you can get a quick replacement!",Negative
Intel,">I understand he has a Raptor Lake currently, and its just about as fast as anything out there.  Except it's not? It's evidently significantly shower in gaming.  >In 6 months there will be 40 and 50 core CPUs that will make anything available today look pretty slow.  Multiple cores / threads are irrelevant in gaming, and even certain productivity workflows, unless your work involves processing over time (as opposed to vector / type design etc).  There's literally no tangible info out there relevant to gaming performance, except a rumor that intel is going to incorporate a similar bLLC technology to _maybe_ compete with AMD on that front. bLLC is probably not going to be incorporated into super high thread chips anyway.  And more importantly, we're talking about CPUs. CPUs are rarely responsible for raw gaming performance in higher resolutions, rather than alleviate bottlenecks and allow the GPU to do its job. So at best, Intel is _rumored_ to match AMDs current performance in gaming. That's not a reason to wait, especially in this climate.",Neutral
Intel,More cores doesn’t matter for gaming.  9800X3D beats 9950X despite latter having double the cores.,Neutral
Intel,">You can't see the future  I don't have to. I see the past, I see how the games are made, how the games work. I know how the logic works. Say, I play Vampire Survivors on 3800X, it's a very light game. Unless I get tons of weapons and tons of enemies on the screen at the same time. FPS then can easily drop below 5, because there's a lot of sequential stuff to calculate. Multi-threading and parallel workload are hard af to do, and STILL every single game will be limited by the thread that syncs the rest and gives the tasks to them. So yeah, sorry, those 52 cores won't do shit for gaming.",Neutral
Intel,"Do you work in the industry? I'm not counting on Intel at this point. Also, of course AMD is releasing Zen 6. You don't need that many cores for gaming, an X3D CPU will probably still beat Intel's offerings. It's been that way for years.",Neutral
Intel,"Nah, AMD refresh in a few months is not going to be dramatically better, we'll have to wait for Zen 6 X3D in early 2027 for that IF the rumors end up being true.  Also, we don't factually know the performance of Nova Lake (or Zen 6), rumors are all over the place, we'll have to wait for independent testing once the CPUs are out, to know for sure how well they'll compete against the competition.",Negative
Intel,"Well, one can ofc, it’s just really expensive!",Negative
Intel,"You are probably misremembering. As I recall (I could be misremembering too), all their results are not using upscaling.",Negative
Intel,"Hmm.. if you’ve waited this long, and with RAM prices as crazy as they are rn, you might be best off waiting roughly one more year and going AMD Zen 6 or Intel Nova Lake. Hopefully by then the AI bubble will have burst (which ought to bring RAM prices back down), and ofc you’ll have the generational uplift of the newer CPUs, and on top of that, another generation of GPUs too! That said, one can always say “wait”, so if you need it now then buy now, just… if you do, buy very quickly, a month from now the Black Friday sales will be over and RAM will cost 50% more or something.",Neutral
Intel,"Lamo I can’t read (I’m kinda stupid as hell and was thinking CPU games) I heard someone say at one point that having games at lower resolution but higher refresh rate can make the games feel extraordinarily responsive but at the same time unless you have a 8k hz mouse and keyboard you aren’t going to be able to send commands fast enough, I have a 4090 and I still only get like 170 FPS on ultra in cyberpunk iirc at 1080p, I have a 5120x1440 and it’s way lower FPS but it’s so gorgeous",Negative
Intel,"So little update :  After some annoying bios flash struggle , pc is up and running again.   Most impressed I am with the temps : on low fan speed the cpu doesn’t go above 80 C with a mild undervolt. While holding 5.25 Ghz all core(Cinebench) .  In regards of AAA 4K Gaming (for me as expected) it really doesn’t make a difference. I would say the only really noticeable difference would be the 1% lows on cyberpunk 2077 dogdown market , where they are much more stable.     In 4K Games I tried :  -Cyberpunk 2077 - Hogwarts Legacy  -Black Myth Wukong -Witcher 3   Cinebench gives around 23k points so I would say the cpu is working as intended.  3DMark Speedway : gained 250 just by swapping the cpu   All in all just as expected and more like a future proving upgrade rather than a „wow what a jump upgrade“ .   If money is tight , I would keep the i9 longer",Positive
Intel,"I hope Asus will give me a replacement, because it's a x670e which I bought last year, but haven't used all slots until I tried to upgrade to 64gb. But when I used them, my computer was looping ram Training 😂",Neutral
Intel,Nova Lake will have 52 cores  16 performance (P) cores  32 efficiency (E) cores  4 low-power E cores  Its just doubling what we have now and adding in some super low power cores.  Lower end skus will have fewer cores.,Neutral
Intel,"Yeah, but the single core performance will be better next year than it is today.  Its not rocket surgery.",Positive
Intel,"The point is that there will be big improvements in the near future and spending lots of coin for 10% speed boost is a waste of money.  The reason Raptor Lake was so much faster than Alder Lake was because the Zen5 stuff was so good so Intel threw the kitchen sink at Raptor Lake to edge out AMD.  However, they stumbled with the ring bus issue, and now intel is obviously behind.  AMD has always made good stuff, but they lost the marketing battle.  Now Intel really has to put out some wow in order to get back in the game, and AMD knows this so they will also be bringing some heavy hitters.  The chip horsepower wars are heating up in a way we have never seen before, and that's great for the consumer.  Big things are coming soon, so the same money spent in 6 months will get you a lot more performance.",Positive
Intel,"Ohh, sorry 😔 it changes everything😱  Honestly, I didn't find information about upscaling so I decided that they turn it on when running benchmark for 4k on ultra config 🤔",Negative
Intel,"I am not so worried about prices, they're crazy, sure, but I spent 6.5k Australian on my last build. It was March 2020, so that's about $21/week, and each week I'd get at least 7 hrs of games in. So at it's worst, it's been $3/hr of entertainment. That is cheap. I think I'd get around $1500 Australian secondhand, which improves the numbers further to about $2.4/hr. A pint at the pub or a gig starts at $15, a 10 minute uber ride is $20.    My PC struggles with Tarkov because Tarkov is complicated as heck and apparently loves 3d v cache, and BF6 spikes CPU above 90c but still plays at 110-120fps 2560x1440. Makes the room hot and it's noisy.    My problem is I feel it is not yet time. It's solely a feeling thing. Completely unscientific. I also don't have the dexterity to build one anymore. So I have to go pre-built which is around 7k for a 5090 build, but they come with junk ram, M2 and poopy 5090 like Palit, PNY or Zotac.    You can only get stuff like Astral or the crazy Gigabyte loose. PSU north of 1000w is very very limited too.",Negative
Intel,">unless you have a 8k hz mouse and keyboard you aren’t going to be able to send commands fast enough  Here's a twist - high polling rate puts crazy strain on CPU, and that would result in stutters and low FPS. And in many games it also makes mouse feel worse, certainly in Arc Raiders which OP plays, [here's](https://www.reddit.com/r/ArcRaiders/comments/1okq2d6/psa_set_mouse_polling_rate_below_4k_5002k_fixed/) a thread for example.",Negative
Intel,".. are you sure it’s a motherboard problem? By that, I mean have you tried all 4 DIMMs without XMP/EXPO at the rated 4800? Bc running with 4 DIMMs instead of 2 can be problematic sometimes, with AMD DDR5 systems.",Neutral
Intel,"Fair enough, I corrected it.  Still, more cores =/= more performance in gaming, not even on all non-gaming scenarios.  Also, the 52 core scenario will almost certainly not get bLLC technology, (it makes absolutely no sense to get it) and it won't compete with OPs 9800x3d, which is probably going to still be the top GPU for combined work / gaming builds.  The only question that remains for Nova Lake in the context of gaming is whether intel can actually succeed in matching - let alone exceed - AMD's V-cache. It's possible, but the impact would still be low when paired with high end GPUs, and we're talking about a company that has yet to provide any similar tech whatsoever.  So telling someone who just got the absolute best CPU for work / play desktops that they should have waited based on a rumor about a low impact release that's probably a year away doesn't seem like solid advice.",Neutral
Intel,"Now you are just backtracking.  You never mentioned anything about single core performance and the “52 cores” is what makes Nova Lake better.     Too early to assume anything now since Nova Lake is coming late 2026, and by then Zen 6 might be coming out also.",Negative
Intel,"Dude, let me simplify it : cores and core speed mean absolutely nothing these days, in the context of high end gaming. It'll take a decade for a CPU like 9800x3d to be a bottleneck due to either core count or performance in 4k.  The only context that cores matter is very specific workstation uses like video editing.  The only thing that matters in this discussion is Intel's answer to AMD's V-cache, and that's still in the realm of rumor / imagination.",Negative
Intel,"Maybe you haven't seen it before, but everyone else has. Also these new CPUs are projected to be released in late 2026 at the earliest, not 6 months.",Neutral
Intel,"No worries!  Yep, no upscaling! Just checked. Another factor that *might* be relevant was the Windows 11 optimizations that favored AMD Zen 5 especially, not long before the article came out.. so idk if it incorporated that or not.",Positive
Intel,"Your finances are certainly helpful here, in that you know that however expensive RAM plausibly will get, it won't prevent you from getting the system you want. That certainly takes the time pressure off.   You're having issues with the games you want to play. Your 9900k + DDR4 is causing some of that, for sure, it's not just X3D. You don't say what GPU you're running, so if it's not a fairly recent gen, that could be holding you back as well. If you do get a 7800X3D/9800X3D and a 80-class GPU, you'll probably have a much better experience. I wouldn't suggest going 5090 however, that's a total waste of money for most people that are gaming at 1440p like yourself.. though if you are looking for an experience that'll max out a current-gen 1440p OLED, then I retract my statement :) .. but, if your fps needs are more modest, just upgrade a 5080 to a 6080 in a year or so, you'll STILL be ahead, costwise.  You don't have to go pre-built and you don't have to build it yourself. I'm sure there's options in Australia to buy the exact parts you want and have them assembled for you, to your exacting needs.   1000W is plenty. 5090 stock is.. well, there aren't a lot of choices, rn, probably bc of the AI bubble. Also, people are still melting power connectors on 5090s these days, it doesn't seem worth the risk, to me.",Neutral
Intel,"Lmao that thread says 250hz is the best for that game, I can’t even make my mouse currently go below 500hz that is fucking crazy, I would assume it would eat into some of the CPU but to attribute the entire issue to mouse polling rate seems kinda odd, I run my keyboard at 8k polling rate and my cpu isn’t maxed out when playing games, and I’m looking at the individual cores usage not just task manager and like it’s only like a 6-10% CPU usage on my i9-10850k I would hope the newer chips can handle the better polling rate since it’s one of the only ways to get that responsive feel out of some games but idk stuffs crazy",Negative
Intel,"Ik. I tried all 4 RAMs on the A2 Slot and tested them without problems, same in B2. But if I put just one in A1 or/and B1 my system doesn't boot/post",Neutral
Intel,But with 4 my system sometimes boots and also detects the 64gb but there are some crashes. I guess the slots are just particular broken?,Negative
Intel,"The rumors are unclear about which Nova Lake SKUs will get bLLC and which won't, but I'd expect that to be on the high end parts. It could also depend on what AMD chooses to do: If they bring out the rumored 9950X3D2 at CES, and it does well, then Intel might feel they need to do similar with Nova Lake.",Neutral
Intel,Why wouldn't the new CPU have faster single core performance than todays stuff?  Do you think single core performance of Zen 6 is going to be slower than single core of Zen5?,Negative
Intel,"I wouldn't say that, core speed still does matter, and it's certainly not hard to think of games that could benefit.. like Civ turn times, for just one example.  As to a 9800X3D not being fast enough, check out HUB vids, they've seen cases where it bottlenecks a 5090. Either way, we're getting faster cores with Zen 6.",Neutral
Intel,"Hmmmm, on this video 7800x3d and 9800x3s compared with dlss and fsr4🤔  and there is no difference 🤔  Why? 👀  https://youtu.be/2k6Ikx8FLdY?si=2lVHi9Gw7tUBPWNy",Neutral
Intel,"I've got a Strix OC 2080ti, 64gb of 36mhz cl18 for Tarkov, the best m.2 drives I could find. It's old, but a beast, which is why I am hesitant to retire it. Twice now I have ordered PCs at night time then cancelled in the morning. It's not the money, that $2-3hr for entertainment I can justify, just for some reason I don't feel it's right! At the time it was just about the best thing you could get your hands on. I ordered a new AIO for bf6, see if that calms it down. Maybe my mid level 240mm isn't enough for a hot summer day anymore.   \> I'm sure there's options in Australia to buy the exact parts you want and have them assembled for you, to your exacting needs.  I didn't articulate this very well, and you're right, such services do exist; though the merchants have either bottom of the tree, or right at the very top of the tree. Neither of which I'm interested in!   I'm considering the 5090 because of the vram, and use some LLM on it, or go 5080 for games and stick the new huge amd ai card on it. I am spending a fortune on Claude, ChatGPT and Gemini each month for work. Annoyingly, they all do different things well. I did some looking around and apparently some of the LLM on huggingface is pretty good.   I was complaining about software that I use in my business (Lightspeed/Shopify/Weebly) a few months ago to a mate of mine who's a software developer. He convinced me AI would help me write little apps/plugins/scripts that would fix my problems, and sure enough they have. But every AI does things a bit differently and when I've struggled with one, another has got it sorted. I purchased an old Dell T620 for a couple of hundred bucks that has obscene amounts of DDR3, and storage, AI taught me how to run proxmox, so I have built small apps running on containers which simplify life, few game servers for friends too. I am planning on commercialising a couple of those next year during my quiet period, so if there's decent LLM that would run on new hardware, that's $1500/yr in subscriptions I'd save.   Thank you for your time and insights, I welcome and more you have, wish I could get you a beer.",Neutral
Intel,There's practically no difference between 8k and 1k polling. 1k is 1ms. You'd need 1000 fps to match that,Negative
Intel,"Oh, in that case, yeah, you have a motherboard or CPU problem.",Negative
Intel,"In any case, the 52 core chip is not getting bLLC. It wouldn't make sense, because 52 cores are useless for gaming.  Threadripper CPUs already have a bazillion cores, and no V cache because it would be conflicting two very different uses of the hardware.",Negative
Intel,"Check out performance of Arrow Lake vs Raptor Lake.  It actually performs worse despite being newer:  https://www.techpowerup.com/review/intel-core-ultra-9-285k/19.html   Even if Nova Lake wind up showing improvements, what percentage would be worth it?  Should people wait a year just for a 1% increase?     I do believe Zen 6 will be better than Zen 5 because AMD has a good track record of improvements each generation, but it doesn't mean AMD wouldn't screw it up when they actually release it.     Anyway you are just talking in circles.  You originally said why people aren't waiting for Nova Lake and I precisely explained why not.  Nothing else you bring up is relevant.",Negative
Intel,">they've seen cases where it bottlenecks a 5090.  lol, where? At 1080p Minecraft?   Link please.",Neutral
Intel,"That was really interesting. Did you notice that the places where there was little to no difference, was where the game was GPU bound? Makes sense, right? But there was a few games where it wasn't GPU bound, and there you see the expected 20% difference.  What surprised me was that it claims it was running all those at 4K + DLSS Performance, which should see roughly 1440p native equivalent results.  Now, to what degree the X3D cache played a part in all that, we don't know. Unfortunately, whoever made that video, didn't test with something like a 7600X.",Positive
Intel,"I've got that sort of RAM in my 12700K system beside me right now! (64GB of DDR4-3600-CL18)  Heh, I don't think an AIO is going to get it done. You have a CPU that's not up to the intense low-thread demands that some modern games demand, and the RAM isn't helping either.. I mean, it's good for DDR4, but you're losing around 20% performance compared to DDR5. Simple as that, really. Perhaps retire it to some other use?  I admit that LLMs like you're involved with, aren't my area of expertise. That said, I do occasionally watch [this Youtuber](https://www.youtube.com/@NetworkChuck/videos) who can be entertaining and has covered some LLM stuff that you might find useful. I'm novice enough in that area that I can't properly judge if it is.   LLMs.. I have complicated feelings about those. Net harm I think, but ofc I don't get a vote, only billionaires do.   Glad I could help! Beer is good! :)",Negative
Intel,Please not CPU 😂 I try to believe that it has to be a motherboard problem 😂,Negative
Intel,"That’s not what the rumors, that all of the Zen 6 and Nova Lake info we have rn are based on, say. The 52 core chip will have it, the 28 core will have it, and ofc there will be plenty of SKUs that won’t have it, just as on AMD Zen, lots of SKUs don’t have X3D cache either.",Neutral
Intel,"Well, yeah, and this guy has a raptor lake i5, so I'm expecting the new Nova Lake to be far better since its being made by TSMC.    I said to wait for AMDs answer to Nova Lake, not Nova Lake.",Positive
Intel,"No, not 1080p Minecraft, lol. Idr which specific vids offhand, but it wouldn’t be too hard to go look, and I think it’s even been fairly recently. Perhaps I’ll track it down for you later today.  Even so, like I said, Civ turn times is one very obvious gaming example where “faster the better” cores is an advantage, bc it means less waiting on a large/busy map.",Neutral
Intel,"Heh.. I mean, it *probably* is a motherboard problem! You could get a cheap (even used) low end CPU for that socket, and swap it in and see. That way you’ll have your answer. If it still doesn’t work, at least you’ll have an “emergency spare” CPU.",Neutral
Intel,"As I recall, and I may be misremembering, only part of Nova Lake is made by TSMC, the rest of it isn’t. Pricing is also a question, I haven’t forgotten how Gelsinger pissed off TSMC leadership, though perhaps Lip Bu Tan has repaired that relationship, and gotten better terms (pricing), idk.   Nova Lake may end up being awesome, but if their 9800X3D-performance-parity CPU SKU ends up being priced at $700 (for instance) in order to have the margins Intel needs, that’s going to be a real problem for them. So we will have to wait and see how things play out.",Neutral
Intel,"I'm not super experienced here myself really, but just thought I'd ask, what's the rationale between the Arc gpu as opposed to Nvidia or Amd?",Neutral
Intel,You really don’t need the sound card for gaming anymore. On board audio should be more than fine.,Neutral
Intel,you might want to check the price of the RAM to adjust your budget if that will affects you or not.,Neutral
Intel,Btw the real price of that RAM is over $400 on amazon.   Also why the Intel GPU? I would go with a 5060 ti 16gb or a 5070 at that price range,Neutral
Intel,I would go with a amd gpu myself the rx 9070 xt is 560 at microcenter,Neutral
Intel,"9950 x3d is only like 150$ more than your current processor and would be probably the best available cpu for a workstation/gaming rig. Might be overkill, but just a thought.  Edit: 9070xt is also another possible upgrade. Currently using one with a 9800x3d for gaming only, and it's stellar.",Positive
Intel,"Graphic Design doesn't really benefit from 16-cores.  The A770 is a bad choice, just a slow GPU for your use case.  Sound cards are pretty much obsolete for regular users. And professionals generally use external audio solutions.  Don't pay $429 for that amount of RAM.  Windows 10 is end of life (officially). Just get on windows 11. If you already have 10 retail, you can transfer the license (and go to windows 11).  Are you in the states? It's easy to squeeze so much more value out of that budget.",Negative
Intel,Get a 9070xt bro,Neutral
Intel,"Video editing for quick sync could be a potential reason. If you have an AMD cpu, you lose out on the Intel quicksync, which can be helpful.   Although you could technically just rock two gpus...",Neutral
Intel,Okay that's great to know. I think I added that as an optional part anyway. I have a strong creative spirit and want to dabble in music production also and considered it for that.,Positive
Intel,"To be honest I posted this in other communities like 7 months ago when I started researching parts and got no replies. Back then it really was like $150, I hadn't realized it went up so much since 😔   As for the GPU, I didn't have a preference for the GPU and think I was just picking high rated compatible parts. I'll look into those options and compare.   I really don't know hardly anything about parts outside of more RAM = less headaches (hopefully). I'm probably overshooting my specs just to cover my bases and hopefully build a PC that's future proof as well.",Negative
Intel,"Appreciate the suggestion and will look into it, thanks!",Positive
Intel,"Hi there, your comment has been removed due to our no grey market / piracy rule (**[Rule 3](https://www.reddit.com/r/buildapc/about/rules)**).  It seems your comment mentions something that seems to be a known gray market or piracy resource. Due to this, we are unable to allow your comment on r/buildapc.  Thanks for understanding.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/buildapc) if you have any questions or concerns.*",Negative
Intel,"/u/Fit_Opportunity89/ Here is an alternative build more geared towards gaming without sacrificing on graphic design performance.  Also has a much better monitor that's currently great value.  For graphic design you'll obviously need to calibrate it properly - though it does have a decent sRGB mode.  I didn't looks at any of the other peripherals, they are much more personal.  [PCPartPicker Part List](https://pcpartpicker.com/list/tRD7fd)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 7 7800X3D 4.2 GHz 8-Core OEM/Tray Processor](https://pcpartpicker.com/product/CzZWGX/amd-ryzen-7-7800x3d-42-ghz-8-core-oemtray-processor-100-000000910) | $359.99 @ iBUYPOWER  **CPU Cooler** | [Thermalright Peerless Assassin 120 SE 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/hYxRsY/thermalright-peerless-assassin-120-se-6617-cfm-cpu-cooler-pa120-se-d3) | $34.90 @ Amazon  **Motherboard** | [Gigabyte B850 GAMING X WIFI6E ATX AM5 Motherboard](https://pcpartpicker.com/product/9xYfrH/gigabyte-b850-gaming-x-wifi6e-atx-am5-motherboard-b850-gaming-x-wifi6e) | $149.99 @ Amazon  **Memory** | [V-Color Manta XSky RGB 32 GB (2 x 16 GB) DDR5-6000 CL36 Memory](https://pcpartpicker.com/product/BZKnTW/v-color-manta-xsky-rgb-32-gb-2-x-16-gb-ddr5-6000-cl36-memory-tmxsal1660836kwk) | $207.99 @ Newegg  **Storage** | [Western Digital WD_Black SN850X 2 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/crKKHx/western-digital-wd_black-sn850x-2-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-wds200t2x0e) | $167.99 @ Amazon  **Video Card** | [PNY OC GeForce RTX 5070 Ti 16 GB Video Card](https://pcpartpicker.com/product/XRvscf/pny-oc-geforce-rtx-5070-ti-16-gb-video-card-vcg5070t16tfxpb1-o) | $749.99 @ B&H  **Case** | [Montech XR ATX Mid Tower Case](https://pcpartpicker.com/product/fc88TW/montech-xr-atx-mid-tower-case-xr-b) | $69.90 @ Amazon  **Power Supply** | [Montech CENTURY II 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/sqbypg/montech-century-ii-850-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-850w) | $82.90 @ Amazon  **Monitor** | [Asus ROG Strix XG27ACS 27.0"" 2560 x 1440 180 Hz Monitor](https://pcpartpicker.com/product/mgWJ7P/asus-rog-strix-xg27acs-270-2560-x-1440-180-hz-monitor-xg27acs) | $189.00 @ Amazon  **Keyboard** | [Tecware Phantom RGB Wired Gaming Keyboard](https://pcpartpicker.com/product/BWqhP6/tecware-phantom-rgb-wired-gaming-keyboard-twkb-p87zobr) | $41.99 @ Amazon  **Mouse** | [Razer Viper V2 Pro Wireless Optical Mouse](https://pcpartpicker.com/product/Qt2WGX/razer-viper-v2-pro-wireless-optical-mouse-rz01-04390100-r3u1) | $74.98 @ Amazon  **UPS** | [APC Back-UPS Pro 1000S UPS](https://pcpartpicker.com/product/nCPgXL/apc-back-ups-pro-1000s-ups-br1000ms) | $194.99 @ Amazon  **Webcam** | [Anker PowerConf C200 Webcam](https://pcpartpicker.com/product/czt9TW/anker-powerconf-c200-webcam-a3369) | $47.49 @ Amazon   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$2372.10**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-11-25 18:41 EST-0500 |",Positive
Intel,"NVidia 50-series added 10-bit 4:2:2 decoding natively to their GPUs and shred intel's on board graphics, which makes AMD a fine choice.",Positive
Intel,Also Msi has a deal on a 1000 psu and you get a 30 dollar steam card. Just check out their website,Neutral
Intel,"Thanks for your suggestions and the alternative build list  really appreciate it and will look into these. I originally made this parts list months ago, so I'm not surprised that some of my picks are now over priced or simply no longer the best option.",Neutral
Intel,"I'm waiting on them to make something that can replace my 3070ti, the minimum performance I want for me to justify an upgrade would be a 9070xt / 5070ti  Intel, i'm waiting, and i'm willing, so show me what you can do.",Positive
Intel,https://preview.redd.it/l09xx3bhg91g1.jpeg?width=1846&format=pjpg&auto=webp&s=85c81cc5c09ff61412c6b72e85e82902ce185990  Does it count?,Neutral
Intel,"I was considering getting an Arc, but it didn't seem like a good fit for an AM4 board. It was also kinda too low end to be a worthwhile upgrade from my previous card, while the 9070 XT was one.",Negative
Intel,"I would've got the b580, but it only has 8 lanes.  So I got the 9060xt.  I'm playing on a gen 3 system, so l would loose a bit of performance.",Negative
Intel,"Not touched arc yet, waiting for them to compete at the mid-high end before I take a serious look",Neutral
Intel,"""yo intel arc is fucking horrendous""   - xXDÆTHZlay3erXx  (RTX 6000 PRO, 9950X3D,MSI GODLIKE X870E, 96GB 8200mhz)",Negative
Intel,"Got a B580 at a great price, small upgrade from my 6700XT which went to another build. Great 1440p performance paired with my 5900X.",Positive
Intel,The $250 price point is compelling to me for the specs.   But does it handle games well? That's what I want to know; $250 is a pretty big purchase for me right now.,Positive
Intel,"I mean i sort of do? My ultra 7 155h laptop has an arc igpu, if that counts",Neutral
Intel,"A770 Owner here. Was between 4060-7600XT-A770 this year, video games was a second focus but wanted to go more for productivity (in a budget). 4060 was fine, but the 8gb VRAM make me go for the other 2 options, mainly if I get to edit very big things in high res., it would be hard to do.  And between 7600XT and A770... The A770 gets 4070 performance on video edition, and Intel codes are far better than AMD. Gaming performance they are almost the same... So far very happy with my purchase.  Since the start of the year, doing the same workflow, with driver updates I got to shave a total of 5 minutes on each video render. At the start was 20m, rn is 15m with the same type of edition. And driver bugs? 0, enever needed to reinstall drivers.",Positive
Intel,"They don’t make anything powerful enough for what I want out of a gaming PC but they’ve made strides and if they keep it up, I could see them continuing to improve, maybe having xx60 competitors in a couple of years.",Positive
Intel,"I love having more competition but I'm not touching it until they make a high end model, I just got a 5080",Positive
Intel,"Had a A770 and it was great, only reaon I moved away was VR. If they could support VR I would go back. Prices are better, it worked really well, the software was great and dev support was amazing.  Halo Master Chief Collection wasn't running well. Raised a support ticket and in a month they fixed it.",Positive
Intel,My sons PC has an A770. Never really had any issues with it at all.,Positive
Intel,"I have an Arc A750, and I'm pretty happy with it so far. I mainly play older games so I can get 60-100fps on 1440p ultra.",Positive
Intel,"Been using B580 with 4K monitor for a while now. 7500F as CPU. With modern upscaling, it is very capable.",Positive
Intel,I use one for an AV1,Neutral
Intel,"I use an A770 16GB for gaming on the 4K TV. Mostly older or lighter games, but in Forza Horizon 5 with the help of XESS there's no problem with high-ultra settings in 4K. hitting rock solid 60fps",Positive
Intel,I have a A770 and I'd say it performs about on par with my 3070 for most games I play. Honestly interested in seeing what more they can do with better hardware design and continued work on improving their drivers.,Positive
Intel,Plex server,Neutral
Intel,"A770, really liked it for it's price point, even though it was a pain at first. Solid at 1440p.   I'll get the b780 if it ever releases.",Positive
Intel,I've got the A770 16 gig and it seems fine.  Hard to tell with games if it's my GPU or the game being next gen ready. I came from a gtx 970 so it felt like a massive upgrade. I remember Balder's gate being real low fps in act 1 on the 970 but very decent on the A770 at 1440. Hunt showdown also gets 90+ fps in busy areas but has had issues dropping to very choppy fps when things are happening close. With Hunt it's very hard to tell what the issue is as it has frequent issues that effect many gpus at the same time. Like a friend playing on amd had different issues at the same time as I did. Honestly couldn't tell you if I've turned the graphics up from low since I had that issue. A770 runs armored core 4 really well to the point that I haven't had to mess with any settings or check fps.,Positive
Intel,"Been using my A770 since it launched. It's been great for a majority of the time, but the beginning was rough.",Positive
Intel,The a750/770 wasn't bad for gaming and neither is the b570/580  I use an a370 for transcoding video and it does a great job.,Positive
Intel,"I have a b580, and I absolutely love it. It is the perfect entry level 1440p card.",Positive
Intel,![gif](giphy|kSlJtVrqxDYKk|downsized),Neutral
Intel,"i have a b580. i just hate nvidia (and also use linux).  nvidia on linux is a pain in the ass. its getting better but i cba. i bought it a couple months ago. it was either amd or intel arc and i decided to just go for it.      Intel arc on linux is quite literally just plug and play. I can't express how much I love this card. i hope intel stays in the gpu market, but seeing recent news the situation seems dire.",Negative
Intel,B580 in my guest computer. Haven't had any issues with it playing all the LAN games my cousin and I play together every week.,Positive
Intel,"Been using an A770 for over two years! I definitely had my fair share of issues, especially early on, but over all it's been great",Positive
Intel,i'm using [intel arc](https://www.reddit.com/r/pcmasterrace/comments/1ouj4en/im_glad_i_use_a_mesh_case/) and i'm very satisfied with it,Positive
Intel,"I've got two, an A310 and a DG1.  I have yet to get the DG1 working though.",Neutral
Intel,"Here, just finished my build yesterday.",Neutral
Intel,A770 16GB LE here.,Neutral
Intel,woulda bought it if not for vram alone,Neutral
Intel,If they had a card that could compete with Nvidia and AMD high end I would consider it. At the current performance tier with B580 as the best card it doesn't interest me.,Neutral
Intel,"I love to not buy Nvidia, just as soon as someone makes a better product",Positive
Intel,Had a a770 in my Linux but sadly I had to many issues getting it to play certain games and running it in my main  rig which at the time had a 6700xt it ran worse than the 6700xt so now it’s just sitting in it box not sure what to do with it,Negative
Intel,is it good for rendering  ? like blender/unreal  stuff  ?,Neutral
Intel,At the very least it is damn good value for money.,Positive
Intel,"I don't use it, but I appreciate there being more options available for people.",Neutral
Intel,It's okay at best,Positive
Intel,B580 at 1080 so far so good. Can lock 60fps ARC Raiders max settings or drop down from max a level and 120fps,Positive
Intel,"I picked up a B580 for stoopid cheap secondhand and am using it to learn about local LLMs on my linux rig for future home automation. Works fairly decent, but my benchmark options are limited on linux for now. It runs a bit warm at idle without an undervolt (55° - 60° C).",Positive
Intel,If they ever release a single fan battlemage card akin to the a380 I'd buy one for my sub 5L build for sure... Tempted as it is by the sparkle a380 but I keep hoping for better especially since the b50 pro is a thing.,Positive
Intel,"Not me really, irs rare to find one here also",Neutral
Intel,It’s great for a budget build but I wanted something higher-end so I went for the 9070XT,Positive
Intel,I wish I could. But my friends have outspoken this idea from me. Ended up with a laptop and RTX card in it.,Neutral
Intel,For me the problem is that Intel's best offering is about as fast as 4060. I got no use for that.,Negative
Intel,"I would swap to ARC, but the rest of my system needs an upgrade first. I don't even have ReBAR support.",Negative
Intel,"Was looking at an intel one of them for a long time, until I realized most FEA & CFD software suites require nvidia graphics cards and so that killed that idea.",Negative
Intel,I am interested but I don't think it is worth replacing my 2080 Super.,Neutral
Intel,lol,Positive
Intel,"I just got an a380 for 120 and put it in my plex server.  Love it, barely spins up for 4K to 1080p transcodes including HDR to SDR mapping.  Makes zero noise.",Positive
Intel,"Yes, got a B580 for MSRP, and put it into my custom Steam Machine!",Positive
Intel,"I mean, is it? While the price to performance *seems* to be good, from what I’ve heard, the drivers kind of suck, and it doesn’t really work well with actual budget CPUs.  It’s good on paper, but it’s not really a mature product yet. I’d guess that by the next generation, they’d be more caught up with the competition at that price range.",Negative
Intel,"I have one in my jellyfin and its been amazing doing 3 transcodes at a time and not sucking up too much power, along with some misc server tasks like trickplay. Looking into the b580 for my personal computer currently. I've heard people say the drivers aren't good but personally I havent really had a problem with them.",Positive
Intel,"I have an A770 in my rig, genuinely never had a problem unless it was Fallout 3",Positive
Intel,I had a B580 near launch. Was my replacement for a 1080ti at the time. It was totally fine for my use case. I did upgrade to a 4080 later but that was more due to my use case changing (built a sim racing rig with a large format 4k TV) than the performance of the card itself. I was also quite partial to the understated looks of the card,Positive
Intel,"Ive used plenty of Arcs and have enjoyed the performance and compatability.  The 1 weird thing is in one older game I played, the game defaulted to my PCs AMD iGPU, and wouldnt run on the Arc without disabling the iGPU in the BIOS.   Probably just an issue with the engine only knowing to expect an AMD/Nvidia GPU",Positive
Intel,You think it would be the cheapest option to make a steam machine equivalent?,Neutral
Intel,I had the B580 for 4 months then switched to a 5070.,Neutral
Intel,I downgraded from a 6950xt to a B580. honestly don't notice much if any difference in the games i play and the power consumption is signifcantly less.,Negative
Intel,"b580 user here. paired with a 5700x3d.  not much to complain about when it comes to gaming performance. HOWEVER, video encoding is an absolute no-no with the b580. (streaming on discord or recording with hardware encoder in obs absolutely kills gaming performance.)",Negative
Intel,"I have one in my shop computer.....I have a gpu from all three in use. A 6800 in my wifes rig, I have a 4080, and the shop has the 580! Guess where all the gaming happens?",Neutral
Intel,I don't run arc on my personal system (have a 7800XT so it'd be a downgrade) but I build a budget system for my little brother using the B580 and he seems to like it,Positive
Intel,"Upgraded from 1070 to b580, and have been very pleased with it. I mainly play dota, arpgs and do some photo editing in lightroom, and thought the b580 was the most priceworthy option for me!",Positive
Intel,"We do. Arc 380. Best thing you can use for a Plex server.   Side note. I can't get DLSS to work on Battlefield 6. Before, it would hard crash. Now it just turns everything greyscale red. But XeSS works.",Positive
Intel,I would have happily gotten one if I didn't have a card more powerful already. I'm a big fan of what intel is doing so far in the GPU space.,Positive
Intel,friend uses it on our private emby server for transcoding.,Neutral
Intel,Decent choice for some price points but I wouldn't call it good good. It's definetely good for the market though. Waiting for some mid-high end moves,Positive
Intel,I would 100% do it if I were in a situation of needing a 250$ GPU. Sadly Arc doesn't cover any other price category except if you wanna buy an old-gen card,Neutral
Intel,"A380 used here for my Plex Server, perfomance is incredible for transcoding media, especially given the price.",Positive
Intel,"i would gotten them but can't find any, only used one and even then, still pretty expensive since they are shipped from overseas, not in stores",Negative
Intel,Intel cards aren't a bad deal for entry level rigs and I hear battlemage is much improved from the A generation cards.,Positive
Intel,It’s a good graphic card. I have the A750. Runs oblivion remastered decently well at 1080p,Positive
Intel,I got the b580 for my wife and it’s great she plays flight sim asserts corsa and I got the free bf6 from the deal which also runs great on b580,Positive
Intel,"Speaking of, I'm planning to get B580 to replace aging 5700XT for mostly encoding to HEVC and light gaming on 1440p (Paradox games like EU5, CK3, Vic3, maybe F1, or other sports games, nothing competitive). Budget is 250 eur up to most 400 eur. For reference, I can get B580 for 250-270 easily, RTX 5060Ti 16GB for at least 430, so it's out of scope.  For US people, this is with VAT 20% included",Neutral
Intel,"I'd like to get one, but I'm waiting for it to be better than my 7900XT.",Positive
Intel,I have one I bought to leave sealed as a collectible. I did buy another A770LE for personal use but sold it to a friend.,Neutral
Intel,I got the Intel A580 last year at microcenter for $90 ☺️ I only game at 1080p medium setting,Positive
Intel,"Just built my first pc with an arc b580 12gb and I'm loving it, been playing both flat-screen and vr and it rips",Positive
Intel,"As soon as they optimize the performance in dx 11 and lower. Efficiency of battlemage is nice but drivers need the aforementioned tinkering. For now planning to go 4070, solid midrange choice and better than 5000 series (proper cuda, hotspot sensor is a must, dont care about dlss or framegen because i never use these).",Positive
Intel,It's fcking terrible.,Negative
Intel,Fantastic transcoding cards for servers.,Positive
Intel,My 4070ti scoffs.,Negative
Intel,"I considered for my latest build, upgrading from a 980ti.  But I was concerned about support and drivers and ended up finding a 9070 at msrp instead.  I would definitely consider Intel GPU's in the future though as they mature.",Neutral
Intel,"Technically I use one, but not for gaming. I run a PleX server on a dedicated PC and I use an Arc A380. It's a beast for that usage, especially at that price point.",Positive
Intel,Not me but I welcome choice in the gpu space. Sick of nvidia and amds pricing,Negative
Intel,I want one for av1 encoding pbad. But other than it sitting in my server for that one use case it wouldn’t get used.,Negative
Intel,Bad compared to a 5090. Sure.  Bad for price to performance. No.  My main problem with arc is that its still early days. If they make another generation assuming the rnd team survives lay-offs I might pick one up.,Negative
Intel,"I don't. I'm all AMD, but I do like what they're bringing to the market. Would love to see them push out something to compete with the high end from Nvidia or AMD.",Positive
Intel,"I have one in my server, best media engine for transcoding on the market, for real! And linux support too!",Positive
Intel,"I sometimes see the B570 at 250 USD (including import taxes and stuff where I live), but since I'm on AM4 I'm not sure if it's a good upgrade when I can save 50 USD more for a 5050",Neutral
Intel,"I've only used the Arc 140m for a little bit, but it was shockingly good.",Positive
Intel,"I use Intel gpu, just happens to be iGpu of the i3 2nd Gen (always thought it was 3rd but it's 2nd just found out today",Neutral
Intel,"I have a B580 Steel Legend, a super cool card, especially for budget builds. I paired it with a 5600x and I can run pretty much anything.",Positive
Intel,I current have the Intel Arc A770 LE. It's been running fairly well and hasn't caused too much problems with it. I def look forward to more software updates and see how well it'll perform overtime through its driver development.,Positive
Intel,Nobody was selling it when I build my current pc and I didn't feel like smuggling one trough the border,Neutral
Intel,They got to make something at at least the 5080 level for me to even consider an Intel card. What they have now can't handle native 4K.,Negative
Intel,"If i ever build a pc, i will go for arc",Neutral
Intel,"Technically yes. Not in my main gaming rig, but my Claw is Arc.",Neutral
Intel,"I've had my 750 in a proxmox server doing passthrough for various things since it came out. When I first got it, I had lots of driver issues of course but that got smoothed out.",Neutral
Intel,its ok,Neutral
Intel,Great if you don’t plan to do anything other than game and don’t need hard hitting production equipment.,Positive
Intel,"I've got one, but it is used primarily to transcode videos on my jellyfin server.",Neutral
Intel,Have a ARC A770 16gb in my gaming server. One of these days I'll have to take it out and throw it into one of my other rigs to see how it does gaming.  https://imgur.com/gallery/CsqR093,Neutral
Intel,I put one in my dads PC .  He only plays 10 year old games and it works perfectly.  No issues with it ever.,Positive
Intel,https://preview.redd.it/ctca68qjjb1g1.jpeg?width=9248&format=pjpg&auto=webp&s=797dea53fd3cf9096881d0a22fc85aab978d821a  My bros system but i guess it still counts.,Neutral
Intel,I wanted to but scored a deal on a 6700 for 160 so couldn't pass it up,Positive
Intel,I got a cheap one as a secondary GPU to drive my secondary monitors,Neutral
Intel,"Used A770 le 16Gb for about 2 years, very good gpu. Heating a bit  too much and too fast in 4K gaming, managed to deliver 4K high quality at 30-60 fps in most of the games. Now switched to Radeon RX 9070XT , this one doesn’t struggle with 4K towards 120 fps, and the ARC770 Sitting in her box for resting.",Positive
Intel,The CPU overhead issue on Intel still seems way to bad to consider it over AMDs entry level options instead imo.   It's great if you have a high end CPU but kinda falls apart with a CPU you'd expect to pair with it.,Negative
Intel,When my 3080ti starts showing up in the bottom quarter of performance graphs I’ll look then. But it’ll be hard to justify enriching Jensen anymore. AMD has my vote.,Neutral
Intel,I don't yet. I really want to though!,Positive
Intel,This meme format is mid at best,Negative
Intel,It the lanes are only 8 a piece can I run two Arcs,Neutral
Intel,"I used an a770 for a while, I liked it but it just barely wasn’t enough for the quality level and target fps in games I wanted to play, so I went back to the older more powerful card I had before. The hdr on the arc card was very pretty though.",Neutral
Intel,"Grabbed the b580 for 235, replacing my 1660s, absolute world of difference, coincidentally my monitor is not 144hz as i previously thought... its 165hz lol rdr2 is amazing 90fps at 1440p, bf6 same story.",Positive
Intel,"I love my Bifrost A770 16gb, but I wanted to play vr and scored a refurbed 3080ti Fe for $450……..",Positive
Intel,I was considering getting one to upgrade from my 3060 12gb when i had it. Got lucky and scored a 4070 for cheap or i wouldve bought one,Positive
Intel,"I built a couch gaming pc and im currently using my old 1080 in it, im really hoping they release the B770 so i can buy it to support arc. Imagine running AMD CPU with Intel GPU lmao",Positive
Intel,I love my b580,Positive
Intel,https://preview.redd.it/6yd6j8wx7c1g1.jpeg?width=4624&format=pjpg&auto=webp&s=d0a6420d49f4b717483a033444d22db1573743ce,Neutral
Intel,Next card I buy will be an Intel GPU for sure. Nvidia is pretty scummy and there's no way I'm putting an AMD anything in my computer lol  EDIT: I'm still very happy with my RTX 3070. It's still more than enough for the games I like.,Negative
Intel,"I recently bought a laptop with the Ultra 7 155H in it and that includes the Intel Arc iGPU. It also has a dedicated RTX 3050Ti, I have been surprised that leaving the RTX disabled I'm still able to emulate PS2 and 360 (Xenia) perfectly. I only use the RTX for heavier Steam games due to heat output and power consumption.  Also, when encoding 1080p video h.265/MP4 container in Handbrake using hardware acceleration the Arc is only 60-90 frames per second slower than the RTX.   I've been impressed I may check out Arc desktop options.",Positive
Intel,A750. 👍,Positive
Intel,Intel ARC A310 in truenas for AV1 encoding.,Neutral
Intel,"Not at the moment, but probably in the near future.   Need some affordable AV1",Neutral
Intel,I have a Sparkle Titan b580 in my server for Plex. I’ve never used it for gaming.  Technically I use Arc in my MSI Claw 8 AI+. Works great in that.,Positive
Intel,it kicks ass in my pley server A380,Negative
Intel,If they can hit high end Nvidia gpus then sure. I'm not a mid range boy,Neutral
Intel,Was actually looking at their GPUs yesterday. Performances are quite good for the price. Maybe they should focus on GPUs more? Their Ultra chips dont come close to AMD.,Positive
Intel,"Threw one in my son's computer, was a good bit cheaper than the equivalent Nvidia card and still a bit cheaper than the AMD.  Doom TDA is the most intensive game he plays and it has no problem with it so that's good enough for me.",Positive
Intel,"Have a b580 & so far so good. Bf6, cyberpunk, sm2, EfT, etc all run great.   I'm not frame chasing, as long as the games I play run with no stutter lags & look good I'm happy. I don't need to see the absolute detailed zit on a character's face in their stubble beard at 4k ultra settings at 196fps.   Would definitely like to see what a beefier card could be if they made one. You get a solid amount of vram, low price, & no burning cable. I hope it has a 1080ti lifecycle. But also waiting to see how much the C, D, etc improve.",Positive
Intel,A750 in a second build. Great for 1080p in most games,Positive
Intel,It's an amazing value. I used one after my 4090 and was kinda shocked how well it played COD and Warzone.,Positive
Intel,"Got an Arc B580 for the price/performance ratio, coming from an RX 5700 XT. Was a good bit of an upgrade in a lot of newer games. Honestly think it's a great card for that purpose.  Unfortunately, I ran into poor performance that was a no-go for me. Yakuza Kiwami had a huge issue (ran at like 50-55 fps with horrible lows that made it jittery, while my 5700 XT easily pushed 144, which makes sense, it's basically a PS3 era engine with better assets). From what I can tell, 0, Kiwami 2, and YLAD all had the same issue, and they got fixed over the course of a few years. The problem with that is that 0 was reported on the forum like 2 years before it was fixed. I was in the middle of a playthrough and couldn't just wait for reasonable, playable performance. I even tried DXVK and got a big boost (like 90-100ish), but it was still very unstable.  So you really do have to beware of older games still, despite the drivers having improved a lot and being overall solid. Some other DX11 titles ran perfectly fine though.  OpenGL performance? Forget it. Necesse uses Java and OpenGL and I managed about 70fps before using Mesa for Windows to get a boost. Still wasn't ideal compared to my 5700 XT, once again (and AMD isn't known for OpenGL performance on Windows lmao).  So overall, I think it's a great piece of hardware with mostly solid drivers, but it's not good enough for me on the driver side and I don't want to wait. It lasted 2 weeks before I returned it and got a 9060 XT 16GB (which is again a good bit of an upgrade from the B580, so the extra cost doesn't hurt as bad. I've been waiting for so long for an upgrade that made sense to me).  This was all in the middle of last month. Got my 9060 XT on the 30th.  I'd also like to note that they did alleviate most of the CPU overhead issues recently, so my 5600G wasn't an issue in games that did run well, like The Finals (actually ran extremely well with maxed out settings, and XeSS is actually really good on the Arc cards).",Positive
Intel,"If I haven't got the RTX4060 LP, I would definitely vuy the B580",Neutral
Intel,"I used a B580 as a media machine GPU for a bit.  The encoding was a little jittery when starting a file or stream, but smoothed out acceptably after 2-5 seconds of play.",Neutral
Intel,Idk if an arc b580 would be good for me since Im still running a i7 6700k with a z170 mobo. But I have an Rx 580 that I've been wanting to upgrade from.   I fell like I need a full system upgrade to take advantage of any significant GPU upgrades.,Neutral
Intel,I have it in my NAS and for plex transcoding its a godsent. It just works.  Do be honest for my main rig it will not be an option for a long time. NVidia has with Cuda just to much stuff that i use daily.   AMD is years behind sadly.,Positive
Intel,I'd buy any companies gpu if its good price and good performance.   But I feel like Ibwould be a beta tester at this point.,Positive
Intel,Arc team reporting!,Neutral
Intel,Replaced my 5700xt with a b580.   Performance is equal or slightly better at 1440p ultrawide.  The 5700xt was running out of VRAM in some games though and I was having very frequent driver crashes too.  The b580 has actually been a smoother experience.  The 5700xt has moved into a pc that had a 1050ti so it loves on being a very capable 1080p card,Positive
Intel,I use the *word* arc. Is that the same thing?,Neutral
Intel,I love these posts. Seeing the joker instantly tells me not to take it serious.,Positive
Intel,"Personally just not there yet for me, what they've offered has been interesting and I think intel should continue developing and release discrete gpus because I do believe they have a chance of rivaling amd and nividia for a share of the market. If they only keep improving on this",Positive
Intel,"Absolutely good, but also absolutely not great. I'd probably still rather just use an AMD card/APU FOR 1080p only because of driver optimization.",Negative
Intel,They are great entry level gpus.  They are NOT great GPUs,Negative
Intel,"Planning on getting the Intel Arc Pro 16 GB for my server, for some video encoding. Such a monster for a little 70w card with 16 GB VRAM.",Positive
Intel,"I friend got an arc A750 for his PC. I think benchmarks showed around similar performance to a 3060 but I could be wrong. Anyways, it was only like... 270 bucks brand new. Looks fuckin sick too.",Positive
Intel,"My system is running i5 13600k and A770 (sparkle titan) for almost 3 years. Ngl, the GPU does exactly what I needed - video rendering in Premiere Pro and casual gaming. The drivers has been improved significantly.   Though, currently I'm not thinking about any upgrades, I might upgrade later - to newer GPUs from Intel.",Positive
Intel,"Been using the A750 since launch, got it for free during the Scavenger Hunt they held. It's been good so far, only recently showing signs of struggle as I'd like to enjoy Borderlands 4",Positive
Intel,Didn’t NVDA recently buy up a massive stake in INTC? Intel Arc is unlikely to survive the next 5-6 years.,Neutral
Intel,"arc a770, it crashes alot",Negative
Intel,It would have been good if Intel didn't scrap the idea for something better than the B580 because if we were to have the B750/770 there could have been a more affordable option to compete against the RX 9070 and RTX 5070 but of course Intel got rid of that notion and we would have to wait for the next series of GPUs if they do plan on making one which would most likely compete against the RX 9060 XT and RTX 5060 Ti 16GB cards.,Neutral
Intel,"I got one, it's meh in performance and drivers, but is heck of a good deal when it's on sale. Can't beat the FPS per money spent.",Positive
Intel,"Hell yea it is, my MSI Claw 8 is killing it out here.",Positive
Intel,"I have three across two rigs. They may not the best, but I can do everything I want at pretty good settings and barely cracked $500 for all.",Positive
Intel,"The B580 is scratching an itch I cannot even begin to describe in adequate terms.  It is the default card for my 2-gamer builds now.  Since it's an 8x card, any consumer grade motherboard with two x16 slots that split the primary slot lanes when both are populated ultimately does not suffer a dip in performance.  Plus the linux friendly nature of the cards means that I can pack a double-bazzite PC in my bag and take anywhere.  It makes road trips and game nights significantly easier.",Positive
Intel,come on this is a stupid question... everyone knows 99% of reddit is on 5090's. Hadn't you seen the Nvidia subreddit,Negative
Intel,Driver issues and compatibility in games frightens me. On paper they look great for the price but I'm worried.,Negative
Intel,Good how? And for what? We talking mid gaming here?,Neutral
Intel,Okay ngl this made me laugh so hard all of a sudden 🤣🙏🏻,Positive
Intel,Intel Arc Raiders?,Neutral
Intel,I have b580 12gb gpu and amd is no go black screen central. Intel has no issues like those amd cards are trash.,Neutral
Intel,The odds of someone responding to this post that has an Intel Arc gpu is near zero. They have less than 1% market share and 0.16% of Steam users.,Negative
Intel,![gif](giphy|1zSz5MVw4zKg0|downsized),Neutral
Intel,![gif](giphy|iibEPf8xEDTedJcDJr),Neutral
Intel,How about replacing 5090 without the flammable connector?,Negative
Intel,Me still happy with my 2080ti:,Positive
Intel,Similar situation. 6800xt. If Intel can provide a meaningful upgrade to my 6800xt I'll seriously consider them. I've zero loyalty as every card I've bought has made me happy and been a genuine upgrade.,Positive
Intel,"Same, other than that I have a 3060ti. Budget cards are great, but i already have something better and im not willing to downgrade",Positive
Intel,"The 3070Ti is barely 4 years old... thanks to the 8GBs, it was obsolete on its release. It could have had so much potential with 16 or 24GB",Negative
Intel,"Same. Offer me good performance per price, enough vram, and solid drivers and my money is now yours.",Positive
Intel,Bmg g31 is going to be released q1. 2026. It will be anywhere from 32 to 44 xe2 cores.  I'm speculating that price tag is going to be around $500 USD.  Will most likely have 16 to 24 gigs of vram.  The next discrete cards after that will be xe3p. Those cards are currently insane. Testing just started for them. Most likely release will be 6 months after Nova lake.,Neutral
Intel,Considering Nvidea bought a share of intel i would be surprised if ARC isnt dead in the water,Neutral
Intel,Intel partnered with Nvidia to drive their APU's.  I don't think a new ARC will come out.,Neutral
Intel,Newsflash: it isn't going to happen. AMD and nvidia are very much ahead. Intel isn't going to make up in a year what amd and nvidia have been doing for decades.,Negative
Intel,Lossless scaling?,Neutral
Intel,"https://preview.redd.it/48776rh76b1g1.jpeg?width=2992&format=pjpg&auto=webp&s=9acb9751d120d4eecab4bd4cf1b532fcb37589b9  I feel you, great encoder so the main gpu can run free",Positive
Intel,Its in danger.,Negative
Intel,Is the A310 connected to the chipset? I had that idea as well (with an Arc Pro A40) but i have already 3 PCie Cards in my system and the space is getting thight...,Neutral
Intel,Adorable,Positive
Intel,"Honestly they're excellent for budget gaming PCs. Hella good performance for their prices.   For high end, though? I agree with you there.",Positive
Intel,"I've got one paired with a 5950x and it runs great.  I'd like to see a b or c series 770+. But I do 1440 gaming, and I've been able to run anything I've thrown at it at 60+ fps.",Positive
Intel,are you not bottlenecked by the 3900x in a lot of titles?,Neutral
Intel,"lose, not loose",Neutral
Intel,PCIe 3.0? You're basically losing no performance on a 16x interface with the 9060XT. Even a 5090 only loses about 2%.  VRAM limited cards on an 8x or 4x interface though... Oof...,Negative
Intel,Lmo rhere's like a 1% difference at most in 99% of gamws dude,Neutral
Intel,"I wanted one, but couldn't find them at MSRP. I bought a 6800 instead.",Negative
Intel,"Used the B580, even if it only has eight lanes, it still gives me some hella good performance at its price point.   $250-270 and I can play 1440p 60fps, that's all I could really ask for.",Positive
Intel,I really wanted Amd and Intel to come out with a competitor that sits in the same category as the 5080.... AMD was rumored to have one but that never seemed to materialize.,Neutral
Intel,That’s never happening so I guess have fun waiting lol.,Negative
Intel,If you look at the content in this sub you'd think 50% of people run a 5090/6000 Pro.  Bit funny cause even among members of this sub I doubt it's over 5%,Negative
Intel,"with their fake msrp/stock on release, and the overhead/driver issues, this gpu was DOA. Even if they completely resolve these problems, it's late, you can get way better products now at this tier, with way better support and resell value.",Negative
Intel,"Ahh same, I really really wanted a B580 but they were not available in my country so I bought a used 6700XT instead.",Negative
Intel,TIL the B580 has better performance than the 6700XT,Positive
Intel,"I'd say yes! I paired it with an i5-13400F and ran Forza Horizon 5 at 4K60, Space Marine 2 at 1440p60, Helldivers 2 at 4K40/1440p60, and Doom: The Dark Ages at 4K40/1440p60.   Settings were all at ultra, only had dynamic resolution set for Space Marine 2.",Positive
Intel,"It eats whatever i throw at it, got it for 235 from b&h photo, also came with bf6 a game i was never going to buy at full price that i now get to play at peak popularity.",Negative
Intel,"I play Avowed on a 1440p ultrawide, no complaints here",Positive
Intel,"It does, it's arc",Neutral
Intel,"I heard that using Virtual Desktop is the best way right now.   However, I think someone who wants to use VR is likely not going to get an Arc GPU anyway due to the more budget nature of the GPU line.",Neutral
Intel,Yeah I made a support ticket about old Minecraft versions having rendering bugs with its old version of OpenGL and they fixed it also.,Neutral
Intel,"For someone windows shopping, what kind of issues did you have?",Neutral
Intel,I’m on a regular 2080 and feel the same way.  Still hoping for a B770 release and that it would be a worthwhile upgrade.,Positive
Intel,"I'd say it's good with i5s, they just seriously improved the CPU overhead performance with i5 and i7.",Positive
Intel,I do wonder if this is a problem with CPUs that don't have iGPUs.,Neutral
Intel,Probably better due to the higher VRAM and clock speed.,Positive
Intel,"Actually, they just fixed that recently. Used an i5-14400F and it ran absolutely pristine.",Positive
Intel,Fun thing: try and see if you can use lossless scaling and combine the rtx with the igpu.,Positive
Intel,"Is amd years behind or just blocked out of the game.  Nvidia holds the market because of cuda, and cuda is proprietary to nvidia and very old tech so almost all programs are designed to take advantage of cuda.  Amd is not allowed to make a cuda equivalent and developers dont want to write their code for something new.    Thats not years of behind, thats a monopoly.",Negative
Intel,"They're partnering up for integrated graphics because, let's face it, Nvidia doesn't do well in the CPU market.",Negative
Intel,"Budget gaming. I was able to play some good games from 2024-2025 at 1440p 60FPS with ultra graphics settings.   Also good for competition. Everyone complains that Nvidia is overpriced and AMD is underwhelming, but then where does Intel sit in the hate train?",Positive
Intel,"I'd wager the vast vast majority of those Arc users are enthusiasts, and those people are more likely to be on PC forums like this",Neutral
Intel,Common RTX 4090 L,Neutral
Intel,Intel C990KS or Intel C990XE does have a ring to it ngl.,Neutral
Intel,I bought a 5090. I undervolted and overclocked it and get a 3% performance boost while using about half the power of stock.  The connector issues are exaggerated and easily avoided.,Neutral
Intel,me living with a turbine 2 ft away from me playing ck3 with my 2080 super:,Neutral
Intel,Me still happy with my rtx4000.,Positive
Intel,"Still using my 1080, it's a lil trooper",Positive
Intel,"Not saying that my 3070ti isn't a good card, I'm playing in 1440p and in most situations it crushes it, but there are those couple of times where I find the performance lacking.",Negative
Intel,"I can understand that without judgement. I only just replaced my GTX 1080 a few weeks ago. Had that thing running for over 7 years and it's still got enough life left in it to go into a secondary computer for a few more years of service.  I don't know if I just don't tend to play demanding games or what but for example, I was still rocking out 60+ fps in Cyberpunk without turning the graphics settings down all that much.",Neutral
Intel,Same here. 6800xt for the last 3 years and still no issues with gaming at 1440 high in most games so I'm good until something is significantly better at a reasonable price.,Positive
Intel,Name a game and I’ve played it in 1440p on a 3070ti. Why do you people thing 8 gigs of vram is basically useless😭😭,Negative
Intel,Even just 12GB like the 4070 would have made it last longer   Same for the 3080,Neutral
Intel,16 and 24 gigs wasted on it as the core can barely utilize 12gigs without shitting the bed.,Negative
Intel,"Unless intel confirms the cancellation of ARC officially as a whole, then that means pretty much nothing.",Negative
Intel,"Maybe you don't get the reason for your downvotes, but at least one of them is that their cpu and gpu departments are completely separate, no that's why there was a big disconnect between Intel cpu and gpu quality, it's essentially 2 different companies sharing intel's name",Negative
Intel,Is that on paper? No? Then it doesn’t exist.,Negative
Intel,"Intel doesn't compete at the high end, if anything it just takes even more market share from amd so idk why Nvidia would stop it.",Neutral
Intel,"The partnership is because, as much as people in this sub argue otherwise, the age of large APUs has begun. They're likely not gonna catch on in DIY desktop, but for laptop and mini-PC's, giant APUs with large iGPUs are going to likely replace entry level dGPU.   So the partnership allows Nvidia to, instead of selling a GPU chip to a motherboard OEM to be soldered, will now sell that chip to Intel to be integrated into the SoC package as a chiplet.   It doesn't mean Arc is dead. It means Nvidia is ensuring they aren't squeezed out of this new emerging market",Neutral
Intel,I'm aware,Neutral
Intel,This is not how it works,Negative
Intel,"By your logic, Tesla shouldn't have had the success it has now since Ford has been making cars for a lot longer.",Negative
Intel,Oh intel pretty much can.,Neutral
Intel,Or video encoding. I've considered getting an A350 for AV1 OBS recordings,Neutral
Intel,"Nope. Video encoding and just displaying my desktop, so my RTX can run headless so I can passthrough it to a VM if I need to.",Neutral
Intel,"While being strangled, JK but yeah slight airflow decrease.",Neutral
Intel,"It's not A310, it's A380 (although it's power limited to 43W) and it's not connected to the chipset, but directly to the CPU. I've picked up motherboard that has two large PCIe slots running at PCIe gen 4 speed, connected directly to the CPU, exactly for this reason. 2nd slot uses 4 lanes though, but it's more than enough for 3440x1440 170Hz uncompressed video.",Neutral
Intel,Would it be good for CAD modeling?,Neutral
Intel,"maybe a slight bottleneck in 1080p, but nothing to worry about.",Neutral
Intel,"It seems like I'm still more bound by the GPU and maybe something else. I can see in MangoHUD in e.g. Tiy Tina's Wonderlands my GPU at 60-70% and my CPU only at 15-20%.  The Ryzen 9 3900X might not be the newest CPU anymore, but it's also not slow. It's a 12-core (24 thread) CPU, which boosts up to 4,6GHz",Neutral
Intel,Exactly.,Neutral
Intel,Loosen up,Neutral
Intel,"read the comment again, slowly this time",Neutral
Intel,"I got the 8gb, so I might run low on Vram in the future.  So it might be a bigger difference when you runout of Vram.",Neutral
Intel,"I found in the UK, the 8gb models were £240, while the 16gb were selling above £300 and I assume msrp.  The b580s were always cheap though, last I was aware they were going under msrp at £215 for some models.  And the b570s under £180.",Neutral
Intel,"For this dumbdumb, what do you mean 8 lanes?",Negative
Intel,Yeah great value for sure,Positive
Intel,Which is good enough for 98% of gamers,Positive
Intel,"I mean I have a 7900xtx so I'm fine as is, but it would benefit everyone having some actual competition in the high end",Positive
Intel,"According to the Steam hardware survey we all run mid level cards. So yeah, not enough pro 6000s out there for the users here.   All joking aside... I really want to try Intel Arc one day.  I was planning a build for my younger brother, and I'm really considering a B580.... In my market Intel is really price competitive vs Nvidia, specially in the mid level cards like the 5060. I also considered the MSI Claw but it apparently sucked for another reason.",Neutral
Intel,That all got fixed in the first month,Neutral
Intel,Its really funny you felt compelled to comment this as a reply to this specific comment and not as a comment as a whole,Negative
Intel,Like?,Neutral
Intel,"Not exactly better, but there was some performance increase compared to the 6700XT. In scenarios I could use Intel XeSS instead of AMD FSR, I got more frames with increased image quality.  For comparison, I paid for a brand new B580 the same price of a used 6700XT. It was a no-brainer and I am not suffering from CPU bottleneck, which isn't a big issue since the .8xxx drivers.",Positive
Intel,"Woo! I love that thing, its so good for an igpu",Positive
Intel,It's not that's its budget. It's the support full stop. They are not supporting a use case that many use. They are throwing away sales.,Negative
Intel,"It was early adopter teething issues mainly. I haven't had any issues recently, Intel has fixed A LOT of stuff over time. My main source of complaint now is just that their driver software is lacking some features compared to my previous Radeon card. I'd rather Intel focus on perfecting the basics rather than implementing a bunch of fancy driver features though",Negative
Intel,A lot of the early issues was with older games. Intel didn’t have years of optimizations to make sure they run well especially with a tech change I can’t remember the details on. I believe they fixed it.,Negative
Intel,"Yeah, I would like to see a 16gb version or at least a better 12gb.",Neutral
Intel,"Okay, that’s good to hear, though isn’t that more of a budget card? I’d *assume* the most common setup would be with an i3.  The main point of criticism I heard is that you can get a last-gen used AMD card for around the same price and get better performance overall. If that’s still true, then I’d withhold on saying it’s truly a good card. Just a step in the right direction.",Positive
Intel,"Yeah, we even have our own subreddit, r/IntelArc",Positive
Intel,If Intel's version of XT and Ti was a -K suffix that would go so hard,Neutral
Intel,Pay more than 1000 bucks for a card. You have to tweak the power consumption or it starts burning. Big clown face.,Negative
Intel,My 2080ti used to be too. Then I removed the dumbass fucking blower and mounted an aftermarket kit on it. Best money of my life. Was like 50 bucks. Temps dropped AND its quiet.,Positive
Intel,undervolt it,Neutral
Intel,My 2080 super is the same lol,Neutral
Intel,GanG!  ![gif](giphy|ctkp9BZJ3zMkGhVZgC),Positive
Intel,They've confirmed since this deal that Celestial will happen,Neutral
Intel,I am interested in how it works. Care to explain?,Neutral
Intel,Okay. How does it work then?,Neutral
Intel,"I just think people are sniffing copium. I want intel to do well in gpu department as much as the next guy, but reality is different. Often dissapointing.  The things that intel has to do to be on-par with current competitiors is astounding. However, I do hope I'm wrong and intel *can* find a way.",Negative
Intel,"Video encoding and just displaying my desktop, so my RTX can run headless so I can passthrough it to a VM if I need to.",Neutral
Intel,"Do you do a lot of video editing? I'm not familiar with this, and I'm wondering at what point I would need a separate GPU for video encoding.",Neutral
Intel,"Smart, are you using a display streaming like parsec?",Neutral
Intel,"That's why I picked up single slot low profile version. Slightly more expensive and power limited, but it doesn't block that much.",Neutral
Intel,Might be! I haven't tested that myself.,Neutral
Intel,"its a productivity beast, but if youre frames are uncapped and your gpu isnt getting to 100 percent usage, youre cpu bottlenecked, as most games only use 1 or 2 cores usage percentage means very little",Negative
Intel,loosen up those pcie lanes step bro 😩,Neutral
Intel,"I was agreeing, although it may not read like that...",Neutral
Intel,"Say this to yourself in the mirror 3 times, no more, no less. It MUST be 3 times for this to work  Come back to this thread  Then behold!  ![gif](giphy|12NUbkX6p4xOO4)",Neutral
Intel,I'm assuming 8 pcie lanes,Neutral
Intel,"8 pcie lanes instead of the usual 16 so half the bandwidth for a given generation.  The PCIe bus is used to share information between the card and the rest of the system. It's usually not a bottleneck (the GPU can't process data as fast as it comes in through the bus), but it's a noticeable one when you're low on VRAM and the GPU needs to fetch data in the system RAM through the PCIe bus. Mostly an issue with 8 GB cards like the 5060.",Neutral
Intel,Ah. Shame that the Intel cards never got a market in my country. Wouldve definitely snapped one when I built my PC,Negative
Intel,"If I were to get one for my kiddo, would I expect more troubleshooting for Steam games or ability to play less of them?",Neutral
Intel,Yeah K or KS would be cool though i think XE as nod to HEDT and the X299 platform with the i9 9980XE and i9 10980XE could also go super hard.,Positive
Intel,Its only 1000 now?,Neutral
Intel,$3000* 5090 is stupid expensive,Negative
Intel,"If you are calling me a clown by not only getting the best card on the market for gaming, but also being able to save nearly half of the electrical cost AND getting a small performance boost on top of that, then you are the clown.",Negative
Intel,Deshrouding rules,Neutral
Intel,"No, I don't do a lot. The main goal wa to use it as a display GPU, but the AV1 encoding is a nice bonus. I run OBS in replay mode (10 minutes) in the background and this intel GPU is used for encoding.  I want my Nvidia GPU to not have any monitors connected to it, so I can switch between using it in my host OS and in the VM, here's an example of how it works: [https://youtu.be/3fiXFv85iRU](https://youtu.be/3fiXFv85iRU)  This allows me to for example stream a game directly from a VM to my TV and still use my PC to do something else, like for example few weeks ago we played Trine 2 with my brother, when I was running the game in both the VM and host (one copy on Nvidia and one on Intel) and we played multiplayer together.  I used to use the VM to play everything like year or two ago, but the Nvidia drivers on linux improved significantly, so nowadays I don't use it nearly as much, but I still do from time to time. Plus it's always nice to have a backup plan for running some anticheats.",Positive
Intel,"I'm using looking-glass. It's a bit different than parsec, it works by copying frames from the main GPU's framebuffer into the shared system memory segment and then to the 2nd GPU's memory. No compression and very low latency (I use virtual display in my Windows VM that I can set to 500Hz, which gives me around 2.5ms of added latency in total).  I also use Sunshine and Moonlight when straming to a TV or remotely to my laptop or phone. I have the cheapest server in OVH that I use as a VPN, so I can access my PC or share some services over internet (like my personal cloud for example: [https://cloud.yayuuu.pl/index.php/apps/memories/s/pKqfn86wLLy6xZM](https://cloud.yayuuu.pl/index.php/apps/memories/s/pKqfn86wLLy6xZM) ), so I can game from anywhere, just login over SSH to my home server, wake on lan PC, start a VM and connect using Moonlight).",Neutral
Intel,"Good point, I guess it isn't necessarily the best gaming CPU, but I quite like its speed for compiling etc.  Getting some 5xxx CPU seems like just a small step up and wouldn't be cheap. And anything even higher would mean basically replacing everything: Mainboard, RAM and CPU. Going with a lower core count and higher boosts might be better for gaming, but would feel like too much of a downgrade.  Ironically most games, which I play besides Tiny Tina's Wonderlands, are such old titles, that both GPU and CPU are complete overkill. And lately I found just far too little time for gaming in general to make it worth it with the current prices to get some DDR5.  Edit: I just checked and at least according to Wikipedia the Ryzen 5000 CPUs aren't clocked all that differently. Its successor the 5900X has a boost of 4,8GHz, but same core count and same L3 cache and same TDP.",Neutral
Intel,I don't know what that means in layman's terms. Does it cap bandwidth like older hdmi resolution? Does it limit my kid's Minecraft terrain generation range? Does it mean I only run 69 mods on Skyrim instead of 420?,Negative
Intel,"To be fair I don't play *that* many different games, but I haven't come across one that simply doesn't work. The biggest game compatibility issue I ever ran into was with StarCraft Remastered not rendering at full resolution. My theory is that they were specifically checking for Intel (what would've been only slow iGPUs at the time) and disabling high resolution. Blizzard fixed it though thankfully.   This video is over a year old, so things have only improved since then. I wouldn't be surprised if a lot of the issues shown have been addressed: https://youtu.be/Y09iNxx5nFE",Negative
Intel,For what I've played using an A750 card on steam the only game I've ever had issues with is Kenshi which would crash out on me for whatever reason but nothing else seems to have any noticeable issues.,Negative
Intel,WTF. I corrected it a bit.,Neutral
Intel,The point is that premium product should be perfectly tuned out of the box.,Neutral
Intel,"There is always room to tweak. Fact is, the burning connector should not happen on a product that expensive.",Negative
Intel,"It’s cool you’ve got the best GP that’s ever U-ed and it’s even cooler that you made it suck less (power), but the reason people are diasgreeing with you is because you’re defending an inarguably bad design with the argument of “but it works on my machine”.",Positive
Intel,"Your not a clown, you’re the whole circus buddy",Neutral
Intel,"Wait, if you don't connect your Nvidia to monitors, does that mean the thing you see on monitors is only rendered by the Arc?    So, is it like how you stream from game pass? Where you let other PC run and calculate the game (your VM in this case) and then stream the game on your host OS?    Are there advantages in doing this? How are the graphics compared to directly using your Nvidia?  When you say that you rarely play on the VM nowadays, do you mean your Nvidia is connected to the monitors? Or is there a way to split the task without VM?  Also when you played Trine 2, does that mean the game looks better on TV than your PC? Since Ark is weaker.  Sorry for asking many questions lol",Neutral
Intel,"😂 Just has less bandwidth, but still really damn good.",Positive
Intel,"It caps the bandwidth. But that's fine because it's not strong enough to even utilize the full bandwidth that 16 lanes would provide. And those extra lanes would just increase cost.   This *may* be a problem on an old motherboard, because 8 lanes of PCIe *may* be too little (idk, didn't see any tests). But 8 lanes of PCIe4 are more than sufficient.",Negative
Intel,"You need to change your expectations if that is your point.  If you buy a stick of RAM today, it is going to run at half speed if you don't go into the bios and correct it. If you don't want to tune your PC, don't build one yourself.",Negative
Intel,Looks like we found someone poor AND stupid over here.,Negative
Intel,"https://preview.redd.it/5utu2xxvee1g1.png?width=5240&format=png&auto=webp&s=a281c9815d6408496deeaa752ed61862de335754  No, I can choose which GPU I use for rendering. That's basically what laptops are doing this with multiple GPUs. I just set these environmental variables in the steam's launch parameters: DXVK\_FILTER\_DEVICE\_NAME=""NVIDIA"" \_\_NV\_PRIME\_RENDER\_OFFLOAD=1 \_\_GLX\_VENDOR\_LIBRARY\_NAME=nvidia   and it will run on Nvidia. I can do the same with any other app, like browser, blender, etc. The screenshot shows an example of the game running bare-metal on nvidia-gpu and being displayed on intel GPU.  To not repeat the same parameters with every game, I've made a simple bash script that just exports these variables (and few more, like DLSS upgrade, etc), so I just type ""setenv %command%"" in steam lauinch parameters and that's it.  When running a VM, I use looking-glass, which is different than streaming the game to another device over LAN (like in your example with gamepass). Looking-glass uses shared memory to copy frames from one device to another, so it only works within one PC, but the video is uncompressed and with minimal latency (like 2-3ms of added latency). This is essentially the same path as offloading rendering but with one additional limitation - windows forces vsync, so I need to run as high refresh rate inside windows as possible to reduce latency. I use Virtual display adapter that allows me to set up to 500Hz and run desktop without any physical monitors attached.",Neutral
Intel,"Trine 2 is not that demanding, both GPUs can run it at max settings just fine. This is also true with a lot of older games. With new games it wouldn't work like this, but there is another way. Linux has a built-in multiseat, that allows logging in 2 different users at the same time, giving them access to different monitors, input devices, etc. I could use a 4k dummy plug to enable multiseat on that device and stream from there to the TV, but I don't have it configured right now, as I still need VM sometimes, so usually it's enough.",Neutral
Intel,"So like a v6 Camaro? Not as fast at the suped up v8, but still better than a Corolla?",Neutral
Intel,"I think it’s less about “not getting the full performance”, and more about it being a fire hazard.   Especially when you pay that kind of money for it.    In fact, one could argue that the consumer shouldn’t be able to make your product into a fire hazard, even if they aggressively fuck around with software parameters. As long as no modifications were done to the hardware - it should be perfectly safe. Sure it may not function properly, but it shouldn’t melt itself.",Negative
Intel,"Your ram clocks actually depends on your other hardware, its not the case for gpu. Its fine that you like it, but other may not, and nothing on the box say i need to understand something about overcklocking\\downcklocking. Also i doubt that some random dude will spend time to play with your gpu settings if you buy prebuilt, you will just get it as it is.",Neutral
Intel,"More like the transmission than the engine. PCIe lanes determine how much compute (horsepower) can get to the CPU (Road) so if your GPU (Engine) is performing really well but is bottlenecked by PCIe lanes, you aren't really getting that level of performance.   It's also why this objection to the Intel cards is a bit of a moot point. If you are running a v6 than you don't need the best transmission, the car isn't making the power to necessitate more HP to the wheels.",Neutral
Intel,"I'm the guy that bypasses restrictive software and hardware to do what I want, so I won't advocate for any more dummy proofing. That is one of the reasons I hate Apple.",Negative
Intel,"So sounds like an Average Joe won't notice, especially compared to other cards of the same price?",Neutral
Intel,"I don’t mind tinkering(in fact I enjoy it a lot, both at my job as a software dev, and as a hobby).  However, hardware+firmware(when not modified) should not allow software to set unsafe parameters, period. And the fact that those unsafe parameters are set from the factory - is all the more insane.",Positive
Intel,Of all the differences between cards at that price range the number of PCIe lanes it has will be the last thing you'll notice.,Neutral
Intel,They are.,Neutral
Intel,depends how heavy the card is. had 4070ti for 2 years without any bracket no issues.,Neutral
Intel,Lego is kind of expensive. A block of wood is cheaper.,Negative
Intel,Put literally anything there.,Neutral
Intel,"They're like $6, better to be safe imo",Neutral
Intel,"""worth it""? really? They're like 5 to 10 bucks at most. It's not a life-altering purchase.",Negative
Intel,Is it worth spending $5 to help protect my $2000 machine from breaking?,Neutral
Intel,Ddu and try other drivers version,Neutral
Intel,"Simple, if its at msrp, get the 16 GB 9060xt, otherwise get the b580, only and only if its also at msrp.  9060xt at 500 usd is fucking highway robbery, and you might as well go full 9070 or 5070 at that price point lmao.",Negative
Intel,"I have a b580. And while I think it’s awesome, it needs a little more oomph. The 9060xt is my current recommendation. The gains are also quite a bit.",Positive
Intel,9060 XT would be my pick. Who knows what's going to happen to Arc after Nvidia's sizable investment in Intel?,Neutral
Intel,"My choices were b580 or a used 6700XT for 250 USD, but there's no available b580 in my country at near MSRP. Damn thing cost 350USD, so I had no choice but to go 2nd hand for 250USD.",Negative
Intel,"9060 xt 16gb?, yeah.",Neutral
Intel,"I put a B580 in my kids PC earlier this year, and they love it. If 1440p is a hard line requirement for you though, you'll need the 16gb of the 9060XT sooner or later. If you're fine to drop to 1080 on those occasions where the B580 can't handle 1440, then save your money and get the arc.",Positive
Intel,"9070 and 5070 are out of Reach too, cheapest 9070 i know of costs like 700 bucks, 5070 is also in that range",Neutral
Intel,"Yeah but the price difference is 250 bucks between the two, I've heard some driver stuff on the B580 not being good, is that actually the case? Cuz im gonna opt for b580 atp",Negative
Intel,"9060 xt 16gb for 500 bucks tho, i feel like 250 euros for the b580 is a much better deal",Neutral
Intel,"Shit bro, then get the b580 then, those 12 GBs of VRAM might come in handy with these dark ages coming soon, as long as its not over 300 dollars it should be a good purchase.",Positive
Intel,A 5070 should be $500 or right now less.,Neutral
Intel,"Drivers for the most part are good. you will get a hiccup here and there and may have to DDU every once and a while. but they are excellent 1080p cards and fairly decent 1440p cards with optimized settings, usually a mix of high/medium. RT is a mixed bag if you care about it. depending on how it's used it could be great or not great in terms of performance.   CPU overhead is still a thing for the majority of games. AM4 X3D CPUs and the r 7600 and up are what i recommend. on the intel side a 12600k or better.   after writing this out, as you can see i think it's awesome. i just wish it could push out a bit  better as per my own standards. but it wouldn't be a loss to go with it. it's a pretty good card.   I would recommend the LE if you can get it. and avoid acer, i haven't seen one myself but it seems to be the general concusses",Positive
Intel,"You can take a list of games and see on the youtube test. But generally if the B580 is that cheap, it is worth it cause not many games has noticeable problem. The CPU overhead has also been fixed.",Positive
Intel,"Check İtopya, a Turkish PC part site and you'll see dem prices my man",Neutral
Intel,the cpu overhead has not been fixed. it's only for a few games right now. but they are working on the issue still,Negative
Intel,https://youtu.be/gfqGqj2bFj8,Neutral
Intel,Did you watch the video? They explain that it’s only on a few titles. And it still has overhead on the fixed titles with certain hardware. Their example used is the r 2600. And even with the 5600 in some cases. Their video title is clickbait.,Negative
Intel,Because that's the few titles that has overhead? Do you even watch it lmao,Negative
Intel,You can head over to the intel subreddit and discord and they’re going to tell you the same thing I did. The overhead issue is not fully resolved. It’s in progress and continues to get better. But it’s not done yet.,Negative
Intel,As opposed to real benchmark data? lmao,Negative
Intel,"it's in the damn data you sent me haha, in rainbow six seige, the R5 2600, it gets a lower result than the 9060xt. it should be about equal to the 9060xt in these scenarios because both GPUs are CPU limited. but the b580 is hit harder because of the overhead issue.  we see this in every single title shown in the video except in dying light 2 and cyberpunk.  the R5 5600 is powerful enough and with the new driver updates able to keep up. we see this in every example shown in the video except horizon zero dawn, even listen to what he says in the zero dawn section. this specific example can be either CPU bound issue, or because the game just loves that X3D cache so much. it's open ended. the driver overhead issue IS better, but it's not fixed.  Anywho, i'm done now, it's time for the weekend. i don't need to try to convince you any more. or i'm just being trolled, it's very possible! I hope you have a nice weekend as well mate.",Negative
Intel,There’s are 100% 3050 and I believe even 5050 models which only run on pcie 75W power. Not sure about 5050 models but you always gotta check the specific version of the card,Neutral
Intel,Ok thanks,Positive
Intel,That's really nicely done.,Positive
Intel,10/10 if you spray paint the ram black as well,Positive
Intel,"Better buy heatsinks for RAM sticks, improve the look and maybe a bit the thermals",Neutral
Intel,Impressive,Positive
Intel,"Black velcro straps mate, going to make your life easier in the long run. Zipties are a pain.",Neutral
Intel,"if the RAM was black it would be even cleaner. A light coat of rustoleum matte black would work if you tape off the contacts, i've painted RAM a million times",Neutral
Intel,That cable management.    Please mark this nsfw,Negative
Intel,Haha green pcb go brrrtt,Positive
Intel,You're right but not necessary at the moment since i'm running 20b model with GPU offloading. I'm running models with partial RAM offloading experimentally. Memories are not getting hot for now. All the model weights and tokens on VRAM. I might need them later if i decide to run more RAM offloading,Neutral
Intel,"Right?  I don't get it, people spend thousands on their pc and then cheap out and use plastic ties instead of spending a few whatevers extra for something that's reusable and less risky.",Negative
Intel,"Yeah I don't get it, you can get like a 10 pack of decent velcro straps at the damn dollar store these days",Negative
Intel,it's really not that good,Negative
Intel,"> GPU offloading  > Intel Arc GPUs  Lol, cute",Positive
Intel,"A750? why so, had it laying around?",Neutral
Intel,Looks nice,Positive
Intel,My big ass hands could never do Mini ATX  After being a pc nerd for a long time and building multiple PC’s. I now enjoy the idea of a small PC. MATX is the sweet spot for me,Positive
Intel,Honestly its a nice little surprise getting to see an Intel based Build these days with how dominant Ryzen + Nvidia Builds have been lately.,Positive
Intel,Slightly disappointed when I realized the lemon deco isn’t in the pc lol,Negative
Intel,dad of the year.,Positive
Intel,I love what you did with the cooling tubes.,Positive
Intel,Sir I'm gonna submit my adoption papers too. I'll need them approved for my next build. Damn what a lucky kid.,Positive
Intel,Loved it! Well done!,Positive
Intel,"When life gives you lemons, build a PC on them",Neutral
Intel,Oh yeah we got one,Positive
Intel,DAMN that's good,Positive
Intel,Looks nice! Is this a small case?,Positive
Intel,so much intel...,Neutral
Intel,I looked at the first picture and for a minute I thought you had to wait after that juicy ram wiped out your 401K...,Neutral
Intel,SG05 white? Nice.,Positive
Intel,looks nice but i don't think that cpu needs water cooling. if you do that just for aesthetics that's fine...:),Positive
Intel,not bad. why a) the smaller build b) the socket and c) the gpu,Positive
Intel,Serious question: Why did you buy an intel processor in 2025 for gaming?,Neutral
Intel,"Yeah, I had it laying around. I'll upgrade it when Intel hopefully release their next line of GPU's. It gets a good 100-115 FPS in Fortnite with epic settings. But it's not for me at the end of the day. He's happy :)",Positive
Intel,"The Arc A750 is basically a RTX3060. It's a good card, and driver updates have made this card a lot more stable than it was at launch.   But yeah 3060 also have more VRAM, so, for 1080p gaming this would be fine.",Positive
Intel,a750 is a bottleneck here,Negative
Intel,imagine using a spare gpu for a budget build is that really necessary,Negative
Intel,"Haha, I got big hands too, it was a mighty struggle with plugging in those cables for power led, button, reset etc. Only went ITX to keep things compact as small bedroom.",Neutral
Intel,"I can't stand the empty space but for airflow, and the old parts take up less space in my closet. Win win until you check the ITX tax and limitations some generations.  With Ryzen we finally saw 6cores+ at reasonable TDP for SFF gaming. A 5600X in eco mode is so clean.",Neutral
Intel,I agree. It was different. Never built an Intel system before. Always been AMD in the past.  I’ve had good experience with a 275HX I use on my laptop so wanted to build an Intel based desktop this time. I especially like the whole idea around the P + E cores and a small NPU.,Positive
Intel,Ah that's the table throwover we put on when the kids get there clutter / toys out all over it. I hate it :/,Negative
Intel,Haha thanks. The wife laughed at the comment though :),Positive
Intel,Thanks :D,Positive
Intel,Yes it’s the Silverstone SG13,Neutral
Intel,https://preview.redd.it/6pp0jbql093g1.jpeg?width=4000&format=pjpg&auto=webp&s=a3746030900eb4438ecf9946a8434f892f319d03,Neutral
Intel,I bought the RAM for £124 which is over priced. About a month ago it was around £80 I think.  Now it’s sold out and the price is over £220  It’s insane!!,Negative
Intel,"No. It’s overkill. But I hoped that if I gave it water cooling it would vastly help keep temperatures down so the CPU could hold its boost clocks for much longer.  I have a 275HX with my laptop, it hits 100C in seconds.",Neutral
Intel,"Performance, clock speed, low power.  This is also the first Intel system I’ve built. Always been AMD in the past and fancied a change.  Single core performance was better compared to AMDs equivalent in the price range. I used Copilot to compare a lot of CPUs in the price range and the 225F topped it off for me",Positive
Intel,intel is the mainstream. amd is still a minority alternative  i don't want a cpu that can't reliably handle ddr5 of 4 sticks or faster than 6000mhz. it's shameful,Negative
Intel,>I'll upgrade it when Intel hopefully release their next line of GPU's  Good luck with that. You just know Jensen is going to gut Intel's graphics unit as the quid pro quo for bailing them out... :/,Negative
Intel,And I might add nothing wrong with gaming at 1080p.... you dont have to have the best of the best to have fun and enjoy yourself.,Positive
Intel,Is this sarcasm or are you lacking brain cells,Negative
Intel,![gif](giphy|xTiTnIilwuFFFpf2Cc),Neutral
Intel,I am glad you got it done in time! My kids are on older rigs but they run what they want to play so it doesn’t matter! We’re solid for the next few years and at that time things hopefully calm down.,Positive
Intel,225f is not hot at all. [mine](https://www.reddit.com/r/pcmasterrace/comments/1ouj4en/im_glad_i_use_a_mesh_case/) using stock cooler stayed around 70-80°C and never thermal throttled during stress tests. my case even has only one exhaust fan,Neutral
Intel,Lmfao ok bro....,Neutral
Intel,Will keep an eye on this. I think highly of Intel more as I think their GPU's are more reasonably priced compared to Nvidia / AMD dGPU variants,Positive
Intel,"Wow, going for the rare but very good b580,",Positive
Intel,Good luck! Let us know which part ends up cutting your finger.,Positive
Intel,Enjoy! Just take it steady and plan cable routing.,Positive
Intel,B580... I see you like challenges,Positive
Intel,"Not bad! I'm assuming you're building a workstation build based on the Intel Graphics Card. Either way, it's solid! :D",Positive
Intel,the cooler is very huge. what cpu you're using?,Positive
Intel,And how long you panicked for thinking you ruined something,Negative
Intel,Done. My fingers are unhurt :),Positive
Intel,"Thanks. It’s not really meant to be a work station. It’s just for personal use which for me is a lot of communication, internet browsing and some gaming.",Positive
Intel,i5 13600kf,Neutral
Intel,and when you plug the HDMI into the motherboard instead of the GPU,Neutral
Intel,Still solid my friend! :D You know your needs better than anyone else. As long as it runs the way you want it to then Yay!,Positive
Intel,"Welcome to the PCMR, everyone from the frontpage! Please remember:  1 - You too can be part of the PCMR. It's not about the hardware in your rig, but the software in your heart! Age, nationality, race, gender, sexuality, religion, politics, income, and PC specs don't matter! If you love or want to learn about PCs, you're welcome!  2 - If you think owning a PC is too expensive, know that it is much cheaper than you may think. Check [http://www.pcmasterrace.org](http://www.pcmasterrace.org) for our famous builds and feel free to ask for tips and help here!  3 - Consider supporting the folding@home effort to fight Cancer, Alzheimer's, and more, with just your PC! [https://pcmasterrace.org/folding](https://pcmasterrace.org/folding)  4 - Do you need a new PC? We're giving away a high-end PC build in a WORLDWIDE constest: https://www.reddit.com/r/pcmasterrace/comments/1nnros5/worldwide_giveaway_comment_in_this_thread_with/  5 - And we're also giving away a white hardware bundle with FPS. Case, PSU and AIO: https://www.reddit.com/r/pcmasterrace/comments/1ox3hmd/fsp_x_pcmr_thanksgiving_giveaway_comment_on_this/  We have a Daily Simple Questions Megathread for any PC-related doubts. Feel free to ask there or create new posts in our subreddit!",Positive
Intel,Can it at least take down the stuff at my job too?,Negative
Intel,"To be fair, this time at least downdetector isn't hilariously down, like last time.",Neutral
Intel,Who's breaking the internet this time?,Neutral
Intel,lol WTH does AOL do these days exactly?,Negative
Intel,"I'm not saying it's DNS, but it's DNS.",Neutral
Intel,https://preview.redd.it/7rzy23sfen3g1.png?width=599&format=png&auto=webp&s=d96af4bfe7bafc78875458e3b108234f42c2eb27,Neutral
Intel,Mass layoffs to replace with ai     Mass internet outages aswell as alot of companies getting false flag reports     2020's just keep getting worse,Negative
Intel,"Such decentralization, much stability",Positive
Intel,Just restare the global router and we all good👍,Positive
Intel,"https://preview.redd.it/j9oxm3xtfn3g1.png?width=269&format=png&auto=webp&s=77fad86d4677e9b3d8a03a71674f58a644723882  as always, steam refuses to take the L.  My bet would be cloudflare trying to adopt clanker code, because they've been shitting the bed a lot recently",Negative
Intel,Did they let the AI bros in on making network infrastructure? Cause it seems like its been failing a lot lately,Negative
Intel,Its funny how the frequency of huge failures seems to have gone up along with AI shit getting more widely adopted.   Correlation and causation not withstanding.,Negative
Intel,https://preview.redd.it/2el0pdjljn3g1.jpeg?width=625&format=pjpg&auto=webp&s=9595c57a5c20afaeacc5d2739644ddc4fc6dbf93,Neutral
Intel,Get used to it with all the AI bullshit being released each day by dumb vibe coders.,Negative
Intel,corpos learning the hard way that AI is shit at everything they were sold on,Negative
Intel,I think they're running low on RAM maybe,Negative
Intel,At least downdetector is up this time,Neutral
Intel,This outage brought to you by AI Code,Neutral
Intel,What website is this graphic from?,Neutral
Intel,"Steam working perfectly fine , Epic games down.",Neutral
Intel,This happens like every week at this point. I genuinely wanna know wtf is going on over there,Negative
Intel,Is there any furry convention going on?,Neutral
Intel,Put everything on the cloud they said.  It's perfectly secure they said.,Neutral
Intel,Is psn and steam actually down? Or just reported from game outages on the platform.,Negative
Intel,It feels like this is testing for something major.,Negative
Intel,"It's fine guys and gals, AI is VIBE coding and it will fix itself within a few minutes without any human intervention.  /s   Can we already replace CEOs, CTOs and shareholders with AI instead of actual people who can fix this shit up?",Neutral
Intel,China just testing the waters for 2027,Neutral
Intel,Wait.... People are still using AOL?,Neutral
Intel,Once in a lifetime outage (that happens every other week),Neutral
Intel,"It's quite worrying how often this has been happening lately, isn't it?",Negative
Intel,Lol. I was in GTA Online where a modder messed up my lobby. Since then I couldn't log back in in any session and also to my epic games. I thought he somehow got into my system and now I'm running deep scans to find any malware. Thank god for this reddit post.,Negative
Intel,They installing backdoors Into everything   We will be china 3.0 with surveillance,Negative
Intel,Welp. I bet its DNS again.,Neutral
Intel,"https://preview.redd.it/8ctdt5546o3g1.png?width=5306&format=png&auto=webp&s=3b4bc598f1ce44bcfe449ff36ea1eb0991383b95  First day working at Epic Games, just optimized some code to make the server smoother 😁",Positive
Intel,"ppl who hate the idea of ""web3"" and ""decentralization"" but also hate when literally any of the centralized crap keeps going down due to enshittification  \*insert daily struggle meme\*",Negative
Intel,Maybe just another silksong release?,Neutral
Intel,all this vibe coding is making it harder not to do my job,Negative
Intel,"Thanks for this, it reminded me that I forgot to grab this weeks free Epic games. Got around 600 games and haven't spent a single penny in that store.",Positive
Intel,Priming...,Neutral
Intel,People still use AOL?,Neutral
Intel,How's AI going eh,Neutral
Intel,TIL AOL is still up.,Neutral
Intel,Ok what happened this time,Neutral
Intel,ahh thats why twitch chats not loading,Negative
Intel,"Didn't notice. I was outside, protesting, to save my country from the mafia.",Neutral
Intel,Reddit somehow is still up,Neutral
Intel,DataKrash from Cyberpunk 2077 living up to its prophecy.,Positive
Intel,[](https://i.imgur.com/WPx7PqF.jpeg),Neutral
Intel,"it's just epic games this time, no ?",Neutral
Intel,"ran out of ram, i assume.",Neutral
Intel,reddit is shitting itself too,Negative
Intel,I say that all the tech lay-offs are starting to show...,Neutral
Intel,"""I'm doing my part!"" - IoT/smart devices probably",Neutral
Intel,yeah i can't log in to epic games (i have wuthering waves there to get cashback on every battle pass)      might be time to move to steam now,Negative
Intel,Could be…. You are all now routed via Deutsche Telekom or Vodafone Germany and its 9pm cet - no good peering was given that day (always)😜,Neutral
Intel,"oh wow, these are all services i use..... not",Neutral
Intel,"Well looks good so fa-    *Reddit Server issues*    Great, now this shit again, thanks sharks, DDS'ers and others, lol",Positive
Intel,I was wondering why I couldn't get into Arc,Neutral
Intel,21 century malthusian trap incoming,Neutral
Intel,RuneScape is good,Positive
Intel,It is China cutting the sea cables,Negative
Intel,it seems that guy's CV is expanding,Neutral
Intel,"To quote Jesse, ""He can't keep getting away with it!""",Neutral
Intel,What am I looking at?,Neutral
Intel,Finally,Positive
Intel,Why is it so much worse lately,Negative
Intel,"Unfortunately not, still sitting at work and everything seems to be working 🙁",Negative
Intel,I only ever find out that the internet is down when it's back up and people are posting memes about it.,Neutral
Intel,"AI Slop innovation, Variable refresh down time rate , advance self ddos capabilities, shit knows what it is doing.",Neutral
Intel,"Yup, I noticed that matchmaking with Fortnite was broken this morning.",Negative
Intel,Are we being for real? Bro cyberpunk 2077 story is literally happening in front of our eyes. We’re screwed chooms,Negative
Intel,![gif](giphy|DUtVdGeIU8lmo),Neutral
Intel,Skynet.,Neutral
Intel,"Good thing I was asleep, and not in one of those beds that need internet for some reason",Neutral
Intel,"Did everyone else start hiring ""talent"" away from crowdstrike or something.",Neutral
Intel,What website did you use for this?,Neutral
Intel,do i remember a movie with thinking machines started this way in the first weeks?,Neutral
Intel,good. makes you realize how much of the digital world is just a scaffolding holding up nothing important… just quiet now.,Positive
Intel,Sauce?,Neutral
Intel,"Public Cloud has 99.999% uptime, huh?",Neutral
Intel,"Another day, another intern breaking production",Negative
Intel,luckily i was asleep,Positive
Intel,Only certain regions or something? I've been online all evening and all good.,Neutral
Intel,"What the hell ralph , not again.",Negative
Intel,"Was about to buy ARC Raiders, but didn't because of that.",Neutral
Intel,Didn't even notice tbh,Neutral
Intel,"It's great that the decentralized internet has been centralized by a few big names so they can make money off of it..  These kinds of outages were predicted, and would be solved by being decentralized. But cloudflare and aws and azure say no.",Positive
Intel,Pokemmo has been working fine for the last 6 hours.,Positive
Intel,What is it today?,Neutral
Intel,Ai doing good,Positive
Intel,always when u get the feeling to play,Neutral
Intel,"Wait, what happened?",Neutral
Intel,"I prefer ""World Wideweb"" please.",Neutral
Intel,Funny enough that this massive outages start with the rise of vibe coding and AI development tools.,Neutral
Intel,THIS IS ROCKET LEAGUE,Neutral
Intel,Installing shut switches like china has,Neutral
Intel,I guess that’s why Amazon was offering time off this morning. Didn’t know AWS was down again lol,Neutral
Intel,Why do games have their own personal charts lol,Neutral
Intel,"my epic games main account is not logging in 🥲, is this the problem ?",Negative
Intel,Need more mergers,Neutral
Intel,"Baseball, huh?",Neutral
Intel,Shit fucked my game progress up,Negative
Intel,People like to go on about how resilient the internet is but you'd only have to (hypothetically) take down AWS and Cloudflare to knock out a large portion of it.,Neutral
Intel,![gif](giphy|4QF3D5HE7NSrLKmT87),Neutral
Intel,DNS or shark. It couldn't possibly be AI code,Negative
Intel,who is it this time?,Neutral
Intel,That’s why I only use a “W”,Neutral
Intel,I thought it was just me,Neutral
Intel,So how many more of these until the Internet just stops permanently?,Negative
Intel,Who let the vibe coders build network infra...,Negative
Intel,"Why is it happening so often, lately?",Negative
Intel,Yesterday the only time I want to play Fall Guys and Epic is down,Negative
Intel,Don't put all your eggs in 1 basket,Neutral
Intel,![gif](giphy|6ILjOfJ1oL7NAc9SQ7),Neutral
Intel,"[Steam goes down]  [AWS and cloudflare stable]  ""*Breaking News: productivity goes down 125%*""",Negative
Intel,Why does Cloudflare keep going down? Are they getting DDoSed? Does some black hat pentester know something we don't?,Negative
Intel,Can someone explain the picture to me?,Neutral
Intel,Whole web goes dark because one cloud trips. Kinda why QAN built rapid redeployment across multiple providers.,Neutral
Intel,It will be fixed when PC 2 comes out.,Neutral
Intel,"Finally someone using the correct term (Worldwide Web) instead of using the ""Internet"".",Neutral
Intel,It'll be the russians again.,Negative
Intel,People still use AOL?,Neutral
Intel,If only there were a solution that didn't use centralized data centers.,Neutral
Intel,Always happy to see cloudfare shitting itself.,Negative
Intel,Great! Now everyone can go touch grass for a second.,Positive
Intel,Unemployeds and or europeons on suicide watch,Negative
Intel,can someone explain to me what this means. My brain understood nothing of what this image is trying to tell.,Negative
Intel,"Old job got hacked one day. It was a remote job. Nothing to do, it was a Sunday, I took my laptop to a brewery, watched football, and kept my Slack open, for updates on when things would go back up. They never did as I suspected, and did my full shift.",Negative
Intel,hahah you wish,Positive
Intel,At mine everything is down. All the computers on the whole site are down,Negative
Intel,Seriously. Only 2 hours to go but it feels like forever.  ![gif](giphy|wJD3qiNjSeHS0dP28T|downsized),Negative
Intel,*Monkeys paw curls* Stuff does go down at your work. But it's the fun stuff that makes work bearable.,Positive
Intel,my job just gets harder if that happens :(,Negative
Intel,"upside to my organization: we're on the gov cloud for most things so the when there are outages (Microsoft is a big one) for everyone else, the gov cloud is usually still trudging along.  Downside being I can't just go ""welp, everything's down I guess I can't work today!""",Neutral
Intel,Right? Everything goes down except the stuff I use.,Negative
Intel,The employees: I sure am glad the network is down.  Also the employees: IT WHY IS THE NETWORK DOWN? WHATS THE ETA ON IT WORKING WHATS THE ETA WHATS THE ETA!?,Negative
Intel,"My IT department is stupid enough to do that by themselves.  They forced a company wide update for office 365 but dont allow us to install any programs. So half of us lost outlook, word, excel, etc for 3 days.  Unfortunately if I just go home Arc Raider is down",Negative
Intel,"When the internet breaks, my team gets flooded with tickets :(",Negative
Intel,"Ya I was working on JIRA, odoo and taking teams calls all day. Why didn’t my shit stop working I wanted an extra day off.",Negative
Intel,wouldnt it be nice if it could handle all the nonsense we deal with daily,Negative
Intel,"Don't start working in airgapped infrastructures; if you do, the whole world could be burning, and you'd be chugging along like nothing is happening.",Negative
Intel,"Man.  I remember when CDK (Management Software for car dealerships) went down awhile back.  That was NOT fun to deal with at work.  Just because our DMS went down, didn't mean we could go home.  Nope, it was instead: Figure out how to do everything on paper for now and we'll back enter it all into the system when it comes back online.",Negative
Intel,I work at a mom and pop shop we were down from last Wednesday to this past Monday. This must have had something to do with it for sure. We had to take manual orders and punch them all in on Monday. Not sure if this was really the reason but seems like maybe they did run our cloud system.,Neutral
Intel,What about downdetector downdetector?,Neutral
Intel,"From ""Mostly Harmless""   ""A meteorite had knocked a large hole in the ship. The ship had not previously detected this because the meteorite had neatly knocked out that part of the ship's processing equipment which was supposed to detect if the ship had been hit by a meteorite.""",Neutral
Intel,I bet it was that devious shark!,Negative
Intel,AI coding,Neutral
Intel,I guess that intern that tried to work at Cloudflare just got a job at AWS.....,Neutral
Intel,Ralph,Neutral
Intel,I saw a “do not push” button and I pushed it. Sorry guys .,Negative
Intel,My bad,Negative
Intel,"Epic Games at least. (All games using EOS/EAC are broken, such as ARC Raiders, Apex Legends & Palworld).",Negative
Intel,Anyone have a bead on Lizard Squad? What are those fucks up to nowadays?,Negative
Intel,Wreck it ralph's back at it,Negative
Intel,ralph?,Neutral
Intel,Mom said it's my turn to break the internet,Neutral
Intel,Vibe coding,Neutral
Intel,Ai,Neutral
Intel,The infamous hacker group known as “Kim Kardashian”,Neutral
Intel,The hacker known as 4 Chan.,Neutral
Intel,Probably all the AI they're trying to cram into the various web services.,Neutral
Intel,A group called the Aisuru botnet. They been setting records and knocking down cloudflare and aws left and right,Negative
Intel,I just wanted to know what the big red button did.,Neutral
Intel,Probably Ralph trying to find a rare spare part to fix his friend's arcade machine,Neutral
Intel,Could be attacks from various hacking groups.  Could also be AI and it's training. Their electricity usage is atrocious and so are the amount of requests they make to train them putting a lot of strain on many grids as well as network infostructure. Plus a lot of key companies have been pushing to replace a lot of developers with AI making their codebase fall apart with nonsensical code that doesn't really work.,Negative
Intel,"It obviously gets America OnLine, you dummy!",Negative
Intel,Old people email accounts,Neutral
Intel,"You just had me go down a rabbit hole to find out.  To start, the original AOL died early 2000. It doesn't exist anymore.  The name has been tossed about between Mega-corps since it's sale in '07. It's just an old dirty wash cloth now.  Under the AOL name, media groups were acquired, The Huffington post for example, but AOL might as well be the middle manager at this point as it's pimp is Yahoo.  The managers are probably retelling stories they heard from the old hats about the good 'ol days but don't really know they are just white wash by the looks of it.",Negative
Intel,Who else will tell me when I have mail? I created my original email account 20 years ago and it will stay alive for as long as I do.,Neutral
Intel,aol exisits because people don't realize turning off subscription payments is a checkbox,Neutral
Intel,"My dad's email address still ends with `@aol.com`, so that's about the only thing I can think of.",Neutral
Intel,"Still give people access to the internet through ""Free Online Trial"" CDs.  They printed enough of those that many future generations will still be able to enjoy going online for free.",Positive
Intel,I just created an AOL email account because @aol.com is the shortest one I could think of,Neutral
Intel,"[AOL discontinued their dial-up service *two months ago* \(Sept 2025\).](https://www.bbc.com/news/articles/cj0yy3e6z4zo) --  Really wondering who was still using dial-up internet in 2025.  Single web pages are often several megabytes these days (just measured the old.reddit.com frontpage — 953 KB).  That's like a ~5 minute download on dial-up (and it could be way longer if you have crappy phone lines, which I assume you do if you live in a location without broadband access).    Email would be viable, although I wouldn't want to use Gmail's web interface because again, web pages are big.  But if you used a traditional mail client (e.g. [Thunderbird](https://www.thunderbird.net/), or [mutt](https://en.wikipedia.org/wiki/Mutt_\(email_client\)) if you're a baller) you'd be fine.  I mean this is kind of obvious to those of us that lived through that era and used email, SMTP/POP3/IMAP haven't changed and email is still mostly text (although HTML emails with embedded images is more prevelant than it was back then).  Makes me wish there was an NNTP (Usenet protocol) interface to reddit so we could access it with traditional newsgroup reader over a low bandwidth link.  No reason that you need to download a full megabyte to get a list of headlines which in pure-text form are a couple of kilobytes.",Neutral
Intel,last I heard they were in the advertising biz,Neutral
Intel,My boss has an AOL email address as his main one. He's 38. Baffling. He needs to modernise and get a yahoo email address.,Negative
Intel,Make AOL great again /s,Positive
Intel,SysAdmin's guide to troubleshooting,Neutral
Intel,"Stages of sysAdmin troubleshooting:  ""It couldn't possibly be DNS""  ""No way it's DNS""  ""It was DNS""",Negative
Intel,I was a sysadmin for 10 years before going into cybersecurity but the DNS haiku always rings true.   it’s not dns. it can not be dns.  it was dns.,Neutral
Intel,"That's barely trying, you'll be right 99 times out of a 100 if you say dns.",Neutral
Intel,Most likely. Hostinger had a major outage this week that was DNS related.,Neutral
Intel,Say the line bart,Neutral
Intel,Or BGP,Neutral
Intel,"Our deskside team have this blown up to A3 as one of the many things stuck to their fishbowl's window to stop users seeing if anyone is in the fishbowl.   They're waiting for one of the plotters to need support so they can do a A0 ""test print"" using it.",Neutral
Intel,Last weeks Cloudflare wasn't DNS.,Neutral
Intel,https://preview.redd.it/fc56mr4nfn3g1.jpeg?width=720&format=pjpg&auto=webp&s=52d1e800481492c15987778e6f174de4464fb975,Neutral
Intel,Nothing to do with AI: the post mortems are public.,Neutral
Intel,Yeah… I don’t have any proof but can’t help but feel like there’s a correlation between ai adoption and these outages,Negative
Intel,Almost like they're related,Neutral
Intel,*better.  Hopefully the internet goes down for a whole year,Negative
Intel,"Shouldn't it be a "" read only "" day since tomorrow is Thanksgiving ?",Neutral
Intel,The web stopped being meaningfully decentralized [a while ago](https://www.opendemocracy.net/en/digitaliberties/web-began-dying-in-2014-heres-how/).,Negative
Intel,That's basically CloudFlare...,Neutral
Intel,Only if it's a QFX,Neutral
Intel,I think the router would win in a staring contest,Neutral
Intel,Vibecoding is a plague.,Negative
Intel,"Clanker code lol, love it.",Positive
Intel,"Steam's just using Akamai, right? Seems a lot more reliable than AWS and Cloudflare. And that downtime shown here was probably just their weekly tuesday maintenance",Neutral
Intel,Adopting clanker code after taking Ls is like hiring Amber Heard to clean up your sheets.,Negative
Intel,Nintendo stayed up the whole time too.,Neutral
Intel,"Sysadmin here. After years of losing battles with management on the right way of doing things, I quit.  There is nobody to replace me and the service will degrade faster than they can replace me with AI.",Negative
Intel,"Without having looking into any of the RCAs, not totally surprising that there would be more outages around this time of year.  Q4 is a big time of year for pushing out patches. 1) so many people take time off in December, which means any deployments, projects, deliverables, etc realistically probably need to be out in November to be out by EoY. 2) Regulation and compliance requirements usually go into effect in January, so in order to meet those requirements, see 1. 3) Holidays are the biggest time for cyber attacks, so orgs want to make sure they are pre-emptive, and high CVEs come more frequently that need to be addressed. 4) Holidays and winter increase demand on online services",Neutral
Intel,Damn Clankers!,Negative
Intel,i remember this as the broken glass meme,Neutral
Intel,It's ok for low impact stuff that doesn't matter much and does not interact with ANYTHING of any importance. Basically if someone wants some excel macro to merge txt files or something. lol,Neutral
Intel,Why don’t they just install more RAM? Are they stupid?,Negative
Intel,According to other comments the downdetector downdetector is down tho,Negative
Intel,[https://downdetector.com/](https://downdetector.com/),Neutral
Intel,"Steam is still online, as always",Positive
Intel,subtle foreshadowing from the earth devs?,Neutral
Intel,Russia's munching on the underwater cables again.,Negative
Intel,You still don't use your AOL CD-ROM to browse the Internet?,Neutral
Intel,One grift doesn't make another grift good,Negative
Intel,Web3 is bundled to an open sewer and is too slow to be useful in it's full implementation.,Negative
Intel,Web3 and decentralization are completely separate concepts.,Neutral
Intel,"correct, I hate everything",Negative
Intel,"Who is hating the idea of decentralization? The whole original point of the www, and the P2P routing it's based on, was decentralization.  It's like going to a water park and hating on water.",Negative
Intel,Me too but with the addition that I haven’t spent a single second in those games xD,Neutral
Intel,Not currently.  I guess.,Neutral
Intel,looks l;iek it back now. finals works,Neutral
Intel,"I am not sure, like do not take my words as truth just hypothetical. Probably they swapped some stuff with AI code wise not high quality and they push it so now we have problems on this side cause they tried to cut costs with AI and instead increased the problems",Negative
Intel,"I don't think it's actually worse, these spikes are a common occurrence, but it took an actual shutdown for the people to start fanatically tracking the graphs. Now it's just trendy to post every single one.",Negative
Intel,"normally me too, but my fav game, the finals went down so i was on the case",Positive
Intel,Worldwide web is so 90s you boomer. Get with the cool kids you silly goose,Negative
Intel,No,Neutral
Intel,nah reddit still up,Neutral
Intel,my fav game went down.,Negative
Intel,Disgruntled worker in the datacenter hits the panic button. Thousands of servers instantly off; hundreds of IT staff working 30 hours to bring everything back up.  Bonus: Test of power backup system. Techs forgot to switch back to main power before shutting the back up down. Thousands of servers down etc..,Negative
Intel,"Meanwhile, the SOC that day:   [Frantic cyber noises]",Neutral
Intel,"Summer 2022 we had a 10 day outage at work.  We were still covid wfh, the weather was glorious. Best summer ever",Positive
Intel,"it did, and it didnt make it easier, in fact it was so fucking much worse",Negative
Intel,Wanna swap?,Neutral
Intel,r/FoundSatan,Neutral
Intel,Like a plow truck driver on a snow day.,Neutral
Intel,https://detectorsdowndetectorsdowndetectorsdowndetectorsdowndetector.com/ seems fine,Neutral
Intel,This is the second Hitchhikers Guide series reference I've seen today.  The universe is definitely telling me I need to reread it.,Positive
Intel,"The one where the front fell off? That's not very typical, I'd like to make that point.",Negative
Intel,Probably chewing on the undersea cables again like it’s his personal snack bar.,Neutral
Intel,Nom nom nom,Neutral
Intel,I bet it was that Joe fella.,Neutral
Intel,We need to get a reeeeaaaaallllly long shark cage to protect the under sea cables.,Neutral
Intel,The shark's name is Pyotr,Neutral
Intel,No more babies! Stop baby shark!,Negative
Intel,"Um, akshewally, it's called vibe coding because of the negative connotations of passing off slop as your own work, thank you very much.",Neutral
Intel,What no way ChatGPT told me we could replace all our engineers and nothing bad would happen /s,Negative
Intel,lol.  you may actually be right.,Positive
Intel,![gif](giphy|55itGuoAJiZEEen9gg),Neutral
Intel,https://preview.redd.it/ym4j1hkfjn3g1.jpeg?width=3024&format=pjpg&auto=webp&s=6e222b1ff1f4b297c9e0539c33534101c717c0b1,Neutral
Intel,Jesus that really is how you go through life isn’t it?,Negative
Intel,"yea im also in a group of ppls that need to know everything about anything on ""their territory"",    usually i go with asking first,    but if there is some device and no one knows what for some things are inevitable",Neutral
Intel,Yup. All I wanted to do was claim my free game.,Neutral
Intel,Pretty sure it’s from a salty EA rep.,Negative
Intel,America Offline now,Neutral
Intel,I still have mine from when i was 12,Neutral
Intel,You're not fucking lying. I get emails from my colleagues.that have the AOL domain and evey single one of them is fucking clueless,Negative
Intel,Im 25 and use AOL as my main email lol. I get a lot of older people who are very surprised.,Neutral
Intel,Yeah haha...those old people...  *sweats in Hotmail*,Negative
Intel,My backup email. For secret stuff! Haha,Neutral
Intel,I'm still not over Yahoo renewing Community for another season.,Negative
Intel,Kinda like how today's AT&T is not composed of ANYTHING from the old AT&T.,Neutral
Intel,"It didn't die in 2000. It bought and merged with Time Warner, flexing its muscle at the height of the dot-com boom, and forming AOL Time Warner (the companies were close enough in market cap that AOL shareholders ended up owning 55% while Time Warner shareholders got 45%)",Neutral
Intel,"My aunt still uses AOL. Like, the program. Don't even get me started.",Neutral
Intel,I figured Americans skeet shot 99% of those cds to be honest.,Neutral
Intel,"""Hmm, let's try 8.8.8.8""",Neutral
Intel,Can confirm - am sys admin,Neutral
Intel,No one ever looks to check if it’s DNS. So much stress lol,Negative
Intel,https://imgur.com/Lj827hB,Neutral
Intel,And here's why it was a Good Thing it was DNS.,Positive
Intel,I mean Cloudflare is a DNS provider,Neutral
Intel,Amazing ad placement,Positive
Intel,"Again, again! Not explicitly AI but I'm sure it's a baked in 'feature' on these  Edit: where's my screenshot 😔",Negative
Intel,Where's your uBlock son?,Neutral
Intel,I got alot of anti ad shit on      That ad like people said what a great placement thanks 4 the share just add onto what i stated.,Negative
Intel,https://preview.redd.it/9dpexco46o3g1.jpeg?width=1284&format=pjpg&auto=webp&s=315e126edd62e8e19a9f38f65932b20e0024699c,Neutral
Intel,https://preview.redd.it/jcn7xf51kn3g1.jpeg?width=1080&format=pjpg&auto=webp&s=3d992d185fe46a0223b5590be0702eb9af56954a,Neutral
Intel,https://preview.redd.it/n7gp7p60ds3g1.jpeg?width=1179&format=pjpg&auto=webp&s=68c7cf3f43ccc4f61d900e00da5b635957a16db6,Neutral
Intel,https://preview.redd.it/qpos67tcet3g1.jpeg?width=1080&format=pjpg&auto=webp&s=7e9fd00b5a2d7011a1eb4da845c3694d106f5911  Ai networking ad on top,Neutral
Intel,"We've been in a code freeze for a week already, and doesn't thaw until Dec 2. (Airline industry)",Negative
Intel,Those are datacenter switches tyvm. Everybody knows the giant Internet router is an MX204,Neutral
Intel,can't spell AIDS without AI,Negative
Intel,❤️,Positive
Intel,When they say they're replacing you just send em this gif  ![gif](giphy|pDgHg2Lcju3Ty),Neutral
Intel,yeah I edited it  Feel free to use it  (hopefully we don't have to),Neutral
Intel,It's an okay basis to base your code on at best. Leaving AI to be an assistance than a hire is the best course of action nobody wants to take.,Negative
Intel,Even nvidia can’t afford ram right now.,Negative
Intel,They tried to download some more but the damn Internet was down,Negative
Intel,"Just in case that goes down, you can use [https://downdetectorsdowndetector.com/](https://downdetectorsdowndetector.com/) to check on it. (Yes, there are more in this chain)",Neutral
Intel,Thx. Figured it was maybe an mis report or something from a few games being offline. If it was full platforms I feel there would have been way more news about it.,Neutral
Intel,those destroyed by American/Ukrainian intelligence?,Negative
Intel,"Haha, I was trying that but I cracked around 350 games in when a game on my Steam Wishlist showed up as a freebie on Epic.",Positive
Intel,That is precisely what happens at my company right now. Bunch of grifters promised sooo much with ai. Everything started to fall apart after a while. Nobody seems to care anymore because you know they just tried to replace us. Luckily our competitors are equally shitty so maybe we won’t go down first. And i hope that a handful of senior devs who still know how to do their job will keep all the pieces together before some major disaster.,Negative
Intel,Unc,Neutral
Intel,"It was a ransomware attack, as I also suspected. So most likely a disgruntled worker in the datacenter who got off with $500k worth of Bitcoin. That job was starting to suck as the business was going downhill. Everyone in my department was upset they didn't hold the place for ransom first, especially as we got laid off six months later.",Negative
Intel,Why on earth were they bringing things back up on backup power? First mistake...,Negative
Intel,"\> \[Frantic cyber noises\]  *Plays the ""We're being hacked!"" line from CSI Miami on repeat*",Neutral
Intel,"Phone ringing non stop, the landline.",Neutral
Intel,"No thanks. I'm rather enjoying getting paid for doing fuck all, just because it could be back ""any minute now"". Been like that for two days now",Negative
Intel,![gif](giphy|3JsJ1zYjTC2vm),Neutral
Intel,"hilariously, it's down as of right now: [https://i.imgur.com/9AJYVyL.png](https://i.imgur.com/9AJYVyL.png)",Negative
Intel,"God, I love human ingenuity/humor.",Positive
Intel,![gif](giphy|DQOaFR6T8ndajFmFZN),Neutral
Intel,"K, I'll bite.... Joe who?",Neutral
Intel,Maybe design the cables so the front falls off.,Neutral
Intel,Vibe coding actually has the negative connotation. It implies it’s all done by ai without a real dev.,Negative
Intel,vibe checked,Neutral
Intel,"I work in dev ops and in the last year ive noticed a shit ton more failures that I have to deal with, most of them related to AI code.  Yea it looks pretty and it may compile, but in production it rarely works without major intervention.",Negative
Intel,"Im just a holy ghost, if you want to talk to Jesus he is arm wrestling satan right now.",Neutral
Intel,At least they don't have to change their name,Neutral
Intel,Emily is away,Neutral
Intel,How're the knees?,Neutral
Intel,I still have my Hotmail account I made in 1997...,Neutral
Intel,"Wait, what? Wasn’t community over? Does this mean I’ll have more community to watch in the foreseeable future? Also r/unexpectedcommunity",Neutral
Intel,"Is there a typo somewhere, or are you just not satisfied with S6?  I think it was OK. Didn't have the magic when all the crew was together, but it was good imo.  Still need a movie though",Positive
Intel,How so?,Neutral
Intel,BEGONE GOOGLE DNS,Neutral
Intel,"Ugh, had my own bunch of issues using 8.8.8.8. Only as a fallback.",Negative
Intel,"Yes, Cloudflare is a DNS provider.  That's not their primary business though, and it's not what caused the outage last week.  [Last week's Cloudflare outage was a CDN issue](https://blog.cloudflare.com/18-november-2025-outage/).  That being said, this haiku is *usually* spot-on accurate for network issues.",Neutral
Intel,Amazing!,Positive
Intel,https://preview.redd.it/r1p996kuhn3g1.png?width=1008&format=png&auto=webp&s=a3716f1a6d65e73b0022294132d21a9a608a64be,Neutral
Intel,https://preview.redd.it/1lwl0rxbon3g1.png?width=1080&format=png&auto=webp&s=dc83928c76489ddc09d9c96d1b7d6b39bc4c1ad0,Neutral
Intel,*looks at QFX serving whole market*  *Stares in worry*   You're right tho,Negative
Intel,"Nah, you can cure AIDS now (the stuff is still getting rolled out).  But you can NEVER cure Clanker.",Negative
Intel,"with ai, we probably will for the future. keep it, maybe u can post it in the next outage",Neutral
Intel,Sam Altman laughs in his supervillain lair.,Neutral
Intel,"https://downdetectorsdowndetectorsdowndetector.com, and if that fails, https://downdetectorsdowndetectorsdowndetectorsdowndetector.com",Neutral
Intel,says u,Neutral
Intel,At data center they don't just use UPS battery they use huge diesel powered generators. There's enough power produced to run the servers.,Neutral
Intel,CSI Miami? Your go-to isn't the classic two-people-one-keyboard scene from that one episode of NCIS?  ![gif](giphy|yUlFNRDWVfxCM),Negative
Intel,https://preview.redd.it/w575sfy3xo3g1.png?width=1044&format=png&auto=webp&s=8de985c23c95ef7fa4ff203610f8025caf7d2468,Neutral
Intel,"Not a cable, but...  https://preview.redd.it/4nni3ar6rn3g1.jpeg?width=1226&format=pjpg&auto=webp&s=6b776663c70156fe27acfbd42fa96b89ef29795c",Neutral
Intel,Joe Biden ofc. Trump said so.,Neutral
Intel,"Joe Swanson, from Family Guy",Neutral
Intel,I hate it when the front falls off of my internet and it has to be towed outside the environment.,Negative
Intel,"Yeah, it basically means “AI did it, humans barely touched anything.”",Neutral
Intel,Vibe coding's negative connotation is that you tried to code while you had a spiked vibe in your butt.,Negative
Intel,I really fear AI. Especially for critical systems. Like life support. Aircraft software. What happens if nobody could fix it.,Negative
Intel,Gettin creaky,Neutral
Intel,How is your back doing these days?,Neutral
Intel,https://en.wikipedia.org/wiki/Community_season_6,Neutral
Intel,What I meant is that I was pretty shocked when I found out the internet dinosaur I only ever knew as an obsolete email provider was the one renewing Community.,Negative
Intel,"Old AT&T was part of ""[Ma Bell](https://en.wikipedia.org/wiki/Bell_System).""",Neutral
Intel,[1.1.1.1](http://1.1.1.1),Neutral
Intel,[4.4.4.4](http://4.4.4.4),Neutral
Intel,"Fine, we'll try them all until we find a working one. 8.8.8.9?",Neutral
Intel,"Reddit is an odd place: the comment about the screenshot is positive, the screenshot after the fact because I'm dumb and on mobile is being downvoted.   What'd I do?",Negative
Intel,Don’t get me wrong I love me my 5120s. Old man MX204 though remains one of the GOATs,Positive
Intel,"Get the clankers addicted to gachas, they're prone to that apparently.",Negative
Intel,"FYI: HIV is the virus, AIDS is the disease caused by the virus.  Afaik there is no cure for HIV, what there is are treatments to reduce the viral load so much that people can lead basically normal lives even with HIV.",Negative
Intel,![gif](giphy|xT9DPr4VjeCgeiLoMo),Neutral
Intel,https://detectorsdowndetectorsdowndetectorsdowndetectorsdowndetector.com/   You forgot one,Neutral
Intel,I love the fact that people do this 🤣,Positive
Intel,Yeah but why would they even be on generator power? The big red button should cut *all* power. The generators shouldn't have started up.,Negative
Intel,"Oh yes, that's the scene. But the typing sounds are implied. You need something more cringy to loop on repeat!",Negative
Intel,"i waited 93 seconds for 800ms tracerts,     downX5 is down, checkmate atheists",Negative
Intel,https://preview.redd.it/f1lbh44u7o3g1.png?width=476&format=png&auto=webp&s=29656ee8ff812ff3c3914dff7a66adedc9bef500  His gluttony knows no bounds,Neutral
Intel,Not J'my Momma?,Neutral
Intel,"Let's not kinkshame, don't bundle masochists into the vibe coding group",Negative
Intel,Just look at the failing US infrastructure. We have plenty of old power plants and machines that need to be running 24/7 and the old guys who used to manage them are retiring at this point and theres no one to replace them.  Its not going to go well.,Negative
Intel,"Oh, misunderstood, cheers",Neutral
Intel,I've been using 4.2.2.2 since the '90s   [Just looked up the history of this legend — interesting read](https://web.archive.org/web/20130215021006/http://www.tummy.com/articles/famous-dns-server),Positive
Intel,God damnit,Negative
Intel,Yeah 5120s are doing well. No MX204s in my network so I have limited experience with them. Glad it worked so well for you,Positive
Intel,"I mean, CyGames banned a huge number of accounts for being bots during the Kitasan Black banner. Even gacha is fighting off the clankers.  (There were so many bots it was legit causing people to lose connection to the servers).",Negative
Intel,"There is a new vaccine that can prevent it that's in the final stage of tests, and there are treatments that clear out HIV coming in the next year or two.  I wouldn't say something unless I found out about it.",Neutral
Intel,https://preview.redd.it/kolkeuwg5p3g1.jpeg?width=1079&format=pjpg&auto=webp&s=3d28fb0cb5fcdb62a282737f614011cee1126161  Now mine wants a kiss,Neutral
Intel,Oh nice. It used to be that you could set [8.8.8.8](http://8.8.8.8) as your primary and Google also had [4.4.4.4](http://4.4.4.4) as the secondary. But I never see anyone using [4.4.4.4](http://4.4.4.4) so I'm not sure how legit that is anymore. Used to be though :),Neutral
Intel,"SHIT I messed up, the secondary for Google is [8.8.4.4](http://8.8.4.4) \- Thanks for the link it was a good read :)",Negative
Intel,Mark of the beast,Neutral
Intel,My background is in provider networks. In that context the 204 is best compared to something like a Toyota Hilux: insanely cost effective and it will do anything you ask of it but it might be a little rough around the edges.  The MX line of routers is Juniper’s best gear by far. Even in a datacenter they’re amazing for interconnection etc.,Positive
Intel,Ah crispr thank you,Positive
Intel,That's awesome. There's another side of my client's network where they use a lot more MXs. I mostly support QFXs and PRXs. I'm reading up on the 204 and I see the aptness of the Hilux comparison,Positive
Intel,I bought used 150$ in Turkey two months ago,Neutral
Intel,"I bought used $100 in Antarctica three months ago, it smells like penguin shit and got hit with $49 in import taxes but still cheaper than yours.",Negative
Intel,Well it was either that or go retail and we know how much they’re going for now. Plus resellers will eventually start charging retail price since they know they’ll be able to anyways,Neutral
Intel,"Man, I messed up. I converted it from my own currency and thought it was more, but I just checked — 5,500 TL is actually around $128. So yeah, I got it cheaper than you did, and I’ve already tested it. It’s been running perfectly with EXPO for 2 months",Neutral
Intel,"A couple things stand out to me here:  - CPU not going above 60°C is strange, unless you’re running a very beefy water cooler. Did you try running something like Cinebench and compare the results to other 5700X?  - Are those 16gb of RAM a single stick or two? If two, are the in the correct slots (one free slot between the two)?  Intels gpu drivers are known to be quite heavy on the CPU, but I don’t think a 5700X should struggle that much.",Neutral
Intel,So  the cooler is not anything special like an assasin spirit evo 120 but I got 2 new coolers for the case because i was sure heating was the problem.But under load It researched 75c yesterday. but it runs cool thats what I was trying to say.I will fix that.  Dual slot and they are in the first and third slot.  My motherboard doesnt support PCIE4 and people told me that could be weird but from everything that i saw on videos and forums it shouldnt affect that much the fps.,Neutral
Intel,"The slow RAM is definitely dragging your CPU down a bit, but it shouldn't be that much of an issue. PCIe 3.0 should also be fine for that combo.  Have you tried running a CPU benchmark?",Neutral
Intel,yes and  yes furmark I think? Seemed normal.,Neutral
Intel,"but thats my problem everything should be normal and ok running ,not optimally ,but its much worse.",Negative
Intel,What were the results? Especially Cinebench?,Neutral
Intel,Cinebench 2024 did it now and it is 757 multicore,Neutral
Intel,Can you up your budget to $350?  9060xt 16gb is more powerful than wither of those options and vram should keep it relevant a long time.,Positive
Intel,"I think the b580 is the better buy. The 5060 has worse fps/dollar, and less vram",Negative
Intel,At a base it's around 20ish percent better but the extra vram in the b580 might help it last longer.  I'd go for the b580,Positive
Intel,"Intel Arc is still today for a  Tech Savy Enthussiast with alot of patience,   There is alot of probleme to be fix still.  The 5060 is your best bet if you want to game without have to troubleshoot every time. and the software stack (feature) is superior, Even if stability-wise the last year of NvidiaApp driver as been a hit and miss.  Radeon would be a choice but checking the current pricing for 9060 xt 8go variant. the saving dont make up for the nvidia advantage.  Specialy that in a close futur Nvidia Neural Texture compression is going to leave Beta and turning the VRAM argument to old history.     [https://www.youtube.com/watch?v=fhECKZQI\_Y8](https://www.youtube.com/watch?v=fhECKZQI_Y8) Here you can see a recent revisit of the Intel Arc B580 driver situation.",Positive
Intel,Never looked into AMD cards (only reason I looked at Intel was because I was bored in my hardware class) but I will take a look,Neutral
Intel,"The b580 is a decent budget card, but the 16gb vram of the amd make it an attractive option for the price point.",Positive
Intel,"My old case was Fractal, thought I recognised it from the inside. Such a solid case.",Positive
Intel,"Yo, I did the exact same thing a few months ago. Disabled all the motherboard, RAM, GPU, etc. lights too .Same case, but with the mesh panel.  Set up looks so clean now.",Positive
Intel,It really is. I went into a micro center trying to get a HAVN bf360. It was OOS and they had this case for $100 on special. I’m happy with the decision.,Positive
Intel,"My PC won't wake up after sleep on previous 25.10.2 so I have downgraded to 25.9.2. It seems that AMD has added into Known Issue in 25.11.1.  >Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.   Ahhh. It is caused by HDMI 2.1... I can't use a DisplayPort on my LG C2 42"" TV sadly so I'll have to stay on 25.9.2 for now.",Negative
Intel,"wait, so the branching did not happen? for RDNA2? I was under the impression that it was already in effect.",Neutral
Intel,"That last known issue is what a lot of us experience. Not fixed, can't use this one either.  Leaving your pc for long enough, like 25 minutes and your system just BSOD quickly into reboot.",Negative
Intel,I just did a clean DDU install to 10.2 last night because something weird was going on. Of course 24hrs later a new driver drops,Negative
Intel,anyone know if the low gpu usage was fixed for Battlefield 6? I had to roll back to 25.8.1,Neutral
Intel,"After install i cant open Adrenalin app, I get starting up for a few seconds and then it's closed and the tray icon is gone, too 😿.",Negative
Intel,I'm going to wait to see if others find it stable before I move on from 25.9.1 I think.,Neutral
Intel,Anyone can tell me if the new release fixes the Adrenalin Panel not showing when trying to open it? I’ve spent 1 entire afternoon try every solutions given by Google but today the problem is still there…,Negative
Intel,"So what's the issue with Cyberpunk 2077? It's been present for quite a few updates now.   I'm asking because I've owned the game since day one but I haven't been able to play it cause my old 1060 6 GB was struggling hard with it, since then I've upgraded to a 7700 XT and for one reason or another I haven't gotten to play it yet but every time I update my driver and check the patch notes it's always a problem with it.   Can anyone with it installed and on RDNA3 tell me if it's playable?",Neutral
Intel,So does this mean Arc Raiders will stop randomly crashing in Windows?,Neutral
Intel,Just installed these zero issues so far!,Positive
Intel,"I dunno what people are expecting from Redstone?  It's on the game devs to implement, Blops 7 has the AMD Ray regeneration element of Redstone baked in.  It's not gonna be some driver toggle and all of a sudden you've got ray regeneration across all games.  Also all the people claiming ""not every game needs an optimized driver, Arc didn't get one"" when the RDNA 2 controversy happened, look, there it is, the optimisations were just late, hopefully it'll fix the crashing some people had in the game.",Neutral
Intel,Anyone know if this fixes the pink artifacting on Chromium based applications for the 7000 series GPUs? Can't check myself because I'm at work.,Neutral
Intel,There was a long delay with the blank screen. Made me a bit nervous,Negative
Intel,At this point i'm sure that cyberpunk will never be fixed.,Negative
Intel,Apparently I'm staying at 23.9.1 because I wanna keep using FSR4 INT8 with my RDNA 2 card.,Neutral
Intel,No fix for being unable to enable Noise Suppression...,Negative
Intel,When does Linux get this,Neutral
Intel,"Unfortunately after updating to 25.11.1 (even with a fresh install after using DDU), on a 9800X3D + 9070XT I am no longer able to open the Adrenalin software at all. It's the same issue as described in this post, caused by some conflicts between having both the RDNA3/4 drivers and RDNA1/2 drivers installed at the same time: https://www.reddit.com/r/radeon/comments/1okhlbw/_/  I had to roll back to the 25.10.2 combined driver which works fine with this setup without any issues, but yeah what a shame. Really hope AMD can resolve this issue shortly in future updates. You would think having both the latest gen AMD CPU and GPU would play nicely with each other, but alas...",Negative
Intel,"Why does the AMD install manager never find the updates for me? The AMD install manager only ever says the AMD Chat is available for install.  To get the updates, I have to uninstall 'AMD install manager' which allows me to manually check for updates in Adrenalin.",Negative
Intel,"Guys, I think I figured something out for those experiencing crashes. My RX7600 was overclocked in the default setting. I created a custom profile that matches the old default and seem to have achieved what appears to be stability in BF6.",Neutral
Intel,i am stable now in BF6 on AMD Adrenaline 25.11.1 ... and the game feels super smooth with FSR on 4k Ultra... so it was the AMD Driver 25.10.2 which was crashing ... annoying :-)  7800X3D + G.Skill Trident Z5 Neo RGB 64GB (2x32GB) DDR5-6000 CL30 + ASUS ROG Crosshair X670E Hero + 7900 XTX + Corsair Shift Series RM1200x,Positive
Intel,"Why is enhanced sync still broken? That was a feature I used to mitigate latency while capping frames to 120 and helped get rid of tearing. Now having it on causes major stuttering in a lot of games. I did come to find out that Vsync can be enabled globally and I'm not sure how long that's been a thing. Since being on the 5700 XT, 6900 XT, 7900 XT and now 9070 XT, I was never able to use it globally but it's been quite a few drivers since I've checked if it worked. I use Vsync and Gsync on my 5070 ti build and notice little to no added latency so I'm glad this is doable with Vsync and Freesync but I did like using enhanced sync and capping the frames to 120 better but this will do.",Neutral
Intel,Windows update keeps trying to update my driver.,Negative
Intel,I'm still having problems with FreeSync stuttering with the RX 7900XTX and this driver. Only when I revert to version 25.9.2 are the stutters gone.,Negative
Intel,No FSR4 on RDNA3 no care,Negative
Intel,"For how long ? One year already...Cyberpunk 2077 not fixed yet.   ""Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.""",Negative
Intel,Yes thank you AMD for fixing Arc Raiders. I had to revert to 25.9.1 to stop the exception access violation issues. This would happen mid-game and I'd lose my entire loadout. Here's to hoping it works.,Positive
Intel,Awesome no stated support for Outer Worlds 2.... I guess driver timeout while playing it is not a driver problem...,Positive
Intel,"I noticed that this fixed my Geekbench scores     When I got my 9070 XT a couple weeks ago, my Geekbench 6.4 scores for OpenCL and Vulkan were about 185K and 187K.  Then they mysteriously dropped to around 135K each, and I believe it was after updating Adrenaline.  Now I just updated to 25.11.1 and I'm back at 184K and 189K for OpenCL and Vulkan.",Neutral
Intel,"> The AMDRyzenMasterDriverV30 service failed to start due to the following error:   The system cannot find the file specified.  Source: Service Control Manager, Event ID: 7000",Negative
Intel,This driver was way better than the version before it(for me at least).,Positive
Intel,"It's been almost 6 months and the 9060XT still crashes in DX12 UE5 games, especially with FSR4.  FFS AMD how long is a fix going to take?",Negative
Intel,"Since switching to this version, I've been experiencing constant driver crashes. i use RX7700XT sapphire pulse GIGABYTE B650 Gaming X AX V2 with 6000mhz 32 gb ram. I don't know which driver caused the constant pink artifacts when I have graphics card acceleration enabled, but it's getting worse with every update. Why AMD? -.-",Negative
Intel,"Sorry, I'm not very expert, I installed AMD version 25.11.1 from 0, before when I started the PC AMD was already open now instead the icon appears but if I press it says ""Amd adrenaline starting up"" is everything normal?",Neutral
Intel,"I am an RX 6800 user, so on the RDNA branch of the driver,  25.11.1 introduced a severe performance regression on a DX9 game (Fallout New Vegas) , with framerate basically getting halved over what I had before  Downgrading to 25.10.2 fixed the issue , not sure if replicatable (I use 70+ mods) , and not sure if it affects any other DX9 games other than Fallout: New Vegas  (My cpu is an intel i5 10400F in case that matters, in New Vegas, on the 25.10.2 Driver, cpu utilization is almost always between 70-100% on the primary core that the game uses, while on the 25.11.1 driver , it was consistently at or below 50% , which leads me to suspect the new driver caused a regression in CPU utilization in DX9 and/or old single-threaded games, downgrading to 25.10.2 completely fixes the issue )",Negative
Intel,"Still no Redstone update. Well, wait for 25.12.1 just started. Hope AMD end the year with a bang.",Negative
Intel,"Brooooo, they didn‘t fix the flickering in BF6 when recording…",Negative
Intel,* Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.  * Fucking LOL.,Negative
Intel,25.10.2 completely broke vsync... not even a mention about this in the notes?,Negative
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,There is new AFMF features too.,Neutral
Intel,Did they fix the crashing for Outer Wolds 2 on 25.10.2?,Neutral
Intel,bf6 fps drop fixed?,Neutral
Intel,What about the cursor lock when pressing hotkey for adrenaline overlay? Is this fixed,Neutral
Intel,"Hopefully they fixed the anti-aliasing this time...  Nah, i'm sure they didn't.",Negative
Intel,"Don't know if anyone else has experienced this in Battlefield 6 on the last 2 drivers but whenever I uses those my GPU usage always stay at 100% load even with 144 fps cap on a 9070 xt. I revert back to 25.10.1 and then it stops doing that, reaches maybe 80% max in menu",Neutral
Intel,"I've been getting black screens since a while ago on my 6750xt, could be after a random alt-tab when gaming or after logging into Windows, on my tv connected via hdmi it looks green but my displayport monitor it is black, what is weird is sometimes  I can win+L and see the login screen again but if I log in it goes black, also while it is black and pc hasn't frozen yet I use an app called chrome remote deskop and I can see and do stuff from my phone, weird. Tried DDU, new drivers, nothing fixed it.",Negative
Intel,How is the driver ? 7700 XT here.,Neutral
Intel,Finally a potential fix for CPU metrics? Look forward to seeing if it’s true!,Positive
Intel,Didn't they just release something already? Now we're getting another like. Do I have to update my rx 9060xt,Neutral
Intel,do yall use ddu for every driver or do yall just update it with the app?,Neutral
Intel,"New AMD update 👏👏👏👏, I'll install it! Send Redstone as soon as possible!!!!! Thanks AMD!",Positive
Intel,I just can’t wait for the instant replay to be fixed. Ever so often when I save a clip the infame notification starts glitching and I know that means the video I’m saving will have graphical glitches as well. It looks like big-ish squares of the image af slightly out of sync with the rest.,Negative
Intel,"There is a bug with Minecraft when using embeddium/rubidium or any forks on the latest driver. Many textures don't render at all. Launching through curseforge fixes it, which is very strange...",Negative
Intel,I'm not seeing this on my 6600xt. Only say 10.2 is available. Do I have to upgrade to that then upgrade to 11.1?,Neutral
Intel,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.    Well that probably explains the crashes from the last driver whenever I locked my PC but that workaround is not an option. Good to see it's been recognised and being worked on at least.,Negative
Intel,Has anyone else been able to get FSR4 to work again with BF6? Worked for me before the season 1 update.,Neutral
Intel,Think this broke Vulkan in POE2,Neutral
Intel,"Hi [u/AMD\_Vik](https://www.reddit.com/user/AMD_Vik/), other players and myself are having problems using DXVK with latest drivers. From driver timeouts to black screen. I'm particularly having problems with Fallout New Vegas, but there is reports in other games. How can I help in fixing these issues? Examples: [https://github.com/doitsujin/dxvk/issues/4999](https://github.com/doitsujin/dxvk/issues/4999), [https://github.com/doitsujin/dxvk/issues/5204](https://github.com/doitsujin/dxvk/issues/5204), [https://github.com/doitsujin/dxvk/issues/4851](https://github.com/doitsujin/dxvk/issues/4851)",Negative
Intel,"After installing this update neither Cyberpunk 2077 or The Witcher 3 will launch through Steam anymore. The hitting play just attempts to launch the game and then turns right back into the play button. Only CD Projekt games, no issues anywhere else.  UPDATE: Actual games run fine if launched directly from their install location. It's the CDPR launcher that Steam usually auto opens that broke after this update.",Neutral
Intel,Hope this patch will fix the driver crash while playing Arc Raiders. It crashes in a way that slows my computer so bad and i have to restart it. Temps are fine. Everything is off except image sharpening.,Negative
Intel,I mean Arc Raiders runs perfectly fine even on 23.9.1. Game optimized new drivers are a joke.,Negative
Intel,Hi u/AMD_Vik  Thanks for the VR refresh rate fix. However a bug that I though was related but still isn't fixed are the Beat Saber VR game wall shaders as they are still broken/distorted. Those shaders work on 24.12.1.  Could you investigate this u/AMD_Vik?,Negative
Intel,I never updated to the most recent driver but when I open up adrenalin and the update manager it only shows the previous one 25.10.2  I had to go to the link to get the newest. Does it always work this way?,Negative
Intel,hardware ray tracing crashes both oblivion remastered and the outer worlds 2 after 5 minutes to an hour of play and it seems completely random on my 9070xt.   To even get it to last that long I had to set my core clock -300mhz and turn off variable refresh rate and hardware accelerated GPU scheduling,Negative
Intel,"Wish i could use this software propelry for my 7800xt, everytime i have adrenaline installed after couple hours of gaming the whole pc black screens and gpu driver crashes running with default settings on the gpu. You have to use DDU to get it back running, gave up with adrenaline and installed only the bare bone gpu drivers without adrenaline and installed msi after burned, it has been running for a month just fine under very excessive loads.",Negative
Intel,"25.10.2 already have Terrible Fps spike and stutter in Gaming, this update did not fix the Problem (wth happen amd??).. 25.9.1 is Still the Stable one",Negative
Intel,Did this fix the insane fps drops in 25.10.2?   Reported here https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps and here https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,Neutral
Intel,"Installing this on Windows 10 got me a system shutdown at the first go xD. Luckily, the second one went just fine.  Also, since you seemingly cooperate closely with Activision on CoD, can you fix CoD:WWII constantly crashing? I just bought a month of Game Pass to play the game, but it's basically unplayable.     It'd be nice if you somehow fixed CP2077 situation - FSR implementation, bugs etc. This game is a showcase every single reviewer runs, not Call of Duty...",Negative
Intel,Did AI create these new drivers?,Neutral
Intel,I can't even install it anymore as it doesn't recognize my iGPU (I have an R5 7600).,Negative
Intel,"The new version 25.11.1 still has the same problem that I had with version 25.10.2, that is, if while I'm in the game I press the Windows key on the keyboard, then when I return to the game the mouse cursor no longer works and I can't do anything anymore, which forces me to restart the PC, another problem is that when I open any game the overlay of the active Adrenalin techniques no longer appears in the top right, for the rest it seemed ok, but given the big mouse problem I mentioned above, I am forced once again as it was also for 25.10.2 to go back to version 25.9.1 which to date is the best and bug-free for my configuration with RX 9070 XT.",Negative
Intel,"Drivers fine for me on Arc Raiders so far, not had any issues with AMD drivers using a 9070",Positive
Intel,I'm glad the CPU metrics are showing again,Positive
Intel,"When I install this driver, I can't open the AMD Software any more. The start-up splash screen is shown for about a second and then it closes again. Doesn't matter from where I try to launch it. So it isn't the right click -> open bug.  There is no event in the Event Viewer.  I've reinstalled it, with prior DDU cleaning and disconnecting the internet connection, three times now... to no avail.  Anyone else?  Edit: Reverting back to 25.10.2 and it works fine. I'm tired of all these little quirks and annoyances I've had since I went for an AMD card...  Edit2: Tried 11.1 again and it worked now. The software started... once. The next time I tried to open it I got:   Download failed: Please visit [AMD.com](http://AMD.com) to download the compatible version of AMD Software.    I'm at the end of my rope here AMD... really getting tired of this",Negative
Intel,"Went to do the usual ""Leave AMD Experience Program"" after uninstalling the Installation Manager, but the option is gone.",Neutral
Intel,The update did not help. The problem with the driver crash remained (( ( Rx 7700 xt ),Negative
Intel,Any ideas for when the crashing when playing NBA2k25 is going to be fixed?,Neutral
Intel,"Is there another work around for system locking up?  My PC monitors never even turned off. I woke up this morning and the PC was simply frozen, had to turn off the power supply switch and turn it back on for it to work.  Couldn't even just do a hard reset.",Negative
Intel,"Im still having issues with easy anti cheat, rust game keeps crashing after a few minutes, maybe 2 or less",Negative
Intel,"2.5.11.1 fastest reroll for me to date, well done.   Booted arc raiders which now have support.  Game does not boot, instead I get a message frem arc davs that the driver has issues and want me to reroll to 25.9 😅   What a fucking joke",Positive
Intel,Shits been crashing my system since the update :( sapphire 7900xt,Negative
Intel,"Tested the new driver on 7700 xt, pink artifacts in browser and some weird flickering, some old bugs are fixed but  there are new issues instead, honestly it is not worth to update drivers at all if you find one driver that works without issues.",Negative
Intel,Still weird artifacts on COD MW2 game. Turned back to 25.9.1.,Negative
Intel,"With my Taichi RX 9070 XT OC after updating to the latest version 25.11.1 I still encountered the same problem that I encountered with the 25.10.2, which is that while I play Battlefield 6 press the windows button to go to the desktop and then return to the game my mouse crashes and I can't do anything, the only thing I can do is click ctrl+alt+delec and the mouse works and then I can restart the PC from there",Negative
Intel,"With this new driver, Adrenaline isn't automatically detecting Epic Games Store games (Steam games work fine). I tried with Fortnite, and it only adds to the games tab after launching it, but it doesn't work with ARC Raiders. I tried adding it manually, but I couldn't get FSR4 to activate. Is anyone else experiencing this?",Negative
Intel,"GV-R9070XTGAMING-OC-16GD    I have problems with Graphipcs since day one i bought from amazon.de. There is no driver that prevents some games from black screen and crashing on desktop, also Adobe Effects alongside Fortnite and others. The error is always amd software has detected a driver timeout on your system. Everytime i send logs, they updated drivers every 14days but no driver helped. I also made registry fix with increasing timeout from default 2s to 8s. Tlddelay did nothing, random black screens and app crashes to desktop. Also tried another cable DP instead of HDMI.    What can i do ???",Negative
Intel,"If anyone from AMD sees this. The recent drivers cause The Division 2 to consistently crash. It sometimes happens after 15m. Other times a few hours but is GUARANTEED to happen at some point. When it happens it's a hard reset case and doing so after about 10 times eventually wrecked my boot/login so I had to re-install Windows (bad AMD, spank!).   I assumed it was an issue with thelatest Windows update like the one that broke the other UBI Assassins Creed games a while back but when I installed 25.9.1 (with factory reset) I haven't had a single crash since.     7900xtx & 98003d.",Negative
Intel,Dose Arc raiders works now or not on new release 25.10.1 had prob with that game could not run it must go dx11,Negative
Intel,"Since this driver update my pc is unusable, only one screen loads the other stays black and after the os loads the screen just freeze, I can hear os sound like connecting and disconnecting of USB but the picture is frozen, I have the rx7900xt I tried to completely take the GPU off the motherboard and connecting back (after reinstalling the driver with factory reset when connected to the cpu display port) and it worked for some hours but after shutting down the pc and booting the next day it came back, you're saying the only fix for now is to roll back to previous driver?",Negative
Intel,У меня Мультимедиа контроллер выдает ошибку. Для этого устройства отсутствуют совместимые драйверы. (Код 28),Neutral
Intel,Noise Suppression still broken. 3rd release without that functionality in a row.,Negative
Intel,"Hay un bug que me suele pasar con varios de los ultimos drivers... cuando desintalo los drivers, la pantalla no vuelve, y no me deja saber cuando la desinstalación del driver termino, debo reiniciar la PC. Con RX 6800 XT.  Le eh pasado DDU, pero el error sigue estando.",Neutral
Intel,"Not sure if anyone else is experiencing this, but after this update, Adrenalin acts like BF6 isn't open so I can't force frame-gen thru the driver. On both 25.10.1/25.10.2 and 25.9.2, enabling frame gen in game doesn't work, so I've had to do it through the driver. On 25.11.1, NEITHER are working, frame gen completely non-functional. Tried DDUing/factory resetting 25.11.1, didn't work. Rolled back to 25.9.2 and works normally again...",Negative
Intel,"for some reasons, whatever game i play it either closes itself or looks so bad visually that the games (yes, games) are unplayable so i have resorted to uninstalling all AMD graphics software (drivers and applications) and am going to try to re-install it and see if i can choose a previous driver",Negative
Intel,is the horrible stuttering/flickering (feeling like dynamic hertz and micro stuttering) experience from the 25.10.x drivers fixed? If not I have to stay on 25.9.2,Negative
Intel,"This version, perhaps even the previous one, installs the AMD Adrenalin Edition software even if I select Driver Only when installing the drivers. Can you solve it?",Negative
Intel,"Adrenalin 25.11.1 terminating itself right after launching.  Can't run Adrenalin UI(App, Program...)  Seems like iGPU & multi-monitor related problems.  for more info [https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin\_25111\_not\_opening\_after\_update/?sort=new](https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin_25111_not_opening_after_update/?sort=new)  I'm going back to 25.10.2",Negative
Intel,Anyone else having trouble even getting the software to open since the update?   I've done a clean uninstall and reinstall of the drivers and software twice and Adrenaline won't even open.,Negative
Intel,"Hi u/AMD_Vik  im still waiting more than month legion go 2023 amd vga driver get released update latest for arc raiders,bo7,but asus rog ally x and xbox rog ally x yesterday updated already.is there a chance,we receive an update for legion go?also amd chipset driver very old for legion go.thank you if you answer me 🙏",Neutral
Intel,"Am I the only one seeing this bug in the metrics overlay? there are two ""gpu temp"", one would be that of the cpu,but written wrong. while the other metrics are written right,I already tried a clean reinstall with ddu, but nothing",Negative
Intel,"Anyone else getting per game Settings not being able to be changed? It sticks to just one whenever you click on a slider, this update and the last had it. Apparently older versions didn't and the only other fix is through screwing with the BIOS which i'd rather not.",Negative
Intel,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable,Neutral
Intel,Still not working AMD NOISE S,Negative
Intel,New Game Support: ARC Raiders  I updated to this driver thinking it would be better for ARC Raiders since that is what I am playing right now but my game is crashing if I try to load into the Blue Gate map. I tried restarting my computer and it still happens. I almost lost all my gear because it took me a bit to roll back my drivers to 25.10.2. Come on AMD do better! It's a supported game on this driver! I surprisingly never had crashes in 25.10.2 unless I toggled the Adrenalin game overlay.  Running a 9060 XT and a 5900X.,Neutral
Intel,"I updated from 25.10.2 and saw the bug report tool pop up after restarting. I had no idea what caused it. I launched Battlefront 2 and the entire system froze, with WinAmp trying to play audio which sounded horrendous. I uninstalled Adrenalin, used DDU to clear everything then did a fresh install of 25.11.1.  On restart, the bug report tool showed again. This time Wallpaper Engine failed to show on the second monitor. I quit the program and started it again and boom - system freezes entirely.  So, this driver apparently has big issues with Wallpaper Engine. Uninstalled, DDU'd, fresh install of 25.10.2 and smooth sailing ever since.",Negative
Intel,I still have issues with the combo : 9070XT + PSVR2 + F1 25 in VR.   The image in VR is still bugged. The only version that is working is still 25.4.1,Negative
Intel,Software doesn't open at all for me. Used DDU but still doesnt work. Deleted CN folder from appdata aswell.,Negative
Intel,"After update I can't open the app. It just loads and crashes. I fully reinstalled windows and the error still persists. I can't update drivers or access the config, I can only download drivers externally. Any insight of what it might be?",Negative
Intel,"@AMD_Vik I still do not see CPU metrics after update via Adrenalin software, what to do?",Negative
Intel,"I'm new to this, i'm on 25.10.2 do i update? the only issue i have is fps drops on fortnite but other games i play are alright (r5 9600 igpu) it usually runs at 60 fps on performance mode but since last weeks of last season it started doing that",Neutral
Intel,"Unfortunately, version 25.11.1 does not start with Windows.",Negative
Intel,Is AMD going to come up with another driver soon?,Neutral
Intel,"My RX 7900 XTX now no longer run Frame Gen. The game becomes completely unusable even reporting 200+ fps, it still stutter like crazy.     Without Frame Gen all good, with Frame Gen, completely unusable for me :/",Negative
Intel,Over a few days after installing 25.11.1 on my 7900 gre system I had a few driver timeouts followed by a major screen-freezing crash which corrupted my drivers. DDU'd and rolled back to 25.10.2 and haven't had any new ptoblems.,Negative
Intel,"After installation 25.11.1 (from 25.10.2)  black screens entered the chat. After DDU and rollback to 25.10.2 they stayed, and after rollback 25.9.1 the same... RX 5700 XT. Sadly 😞.",Negative
Intel,"is there 25.11.1 for windows 10? the filename that i downloaded from AMD website is ""whql-amd-software-adrenalin-edition-25.11.1-win11-s"" where usually its filename includes windows 10 along the lines",Neutral
Intel,"they need to fix the BF6 texture corruption glitch, it's annoying af. had to roll back to 10.2",Negative
Intel,Any word on fixing the driver timeouts on the 7900xtx its a bloody joke worst gpu i have ever bought,Negative
Intel,Any of you also have issues with afmf2 and the game not opening adrenalin software or showing performance counter after enabling it?,Negative
Intel,"this shit was fucking with my PC, DDU current drivers and reinstalled 25.10 straight from Gigabyte Program and everything works again",Negative
Intel,getting bsod randomly since 25.9.1 sad..,Negative
Intel,"I started having an issue since the 25.11.1 update with unreal editor where all of my tools menus instantly close, nothing else changed except for this driver update and I've heard of Nvidia having similar issues with driver updates in the past so I think it may be the cause, Going to revert to an older driver and see if it works",Negative
Intel,For me the driver just times out randomly during normal stuff like youtube shorts. Today I opend steam and the driver timed out. That never happend with 25.10.1.,Neutral
Intel,"I've spent the last few days uninstalling, reinstalling, DDUing, doing everything I could think of to get Adrenaline to start/work. It would show the splash screen and then quit. No way of re-starting it. Couldn't open anything that used Vulkan and got errors. Couldn't install the Windows Store version cause ""driver error"". I eventually used DDU one last time and uninstalled everything AMD and was able to just install the driver through MyASUS. Now I'm able to open all the software again that wasn't starting before. I'll be holding off on installing Adrenaline again anytime soon. Sucks cause I want the features, but I couldn't use the programs anyway. I miss having nvidia.",Negative
Intel,"It seems on the latest Radeon driver that freesync is broken within CS2 when running fullscreen windowed. Freesync works initially when the game starts. But as soon as I alt tab, freesync breaks and I get screen tearing. I rolled back to 25.9.1 and I can confirm it works again as expected. So it seems this is a recent regression. Can we get this addressed please? u/AMD_Vik",Negative
Intel,"Been having issues with VLC freezing and stuttering during playback (video only, not audio) since anything after 25.9.1. Guess I'm gonna roll back to that until it gets figured out.... really frustrating.",Negative
Intel,"The Adrenalin Software instantly closes and restarts if I try to click on the ""Record & Stream"" tab (no crash/error report, it simply closes and then restarts in background).       Dunno if it's from 25.11.1 or not, it was the first time I was going to try it. Didn't tried a DDU full reinstall either, just a simple reinstall of the driver but for no use. Guess I will just use other software for recording so whatever but I'm curious if it's really a driver issue since I got no report pop up at all.  Gpu is a 9060 xt 16 gb.",Neutral
Intel,Substance Designer won't start with this one. Access violation with amdvlk64.dll. Adrenaline won't start either,Negative
Intel,"Can we use FS4 on rx6000 series now without it crashing now on this driving now, or do i always need to keep downgrading my driver ?",Neutral
Intel,So no redstone yet,Neutral
Intel,FSR AI frame gen??? Didn’t they say that’d it would also have a driver toggle?,Neutral
Intel,Did AMD ever add support for Cronos?,Neutral
Intel,Well Star Citizen will load now!  Now some longer term testing....,Positive
Intel,Anybody tried this with Anno 117 yet? I’m hoping it helps performance,Positive
Intel,Problemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemas,Neutral
Intel,NO LA DESCARGEN ES MAL LAGGGG EN LOS JUEGOS,Neutral
Intel,So are the issues with Arc Raiders fixed? I had to roll back to 25.9.1 because 25.10.1 kept crashing my game. Did they actually fix it?,Negative
Intel,Disabling ULPS seem to fix the crash. My pc crash pretty often when i wake the screen or turn it on if i dont disable ulps with msi afterburner. 9070xt,Negative
Intel,Doom: The Dark Ages does not lauch with the latest driver. I had to rollback to 25.9.2 in order to play. Please fix,Negative
Intel,Last driver crashes Apex Legends every joined game. Im fuckin over amd.,Negative
Intel,Support for Anno 117: Pax Romana - Incorrect. I have needed to downgrade back to 25.9.2 to play Anno 117: Pax Romana without crashing. The last updates have been a joke. I now cannot use adrenaline due to needing to play on this earlier version to be able to play any new games. Does anyone know of where I can express my complaints?  Edit: This is also the same for Arc Raiders.,Negative
Intel,"Here we go again, jetzt stürzt Battlefield 6 wieder ab. Mit dem Treiber davor hatte ich es in den Griff bekommen außer XMP war aktiviert, dann stürzte es dennoch ab.    Also es scheint definitiv ein AMD Treiber Problem zu sein.    Gut das bei euch die Kunden die Tester sind und nicht ihr das übernehmen müsst.  PS: Gespielt wird mit einer 7900XT und einem 7800X3D.      Das war definitiv meine erste und letzte Karte von AMD. So viel Probleme hatte ich mit Team Grün nicht.",Neutral
Intel,Yeah same here LG c5 42inch 😰,Negative
Intel,"Having system crash issues after putting the PC into sleep mode. Samsung 57"" Odyssey Neo G9. Now I know who to blame.  Reinstalled AMD drivers and changed the settings so that my PC never gets into sleep mode (turns off the screen, but doesn’t sleep or hibernate). This fixed the issue temporarily for me :(  Also from the system crash minidump, it's very clearly an AMD driver issue  **IMAGE\_NAME:  amdkmdag.sys**",Negative
Intel,"I have this but on display port, HDMI works fine",Neutral
Intel,"DDU with full uninstall of all AMD related things and then chipset driver install, fresh GPU driver install fixed the crash from wakeup for me.",Neutral
Intel,"I had to go back to version 25.9.2 but I no longer have AMD Adrenalin. If I try to install it, it reinstalls version 25.11, which crashes my game. Is it necessary to have AMD Adrenalin? I have a 7900 XTX and a Ryzen 9 7950X3D.",Negative
Intel,Could you try a DisplayPort to HDMI adapter? I wonder if it works in this situation =D,Neutral
Intel,I have the same issue with display port but it’s okay with hdmi :/,Neutral
Intel,"Honestly, I plan to make sure my next display has Display Port in it. Mostly for linux though.",Positive
Intel,"There will already be branching inside the code of the driver. This has been the case already, to various degrees, for years. It's not new.  It's just whether AMD wants to formally spread those branches out on a file / compilation level and distribute different packages. And then publicly whether they commit to updating all branches of code or only some.  For any particular bug / feature / optimisation, there will be some cases where it's the same code path for practically all RDNA versions, you fix it once and it applies to everyone. For some, it might be very similar but not the same, just some slight tweaks and what gets fixed in RNDA3 can also be applied to RDNA2. For some, there's some hardware feature of RDNA3 that would make the fix easier there and it will be much more work to adapt the same to the RDNA2 branch. Of course, we have very little outside insight into the exact spread of these cases that AMD wants to pay their engineers to work on.",Neutral
Intel,There is a separate code path for specific things bit both are in those combine driver. Its mostly about certain ray tracing extensions   There are also fp8 and fp16 codepaths    People misunderstand and thought its like pre rdna stuff.,Neutral
Intel,"V25.10.2  here… I have both CPU and GPU by AMD and if you download the specific package, they install drivers only for the specific hardware (and they have different dimensions). For installing both drivers you have to download the AutoDetect package.  EDIT: typo",Neutral
Intel,"I'm one of the 5 people still running a Vega 64 and for years we've had a separate driver ""branch"" despite being able to install new Adrenaline versions. Bf6 beta wouldn't run without spoofing my actual internally installed driver, which hadn't been updated since they dropped support.",Negative
Intel,combined again it looks like 🤷‍♂️,Neutral
Intel,the display team are working on this with priority. Hoping to have this out in the next release. There are two similar display issues on their radar which are both P1.,Neutral
Intel,"what is triggering this? I can let my amd pcs run all day without any crashes. (7900XT,5700XT and 6900XT)",Negative
Intel,"Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  Not criticizing, just curious as to use-case.",Neutral
Intel,You try install last chipset driver ?,Neutral
Intel,"Bummer you're having issues. Hopefully AMD gets it straightened out for you. Out of curiosity, why did you decide to use HDMI on your monitor instead of display port? I thought HDMI was mostly used for TVs nowadays.",Negative
Intel,So it's the driver that's why that happens 😡 and it's not fixed?,Negative
Intel,Thank you for your service,Positive
Intel,"I've had the ""device_hung"" error and the best drivers are those you are on.   AMD needs a lot of help with their drivers...",Negative
Intel,Any update mate?,Neutral
Intel,"No, radeon drivers update cannot upgrade your CPU so that it would stop bottlenecking your GPU.",Negative
Intel,"Had the same issue, just ddu and reinstall the drivers manually, problem now is with my hardware monitor screen not working, I have an aorus board with an internal hdmi, that is not being detected, will try to reinstall chipset drivers",Negative
Intel,"multi gpu? got the same problem with rx 6400 + 7900xtx. plugging in my secondary display to 7900xtx fixed the issue, but what's the point of secondary gpu if its not working properly...",Negative
Intel,"I'm the opposite, I just want adrenaline app to stop everytime I right click on desktop or open file explorer.... Bruh",Negative
Intel,"Workaround:  Win+P and disable the second Monitor in iGPU, then start adrenaline and you can enable the second monitor again  software should be okay then",Neutral
Intel,"yeah same, my experience with 25.10 was terrible, had to DDU it once to get back my CPU metrics in Adrenalin, then DDU’d it again to go back to 25.9.2 since games were stuttering.",Negative
Intel,Same.,Neutral
Intel,"Not stable, ddu install 11.1 and lag in old dx11 mmo game(BDO)  Same as 10.2, 9800x3d with 7900xtx, i can keep 144fps in town , 11.1like  fps 60-65 and lag spikes  Im go back to 9.1",Negative
Intel,"Both 10 and 11 are shit with 7xxx series, myself and most others have gone back to 9.1 or 9.2. Might be alright with the 9070, might not",Negative
Intel,I'm playing it on 7900 GRE with FSR4 INT8 with no issues.,Positive
Intel,na it works the problem only occures when using PathTracing and you are not going to play on an AMD card with PT anyways and especially not a 7700XT,Negative
Intel,"I played recently the latest patch of Cyberpunk 2077 with RX 7700 XT with drivers from February this year, 25.2.1 and had no issues, no stutter, no lag, no crashes, the problem is with path tracing or ray tracing something, you don't have to use that even and it lowers fps probably for very little visual gain.",Positive
Intel,If it still crashes set RTX Global Illumination to Static.,Negative
Intel,"pretty sure anti lag was causing my system to freeze up when in arc raiders on 25.10. turned it on and issues, turned it off and haven't had issues since",Negative
Intel,"Optimizing has nothing to do with fixing crashing, generally, that would be bug fixing.",Negative
Intel,"The only Redstone component that will work through the driver for upgrading existing features (like FSR3 FG) will be ML FG, but only for games that have FSR 3.1.4+(and FSR FG), as they’ve already announced in a GPUOpen article quite a while ago.   Ray Regeneration (RR) and Neural Radiance Cache (NRC) require dev implementation since they’re much deeper in engine code/inputs.",Neutral
Intel,"Does BO7 have ray tracing? Where is that feature mentioned, would be keen to read about it 🤓",Positive
Intel,"Itll probably work through optiscaler, but a game like indiana jones has locked dlss inputs, so you need dev implementation",Neutral
Intel,"And all my USB devices dropped for a while, far worse experience than the usual double screen flicker & it's done.  Edit: seems like that was because of iGPU driver for my Zen4. In the end and after 6+ reinstalls, I'm stopping on 25.9.1 and have Vulkan working again.",Negative
Intel,What is wrong with CP? Just got my 9070 after 10+ years of nvidia :o   Ah nvm i see it.,Negative
Intel,Ugh,Neutral
Intel,"Most likely when your distro provides a kernel update.  I'd give this a quick read: https://www.gamingonlinux.com/guides/view/how-to-install-update-and-see-what-graphics-driver-you-have-on-linux-and-steamos/  Pretty much the default is that you will be using the Mesa driver collection and they were last updated yesterday.  The fixes there do not correlate with the ones in this thread (I think).  As long as you are on a ""cutting edge"" type distro then you can expect that update in the next few days/weeks.  If you are on a ""stable"" distro that doesn't update often, you may be waiting a realllly long time.",Neutral
Intel,"Get what exactly? Features? Bug fixes? Optimizations?   Every 2 weeks there are new Mesa driver updates with bug fixes and performance optimizations. As for features, you have to wait a few months for versions with big numbers (25.2 is soon updating to 25.3, adding Anti-lag 2).   Depending on your distribution, you can get these updates faster or slower.",Neutral
Intel,"Linux doesn’t work like windows. You get updates directly via Mesa stack. When your distro provides mesa package update, that’s when you get driver updates, and they’re completely different from windows branch.",Neutral
Intel,just uninstall it I prefer manual check myself.,Negative
Intel,So AMDs default driver overclocks and doesn’t reflect that in the values?,Neutral
Intel,Same issues here i underclocked it but this new update just made it worse,Negative
Intel,ok it is still crashing ... complete reboot :(,Negative
Intel,"I feel like that crash is more on DICE's side, since Nvidia users get the same exact crash, although less often.  I tried everything I saw on the internet, nothing really works. Sometimes I can play for hours on end, other time game just crashes randomly after 10-15 minutes.  I am going to try to downgrade to 25.9.1 and see how it fares, since I remember that driver being really stable for me (6800XT).  Edit: been playing for 4 hours, no crash yet. Never had such a long session without the game crashing.  Will update in the next few days.  Edit 2: haven't crashed once, been playing at least 2 hours every evening.",Negative
Intel,Okay.,Neutral
Intel,I’m hoping Valve’s new steam machine will push them on that since it’s RDNA3 based.,Neutral
Intel,The RT/PT reflections leading to CTDs issue? We're working with CDPR to resolve this - it's not caused by the driver software.,Neutral
Intel,"Stuff like this is honestly bugging me.  First AMD card and while it is okay to have some driver issues, stuff like that shouldn't be a thing at all. It can't be that some bugs keep existing for multiple months let alone more than half a year.",Negative
Intel,A much more fundamental thing like playing video in a window without stuttering took 3 or 4 driver updates.,Neutral
Intel,"> One year already...Cyberpunk 2077 not fixed yet.  On year, I did not know.  I was thinking of a 9070XT earlier in spring, but choose... differently",Negative
Intel,welcome to amd,Positive
Intel,Yeah hoping it works. I guess the point about crashing with easy anticheat was the fix? Doesn't specifically mention but it was with raytracing enabled I was getting the crash. I suppose we shall find out.,Neutral
Intel,"Weird. I was running 10.2 and things were running fine for the most part. I kept getting a pop-up on launch saying there was a known issue with that driver and Arc Raiders though, so I rolled it back to 9.2 and had more issues on that release than the one with the stated problems.  My fan control profile broke and stopped tracking gpu temp so last night I did a clean DDU install back to 10.2.  Of course 12 hrs later the new release drops haha  Hopefully Fan Control continues to play nice if I just update",Negative
Intel,Same. Never even had Ryzen master installed.,Negative
Intel,"I'm receiving the same error in Event Viewer, but I have installed Ryzen Master. Most likely it's also a component of the Adrenalin drivers for system tuning and monitoring.  Registry search shows two keys for ""AMDRyzenMasterDriverV30"" (in both CurrentControlSet and ControlSet001): Computer\\HKEY\_LOCAL\_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\AMDRyzenMasterDriverV30  The ImagePath points to: C:\\Windows\\System32\\AMDRyzenMasterDriver.sys and the file exists. It's valid.",Neutral
Intel,25.2.1 had no pink artifacts on my Pure 7700 xt Sapphire but i switched to 25.11.1 and got it now also and also some flickering.,Neutral
Intel,What is redstone?,Neutral
Intel,What's weird is Black Ops 7 has ray regeneration.,Negative
Intel,"I'm afraid it may be released in January like they drop new major this year like AFMF 2 upgrade, FSR, etc..",Neutral
Intel,vsync issue fixed with win 11 KB5068861 update.,Neutral
Intel,had no issues with vsync on 25.10.2,Neutral
Intel,works fine for me,Positive
Intel,I agree.  I also have this issue  Weird is if you turn on amd overlay it fixes itself,Neutral
Intel,"That it did, lol. My only complaint.",Negative
Intel,"Not a problem on Linux with Wayland. Ever.   Main reason I ditched Windows 11, constant screen tearing and input lag made me go crazy.",Negative
Intel,The new AFMF features were added in 25.10.2 - Still waiting for AFMF 3.0 at this stage,Neutral
Intel,Suggest if you are using powertoys to stop... After their last game update it was crashing left and right. Powertoys was cause. I only crashed once and I believe it was cfeen blanking was cause for that. I was away too long.,Negative
Intel,"Fps drop over time? That's a game issue, it's got a memory leak",Negative
Intel,"What kind of FPS drops? I was having dog shit frame pacing, and hardcore drops to like sub 60 FPS at 1440p medium/high with a 6950XT. I went through every setting and found that anti-lag was the culprit. Once I turned that off it all went away. Then with future frame rendering it got even smoother. Now I can hold 100+ FPS at all times, sometimes peaking over 200 FPS in some scenarios. And frame times are smooth as butter.",Negative
Intel,I’d settle for bf6 going one entire game without drivers crashing the game and freezing pc,Neutral
Intel,"I'm also having this ""problem"" with driver 25.10.2; I haven't tested it with 25.11.1 yet.",Negative
Intel,Crashes?,Neutral
Intel,I have this problem in all games.,Negative
Intel,"Hello, I've been having this issue and I have exactly your gpu and cpu, whenever I played valorant and I alt tabed many times the screen goes black and keyboard become unresponsive but I can still hear friends in discord and they can't hear me, after conctacting valorant support and messing with alot of settings I think  what fixed it for me is to add these in windows defender exclusions : C:\\Riot Games\\VALORANT\\live\\VALORANT.exe   C:\\Riot Games\\VALORANT\\live\\ShooterGame\\Binaries\\Win64\\VALORANT-Win64-Shipping   C:\\Program Files\\Riot Vanguard\\vgc.exe   C:\\Program Files\\Riot Vanguard\\vgm.exe   C:\\Riot Games\\Riot Client\\RiotClientServices.exe   I hope this helps",Negative
Intel,"DDU was only ever for switching GPU families, e.g. Nvidia or Catalyst to Adrenalin.",Neutral
Intel,"Sad to say I am having the same issue. Did you find any workarounds, or just downgrade?",Negative
Intel,"You don't need to make it work on Adrenalin, Battlefield 6 already has FSR 4 support integrated into the game, so you don't need to override, just turn off FSR 4 from Adrenalin.",Neutral
Intel,Works on 25.10.2. I have been seeing ppl said it doesn't work on this driver. It also didn't work on 25.10.1 for me as well.,Negative
Intel,Epic version runs just fine.,Positive
Intel,Cyberpunk GOG last version patch runs fine on this driver.,Positive
Intel,"Hey there, can you give an example of how this looks now versus how it's supposed to?",Neutral
Intel,are you able to install driver only with the newest drivers? When i choose minimal or driver only it still install full version of the software anyway. Only 25.9.1 works perfectly.,Neutral
Intel,Yeah... it can happen. I was already locked outside windows after a bsod with my old nvidia card. I had to use a linux usb drive to fix it through the command line.,Negative
Intel,It's most likely the same as I've described here:  [https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/](https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/)  But yeah unfortunately seems like all you can do for now is either roll back to 25.10.2 or disable your iGPU if you don't use it,Negative
Intel,probably because those issues are build specific.  I've not had any freezing or locks on any drivers this whole year.,Neutral
Intel,"The game is booting, this message was for the 25.10 they just didn't removed it",Negative
Intel,I don't know your system specifications but in many cases setting the BIOS to PCIe v4 could help.     Try change in your bios,Neutral
Intel,no I hit alt R opens right away.  And i did an upgrade install over top of 25.10.2 no DDU or AMD clean up utility.  When you say you did a clean install was that just from control panel and remove?,Neutral
Intel,"First time yes, i downloaded with -s letter, but the last time i downloaded smth like -combined(1.6 gb). All two's is for WIn 11.",Neutral
Intel,"To be clear, are you able to confirm that VRR is disabled after you alt-tab? Do you have a display-side OSD to verify?",Neutral
Intel,"Not even 7000s have support for FSR, you'll have to stay on 25.9.1",Neutral
Intel,My 9070 xt crushes while I try to use fsr 4 on new drivers,Neutral
Intel,Why don't you try it and let us know if you can. Would be helpful for lots of us,Positive
Intel,It's in Redstone. Still not out yet,Neutral
Intel,Didn't work for me...,Negative
Intel,Wait until you see how much your browser's cache is churning...,Neutral
Intel,Why cant you use Adrenalin? I'm using it on 25.9.1,Negative
Intel,I just received a windows extension update for my LG monitor. If you can boot up go check.,Neutral
Intel,The last time I had this problem it was a RAM issue.,Negative
Intel,I have this for my ThinkPad laptop with its internal display. Good to know I'm not the only one.,Positive
Intel,"if it happens, fill in the bug report that pops up. more reports will help AMD identify the issue better.",Neutral
Intel,I have DDUed the driver before upgrading. I still face the GPU crashes during display standby.,Negative
Intel,"Wait, chipset drivers uninstall too? I'm going to have to fix that on my system...",Negative
Intel,Do u reintall already up to date chipset drivers?,Neutral
Intel,You should use the Factory Reset installation in the AMD Driver Installer so it removes the newest AMD Adrenalin and install back the correct Adrenalin driver.,Neutral
Intel,"I don't have a DisplayPort to HDMI 2.1 adapter to try, sorry.",Negative
Intel,doing so (separation) will create a freak out shitstorm part 2.,Negative
Intel,> People misunderstand and thought its like pre rdna stuff.  It is. They've simply bundled two separate drivers together. RDNA1/2 are still stuck on the same 21033.x branch while RDNA3/4 have moved first to 22021.1009 and now 22029.1019.,Neutral
Intel,Is it combined though?  7900 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov.exe (25.20.29.01 ?)  6800 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-s.exe (25.10.33.03 ?),Neutral
Intel,All it takes is one random Reddit comment with some upvotes spreading misinformation and you will see people posting articles about it,Negative
Intel,"You have two differnt drivers installed and running at the same time, one for APU and the other for GPU? is that what you are saying?",Neutral
Intel,Combined only as far as the two driver packages are now in one file. RDNA1/2 are still stuck on 32.0.21033.x branch. RDNA3+ are on 32.0.22029.1019.,Neutral
Intel,Last driver was branched right? I never updated coz I had just fidled with the files to make FSR4 (INT8 or whatever its called run) and didn't wish to do it again...,Negative
Intel,Can we please have a fix for the 7700 xt artifacts showing? It's been ignored for so long. Using hardware acceleration on chromium (and not just that) causes pink artifacts everywhere.,Negative
Intel,Thank you for communicating,Positive
Intel,Unfortunately happens to me too. So for me it’s a big issue as I can’t update to this driver until it is fixed 😰,Negative
Intel,"I have a dual PC setup, one system with a 9070xt where I have seen this bsod related issue, mostly when toggling HDR.  The other system has exclusivly an RTX 3090 that also has the same HDMI 2.1 related system bsod, but Nvidia driver instead.   It idles, display sleep, 1 in 5 ish chance it locks up or bsods on wake.  Issue goes away using a non 4k 240hz display.     I believe this system crash is deeply related to DSC on Windows.  I only got these two PC bsods when I bought a 4k 240hz display.  Returned a monitor (bad oled) and the issue went away.  Got a new oled a few weeks ago and now I have these bsods again.     Never had a bsod before I got these 4k 240hz displays.  Fresh Windows 11 installs too between both PCs and between my first and second oled.  Systems are both solid and stable.     Linux works fine, but no HDMI 2.1 support thus no 240hz, which makes using the oled pointless imo.   While digging into my issue, I wanted to see if DP2.0 / uhbe20  compatible display would have these issues, but my current display only has HDMI2.1 and dp1.4.  Hopefully someone else had experience with them on 4k 240hz.",Negative
Intel,Thank you AMD my bad for getting upset,Neutral
Intel,Thank you.,Positive
Intel,Any word on when noise suppression will be fixed? I would actually lowkey love a right up and why it’s failing. Would be cool to see the technical details if that’s possible. (I’m actually more interested now on why it’s not working vs just getting it fixed).,Neutral
Intel,Thank you!,Positive
Intel,There's a long-time standing issue with Chrome and hardware acceleration. Will that ever be made a priority?,Negative
Intel,Redstone when?,Neutral
Intel,"LG OLED CX and newer are good monitors.  I am using it with my PC and PS3, PS5, Blu-ray 4K Player...  I must say that that was my very first actual driver issue with AMD.",Positive
Intel,Not always true. HDMI 2.1 has more bandwidth than dp 1.4. I have a ASUS pg32ucdm and HDMI looks better and uses a lower dsc ratio. I have a Nvidia GPU in my main PC however.,Neutral
Intel,"Yes, my 9070XT is paired with a 5600X hooked up to my TV for couch gaming. I have a separate PC with a monitor for desktop gaming. My TV doesn't have DisplayPort, very few did when I bought it.",Neutral
Intel,"I currently run a gaming monitor as my primary one, with DP, and a plain old Lenovo office monitor as my second one. It only has HDMI, so i wouldnt be able to use it with DP.  I assume this may be the case for a lot of people. Getting a ""higher spec"" 2nd monitor really isnt a priority for me, as i just need a second screen for productivity when working instead of gaming.",Neutral
Intel,Non pc monitor tvs are sometimes cheaper especially for larger sizes. I’m on lg c5 oled 42inch and it only has hdmi…,Neutral
Intel,"Not universally true, hdmi and displayport depending on the version supported are very comparable.   Dp 1.4 Vs hdmi 2.1 is where hdmi has more bandwidth, before this dp was the easy choice.",Neutral
Intel,"> Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  ~~Tell this to Valve, who are about to bring out a gaming-focused mini PC which **only** has HDMI 2~~",Negative
Intel,"This was true prior to HDMI 2.1 but no longer the case. However, monitors with HDMI 2.1 do still cost more than those with DP1.4 which has similar albeit slower bandwidth.",Neutral
Intel,"Yes, I am on a television. ""HTPC gaming (and turbo tax) from the recliner master race""!",Positive
Intel,> Are y'all playing on televisions?  Do you guys not have phones?,Neutral
Intel,LG OLED TV as a monitor. I have LG OLED CX which comes with HDMI 2.1.,Neutral
Intel,My game keeps crashing due to the same. I want this fixed ASAP. You'd think they'd test the biggest recent releases before publishing an update.,Negative
Intel,"I am also having a ton of crashes in BF6 RedSec, but I'm not getting any errors. Game just freezes, then crashes.",Negative
Intel,"I got a new pc 2 days ago and i literally cannot downgrade my drivers without amd forcing me onto the latest, even if i download the more stable drivers directly. Got any advice?",Negative
Intel,"I kinda fixed it by turn off XMP/EXPO running the ram the lowest bus, still crash to but 1 crash every 2-3 hrs still better than 15 minutes.",Neutral
Intel,there are multiple reports of lower gpu usage with 25.9 and above.. after going back to 25.8 I have 15%+ fps  I guess the constant crashing with rdna1 cards is also not related to these drivers,Negative
Intel,"Tried but didnt work, tried the 25.9.1 and get error that no Software is installed tried the auto update to 25.11.1 again and same error with open and shut down again",Negative
Intel,"open powershell as an admin and paste this in. (type powershell in the search bar and there will be a choice to run as admin)  Get-AppxPackage -AllUsers | Where-Object {$\_.Name -like ""\*AdvancedMicroDevicesInc-RSXCM\*""} | Remove-AppxPackage -AllUsers",Neutral
Intel,"Thanks a lot mate, i tried everything and adrenalin not worked but after yours advice it's fine. Thank you!",Positive
Intel,OK thought I was the only one. 25.10 is bad bad,Negative
Intel,"25.10.2 just straight up didn't work for me when trying to play Arc Raiders (constant UE crash when loading into a match), in fact it actually gave me a popup when launching the game to downgrade to 9.2.  Only driver since switching back to AMD to give me issues.",Negative
Intel,Thanks for testing it,Positive
Intel,"I posted before in an arc raiders thread, but I've had zero issues with my 7900gre on 25.10.2. Playing since launch. I don't use any sort of overlays and play in borderless windowed always. Performance is locked on 140+fps native res 1440",Positive
Intel,I thought FSR 4 was only on RDNA 4? 🤔,Neutral
Intel,My thoughts exactly. Thanks.,Positive
Intel,Ohh okay. Yeah I don't use any of that lol. Hopefully it won't give me any issues. Plan on playing it after FF7Rebirth.,Neutral
Intel,I tried that before and amd antilag off. I tried all the suggestions.   Been really happy with Bazzite so haven't been back in windows for 2 weeks,Positive
Intel,I tried that and all the other suggestions at the time. Nothing worked.  I'm running the game in Linux now and have 0 issues. I can't even crash it when I try,Neutral
Intel,"There's not much documented about it  Videocardz has an article   AMD launches first feature from FSR Redstone with Call of Duty Black Ops 7, only for RX 9000 GPUs - VideoCardz.com https://share.google/VHJiZgwO6eqkMmHV3",Neutral
Intel,"Well currently none of the FSR 4 tech works in vulkan titles, through Optiscalar or driver override",Negative
Intel,Pink path tracing and crashing after a few minuets of gameplay. If you don't enable path tracing the game seems to work just fine.,Neutral
Intel,What are you seeing? I played last night on latest driver and everything seemed fine to me. Playing Ultra RT with Auto on FSR4.,Neutral
Intel,"Yes, modern AMD cards use various telemetrics to push past the rated max boost frequency when headroom is available. Even on default. Problem is, not every board runs stable under these circumstances...  This issue is known to AMD but they don't seem to care.",Negative
Intel,"Have you tried undervolting your GPU?   I've played BF6 since launch with AMD drivers before the game ready one and currently still on 25.10.2 and I've had 0 crashes while playing the game.     Since I got this 7900 GRE that I've ran it with 2703MHz min / 2803MHz max and 1010mv, power limit -5%.",Positive
Intel,Would be absolute insanity to release a brand new rdna3 product if its not about to recieve fsr4,Negative
Intel,"But that's Linux based, So while we may get FSR4 in Linux, not necessarily in Windows.",Neutral
Intel,I guess you can't drop any hints as to whether this work with CDPR also involves adding Ray Regeneration to the game 👀?,Neutral
Intel,Fun fact - i am dual booting and on Linux this bug is not existent...:)),Positive
Intel,"On 10.2 the only crashes I was getting was if I pulled up the Radeon overlay while in game. My guess is that the anti-cheat doesn't like it, but as long as I avoided doing so it ran fine for me.",Negative
Intel,Hi. Did you ever resolve this? I'm getting the same error. Thanks.,Negative
Intel,"Yeah, I'm really starting to lose track of what's going on with AMD. I actually love AMD products, but I'm getting fed up. I've been waiting for a fix for over six months, and it never comes. Instead, new features are added, and bugs persist. AMD is making it really hard to continue relying on their products, which is sad. :/",Negative
Intel,https://www.amd.com/en/products/graphics/technologies/fidelityfx/super-resolution.html,Neutral
Intel,It's a thing you can search for on Google,Neutral
Intel,will it work with current version? Or is it based of some pre-release alpha driver for redstone?,Neutral
Intel,"They promised redstone as a feature for 25h2. So if they dont, that could technically be grounds for a lawsuit",Negative
Intel,ahh i'm on Win 10 so probably why I didn't see it.,Neutral
Intel,Weird. Why is it a Windows fix?   I also got vsync bug in specific games and the weird thing is that I needed to turn on the and perf overlay in order to work,Negative
Intel,"Yes, but was it in the previous WHQL driver ? I'm not sure.",Neutral
Intel,there is an another bug appears in 25.10.2 drvier. 25.10.1 works fine.  [https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows\_25102\_driver\_performance\_for\_battlefield\_6/](https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows_25102_driver_performance_for_battlefield_6/),Neutral
Intel,It's not only BF6  https://www.reddit.com/r/radeon/comments/1oj98iy/amd_software_adrenalin_edition_25102_release_notes/nm4hh37/  https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,Neutral
Intel,"I tried everything I saw online: meshes on low, XMP lower/off, chipset drivers reinstall and other stuff. Nothing worked.  I downgraded back to 25.9.1., haven't had a crash in days.  Kinda miss the improvements for AFMF they brought with 25.10 for other games, but eh I'd rather play BF6 without it crashing randomly.",Negative
Intel,"Didn't test long enough to see if it crashes as I reverted back to old driver when I noticed my gpu was permanently at 100% load which it never was before on old driver, doesn't feel like it's supposed to do that with a 9800x3d & 9070xt + 32gb ram, It's moving between 70-95% usage on old driver",Negative
Intel,Either launch with curseforge or rollback,Neutral
Intel,"Damn, didn’t work for me last driver either. I can get FSR4 to work in other games just not BF6",Negative
Intel,Sorry for not replying in time with the pictures but I just saw that on Twitter that Beat Saber and AMD are now aware of the issue. The distorted flickering issue on the walls.  https://xcancel.com/BeatSaber/status/1993629046802882685  However there's another issue. I had not actually tried to use an Index at 90Hz until the other day. I discovered that the latency bug is back for 90Hz mode. As in I have to adjust the photon latency to ~5ms in the Steam debug commands to make it usable but not fixed. Just like in the the drivers before 24.12.1.   120Hz mode still works fine.,Neutral
Intel,"Even the rollback doesn't fix it for me now. It being related to the combination of iGPU and GPU makes sense though. Sadly I actively use the iGPU, so thats not an option for me.",Negative
Intel,You 100 procent sure on this?,Neutral
Intel,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable  I will also change in BIOS and completely disable integrated GPU in BIOS,Positive
Intel,"I had auto update on (my mistake), so it did install over the previous driver. I've downgraded for now and it seems to be working just fine.  But no, after uninstalling from control panel I booted into safe mod and used DDU.   Then I downloaded the newest installer from the website. Restarting where needed during this process.   Didn't matter how I tried to access the software. Wasn't active in the system tray, not there in task manager.   Right click on desktop and open from there, click the exe, or go into a game and try Alt R. Didn't open with any of these methods.",Neutral
Intel,Driver with -s letter after black screen and reboot PC tells me that this driver isn't for my graphic card🤡,Negative
Intel,Does fsr 4 work on 25.9.1? I thought the only driver that works without having to change any files and risk a ban online is 23. 9.1?,Neutral
Intel,"I had to rollback to driver 25.9.1, start up Star Citizen, switch the renderer to Vulkan, exit game, start SC again to verify it's working with Vulkan, then reinstall the latest driver again. Now it works, just don't switch it back to DirectX. No idea who dropped the ball on this one but Easy-Anticheat sure doesn't like the atidxx64.dll.",Neutral
Intel,I'm not sure. However when I install the driver it states that I cannot use adrenaline and gives me a link to the newest driver. Wondering if there is an earlier version of adrenaline I need to install.,Neutral
Intel,What do you mean extension update??? Do you mean lg firmware update or something Ina  windows update? Where do I find this?,Neutral
Intel,They do not.,Neutral
Intel,"Well, I used the system before the latest updates normally, for maybe half a year. Installed every update without DDU. I wanted to triple boot, formated the drive, left windows to install stock updates and drivers, went to install chipset and GPU driver and voila, every wakeup, just before I could type the pin, kernel panic and shutdown. DDU, just the driver, installed again, black screen. Went into DDU again, wanted to see if I missed anything and without reading, uninstalled everything AMD related, went into windows, installed chipset drivers from vendors site (ASUS) and then the latest GPU drivers, and havent had issues. I have a monitor connected via DP and a TV via HDMI.",Neutral
Intel,Dont buy an adapter. Most of them are trash and dont put out the advertised resolution and refresh rate. Not to mention most of them dont support vrr or HDR.,Negative
Intel,The one linked in the release notes is combined. (in the sense that it's two drivers in one executable) (And 1.6GB)  .....amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov-combined.exe,Neutral
Intel,"AND is taking away one additional driver feature per day, you say?",Negative
Intel,"Yes, I’m talking about 25.10.2 (posted the other day here: https://www.reddit.com/r/AMDHelp/s/nNunS1QlFX ). If you go to AMD download page and select “GPU” you get a file that has a different dimension from the one you download if you choose “CPU”. If you do the following steps 1) DDU (or AMD CleanUp Utility), 2) install CPU drivers => GPU not recognized or with a generic driver. If the you install the GPU driver you get the GPU correctly recognized and a yellow esclamation mark on the integrated. To have both VGAs working at the same time I have to download the AMD Auto Detect package (the file name ends with “minimal_install), but Adrenalin App does not open.",Neutral
Intel,Thank you for explaining it before the rage baiters go nuts.,Neutral
Intel,I've been following this one pretty closely. Unfortunately the fix *just* missed 25.11.1 release cycle; we hope to get this out in the next one.,Negative
Intel,"I have a 7900XTX and have this issue at times. It normally clears quickly on its own, but seeing pink pixels on the screen worried me that I had an issue with the actual hardware",Negative
Intel,"I totally understand. if you're running with an affected display config, you should be fine on 25.9.1 or the 25.10.1 preview driver for BF6. We'll get this out as soon as we can.",Neutral
Intel,"Appreciate you reaching out with this - that's an interesting data point to consider.  In this case, the quirky behaviour with HDMI 2.1 + FRL was attributed to a change made in the display stack, I don't think it explicitly involves display stream compression to repro, just high enough display bandwidth (though I could be mistaken)",Positive
Intel,"Interesting. I've been experiencing random BSODs on my PC too on a fresh Win11 install too, I was also suspecting it was something to do with auto HDR toggle but I didn't know for sure.  I'm running 180hz 1440 LG panel but I don't recall if it's using hdmi or DP. I've also never experienced BSODs until the last week or two, since i did a fresh install and I've had the same monitors/PCs for a long time.",Neutral
Intel,Don't apologise - this one is worth getting upset over. Hope it's fixed for you all very soon.,Negative
Intel,Was yours the DisplayPort config or HDMI? I may have a fix for this ready if you're available test,Neutral
Intel,Can you provide me some context? Are there any posts detailing the issue either here or at the chromium project side?,Neutral
Intel,Already launched in COD 7,Neutral
Intel,"Im currently using dp 1.4 on my 7900 gre.  My monitor has 2.1 hdmi, so you’re saying i shoulf switch to hdmi?",Neutral
Intel,Some of us are using DP 2.1 though. DP 1.4 is a thing of the past.,Neutral
Intel,Are you sure? The steam machine has HDMI 2.0 and DP 1.4 listed in the specifications,Neutral
Intel,"Ah ok, that definitely makes sense if you're using a TV. Hope they straighten it out for you soon.",Neutral
Intel,What I did is get the 25.8.1 drivers shut off the fluid motion frames and in Windows set bf6 to run full power on your graphics card. Try some of those things and see if that helps you.,Neutral
Intel,"I tried to reinstall the chipset drivers, the same error, tried to install the drivers from gigabyte site the second screen worked but uninstalled adrenaline and it ran the auto update that disabled the secondary screen and the issue of no adrenaline opening came back, going back to older version, thanks AMD",Neutral
Intel,With the compiled leaked DLL you can use it on RDNA3 as well.,Neutral
Intel,"Wait until December, there's the 5th anniversary and they'll supposedly drop something new.",Neutral
Intel,For me playing it with path tracing on eventually the game crashes. Had no issues playing with ray tracing: psycho though.,Negative
Intel,Amen to that. Not a single crash since the game released on CachyOS. (9070 XT),Positive
Intel,Thank you! Exciting keen to see what it’s like,Positive
Intel,"It has nothing to do with the lack of vulkan support, even when fsr4 comes to vulkan, the game still needs ray regeneration implementation from the devs, you cant use optiscaler through dlss inputs, like you probably will be able to in fx alan wake 2",Negative
Intel,I suppose is going to be fixed as soon as Amd Redstone go out. Basically is going to add every tecnology that Path tracing needs to work. Off course the devs would have to implement them in the game too,Neutral
Intel,"The issue is if you try to use path tracing. Which to be fair, you probably shouldn't unless the miracle of them getting Virtuous to implement Ray Regeneration in Cyberpunk happens.",Negative
Intel,yes i tried it ... maybe it is because of the 7800X3D ... i really dont know... i will wait for the next patch ... should come tomorrow 18.11. ... but i have read so many threads and i am not the only one...,Neutral
Intel,Hmm fair. For me on 9070XT if I had raytracing enabled it would crash most regularly on loading back to the menu after extraction which was annoying but at least not game destroying lol. But yeah annoying enough that I turned RT off entirely.   I hope this means I can turn it back on because the game runs well enough for it.,Negative
Intel,"Yes. So far, so good. I'm not 100% sure what fixed it.      I uninstalled both Adrenalin and Ryzen Master standalone applications. Deleted the ""amdryzenmasterv"" keys. Rebooted.  Then I installed Adrenalin and used the Ryzen Master installer in Adrenalin (Performance > Metrics > Install Ryzen Master).  I think this problem might have something to do with a handshake breaking between Ryzen Master and Adrenalin, after upgrading just Adrenalin.   From now on, I'll probably do clean installs, removing and reinstalling both Adrenalin and Ryzen Master, through Adrenalin Performance tab.",Positive
Intel,"My last two processors are ryzen amd, my last three video cards are amd radeon and next one will be amd also, nvidia has driver issues as well, to be honest.  But yes, some issues are too annoying and updating drivers is lottery at this point. I would advice against updating drivers unless you need it for specific new game or have the latest video cards - rx 9070 \[xt\]  or 9060 xt .  I should have sold the 7700 xt and bought 9060 xt 16gb instead but too late for that now and it has problems as well, so lottery as i said, its a risk.",Negative
Intel,"I see, btw I'm just telling the natural AMD trends on its major update/features on its GPU (kinda). Wish it coming sooner..",Neutral
Intel,lmao chill out dude go touch some grass,Negative
Intel,Could be grounds for lawsuit… That’s funny!,Neutral
Intel,Because of MPO.,Neutral
Intel,yeah same with 25.11.1 25.9.2 works for me,Positive
Intel,"25.10.2 was the previous WHQL, so also yes :P",Positive
Intel,"Huh, seems to be running in DX12 for me, as reported by MSI AB, and FSR4 is working",Neutral
Intel,Which driver version and does it still crashing?,Neutral
Intel,OK I will install it now and test it and get back to you. Give me 10 mins.,Neutral
Intel,Yuppp. It does not work on bf6 but it works on ark survival ascended for me. Seems it might be a bf issue. I saw ppl saying they were getting more fps with this driver in BF6 but I think they didn't realize it's using FSR 3.1 and not 4.,Negative
Intel,I'll work with the engineer from that ticket check if that issue has somehow regressed.,Neutral
Intel,Try using DDU to uninstall your current drivers and do a clean install of the 25.10.2 combined drivers if you havent already (you can find it here):  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html,Neutral
Intel,"Yup just need to say ""No""",Neutral
Intel,"whew thanks, good think i noticed it first before updating. i have 25.10.2 and 25.9.2 here and they both have windows 10 along their filename so i might as well asked.",Positive
Intel,I don't see how it would work on 23.9.1 lol,Negative
Intel,"Did you use DDU to uninstall the drivers and then reinstall using a driver installer from the AMD website? I found this guide yesterday and it was extremely helpful, specifically step 8:  https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps/",Positive
Intel,I did it this morning before the new driver and confirm chipset drivers were untouched,Neutral
Intel,Agreed the variability and stability of adapters can be quite hit and miss. Just wait it out unfortunately and don’t use the latest drivers. At least AMD owned up to it so I can’t be too upset but hopefully they really do fix this soon as new users may not understand what’s happening and think their card is bad.   We are already seeing this in other Reddit post from other users. I have been playing some damage control and telling them to downgrade and suddenly it’s stable for them and they don’t think their card is dead doh. So yeah AMD really needs to fix this soon.  More people use HDMI than what people think and those TVs don’t always have a DP connector at all.,Negative
Intel,"ah, that explains it. Thanks. :)",Positive
Intel,"What if one does not check release notes? V25.10.2 point to different packages, for installing both CPU and GPU you had to download the AutoDetect package (named “minimal install”). Obviously I’m referring to AMD driver download page.",Neutral
Intel,"Okay, as long as they have a workaround for that... I was planning to get a 9060XT, but also keep my 6700XT for lossless scaling frame gen, was worried the branching would affect that..",Neutral
Intel,"What a mess. I have rdna2 and 3 on the same PC, so combined driver is the solution for this? Or auto detect is the only way?",Negative
Intel,"Damn, that's a huge relief to hear. Most of the frustration came from not knowing whether the issue was even acknowledged.   Thanks a lot for the update.",Positive
Intel,What about Noise Suppression not working since 25.9.2?,Negative
Intel,Finally. The last driver without pink artifacting was 25.4.1  It has been 7 months. AMD Matt acknowledged the bug on [the amd forums](https://pcforum.amd.com/s/question/0D5KZ000011WpJJ0A0/pink-square-artifacting-chrome-discord-and-steam) 5 months ago.  Please let it be fixed with the next driver update. So many 7000series users are on the verge of swapping to Nvidia.  https://i.redd.it/ywrg9at3lp1g1.gif,Neutral
Intel,"what about whatever changed post 25.3.1 for multi monitors? Any driver 25.3.1 and older i can run my main at 200hz, my side at 165, and my third at 60 without issue. Any driver newer than 25.3.1 and running my main at 200hz then playing a game will cause the second monitor (165hz) to flicker, glitch, and freak out the entire time, until the game on the main monitor is closed. Dropping to 165 on the main monitor down from 200hz \*fixes\* the issue but, i paid for 200hz and would like to be able to actually use it again.  (7700xt system)",Neutral
Intel,Hell yeah 🙂 amd I appreciate you see the issues Iv been a fan boy for a long time and I know it will be fixed,Positive
Intel,"Hi. Sorry, I thought this was pretty common as I've had this issue with two different systems and 3 AMD GPUs across 2 generations. [Here.](https://www.reddit.com/r/radeon/comments/1jayump/chrome_full_screen_freezes_after_switching_to_amd/) [Or here.](https://learn.microsoft.com/en-us/answers/questions/3890720/parts-of-screen-freezes-when-using-chrome) Chrome (or other Chromium browsers, I've had this happen with Edge too) in fullscreen, with graphics acceleration enabled.   Best way I can describe it is that very often the program fails to update its display information?? Most of the screen freezes to a certain frame and only small, sporadic parts of it - never the same ones, sometimes more, sometimes less - continue to be updated properly as I scroll or take other actions. Making the window smaller fixes this instantly. It seems to be tied to graphics acceleration as it just doesn't happen at all with it turned off. The issue seems to get exacerbated when the GPU is being stressed.   And in the same vein, Chromium browsers do not seem to mesh well with AMD hardware? I've had plenty of weird behaviors ranging from TDRs, WoW freezing in the background, crashing and restoring itself which do happen less often if I'm running Firefox in the background instead of Chrome.",Neutral
Intel,"Keep in mind this only actually matters if one protocol or the other is **actually** limiting bandwidth for the [resolution * FPS] you want to display on your monitor  For instance, DP 1.4 can do ~25Gbits/s, which translates to roughly 1440p @ 240fps, or 4k @ 120fps  If you have proper cable into HDMI 2.1, it should be able to do ~42Gbits/s, which would instead be 4k @ 190fps  Read up the wikipedia tables for bandwidth specs and resolution / fps / bit depth rates for more",Neutral
Intel,Yeah most people are still have dp 1.4 monitors though. No way I'm swapping my monitor just for dp 2.1 as it's just too awesome. Although these tandem OLED screens look interesting... Hmm lol,Positive
Intel,"lol, listened too much to linus  https://youtu.be/g3FkuZNSGkw?t=261",Neutral
Intel,That was my very first actual driver issue I experienced with AMD.,Negative
Intel,Oh that's nice! I'll look into it when I get the chance.,Positive
Intel,Cool. Thank you,Positive
Intel,I might turn on ray tracing in like a medium or low setting just for shits and giggles or screenshots but I don't plan on playing with it on all the time since I'll also play at 1440p so I don't wanna have a really bad experience 😖,Negative
Intel,Exactly! It's insane. I can't believe running through a translation layer is more stable than running native,Positive
Intel,"but i dont know something is also wrong with 25.11.1 ... AMD Adrenalin randomly vanishes from tray     i dont want to roll back to 25.9.1 :-(    but this version was the last which was good so far      maybe when we have luck amd is able to provide next time a ""perfect"" driver without any issues?",Negative
Intel,"Could be it, nothing hardware wise at all and just software too.     Yeah, you're not the 1st I've read that says they have issues on the game.     If you're crashing and the system reboots it does make me think of some stability issues and not really just GPU, but I'm guessing you've already tried stability tests and using the system without OC+A-XMP/EXPO disabled.     Once upon a time I remember reading some people, on other instances and other games, having crashes like that (complete reboot) due to PSU instability as well.     Good luck!",Neutral
Intel,That's strange. My game defaulted to everything maxed out including running ray tracing and I haven't noticed any issues. The game runs incredibly well.  I've got a 5800X3D & 9070XT as well.,Positive
Intel,"Fair enough, and yeah sooner the better for all of us",Positive
Intel,It absolutely could be.  Europe has some pretty good consumer law and promising features that could impact your purchase decision are part of this law.  It can fall under misleading advertisement,Neutral
Intel,"It's not freezing, it's just that the GPU usage is practically at 100% all the time, even with V-Sync enabled. With the 25.9 drivers, this doesn't happen, and the clock speed varies normally, increasing and decreasing as needed when V-Sync is enabled. These 25.10 drivers are causing the GPU to consume more energy unnecessarily.",Neutral
Intel,Fingers crossed,Positive
Intel,"Allright ty, will Install new, any differences in performance?",Neutral
Intel,"im running 25.11.1 on win10 7900xt. no problems besides afmf2 breaking the performance overlay, which ive had for multiple updates now",Neutral
Intel,I don't think you understand what I mean.. If you downgrade to that you can get fsr 4 to work on 6000 series without having to change any files.,Neutral
Intel,Thank you for this. This was very helpful. Got adrenaline working fine now.,Positive
Intel,"I wish my LG C4 42"" had a display port. Its my primary monitor.",Positive
Intel,"Yep, AMD is not so quick in fixing their driver download page, I still can remember when X870E chipset came out but you found only X670 option. The drivers were the same, but man it’s your flagship chipset! Anyway, drivers install but Adrenalin App does not work for me… and have zero time to reinstall Windows.",Negative
Intel,"Don't do that, i'm suffering with both 7900XTX + RVII (and even with RX6400)",Negative
Intel,"I feel like AMD can do a bit of a better job communicating this, despite Vik's great efforts in this subreddit. Just saying: hey guys, we know about this, it sucks, bear with us please can make things better in the long run in my opinion.",Neutral
Intel,"We've not observed anything like this internally, and I've personally not seen this at all in the field. This one may be specific to your display combination. I'll talk to a colleague in our display team about what we can do to learn more about that behaviour on your side - we might ask you to capture a special kind of log file during a state when your secondary display is blanking with gameplay on the primary",Neutral
Intel,Can you tell us what content you have on the secondary panel when it starts misbehaving? Is this behaviour affected if you disable VRR on it too?,Neutral
Intel,"I see. I recall the partial display freeze being related to DWM & multi plane overlay in Windows. One of our community testers encountered this and remedied with the overlayminFPS DWORD here: https://www.reddit.com/r/Windows11/comments/1kgp7ar/cause_and_solution_to_windows_24h2_related/?sort=new  If you're getting TDRs, with chome(ium) + gameplay, pass us over a kernel memory dmp and we'll take a look into it",Neutral
Intel,"Oh yeah I noticed that too when watching it, kinda funny of him to say it as it's on screen",Positive
Intel,I've had a really stable 60 fps experience even with path tracing (minus the crashes) but I'm on a 9070 with fsr 4 on quality,Positive
Intel,Can't comment on the vanishing of the tray since I have mine configured to hide it from tray always.,Neutral
Intel,also i have coil whine since this driver 25.11.1. ?!  also in idle sometimes...  very strange driver...,Negative
Intel,Every roadmap since forever has a small print disclaimer at the bottom at the page saying that release times are subject to change.,Neutral
Intel,"I upgraded from 25.9 because it started crashing, it was not crashing until yesterday, also I have limited gpu usage, clockspeed it is working sometimes, sometimes not.",Neutral
Intel,Didn't really paid attention to it :( but at least my game doesn't crash anymore!!,Negative
Intel,"did you download the same filename with the one i mentioned? i tried downloading windows 11 link and it also gave me the same filename, lol",Neutral
Intel,No you can't.,Neutral
Intel,"I don' think you understand either, 25.9.1 is that driver, 23.9.1 was released in 2023 before FSR 4 was even a thing.",Negative
Intel,"Glad I could help, the crashes on Arc Raiders were pissing me off big style so I wanted to share what helped me.",Positive
Intel,"They are TV's, not pc monitors. Buy the right tool for the job",Neutral
Intel,"I couldn't find the DWM key so I made one, we'll see if that fixes it. Thank you for replying. But please, forward it to someone. There must be people out there on AMD hardware that are not tech savvy and this just breeds bad reputation.",Neutral
Intel,"Nope, that didn't fix it. Chrome continues to forget to update its display. And WoW continues to crash in the background leading to massive stutters when panning the camera around quickly, along with plenty of other odd behaviours.",Negative
Intel,"Yeah, I'm facing the same issue on RX 9060 XT   Is it a GPU driver issue, or a Windows issue that Microsoft needs to fix?",Negative
Intel,"That's not how it works. Small print won't save you from stuff like this, you think that they can stop releasing this software for 2-3 years because some small print said 'Subject to change'?   European consumer law would absolutely support the consumer in this case. If you promise a feature by a certain date, you will need to deliver it. If the development takes way too long, you are in for a ride",Negative
Intel,What about 25.11.1?,Neutral
Intel,Yeah same for me. Considering how similair win10 and 11 are under the hood i just went with it. Still absolutely no problems sofar.,Positive
Intel,Can't what? I have been using fsr 4 on my 6700xt for a month now using this tutorial   https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1,Neutral
Intel,"> 23.9.1   Come on man, it's clearly a typo and you know it. That's why he's confused.",Negative
Intel,"Look online for fsr 4 on 6000 and 5000 series, you will understand,    Edit :I will just link you a tutorial to understand...      https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1",Neutral
Intel,Did you reboot after setting that key? Is the display with chrome still only partially updating?,Neutral
Intel,"Haha. Sure thing buddy. If that would be the case, that a small print on a roadmap can't save a company, we would have hundreds of lawsuits in EU. You must be new to the hardware scene.",Negative
Intel,thank you,Positive
Intel,"Uh huh, yeah I did it too. You still have the overwrite DLLs. Hence the flipping tutorial. Completely irrelevant to this topic, anyways.",Negative
Intel,"Not a typo, I was asking about something else and he missed my point...",Negative
Intel,"Yes, it can't.   They could add that they can sell your organs, doesn't make it legal though.   Also lawsuits usually don't happen because of a feature or if they happen, in a very small amount that isn't meaningful. Class action also doesn't exist in Europe, so obviously you won't hear about it like in the USA.",Negative
Intel,"But flipping the dlls could get you banned from online games , that's why i was wondering if it's finally fixed in this update and we wouldn't have to flip the dlls anymore..",Negative
Intel,"You literally claimed we could do this without overwriting any files, that was obviously false.  Anyway, no. I would not expect AMD to bring FSR4 to RDNA3 anytime soon, if at all.",Negative
Intel,"What a disgusting build, I love it",Negative
Intel,the content we crave,Positive
Intel,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",Neutral
Intel,What GPU are you using in your build?  All of them,Neutral
Intel,you're one hell of a doctor. mad setup!,Positive
Intel,The amount of blaspheming on display is worthy of praise.,Negative
Intel,Brother collecting them like infinity stones lmao,Neutral
Intel,I'm sure those GPUs fight each others at night,Negative
Intel,Bro unlocked the forbidden RGB gpus combo,Neutral
Intel,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,Neutral
Intel,What the fuck,Negative
Intel,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,Positive
Intel,Yuck,Negative
Intel,Wait until you discover lossless scaling,Neutral
Intel,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,Neutral
Intel,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",Negative
Intel,Now you just need to buy one of those ARM workstations to get the quad setup,Neutral
Intel,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,Positive
Intel,Love it lol. How do the fucking drivers work? Haha,Positive
Intel,What an amazing build,Positive
Intel,wtf is that build man xdd bro collected all the infinity stones of gpu world.,Negative
Intel,You’re a psychopath. I love it,Negative
Intel,This gpu looks clean asf😭,Positive
Intel,The only setup where RGB gives more performance. :D,Positive
Intel,Now you need a dual cpu mobo.,Neutral
Intel,Placona! I've been happy with a 6700xt for years.,Positive
Intel,absolute cinema,Positive
Intel,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",Neutral
Intel,"Brawndo has electrolytes, that's what plants crave!",Positive
Intel,"You have always been able to do something like this! Though cross manufacturer has not had ""benefits"" until Vulkan in some spaces for games, now lossless scaling, but for anything requiring VRAM or software level rendering, it's been there as an option.   Unironically, ""SLI and crossfire"" I'd argue, is back, but not in the traditional sense.  I did this with a 6950XT and a 2080ti, simply because I wasn't able to jump on the 3090/4090 train in time tho.",Positive
Intel,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",Neutral
Intel,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",Neutral
Intel,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",Neutral
Intel,Team RGB,Neutral
Intel,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",Neutral
Intel,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here: https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",Negative
Intel,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",Positive
Intel,"OpenCL works on all of them at once, and is just as fast as CUDA!",Positive
Intel,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",Positive
Intel,Start here with FluidX3D: https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md 🖖,Neutral
Intel,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",Positive
Intel,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",Neutral
Intel,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (I’m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),Negative
Intel,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,Neutral
Intel,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,Neutral
Intel,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",Negative
Intel,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",Positive
Intel,Thank you so much for the very detailed response!,Positive
Intel,Well worth it!,Positive
Intel,Thank you my man!! Looking forward to run some tests once I get home.,Positive
Intel,That's awesome!,Positive
Intel,"Yes, but SLI is a bad description for it.",Negative
Intel,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",Neutral
Intel,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",Negative
Intel,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",Negative
Intel,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",Neutral
Intel,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",Neutral
Intel,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",Neutral
Intel,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",Neutral
Intel,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",Negative
Intel,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  ​​​  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",Positive
Intel,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",Neutral
Intel,Why are you connecting the monitor to the gpu and not the mobo?,Neutral
Intel,"👍   thanks for the info, this'll definitely come in handy eventually.",Positive
Intel,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,Neutral
Intel,No worries mate. Good luck,Positive
Intel,"For some reason I switched up, connecting to the gpu is the way to go. I derped",Neutral
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,It's alive. Rejoice.,Positive
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",Neutral
Intel,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",Negative
Intel,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,Positive
Intel,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,Neutral
Intel,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,Negative
Intel,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,Positive
Intel,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",Neutral
Intel,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",Negative
Intel,I'm fairly sure they use dxvk for d3d9 to 11.,Neutral
Intel,Could just be a cache issue,Neutral
Intel,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,Neutral
Intel,Glad you brought up Nvidia as I didn’t know this had improved until the testing around Arc showed it had gone.,Positive
Intel,"According to the graphs, AMD has slightly less overhead than NVIDIA.",Neutral
Intel,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",Negative
Intel,"Lowest with DX11 and older, but not with the newer APIs",Neutral
Intel,And when is the last time HUB did a dedicated video showing the improvement in overhead?,Neutral
Intel,or it's just a cache/memory access issue,Neutral
Intel,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",Neutral
Intel,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",Negative
Intel,"Intel uses software translation for DX11 and lower, so it does matter for them.",Neutral
Intel,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",Neutral
Intel,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",Negative
Intel,That's not true. Intel's issue is being too verbose in commands/calls.,Negative
Intel,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",Negative
Intel,HUB used DX12 games that also showed the issue.  It's something else.,Negative
Intel,"The comment to which I am replying is talking about nVidia, not Intel.",Neutral
Intel,"I’m pretty sure HUB doesn’t like Nvidia *or* AMD. They’re calling it how it is, these parts are too damn expensive.",Negative
Intel,That's actually... just worse news.,Negative
Intel,I always dreamt of the day APUs become power houses.,Neutral
Intel,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",Neutral
Intel,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",Negative
Intel,Damn Why is AMD even involved in iGPU,Negative
Intel,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",Positive
Intel,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",Positive
Intel,almost there,Positive
Intel,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",Positive
Intel,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",Positive
Intel,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,Negative
Intel,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",Neutral
Intel,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,Negative
Intel,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",Negative
Intel,"It depends on what your goals are for a laptop.  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!   I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",Negative
Intel,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",Neutral
Intel,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",Negative
Intel,yes its so bad. better go buy some steam deck or ally x,Negative
Intel,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,Negative
Intel,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",Positive
Intel,How are they going to feed all those CUs? Quad-channel LPDDR5X?,Neutral
Intel,That's considerably faster than an XSX.,Positive
Intel,>That's tapping on 4070/7800 levels of performance.  What is?,Neutral
Intel,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",Neutral
Intel,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",Positive
Intel,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,Neutral
Intel,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,Neutral
Intel,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",Neutral
Intel,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",Negative
Intel,It's called satire. You're just salty because you're the butt of the joke.,Negative
Intel,throw it in the next steamdeck and I’ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,Neutral
Intel,Praying the blade16 gets it.,Neutral
Intel,"This is the rumor, if you’re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",Neutral
Intel,256 bit bus + infinity cache.,Neutral
Intel,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,Neutral
Intel,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",Positive
Intel,"Yes, it’s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",Positive
Intel,The rumored 40CU strix halo chip. Not the actual chips released this week.,Neutral
Intel,7500mhz ram and the 780m,Neutral
Intel,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",Neutral
Intel,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",Positive
Intel,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",Neutral
Intel,Literally where did you see 40-60% uplift at half the power?,Neutral
Intel,> 40-60% performance uplift at half the power  Source?,Neutral
Intel,"i chuckled, then again im not a fanboy of anything",Neutral
Intel,Dont expect 40CUs in a handheld anytime soon,Negative
Intel,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",Negative
Intel,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.  The system should have an actual TRUE quadcore memory setup.  Many of these systems have currently (and will absolutely continue to have) dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.  This matches my system that runs a 780m with 7500mhz lpddr5x.  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.  This will continue to hamstring these APUs regardless of how many cu they throw at em.",Neutral
Intel,"“Absolute monster”? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.    AMD has a long way to go before claiming “Monster” class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet aren’t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Max… let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",Neutral
Intel,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,Positive
Intel,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. That’s far from desktop 4070, never mind 4080.",Neutral
Intel,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,Positive
Intel,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,Negative
Intel,">AMD has a long way to go before claiming “Monster” class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",Neutral
Intel,"APU is one of AMD’s biggest markets. You are kidding if you think they don’t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they don’t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games aren’t the only thing APUs are used for so PS5 isn’t wholey in the conversation. PS5 also costs monthly to play online and their games aren’t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",Negative
Intel,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",Negative
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",Neutral
Intel,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,Positive
Intel,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",Negative
Intel,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",Negative
Intel,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,Negative
Intel,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",Neutral
Intel,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",Neutral
Intel,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",Negative
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,Positive
Intel,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,Positive
Intel,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",Positive
Intel,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",Neutral
Intel,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",Negative
Intel,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,Neutral
Intel,"Installs beta software, proceeds to complain about it",Negative
Intel,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,Negative
Intel,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",Neutral
Intel,What Ghost of Tsushima issue?,Neutral
Intel,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",Neutral
Intel,"This was a graphics card, not a ‘GPU’ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",Neutral
Intel,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",Negative
Intel,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,Negative
Intel,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",Neutral
Intel,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",Neutral
Intel,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,Neutral
Intel,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",Neutral
Intel,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,Negative
Intel,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,Negative
Intel,The documentation for it would still be in their archives,Neutral
Intel,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",Neutral
Intel,"Pretty annoying how everything follows the same linear fps/price curve, there’s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",Negative
Intel,Wish Arc cards were better. They look so pretty in comparison to their peers,Positive
Intel,Thats actually a pretty solid and accurate breakdown.,Positive
Intel,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,Positive
Intel,3080 still looking good too,Positive
Intel,What they have peaceful then 4k series?,Neutral
Intel,Just get a 4090. I will never regret getting mine.,Positive
Intel,i miss old good times where radeon HD 7970 as best single core card cost around 400$,Neutral
Intel,"Damn, the A770 is still so uncompetitive...",Negative
Intel,"It's like the free market priced cards according to their relative performance. How weird, right?",Negative
Intel,How is that possibly annoying,Negative
Intel,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,Positive
Intel,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",Positive
Intel,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",Negative
Intel,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,Positive
Intel,8gb perfectly fine today :),Positive
Intel,"Ah yes sure, now where did I leave my 1500 euros?",Neutral
Intel,"I don’t mind free markets, I’m just saying the state of the market is less fun now than it used to be.",Negative
Intel,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,Positive
Intel,"Yeah, i like the black super series.",Positive
Intel,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",Negative
Intel,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",Neutral
Intel,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,Neutral
Intel,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",Negative
Intel,"""The ability to “turn it off and on again” should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",Neutral
Intel,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",Negative
Intel,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",Positive
Intel,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",Negative
Intel,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",Negative
Intel,"Yo, I saw the title and thought this gotta be Gnif2.",Neutral
Intel,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",Negative
Intel,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",Negative
Intel,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",Negative
Intel,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",Positive
Intel,"I’ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, it’s usually a day late and a dollar short. Blender on Linux still can’t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that there’s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but we’ll have to wait and see.",Neutral
Intel,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*™ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",Negative
Intel,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",Negative
Intel,Long but worth it read; Well Done!,Positive
Intel,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",Neutral
Intel,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,Negative
Intel,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",Neutral
Intel,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,Negative
Intel,I’ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,Negative
Intel,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",Negative
Intel,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,Negative
Intel,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",Negative
Intel,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",Negative
Intel,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100ºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",Negative
Intel,Lmao as a recent AMD intern I feel this in my bones. I still can’t fathom just how little effort is put into software stability these days.,Negative
Intel,100% all of this...  Love looking glass by the by,Positive
Intel,How does say VMware handle this? Does it kind of just restart shit as needed?,Neutral
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",Neutral
Intel,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",Positive
Intel,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",Negative
Intel,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,Negative
Intel,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",Negative
Intel,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",Neutral
Intel,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",Neutral
Intel,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",Negative
Intel,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",Neutral
Intel,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,Neutral
Intel,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",Negative
Intel,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",Negative
Intel,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",Negative
Intel,TL;DR. **PEBKAC**.,Neutral
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",Negative
Intel,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,Negative
Intel,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",Negative
Intel,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  😤😭,Negative
Intel,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",Negative
Intel,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,Negative
Intel,"Thanks mate I appreciate it, glad to see you here :)",Positive
Intel,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",Positive
Intel,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",Negative
Intel,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",Neutral
Intel,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",Negative
Intel,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,Positive
Intel,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,Neutral
Intel,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",Neutral
Intel,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,Neutral
Intel,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",Positive
Intel,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",Neutral
Intel,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",Negative
Intel,"Excellent post, very informative. Would take issue with this though:       ""Speaking of VRAM, The drivers use VRAM less efficiently. Look at any side-by-side comparison between games on YouTube between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",Neutral
Intel,"Funny, I saw the title and thought the same too!",Positive
Intel,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",Negative
Intel,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",Negative
Intel,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,Neutral
Intel,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",Neutral
Intel,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",Negative
Intel,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",Negative
Intel,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",Negative
Intel,ursohot !  back to discord rants...,Neutral
Intel,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,Negative
Intel,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",Negative
Intel,"I dunno man. I’ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I haven’t used anything by team green lately, so it may all be the same shit , different pile.",Negative
Intel,Lol same with me tbh I haven't had any problems 😂 but I guess some do idk 🤷. I have crashed less with AMD than my old  Nvidia card.,Positive
Intel,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",Negative
Intel,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me £540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent £200 more on a second hand 3090 instead of this.",Negative
Intel,"It doesn't handle it, it has the same issue.",Negative
Intel,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),Neutral
Intel,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",Neutral
Intel,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",Positive
Intel,Me neither. I use a RX580 8GB since launch and not a single problem.,Positive
Intel,Because they're talking absolute rubbish that's why.,Negative
Intel,You are one of the lucky ones!,Positive
Intel,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",Negative
Intel,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",Negative
Intel,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",Positive
Intel,lol your flair is Please search before asking,Neutral
Intel,"Hey OP — Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,Negative
Intel,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,Neutral
Intel,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",Negative
Intel,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",Negative
Intel,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,Neutral
Intel,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,Neutral
Intel,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol 🤣🤣🤣🤣   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",Negative
Intel,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",Neutral
Intel,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",Positive
Intel,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",Negative
Intel,"""NVIDIA, it just works""",Positive
Intel,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,Negative
Intel,What is the AMD Vanguard?,Neutral
Intel,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",Negative
Intel,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,Negative
Intel,You misspelled $2.3T market cap....,Neutral
Intel,"Okay yeah fair enough, hadn't considered this. Removed it from my post",Neutral
Intel,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",Neutral
Intel,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",Neutral
Intel,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",Neutral
Intel,This is not a fix. It's a compromise.,Negative
Intel,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",Neutral
Intel,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,Negative
Intel,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",Negative
Intel,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,Neutral
Intel,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,Negative
Intel,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",Negative
Intel,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",Negative
Intel,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",Negative
Intel,The comment I quoted was talking about people playing games having issues.,Neutral
Intel,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,Neutral
Intel,The thing I quoted was talking about people playing games though.,Neutral
Intel,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",Negative
Intel,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",Negative
Intel,"Idk, I don't use Linux",Neutral
Intel,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",Neutral
Intel,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",Neutral
Intel,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),Neutral
Intel,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",Neutral
Intel,Because adding a feature for a product literally gives users more control for that product.,Neutral
Intel,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,Neutral
Intel,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",Negative
Intel,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,Negative
Intel,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",Neutral
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",Negative
Intel,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,Negative
Intel,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",Neutral
Intel,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",Neutral
Intel,*wayland users have joined the chat,Neutral
Intel,You're falling for slogans.,Negative
Intel,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",Positive
Intel,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,Negative
Intel,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),Neutral
Intel,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",Neutral
Intel,Honestly after a trillion I kinda stop counting 😂🤣,Neutral
Intel,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",Neutral
Intel,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",Negative
Intel,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",Negative
Intel,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",Neutral
Intel,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,Negative
Intel,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",Neutral
Intel,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",Neutral
Intel,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",Negative
Intel,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",Negative
Intel,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",Neutral
Intel,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",Negative
Intel,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",Neutral
Intel,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",Neutral
Intel,Oh then just ignore my comment 😅,Negative
Intel,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",Positive
Intel,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,Negative
Intel,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",Positive
Intel,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",Negative
Intel,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",Neutral
Intel,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,Negative
Intel,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",Negative
Intel,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,Negative
Intel,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,Negative
Intel,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",Negative
Intel,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",Neutral
Intel,"Too soon to tell, but hopes are high.",Positive
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",Negative
Intel,"Agreed, they cannot rest on their laurels.",Negative
Intel,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",Positive
Intel,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",Neutral
Intel,console gamers know pc’s are better and don’t really complain about upscaling and 30fps.. you’re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldn’t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,Neutral
Intel,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,Neutral
Intel,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",Negative
Intel,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned off   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",Negative
Intel,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",Negative
Intel,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",Neutral
Intel,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",Neutral
Intel,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,Negative
Intel,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,Negative
Intel,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,Negative
Intel,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",Negative
Intel,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",Negative
Intel,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,Negative
Intel,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",Negative
Intel,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,Neutral
Intel,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete. And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",Neutral
Intel,When did they do this switch? I remember years ago when I configured that their windows drivers weren’t being so nice to the card detected in a VM.,Negative
Intel,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",Neutral
Intel,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",Negative
Intel,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",Negative
Intel,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",Neutral
Intel,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,Neutral
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",Negative
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",Negative
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",Negative
Intel,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",Neutral
Intel,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",Negative
Intel,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",Negative
Intel,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",Negative
Intel,Oh and XE also have bug feature reporting.  Omfg!!!!,Negative
Intel,Nobody is 100% right ;),Neutral
Intel,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),Neutral
Intel,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",Negative
Intel,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",Negative
Intel,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",Neutral
Intel,What about using a DP to HDMI 2.1 adapter for that situation?,Neutral
Intel,"2021 my guy, it's right there on the date of the article.",Neutral
Intel,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,Neutral
Intel,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,Neutral
Intel,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",Neutral
Intel,And I guess infallible game developers too then. /s,Neutral
Intel,So you decide what criticism is valid and what not? lol,Neutral
Intel,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,Negative
Intel,"Yup, but do you see them making a big press release about it?",Neutral
Intel,that is not how it works but sure,Negative
Intel,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,Neutral
Intel,>whine about Redditors.  The irony.,Negative
Intel,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",Neutral
Intel,learn to comprehend.,Neutral
Intel,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,Neutral
Intel,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",Negative
Intel,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",Negative
Intel,"No, that would be you obviously /s",Neutral
Intel,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,Neutral
Intel,"Yea, given the state of XE drivers every major update has come with significant PR.",Neutral
Intel,Why not ;),Positive
Intel,Go word salad elsewhere.,Negative
Intel,"I have replicated the issue reliably yes, and across two different systems.",Neutral
Intel,If discord crashes my drivers.. once every few hours. I have to reboot,Negative
Intel,Discord doesn't crash my drivers  I don't have to reboot.,Positive
Intel,Really love how the 6000 series radeons look.,Positive
Intel,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",Neutral
Intel,That's a good looking line up,Positive
Intel,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",Negative
Intel,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",Positive
Intel,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",Positive
Intel,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,Positive
Intel,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",Negative
Intel,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,Negative
Intel,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",Negative
Intel,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",Negative
Intel,That's suprising. Generally the 70 series and AMD equivalent cards are more popular in Germany. Is 4060ti discounted heavily?,Positive
Intel,"Always happy to see ARC succeeding. We need a third competitor, and they've got what it takes.",Positive
Intel,Im more confused about how do you guys still buy RX 6xxx series. Only RX 66xx series that are still widely available in my country. RX 67xx and above is just OOS.,Negative
Intel,That 7900xtx sale number is insane,Negative
Intel,That just shows that most people that buy GPU's don't know a thing about them.,Negative
Intel,"4060 Ti is not discounted well at all here.   390€ and 460€ for 8/16GB versions, really bad deal which is why I am absolutely baffled to see it this high in the list. Might be christmas shoppers who just buy the highest nvidia card within their budget without doing any research - that's my best guess actually.",Negative
Intel,best discounts were 6750xt 6800 and 7800xt,Positive
Intel,4060 ti is selling as a surrogate Quadro for AI/ML. $400 for 16gb is not awful in that context and you get full CUDA support,Neutral
Intel,"Intel is putting in the effort on the software front as well. Arc cards are great value, especially with the improvements intel has made with driver updates. With talk about XeSS being integrated into DirectX with Windows 12 (among other upscalers as well) I hope that intel GPUs will be even more competitive.",Positive
Intel,"Battlemage needs to be stable and competitively priced.   First impression matters so much these days, it cannot be overstated imo.",Neutral
Intel,"They're not out of stock there, duh",Neutral
Intel,Yeah the Germans love the 7900xtx in particular for some reason. It was really high up considering its price in last week's summary as well. I guess they don't appreciate the 4090's price.,Positive
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"I have so many (adult) friends who always bought NVIDIA, will always buy NVIDIA and don’t do a single bit of research.   Had one friend even ask me, I recommended the 7900Xt for her budget and she got the 4070 Ti because she didn’t trust my word or AMD  It’s such a big habit issue. Shopping season really showed me that nvidia could literally sell a brick and habit shoppers would buy it",Negative
Intel,">390€ and 460€ for 8/16GB  That equals to $350 and $415 before taxes, a lot lower than in USA ($390 and $450 before taxes)",Neutral
Intel,"They are pretty much gone in Germany as well:  6950XT - only XFX for 599 at Mindfactory   6900XT - gone   6800XT - only XFX for 499 at Mindfactory and Asrock for 580 at various   6800 - only XFX for 435-440 at Mindfactory and some   6750XT - a bunch of models for 373-405 at various   6700XT - a bunch of models for 344-369 at various   6700 - XFX and Sapphire for 298 and 322 from one retailer   6650XT - gone   6600XT - gone     So the high-end is gone except that XFX stock that will probably be gone in the next 2-3 weeks as well, the lower end is gone. Only 6750XT and 6700XT is still there a bit more and of course the 6600 because that has no replacement at it's current 200-220 price, the 7600 is way more expensive at 270-320.",Neutral
Intel,"I know, but i mean, both 6750 XT and 6700 XT still has more than 300 stocks each in single week (if i read the stat correctly). That's a lot of GPU. I thought the stock is already thin globally which in turn make stock in my place nonexist.",Neutral
Intel,"I thought I saw some 8GB models as low as $330 and 16GB models at $400 but even then I would much rather get another GPU like a 6700XT  (you can still get at here and we even had one down at 299€ for a while, think they are back at 330€ tho)",Neutral
Intel,the card is pretty bad if you missed that somehow,Negative
Intel,AMD probably ships leftover to countries in which they know it will sell,Neutral
Intel,"Sure, but I was replying to you saying ""4060 Ti is not discounted **well at all** here""",Neutral
Intel,"Because most people are not willing to pay that much for GPUs that weak. Sure ""discounted well"" is subjective anyways, I got your point that they are below MSRP but I am pretty sure you know what was implied when reading my comment.",Negative
Intel,"As you notice the photoshop version differs, so you can't compare them really",Neutral
Intel,I also another insanely high score:     7900X + RTX4090 [Photoshop score 8995](https://benchmarks.pugetsystems.com/benchmarks/view.php?id=168538),Positive
Intel,So basically PC games are never going to tell us what the specs are to run the game native ever again.,Negative
Intel,"Well, folks, the game is basically software ray tracing an open world regardless of settings at all the times using compute shaders from what I gather. I mean, what'd you expect from something pushing consoles to the brink? That it would not be equally demanding on PC?",Neutral
Intel,The true crime here is needing FSR to reach these requirements.,Negative
Intel,"Oh wow, this is probably the first time I'm seeing dual channel listed in the RAM section of specs requirements.",Positive
Intel,im more baffled as to why they are comparing a RX 5700 to a GTX 1070. Id have thought they would have said a RX 590 (instead of 5700) or RTX 2060 (instead of 1070),Negative
Intel,> Enthusiast   > 1440p [...] with FSR Quality [...] 60 FPS  1440p + xxxx Quality upscaling is just another way to say 1080p you cowards. It's 2023 and enthusiast level is 1080p @ 60 FPS.   Looks pretty though with the amount of vegetation.  I'm surprised that they put the RTX 3080 and RX 6800 XT side by side in a game that has forced ray tracing. I guess that's the advantage of being cross platform and AMD sponsored.,Neutral
Intel,"well, at least the chart is easy to read, not a complete mess",Neutral
Intel,This gives me hope that Star Wars outlaws will also have FSR 3. 🤌,Positive
Intel,60fps with balanced upscaling on a 4080/7900xtx … That’s not a good sign,Negative
Intel,It's joever. We are officially at the point where devs are using upscaling as the standard to reach playable frame rates instead of optimizing their games.,Negative
Intel,"If you look in the top right, these seem to be based on ray tracing being enabled? If that is the case, these specs actually seem reasonable.",Neutral
Intel,"FSR and DLSS were supposed to be optional, now they seem to be mandatory in any graphics setup.",Neutral
Intel,Yay finally a game that'll have both fsr 3 and dlss 3 so it'll work perfectly on either gpu brand at launch.,Positive
Intel,"Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. The requirements should all be done without any upscaling or frame generation tech to be upfront and honest about how the game will run. Upscaling will then just be an available option for users to further increase performance.",Negative
Intel,"Actually not to bad for current standards, 1440p 60 with fsr quality on a 6800xt/3080 is great if high includes RT, if it doesn't then it sucks",Negative
Intel,"So with my 6800xt, which should be able to do 1440p high settings at over 60fps easily, I'll have to use FSR just to get 60fps? Great, another unoptimized game headed our way.  So sick of the awful PC ports and such. 6800xt is still a very powerful card yet I'm seeing more and more games require it for 1440p 60fps high or even 1080p 60 fps high. Ridiculous.  The reliance on upscaling tech instead of properly optimizing the game is becoming a problem. And it seems this game has forced RT which is idiotic. RT destroys performance and should always be an option.",Negative
Intel,"If you notice, all the mentioned AMD GPUs are significantly better than Nvidia GPUs, so does that mean bad performance for AMD GPUs?",Neutral
Intel,So i bought an 1200€ card (rx 7900xtx in my region and at the time of launch) and can now enjoy new games at 60 fps with BALANCED fsr upscaling. We love to see it. /s,Positive
Intel,so I'm doomed to use FSR even at 1080p ? what a shame pc gaming has become.,Negative
Intel,"I honestly don't get this upscaling outrage. It's not a fixed requirement. If you want to play native, just turn off upscaling but you will have to turn down settings to compensate. Everyone wants to have their cake and eat it too. People want super low requirements, super high graphics with high FPS.  The thing is game devs/publishers want to highlight all the ""eye candy"" in games to show their advancement and that's how they want to advertise it. They have a certain level of visuals they want to present for the game. So they crank up the baseline and focus on making it look as good as they possibly can and by using upscalers to get to the playable FPS.  Other option is they ditch making games look any better and just keep it restrained to good FPS at native. But then you'll have the other subset of complainers that will complain about the game's graphics looking like X many years ago.  Take Alan Wake 2 for example, The medium setting looks better that some High-Ultra setting games from a few years ago. Running a AAA title 2-3 years ago at 'medium' preset may not equate to running a current AAA title at medium preset.  The games are getting too demanding because they are starting to push the limits visually (partially because on AAA titles, that's something they really focus on for advertising), if your card can run it, great, if not, adjust your settings to match your cards capabilities. If you bought a 6700XT 2+ years ago to play native 1440P on high setting, guess what, it's not always going to play 1440P at high settings as the years go by. You have to adjust your expectations of the card's capabilities as time goes on. Medium preset now is likely better than High preset then. If you want the ""same"" performance you used to get, match the visuals and see how it does.  Seems like the only way to get people to stop complaining is to just halt all visual/graphical progress and stick to limiting the game's visuals so they run well on mid/low tier cards.  People keep throwing out ""OPTIMIZATION"" but on one knows what/how they want that done. WHAT exactly do you want optimized? How do we know the game isn't optimized to the graphics/performance it gives?  What if they just made the ""High Quality"" preset look like games from 2018, would that now be considered ""Optimized""?  (I'm not talking about this game in particular, just in general, it was same outrage w/ AW2 and that turned out better than expected)",Neutral
Intel,I hope they will bundle this game with CPUs/GPUs,Positive
Intel,holy hell so for 1080p you need a 1440p card because not even a RX 7600 is on par with a 3060ti or a 6700XT,Negative
Intel,Recently upgraded GPU and CPU and crazy that newer games are already requiring the newest GPUs to play them. Personally I don't think it's a horrible thing but at the same time it just screams horrible optimization.,Negative
Intel,This game better look like real life with those specs. I does looks beautiful!,Positive
Intel,"Another garbage with forced upscaling at baseline. When upscaling winded up - everyone was ""Hell yeah, high refresh rate AAA gaming""  Fast forward to today: ""FSR performance just to hit 1080p 60fps - yay..""   I fucking knew it.. it was so obvious upscaling will turn into this and replace game optimizations. Why optimizing shit, when you can tell people to turn on fucking FSR / DLSS / XeSS performance??   What a fucking shitshow... And few weeks back when I said I'm not gonna buy AMD GPU again because upscaling is getting mandatory and DLSS is just objectively superior - I was laughed in the face. Every major AAA game onwards will be shoving up upscaling as base line performance - so your typical 60fps marks (and 30fps - yes apparently that is still a thing on PC in 2023). There's no escape from this.",Negative
Intel,Damn. My 5700XT did Mirage fine on Medium/Low shadows. Now its basement quality.,Negative
Intel,This is starting to get scary and not cool. Every stat is not native performance and a 6800xt with fsr at 1440p is only pulling 60fps?!?!? What the actual fuck is going on?!?!? Midrange is definitely on its death bed if this is the new norm...,Negative
Intel,I love it how absurd these things are these days.,Negative
Intel,Really tragic this IP ended up with the most anti-consumer publisher in the world.  Layers upon layers of DRM.  Timed Epic exclusivity. No achievements.,Negative
Intel,Does anyone know if it supports SLI or crossfire?,Neutral
Intel,Source  https://news.ubisoft.com/en-gb/article/G0eJBcH8NcvNO2MPqb1KV,Neutral
Intel,"Hopefully this is another Alan Wake situation where the game performs better than what their system requirements would suggest. With this being a based on a movie game, I don’t have super high hopes that that’ll be the case though. Going from movie to game and vice versa almost always has bad results.   Also, displaying system requirements with upscaling is a bad joke. This is basically saying you’re not getting native 1080p 60fps without a 6800xt or 3080. Upscaling is definitely becoming the crutch a lot of folks feared it would be.",Negative
Intel,I thought upscaling technology is for low end gamers to get more FPS or people who work on 4k monitors and use upscaling to get better frames on them. but it seems to be used as an excuse for bad game optimization. What a complete joke!!,Negative
Intel,They are so bad this days that if the say the specs without the upscaling it would be 4090s all across the board,Negative
Intel,Why in the f*ck is upscaling included on a specs page?,Negative
Intel,no more software optimization and full upscaling  bleah,Negative
Intel,Now THIS is what I call a reasonable spec sheet for a next-gen game 👏👏👏,Positive
Intel,"Surprisingly good specs. 1080p high 60 FPS on 3060ti/6700XT is what we can expect from 3 years old cards in open world games.  Yes, I know it's with upscaler. But if the footage stays - it's understandable. Ubisoft is always about open world games afterall.",Positive
Intel,"I've never even heard of this game, nor care about it, but these system requirements offend me.",Negative
Intel,seems like graphics reached its eak with ps4 and games dont look that much better naymore and requires 4k dollar pc to run with dlss,Negative
Intel,"What about steam achievements ? :), i want to show off perfect games on my steam profile :)",Positive
Intel,"Telling me my $1,000 GPU will only get me ~1200p at 60fps tells me you don't want me to buy your game.",Negative
Intel,How about support them with games that people actually PLAYS ? Like Alan wake 2 🤣,Neutral
Intel,Fuck FSR and DLSS. This isn’t how they are meant to be used.  Rather than making a good experience better they are being used to make unplayable trash playable. Papering over shit programming.,Negative
Intel,Nice,Positive
Intel,"If Anti-Lag is still locked by then , please enjoy your frame gen input lag.",Neutral
Intel,"The game has a huge graphical potential, it's the right time to release that much potential environment in this era .   But it wasted its potential by considering lower class GPUs there by reducing polygonal complexity and complexity of environment including terrain and vegetation , animal complexity ,etc.....  Imo, it needs to have 2X polygon than what it seems.  It seems ultra poooooorly optimized as per the recommended requirements coz it's a bad graphics.",Negative
Intel,Native gaming died or what ? Wtf they turning pc gaming into console gaming,Negative
Intel,"60 FPS WITH Frame Gen AND FSR balanced? God, that's gonna be a horrible experience.  I really don't like the direction we are heading.",Negative
Intel,4k balanced FSR means 1200-1240p ... lower than 1440p ! And 4080-7900 XTX for that ?,Neutral
Intel,Their supporting FSR 2 and 3? what the hell does that even mean? Unless they plan on releasing b4 FSR3 becomes drops.,Negative
Intel,"Omg, it would be a graphic master piece or  bad optimized thing.",Negative
Intel,First time I see matches recommendations for nv and amd GPUs...,Neutral
Intel,Rip laptop rtx 3060 6gb,Neutral
Intel,"How do FSR 2 & 3 work with tech like ReShade?  ReShade has a pretty cool shader that will make any game into stereoscopic 3D.   When it works it works well, but it doesn't work in every game.  I'm curious because I would like to try it for this, since like 99.97% of Avatar's appeal (shtick?) is the 3D.",Positive
Intel,Upscaled 1080p low settings and only 30fps with A750? At least the game has xess.,Neutral
Intel,*NATIVE* resolution gang ftw!,Neutral
Intel,"I have been wondering with a few releases lately (Forza Motorsport, Cities Skylines) what GPU the dev machines they are using. I have a 7900XT and it ain’t enough to get above 60FPS without variable resolution at 1440p, seems kinda crazy that one of the top 5 cards from the latest gen can’t push enough pixels for these games.",Negative
Intel,Looks capped at 60fps?,Neutral
Intel,"Funny, how it only tells you the settings for FSR but not for DLSS and XeSS even though those 2 are also supported.",Neutral
Intel,How come 6800XT is mentioned? But not the 7800xt? As recommended specs for 1440p?,Neutral
Intel,"Rip my 780m and rtx 4050m. I don't understand why upscaling is ""mandatory"" not ""supplementary"". Do we need a SLI rtx 4090 for native 4k@60hz?",Negative
Intel,Is it using UE5?,Neutral
Intel,"Ubisoft, rip on launch.",Negative
Intel,guessing no DLSS3 then?,Neutral
Intel,I guess that fsr3 will be added in an update at some point since everything mentions fsr2. It will be interesting to see it running on my gtx 1060 since it seems to be a big upgrade on older cards.,Positive
Intel,"Another game with upscaling baked into all presets. ""Free performance"" they said. I expect frame hallucination tech to be included in these presets in 1-2 years tops",Neutral
Intel,"Can't wait to be ridiculed by Alex at DF again for wanting PC requirements that don't include upscaling.   Upscaling should be an assist, not a requirement.",Negative
Intel,Farewell 1660ti… looks like it’s time for an upgrade,Neutral
Intel,well my 3300x is now obsolete for these new AAA games...,Negative
Intel,4k ultra right up my alley 😏,Positive
Intel,fsr3 frame gen but in the specs themselves FSR2 is stated everywhere. What? So there's FS3 FG bu no FSR3 upscaling?,Neutral
Intel,7900xtx will do 4k 120fps with FSR 3 then I guess?,Neutral
Intel,What must one do to achieve a higher rank than an enthusiast? A demi-god?,Neutral
Intel,"I think we're at a point in gaming where DLSS / FSR will be mainstream in pretty much every game now. I mean even my 4080 can't get 60FPS 1440p in the Witcher 3 or Cyberpunk maxed out with ray tracing, have to use DLSS to increase to above 60fps. I do think DLSS / FSR is actually good technology, but for those enthusiasts who like to play native, I feel like having a $1200 GPU that can't run 4K or even 1440p ULTRA settings at 60fps depending on the game, is just a cash grab.",Neutral
Intel,Is vrr fixed with frame gen then?,Neutral
Intel,"Will someone please get a message to developers that FSR/DLSS should be *optional* and used only if you want to sacrifice some visual fidelity in order to run 144 FPS+ on high refresh rate displays, and let them know FSR/DLSS should never be *required* to get a 90GB bundle of spaghetti code to *barely* reach a blurry ass 60 FPS?  K? Theeenks!",Negative
Intel,"Yet another AMD sponsored game without DLSS....      EDIT: Nm, it appears this game will offer DLSS thank god.",Positive
Intel,Hoping with my 7800x3d and 7900xtx fsr 3 4k maxed is around 120fps for my tv refresh rate. To be honest at my age I barely notice the quality difference of any input lag.,Neutral
Intel,They recommend upscaling even at 1080p.. disgusting ew,Negative
Intel,pretty much this we all knew they would start using upscaling as a crutch.,Negative
Intel,Welcome to modern times and how you can’t just brute force everything anymore. Or you can go buy a 4090 and not complain about how expensive that is.,Negative
Intel,"Yeah, I'm not crazy about using upscaling techniques on everything to gauge it's performance and requirements. I think more time needs spent on optimization but I could be ignorant for stating that.",Negative
Intel,My thoughts too…,Neutral
Intel,Thanks to all of you that were screaming dlss looks better than native lmao.,Positive
Intel,"I tend to be much more picky than those around me. A group of people I play with were all playing Remnant 2, and I FSR was driving me crazy, they all have way less powerful computers than I do, and none of them cared at all FSR was on. In fact they were impressed with how well it ran and how it looked. I think there is a general 80% of the PC population that doesn't mess with graphic settings and just wants the game to play. I'm a min/maxer when it comes to graphics where I constantly tweak it until I get the perfect output for me.     I'd love to have real data from games like this on how many people actively change graphic settings.",Neutral
Intel,and a year ago i kept saying this here and i got downvoted to hell   its so obvious that the game engine devs and game companies pressured both nvidia and amd for this because they can release games faster and somewhat unoptimised,Negative
Intel,"Upscaling, once a promising and beneficial tech, now abused by almost every dev cause they are lazy to optimize. :(",Negative
Intel,Nope we as a community abused a nice thing,Negative
Intel,"Alan Wake 2 (and others) is proof that this doesn't matter any more. DLSS Quality looks and runs better than DLAA native rendering as shown a number of times and documented by tech power up and actually observed by those of us playing it  It's about time people stop crying the ""omg no native rendering"" beat as this is a mentality that is irrelevant in 2023. It's pretty clear that too many games come with performance issues and as such ""native"" rendering has the worst performance in those titles. At least upscaling offers a means to get better performance and in the vast majority of titles, better image quality as a result too with intricate details rendered at a higher fidelity thanks to image reconstruction. The likes of Digital Foundry showcase those benefits often in their videos. Plus the fact that all of the consoles upscale yet nobody bats an eyelid, yet for some reason a small portion of PC gamers seem to demand it no matter what, and then complain when it doesn't run as well as they had hoped in various titles.  It's a hard pill to swallow, but it's \[upscaling\] progression and the key driving factor as to why we can path trace and use other modern engine tech with such good performance and visual quality. This is probably the wrong sub to say such things, but this is the truth, and I fully expect pitchforks to be mounted.... But don't take it out on me, point the finger at AMD and ask them why they are falling so far behind.  At the end of the day if it looks good and runs good then nobody needs to bat an eyelid on whether it's got native or upscaled rendering. Everyone benefits as a result. Fact is the vast majority of modern games look and run better when using DLSS with only a handful of FSR exceptions (Callisto Protocol, for example).  Personally I want improved image detail, sharpness and performance, not just AA that DLAA brings to the table since it's just a more modern AA method vs ancient TAA/MSAA/FXAA etc. I don't care if there's no native option, I can just run with DLDSR if I want to render at a higher res and then apply an upscaler to that output to get the best of both worlds.",Neutral
Intel,Didn't take them long to make upscaling worthless.,Negative
Intel,i smell a burgeoning cottage industry of game spec reviewers!,Positive
Intel,"I've hated upscaling since the beginning.   Now, it's being proven right in our faces that companies are no longer making 100% they're making 45% done games, and then they rely on upscaling and fake frames to do the rest.   Why have a fully optimized game that runs well natively when you can just lower the resolution by 3x then have frame generation help?   Full force ahead to frame generation= Every game will be optimized so badly that you're forced to upscale",Negative
Intel,"It’s literally right there in the hardware specs. 1080p fsr2 quality is 720p input resolution, so at native the game runs at 720p low on a 1070 or 5700.  It is the inverse of what reviewers have been doing with their benchmarks - if you are a fan who really has a stick up their ass about native res, just do the math yourself. They’ve given you all the pieces you need to calculate it out yourself, but that’s not how the game is supposed to be run so they aren’t going to put native res in the official marketing material.  Games are optimized around upscaling now and it is unhelpful and misleading to pretend otherwise. Rendering 4x as many pixels is *obviously* going to throw things off with more intensive effects, and lead to a generally unplayable experience.",Neutral
Intel,"Judging by how games work on 4090/7900 xtx, these cards turn from 4k 60 fps to 1080p 60 fps",Neutral
Intel,requirements have been a joke for over 20 years at this point i dont know why people are surprised.   on the other hand crappy devs using fancy upscaling and fake frames to get the game playable instead of just making a well made game is infuriating,Negative
Intel,yeah this is the new standard,Neutral
Intel,"RT is the future. With this game and Alan Wake 2 using software RT at all times and hardware RT for anything beyond ""low"", RT is the future.  At some point GPUs will be so powerful and game engines will have RT in the bag that RT will be used in basically all indie games...and then suddenly nobody will talk about RT anymore because RT is in everything for years.",Positive
Intel,and with fsr now standard on consoles it(or a similar tech) will be standard on pc :(,Neutral
Intel,"people also forget both AMD and nvidia didn't really offer a generation leap in GPU performance this gen, sure it's popular to call games ""unoptimized"" (I know many AAA releases are, not denying that) but the issue compounds with the fact that game devs couldn't predict a GPU generation to be this bad caused by inflated crypro-boom margins a generation prior",Negative
Intel,"As with almost every game release recently, expect the game to perform a bit better than the requirements.    Sometimes the hardware can achieve 60 FPS most of the time but drops below that in other times, so the devs just say it's guaranteed 30 FPS and call it a day. If you look at the 5700 vs 6700XT, the gap is almost 50% in most titles, this means that if the numbers for the 5700 were accurate then the 6700XT will be getting around 45 FPS at 1080p low (unless the game utilizes special hardware only available on the 6700XT) but the game targets 60 FPS at 1080p high on the 6700XT which leads me to believe the 5700 is gonna perform much better which should leave room for turning off upscaling.",Positive
Intel,"It does state ray tracing in the top right, we don't know if that's included in the requirements or not. I would definitely reserve judgment until we find out.",Neutral
Intel,Don't forget about Frame Gen too. IMO that's the worst part because that means it's 30 fps without it. Imagine the input latency...,Negative
Intel,The actual true crime here is even having fsr to begin with. It should just have dlss,Negative
Intel,Seems like they tried to cover every basis with these.,Neutral
Intel,"Yeah it's kinda weird, even games that had a massive difference of performance when using dual channel RAM never listed it on their requirements :/",Negative
Intel,"I've noticed that many developers do this, for some reason developers seem to have some kind of secret passion for the 1070 lol",Positive
Intel,It's because the minimum settings is with software RT enabled and thus they're using the fastest cards without hardware RT that can reach 1080p30 FSR2.,Neutral
Intel,"GCN support is over, RDNA1 is the lowest currently supported arch.",Negative
Intel,"In 2023, RX 5700 performs equally or better than the RTX 2060 Super/2070. The 5600XT is more like a 2060 competitor.   Either the game runs like shit on AMD hardware, or the system requirements are wrong and they just write down the components that they tested with.   Another thing is that the game is doing is software based RT by default and that might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.",Negative
Intel,"They're not, those are the cheapest/slowest GPUs that have 8 GB of VRAM. Clearly game devs are done pandering to people who bought Nvidia's e-waste.",Negative
Intel,What do you mean forced raytracing?,Neutral
Intel,1440 + upscaling looks better than 1080p because you're on a 1440 resolution monitor.   Do you even 1440p?,Positive
Intel,"Honestly RT has a lot of fluff and the majority of which is the most costly for the least amount of noticeable fidelity increase. Try path tracing in Cyberpunk with reduced rays & bounces and you'll still get great soft lighting and GI without most of the performance hit when full tilt. If games only leverage certain aspects of RT then the performance is still comparable, that's also why the 3080 and 6800XT perform similarly in Fortnite with full RT/nanite/lumen settings.",Neutral
Intel,It's actually 960p :(,Negative
Intel,"can advertize pushing all the triangles and RT if you dont stick with a low resolution, certain things launched WAY before they should have in the consumer space.",Neutral
Intel,Support FSR3.0 and being able to use FSR3.0 at launch gives me a bit of skepticism lately,Neutral
Intel,We'll see in a year.,Neutral
Intel,AFAIK  &#x200B;  It is with RT,Neutral
Intel,Seems pretty good to me given it is at 4K with RT,Positive
Intel,"Likely includes the RT features , its also 4k",Neutral
Intel,"Its ubishit, what do u expect, however, since the game has RT reflections, shadows and GI, then i would say this is based on AMD shit RT performance, so lets wait and see how the 4080 is gonna perform.",Negative
Intel,"I wouldn’t mind so much if it was exclusively a 4K thing, but now devs are leaning on upscaling *at 1080p*, where it still objectively looks like ass and blurs everything in the background.",Negative
Intel,"Everyone asked for this. ""iTs tHe fuTuRe"".  ""LoOks bEtTeR tHan nAtiVe"". ""BiGgeR nUmBeR bEtTer"".",Negative
Intel,and we were called [names that frankly should have resulting in bans] when we said this is where it would lead to. and we still get shat on for saying the same about fake frames,Negative
Intel,"I think the top right table is listing the available features/settings the game supports. Depending on the presets, RT may be enabled by default or it may be a separate option from the presets. I guess we'll find out.",Neutral
Intel,"Nothing will stop DLSS / FSR nor RT in new games at this point.  The problem with the rising hardware requirements is, that it will keep a lot of 1440p/4k dreaming gamers with 1080p for a very long time and FSR doesnt really look good enough in 1080p.",Negative
Intel,I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.,Neutral
Intel,Exactly! It even got xess so Intel users also can use xess,Positive
Intel,">Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. T  its not much different for consoles , like alan wake 2 uses Low to mid settings and FSR balanced on PS5 even on switch titles use scaling and no man sky even FSR and stuff.",Negative
Intel,"I agree with this, that’s not that outrageous if this does include RT. Non RT would be pretty bad to need fsr but that is the quality mode.",Neutral
Intel,This is going to be the exact same situation as with Alan Wake. The game is going to come out and the requirements are going to make sense. People just aren't used to seeing games with mandatory rt.,Neutral
Intel,I would say pressured bc they announced it over a year ago and promised we will get it in september. Thats the problem with games an tech they all make promises and then cant keep the deadline. Just make smth new and when it works then announce it and make the last fine adjustments. If you make promises to early you set yourself up for failure.,Negative
Intel,"Maybe on the lower end, but for enthusiast and ultra... no, they're not ""significantly"" better. These are pretty on-par comparisons (outside of RT).",Negative
Intel,"How so? The 4080 and 7900xtx are neck in neck, same as the 6800 and 3080.",Neutral
Intel,"It's 4k, it will take another 5-8 years to be mainstream.  1440p slowly creeping to 10% marketshare give it another 2-3 years to be mainstream",Neutral
Intel,"Consoles use fsr as well, and not just fsr but often at low precision making the upscale rougher. Its an industry wide trend.",Neutral
Intel,">what a shame pc gaming has become.  Could be worse , Consoles use mostly FSR balanced like in Alan wake 2 the PS5 uses Low to mid settings and FSR balanced.  its likely the trend of going from specially home made engines  catered to the games needs to "" Can do everything , but nothing great"" multi tool engines like UE.",Negative
Intel,"> what a shame pc gaming has become  I mean, PC gaming is in the best state ever. So many amazing games that run so well. Just don't buy all these shitty games, not like there is a shortage.  Tbh this game looks like standard movie cash grab shite anyway. Solid 4/10 game I bet.",Negative
Intel,Which means if all games will require upscaling it will force AMD to improve there's. I'm on board with you though will see way less optimization going forward which is not what we want.,Negative
Intel,"Absolutely agreed my friend. Time to leave pc gaming and focus on building yourself. The gaming industry is corrupted to the extreme, and will continue to do so as long as players continue to praise lazy developers. In the pussy that whole shit.",Negative
Intel,"Mirage is PS4 game, Avatar is PS5",Neutral
Intel,"Update: I must've read that wrongly somewhere! Apparently, it uses their own Snowdrop Engine 😳  Well AC: Mirage is nowhere near a next-gen game. Avatars of Pandora, on the other hand, uses Unreal Engine 5 with hardware ray tracing. So it's only natural it needs more horse power. But it'll look 10x better than AC: Mirage. So there's that 😉",Negative
Intel,Timed epic exclusivity? Aww man.,Positive
Intel,Was guaranteed to happen when up scaling was announced... It's an excuse for less optimisation or to push fidelity more.  I saw a video of this avatar game it got an insane amount of plants visible at all times.,Negative
Intel,You... are.. joking... right..?,Neutral
Intel,Because it’s not coming to steam. It’ll be an epic games exclusive so everyone will pass on it until it hits steam in 18 months. At which point no one will buy an 18 month old game for full price and will wait for a 50% off sale at the minimum.,Negative
Intel,Kind of a messed up opinion.  Maybe games don't look much better than PS4 on your PC but they sure as hell do on mine and that would still be true at a lower resolution and framerate.,Negative
Intel,Ubisoft no longer employs achievements on new titles.  Don't get your hopes up.,Negative
Intel,Try ubisoft achievements :),Positive
Intel,"It's doing rt at all levels and 4k? Ye... It isn't mainstream yet, heck 1440p is like At 10% market share",Neutral
Intel,"Iam using afmf daily, the input lag introduced by afmf is between 5-12 ms for me.  Doubt any can literally feel this.  I have afmf enabled in all possible games & emulators.  If you think it's bad for you, you don't need to use it",Negative
Intel,Pretty much yeah... Was inevitable that Devs will use up scaling as excuse.  Same for taa being a absolute shoddy aaatleast up scalers do better aa :/ . But even consoles run now heavy fsr like the PS5 uses fsr balanced on WLAN wake on mostly low settings,Negative
Intel,We don't know the settings.  If its like cyberpunk psycho rt on release which 3080 and 3090 easily needed dlss balanced or performance for on 1440p and 4k...    Then I can see the same case in this game for 4080 and similar cards  Also they dont speak about frame gen just fsr2 ( which doesn't exist here because they support fsr3 which does up scaling and frame gen it's not split like dlss 2 & 3),Neutral
Intel,"The fans of avatar, or people that like fantasy games or just games to have fun in.  Luckily games don't need to be for everyone.",Positive
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Really comes up to how far the rt and lumen stuff can be pushed, if it goes to cyberpunk psycho rt levels? At release it needed a 3080 and 3090 with dlss.   So it could need top end gpu of this gen if its a similar case",Neutral
Intel,yup that is why I will play 1440 UW native with a 7900XTX.,Neutral
Intel,I guess they are confused with dlss.  Dlss 2= up scaling Dlss3 = frame gen  So they thought its for fsr3 too. ( as in fsr2 up scaling and 3 frame gen)   But fsr3 can do frame gen or not.  Fsr3 is up scaling and frame gen.,Neutral
Intel,"Likely mid range gpu and a suit yells in the background ""30 fps with up scaling is fine""",Neutral
Intel,"Likely the same, they didn't want to make the graph even more annoying, hence why fsr3 and rt and xess are mentioned top right.  They got anyway already confused with fsr3 and fsr2 ( cause 3 is fsr frame gen and up scaling it's not like dlss where 2 is up scaling and 3 is frame gen)",Negative
Intel,Very similar performance I guess,Neutral
Intel,Ye,Neutral
Intel,They've been using it since forever. The difference is that PC GPUs were 10x faster than console GPUs so there was no need for it on PC.,Neutral
Intel,"It mentions dlss, fsr3 and xess support, doubt they limited it to dlss2.",Neutral
Intel,"Likely, they were confused because dlss 2 and dlss3.  So they thought its fsr2 and 3 too.",Neutral
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,Honestly... Maybe fsr3 frame gen can help you but... A 3300 Was already outdated when it came out.,Negative
Intel,Most likely the upgrade part called FSR 2 and the frame creation part called FSR 3,Neutral
Intel,If they didnt get confused between FSR 3 and 2 then yes fsr 3 roughly doubles fps same for AFMF.,Neutral
Intel,"Nah usually enthusiast in hardware did mean "" max settings no compromises high frames"" but let's say... This changed the last 3 gens sadly.",Negative
Intel,"with AFMF it works , didnt test with FSR3 now.",Neutral
Intel,Reading before raging :),Positive
Intel,"It is even low quality with 30fps and still needs 5700 ~~XT~~. That is pretty bad :D  *Corrected to 5700, which I think is still a lot for FSR2 at low fullHD, but definitely cheaper GPU.",Negative
Intel,That's with 5700 and 1070.... WHAT exactly are you expecting those cards to do in visually demanding AAA titles in 2023?  People have some crazy expectations. Everyone wants the eye candy but no one wants to pay the performance cost.,Negative
Intel,For 30fps even lol,Neutral
Intel,"Especially recommending FSR.  DLSS at 1080p? Sure why not?   FSR at 1080p? Disgusting, absolutely horrific.  Anyone who downvotes this is a fanboy, end of story. Nvidia's DLSS simply is better at reconstructing an image. At lower resolutions that is SIGNIFICANTLY more important.",Negative
Intel,Or be happy your shitty video card is still supported.,Negative
Intel,and we still get shat on for saying they are or admitting its a bad thing. corpo fanbois are so cringe,Negative
Intel,Even 4090 users use DLSS because it fixes TAA problems. And has a sharpening slider for easy sharpening adjustment.,Positive
Intel,When did I ever complain about how expensive gpus are. I've always been in the category that they are luxury item and vendor can charge whatever they want for em. So you aint never hear me complain about gpu prices.,Negative
Intel,I can't market a game for running with 120fps 4k with raytracing on on a RTX 4060.  But I can market 400 completely identical missions that take up 150 hours of your lifespan and empty out your soul.,Negative
Intel,Yea I know nothing about optimizations and if it could or couldn't fix this or that. I just want them to tell me exactly what I need to run the game at X resolution a Y preset with 60 fps. If that calls for a 14900k and a 4090 then just say so.,Negative
Intel,"Nah, I think we're really beginning to see a trend here with abusing upscaling for free mileage.  Not that upscaling isn't great but...  Apart from RT, on medium settings, I can't really see    massive improvements on visual quality that would warrant such a massive increase on requirements.  UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  We'll probably see this continuing to be a trend for a while, and then as tech novelty wears out, some devs will come out with more optimized and scalable titles.  With hardware prices being as they are, and with 60fps now being highly requested in consoles as well, it's pretty much a given.",Negative
Intel,"People accepted upscaling with open arms, so now devs know they can rely on people using it. Thus they don't have to care about optimizing for native res anymore. Oh you have a 4k card? Okay well we'll make sure it can run the game at 1080p so you can upscale it.",Neutral
Intel,"No average consumer wanted to come to this point. The market leader started pushing upscaling instead of just making stronger GPU’s and there’s nothing AMD can do to win them, only join them.",Negative
Intel,Turns out TAA is worse than AI TAA with AI upscaling.   Of course devs are gonna use it to cover their own asses. And they get perf boost!  Let's face it. The FUTURE is AI everything. Nobody is gonna talk shit about frame generation and upscaling when it looks perfect each frame.,Negative
Intel,But it does in many ways.,Neutral
Intel,"I don't know who those people are. I play all my games on native, whether I'm using an amd or nvidia card.",Neutral
Intel,"dlss looks better than native with TAA because taa is so bad, but dlaa/fsraa at native >>>> upscaling",Negative
Intel,you think engine devs can pressure AMD and even Nvidia? Come on man. Be real.,Negative
Intel,"Ever notice how multi-GPU vanished the moment DX12 shifted the onus from AMD/Nvidia over to the developers? It shows just how much effort they put into getting SLI/Crossfire working when developers instantly drop them the moment they have to do the bulk of the work (significantly less, I might add) themselves.",Negative
Intel,"If I relabeled ""Medium"" preset to ""Ultra"" preset and changed nothing else and told you that you can run this game at Native 1440P Ultra on a midrange 7800XT card and get 60+FPS, would that be ""optimized"" now?",Neutral
Intel,speaking facts my guy,Neutral
Intel,You shouldn't have downvote dood wtf redditors ?,Negative
Intel,People with AMD cards dislike upscaling more because FSR sucks ass lol.,Negative
Intel,"Uhm, me btw...",Neutral
Intel,"> Who actually plays games at native these days, if it has upscaling?  I do.",Neutral
Intel,"I won't judge your preferences, but at least using native rendenring isnt the clown show we have here. Native 4k look the same no matter what gpu render it (performances aside). Now with all the quality preset, we can't figure what quality to expect. 4k60fps at balenced preset? What's that crap even supposed to mean as thoses presets vary from title to title, from NVIDIA to amd. And btw, I'm still using fxaa over taa and fsr when i can, don't ask me why.",Negative
Intel,"You're even seeing this on console, where Spiderman 2 now has RT reflections in every mode. Even Unreal 5's Lumen renderer is a software raytracer that can be hardware accelerated.  RT is the future because there are scenarios that rasterized rendering is just not capable of resolving. Screen space reflection artifacts, light leaking, weird glow on indirectly lit models... I find these so distracting and immersion breaking.",Neutral
Intel,onto absolutely nothing.,Negative
Intel,"Wait what? No performance improvement this gen? Bullshit, RTX 4090 is literally 2 times faster than RTX 3080 and RTX 3090 is only 10% Faster than the 3080  you do the math. RTX 4080 is 50% Faster than RTX 3080.",Negative
Intel,Or the game performs at 50 fps at 1080p low on 5700 but 50fps is a weird number to target and they can't say 60 so they just say 30fps and call it a day.,Negative
Intel,Not if it's using mesh shaders like Alan wake. In that game the GTX 1650 outperforms the 1080ti.,Neutral
Intel,The game is raytracing only with a ridiculous ammount of foooliage.,Negative
Intel,"I'm so glad this is finally getting more main stream spotlight.  I do a little bit in Dota 2 and the amount of people that simply have a terrible experience because of a single stick of ram in that title is staggering.  Sure, it's an outlier, but a lot of esports/CPU bound games see similar issues.",Negative
Intel,i wonder when dual rank and dual channel will start being listed too. since dual rank does help gaming,Neutral
Intel,Honestly my 1070 still runs great for what it is. It’s in my sons rig now because it couldn’t keep up with 1440p depending on the game and definitely not 4k. Still a fairly solid 1080p card.,Positive
Intel,False.  guy blocked me lmao,Negative
Intel,"I thought that was on Linux, though I might be wrong",Neutral
Intel,1070 doesn't have hardware RT though.,Neutral
Intel,According to the technical director there's no non-raytracing mode. I'm assuming that means software ray tracing for the minimum spec.  Source: Skill Up video - https://youtu.be/v91y87R5iDk?t=332 (5m32s),Neutral
Intel,Spoken like someone who has never upscaled 1080p content on a 1440 screen. The pixel ratios are all off. It's fucked.  1440p is perfect for watching old 720p HD content though.,Negative
Intel,Completely agree.,Positive
Intel,Damn my brain must have been on auto pilot. I'm so used to 2160p + FSR quality neatly equalling 1440p.,Negative
Intel,I see the 5700 and the GTX 1070 on the minimum specs. Are those cards going to be RT capable for this game? (It's 30FPS LOW with FSR but still.),Neutral
Intel,"Sometimes the DigitalFoundry reviewers use their meme powers for good (eg:fighting shader compilation stutter), but their  >""it's better than native!""   memeing about upscaling seems to have gone sideways.",Negative
Intel,"Dude tells truth, go to Nvidia sub, and you fond every second post someone telling that DLSS looks better then native, and every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote.",Negative
Intel,It's purely on the devs for targeting low performance.   We could be getting 240fps at 4K using those upscaling techniques.,Negative
Intel,"Derp, derp.",Neutral
Intel,Sounds like John on Direct Foundry Direct every week.,Neutral
Intel,"""Avatar will be a title that will only appear with ray tracing. But we're developing the game so that the quality and performance will be scalable.""  Seems I was right.",Positive
Intel,">I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.  Facepalm...  Avatar Frontiers of Pandora very literally uses a custom engine, not Unreal Engine.",Neutral
Intel,Yep. This company learned something from all the failure game launches that happened this year. The game companies can't just implement one gpu company's tech and not the others bc then it'll only work on the one gpu that you implemented the tech for and it won't work on the other gpus from other companies. Meaning for example if a game company only implemented fsr it's only going to work on amd gpus and not on Nvidia and Intel gpus and vise versa is true too for both Nvidia and Intel gpus for dlss and xess.,Negative
Intel,Avatar is RT only. There is no non RT mode.,Neutral
Intel,This is mad with snowdrop engine. It's Ubisoft they don't use unreal...,Negative
Intel,"Yep, I missed that. Good call! Makes sense why the specs are a bit higher then but if it is forced on, that's going to be rough on any one with a more mid tier card.",Neutral
Intel,">  Consoles use mostly FSR balanced  heck sometimes they use performance or less, heck a lot of the time even. Especially with RT or performance modes",Neutral
Intel,They have really long way to go. Take for example Lords of the Fallen - UE5's TSR is better than FSR. Take Alan Wake 2 and you get ton of shimmering on the edges. It's also not like nvidia not improving - so while AMD makes a step forward so will do nvidia..   There's also question how much they improve without dedicated accelerators that both Intel and nvidia is using. Intel jumped late to the party and they're already ahead - which just shows how much AMD cares. It's rally embarrassing considering both XBOX and PlayStation are also relying on FSR - ant that's bigger market than what nvidia has.,Neutral
Intel,I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?  I mean the facts are the facts. I need to play on lower settings till we get a 5070 or 8800xt.,Neutral
Intel,"It uses their own in-house engine, Snowdrop. The last game this developer did was The Division II which was excellent.",Positive
Intel,"Took a look at some gameplay on YouTube, and it does look nice enough, though water looked like ass, and as mentioned already, it's an in-house engine (which is nice to see TBH).",Positive
Intel,You will never see a high production game from Ubisoft ever on the Steam store ever again at initial launch.  They are the anti-christ of the gaming industry seriously.,Negative
Intel,Consoles are the baseline in most multiplat games and they've been using upscaling since forever. The real reason is just that we don't have GPUs anymore that are 10x faster than consoles like last gen. You can't bruteforce high resolutions in games that run at like 720p on consoles with GPUs that are only 2-3x faster.  If upscaling didn't exist on PC the minimum here would just be 720p at the minimum and 1080p-1440p at the max.,Negative
Intel,Next-gen doesn't mean that current gen can run it well. It means it's so far ahead that most curren-gen hardware can't run it properly + top-end hardware can only barely run it well.,Negative
Intel,"Exactly. These paid exclusive games launches only make me care less whenever they actually release.  It's not just Epic, Meta has done it with several games (Moss Book 2) and when a proper looking pcvr port comes a year later it sells bad because people forgot about it.",Negative
Intel,"Is that with significant RT? I assumed it was ""normal"" ultra settings.  Still I can play Far Cry 6 and RE4 with AMD sponsored RT at 4k native with 100fps. This game will have to look amazing to justify what... a quarter of that performance?",Neutral
Intel,"You are correct. I overlooked the part where, under Ultra settings, it said that was with FSR2. Now it doesn't seem as bad. I mean, it's still not good because it's rendering somewhere between 1080p and 1440p, but at least it's not 30 fps without Frame Gen/60 fps with it like I thought.",Neutral
Intel,"It might be, AFAIK neither Nvidia nor AMD still support their implementation of it in their drivers.  But I've tried it on a few games and it is pretty cool to me, much stronger than almost any modern 3D movie (IMO why 3D has gone into hibernation), and I gather that Cameron designed the franchise around 3D.  Plus the Avatar 1 game was in 3D.  FWIW.",Positive
Intel,This is what I am thinking too.,Neutral
Intel,"You say that, but they specifically mentioned FSR3, and just DLSS, so i have a feeling DLSS3 will have to go via a mod again.",Neutral
Intel,"i have a believe that even 4C8T will survive for the decade, like the case of that 4790K. But turns out these newer UE5 games are wrecking even newish CPU like the 3600. i mean cmon it's still a 4 years CPU old which i think it's not old yet",Neutral
Intel,Let's hope so. 30fps is far from recommended for FSR 3,Negative
Intel,Yeah I heard it works with that hoping it works with fsr3 now,Positive
Intel,"I've noticed that for some reason newer games look blurry at 1080 even without any kind of upscaling and I'm not sure why, older game look much sharper at 1080p",Negative
Intel,"Just lazy optimisation, that’s pretty much a PS5 equivalent GPU and you can be damn sure that the console version won’t be 1080P 30fps with FSR",Negative
Intel,"No it says 5700. NOT 5700XT, and yes there is a difference. Quite literally the difference between a 1070 and 1080ti. Or a 2070 and 2080.",Neutral
Intel,no no my 7 year old card needs to be able to play at high settings. so does my low end card from today. gotta be high or its just bad,Negative
Intel,Crazy expectation? A game released in 2023 should be able to run at 1080p low 30fps on a low end card like rx6500xt (lowest end modern card you can buy now).  Optimization matters.,Neutral
Intel,"Yup, these cards came out around the time of ps4 I think..that's a long time ago in the graphical quality of games we are seeing.  People should be happy to a point that those cards can even run some of these games if upscaling helps them run that's a bonus as it means they don't need to upgrade if those graphics settings are acceptable to them.",Positive
Intel,"I have a 5700 and it does better at 1080p on these games than what is normally posted. Now it does need quality fsr but its not the worst thing, anything lower than quality is. Bottom line is these are not 100% correct and never have been.  I have zero interest in this game but the specs seem fine even though I wish upscaling wasn't what was relied on for optimization.",Neutral
Intel,"well you dont know what they mean by 30 fps. It could be de 1% lows or the average, we dont know. The easy assumption is that it will be 30 fps average. but that's crazy that a game at 1080p low cant be run by a 5700. It better be a new Crysis.  edit: apparently it uses raytracing only for lighting which would explain the hows and why it crushes the framerate so bad especially on a gpu that doesnt have hardware raytracing like the 5700.",Negative
Intel,"Or in the case of games like Alan Wake 2, you will get unplayable fps if you try to running it at 4k native max settings. Even with a 4090, you will get a glorious 20fps.",Negative
Intel,Assassins creed origins and odyssey side quests/collectibles oh my god,Positive
Intel,Yeah it doesn't give us a actual gauge to understand what we need for set native parameters. I mentioned optimization out of concern that they rely too much on upscaling and it seems like it's the lazy way out of something. It's unrealistic that a usable gaming experience be in a top tier hardware setup that costs $2k+ and top tier hardware of just the last generation are just barely cutting it anymore.,Negative
Intel,I appreciate your insights and opinions. Thank you.,Positive
Intel,">UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  Because they market how ""good"" it looks, not how ""well"" it runs. There's a big influence from marketing perspective to place high focus on ""eye candy"", it's what sells.  The how ""well"" it runs is usually restricted to the small % of us gaming nerds. ![gif](emote|free_emotes_pack|flip_out)",Neutral
Intel,console games started upscaling way before PCs .,Negative
Intel,This sub is rife with crazy conspiracy theories. I don't think it was this insane a couple years back.,Negative
Intel,you forgot who owns one of the most popular engines out there?,Neutral
Intel,"I guess technically it would count but then everyone would flame it for looking bad. But seriouly, what happened when Ultra settings ran real smooth on top of the line hardware? Gone those times.",Negative
Intel,downvoted by devs lol,Neutral
Intel,> RTX 4080 is 50% Faster than RTX 3080.  The msrp is 71% higher too.,Positive
Intel,"Yeah at 1200 bucks and above you got one, that's true but many cards below didn't offer much.",Neutral
Intel,that's stoopid but it's somewhat more understandable. Why a 5700 would struggle so much since it doesnt have hardware raytracing,Negative
Intel,Yep it's like a 10/15% performance uplift in some games but I'd guess they would prefer to keep the requirements sheets somewhat simple.,Neutral
Intel,Why is there a dialog message about unsupported hardware when you try and run a 390X?,Negative
Intel,It can do software based RT just like every other modern GPU out there.,Positive
Intel,Well then no wonder rx 5700 can't manage 30 fps lol,Negative
Intel,"Wait so we are finally getting games made only with ray tracing in mind? This is actually great, 2023 is officially the year the new generation started",Positive
Intel,Avatars uses a Lumen like software RT solution.,Neutral
Intel,"yeah they never finished the sentence for some reason. The full sentences is ""its better than native, when native has forced taa!"" which is why dlaa/fsraa at native look so much better than the upscale versions. normal basic TAA is just so bad, that even upscales not using it look better than native using it",Negative
Intel,"The AA side of things might be better, but I swear no matter the resolution textures become a blurry mess even with Quality mode etc, and it irks me that most people don't seem to notice that.",Negative
Intel,Some people seem to have very low sensitivity to added blur. See also *depth of field blur* and *motion blur* being on by default in every game. It's funny how much people rage against FXAA/DoF/Motion blur but ignore DLSS.  Of course only one is tied to someone's purchasing decision :),Negative
Intel,"Yeah, thatsl happened.",Neutral
Intel,">every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote  Well, yes, you are using shitty FSR which is literally unplayable no matter resolution instead of godly DLSS that is literally better than native even at 720p/performance. The only way you could have seen the ever unmatched DLSS for PC kings instead of the plebean FSR is through a compressed youtube video so your opinion is literally invalid. I hope I am being sarcastic enough.",Negative
Intel,"The problem is some (if not most) games have such a bad TAA it might as well be true, and if often cannot but turned off.",Negative
Intel,"""We could"" but not profitable. Just let imagine these games does not exist in our reality.",Negative
Intel,You didn't understand. It's another Swiss knife engine,Negative
Intel,"It is indeed not a great situation. Things like this shouldn't be forced on, and it leaves me feeling like they haven't given the devs enough time to optimize the settings.",Negative
Intel,It works because most consoles players are noobs they don't care about the tech. They just want to sit on the couch and play. I'm hoping we see them start working more on FSR3 and upgrading older games to it. And you are right nvidia will keep moving forward but so will AMD. They may never catch them since they have a head start.,Positive
Intel,AMD just released drivers for Alan Wake 2 that fixed the flickering while using FSR.,Positive
Intel,The PS5 has a 6700.,Neutral
Intel,">I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?   Not exactly. It's a custom quasi-RDNA2 design (has the improved RDNA2 compute units, but it's missing a few features such as fully-DX12-compliant mesh shaders and VRS-capable ROPs).  For most purposes, it's pretty comparable to a 6700 (non-XT), with the same number of CUs (36), and similar clockspeeds.",Neutral
Intel,"Oh, wow. I didn't know that. But it'll likely be on a similar level as UE5, right?",Positive
Intel,Next gen DOES NOT MEAN that your CURRENT GEN hardware can run it well for sure... Why don't people get that?,Negative
Intel,That reminds me to grab moss 1 and 2. I just got a quest 3.   Agreed though. I’ve been waiting for kingdom hearts 3 to come to steam for so long that I’m basically going to skip it at this point. It’s still stuck on the epic game store.,Neutral
Intel,Far cry 6 and re4 are older and or lower demanding games.  And yes any Mode here does either Hardware or Software rt from. Min to max.,Neutral
Intel,It's a 4 year old cpu yes... But how it was built (4c) was like 10 12 years now old.  6 cores are since a few years already base and more and more 8 cores are standard.,Neutral
Intel,TAA,Neutral
Intel,Temporal anti aliasing.,Neutral
Intel,"TAA is the devil. it forces a blur on every frame, and only gets worse as the resolution goes lower. its why fsr, xess, and dlss can look better than """"native"""" because it doesnt use the horrid plain taa solution so many games do. and why dlaa, and fsraa are so important and need to replace taa completely",Negative
Intel,"Ahh, welcome to r/FuckTAA",Negative
Intel,Even 4K looks blurry with some implementations of TAA,Negative
Intel,they're giving you the bare minimum until your upgrade!,Negative
Intel,Because they are built with upscaling in mind so they have a blurry looks intentionally as it works best with aggressive upscaling.,Neutral
Intel,"Not even close, you're completely out of your mind. The 5700xt is 9% faster than the 5700. The 1080ti is **32%** faster than a 1070 and the 2080 is **19%** faster than the 2070.   Source from techpowerup: [https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339](https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339)",Negative
Intel,"Well the difference is only 256 shaders more on the 5700xt. The 5700xt has 2560 shaders and the 5700 2304. That's 11% more. That's not that much.  The 1080ti has 86% more cuda cores than the 1070, as well as more and faster memory. I would not compare now so haha",Neutral
Intel,"The 5700 and 5700xt is more like a 1070 and 1070ti situation. It's not that big a gap, and you can close it even tighter because the majority of RX 5700 cards can be flashed to a 5700xt bios tightening the gap farther.",Neutral
Intel,"The 1070 was released in 2016. Its already the end of 2023. That card belong to ps4 era. Wake up.   People in this sub are delusional, really.",Negative
Intel,Those are some dumbass expectations. Maybe you should correct them instead of blaming the developers for it.,Negative
Intel,"The 6500XT offers $200 2016 performance. In fact its often worse because of how gimped it is interface wise. Its one of the worst products released in recent years.  Its not the game developers fault AMD pretty much rebadged the RX 480 4 times and they can't be expected to sink their optimization to that level.  RX 480, RX580, RX 5500XT and RX6500XT are virtualy the same product performance wise. RX 5500XT is the most consistent one of the lot offering a somewhat more modern arhitecture in its 8GB variant. The 6500XT on the other hand while on a newer arhitecture is an unmitigated disaster that is on par at best and sinks even below the RX580 when memory, bus and PCIE interface constrained.  The console baseline is around 6700XT/2070 Super/3060ti level. People better get used to that as its gonna become or already has become the norm for 1080p high settings gaming. The 5700 is upwards of 30% slower by default while also having less hardware features. Its gonna become the minimum norm going forward alongside the 2060's of your time.",Negative
Intel,"the 1070 was 2016, the 5700 was 2019. While both not new its not 10 years old like the PS4 (2013)",Neutral
Intel,This is literally only true if you use path tracing. I think you're trying to insinuate the game is unoptimized when it just scales really high with visual features if your hardware supports it. It straight up runs great for the visual features it uses.,Neutral
Intel,"That was exclusive to console releases, no? And never to the point of replacing AA sharpening and rendering the game from 720p…",Neutral
Intel,No I havent. AMD is bigger than Epic.,Neutral
Intel,"Right, you'll have different complaints but complaints none the less even though the new Ultra might still look slightly better prior gen games and offer high FPS but people would complain that the graphics are not advancing fast enough, they look the same etc etc.  Yah those days are likely gone but it's hard to say if it's because the game is unoptimized or if the dev is really pushing the limits of what they can do graphically and use upscaling as a buffer because what they have in mind, or what they intend to show is too taxing for current hardware. We won't really know until a game is released  So screaming game is ""unoptimized game, lazy devs"" seems to have just become the norm before anyone's even played it. This is why I've never in my life ever pre-ordered a game. I still don't understand why people do it (esp now that games are digital) but lot of people do. Then again I've never really paid for a ""skin"" or some other visual add-on so maybe that's what attracts people.  I'm not saying games aren't unoptimized, those do exist, they offer nothing more but run like crap. We know what those game are, it's not hard to tell. But then you do have games like AW2, which before release, we were hearing the same exact complaints and it looks amazing and the minimum requirements were on the conservative side. The game looks better at ""medium"" than high/ultra presets of some games, heck you can even argue that at some low settings it looks better than some games at medium/high presets. The presets are all relative anyways. There's no universal settings that define what each preset represents.  I just don't get the outrage on these minimum spec sheets compared to what the game is showing graphically. At least wait until it releases and we have some hard data.",Negative
Intel,"I'm also a dev dood sometimes it's literally a ""foutage de geugle"" I had a friend who talk about how shit devs in EA (the people who's making the engine) code and doesn't optimise",Negative
Intel,"The commenter I replied to said there wasn't a big generational improvement for GPUs, while I said there was. But I agree that 4080 is expensive for what it offers.",Neutral
Intel,It was the same price as a 3090 at launch and offers 90% more performance.  It was definitely a generational leap.,Positive
Intel,It's software ray tracing which isn't accelerated by hardware.,Neutral
Intel,and i guess it can be hard to tell if ram is dual or single rank with just two sticks of it too.,Neutral
Intel,>might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.  I was pointing out that 1070 doesn't have hardware RT either.,Negative
Intel,So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?,Negative
Intel,"Why not make it toggleable though, you can choose other graphics settings so it makes no sense. Most people don't play games with RT enabled",Negative
Intel,Still kinda weird that they put the RX 5700/A750 on the same ballpark as the 1070... Being that the newer GPUs actually perform closer to a 1080(Ti).,Neutral
Intel,Sounds like improperly configured negative LOD bias. If you have Nvidia you can fix it with nvinspector. On AMD there's nothing you can do IIRC if Devs mess it up.,Negative
Intel,"Exactly what I'm telling them all the time, that I can see difference in textures, in hair, fences, powerlines etc. And every time I get downvoted by the army of ""but it looks better then native"". I feel like majority of that sub needs to go get a prescription for new glasses",Negative
Intel,"So if you stopped measuring who's bigger, why do you think I have no access to DLSS? Tested on rig with 4080 on a C2 42"" screen. Once you stop thinking with the tip of your head where probably Nvidia's logo is tattooed or maybe Jensen's face, who knows, you could come to realization that one person have access to more hardware then only his personal rig.  Do I say that FSR is better then DLSS? No! Both of them have their issues, but none of them is better then native. Keep on fanboying mate, instead of providing normal convoy as grownups",Neutral
Intel,I don't think YOU understand what you're talking about.  Did you seriously expect Snowdrop to be something used in like one game and then discarded completely?,Negative
Intel,"The thing is: creating good looking lightning with RT is much easier than with prebaked illumination. I can fully understand, why developers of new games are using RT as the basis of their game.",Positive
Intel,Minimum requirements go up as games get/look better. They probably made this game with the series s as the minimum. Low settings probably looks better than high in most games.   Rt for the last few years has been added on top of already complete games. Now games are being made with rt as the standard and they would have to manually do a ton of extra work to support 7+ year old hardware. It's the same as having to drop support for last gen consoles to be able to deliver a better game.,Neutral
Intel,"And still - superior upscaling makes it worth paying extra, RT on higher end is worth paying extra. AMD even this gen doesn't undercut price by enough, especially in EU where power efficiency also play a role. For example, assuming I'm gonna use GPU for 2 gens, then I save ~90-100€ with my usage just on electricity alone. I know it's over period of 4 years, but still it's price difference that will recoup over time, even if I have to pay extra upfront. AMD is simply no longer a value proposition.",Neutral
Intel,"They did not, lol - that fixes completely different thing, not the FSR caused shimmering edges.",Negative
Intel,Both excellent 👌 Down the Rabbit Hole was another solid one.,Positive
Intel,"RE4 came out this year.  Anyway the difference between us is I don't think any lighting effect is worth a $1,000 GPU using 1080p resolution and 60fps, but I know many disagree. That's life.",Neutral
Intel,"3600 and the 3300x was built with the TSMC 7nm if i recall, that was 2018 cutting edge tech. and somehow in gaming it's already obsolete. i was planning to replace mine with 5700x  but i cancel it because it's still very much capable for what i do now. but now after UE5 i think i'll reconsider it again, maybe even as far as 5800X3D",Neutral
Intel,It is crazy that I see everyone saying that modern games look blurry. I've known for years taa does this but it seems like I see more people than ever dissatisfied by it.,Negative
Intel,I have to turn it off in borderlands 3. Ugh.,Negative
Intel,r/FuckTAA,Negative
Intel,"My friend nicknamed it after his eyesight. When he wears glasses he has 20/20, but when he doesn’t he says that he sees the image about the same as without glasses",Neutral
Intel,"Out of curiosity what resolution do you usually game at and at what distance? I find taa usually looks pretty good in modern games, but I'm also playing on a TV (48 inches but I'm not at monitor distance anymore). When I played on my 1920x1200 monitor I HATED TAA, well, not as much as fxaa, but moreso than smaa, and I missed msaa or SGSSAA with a passion. Now on the TV at 4k (lol, usually not on a 6700xt), 1440p, or even 1080p I find TAA can do a pretty decent job paired with varying levels of the Radeon sharpness adjustment in the adrenaline drivers. But again, probably due to viewing distance.  Looks leagues better than FXAA and I prefer it to SMAA now. But again, I'm sitting further away and tweaking sharpening to my liking. FSR2 seems to be a wash most of the time but the performance gain is worth it.",Positive
Intel,You dumbass do understand that even the console version of the recent games are total mess? Alan wake 2 have to run on reduced settings to even reach 30fps on xbox/PS. Callisto protocol used to have frametime issue on consoles. Console ports still targets 30fps on quality settings with upscaling while target should be 60fps with 120fps for performance mode.,Negative
Intel,"Yes, seems you are correct, it was the ps4 pro that came out in 2016, still, I wouldn't expect that to be capable of running some of the games we are seeing now without some serious compromises.",Neutral
Intel,I never said that Alan Wake 2 is unoptimised. I just said that even a 4090 can’t max out the game at native 4K resolution.,Negative
Intel,epic is tencent...,Neutral
Intel,"In terms of absolute numbers sure, 3090 always had a stupid price tho for being 10% faster than a 3080.   In terms of performance per dollar the 4090 even at 2x 3080 performance was worse value since you pay 1600 not 1400.",Negative
Intel,"Yes, you either have to get the spec sheet of the RAM modules (That even then it may not specify if it's a single/dual rank module) or test it in a system and use HWinfo/CPU-Z. Not as straightforward as dual channel that's mostly just using two identical modules.",Neutral
Intel,You don't need RT hardware to do software RT. That's what I'm saying.,Neutral
Intel,"For this particular game, there is a software fallback. Also, even if we did get a game that mandated hardware support for RT they'd still likely build it with console in mind so even RDNA2 GPUs shouldn't have issues",Neutral
Intel,"The first RT/DLSS capable cards are 5 years old now, way older than the current consoles. You are definitely not being left behind if you're only outdated by 1 gen lol",Neutral
Intel,I mean the last non ray tracing cards were released 4 years ago (1650/60 and 5000s from AMD),Neutral
Intel,Games running poorly on computers weaker than current gen consoles isn't exactly anything new.,Negative
Intel,"No, you have multiple generations of games from prior to raytracing to enjoy.  Games shouldn't be held back forever just because someone wants to game on a GeForce2 MX400.",Neutral
Intel,You are stuck playing older games. Unless someone can find a way to disable it with a mod.,Negative
Intel,> So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?  A gen old is second gen RT e.g. rtx 3000.  We're halfway through the third RT/matrix generation right now.,Negative
Intel,Yes hah,Neutral
Intel,"The 3090 or 6950xt will very likely run this game extremely well, they're a gen old.",Positive
Intel,"For the AMD GPU, it's probably because minimum will be the max it will support. Medium probably requires Hardware RT.  For Intel, I don’t know. Perhaps it signal not so great optimization for Intel GPUs...",Neutral
Intel,DLSS is very strong in thin lines like fences & hair. Much better than most TAA solutions. Not sure about FSR in general.  An example: https://pbs.twimg.com/media/E6erA6BVkAAuf_M?format=jpg&name=large  Only DLSS resolves the lines properly.,Positive
Intel,"I mean, it's not that black and white. Quality upscaling to 1080 will obviously be more noticeable than 4k. Maybe you *can* tell the difference between 4k native and dlss quality, but people aren't gonna believe you unless you prove it with some kind of blind test, but I doubt you'd be willing to go out of your way to do that.  Even if you could, the benefits of better aa might outweight some negligible drop in textures or whatever it is upscaling reduces. Though that's irrelevant when DLAA or NAA are options too, I guess.",Neutral
Intel,"Test, which one looks better, explain.  https://imgsli.com/MjE3MzEy",Neutral
Intel,Oh man the hair... its so crazy how much upscaling kills the hair..,Negative
Intel,i mean they already arent the smartest bulbs considering they went team green.,Negative
Intel,Reading comrehension dude.,Neutral
Intel,"I do, but thanks for your interest.",Positive
Intel,That will depend on the user. Power is cheap here in canada so none of thar factors into my buying decisions. And rt will depends on the games you play also not everyone plays tripple a games.,Neutral
Intel,"You need to get 23.20.17.05.  Fixed Issues Intermittent flicker may be observed on some textures while playing Alan Wake 2.  Fixed for most people, also has a nice performance boost of about 10fps from what I'm seeing.  That being said I always prefer native resolution unless absolutely necessary. So far only had to use it in Martha is Dead.",Positive
Intel,Nice. It’s on the list. Thanks man,Positive
Intel,Yes.... 7nm was but 4 cores the actual important thing was obsolete like 10 or 12 years ago.,Neutral
Intel,Probably because TAA is (unfortunately) more prevalent than it ever was.,Negative
Intel,"Nah, most people are FINE with it. Its people on FuckTAA that are mad.  The thing is, you have sharpening filters today. You can adjust the sharpening for TAA. You can make it look oversharpened if you want. There's driver level sharpening for both AMD and NVIDIA. Both also have software sharpening filters.  And DLSS and FSR bot have sharpening sliders.  There's 3 ways to adjust how blurry and how sharp the game looks for everyone. It's a non issue and even Digital Foundry acknowledges the benefits of TAA for game devs vs things like the blur. At the end of the day, these games are looking great...in this era, with exceptions. Not all games are blur. The whole blur thing was like in the past 4 years since upscalers started and TAA became the norm. But with all the sharpening options, and with devs actually used to making TAA not so soft looking, things are definitely getting a lot better for TAA.",Neutral
Intel,Oh my bad if I read that wrong,Negative
Intel,And I'm saying 1070 doesn't have hardware RT so it's just as gimped as AMD cards when doing RT.,Negative
Intel,Still pretty annoying nonetheless. Until it's stable no way should it be forced upon a game when someone's willing to run it normally at native.,Negative
Intel,"and the 2080ti is still winning, best purchase in gpu i made in a long time.",Positive
Intel,imagine being mad that 4 year old cards arent high end,Negative
Intel,I'm on a 6700XT. 🙇. I just feel very shocked that I would probably need a gpu upgrade soon. With the demands of new games. Also 1440p ruined me.,Negative
Intel,"Technically makes sense, but they can just make RT something you can toggle between Software/Hardware and they also could just add the Vega 56 in the requirement sheet being that was AMD's competitor to the 1070 or they could've added the GTX 1080(Ti)/RTX 2060(Super) into the sheet.  On Intel's side they only have the A580 as the closest GPU to a 1070, even when it's too similar to the A750 performance wise :/",Neutral
Intel,"It is that black and white for me. I can immediately see the added image softness from upscaling, and it's especially prevalent with DLSS 2.5+ unless you enable a sharpening filter. DLAA also has weird issues like blurring textures (almost like it reverts to initial mipmap) while moving, usually at mid-distance from camera. It's too distracting and I have to go back to regular TAA. Immediately goes away.  1440p to 2160p is better than 720p to 1080p, but it's still noticeable. Detail is immediately lost as lower resolution image is scaled to higher resolution. Is it terrible? No. Actually changing monitor resolution outside of native and having all UI elements scaled looks terrible. That was what we used to do and upscaling is much better than that alternative. LCD panels need to stay at native signal resolution and GPU should scale render resolutions.",Negative
Intel,"And you look at static image all the time while gaming? It's different story when movement is involved, that's where you find all the artefacts you get. Static image doesn't tell a thing when it comes to gaming",Neutral
Intel,"Yes, and that's driving me crazy. I really couldn't stand it in Cyberpunk (in other titles it's either hit or miss), that I play it in native 4k instead.",Negative
Intel,"Wouldn't phrase it like that tough. I had that choice too in January, and actually by going 4070Ti instead of 7900XT I would be able to play Cyberpunk in higher fidelity and better frames then on mine, because AMD cooked FSR3.0 almost a year in the oven, and God knows how long it will take for CDPR to add it to the game, if ever considering that active development on game is ended and they are on bug fixing sprint right now. Also AMD needs to fix FSR in general, because how it works with smaller details like hair and fences is awful. But don't get me wrong, I love my card, just feel I git sold on features that would come handy right kow instead of when I don't need them anymore",Neutral
Intel,"Sorry mate, guess I need to read comments early morning instead of late night, just saw that last bit. My apologies. I'm just tired of upscalare wars whatever sub I go, and green team fanboys screaming that ""FSR is shit, DLSS is better then native"" and then throwing around static images for comparison",Negative
Intel,"It's for textures, not object edges from FSR use, lol. Two completely different things",Neutral
Intel,"people echoing the same shit like ""ooh 4C is obsolete, you need to upgrade that geriatric cpu"". 4C is still solid for gaming (ofc no UE5) and literally anything beside doing compute intensive professional workloads.",Negative
Intel,"Sharpening makes the image less blurry, but it doesn't recover the texture detail lost from TAA blur.",Negative
Intel,"""Even Nvidia foundry""   \---   they are like chief proponents of blur-tech and they don't care about how sharp and easy-to-see the image is. TAA flattens the image so you can't see which objects are far and which are near, you have to strain your eyes and mind to determine that which is unbearable to many. Just give us option to disable it even if it breaks some effects (which is just lazy thing to do on the side of devs)",Negative
Intel,"Sharpening fixes blur, not clearity. That is TAA destroys+add significant ghosting no matter the implementation.      Sharpening looks was worse than the normal cleairy of games, it's not the same thing at all.      >Its people on FuckTAA that are mad.  We are mad because it being forced 1 and 2 it's being used to hide lazy effects that break without temporal frame smearing. People are now punished for turning off forced frame smearing.",Negative
Intel,And I'm saying that maybe the GTX 1070 is better at even software RT compared to AMD cards with no RT hardware. GTX 10 series card did get RTX support through CUDA even though they were horribly bad at it. Older AMD cards with no RT hardware didn't get any support like that from AMD.,Neutral
Intel,"You're not forced to use upscaling though. Unless you replied to the wrong person this is about ray tracing, which can be run at native just fine. If your GPU can't handle RT at native res, then just use upscaling.",Neutral
Intel,The minimum requirements for this game includes de 1070: a 7-year-old gpu from 2016.,Neutral
Intel,"I'm not really shocked, and was out there telling people not to buy those cards on day 1 in large part because of their deficiencies in RT performance and complete lack of matrix accelerators (which Nvidia and Intel have both made a large part of their architecture and got shockingly good results with - DLSS and matrix-XeSS are twice as good as FSR)",Negative
Intel,5700 was probably the lowest AMD card they had to test with.,Neutral
Intel,"It also looks better in gameplay, less ghosting, less shimmering, less aliasing.   So which one looks better? You clearly see the difference in textures/fences/powerline, right?  I even made it easier for you, both images are 1080p output.",Positive
Intel,I personally didn't try DLSS even once but FSR 2.0 does have shimmering I notice to various degrees. It was very noticable on fur in Ratchet & Clank cutscenes for example. Overall I am just happy my 5700xt could still keep up with 1440p.,Neutral
Intel,Hmm yeah whatever you want to believe and let's you sleep.  Have a nice day.,Positive
Intel,AMD CAS is aimed to restore detail,Neutral
Intel,"They aren’t chief proponents of anything, TAAU has been popular for well over a decade at this point and it’s the default approach for consoles (checkerboarding has moire and aliasing issues) and digital foundry simply recognizes this is the reality of how the console world has settled on doing graphics.",Neutral
Intel,The problem with Nvidia is they are massively overpriced. One 4080 would probably cost the same as my whole build or more.,Negative
Intel,"Yes I see which one looks better, and as I said before it's a static image. While I didn't use upscaling in this same exact example, I can go by my own experience and tell that it doesn't look better then native let's say in same Cyberpunk or any other game. And I will beleive my own eyes on my own screen then static images tossed around. TBH, in my opinion it all depends on screen and screen size too, as to me that difference between upscaled and native was less noticible on my old 27"" 1440p screen then it is now on 42"" 4k. DLAA tough, that's another story",Neutral
Intel,"I tried both, and neither of them are better then native as people tend to claim. In my testing FSR have big problem with smaller details like hair, fur, fences, powerlines and as you mentioned, you get severe shimmering effect. I really hope AMD will keep on working on it, an try to get away from that. But DLSS also adds shimmer, it's just less visible then in FSR (atleast in my testing on Cyberpunk pre 2.0), and whatever upscaling method I've used I also could see that image is upscaled (lower res textures here and there etc.). I like idea being upscaling tech, it helps prolong life of GPUs, and for more powerful GPUs to run higher settings with more fidelity, but there's still work to do on them as they aren't perfect yet.",Negative
Intel,Yeah but a 3060ti didn't,Neutral
Intel,"In Cyberpunk you are not comparing upscaling vs no upscaling at the same settings though. You are comparing low RT native vs path traced upscaling and at 4K, the decission there is absolutely clear. On my 4k OLED screen, upscaled PT is so clearly the way to go, there is no competition.",Neutral
Intel,"The gameplay looks exactly like the image + your typical TAA/DLSS smearing. And as I said, in my example DLSS also looks better in gameplay, because as you see... native has a ton of aliasing which is super noticeable while playing (shimmering). There are so many objective tests, which show why its most of the time better than native. (up res to 80% instead of 67% and its always way better, even at 1080p)   Well when did you try DLSS the last time? Newest versions are miles ahead of the ""old"" 2.0 version and pretty much always ahead of standard TAA.  DLSS is essentially a way better TAA solution + AI to fight TAAs issues + lower res. Its so much better than your standard TAA, that youre able to reduce the res and it will still look better in movement. Think about it like codecs for streaming... there are codecs which need a way lower bitrate for the same quality than others. (av1)  Everything graphics related is not rendered at full res anyway (shadows/lighting/lod etc...), there is no line between native and upscaling. Talking about native is nonesense, just image quality counts and thats were DLSS or even FSR 2 (some games) gets better and better. There is a reason why consoles use it... it looks better than without it.",Positive
Intel,Confirmed Can it run CP2077 4k is the new Can it run Crysis,Neutral
Intel,It'll be crazy seeing this chart in 5-10 years with new gpu's pushing 60-120 fps with no problem.,Positive
Intel,In 1440p with 7900 xtx with fsr 2.1 quality it doesn’t even get 30fps,Negative
Intel,"Makes sense. Is almost full path tracing, it’s insane it’s even running.",Positive
Intel,good. this is how gaming should be. something to go back to before the next crysis shows its teeth,Positive
Intel,3080 falling behind a 3060? what is this data?,Negative
Intel,wake me up when we have a card that can run this at 40 without needing its own psu.,Neutral
Intel,5090 here I come!,Positive
Intel,"Playing the game maxed out with path tracing, FG, RR and DLSS set to balanced at 1440p. Over 100fps 90% of the time. Incredible experience.  *With a 4070 ti and 13600k",Positive
Intel,"I mean, Path tracing is to Ray tracing, what Ray tracing is too rasterization.",Neutral
Intel,When Cyberpunk first came out the 3090 only got 20 fps in RT Psycho mode.  https://tpucdn.com/review/nvidia-geforce-rtx-4090-founders-edition/images/cyberpunk-2077-rt-3840-2160.png  Still does  Fast forward just one gem and you don't see anyone saying its demanding with many able to get RT Psycho on thier cards as new cards got faster.  Give it 2 gens and you are going to get 4k60 here.  Gpu will improve and get faster.,Neutral
Intel,"Look at that, my 6700XT is just 19fps slower than the RTX 4090 in this title.",Negative
Intel,It's a good thing nobody has to actually play it native.,Neutral
Intel,Well good thing literally nobody is doing that…,Negative
Intel,The future is not in native resolution so this is really pointless information.  Cyberpunk looks absolutelt mindblowlingy insane with all the extra graphical bells and whistles it has gotten over the years and with Nvidia’s technology it runs so damn smooth as well.,Positive
Intel,4.3fps lol. No amount of upscaling is going to fix that and make it playable. People were saying the 7900xtx had 3090ti levels of RT when it launched. A 4060 ti is 50% faster then it.,Negative
Intel,That's actually pretty amazing for any GPU to get a metric in frames per second instead of minuites per frame.,Positive
Intel,"Meh I get like 90+ fps with everything cranked with RR, FG and DLSS(Balanced) toggled on with my 4090 @ 4k.  Path tracing does introduce ghosting which is annoying buts not really noticeable most times but at the same time with RR enabled it removes shimmering on 99% of objects that is normally introduced with DLSS so I am willing to compromise.   Honestly as someone who used to have a 7900XTX I am disappointed with AMD its clear that AI tech in gaming is the way forward and they just seem so far behind Nvidia now and even Intel(going by some preview stuff). FSR is just not even comparable anymore.",Negative
Intel,Who cares when it looks worse than with dlss and ray reconstruction on top of running a lot worse? Native res 4k is pointless.,Negative
Intel,"Well the upscaling in this game is really good, DLSS with ray reconstructions AI accelerated denoiser provides better RT effects than the game at native with its native Denoiser.  Also Path tracing scales perfectly with resolution so upscaling provides massive gains. using DLSS quality doubles performance to 40fps, and dlss balanced gives 60fps on average, performance about 70-80 or 4x native 4K, that includes the 10-15% performance gain RR gives as well over the native denoiser as well.  I've been playing with about 60fps on average 50-70. with DLSS Balanced and RR and its been amazing. I don't like frame gen tho since it causes VRR flickers on my screen",Positive
Intel,"Running Ray Tracing or Path Tracing without DLSS or Ray Reconstruction is like intentionally going into a battlefield without any gear whatsoever, it's absolutely pointless and suicidal, what we can clearly see here though is top of the line 7900 XTX losing against already mediocre mid-range 4060 Ti by over 50%, which is just beyond embarrassing for AMD Radeon.  All this says to me is AMD Radeon need to get their shit together and improve their RT / PT performance, otherwise they will continue to lose more Marketshare on GPU department no matter how hard their fanboys thinks that they are pointless features, just like DLSS was back on 2020 right?  Also, with my 4070 Ti OC i can run it at average of over 60 FPS at 1440p DF optimized settings with DLSS Balanced + Ray Reconstruction without even using DLSS Frame Gen, with it on i can get over 80+ FPS",Negative
Intel,Running this way means you lose RR…why in the world would you run at native 4k?  It’s completely pointless now with RR.,Negative
Intel,Im having 80 fps  thanks to dlss 3.5  and its looking better than ever,Positive
Intel,"Why would I run native? I have amazing DLSS, ray reconstruction for huge image gains and frame gen. Nvidia offering all the goodies.",Positive
Intel,"I have a 13900KF-4090 rig and a 7800X3D-7900XTX rig. They are connected to a C2 OLED and to a Neo G9.  I've been holding on to play the game till 2.0 update. I've tried many times with different ray-tracing options and they all look good and all. But in the end I closed them all, turned back to Ultra Quality without ray-tracing and started playing the game over 120FPS.  This is a good action fps game now. I need high fps with as low latency as possible. So who cares about ray-tracing and path-tracing.  Yeah ray-tracing and path-tracing are good. But we are at least 2-3 generations away for them to become mainstream. When they are easily enabled on mid-range gpus with high-refresh rate monitors, they will be good and usable then :)",Positive
Intel,"If you have the HW, go all out. That's why we spend on these things.  I'm getting the best experience you can get in the premiere visual showcase of a good game.   It's path tracing. It isn't cheap but the fact that we have DLSS and FG with Ray reconstruction is a Godsend. It looks stunning and it's still early in development.",Positive
Intel,This doesn’t seem right…. 3080 performing worse than a 2080TI?,Negative
Intel,How tf is a 2080 ti getting more fps than a 3080?!?,Negative
Intel,I'm not seeing the RTX A6000 on here...,Neutral
Intel,4 fps for 7900 XTX  Oof. Shows you how much of a gap their is between AMD & Nvidia with pure ray tracing.,Negative
Intel,"Everything maxed, PT, 1440p, DLSS+RR+FG, input feels good, game looks and runs great, 100+ fps",Positive
Intel,"""Get Nvidia if you want a good ray tracing experience""  Yes Nvidia GPUs give a better ray tracing experience but is it really worth it if you are required to turn on DLSS? Imo, the more AI-upscaling you have to turn on, the worse the Nvidia purchase is.   I have a 6900XT and I will readily admit that the RT experience (ultra RT, no path tracing) at native 1080p resolution is ok, like 30-45 FPS (around 50 on medium RT settings), but if I turn RT lighting off (so RT shadows, sunlight, and reflections are still present) suddenly I get pretty consistent 60 FPS (i left my frames tied to monitor refresh rate, so 60 FPS is my max) and i can't tell the damn difference at native 1080p compared to RT medium or Ultra.   So would I spend another $400-$1000 to get an imperceptible outcome (imperceptible to me that is)? Most definitely not.",Negative
Intel,Does anyone actually play this game? Its more of a meme game imho,Negative
Intel,4.3 fps 😂😂😂,Neutral
Intel,"RTX 4090 DESTROYS 7900XTX with over 400% increased FPS, coming in at an astounding…. 19.5FPS 😫😂",Positive
Intel,"Overclocked RTX 4090 can (there are factory OC models that run up to 8% faster than stock). A measly 2.5% overclock would put that at 20 FPS.  Native is irrelevant though, DLSS Quality runs much faster with similar image quality.",Neutral
Intel,[85-115fps (115fps cap) in 4k dlss balanced](https://media.discordapp.net/attachments/347847286824370187/1154937439354368090/image.png) ultra settings,Neutral
Intel,Lol the 3060ti is better than a 7900xtx...crazy,Positive
Intel,Why would you use it without DLSS? Upscaling is the way of the future and AMD better improve their software to compete. Future GPU's aren't going to be able to brute force their way to high frames.,Neutral
Intel,Who cares about native though.  Cyberpunk PT at 4k DLSS Balanced + FG is graphically so far ahead of any other game ever released that it literally doesn't even matter if other games are run at native or even 8K. Cyberpunk is just in league of its own.,Neutral
Intel,ThE gAMe iS PoOrLY OpTImiSed!!!,Positive
Intel,Who cares about 20 fps native? Use the tools and tricks provided by the game and card manufacturer and you are good. The whole native rendering argument can be thrown in the garbage bin.,Negative
Intel,"The 7900xtx losing NATIVELY to the 4060ti 16gb is embarrassing, that card is half the price and overpriced, nevermind Nvidia's far superior upscalers. AMD falls further and further behind in RT, sad.",Negative
Intel,"99.7fps  with everything maxed out at 4k balanced with frame gen.   64.3fps with everything maxed out at 4k balanced w/out frame gen.   29.4fps with everything maxed out at 4k native.   That’s a big difference from this chart what I’m getting vs what they’re getting. For record, I’m using a 4090 suprim liquid undervolted to 950mv at 2850mhz. Stock is 2830mhz for my card.",Neutral
Intel,"Even with frame gen and DLSS quality, Ultra+Path Tracing looks incredible. I get solid 90 FPS. Funny thing is I can't get into playing CP 2077, even with incredible graphics and a QD-OLED monitor. It's just not my type of game :-D",Positive
Intel,I was wondering why a thread like this with the name like this is in this sub. And the I see the name of the OP. And it all makes sense,Neutral
Intel,"""4090 is 4 times faster than 7900XTX""",Positive
Intel,"So? Native 4k is such a waste of compute hardware, only an idiot would use it.",Negative
Intel,People need to change their mind frame. AI is here to stay and will be a major part of graphics rendering going forward.,Neutral
Intel,I played 2.0 yesterday with 2k ultra and path tracing with 155fps (DLSS3.5),Neutral
Intel,"The 4060 Ti literally better than the 7900 XTX in RT, cope snowflakes!111!11! /s",Positive
Intel,"So Nvidia only needs to increase their RT performance 3x while AMD needs to do 14x. We are not seeing that within a decade, if that.",Neutral
Intel,Sounds like Ray tracing isn’t ready for prime if nothing can run it and you have to use AI smoke and mirrors to fake it playable. I’ll worry about Ray tracing when it’s a toggle on or off like shadows and there is no major performance hit. See you in the 9000-10000 series of Nvidia or 12000-13000 series for AMD.,Negative
Intel,Native resolution is a thing of the past. DLSS looks better than native anyway.,Neutral
Intel,Is this with DLSS + FG?,Neutral
Intel,"Ray tracing already wasn't worth it imo, that's why I saved money buying an AMD card. It's not worth the cut in frame rate that then has to be filled with upscaling that makes frames look blurry anyway, so what's the point of even using it?",Negative
Intel,"Meanwhile me using medium settings + performance dlss on 3080 ti to get that locked 120fps. I just can't go back to 60fps. The game still looks amazing, especially on blackstrobed oled.",Positive
Intel,What is even with with all these path traced reconstruction bullshit lately? Ray tracing itself isn't even that mainstream yet.,Negative
Intel,No one is using these settings.,Negative
Intel,"Ray tracing is fun and all, but games have really improve much.. Same animations.. Same type of quest... Same listening to notes.. There have to be better ways....",Positive
Intel,"4K rendering for games like this is pretty pointless unless you're upscaling it to a massive 8K display via DLSS Performance.  Ultimately, the final image quality is going to be more affected by how clean the Ray / Path Tracing is than whether you're rendering at Native 4K or not.",Negative
Intel,"I'm honestly kind of shocked my 4070 is that far above the 7900XTX. I'd have thought it would be no where near close, regardless of ray tracing.",Negative
Intel,cyberpunk sucks don't worry about it,Negative
Intel,3080ti?,Neutral
Intel,"Yes, that is true, but what the numbers do not tell you is that if you tweak some settings, you are able to get 90 fps on that resolution with path tracing with no problem.",Neutral
Intel,I feel like the 4k resolution is here to stay for a couple years boys. Get yourself a decent 4k monitor or OLED TV and chill.,Neutral
Intel,"This is why DLSS3 and frame generation are so important. 4x the performance at over 80FPS with a relatively minor amount of silicon dedicated to AI, with only minor visual degradation.",Positive
Intel,"It's way too early for using ray / path tracing in games comfortably, maybe in 10 years it'll be different, but for the foreseeable future I'm more than happy with normal rasterization.",Neutral
Intel,Who cares tho? Its not even the way it's meant to be ran. This tech is is a synergy. No point in trying to 'flex' native at this point. A 4090 gets very high FPS and amazing visuals when doing things right.,Negative
Intel,can wait in 2030 with the 8030RTX low profile 30 watt card run this game with full path tracing in 4k 120fps lol (in 2030 a low end card will run this like butter don't down vote me because you didn't get it lol),Positive
Intel,"Seems like this tech is only around to give Nvidia a win. Also path and ray tracing doesn’t always look better. If the one true God card can’t run it, is it reasonable tech or is it a gimmick? Especially seeing how like no one has a 4090. Like a very small percentage of PC gamers have one. Most aren’t even spending $1600 on their whole build!",Negative
Intel,What body part do you think will get me a 5090?,Neutral
Intel,"Once they introduced this dlss upscaling anti-lag all these gimmicks I don't think we've seen true 4K native and almost anything lately.  This is maybe one game that does look that good but many of the games don't look that great and have some really serious system requirements when there are games that look much better and are much better optimized.  Companies are taking shortcuts, instead of optimizing they give you suboptimal and use FSR dlss all these gimmicks.  Don't get me wrong I always thought it was great for people who wanted to reach a certain frames per second someone with a weaker system to allow them to play at least the minimal at a steady frame rate.  It just seems like this is the normal now non-optimized games.  The 4090 is a beast if you could only get about 20 frames native with Ray tracing it tells you everything you need to know about Ray tracing.  It needs to be reworked that needs to be a new system I believe one real engine has it built in in a certain way probably not as good as Ray tracing directly.  To me it seems like ray tracing is hampering many games that tried to add it, too much focus on that instead of actual making a good game.  Let's daisy chain 4 4090s ..gets 60 fps.. and then house goes on fire!  Obviously I'm being sarcastic but realistically it's actually pretty sad a 4090 20 frames per second.",Negative
Intel,"Its not really surprising, raytracing is decades away from being truly mainstream. I applaud the misguided pioneers who betatest the technology, their sacrifice will be honoured",Neutral
Intel,"bro, for who are they making such intensive/badly optimised games? for aliens with more powerful pcs? i trully dont get it.",Negative
Intel,That not even proper path tracing.,Negative
Intel,And I thought that ray tracing was a needlessly huge performance drop for a barely noticeable difference...,Negative
Intel,"Can't wait to buy a $1,600 GPU to run a game at 20fps!!!  &#x200B;  Nvidia fans: i cAnT wAiT 🤑🤑🤑",Positive
Intel,Native is a thing of the past when dlss produces better looking image,Neutral
Intel,Lmfao ... where are all those nvidia fan boys who prefer eye-candy path trace over 60fps?,Negative
Intel,Sounds like some efficient tech. Lmao. What's the fucking point if it makes the game unplayable? RT just washes out the image and looks like shit. Kills the contrast. Can't wait for this fad to end,Negative
Intel,In my opinion it's too early for Ray / path tracing it's computationally expensive and if you are using rasterization at ultra settings not only does look nearly identical but it's easier to do the work for the graphics and that gives you more fps...,Negative
Intel,"Rather than having 2-3fps, I would go and see a video of path tracing.",Neutral
Intel,"Does this fact really matter tho? Dlss in cyberpunk is known the look on par or even better than native, frame generation will further smooth it out. I can definitely see the 7900xtx being able to do path tracing at 1440p at least once amd release fsr 3 and better once they release something to now match ray reconstruction which also give a small boost in fps making path tracing even more doable.",Positive
Intel,cool idc i wont play that shit at such shit settings,Negative
Intel,Do we need this BS? Seriously? Gpu’s will end-up costing over 2 grand soon,Negative
Intel,I don't even get the hype around ray tracing most games don't have it and when they do it either makes the game look blurry or isn't noticeable unless you're in a weirdly lit area or looking at a reflection,Negative
Intel,"Am replaying it with a 6950xt and a 7700k(defo bottleneck).  All maxed out(no RT) at 1440p. If I wanted 4k I would have to enable FSR.  Its completely playable imo.   Mostly 50-60fps.  For me RT vs off vs path tracing.   The game doesn't look more real or better, it just looks different.  But ignorance is bliss.",Positive
Intel,This chart is bull. Since when is a RTX 3060 faster than a 3080. Someone needs to seriously question the source of the chart,Negative
Intel,Just goes to prove once again that Raytracing was pushed at least 4-5 gens too early as a pure marketing gimmick by Nvidia,Negative
Intel,"The Tech Power Up article says:   >Instead of the in-game benchmark we used our own test scene that's within the Phantom Liberty expansion area, to get a proper feel for the hardware requirements of the expansion  So it's very hard to reproduce their data at the moment, but I've run 3 tests with the built in benchmark (run to run variance below 1%, so 3 runs are enough) and my config is averaging 25.7 fps at 4K native with Path Tracing, so the title's premise is not correct. Still, the game is not playable without upscaling at 4K with Path Tracing, so the basis of the sentiment remains correct. Benchmark data available [here](https://capframex.com/api/SessionCollections/279f9216-bdb8-4986-a504-91363328adbe).",Neutral
Intel,RTX 4090 1600€ 19FPS 👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼,Positive
Intel,"Yay i will get 13,8 FPS with my 4080 👍😂 Someone want to borrow some frames? I don't need them all!",Positive
Intel,The first card on this list a normal person would consider buying is a 4070. 8.5 fps lol.   Even with upscaling that's basically unplayable.   Then you have all the UE5 and path trace type demos where a 4090 is only getting 60fps with upscaling.. at 1080p lol.   We are not remotely ready for these technologies yet. They're at least one if not two console generations away. So multiple years.,Negative
Intel,"Looks like 4K is mainly just for videos, not gaming for the time being.",Neutral
Intel,How can you bring out such technology and no hardware can process it properly.  an impudence,Negative
Intel,the question is who need path traycing? i mean that cool and beautiful but 20 fps with a 1600dollar price tag that wild.,Negative
Intel,You could just you know... disable path tracing and get 70 fps or go below 4k,Neutral
Intel,"That what I expect when a game gets built specifically to market Nvidia, but Nvidia over estimated their technology.",Neutral
Intel,"um...im running cp 2077 on a 4090 at 70-90fps, 5k monitor with path tracing/ray reconsturction on and most settings maxed out...",Neutral
Intel,Yo guys let's make this really unoptimized game then rely on hardware manufacturers to use AI to upscale the resolution and insert extra made-up frames to make it playable on a $1600 GPU lmao.,Negative
Intel,"so it is settled, Devs have zero optimization in games.",Negative
Intel,"Love how shit the 3080 was lol, even when it came out it was shit. But somehow everyone loved it",Negative
Intel,"Or, hear me out, developers could just do better. I've seen far better looking games without the constant frame hit.   Cyberpunk hasn't looked good enough to justify the frame rates from the get go.",Negative
Intel,Once again proving RT is useless,Negative
Intel,Cyberpunk w/ path tracing at max settings seems even more demanding than Crysis was at launch.   20fps with a 4090 is insane.,Negative
Intel,"""only gamers get this joke""",Negative
Intel,"People have forgotten or are too young to remember when Nvidia had hardware tessellation features when that was first emerging. They had tessellation maps under the ground and water in Crysis that were doing nothing other than tanking AMD (ATi) performance. I am not saying this whole Cyberpunk thing is exactly the same, but it's essentially similar. In all honesty, turning everything up to 11 on my 3090 doesn't net much visual gain beyond some mirror like reflections and nothing I really care about when actually playing the game (compared to standard High/Ultra) and not pixel peeping, which seems to be the new game now for people buying expensive GPUs, so they can justify their purchase. Meanwhile, everyone else just gets on with enjoying video games instead of living vicariously through someone's 4090 at LOL 1080p",Negative
Intel,Starfield is peaking around the corner.,Positive
Intel,starfield as well,Neutral
Intel,I remember when physx was so demanding people had a dedicated 2nd Nvidia graphics card just to turn the setting on in Arkham City. Now it's considered so cheap to calculate that we just do it on the CPU lmao,Neutral
Intel,The new Crysis,Neutral
Intel,"I wouldn’t be so optimistic. Transistor shrinking is crazy hard now, and TSMC is asking everyone to mortgage their house to afford it.",Negative
Intel,How long until a $200 card can do that?,Neutral
Intel,Most improvement is probably going to AI software more than hardware in the next few years.,Positive
Intel,"8800GT giving advice to 4090:  “I used to be 'with it. ' But then they changed what 'it' was. Now what I'm with isn't 'it' and what's 'it' seems weird and scary to me. It'll happen to you!""",Neutral
Intel,GPU need to have its own garage by then,Neutral
Intel,Yep! Insane how fast things change.,Neutral
Intel,"Most likely there will be no gpu that supports path tracing and gives you native 4k 120fps in 5 years, maybe even in 10 years.  The technology has slowed down a bit. It’s increasingly more challenging to make more dense chips.  That’s why Intel has been struggling for many years already and every iteration of their cpus gives only minor improvements. AMD went with chiplets but this approach has its own problems.  Nvidia stands out only because of AI. Raw performance increase is still not enough to play native 4k even without ray tracing.",Negative
Intel,Yeah rtx 12060 with 9.5 gb Vram will be a monster,Positive
Intel,I'm willing to bet hardware improvement will come to a halt before that.,Negative
Intel,"In 10 years AI based upscaling will be so good, no one will want to natively render unless they are generating training data",Neutral
Intel,Transistor density advancements have been declining for a good while now. We can't expect hardware performance gains of old to continue into the future,Negative
Intel,Like the 1080 barely doing 4k30 and now we have gpus that do 4k120 id way heavier games.  Its still weord to me to see 4k120,Negative
Intel,But then the current gen games of that era will run like this. The cycle continues,Neutral
Intel,TRUE! i think 5-10 years was the actual point in time where anybody should have paid their hard earned dollar for raytracing gpus. instead ppl dished out $1000s for the RTX2080/Ti and now are sitting on them waiting for raytracing to happen for them xD,Neutral
Intel,Who knows what new tech will be out in even 4 years lol,Neutral
Intel,"And that’s when I’ll turn on this setting in my games. Fuck having to pay $1600 for a sub 30fps experience. Path tracing is far from being a viable option, it’s more like a proof of concept at this point.",Negative
Intel,"You wish, GPU makers have stagnated like it's crazy, even the graphics have peaked. I expected to see CGI level graphics like Transformers now",Negative
Intel,"...but will cost your everlasting soul, body, firstborn and parents. If it's on sale.",Negative
Intel,It is path trancing though. The technology used by Pixar to make Toy Story 4 (though they spent up to 1200 CPU hours for one frame) Path tracing used to take up to a day per frame for films like the original Toy Story. And they had their supercomputers working on it. It is a miracle of modern technology that it even runs real time.,Positive
Intel,"You basically need a 4090 to crack 60 fps at 1440p w/ dlss on quality without frame gen. It looks good, but not good enough to run out and buy a 4090.",Negative
Intel,"Same card, I just turned off RT at 4K. 75-120fps is better than 40 with muddy but accurate reflections",Neutral
Intel,"The 7900xtx is one of the worst gpus iv ever had lol on paper it looks so so good, In games it just play isn’t. Fsr is garbage, 4080 mops the floor with it :( plus the drivers from amd besides starfield have been awful this generation",Negative
Intel,I think that most of the progress will go together with software tricks and upscalers.,Neutral
Intel,"lol. And you missed the 2080Ti.   Every result under 10 fps is just to be ignored, it isn't representing anything outside of ""woot, the card managed to chuck a frame our way"".",Negative
Intel,That's VRAM for you,Neutral
Intel,we are counting decimals of fps its just all margin of error.,Neutral
Intel,If you buy a card with low ram that card is for right now only lol,Neutral
Intel,Wake me up when a $250 GPU can run this at 1080p.,Neutral
Intel,Well DLSS isn't best. DLAA is,Negative
Intel,Reviews have been saying that the game looks better with DLSS than native. Not to mention runs extremely better.,Positive
Intel,Can’t you just disable TAA? Or do you just have to live with the ghosting?,Negative
Intel,TAA is garbage in everything. TAA and FSR can both get fucked,Negative
Intel,"Yeah. 70 to 100fps on a 4080, but with 3440x1440 and DLSS-quality-FG-RR (nvidia needs new nomenclature....)",Neutral
Intel,"same, high refreshrate at 4k with optimized settings + PT + FG. With a 4080 of course, it's insane that it can look and run this great.",Positive
Intel,"Not exactly. Path tracing is just ray tracing, but cranked up to 99% with the least amount of rasterization possible.",Neutral
Intel,"> Give it 2 gens and you are going to get 4k60 here.  Assuming the 5090 literally doubles a 4090 (unlikely), that only gets us to 4K 40hz.  Assuming a 6090 doubles that, 80. which won't be bad.  Going with more conservative 50% boosts. 5090 will give 30. 6090 will give 45.  And i feel like 50% is being very generous, as nvidia have claimed moores law is dead and they can't advance beyond a 4090 by much. I'd guess we get 30% uplift in 5090 and maybe 10-15% uplift in 6090. So we'd still be under 4K30.",Neutral
Intel,Imagine buying a 4090 and then using upscaling.,Neutral
Intel,Path tracing is so much more demanding than ray tracing due to light scattering being modelled. It is a marvel it even runs.,Positive
Intel,PC gaming is in Crysis.,Neutral
Intel,Well it's a good thing 99.999999% of PC gamers don't give a shit about ray or path tracing.,Negative
Intel,You're the first other person other than myself I've run into that had an XTX but ended up with a 4090 in the end.,Neutral
Intel,"Plus frame generation works very well in Cyberpunk in terms of image quality. In some games, you need to get closer to ~80 fps output for an acceptable image quality with FG. But the FG in CP2077 is decent with 60 fps output, ~~and I get ~65-70 fps output with quality DLSS + FG at 4k on a 4090.~~ EDIT: I misremembered what I was getting. With path tracing, DLSS quality, frame generation, and ray reconstruction, I got 80.1 fps with the benchmark!  Of course there's the matter of latency, and the latency of CP2077 with FG output of ~65-70 fps isn't great. So I'll often use DLSS balanced + FG. Thanks to ray reconstruction, this now looks very close enough native 4k (to my eyes), with acceptable latency (to me), at a high framerate output.",Positive
Intel,"Nvidia features are always useless until AMD copies them a year or two later, only then they become great features 😁",Positive
Intel,"Tbh, list the games that have ray tracing right now. Pretty few.  It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it). I would personally go amd over nvidia because those 50 or so euros more that nvidia has against the amd counter parts simply are too much for just better rt and  dlss. I could spend those for a better amd card with generally better performance.  Regarding dlss, personally I would just lower the graphics before resorting to kinda bad generated frames, fsr even worse. But that's my point of view.  I find that amd still has to fix some driver issues but other than that, they are a fine brand. (so is nvidia)",Neutral
Intel,Yeh because native 4k looks worse than dlss + RR 4k,Negative
Intel,"Yeah the game with DLSS, RR and path tracing at 1440p looks amazing and with very high fps",Positive
Intel,What's the point of having $5000 PC when you're still gonna have literally the same graphics as $1000 PC then?,Negative
Intel,"This is a tech demo. That's the whole point. It's not really playable yet, but the game really is meant to showcase what is possible in the future and how close we are getting. That's what Nvidia is doing here by funding this whole project.  Crysis who many people are comparing this to, was in itself quite revolutionary for its time. The destructible environment in Crysis to this day holds up, and that was it's killer feature really.  You're gonna have swings at the future that miss as well, and that's ok.",Positive
Intel,"Did you try DLSS?  Imagine getting down voted for asking if he was using DLSS. What a fragile community. Sometimes it's okay to use DLSS, other times you better not or people will come to your house with torches",Negative
Intel,VRAM,Neutral
Intel,"Depends on what you are after i guess, is it worth it for me? Absolutely, DLSS+FG+RR with PT is a great experience.",Positive
Intel,still better than 90% of games in 2023,Positive
Intel,Arguably better now Ray Reconstruction has been added. It's quite a big image quality upgrade.,Positive
Intel,"…no it does not, unfortunately. Have you seen the artifacting in Cyberpunk?",Negative
Intel,"Someone ate up the marketing. I often see people wonder why games are so unoptimized today, your answer is right here. \^",Negative
Intel,All graphics you see on your computer screen is fake,Negative
Intel,Path tracing is more demanding than Ray tracing,Negative
Intel,"That's why I downscale from 1440p to 1080p, running DLSS and using Contrast Limited Sharpening. It looks better than native resolution and still performs well.",Positive
Intel,I guess between the 3090 and the 4070,Neutral
Intel,Yep hahaha,Positive
Intel,RemindMe! 7 years,Neutral
Intel,"No, this tech is around so game developers can sell more copies of their games by making them look better. The only reason why Nvidia is winning is because their GPUs are much better at intensive ray-tracing.",Positive
Intel,"This is an absurd, fanboyish thought lmao",Negative
Intel,dont let novidia marketing see this youll get down voted into oblivion,Negative
Intel,I would still argue it's still in proof of concept stage. The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native. That will the allow the lower end GPU'S to get 4k 60 or 1440p 60 upscaled.,Neutral
Intel,I mean I have no problem playing at 30fps dlss balance at 1080p DF optimized settings on a 2080ti with pathtracing idc what anyone says RT/PT is the future it just looks so much better than Rasterization,Positive
Intel,"Possibly a little more. Assuming that the leaked 1.7x improvement is right and path tracing performance scales the same, that would be about 34 fps at 4k native. Might be possible to hit 60fps or close to it with DLSS Quality.",Neutral
Intel,It could as well do 60fps if Nvidia adds a lot of path tracing HW into the GPU.,Neutral
Intel,Fanboyism of both kinds is bad,Negative
Intel,"It won't end, but hopefully we see games that have art designed with RT lighting and reflections in mind.  Cyberpunk isn't it. We'd need a full rebuild of the game, every location, every asset and surface remade, so it looks like originally intended by the artists.",Neutral
Intel,Since it needs more than 10gb vram,Neutral
Intel,Cyberpunk is more optimized than majority of games that came out in last two years. Just because you can't run it at 4k 120 fps with path tracing doesn't mean it's not optimized.,Positive
Intel,It’s not unoptimized at all. Game runs great for the quality of the graphics. You guys just don’t understand the insane amount of power path tracing takes. The fact you can do it at all rn is insane but once again people just throw “unoptimized” at it because they don’t understand the technical marvel happening in front of their eyes.,Negative
Intel,"No, this game is actually very well optimized. It's just extremely demanding at 4k native, which is why DLSS exists.  It would be unoptimized if it ran like this while looking like nothing special, but this is probably the game with the best graphics, period. At the very least, it's certainly the game with the best lighting.",Positive
Intel,"and people will be like ""well RT is the future""  raster was fine for over 15 years and raster games look great but we get it we must let incompetent game devs and their bosses make shit quality games because average gamer is ready to spend $100 to pre-order electronic version of horse shit and then justify it for years because they can't admit they wasted money",Negative
Intel,It's settled that you have no idea what you talking about,Negative
Intel,Its full path tracing u cannot really optimize this much.,Negative
Intel,"It always did. Most games don't scale well, Cyberpunk does.  Look at the last two years of pc ports and you will understand how cyberpunk looks and performs like it's black magic.",Neutral
Intel,"Baked illumination requires you to choose where to add light sources and how the lighting is applied to everything, it's a creative choice of the author, the AI needs instructions to know what to do, so it would need constant handholding, on the other hand RT uses simple physics to calculate how light bounced and NN to apply the light on surfaces.",Neutral
Intel,But the destructable environment and the water graphics and other things in Direct X 9 made high end pcs kneel as well.  I remember playing on my 9800gtx/+ with my Intel Q9300 quad core (lapped and oc'ed to 3.0ghz - EDIT: checked some old evga posts got it to 3.33ghz) with 2x4gigs DDR2 @1000mhz cas 5 trying to maintain a sustained 30fps at 900p resolution on my 1680x1050 monitor. And I oc'ed the crap out of that 9800gtx 835 mhz (cant recall if it was unlinked shadder or not now) core on blower air (won the silicon lottery with that evga card).  Tweaking settings was mostly user done and guided with old school limited forum help. Ahh the good old days of having lots of time during school breaks.,Neutral
Intel,"The equivalent of a 4090 at the time, 8800 ultra (768MB) got 7.5 fps at 1920x1200 very high quality. The other cards were not able to run it. Even the lowest tier last generation card runs this, even though it's at 5% speed.",Neutral
Intel,No and no again lol  The 8800gtx could do 1152x864 high and that was the top dog. DX9 cards could do that at medium but high shaders was like a 25fps average tops and that dropped into the teens on the snow level.   It took things like the gtx480 and HD 6970 to do 1920x1080 30fps max settings. That's before getting into Sandy Bridge being the first cpu that could sustain 40+ fps if the gpu power was there.   Crysis came during the core 2 duo/k8 athlon and first wave dx10 cards era and it took *2 more generations* of *both* gpus and cpus to do it some justice.,Neutral
Intel,"My 8800ultra was getting single digit fps at 1080p ultra, so CP isn't quite as demanding.",Neutral
Intel,Amd had a tessellation unit. It went unused but it was present,Neutral
Intel,"My fun story for PhysX was Mirrors Edge. I don't remember what GPU I had at the time but the game was pretty new when I played it. Ran fine for quite some time until one scene where you get ambushed in a building by the cops and they shoot at the glass. The shattering glass with PhysX turned on absolutely TANKED my framerates, like single digit. I didn't realize that the PhysX toggle was turned on in the settings. This was at a time when PhysX required a dedicated PCIe card in your system.   Once I turned it off it ran fine. Now I can run that game at ridiculous framerates without my system getting warm.",Positive
Intel,"I remember around about 2008 when companies like Asus and Dell were selling ""Physics Processing Units"" and some claimed that these would be commonplace in gaming machines just like Graphics cards had become 10 years previously.",Neutral
Intel,"Hardware accelerated physics (as in on the gpu), is different than what's going on the CPU.",Neutral
Intel,hobbies quaint consist aromatic political hat aback boat relieved crush   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"People used to have ATI/AMD for main and a lower-end NVIDIA for PhysX.  When NVIDIA found this out they pushed out drivers that disabled PhysX on their cards if an ATI/AMD card was detected, limiting you to the intentionally piss-poor CPU implementation of PhysX.  Think about that crap.  One day everything's going fine for consumers, the next day NVIDIA decides they don't like how consumers are legitimately using their cards and gimps everyone, weaponizing a physics engine company that they bought in 2008.",Negative
Intel,"Oh damn, I forgot about those cards.  I wanted one so badly.",Positive
Intel,Remember when companies tried to sell physics cards lol,Neutral
Intel,>PhysX  It never was a dedicated Nvidia card - it was a [dedicated psysx](https://www.techpowerup.com/review/bfg-ageia-physx-card/) on which the tech later was bought by Nvidia and implemented in it's own GPU's.  But the things never became really populair.,Neutral
Intel,"This makes me wonder if we'll ever see something similar with Raytracing, where we get a 2nd GPU purely for RT, and then the main GPU just does all the other stuff. Would that even be possible?",Neutral
Intel,I remember when people had a dedicated PhysX card.,Neutral
Intel,The ray/path tracing in this case done by specialized hardware which has more room to grow faster.,Neutral
Intel,"I would, transistor shrinking isn't the only method of increasing performance, and honestly, these companies have to keep putting out better cards to make money.  There have been many breakthroughs over the last few years. I give it another 5 as both amd and nvidia are pushing ai accelerated ray tracing on their cards, nvidia is in the lead for now but amd will eventually catch up.",Positive
Intel,There is still a big leap possible since all lithography processes at the moment are hybrid euv and duv.   But the moment everything is done euv things will drastically slow down.,Neutral
Intel,"No those are just pennies in the pocket of Nvidia, but as the consequence you as the customer need to take a mortgage on a brand new GPU.",Neutral
Intel,"Yeah the next 10 years are gonna be spent getting budget cards up to 4090 level performance, rather than bringing the bleeding edge to 5-10x the 4090 performance. As long as 4K is king, there’s not much incentive to do the latter.",Negative
Intel,"In 10 years we will be begging one of several thousand test-tube created Musk Family members for $200 so we can buy a cheeseburger.  But the jokes on them, we’re just gonna spend it on space crack.",Negative
Intel,"Never. Even if performance can be pushed that far, by the time it happens there won't be such a thing as a $200 graphics card anymore.",Negative
Intel,It's not happening unless the market crashes and they start focusing on offering good price/performance cards instead of bumping up prices every generation.,Negative
Intel,25 years,Neutral
Intel,"The software needs to run on hardware, right now it eats through GPU compute and memory.",Neutral
Intel,And sadly ended up with 450 Watt TDP to achieve that performance.,Negative
Intel,This. We'd be lucky to see more than 3 generations in the upcoming decade.,Positive
Intel,"Having seen a few newer games on relatively low resolution CRT display, I can't help but think it might come down to improved display tech and embedded scaling. Like DLSS3 features in the display instead of the GPU.",Neutral
Intel,Not with that attitude,Negative
Intel,Love how it's still gimped on memory size 😂,Positive
Intel,"They just need to render the minimum information needed , and let the ai do the rest",Neutral
Intel,"Don't get upscaling, to me it looks shite. Why spend hundreds or thousands on a GPU to not even run at native resolution.   It's something Nvidia are pushing because they've got a better version of it at the moment.  Maybe it'll improve, it's pretty much snake oil currently imo.",Negative
Intel,"Feels pretty good with 120 frames. Playing with a controller, i don‘t mind or even feel the input lag on a projector screen.",Positive
Intel,"It's mad that even with a 4090 running games at 4k, they still look less realistic than the low-res FMV sequences in The 7th Guest from 30 years ago.",Negative
Intel,"Do keep in mind that despite both being named the same, the devil's in the details. Movies use way more rays per scene and way more bounces too.   Path tracing in CP2077 shows temporal artifacts due to denoising, something that doesn't happen in movies. It is being improved with the likes of DLSS 3.5, but it is still quite off when compared to said movies.",Neutral
Intel,"Keep in mind the systems that rendered toy story 1, costing upwards of like 300k IIRC, have less power than your cell phone today and were super delicate/finicky machines. There's a dude on youtube that got a hold of like the second best one that was made at the time and the machine honestly was really impressive for when it came out, but it pales in comparison to even a steam deck really.",Neutral
Intel,"> Path tracing used to take up to a day per frame for films like the original Toy Story  Toy Story didn't use path tracing though, A Bug's Life was Pixar's first movie to use ray tracing (not path tracing) and only for a few frames in the entire movie for specific reflections, they started using ray tracing more generally for Cars and I can't find exactly when they started using path tracing but it should be around the early 2010s which is also when the other Disney animation studios started using path tracing",Neutral
Intel,No it's not a miracle. Because it's downscaled a lot and also uses lower quality math and piggybacking on AI to make up for it. It's basically running through 3 layers of cheats to produce results that aren't that much better than just traditional raster.,Negative
Intel,"That’d be some next level consumerism, paying 1500$ minimum to turn on a single setting in a single game just to play it wayyyyy slower than you would otherwise",Negative
Intel,"Hell playing path tracing with my 2080ti at 25 FPS is still looks absolutely fantastic, I would absolutely play with pathtracing on a 4090 constantly. Idc dlss looks great with RR even at 1080p. I won’t upgrade for another year or 2 (just bought a phone so I’m broke right now)",Positive
Intel,4090 getting 60fps at 4k balanced dlss with no frame gen. 100 with frame gen. I can make it drop into the 20s if I stand right next to a fire with all the smoke and lightening effects or if I go to a heavily vegetated area it’ll drop to mid 40s. But it’s stay consistently at 55-65 and even goes Into the 90s if I head out of the city.   Haven’t tried it at quality or with dlss off though. May go do that now that it says only 19fps lol. Have to try it to see for myself,Neutral
Intel,"https://www.tomshardware.com/news/nvidia-dlss-35-tested-ai-powered-graphics-leaves-competitors-behind  Well it's so close (54fps) it's more like a 3080Ti or higher from 3000 series or a 4070TI + from 4000 series it seems? The old 3000 series is punching way above it weight vs the 7900xtx, which was meant to deliver similar RT performance to the 3090.. which it doesn't.",Neutral
Intel,At some point the RT pixels are so expensive that native resolution w/o DLSS and frame gen is just not gonna work for the time being.,Negative
Intel,"On the other hand, if you set the new DLSS 3.5 to performance (which you should in 4k), and just enable frame generation, you get 90+ in 4k with basically zero issues unless you pause to check frames.",Positive
Intel,So rendering at 960p? Oof...,Negative
Intel,"you take that back, I literally returned my 4080FE for a 4090 liquid x for more frames in cyberpunk.",Neutral
Intel,I almost see no difference.   And to see the little diffidence I need to take two screenshots and compare them.,Neutral
Intel,The game look is ruined on PT it makes everything look completely different.  Regular RT keeps the style of the game and performs good.,Negative
Intel,Dont use useless raytracing and you wont have any problems lol,Negative
Intel,"Because PC gaming blew up during a time where you could buy a mid range GPU and not need to upgrade for 5-6 years. Now those sample people buy a GPU, and 2 years later it can't run new games. At least that's my theory.",Negative
Intel,AW2 looks insane. Can't wait to play soon,Positive
Intel,Well the 3050 managed to hold with just 8GB while the 3070Yi crashed?,Neutral
Intel,"Yeah, and people insisted on defending the configurations at launch lmao. The cards just won't be able to handle heavy loads at high resolution such as this game, regardless of how fucking insane the actual processing unit is. You can't beat caching and prepping the data near the oven. Can't cook a thousand buns in an industrial oven at the same time if there's trucks with 100 coming only once an hour.",Negative
Intel,My 3080 in shambles,Negative
Intel,"It looks better than native even with fsr quality, the Taa in this game is shit",Negative
Intel,You dont need to increase raw performance. You need to increase RT performance.,Neutral
Intel,Imagine!,Positive
Intel,DLSS unironically looks better than native TAA even at 1080p in this game because of the shit ghosting.,Negative
Intel,Only people who don't give a shit about high quality graphics don't care about ray tracing.,Negative
Intel,I got a stupidly good deal on the 4090(buddy won an extra one from a company raffle) and it just so happened my XTX was one of the cards suffering from the mounting issue on release.  Honestly very happy that all of that happened. Sure I would have been happy with the 7900xtx if I got a replacement but the 4090 just kinda blew me away especially with all the DLSS tech behind it.,Positive
Intel,"Nvidia used cp77 as a testing grounds for their dlss3 and rt tech, it'd no surprise it looks and performs relatively good, they literally partnered with cdpr for that.  Do not expect these levels of RT on most games anytime soon, probably in a couple years (with rtx 5000 series) the tech will be more mature and not as taxing for most cards, because needing to upscale and use framegen to get 60fps on an RT PT setting is kinda absurd.",Positive
Intel,">only then they become great features  Watch this happen with ""fake frames"" FSR3 in real time",Neutral
Intel,>Ray tracing as of right now is a gimmick   This line was already false years ago when control launched. Now it’s just absurd,Negative
Intel,"There are a lot of new games that features Ray Tracing on them as of now, too bad most of them doesn't look as visually as good as they could be though as most of the time their RT effects are dumbed down or reduced to the point it's pointless.  But the thing is that wasn't the point even from the beginning, RT Overdrive on Cyberpunk is literally considered as a Technological showcase  What matters here IMO for all of us is PC platform in general is showcasing here what a real high-end computers can achieve beyond what current gen console can do, and they are showcasing it really well here on Cyberpunk with much better visuals and using plenty of tricks to make them more than playable, where they shouldn't be considering how much demanding they are over standard, that is impressive enough to me and shows a good sign of technological engineering innovation.   As what Bryan Catanzaro said on his interview on DF Direct, work smarter not harder, brute force is not everything, of course it still matters a lot but doing it alone is starting to become pointless just like how it is on real life.",Negative
Intel,Much more fps :) Newer generation cards have at least 50-60% better raster performance then prev gen.,Positive
Intel,The Evangelical Church of Native,Neutral
Intel,"So just to be clear, you don't like running games at native resolution? Because the purpose of DLSS is to improve performance by specifically not running at native resolution",Negative
Intel,Nice but ive been playing video games since the 70s and its Shit end off..,Negative
Intel,The future of frame generation is going to be huge. I can definitely see 360p frame rendering upscaled to 4k with 80% maybe even 90% of the frames being generated. All with equal to or even better visual quality.   A 20 FPS game like this will become a 100-200fps game.  We may be looking at the end of beefy graphics cards being required to play games. Just a good enough mid or low tier chip (granted 5-10 years from now) with frame generation might be enough.,Positive
Intel,"After 2.0, it's around 100""fps"". Just 60""fps"" (meaning with FG enabled) is far from being playable due to latency. You need at least 90""fps"" for playable experience.   Cyberpunk PT vs RT is a huge difference in terms of graphics, while DLSS Quality vs Balanced is really slim in comparison. What you say would be a really bad trade-off.   At 1440p DLSS Quality it's easily 120+""fps"" which is perfectly comfortable.",Neutral
Intel,Cyberpunk is greatly optimized. It looks fantastic for the hardware it asks. Starfield looks like game from 5 years that asks hardware from the future.,Positive
Intel,"Yeah, sure, now go back to play starfield.",Neutral
Intel,"There literally is a /s, what else do you need to detect sarcasm?",Negative
Intel,"Someone made a really cool comment yesterdays on Reddit that even a generated frame using path tracing is less fake than a rasterized frame. It is effectively closer to reference truth. I had never thought about it that way, but people really are clutching their pearls with this shit",Positive
Intel,"I know, which makes path tracing even worse off imo.",Negative
Intel,I will be messaging you in 7 years on [**2030-09-23 15:26:41 UTC**](http://www.wolframalpha.com/input/?i=2030-09-23%2015:26:41%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Amd/comments/16pm9l9/no_gpu_can_get_20fps_in_path_traced_cyberpunk_4k/k1v42p3/?context=3)  [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FAmd%2Fcomments%2F16pm9l9%2Fno_gpu_can_get_20fps_in_path_traced_cyberpunk_4k%2Fk1v42p3%2F%5D%0A%0ARemindMe%21%202030-09-23%2015%3A26%3A41%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%2016pm9l9)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,Neutral
Intel,haha this is gold 🥇,Positive
Intel,"At the moment, it’s definitely what it seems like.  Especially seeing how there’s only one card on the planet that can run this. Do you have a 4090?",Neutral
Intel,">I would still argue it's still in proof of concept stage.  Not at all, in fact another AAA game with ""path tracing"" is coming out next month.  >The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native  Forget ""native"", that ship has already sailed. Nvidia, AMD and Intel are all working on machine learning techniques, and several game developers are starting to optimize their games around DLSS/FSR.",Neutral
Intel,"Nope, not even close.  Shaders 3.0 - that's improvement. Ray tracing is noticeable, but nowhere near to justify the hit.  I've looked techpowerup comparison RT vs LOW (lol). Low looked far better and cleaner, then blurry mess of RT (Overdrive).   I don't need ""realistic"" lighting, I need pleasent bisual part with good gameplay. For ghraphics I have a window in my appartment.",Neutral
Intel,"In a screenshot, sure. When actually playing the game, unless the fps is low enough to remind them, 99,99% of people will forget if they left it on or not.",Negative
Intel,Pixar movies will sometimes take the better part of a day to render a single 4k frame on a render farm. So the fact that Cyberpunk takes more than 0.05 seconds to render a 4k frame in its path-tracing mode on a single 4090 clearly means that it's unoptimized garbage! /s  EDIT: Apparently people in this thread don't understand sarcasm (or don't understand how a path-traced game can take longer than 0.05 seconds to render a 4k frame with high-poly path tracing and still be optimized).,Negative
Intel,Man you had to lap your q9300 to hit 3.0ghz? Those chips really weren't OC friendly.  My q6600 hummed along 3.2 no problem on 92mm arctic freezer and a slight voltage bump.  I was so pumped when I upgraded to a gtx 260. Crysis was a dream.,Positive
Intel,Funnily Crysis still have better destructible environments than Cyberpunk has tho,Positive
Intel,"This. People losing their shit either weren't around when Crysis launched or have forgotten just how demanding it was . PT CP kicks the shit out of a 4090, Crysis murdered my 8800ultra.",Negative
Intel,crysis did not do 1080p 60 on maximum setting and dx10 on a 8800GT.   I had a 1280*1024 monitor back then and barely had 30 fps with my 8800GT.   Even the 8800 Ultra did not do 60 fps on full hd.  https://m.youtube.com/watch?v=46j6fDkMq9I,Negative
Intel,"Man, I remember how hype the 8800gt was. Thing was a hell of a big jump compared to previous cards that came out, prolly our first REALLY powerful card.  Still remember the old xplay video where they build a PC to play crysis and Bioshock. Put 2 8800GTs in sli in there with a core 2 quad which costed one thousand dollars back then!",Positive
Intel,"My poor HD 4850 got pushed to the limit to give me 1280x1024, lol",Negative
Intel,I remember thinking $300-400 (IIRC) was so much for a GPU back around those days.  7800XT is the closest we've seen in a while and not even close to the top end for today.,Negative
Intel,This is still the case to this day because the game includes a really old DLL file for PhysX. The other day I followed the instructions on the PCGamingWiki to delete some DLL files in the game directory and only then it ran perfectly smooth on my RTX 3070.,Neutral
Intel,LOL I remember this exact scene also.,Positive
Intel,"CPU PhysX back then was single-threaded and relied on ancient x87 instructions if I recall correctly, basically gimped on purpose. Even with a 5800X3D the shattering glass reduces the frame rate to the low teens. Sadly an Nvidia GPU is still required if you want to turn PhysX effects on for games from that era, though I hear that it's possible again to use it with an AMD GPU as primary.",Negative
Intel,"OMG I had the exact same experience, hahaha I remember it vividly, I was so confused why that room would just destroy my FPS until I figured out PhysX was enabled LOL",Negative
Intel,"I'd like to see ray tracing addon cards, seems logical to me.",Neutral
Intel,"TBH, I could probably run some of the old games I have on CPU without the GPU.",Neutral
Intel,glorious squash wild file crawl ancient crowd racial soft north   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,You could probably run Mirror's edge with physx on today's hardware without gpu fans turning on,Neutral
Intel,Last time I tried on an R7 1700X and RX580 I still couldn't turn on PhysiX without it being a stutter party 2 fps game.,Negative
Intel,What gpu you have,Neutral
Intel,Now it even runs fine on a Ryzen 2400G.,Positive
Intel,"And they were right, PhysX and systems very much like it are still used but things have advanced so much nobody even thinks about it and it no longer requires dedicated silicon.",Neutral
Intel,"Yeah, it’s been common knowledge for many years now that Nvidia are the most ruthlessly anti-consumer company in PC hardware, and it’s not particularly close.",Negative
Intel,Ah good old Ageia before nvidia bought them out https://en.wikipedia.org/wiki/Ageia,Neutral
Intel,"No, you could actually install two Nvidia cards and dedicate one of them to only PhysX.",Neutral
Intel,"It would certainly be possible, but it wouldn't really make sense. Splitting it up on multiple GPUs would have a lot of the same problems that sli/crossfire had. You would have to duplicate memory, effort, and increase latency when you composite the final image.  It may or may not make sense to maybe have a raster chiplet and a ray tracing chiplet on the same processor package on a single gpu. But, probably makes more sense to have it all on one chiplet, and just use many of the same chiplet for product segmentation purposes instead.   A separate PPU did make sense tho, I'm still annoyed that the nvidia ageia deal essentially kill the PPU in the cradle. Our gaming rigs would cost more if PPUs became a thing, but we could have had a lot better physics then we do today. There is still a revolution in physics to be had some day...",Negative
Intel,would be cool if 2000/3000 series users could get a small tensor core only PCIe card to upgrade to framegen,Positive
Intel,"That's true, but if he's talking about *the equivalent* of a current $200-class card, I'd say about it's 10 years, what do you think?",Neutral
Intel,At least with the 4090 you can run 70% power target and still hit insane FPS while only pulling around 300w which is the same as my old 3080. The gains with that extra 150w are a couple percent at best. Not worth it to me.,Negative
Intel,Intel design will be a little bit different as far as now.  If I understood it right AMD chiplets communicate via lines on pcb but Intel wants to make something like chip-on-a-chip.,Neutral
Intel,bedroom ring library cows summer thought aspiring worm north joke   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"It's also worth noting that those early movies don't use path tracing either, Pixar switched to PT with Monster University around 2013 IIRC.",Neutral
Intel,"There is a lot of room for improvement, both in the software and future generations of hardware.  It's coming along though!  Overdrive mode looks nice, but there's just a lot more ghosting than the regular RT mode.",Positive
Intel,"They are still the same thing and should be named the same, your disclaimer is just semantics about the implementation.",Neutral
Intel,"Weird argument.  Traditional raster is cheats upon cheats upon cheats already.  In fact, all of the lighting entirely in raster is essentially cheats.",Negative
Intel,It will not work well/look good using a single frame worth of data either due to the low ray counts. Digital Foundry has shown Metro Exodus builds the lighting over ~20 frames or so and it seems like Cyberpunk is even more. Even Cyberpunk ray pathtracing doesn't look as good in motion due to that same build-up effect.  You need to cast 20-50x the rays to have enough data for a single frame but then you will be measuring the game performance by seconds per frame.,Negative
Intel,"RT looks significantly better than raster when it’s not just using simple reflections like a Far Cry 6. Raster is all cheats, the so called reflections are essentially rendering the scene or part of the scene being reflected in reverse then applying fake material effects to mimic metal, water, etc. Raster also takes HUNDREDS to thousands of person hours to get the fake reflections and lighting to look decent to good. That’s why devs love RT and want it to grow, as RT/hardware does the work for them.",Negative
Intel,It's significantly better than raster. It kills fps but the quality is great,Positive
Intel,Better graphics needing more expensive hardware is hardly a hot take.,Neutral
Intel,"Yes, better graphics costs performance. SHOCKING",Negative
Intel,People do it!,Neutral
Intel,It’s not way slower. I get 110 FPS at 4k with all DLSS settings turned on and honestly it’s insane.,Positive
Intel,"I shamefully admit I bought a (inflation adjusted) $400 console in the past to play one game. That's marginally better than buying a GPU for one setting in one game, but still.",Negative
Intel,Sounds like most nvidia fanboys,Neutral
Intel,"If you're into the eye candy, I can see a lot of people that have the money doing it.  Not everyone though. That's around a mortgage payment for me.",Neutral
Intel,And yet people will tell you how nvidia is mandatory. Like you want to overspend to play 1 game with shitty fps? I don't understand these people.,Negative
Intel,Nvidia fanboys being happy with fake frames and fake resolution will never not be funny to me.,Negative
Intel,What? I thought amd was always more tuned to raster as opposed to reflections,Neutral
Intel,Which is why nvidia is rabidly chasing AI hacks,Negative
Intel,This and rumors are saying that the 5090 is gonna be 50-70% faster than 4090 which wont be enough for native 4k either.,Negative
Intel,Say that to the people playing upscaled games at 4k (540p) on PS5.,Neutral
Intel,"I understand some RT effects might not catch everyones eyes, but seriously, path tracing cyberpunk vs 0 RT cyberpunk IS a big difference",Neutral
Intel,Huh? [Watch this is in 4K on a big screen.](https://www.youtube.com/watch?v=O7_eHxfBsHQ&lc=UgyO2vRiSI6CRU5kYmV4AaABAg.9uyI5P8oamz9uylTN_rdmL) The differences are really apparent in each version.,Neutral
Intel,"PT looks way better in the game to me - it makes everything in the environment have a more natural look to it.  It adds, not ruins it...",Positive
Intel,"That's how I feel as well. It looks awesome don't get me wrong, but it doesn't look like cyberpunk to me.",Negative
Intel,It's not a perfect way of measure but you can clearly see how (at least on Nvidia GPUs) the 8/10GB cards are way behind the >11Gb cards. Meaning you need 11 or 12GB of VRAM for this scenario which cripples the 3080 but not the 3060.  We said from the start these configurations are shit but no-one listened. There you go.,Negative
Intel,The crypto miners did me a comical solid by preventing me from acquiring many countless times and hours I wasted (came from a gtx1070 before finally being able to upgrade). Was able to get my 6900xt eventually for around $600 with 2 games. Becoming increasing thankful for the extra Vram these days.   Once I start getting through my game backlog and into the gard hitting ray tracing ones will hopefully upgrade to something with at least 24gigs of GDDR# lol.,Positive
Intel,"I mean, Alan Wake 2 is also releasing with path tracing so my guess is we're definitely gonna see more games featuring heavier RT, although not necessarily path tracing due to the performance hit.",Neutral
Intel,"I expect path tracing support to be the exception for a while because most developers will consider the non-trivial dev time to implement it not worth it. However, I'm sure Nvidia would be willing to throw dev help at most developers who would be willing to implement path tracing with their help.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode. Presumably, a less complex world helps, but they may also have implemented opacity micro maps, which I'm not sure if Cyberpunk has done in the 2.0 update. Perhaps they're cherry-picked examples for the trailer.",Neutral
Intel,Raytracing now is a gimmick *  ( * - in AMD sponsored games where RT effects are so tiny you have to zoom on to notice them),Negative
Intel,"Except this is a tech demo. CP77 has lots of cut corners to achieve that, NN was trained to keep the interface stable in that game and even then, ray reconstruction creates horrible artifacts at some angles and distances. And that's with Ngreedia's engineeers literally sitting at CDPR's office.",Negative
Intel,You can already have 120fps on $1000 PC,Neutral
Intel,It improves both looks and fps so it is a win win,Positive
Intel,"The issue with this is the inputs will only be registered at 20Hz. This would feel awful. The benefit of high FPS is having very low latency. This will just make it look smoother, not feel any better than 20 FPS",Negative
Intel,Considering there is no input on the generated frames that is going to feel horrible to play. Base FPS (incl DLSS) really still needs to be 50-60fps to feel playable imo.,Negative
Intel,I'm thinking will there ever come a day where 100% of frames are generated by AI. Like the data inputs directly from the card into the neural network and it no longer requires physical hardware limitation to generate frames.,Neutral
Intel,"Don't forget that all NPCs will be controled by AI to simulate a real life.  They'll wake up in the morning, shower, commute to work, work, then go home.  All this to do it over and over and over until they die...  mhhh... this is kinda sad to type out.![gif](emote|free_emotes_pack|cry)",Negative
Intel,It absolutely is. I’ve been playing at 1440p with DLSS Balanced and Ray Reconstruction and it is absolutely the worst version of DLSS I’ve ever seen. It’s basically unusable.,Negative
Intel,There are horrible artifacts with RR. And ghosting. It replaces artifacts with different ones. Needs a lot more work tbh. Or training.,Negative
Intel,"The thing about ray and path tracing is that they are demanding on their own, not because they are badly optimised",Negative
Intel,"I have a 4070. It runs the game just fine with the tools that have been set in place.  This is absolutely not a pissing match. Cyberpunk is demanding, and takes advantage of lots of tech available for upscaling and ray tracing and frame generation. AMD card’s currently do not match Nvidia’s in those categories, not even close. This us not an attempt to showcase Nvidia tech, but it does take advantage of it.",Positive
Intel,Because it's not about the flagship card. Only if the flagship can attain good performance at native then you will have headroom for lower end cards. Think about it. Of course upscaling is very important. But you also need some headroom to upscale.,Neutral
Intel,"Yeah. I wanted better cooling performance after upgrading to the artic freezer 7 as well with those weird crapy plastic tension clips you pushed in manually, fearing bending them (worse than the stock intel cooler tension twist lock). Kept having random stability crashes until after i sanded it down for better thermals...Good old sandpaper and nerves of steel fearing an uneven removal of the nickel coating.  Was trying to maximize performance as back then core 2 duo and the extreme versions were king and they had way higher single core clocks and were easy to oc. Wanted the multhreaded for Crysis, LoL (back when you had to wait 5min+ to get into a game), BF2, and was eventually playing TF, Day of Defeat and Natural Selection.  My upgrade back then was to the Evga 560ti DS, which I ended up installing a backplate and  a gelid cooler. They had wayy better thermal tape/individual heatsinks for the memory/vram chips for the heat transfer. Evga back then told me if I ever needed to RMA it that I would just need to re-assemble it as it was shipped. Remember using artic solver ceramique instead of as5 due to it potentially not degrading as fast as well.   Good times =)",Negative
Intel,"Yea same here, didn't play crysis until I got a hold of a 260 after having 2 8600gt's in sli. Played mostly wow at the time and those 2 8600gt's in sli got blown away by the gtx 260, but man that card was hot!  Remember in 2011 upgrading to a gtx 570, the first gen with tessellation and playing dragon age 2 which was one of the first games to have it. Ended up turning off tessellation cause it hit the card too hard, least until I got a second 570 in sli, which sadly died like a year later due to heat while playing Shadow Warrior.",Positive
Intel,Q6600 was the bomb. I ran mine at 3.0GHz all its life and it's still alive today. Had it paired with an 8800GT which was IMO one of the best GPU value/performance of all time.,Positive
Intel,Had 3.4ghz on my q6600 didnt want to hit 3.6ghz on modest voltage but 3.4 all day and was quite the lift in fps in gta4 compared to stock 2.4ghz. Run 60fps 1440x900 no problem those where the days when OC gave actual performance boost,Neutral
Intel,> I was so pumped when I upgraded to a gtx 260. Crysis was a dream.  In those days I was a broke teenager and I remember upgrading to a GTS250(rebadged 9800GTX+ with more vram) and crysis still hammered my Athlon x4 965(?) system.  While my richer neighborhood buddy upgraded to a 260 just to play Left 4 dead.,Positive
Intel,"After checking sone old Evga posts, (mod rigs server is done, along with old signatures), I was able to get to 3.33ghz.",Neutral
Intel,"Yeah my lapped Q6600 was my daily driver at 3.2 GHz with a Tuniq Tower 120 Extreme. It ran it pretty cool. I think when Crysis came out it and my 8800 GTX could do 20-25 frames at 1680x1050 with drops into the teens. EVGA replaced it with a GTX 460 when it died, big upgrade.",Positive
Intel,moving data between the two is the issue,Neutral
Intel,grey cagey sharp detail handle north grandiose connect sparkle hungry   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"I wonder the same thing. But I guess if Nvidia would put it all in an extra card, people would just buy more amd to get the best of both worlds.",Neutral
Intel,Game physics doesn't seem to be a focus anymore though,Neutral
Intel,I'm ashamed I even own one NVIDIA card.    Unsurprisingly it's in a laptop that I bought during the [illegal GeForce Partner Program](https://hothardware.com/news/nvidia-geforce-partner-program) before the public knew about the program.  Not making that mistake again.  Screw NVIDIA.  Every other card I've ever gotten has been ATI/AMD (and obviously the low-end integrated Intel GPUs on some office laptops)  EDIT: Keep them downvotes coming.  You've all been bullshitted by NVIDIA's marketing.  Your boos mean nothing; I've seen what makes you cheer.,Negative
Intel,Anyone who owns an Nvidia GPU should be ashamed of themselves tbh.,Negative
Intel,"There was more nuance to that ""bugfix"" than just ""it was a bug that got fixed""   Modders were able to re-enable the AMD/NVIDIA PhysX combo basically immediately via editing DLLs.  Not driver replacements, but actual DLL tweaks.  The speed at which modders were able to fix what NVIDIA ""broke"" combined with the public outrage put them in a funny legal spot.  If they continued with PhysX disabled and claiming it was a bug then the bug can easily be fixed, as shown by modders doing it first.  So either fix it and get everyone back to normal, or leave PhysX disabled even though it can be easily enabled and open the company up to potential anti-competitive lawsuits.  Obviously not many modern games use the GPU implementation of PhysX anymore and the performance difference between current GPU and CPU implementations are negligible anyway, but their intent at the time was clear once it was shown how quickly modders were able to get everyone back to normal.",Neutral
Intel,"Yes, but not after Nvidia bought Ageia.",Neutral
Intel,"If the $200 cards of today aren't 3x better than the flagship cards of 10 years ago (they're not), then you shouldn't expect a $200 card in 2033 to be 3x faster than an RTX 4090. Average annual performance gains are more likely to decrease between now and then, rather than increase.",Negative
Intel,I like how you said: static image  because in motion upscaling is crappier,Positive
Intel,"It isn't a weird argument. Rasterization is premised on approximating reality, while RT is simulating it.  The extreme few amount of rays we trace right now, including bounces, means effects are often muted and the performance hit is enormous. To compensate, we're using temporal upscaling from super low resolutions *and* frame interpolation.  Temporal upscaling isn't without it's issues, and you know how much traditional denoisers leave to be desired. Even Nvidia's Ray Reconstruction leaves a lot to be desired; the thread on r/Nvidia shows as much, with ghosting, artifacts, etc. all over again. It's like the launch of DLSS 2.0.  All of that, for effects that are oftentimes difficult to perceive, and for utterly devastating traditional rasterization performance.",Negative
Intel,"It's a shame raster is so labor intensive, because it looks so interesting when done right.     I look at screenshots from cyberpunk, Raster / Ray Trace / Path Trace, and in most of them the raster just looks more interesting to me. Not constrained by what would be real physical lighting. The 100% RT render versions of old games like Quake gives a similar feeling.     But I imagine there will be more stylistic RT lighting over time, it saving a ton of labor is all things considered good, freeing it up for actual game design.",Positive
Intel,">That’s why devs love RT and want it to grow, as RT/hardware does the work for them.  It's the marketing departments that love RT. Devs hate it because it's just yet more work on top of the existing raster work.",Positive
Intel,"It *is* way slower, you're just compensating it by reducing render resolution a ton",Negative
Intel,If it was bloodborne i am guilty of that myself,Negative
Intel,price zealous rinse detail yam rainstorm groovy automatic snails fact   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,4k max without dlss or fsr or frame gen I still get 30fps. Sorry you can’t even hold 10. Guess I’ll shill some more to at least not be spoon fed shit from amd,Negative
Intel,"Ray tracing and path tracing aren't just ""reflections"" , but this thread is specifically about the new path tracing modes in CP2077.",Neutral
Intel,Rasterisation is a “hack” too,Negative
Intel,If it works it works....computer graphics has always been about approximation,Positive
Intel,"4090 is \~66% more powerful than 3090 overall, but it has 112% more fps in this test.     The RT capabilities of NVIDIA cards has grown faster than general capabilities.",Positive
Intel,"I guess on the internet, you can just lie. There’s no game on ps5 that upscale from 540p to 4k.",Negative
Intel,"I guess I was wrong on that one, although SWJS upscales from a resolution as low as 648p, which is not much.",Negative
Intel,It's big in certain lighting situations. In others it's not as noticeable as you'd hope,Neutral
Intel,"PT vs no RT comp is silly.  U should compare PT vs RT, and there, the difference is minor and not even subjectively better. Just different.",Negative
Intel,Natural? It over saturates the light bleeding into the entire scene. Everything becomes so colorful. Even when floors or ceilings are not supposed to be reflective. Literally tiles with grainy texture suddenly becomes a mirror..  It's ridiculously overdone.,Negative
Intel,Guys I'm trying to suffer while I play the game what settings will let me suffer the most,Negative
Intel,"I mean this is native RT/TAA. Was never meant to be played this way. You need to use DLSS and Ray Reconstruction. With that setting, i get about 40-50fps at max settings with my 3080. Not too bad.  The thing about AMD is that they don't have any of this AI technology (yet). You have to rely on raw power, which won't get you far.",Neutral
Intel,"Until the next generation of consoles, I think path tracing will be added to something like a couple major games per year with Nvidia's help. If I'm right, that's wouldn't be a lot, but it's definitely be a bonus for those who have GPUs that are better at ray tracing.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode (which could be for a variety of reasons, including CP2077's larger world).",Positive
Intel,Alan Wake 2 is also an outlier since the same dev made Control which was a test bed for DLSS 2.0 and one of the earliest games to fully embrace ray tracing. Alan Wake 2 is also NVIDIA sponsored and looking to be another test bed for them.,Neutral
Intel,"Redditors trying to have reading comprehension (impossible)     >It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it).",Neutral
Intel,"Every single game has cut corners, starfield has multiple cut corners. Tech demo would imply there's no great game under great visuals.",Negative
Intel,Get that fps with a 5120*1440 240Hz monitor,Neutral
Intel,"It's almost like crushing your fps with all these fancy new graphic features isn't worth it, even on the best cards. It's user preference in the end.  That said, many gamers, once they play at high hz screen and fps, find it very difficult to go back to low fps for any reason. Games feel slow by comparison, no matter how great the lighting looks.",Negative
Intel,"Both DLSS and FSR have weird image artifacts (FSR is worse though) so i guess if you don't see them then DLSS only works in your favor then.  I personally want as little noticeable image artifacts from DLSS/FSR when I play so opt for native resolution, generally.",Negative
Intel,Have you tried dlss quality. Dlss balanced sacrifices image quality and dlss quality doesn't/can enhance it sometimes,Negative
Intel,">a lot of the comparisons just make it look like it gives too much bloom  This is due to colors being compressed to SDR. In HDR that ""bloom"" looks awesome as it's super bright like it would be in real life but still not losing any details. You lose those details only on those SDR videos.  PT just looks and feels way more natural than just RT, when only raster literally looks like it was two different games from two different hardware generations.  Once I saw Cyperpunk PT there is no way I'd imagine playing this game in any other way. All other games look absurdly ugly next to it now. It's something like when you get a high refresh screen and can't look back at 60Hz anymore.",Negative
Intel,"Trust me, I know how the technology works. That doesn't change the fact that it's not worth it imo. I'm not  paying 4090 money to have the newest fad that just makes games look over saturated and glossy.",Negative
Intel,We are talking about running the game just fine now are we? Because rasterization is just fine. I will consider ray tracing reasonable when you can do it maxed out for $1200(GPU) native. Until then it’s a gimmick. IMO,Positive
Intel,"So even you, someone who is an enthusiast of this sort of thing, i.e. a small minority of the userbase, had to spend \*minutes\* benchmarking to realize you didn't have RT turned on.   Yeah, you would lose that bet.",Negative
Intel,Those wolfdale C2D were beasts. I had a buddy with one and we took it 4.0. And it kept pace/beat my C2Q as most games didn't utilize MC as well back then.  Man XFX and EVGA all the way. Shame they both left the Nvidia scene.,Positive
Intel,"Ah, WoW. That started it all for me on an C2D Conroe with ATI x1400 on a Dell inspiron laptop freshman year of college.",Positive
Intel,8600gt in SLI man you are Savage!🔥,Positive
Intel,"I had the same setup, and i managed to get my Q6600 stable at 3.2.  Was a blazing fast machine and it still shit the bed in some parts of crysys",Positive
Intel,there there’s no way there will ever be another 8800 GT. you got so much for your money.,Negative
Intel,"Those 9800gtx+ were solid cards, it's what I upgraded from.  You make due with what you had.  My first system was a Craigslist find that I had to tear down and rebuild with spare parts from other systems.",Neutral
Intel,Seemed like a perfect use case for the sli bridge they got rid of.,Positive
Intel,Why would it need to send the data to the other card? They both feed into the same game.,Neutral
Intel,Big up the Vega gang,Positive
Intel,"That is because destructible buildings and realistic lighting REALLY does not go hand in hand. Realistic looking games use a ton of ""pre-baked"" light/shadow, that might change when ray tracing is the norm but it has a delay so things still look weird.",Negative
Intel,"I remember playing Crysis, flipped a wheeled garbage bin into a hut, which came down on top of it. I blew the wreckage up with grenades and the bin flew out, landing on it's side. Took pot shots at the castors and the impact made them spin around in the sockets.   Here we are 15 years later and game physics have barely moved an inch from that",Neutral
Intel,Cause regular stuff is so easy to simulate or fake simulate that super realistic complex items are not really worth the trouble.  &#x200B;  water still looks like crap in most games and requires a lot of work to make it right.  and even more processing power to make it truly realistic.  Cyberpunk is perfect example.  the water physics is atrocious.,Negative
Intel,It's because it's not the big selling point now and as the comment you're responding to... Things have gone so far no one really thinks about it anymore. Ray tracing is what physx used to be or even normal map and dx9/10 features you don't consider now.,Negative
Intel,"Honestly, same here - I hate Nvidia so much for what they do so shamelessly to the average consumer that I’d struggle to recommend their products even if they *were* competitively priced.",Negative
Intel,"People are just stupid, if game companies just slap ray tracing on and don't tweak the actual look of the games lighting, gaming graphics is doomed. Rt is not the magic bullet people seem to think it is, everything looking 'realistic'=/= good art design.",Negative
Intel,"I thought RT was loved by devs, as it allowed them to avoid the shadow baking process, speeding up dev time considerably?",Positive
Intel,We are guilty of the exact same sin.,Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"All rasterization does is approximate colors for all the pixels in the scene. Same thing does raytracing, path tracing, AI reconstruction and whatever other rendering techniques there are.   The final result is what's most important, doesn't really matter how it's achieved",Neutral
Intel,would you care to explain ? Kinda interested to here this,Neutral
Intel,jellyfish follow simplistic plucky quarrelsome unwritten dazzling subsequent mourn sable   *This post was mass deleted and anonymized with [Redact](https://redact.dev)*,Negative
Intel,"It isn’t minor. The PT with DLSS 3,5 is quite a bit more accurate and reactive, as in dynamic lights are reacting closer to instantaneous than without. The best way is to watch on a 4K monitor or to actually play it on someone’s 4090/4080 or wait for AMDs FSR 3 to see if that helps it do path tracing.",Positive
Intel,> Literally tiles with grainy texture suddenly becomes a mirror  PT doesn't change what the materials are bro. You are just seeing what the materials are meant to look like.,Neutral
Intel,Turn on path tracing. Embrace the PowerPoint.,Neutral
Intel,Control also has some of the best implementations of RT I've ever seen. It's one of the very few games I've turned on RT and felt like it was worth the performance hit. Everything else I've played has looked very good with RT on but not good enough to be worth the performance hit.,Positive
Intel,"Of course it is not perfect but it also allows max RT, PT, RR which improves the graphics greatly...and i do not enjoy below 100 fps either",Negative
Intel,"I tried 1080p DLSS Quality and it was just as bad, but I’m not really happy with the performance of 1440p DLSS Quality (like, 30-40fps), so I didn’t want to use it.",Negative
Intel,"You're is coming off as salty that you *can't* afford a 4090. If other people wanna spend the extra cash and turn on all the bells and whistles, why does that bother you?",Negative
Intel,"Mate, you can't fucking run starfield at good fps, a game that's raster only that looks like from 5 years.  Look at how many games scale and run badly in last two years and compare that to cyberpunk",Negative
Intel,"This just absurd and frankly entitled. Not everyone can afford a PC strong enough to run the most demanding games absolutely maxed out.   I never could, couldn’t even afford a PC until this past summer.   This obsession with performance metrics is ridiculous. Like, less than 5% of PC gamers even have 4k monitors. Almost 70% still have 1080p.   CDPR made a VERY demanding game that looks GORGEOUS completely turned up. Modern hardware cannot handle it natively, it can’t. It is not a gimmick that a company has developed a superior technology that allows the game to be played at almost the same quality as native, significantly smoother and faster. You’re in denial if you think otherwise.  I’m not tied to either brand. I have an AMD CPU bc it was the best option for what I wanted, and a Nvidia GPU for the same reason. I care about RT, frame gen, all of that. The Witcher 3 still is my favorite game, and the entire reasoning for building a gaming PC over a console is graphical quality and speed while doing so. That game is absolutely gorgeous with RT on. Same with Cyberpunk, which I can enjoy thanks to DLSS.   You don’t care about those things? That’s fine, but it is solely a personal opinion.",Negative
Intel,"Yea well I was defining an arbitrary performance. It's not about having native performance. But, but if a flagship is able to get 4k 60 with path tracing at native then that allows headroom for 60 tier or 70 tier cards to perform well.  In this example the 4090 gets 19.5 fps at 4k. The 4060ti gets 6.6 fps. Let's assume a next gen flagship is able to reach 60 fps at 4k. That would theoratically allow the 60ti card to reach around 20 fps. Once you do your various upscaling and frame generation techniques you can then reach 60 fps ideally with a mid range card.",Neutral
Intel,"The quad core showed up the core 2 duos like 2 yrs after because of emerging support and I was on cloud 9. After evga left, i jumped ship with my XFX 6900xt and I think I am here to stay with team Red until value/longevity says otherwise.",Neutral
Intel,Shame you don't. They did it for a reason.,Negative
Intel,The 9800GTX was a bad ass. I'd consider it the biggest upgrade in generation until the 1080 and then the 4090.  It was far more powerful than anything near it.,Positive
Intel,"I do miss it, but it was a pain in the ass to get working with my custom cooler due to the HBM.",Negative
Intel,They've actually gone a bit backwards. Devs dont seem to care about implementing physics anymore. It's just an afterthought. And you can forget about destruction,Negative
Intel,"I gobble up the downvotes every time I say it and I don't give a crap.  Every other corporation sees through their bullshit.  Otherwise every console would have NVIDIA cards.  No console has an NVIDIA card anymore.  But the general population is more than willing to get bullshitted into buying their cards against their own interest.  EDIT: Forgot about Nintendo Switch, as is tradition.",Negative
Intel,"Anyone can say anything.  Anyone can change their intent after the fact.  Happens all the time.   Judge them by their actions alone and ignore what they say.  Going solely by **events** :   1) People use (mostly used) NVIDIA cards for driving PhysX   2) NVIDIA disables this ability, locking everyone to the god-awful CPU implementation at the time   3) Modders edit DLLs to get functionality back   4) NVIDIA THEN says it was a bug that will be fixed    It's not foil hat because I'm coming to this opinion based solely on what's measurable and real; events and actions.   It's not conclusive, but I'm not going to rely on any company's PR statements to sway me one way or another because they will (obviously) say whatever they're able to prevent lawsuits and public image damage.  Based strictly on **events** that looks like damage control on a bad decision.  They rectified it, even more recently made PhysX open source, I give them credit for making it right, but the intent at time time was clear.",Neutral
Intel,This is why I'm excited for Alan Wake 2. It is coming out with path tracing always having been part of it. This will definitely mean they built the art style around that.,Positive
Intel,"Yeah, I can't really blame people for not recognizing great design if they are not interested in it. I don't think most people are interested in design, even though we all benefit from it without knowing it.     People might get some sort of subtle feeling when they see some great craft made by a experienced professional, but they can't really put it into words or evaluate the quality of the design without experience. They likely can't tell what cause the feeling to begin with even if they can notice it.     But everyone can instantly judge how real something looks, which is the go-to standard for ""good"". And bigger numbers are better, even if they are fake. Niche features that nobody uses also sells, as long as it's a new feature with a captivating name.     This reminded me. Live action lion king, what a travesty, stripping away all the artistry and expression for realism.",Neutral
Intel,"If you're using RT exclusively maybe, but no one in the games industry is - nor will they be any time soon.",Negative
Intel,"A lot of raster techniques reduce the complexity of lighting into very naive heuristics. Like shadow mapping renders the scene from the light's pov to create a depth map, then the objects that are at a greater depth have darker colors drawn (I'm simplifying that but roughly how it works). It's like teaching a kid how to illustrate shadows on an ball vs the how light actually works. Raster techniques have evolved a lot since then, and use raytracing in limited contexts, screen space reflections and ambient occlusion for example, but they're still operating with extremely limited data in narrow, isolated use cases, which is why they often look awful. There's just not enough information for them to look correct, just enough to trick you when you're not really paying attention.",Negative
Intel,All lights in a rasterised scene are “fake”.   Someone explains it much better here:  https://reddit.com/r/nvidia/s/eB7ScULpih,Negative
Intel,Or a bag of fake tricks.,Negative
Intel,">Every scene is lit completely differently with PT enabled.  It's really not, and it's very disingenuous of you to try and pretend it is. There are plenty of screenshot comparisons showing the difference in certain areas if you want me to link them, and I have the game myself and use a 3090.  There are differences, but saying ""every lit scene looks completely different"" with PT Vs regular RT is false.",Negative
Intel,"I'm not talking about DLSS 3.5. I'm talking about PT vs regular RT in CP2077. The visual difference is subjective, it looks different, is it better? Not in my opinion, it over saturates the scene.  DLSS 3.5 is a separate issue, it fixes some of the major flaws of PT, the slow reflection updates.",Negative
Intel,Have you tried Metro Exodus Enhanced?,Neutral
Intel,At less than 4K resolutions Quality is the only sensible option. The resolution it upscale from becomes too low otherwise and DLSS then struggles to have enough data to make a good image.,Negative
Intel,"It doesn't, I have no problem with people buying what they want. My problem is people assuming I can't afford something...",Neutral
Intel,What is this? A reasonable comment in this dumpster fire of a sub?,Negative
Intel,Thinking that a $1200 graphics card should be just strong enough to run everything Max is entitled. I wear that crown.,Positive
Intel,"I've got an evga 3080ti now. I suspect my next card is either going to be an XFX 8800xt, or catch an XFX 7900xtx on clearance.",Neutral
Intel,"I don't blindly support a single company, such as simply switching to AMD because my chosen vendors left nvidia. That's how competition stalls.  That being said the card I have now is a product of what I could get my. Hands on during the crypto boom, and is EVGA.  My next card is likely to be an XFX 8800xt/7900xtx.  So not sure why you're in here attacking me.",Neutral
Intel,Except it was just a rebadged 8800GTX/Ultra,Neutral
Intel,"I mean, the Nintendo Switch is using Nvidia graphics, albeit it’s a tiny 28nm iGPU from 2014… but I get your point. They’ve opened up a large technological lead over AMD thanks to the RDNA 3 debacle, and I’m *still* recommending only Radeon GPUs because Nvidia is just that hubristic.",Neutral
Intel,Pixel art go brr,Neutral
Intel,Yeah they will. Lumen and lumen style gi systems will have to exist for fallback so the industry can move on.,Neutral
Intel,"Yeah, I agree. It’s just a shame because it’s not really playable otherwise. DLSS with ray reconstruction is a little mixed.",Negative
Intel,"It just doesn’t make sense. Game developers do not set GPU prices.   The game runs well on a $1200 GPU. Perfectly well. Not completely maxed though, and it’s weird to think the game shouldn’t be able to make full use of available technology if it wants to. My GPU cost less than half that and runs the game very well with RT on thanks to DLSS.",Negative
Intel,"RR is not perfect, but I think overall Cyberpunk 2.0 looks so much better than it did previously - and it looked pretty damn good earlier too!  [Digital Foundry's video](https://www.youtube.com/watch?v=hhAtN_rRuQo) is again the yardstick for how many small things they point out where RR improves the situation, even if it has some things where it fails atm. But maybe DLSS 3.6 will solve those.  I do feel DLSS itself has also improved in the past few years in terms of ghosting and performing at lower resolutions.  On a 1440p monitor these are the starting point resolutions:  - Quality: 1707x960p - Balance: 1485x835p - Performance: 1280x720p - Ultra Perf: 853x480p  So even the highest quality option is sub-1080p. I wish Nvidia introduced a ""Ultra Quality"" preset that would be say 80% per axis resolution, something between DLAA and DLSS Quality. At 1440p this would be 2048x1152.",Positive
Intel,"Speaking of the AMD overlay, am I the only one that gets near zero CPU utilization every time they pull it up? GPU utilization fluctuates up and down depending on resolution and graphical settings, so it seems to be fine, but my CPU utilization readings never go above more than a few percent tops. I have a 5800X3D.",Neutral
Intel,GN has already made a [video about it.](https://www.youtube.com/watch?v=5hAy5V91Hr4),Neutral
Intel,Honestly this is everything that I wished Adrenalin overlay had. It's pleasantly lightweight too.   Hats off to Intel's development team. They didn't need to publish this at all.,Positive
Intel,I wonder if this will get added to Mangohud and Gamescope.,Neutral
Intel,This is quality. Great work.,Positive
Intel,Doesn’t capframeX uses presentmon as its monitoring tool?,Neutral
Intel,I can finally see if it really is the ENB taking down my Skyrim gamesaves,Neutral
Intel,"FYI, the open-source version has been available for years, was last updated 9 months ago, and does not contain an overlay or GPU busy stats. (It also has version number 1.8 while this new thing is v0.5)  Maybe they're planning to release source code later, but right now the source is closed.",Neutral
Intel,This is pretty damn awesome and will be incredibly useful when they develop the future features Tom mentioned. (insightful metrics),Positive
Intel,"This is not bad at all. But it's not replacing Radeon overlay for me... yet.  1. Needs an option to minimize to system tray.  2. Many important Ryzen CPU metrics (temperature, power consumption etc.) aren't available without an ability to access Ryzen Master SDK. 3. Radeon Overlay can continue to monitor on the desktop, which I use constantly. I tried manually hooking DWM.exe and it won't. I might just be dumb, but I couldn't get it the overlay working without 3D running. 4. Will give credit where credit is due, unlike Radeon overlay, it works right with OpenGL. 5. And the credit is immediately removed, because unlike Radeon Overlay, it doesn't work right with DXVK.  6 out of 10.",Negative
Intel,This is so good. The only thing I dislike is that there isn't a way to scale the whole thing up an down.,Positive
Intel,"At this rate Intel will have a more polished driver and software stack than AMD in 2 or 3 years, if not before.  Seriously, I suck for being able to buy an AMD GPU like the 6990 I owned, but I can't get near any hardware they made while using my system for work.  I need a CUDA alternative that works on more than 6 GPUs on specific linux distributions when the red moon shines above my building.",Negative
Intel,it's sort of misleading to call it opensource    when whatever intel beta version released is just MSI installer and binaries inside ...    i see no source of what is available for download ...   what's on gihub is something old w/o overlay,Negative
Intel,Thanks Intel! I will try this out at least since I hate MSI afterburner.,Positive
Intel,The download link gives a warning in Windows:  >This site has been reported as unsafe   >   >Hosted by intelpresentmon.s3.amazonaws.com   >   >Microsoft recommends you don't continue to this site. It has been reported to Microsoft for containing harmful programs that may try to steal personal or financial information.  ???,Negative
Intel,"I really wanted to test with it why I was getting stuttering in Apex. Unfortunately, due to some work, I would be possible on Sunday.",Negative
Intel,Doesn’t work for me. It crashed at the start with an error message and made Dolphin run way worse.,Negative
Intel,And AmD gives far more than Nvidia.,Neutral
Intel,I can't get this to work. Is anyone having the same issue?  CPU Temperature (Avg) NA C  CPU: 5800X3D,Negative
Intel,I have a 5800x3D and PresentMon does not recognize it. Says UNKNOWN\_CPU.  Anyone?,Negative
Intel,People have reported that cpu usage in Radeon software is not very accurate.,Negative
Intel,I saved this because it is posted so frequently lol  https://www.reddit.com/r/AMDHelp/comments/14n55gd/cpu_usage_percentages_differ_between_task_manager/jq66gop?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=1&utm_content=share_button,Positive
Intel,"Probably overlay wasn't updated for W11 22H2.  Microsoft updated something and CPU reading was broken for MSI Afterburner also, after working the same for a decade or more before. They fixed it in the meantime.",Negative
Intel,"I had the same problem, googled around and found the problem to be c state in the bios. Just turn that off and it should be accurate.",Negative
Intel,Just use afterburner as OSD.,Neutral
Intel,I had that issue with my 5600X both with Nvidia and AMD overlay. More recently it randomly fixed itself.,Neutral
Intel,"Yeah adrenalin was useless for it. It was displaying 1-2% usage for me, but msi afterburner, hwinfo, and rtss were showing much more, like 30-50, which made more sense.",Negative
Intel,You beat me to it :),Positive
Intel,"Yup, ""new Intel"" definitely seems to be getting nicer and with a ""younger"" company culture. Also, it's a smart move. They're building a nice little ecosystem around Arc cards, which is eventually what is going to drive sales when performance and stability matches NVidia and AMD.",Positive
Intel,And it's open source!  I'm liking Intel more and more since they're getting their ass kicked.  Apparently it was one guy's pet project.,Positive
Intel,Technically it should be possible to add in MSI afterburner because it's open source,Neutral
Intel,"The new PresentMon shows more information.  Since it's open source, capframeX can implement this as well.",Positive
Intel,It was a pet project of one of the Intel engineers.   6/10 is not bad!,Positive
Intel,It's still beta. I'm sure they'll fix it.  Intel has amazing engineers.,Positive
Intel,Intel is working on their 2nd generation GPU and it's said to compete with Nvidia highest tier.  And now AMD isn't planning to release a big RDNA4 chip.,Neutral
Intel,"didn't nvn get merged with mesa recently edit: sorry, I mean NVK not NVN",Neutral
Intel,I hate afterburner and RTSS. This is way better,Negative
Intel,"people probably spammed microsoft because this PresentMon also gives you the option of enabling amd, nvidia, or intel telemetry.",Negative
Intel,"Thank you for contributing exactly NOTHING to this discussion.  Truly, the world is a better place because thanks to you.",Positive
Intel,"Tell that to anyone that needs CUDA. Or GPU accelerated workloads in general.  Id suck for being able to get something like the 6990 I owned back then, but between lack of support for almost all workloads and the driver being shit since like forever... Nope.  Yes, yes, nvidia's control panel is old and looks like a windows 98 app. But it at least works and actually saves youre settings, unlike AMD's one that half the time simply forget about everything.  I currently own 5 systems. 2 nvidia ones, 3 amd ones. And all of the amd ones had driver related issues.  You can't simply work with that. Is not viable. And as a gamer you should not need to see if the drivers are working or not.  Fuck, amernime drivers exists just because how hard amd sucks at it.",Negative
Intel,"Well, everyone uses RTSS anyway and it gives you basically everything.",Neutral
Intel,Similar issue here with an i5 12600kf. Can't get temperature readings from it. I'll need to look at it more.,Negative
Intel,"Really? Good to know it's not just me, then. Nonetheless, this seems like a pretty egregious oversight on AMD's part. If I'm not mistaken, CPU metrics are on by default in the overlay and every time someone switches it on, it's an extremely visible reminder of work that AMD still needs to put into the drivers.",Negative
Intel,Speaking of said software why does it keep setting gpu max frequency to 1200MHZ?,Neutral
Intel,"Thanks for sharing your findings. It's pretty damning if even the Windows Task Manager does a markedly better job than Adrenalin. That's probably an understatement since, as I found out via other helpful Redditors here, the Adrenalin overlay may as well be broken. I'm just glad it's not something wrong with my system.",Negative
Intel,Afterburner fucks with my settings in adrenaline,Negative
Intel,"NVIDIA's highest tier? Nah, but around RTX 4080 is the target, which might be bad news for AMD.",Negative
Intel,"By NVN do you refer to Nintendo's API for nvidia? Or you wanted to refer to NVK that got into mesa?  By CUDA alternative I mean stuff like ROCm. CUDA have so many years of development on top of it and a stupidly widely support from software vendors.  While yes, NVK could help to alleviate the issue, is not even close to what native CUDA can do regarding GPU usage for general computing.  The issue with ROCm is mainly a lack of support on a lot of software and lack of support for a lot of hardware.  On the nvidia end you can get the cheapest trashiest GPU and it will have the same level of CUDA support as the most expensive enterprise product.  Yes, performance is not the same, but I can use a 3090 Ti for loads that needs more than 12 GB vram without needing to buy a dedicated AI GPU, and if a given task needs serious shit, then again, I can buy a dedicated solution that will run the exact same software with the exact same code I ran on the ""weak"" 3090 Ti.  That is not possible with AMD, if you buy an enterprise grade GPU from them, you need to build all your software stack for it instead of just moving it and keep going.  And while yes, developers from tools like PyTorch COULD improve their ROCm support, I totally get why they are not doing that. ROCm works on like what? 7 GPUs? On specific linux distros?  Is a matter of scalability and costs. ROCm needs to be supported on ALL AMD GPUs in order for it to be something worth developing for, otherwise there is not really a use case where the end user wont be writing their own stack.  And again, if you can start small, then scale up, nvidia is the only option, so AMD on this space is either ""I got a stupidly great offer where AMD provides the same hardware power and performance per watt at like half the prize so I justify building all my software from scratches"" or ""Ok, I'll go for nvidia and just use all the software already built for it and call it a day""  I really, really want to being able to use AMD GPUs aside of trivial tasks, but this is a very, VERY gigante issue that appears a lot, especially on the profesional space.",Neutral
Intel,Except PresentMon seems to not be compatible with Ryzen CPUs at all... :/,Negative
Intel,I get downvoted for asking a valid question?,Negative
Intel,Thank you for continuing to contribute Nothing to this conversation.,Positive
Intel,Clearly not more than this beta of presentmon,Neutral
Intel,I use amds for overclocking and that's about it really. But it's still far more tools than nivida gives,Neutral
Intel,"To defend AMD here as loathe as I am lately. CPU utilization isn't a very useful metric to begin with, so it misreporting would be super low priority.  The stat tells you nothing useful about the hardware, just the thread scheduling. CPU could be stuck in wait and reporting a high number while doing no work or it could be bottlenecking somewhere else while reporting a low %.",Negative
Intel,Where are you seeing this?,Neutral
Intel,"The Adrenalin overlay is good for really everything except usage. But yeah, this has seemed to have been a problem for a good while now. Ive been using AMD for a little over a year now at least Adrenalin and the CPU usage was always wrong on two different CPUs (AM4 and AM5) as well. Me and that other person tested it on my system and he had 3 systems running different AMD CPUs and all were the same. Ill find the thread one day but its been a while so it is way down the list of my comments.",Negative
Intel,"False setup.  Disable the button in the ui ( not settings)  For ""load on startup""  You literarily told afterburner to load it's settings on start up.  Huge mistake many people do because most people think it's ""boot on startup"" but in reality it's loading it's oc settings and stuff effectively overwriting adrenaline if you leave that enabled.",Negative
Intel,3070 Also was their target for their first generation. I don't expect Intel to suddenly be great just because that's what they aim for.,Neutral
Intel,And XeSS 1.1 looks better than FSR.  XeSS 1.2 was just released and it has even more improvements.,Positive
Intel,Great news to gamers though,Positive
Intel,ah sorry I meant NVK,Neutral
Intel,"I don't know, I didn't downvote you.",Neutral
Intel,My point is if you've got nvidia card you simply use 3rd party soft and got everything and more. So comparing that AMD's software to Nvidia's is kind of meaningless.,Neutral
Intel,"I actually agree with you, but I don't need the metric to be some scientifically accurate reading of performance or whatnot, I just think it'd be a useful tool for adjusting game settings. For example, if I'm trying to optimize performance in a game, does this setting make CPU utilization go up or down? How much almost doesn't matter. Is there a bottleneck, where is it, and am I relieving pressure on it with these settings or not?",Neutral
Intel,It’s where you oc in adrenaline,Neutral
Intel,"There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Not because upscaling, but also because how important is to have drivers that work in tandem with specialized hardware units in the GPU.  Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  AMD fails hard on 2 of those things.  Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.",Positive
Intel,"Yep, XeSS is just great, I happily use it in MWII on my Arc A770. But I still use DLSS on NVIDIA. But both are better than FSR.",Positive
Intel,"Then you should be looking at ***GPU*** usage, because CPU usage will not help you answer any of those questions",Negative
Intel,"Problem is, by itself it still tells you little to nothing. If GPU utilization drops like a brick you know something is bottlenecking. If CPU utilization changes in the absence of other data you know absolutely nothing of value.   The real kicker is the CPU utilization may not even be ""wrong"", it may just be polling at a very low rate because that's the other thing with these OSDs if you poll too frequently for up-to-date data that can kill performance from the overhead.",Negative
Intel,"> There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Agreed, not to mention that when they made Alchemist, yes their target was 3070 and they underperformed that, but they will eventually hit their targets as they learn to improve their architectures. For a first shot, getting to around 85-90% of their target is actually quite good. It's not like they undershot by 50% of the target. They got close.   But like you said, software is super important and these days having good software is half of a driver. I have an A770, the driver actually pretty good, like the software menu. It used to suck when it first came out because it was some crap overlay, but over time it's become a separate program and it works really good considering they're not even a year into Arc's release. They also have a good set of monitoring tools built in and the options are all simplified. They don't have super unnecessary things like a browser integrated into the driver suite, or a bunch of other stuff. I'm sure by the time Celestial is around if Intel does stick with Arc, that they will be ahead of AMD on the software game, assuming AMD continues down the same path they have now.  > Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  Yep, tools like [this one from NVIDIA](https://youtu.be/-IK3gVeFP4s?t=51) have made it easier than ever as a dev to optimise for NVIDIA hardware.  > AMD fails hard on 2 of those things. Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.  Yep because Intel recognises that to get close to NVIDIA they not only have to make great hardware, but also great software to go along with it. You can only do so much with hardware to match NVIDIA. You have to make great software along with in-game solutions like XeSS to match NVIDIA. FSR is behind not only DLSS but now also XeSS. AMD is just not competitive with NVIDIA on all fronts. Arc is just getting started, give Intel 5-10 years of time in GPU and they will outpace AMD for sure.",Positive
Intel,"There are many settings that directly impact CPU usage, so of course CPU utilization reading can be useful.  Not everything needs to be precisely accurate, this is a lousy defense. Getting 0% reads all the time is entirely worthless but some accurate reads is still useful information.",Neutral
Intel,"You are going out of your way to defend a broken feature.    7800X3D shows up as 1% CPU utilization during games such as Watch Dogs, Stellaris, Age of Empires. Clearly broken. Undefendable.",Negative
Intel,"And with a bit of luck, they will also hit AMD in the console market.  Right now AMD stays afloat GPU speaking because consoles, they were during a lot of years the only vendor offering both CPU and GPU in a single package, reducing costs A LOT.  Nvidia was not able to compete against that with the lack of CPUs and Intel with the lack of GPUs.  Now that Intel is working on dedicated GPUs they are going to eventually create APU like products, its just the natural outcome of producing both CPUs and GPUs.  And then AMD will NEED to compete against them instead of being the only option.  And if that happens, unless FSR gets turned into a hardware specific tech like XeSS that falls into different techs depending on the GPU its running on, AMD will be murdered.  Better software stack, better general hardware to accelerate and improve upscalling keeping costs down AND being a second player pushing prices down towards Sony and Microsoft?  AMD will be screwed up if they don't up their game. And hard.",Neutral
Intel,"Right, so what kind of useful information can you tell me if I raise graphics settings and CPU utilization goes from 12-16% to 16-17% while framerate goes down from 120 fps to 110 fps?",Neutral
Intel,"All CPU utilization tells anyone is thread scheduling, and nothing about what the threads are or aren't doing. Like the only situation that it marginally tells someone anything is how many threads a program uses. But that usually doesn't fluctuate with games, games usually scale up to <x> number of threads and that is that. GPU utilization is the one that is far more useful for spotting bottlenecks, general performance observations are far more useful for tweaking.  Even Intel and co. will tell you that with modern processors and all the elements they contain as well as hyperthreading/SMT CPU utilization isn't a very useful stat anymore. Meant a whole lot more when CPUs were one core unit and didn't also include the FPU and memory controller and everything else.",Neutral
Intel,"Cause unfortunately even if it wasn't broken, it's not useful in any meaningful capacity. Should it be broken? No. Would anything notable come of it if it were fixed? Also no. Given the bugs they got and room for improvement elsewhere it's one of those things that should be super low on the priority list or just removed wholesale.",Negative
Intel,"That whatever setting you are changing does not have much of an impact on CPU utilization, and if your GPU allows, you can use it.  What is the purpose of this question, are you really not aware of any settings that impact CPU usage? Some RT settings have big impacts on CPU usage too.",Neutral
Intel,"The problem you're missing here is that settings that are CPU demanding generally don't have an easily observable impact on CPU usage, if they have any impact at all.  The reliable way to see if a setting is CPU demanding is to turn it up, see the frame rate go down, and then look at how **GPU** usage changed.",Neutral
Intel,"I'm aware that some settingd impact CPU usage, but I think looking at the CPU usage is about as useful as your daily horoscope, or AIDA64 memory benchmark",Neutral
Intel,"We kind of know where it will land. It will be a 3050M level chip, maybe a bit better, but will have improve scaling and frame gen.   8060S is 40 RDNA 3.5 units. One Xe3 unit is about 2.1 RDNA 3.5 units. That put it at about 65% of Strix Halo, though it will have a worse memory bus and no MALL cache. Somewhere in that range.   So that 60% is almost bang on the 3050M. Maybe a bit better. It won’t be like the 4050 but 3050M isn’t bad for an iGPU that fits in a normal socket",Neutral
Intel,"> and no MALL cache  PTL does have 8MB of memory side cache, fwiw.",Neutral
Intel,"I don’t know how this score compares to the 3050M. I only know that this score is about 55% of the B580. And the B580, at 2K and 4K, is about 1.7–2 times the performance of the desktop 3050",Neutral
Intel,Really don't understand why they don't go with a larger cache.  Pretty sure they still have a bunch of cache chunks spread all around the SoC.,Negative
Intel,"in configs without die-to-die memory performance in general should be worse if bandwidth limited. despite not being specifically dedicated to onboard memory like Lunar, people are still planning configs with local LPDDR5x, though peak bandwidth is limited by a 128bit bus.",Negative
Intel,I see it personally as not really being at 20W if you’re asking much from the GPU. It’ll increase the juice dynamically if it gets demanding enough. So it’ll be hard to say unless you force the power limits way down manually.,Neutral
Intel,"I actually like the idea of discrete GPU naming scheme for the new iGPU, 300 series for integrated graphics is really makes sense but they should add 'M' suffix to make it clear.",Positive
Intel,"I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  Also looks like the 3\_8 and 3\_6 versions are differentiating maximum boost clocks, though I wonder if instead those may reflect that configurable upper TDP bound. Might make sense for 65W and 80W to be differentiated if that will coincide with anything about the ""experience-based"" PL1 behavior.",Neutral
Intel,There are 2 dies.  One for professional workload which will be mass produced.  Second is for gaming. Even a 10-12 core xe3 will be barely enough for modern 1080p. Lunarlake can only run alanwake at 1080p at low settings getting only 25 frames so even if this is 50-100% better this is the minimum for a 2026 product.   I see no point of a 8 xe3 core system when all people will do is just complain.,Negative
Intel,"quite confusing, Xe3 should start from C (celestial), if using name like B390  we think this  is a battlemage Card (Xe2)",Neutral
Intel,"I wonder if rumors about Zen 6 clocking way higher than current cpus turn to be true, and the 5.1ghz max on mobile PT mean Amd might have an edge in next generation   Only time will tell",Neutral
Intel,please add M for Mobile or i for iGPU  * Arc B390M Xe3 Graphics * Arc B390i iXe3 Graphics,Neutral
Intel,"A clock speed regression vs the prior gen on N3B, with a remark that it's difficult to cool, really isn't a good look for the process side. 18A branding with more like N4 performance...",Negative
Intel,Will the 10 core Xe be better than radeon 890m or worse?,Neutral
Intel,"i mean i get this is a laptop part but man 16 threads is not much to phone home about when it comes to horsepower, isnt next gen desktop aiming for something like 48 threads?",Negative
Intel,Still weaker than x3d,Neutral
Intel,Yes indeed we need that M&M. Mobile platforms are not a priority for me and are dedicated mobile gpus really comparable to Big Boy Discrete GPUs? It is very confusing.  Lunar Lake laptops should fall in price. Has anyone used Lunar Lake and if so which models? Buying latest gen is for guinea pigs and the rich!,Negative
Intel,Doesn't the B already serve that purpose?,Neutral
Intel,">I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  The 10 Xe3 core model is a binned down 12 Xe3 N3 die, and I doubt yields are so bad that they would even be able to find more dies where they have to disable more cores.   The other die is the 4 Xe3 Intel 3 die, so you can't go up from there.",Neutral
Intel,To be fair Alan Wake 2 low settings look great. This was covered by DF awhile back they said in some ways Alan Wake 2s low settings look better than some modern games high.,Positive
Intel,"I mean, but that logic, most of Intel's historical bigger iGPUs don't make sense. There are use cases other than AAA gaming. Media creation is another big one.",Negative
Intel,"Xe3 is not GPU family name but GPU core architecture, it's like Nvidia Ampere, Ada Lovelace. But Alchemist, Battlemage, Celestial is GPU family name.   Panther Lake 12Xe3 being B series GPU makes sense because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture. Intel confirmed Celestial will have Xe3P.",Neutral
Intel,"Think of it like AMD Zen X+ nodes. Ryzen 8000 is more or less a laptop only APU series on Zen 4+.   Xe3 is a half-generation, it doesn't get the letter upgrade to C, but it gets the 3, signifying a new architecture, but not a new generation.  Zen 4+ is a half-generation, it doesn't get the number upgrade to 5, but it gets to be 8000-series, signifying a new architecture, but not a new generation.",Neutral
Intel,"Mobile Zen 6 is likely to come around the same time NVL does. Both should use N2 and will presumably have similar frequencies, well above any 18A parts.",Neutral
Intel,"Considering the timing of 2nm, zen 6 would be around late 2026, and mobile zen 6 late 2027 wide availability. for whatever reason amd takes forever despite high mobile demand, but this quarter it looks like it worked out for them (maybe a big bump up from x3d sales).",Neutral
Intel,What're you doing on a laptop?,Neutral
Intel,"These are for thin and light office notebooks and light gaming. Think Lunar Lake. For CPU power, Nova Lake H will exist.",Neutral
Intel,It's for handhelds and office laptops not hyper enthusiast shit.,Neutral
Intel,Nobody buys AMD laptops,Neutral
Intel,">Still weaker than x3d   Source?? Also Panther Lake is H series only, HX will be based on Nova Lake.",Negative
Intel,Yeah so it is weaker for gaming with a dGPU than the 0.2% of laptops currently sold that have either a 7945hx3d or 9955hx3d that makes up for less than 0.1% of all laptop users. What's your point?,Negative
Intel,"While I haven't used it daily or anything, and I've only done initial setup on the Lunar Lake, the feedback we've gotten both on Arrow Lake and for Lunar Lake (e.g. 268V and 265H) Dell models is that it's a big increase in battery life and performance. The integrated graphics (e.g. 140V and 140T) are very capable compared to a **workstation** grade NVIDIA Ada 500 GPU, but they are not even comparable to a gaming GPU like the GeForce 4060 or even a 5050.  The integrated graphics do however get used for 99% of all workloads unless explicitly specified because they are vastly more battery efficient and draw less power compared to a dedicated NVIDIA chip, meaning you can have a much smaller external power supply, and your graphics performance in those basic desktop workloads with one of these chipsets will be **much** better than previous generation Intel chips. Exceptions are obviously something like gaming or AutoCAD that specify to use the high performance dedicated graphics chip.  140V/140T are barely functional for modern AAA gaming, but if you stay 5-10 years back for AAA titles you might be okay. It will smoke most Indie games. Just look at the per title benchmarks for a 140V/140T and you can see if your game benches. You could probably get away with a lot of functional mobile gaming without a dGPU, but I wouldn't expect to be able to play a recent Call of Duty or Black Myth or anything with anything like an acceptable framerate at a decent resolution. This integrated graphics chip compares very favorably to its more common Ryzen 7 equivalent, I believe the 780M, and it's a very good APU for handhelds overall due Lunar Lake's power efficiency compared to other X86 chips.  You have to understand that for these next two generations Intel seems to be making big strides in terms of both power efficiency and integrated graphics for mobile, it's a very attractive option and the first time I've seriously considered a laptop without a dGPU. I think Panther Lake is going to be a very nice kit next year for both laptops and handhelds and give AMD a run for its money.   I suspect AMD genuinely needs a new APU graphics architecture implemented next year to keep up, which I expect them to. Not a bad problem to have.",Positive
Intel,"Yeah for sure. It's a small die and should be yielding pretty high. See also the number of 4+8+4 SKUs. Looks like the larger CPU tile is also yielding decently, so not a ton to cut down.  I'm partly saying that because a larger Intel3 die was certainly possible. Even if it was 6 Xe3 cores and built as half of the larger die (just one of the two render slices)  it would fill the void a bit more.",Positive
Intel,Which is rather silly. They should've named celestial Xe3 and the current Xe3 as Xe2P,Negative
Intel,"> But Alchemist, Battlemage, Celestial is GPU family name.  Specifically, *discrete* GPU family name.   > because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture  That is simply not true. Xe3 brings much bigger changes over Xe2 than Xe3p does over Xe3. That's why they were named that way.   > Intel confirmed Celestial will have Xe3P.  No, they actually haven't said anything about Celestial (again, as a dGPU) at all. They said that C-series naming (i.e. NVL iGPU) will start with Xe3p.",Neutral
Intel,Highly immersive porn on the go,Positive
Intel,"Idk, FEM sim of a pressure boiler?",Neutral
Intel,These are H series chips. Even the U series chips don't go down to LNL min power levels.,Neutral
Intel,"NVL-H is 400 series, *replacing* this, next year. Not supplementing this lineup.   Adding more cores won't do anything for gaming.",Negative
Intel,PTL extends up to the -H series too.,Neutral
Intel,"HX this year will still be Arrow Lake.   Nova Lake will be a full line up, with S, U, H, and HX, but end of 2026 / early 2027",Neutral
Intel,">Source??  You can't seriously be asking for a source for whether or not this part will be able to power dGPU gaming laptops better than X3D chips.   >Also Panther Lake is H series only, HX will be based on Nova Lake.  Not till late next year or early 2027. It's all arrow lake till then.",Negative
Intel,> Source?   Common sense suffices. It's a tick core with a clock speed regression at that.,Neutral
Intel,"Surely a cost decision. The 4Xe die, including the choice of Intel 3, is supposed to be the cheapest thing to deliver an acceptable mainstream PC experience. They need PTL to be a proper volume runner and start displacing the RPL that's still a large chunk of sales. WLC should hopefully finish the job.",Neutral
Intel,Just get a Vision Pro?,Neutral
Intel,"Tbh, more cores would just make that go faster, but 16 would already be plenty. Especially for something like that where it's probably going to be a linear analysis and ram constrained if they actually modelled the gas (which would not necessarily be required).",Neutral
Intel,Lmao this is funny we both responded to the same comments with the same things within like 2 minutes of each other.,Positive
Intel,I don't see anything wrong with asking for actual benchmark information especially when there isn't anything official. X3D is nice but it isn't the end all be all. I would be curious to see if Intel can manage to compete.,Neutral
Intel,"Oh I totally agree, but it would've been nice you know? Jumping to 6 Xe3 is a  significant area increase for a tiny tile. I understand exactly why the 4-10 gap exists, but I can't say I don't wish there was something to fill that gap if only because it looks weird.  I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.",Neutral
Intel,"PTL's main changes are fixing MTL/ARL's terrible SoC design, which should net a few % performance. It'll see a mild IPC increase, getting a few more % performance. And it'll lose a bit of clockspeed, erasing most of those gains.  Expect PTL to be very similar performance to ARL, but with lower power consumption, a much better iGPU, and most importantly to Intel: Using their own fabs instead of TSMC.  It absolutely won't be X3D in gaming.  Edit: Actually shocked that people think this would compete with X3D.  9955HX3D is \~16% faster than a 275HX in gaming...and a 275HX itself is easily 10%+ faster than a 285H in gaming.  Not even Intel themselves are claiming this. Their own marketing refers to PTL as ""ARL performance with LNL efficiency"". Nobody realistically expects PTL-H to see a 25%+ gaming improvement over ARL-H. The fact that IPC increase is less than 10% and clockspeed is slightly lower than ARL-H should make this obvious",Neutral
Intel,CGC is a LNC tick. This is well known at this point. And we see it's even a clock speed regression.    Even entertaining the notion it will close the gap to AMD's X3D chips is just delusional.,Neutral
Intel,"Oh, yeah, I get you. Wish they could give more granularity. Just personally think some sacrifices are worthwhile if it can condense Intel's mobile lineup back down to something sane again.   > I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.  Yeah, should be a good fit. Shame they don't have anything with a bit more CPU umph, though. 4+8+4 and only up to 5.1GHz is *fine*, but not great. Especially without an HX replacement.",Neutral
Intel,"Two options seems right, either you care about it or you don't.",Neutral
Intel,"Its not ""delusional"" to want to see actual numbers instead of speculation. I have been in this game long enough to see plenty of speculation even with accurate information not give the actual numbers.",Negative
Intel,"Given how well ARL HX was received in gaming laptops, I think they may wait to have something from NVL take that top spot. 5.1ghz does seem low though. ARL-H will happily do 5.4 and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP.  I suspect these may not be totally final clocks though they do seem reasonable.",Neutral
Intel,"Honestly, surprised ARL-HX is doing as ok as it is. The deficits of the architecture in gaming are well known. If it could hit the same clocks and core counts, PTL should look a lot better still. And all that besides, ARL's cost structure is horrible. For Intel's own sake, the sooner they move on, the better.   > and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP  From the same leaker, these chips are at 65W or even 80W TDPs, so they're not merely power limited. It seems that 18A just significantly underperform some expectations, though in line with some rumors and the gist of the revisions Intel's been making to its projections over the last year or two.  > I suspect these may not be totally final clocks though  If they're defining SKUs and such, these clocks need to be finalized for all practical purposes.",Negative
Intel,"If I'm reading correctly those are max power limits, not the TDP,  though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  As for why ARL HX is doing well in gaming laptops, I think a good bit of that is also part of what made it lackluster on desktop. It doesn't really scale up that well with higher TDPs and power limits, but it does seem to scale down. The 285HX with its 55W TDP and 160W max limit doesn't perform far off the 125W TDP and 250W max of the 285K.  It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system. The 9955HX3D is very impressive, but quite a lot of laptop buyers seem to value the ability to do more laptop-like things with their gaming laptops than the extra frame rate. I'm hoping this gets shaken up as AMD adopts new packaging tech as seen in Strix Halo.",Neutral
Intel,"> If I'm reading correctly those are max power limits, not the TDP, though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  Not guaranteed given it's just a twitter leak, but I'm assuming the leaker is using the term TDP consistently with Intel's historical usage, i.e. PL1. For ARL, 115W is PL2. I would also assume there wouldn't be the disclaimer about it being hard to cool if they cut the PL2 so much, though PowerVia does create some interesting complications there, so maybe not quite apples to apples.  Either way though, don't think it should have much impact on ST boost. You're talking a good 70%-ish of power going to compute, so even at 65W PL2, that's still 40-50W available for one core. Should be *easily* sufficient to hit whatever the silicon is capable of.  > It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system.  Yes, and this is something I've very much looking forward to with NVL-HX. At this point, the biggest demerit of the -HX platform vs -H is the use of standard DDR5 vs LPDDR. That's because it's still based on the desktop silicon with the different SoC/hub tile. But with NVL using a shared SoC die, they should be able to offer an -HX platform with the core counts people expect (though probably limited to single die 8+16), but the power/battery life advantages of -U/-P/-H. In general, should help make the -HX more of a straight-up upgrade than the tradeoffs one faces today.  AMD has this situation even worse today, because there's a much bigger gap between their desktop SoC architecture and the mobile one. Though as you say, they may also bring them closer together in the future.",Neutral
Intel,Try the shunt mod,Neutral
Intel,"Cool, errr...  icy",Neutral
Intel,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Neutral
Intel,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Positive
Intel,did you use dry ice? how did you hit sub-ambient?,Neutral
Intel,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Neutral
Intel,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Neutral
Intel,Are you in the US? If so how were you able to get Maxsun?,Neutral
Intel,Oh... for sure 😁,Positive
Intel,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Positive
Intel,Car coolant in the freezer 😁,Positive
Intel,Great work dude! Only 200MHz to go 😉,Positive
Intel,That's the way! Let us all know the results.,Positive
Intel,I am in Australia.,Neutral
Intel,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Neutral
Intel,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Neutral
Intel,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Positive
Intel,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Neutral
Intel,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Neutral
Intel,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Negative
Intel,Oh! You put the car coolant to run through a freezer? Wow! Nice,Positive
Intel,But Core Ultra 255Hx is almost $150 more than Zen 4 8945HX on lenovo Legion. That price gap is enough to upgrade 5060m to 5070m.,Neutral
Intel,"Can I redeem the codes to my accounts on a different Intel system? I bought a B860 motherboard and an Ultra 5 245k, but I won't be building that system till Christmas. I'm currently running an 8700K on a Z370.",Neutral
Intel,Idk if I did it wrong but redeemed my cpu but not my arc card on the website. Couldn’t contact support because it kept throwing invalid captcha at me.,Negative
Intel,just purchased a laptop from micro center with a 275hx but don't know how I would redeem this specific offer as I only get the one that lets you pick 1 of 4 games. Does mine not qualify/is micro center not participating?,Negative
Intel,"nope, you need to have installed Ultra processor to get promo game, because Intel used software to check it",Neutral
Intel,I really would like to know how it will compare to AMD Ryzen AI MAX+ 395 (Strix Halo) APU?,Neutral
Intel,That naming scheme really is complete and utter dogshit,Negative
Intel,Sounds like fooz will be getting a taste soon? Christmas or Q1 2026?,Neutral
Intel,Can we just toss random darts at tech words and numbers to assign a different naming scheme to each and every different Intel Product?,Neutral
Intel,I'm looking forward to check how those series will perform!!,Positive
Intel,"This isn't a ""halo"" product. An upgrade to Arrow Lake/Lunar Lake when it comes to iGPU, but not Strix Halo. It won't cost as much as well.",Neutral
Intel,look forward to new APUs,Positive
Intel,Wait for Nova Lake AX. It will have 48 GPU cores instead of 4/8/12.,Neutral
Intel,Likely will be worse since it has only 1/3 the power usage and significantly smaller chip size.,Negative
Intel,"It's meant to confuse you in purpose, so you ignore it and go by the 3/5/7/9 scheme. Marketing success is dependent on the company leading the customers to the way they want it. So it needs to be complex and confusing.",Negative
Intel,"also why is everything in a lake. like really that's the last place you want your chips to be.  and why is it 255k, 285k and so on instead of just 250k and 280k.  and why is it arc xe3 and not just xe3 ahh  so many weird conventions really. no consistency at all. just pure confusion.  apple and nvidia get it right too, it's really not that hard.",Negative
Intel,"'Strix Halo' is lees about being a halo product, because it’s just an AMD internal naming scheme for the set of specific APU products. APUs that will follow 'Strix Halo' will be called 'Gorgon Point'. Since 'Strix Halo' is already out and the Panther Lake in question is about to come out it might be better to compared it to the next AMD 'Gorgon Point' APU line? But I still think it can be compared to 'Strix Halo'. At least I’m looking forward to see the comparison.  Edit: changed 'Strix Point' into 'Gorgon Point' since that’s what I meant.",Neutral
Intel,You’ll be waiting till 2027 on the amd side.,Neutral
Intel,All halo iGPUs are way too overpriced. Even regular iGPUs are overpriced going into $1K laptops.   And if you want to spend that money you can do it today with Strix Halo.,Negative
Intel,"Lesser power usage doesn’t always equals to performance losses.  See the Apple Axx SOCs and the Qualcomm snapdragon SOCs.  Even with the x86-CPUs over time they had more performance gains while maintaining almost the same power consumption.  Not anymore, but they could catch up with better chip design?",Neutral
Intel,>also why is everything in a lake. like really that's the last place you want your chips to be.  Easier to cool them,Negative
Intel,"Stop trying to find meaning in the naming scheme. Patience grasshopper, all will be revealed in time.",Negative
Intel,Like how is every chip company so bad in naming stuff?  Intel names things from geographical stuff to avoid politics etc. So I get the lakes  The numbers I dont.,Negative
Intel,"> APUs that will follow 'Strix Halo' will be called 'Strix Point'.  What? No, that's not what any of these terms mean. The Strix lineup is already out. ""Strix Halo"" is literally their halo, big compute/iGPU product.  PTL will compete with normal Strix Point and its refresh next year.",Neutral
Intel,Really? This should be good for Intel in 2026.,Positive
Intel,These Halo iGPUs are meant for LLM first and foremost. If you just want to game just get a normal RTX laptop.,Neutral
Intel,They are like 20% more efficient.  You are talking about 200% more efficient if it hits the same performance.    They might as well make it a mobile chip if that is how good it is. Would be great on phones.  Would be better than most pcs right now on 5W envolope.,Positive
Intel,"It's intentional, and non specific. So most go by the 3/5/7/9 naming. In order for them to have max profit, they need to lead you to the chips they want you to buy.",Neutral
Intel,'Strix Halo' is a product line as 'Gorgon Point' is also/another product line. Both have different variants.  In the case of 'Strix Halo'  [Check](https://hothardware.com/news/amd-strix-halo-cpu-rumors)  Find the official lineup here  [AMD](https://www.amd.com/content/dam/amd/en/documents/partner-hub/ryzen/ryzen-consumer-master-quick-reference-competitive.pdf)  Edit: changed 'Strix Point' to 'Gorgon Point' since that what I meant.,Neutral
Intel,👍,Positive
Intel,"Yes, Strix Point lives *alongside* Strix Halo. Strix Point is already out, and came out *before* Strix Halo at that. It is not the followup to Strix Halo.",Neutral
Intel,You are right. I meant 'Gorgon Point'.,Neutral
Intel,"Gorgon point is just a refresh of strix point. These terms mean less in the next gen anyway as the follow on products have quite different names. Medusa point isn't really the successor to gorgon point, for example.",Neutral
Intel,My brain hurts and I’m still confused,Negative
Intel,That's why you have guys that are completely ChatGPT-levels of confident when they are completely out of whack with the info lol. No worries.,Neutral
Intel,"I think him saying ""unreleased products"" could mean it's still coming.",Neutral
Intel,I think B770 is locked in and to release shortly isn't it? sure I saw a leak of packaging details etc.,Neutral
Intel,"We finally got reviews for the B50 and it is a SFF gem! That said… I wish Intel would get better at promoting upcoming products, they seem to be slowed down by the restructuring.  I’m going with the flow, whatever will be will be. I’m waiting for Dividends to kick back in so I can retire with Intel.",Positive
Intel,"Boy, I am glad they picked an easy naming convention.   /s  After Xe, Xe1, Xe2 no doubt next would be Xe3, then Xe3p  After A-series, B-series, no doubt next would be C-series.    Too bad those generations do not line up",Positive
Intel,Intel is not serious with dGPUs,Negative
Intel,"If it was coming, you expect them to talk about the future even a little bit. Instead, nothing.    They cancelled Celestial over a year ago now. Sounds like things haven't improved since.",Negative
Intel,"I think him saying he doesn't talk about unreleased products is 100% bullshit. He talks about upcoming tech constantly. This is not just unreleased, it's unannounced, unclaimed and nonexistent outside of pure speculation.",Negative
Intel,B50 is interesting once the software is there (planned Q4).,Positive
Intel,"Yes, they are literally just playing. It's all a game lol",Neutral
Intel,"They have released about the same number recently as AMD, I'd say they're pretty serious. The B50 is a pretty compelling product too, big features for the price.",Positive
Intel,They are as serious as AMD.,Neutral
Intel,This interview happened during the quiet period so I don't think he could talk about the future,Neutral
Intel,"They cancelled Celestial over a year ago now. Sounds like things haven't improved since.   MLID people showing themselves.  Tom Peterson previously said Celestial discrete hardware was already done and they were working on Druid. So if they cancelled it, it must have really sucked.  I am sure after they release Celestial 70 series beater, MLID will come and say they cancelled the 90 series beater and it's for sure dead from now on!",Negative
Intel,Where did you hear that it was cancelled?,Neutral
Intel,"We are supposed to get B60s also but they will likely be very limited and part of Battlematrix. Intel is moving very slow with Pro and Consumer GPUs and they can’t rely on TSMC for supply and obviously are not ready to manufacture through IFS? We are getting left in the dark, all we can do is wait.",Negative
Intel,There has been no update regarding celestial dGPUs internally.,Negative
Intel,AMD is skipping a generation to focus on the next. Intel has lost its focus on GPUs. These are not the same things.,Negative
Intel,Lol no. ARC was 0 margin product. Now it's fate depends on the whims of VPs not engineers.,Negative
Intel,"You could likewise point not that there was no word about dGPUs in the PTL presentation either. I think people need to accept that it's just not happening, at least for the foreseeable future.",Negative
Intel,"> MLID people showing themselves.  I'm not getting this from MLID.  > Tom Peterson previously said Celestial discrete hardware was already done  No, that's absolutely false. Actually watch the interview instead of reading reddit comments. He said Xe3 (specifically in PTL), not Celestial, was done. And this was after the PTL tapeout was announced, so that didn't even tell us anything new.   And as we now know, they don't consider that even in the same family as what would be Celestial.",Negative
Intel,Ex-Intel coworkers/acquaintances.,Neutral
Intel,>There has been no update regarding celestial dGPUs internally.  Do you have internal information?,Negative
Intel,Why would they? Battlemage is not even finished. Battlemage is not even 1 year old yet. They will still release B7XX gpus and probably B3XX.  I expect them to tallk about celestial by next year.,Neutral
Intel,> Intel has lost its focus on GPUs  So despite them repeatedly telling you they have not... they have?,Neutral
Intel,"Battle mage is not a 0 margin product...  I know how much silicon cost etc due to my job. Believe me there is at least %30 gross margin in Battlemage and that is assuming somehow Intel got a worse price compared to my small ass company.     It's not profitable due to amount of R&D it takes to develop it, Intel earns a significant chunk for each Battlemage sold. They are simply not as greedy as Nvidia and AMD to earn market share.",Negative
Intel,Dude XE3 even has some test shipment reports etc. It's too late to cancel.   Sure if it's not good maybe we will only see B580 replacement.    But it's literally impossible and stupid to cancel it right now. Especially given how much gross profit they made from B580,Negative
Intel,"So, no news story has come out stating that?",Neutral
Intel,Xe3P-HPM suggests otherwise,Neutral
Intel,"Yes, through my ex colleagues",Neutral
Intel,"No, I don't listen to them, they have a nasty habit of downplaying bad situations. I'm going by their actions.",Negative
Intel,"> Dude XE3 even has some test shipment reports etc  Celestial wasn't base Xe3, and didn't tape out before cancellation. What test shipments are you referring to? PTL?  Btw, they still aren't saying anything about BMG G31, and that was much further along than Celestial was.   > But it's literally impossible and stupid to cancel it right now.  You can cancel a product at any point before it's released. Anything else would be sunk cost fallacy. Surely you're aware of the massive budget cuts and layoffs they've announced. Not everything can survive.   > Especially given how much gross profit they made from B580  By all reports, BMG still wasn't profitable for them. Hell, even if it *was* profitable, doesn't mean profitable *enough* for Intel to keep funding it in this environment. They're prioritizing spending reduction, not profit maximization.",Negative
Intel,Why would Intel tell you? It would just stall selling all current ARC cards.,Negative
Intel,No. Or at least not from any reliable source. Obviously discounting MLID and his ilk.,Negative
Intel,What about it? That some reference exists in drivers?,Neutral
Intel,"You don't even know that?   Lip Bu has been hiring gpu designers not firing them.   Most of the Cuts are from foundry side and slightly from gaudi side.    Intel if anything is focusing on gpus to create AI inference gpus.    Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.      You should check your sources, Lip Bu would cut 14A before he cuts Inference gpu side.",Neutral
Intel,BMG was 0 margin product,Negative
Intel,"Of which are recently implemented, as in 'in the past week' which would be exceptionally stupid to do for a cancelled project",Negative
Intel,"> You don't even know that?  What do you claim I do not know?  > Lip Bu has been hiring gpu designers not firing them.  Celestial was cancelled under Gelsinger, as well as several rounds of client GPU layoffs. If Lip Bu is hiring anyone, it's not to build the team back up again.   > Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.   I am specifically talking about client graphics, yes. There's shared work, and arguably should have been more, but they were quite different. Client iGPU, client dGPU, and server dGPU were basically all separate SoC designs.",Negative
Intel,They're also using Xe3p for NVL-P and that Island AI product.,Neutral
Intel,"It wasn't though?  As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  I don't believe there is nothing Intel can do convince you. I heard these news every single time.     Also just look at job listings, there is many for gpu development.",Negative
Intel,"Which means it is being used, produced, and cannot be disqualified yet, nor does anything, not one trustworthy source, show it is cancelled",Negative
Intel,"> As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  Go ahead and point out where *I* said ACM or BMG were cancelled. I have no idea who ""you guys"" are, nor do I care what others are or are not saying.  > I don't believe there is nothing Intel can do convince you.  Well yes, if they cancel a project, and I know they have, I'll say they cancelled it. The way Intel can convince me to say otherwise is by... not cancelling projects. And you *do* realize they haven't talked about client dGPUs past BMG in many, many months, right? It's not like Intel's really denying anything.  I'm not sure what you're looking for here. Should I lie and pretend not to know what I do? To what end? It's not like Intel's harmed by me saying they cancelled such a project. That's not something you can keep a secret indefinitely, and anyone who *really* cares already knows.  > Also just look at job listings, there is many for gpu development.  If you lay off 10 people, 5 more leave on their own, and then you backfill 4, that's still a net loss. They still need some people, but not as many as they had, and not for client dGPU.",Negative
Intel,"Intel have not claimed there to be upcoming Celestial, or more Battlemage or anything like it. How did their silence convince you of anything?",Negative
Intel,Actually after the Nvidia deal there's definite reason for Intel to cancel future ARC development.  They have a partner that makes better GPU than them. Why would they continue? If the Nvidia deal is successful I expect even their iGPU will disappear.  In no sense it makes sense to develop a line that's redundant with what your partner is doing.,Negative
Intel,"> Which means it is being used, produced, and cannot be disqualified yet   Again, Xe3p lives for various things. Celestial, the client dGPU planned to be based on that IP, is dead. I'm not sure what contradiction you think exists here.    > nor does anything, not one trustworthy source, show it is cancelled   Intel's refusal to talk about future gen client dGPUs and the mass layoffs in the hardware team don't tell you anything? Or what about Gelsinger's remarks about less focus on dGPU?    This is how Intel usually handles cancellations btw. They simply pretend it never existed, unless investors really demand to know. Even then they like to delay the acknowledgement.",Negative
Intel,Think pretty easy naming. Any X infront just automatically means better iGPU,Positive
Intel,"Looks pretty snappy at \~75% faster than the top-end LNL chipset and this is with pre-release drivers.  Granted TDP is probably higher.  If they get the drivers cleaned up then it might release a bit higher which makes it a viable (though still materially weaker) thin and light XX50 dGPU model alternative some some of the market.  Should do well addressing the 'I want a thin and light laptop, but I want it to have an ok GPU' crowd.",Negative
Intel,"Nice to see the G14 with an Intel CPU. Thought the lineup was AMD only tbh, while the larger G16 laptops get Intel.",Positive
Intel,"Panther Lake 12 Xe3 performance looks great to match RTX 3050 laptop performance because the entire chip only draw half power of RTX 3050. Seems like using 18A BPD really paid of to reduce CPU consumption by a lot with the helps of Tsmc N3E for iGPU.    Also it's so weird to see Asus Zephyrus G14 with Intel chip, usually it always has Amd CPU paired with Nvidia GPU. I heard G14 is pretty popular gaming laptop but this laptop during full load can use 120w+ power.    Using Panther Lake 12 Xe3 will makes this laptop looks even more appealing because it reduces power requirements from 120w+ to 45w but still giving about the same GPU performance which is insane. This is a massive game changer for people who use laptop as portable gaming machine and to those who travel a lot. I can totally understand why Asus this time use Panther Lake for G14.",Positive
Intel,Naming for these chips are terrible,Negative
Intel,"Can Asus send me this laptop for review? I have 14 followers on snoozetube and 60% are probably bots, but bots are human too?",Neutral
Intel,Can't wait for 14inch laptops with actually good battery life and convenience than the cheap gaming laptops it's going to kill,Positive
Intel,"Nah, should've kept that info at the end like every other Intel and AMD CPU ever made.  But otherwise this branding really feels like AMDs APU line, where they had to emphasize their iGPU was better than average.",Neutral
Intel,"The possibility of being nearly 100% faster than Lunar Lake in some tasks, and minimum possibly 50% faster while being able to fit it into a sub 3lb/1.5kg design with a 80+wH battery is going to really nice. If the 4 LP-e cores scheduling work well and maybe a more efficient OLED panel you could easily get true 24 hrs use on x86",Positive
Intel,"If the game could be 50–60% stronger, that would be That would be a killer",Negative
Intel,"That’s been true for the past generations, but it looks like it will change this generation",Neutral
Intel,Its GPU part isn’t 18A at all — it’s actually N3E and 4Xe3 integrated graphics use Intel 3.,Neutral
Intel,Still better than Ryzen 365 AI pro MAX+,Positive
Intel,"I disagree, GPU focused = X (like Xe3). Just takes getting used to , but otherwise it follows the same 3 7 9 scheme that probably didn't make much sense at first either :)",Neutral
Intel,https://browser.geekbench.com/v6/compute/compare/5050048?baseline=4771132,Neutral
Intel,The typical consumer doesn't know anything about the last letter. Having it in front will be much more successful to communicate to consumers the difference.,Neutral
Intel,Yea putting ai the model name is disgusting 😂,Negative
Intel,I can't wait for the Ryzen 688S AI Pro MAX+++,Positive
Intel,"I agree with this and now snoozetube creators are doing 128gb reviews for the 365 AI Pro Max+ and glossing over the fact that it costs decent money but lacks any kind of power when compared to discrete GPUs.  Amd continues to pump out expensive APUs that are mediocre, while doing everything related to Radeon half heartedly.  Why is that?",Negative
Intel,"Nerds argue over names for tech products but will eventually figure out some kind of logic in why they named it that way. Entire generations need to be released and compared.   As for average users they will always be perpetually clueless and unfortunately will become influenced by an influencer with no integrity and or a store associate who has been trained on scripts that make the most money for the store.  God help us all, I pray for Jesus - just like Pat Gelsinger, who will get no credit for the Intel turn around.",Negative
Intel,Ryzen metaverse Ai max++ 3D Hypercache macroboost,Neutral
Intel,I think it's rather on point. the 395+ is a beast for running large MoE AI models. It's value for money in that respect is almost unbeatable.,Positive
Intel,you forgot the x3dx2    when both cpu tiles are stacked on 3d cache tiles.,Neutral
Intel,Ultra TypeR S-line AMG M Bi-Turbo CCXR LM Harley Davidson Edition,Neutral
Intel,"Very very few know anything more than that, usually completely unaware that there's a whole SKU number after that.  How many times do you hear stories about some user proudly boasting about having an i7, only to find out that it's like a 6th gen, and they don't even realize / believe that something like a i3-12100 is actually a better CPU.     The average user understands the difference between, say, a Core Ultra 5 and 7, because the ideal of 3, 5, 7, 9 being product tiers exist in plenty of industries, like BMW's product line. Bigger number = more performance. How? By how much? No clue to them.  So since the average user is going based off just the name 5, 7, or 9, having that X visible in a location they'll see is certainly very important. They'll notice the X.",Neutral
Intel,"It would be like a Chromebook named ""Chromebook CloudCompute+"" just because that's what those are built for",Neutral
Intel,I personally prefer them Name it Ryzen 3 / 5 / 7 / 9. It’s easy to understand and easy to compare to intels naming but sadly both companies have ruined it now.,Negative
Intel,That's honestly sounds even more cringe. Can you imagine Amd Ryzen 9 395X3DX2 AI Pro Max+? That's ridiculously bad LMAO,Negative
Intel,BMW Individual M760i xDrive Model V12 Excellence THE NEXT 100 YEARS,Neutral
Intel,I actually did my research and found out that core ultra 5 125u is not much different from core ultra 7 155u... Ended up buying ProBook with core ultra 5 125u and saved money for upgrading the ram and SSD,Neutral
Intel,"but i think its gonna happen aye,  i wonder if the RAM bandwidth needs for AI benefit from cache like games do, or are they better slapping more ram channels on it...",Neutral
Intel,"Yeah, all of the U chips within a generation are the same physical chip, just different bins (usually tiny clockspeed differences). I don't think they even have core count differences any more for the most part.",Neutral
Intel,Same core counts too,Neutral
Intel,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Neutral
Intel,"To summarize from the article for some folks:  6300 puts this GPU as 33.4% faster than the 140V, 71.3% faster than the 890M, and 54.3% of the 8060S. Just over half of Strix Halo's top config.  Bear in mind though, that this benchmark favors ARC GPUs compared to gaming results. The 140V and 890M are roughly equal and this benchmark puts the 140V as 28.4% faster.",Neutral
Intel,"Ugh, who is naming these products?!?!? Between the internal code names (which are now publicly used) and the actual product names, it's a mess. As bad as monitor naming.",Negative
Intel,"Sick, Panther actually sounds good, and Lunar/Arrow on mobile already sounded good to me. Please keep pumping those iGPU numbers Intel Arc engineer bros.  AutoCAD on integrated Intel graphics WHOOOOOO  AMD step it up next gen please, Intel is no longer a static target in graphics, kthx.",Positive
Intel,This sound more realistic and not as good as the 50% better Intel announced.   For Intels sake I am hoping the 50% is real,Negative
Intel,... with early drivers and probably no decent support yet. Its also probably an engineering sample? 😉,Negative
Intel,Wow pantherlake is looking enticing. Should I wait for novalake? Will novalake have same battery life like lunar lake or pantherlake?,Positive
Intel,"It's going to cost like $10,000, right?",Neutral
Intel,"The top lunar lake has 8Xe2 cores, and this has 12Xe3 cores. 30% faster seems… bad? Like why not 50%+ given 50% more cores and architectural improvements?",Negative
Intel,"We have seen how Lunar Lake on MSI Claw 8 AI+ performs on early firmware and drivers, Intel even managed to improve Lunar Lake performance up to 30% after that. I expect 50% performance improvement on Panther Lake is very possible.",Positive
Intel,seee ceerto seeeh,Neutral
Intel,"It really is a mess, someone should've been fired long ago.",Negative
Intel,"The only thing they want you to care about is 3/5/7/9.  If the naming is too clear, then they can't steer you to the higher margin/profit variants. At some level it needs to be unclear. Not defending them, but how things are.",Negative
Intel,Intel wasn't a static target since the 10th gen. They've been pushing (at least in terms of performance) since then. It's mostly the efficiency which stepped back. And they messed up their current lineup in terms of performance and pricing but they've improved for sure.  Next gen seems good for them though. Hope they price it well. AMD isn't zen 1/2 either. They're bumping core numbers too and introducing some new cores as well.,Neutral
Intel,"I mean, why?  Is AMD going to be coming out with anything more powerful than the HX 370 in that power range?",Neutral
Intel,because 50% more cores need 50% more power for 50% more performance.,Neutral
Intel,"There are so many variables here that we have no way of accurately saying that.  It's possible that Xe3 is simply treated more like RDNA in this benchmark, and that the Xe2 140V is unfairly biased towards in that way. If that is the case, the 890M and 8060S may be better metrics to base off, and we do see over 50% gains for a 50% wider GPU than the 890M.  It's also possible this is a power-limited scenario. Like for like on TDP, this would be a solid improvement given the GPU both has to be moved off-tile compared to Lunar Lake, and has twice as many CPU cores to fight for power.  This could be pre-release drivers not getting the true 100% out of the hardware or silicon with non-final clocks still being tuned. The point is there's no way to know for sure.",Neutral
Intel,I don’t understand why you people continue to use that MSI claw to demonstrate “intel improved lunar lake performance post launch” when there are millions of lunar lake laptops that never had performance issue of that one handheld.,Negative
Intel,"they're keeping their jobs because this is technically better than the past. back in the day you'd have model numbers exclusive to a retail store, much less specific to an OEM, because businesses wanted to feel like they got a bespoke deal.   These days the SKU naming is mostly for accounting purposes, while the ""real"" naming decisions are made by the OEMs. basically most people are buying the Thinkpad/Yoga/ROG ""brand"" rather than the specific processor model, which only a much smaller crowd bothers to comprehend. It's like how people buy the Steamdeck rather than whatever APU is in there, which is fairly old at this point.",Neutral
Intel,"Energy efficiency is huge in mobile but I was explicit in saying they were no longer a static target in graphics, where they were not making significant or impressive gains in iGPUs for many years, it seemed. Now they are making one of the best iGPUs on the market.",Positive
Intel,"Possibly yes.   Current AMD handheld chips are better than Lunar Lake performance. Assuming normal cadence. Next generation would be similar performance to Panther Lake.     I was hoping more of a leap frog, rather than similar performance. %50 would be a very clear edge.",Positive
Intel,Is the TDP fixed to be the same between LL and PTL in this test?  I genuinely don't know.,Neutral
Intel,But LNL's TDP is too low compared to H45 cpu,Negative
Intel,"> because 50% more cores need 50% more power for 50% more performance.  insightful, and easy to forget given feature bragging    :)",Positive
Intel,FTFY: because 50% more cores need 50% more power for 35-40% more performance.  Cause it don't scale linearly.,Neutral
Intel,"Mostly because people with LNL laptops are less likely to games since gaming are not the point of those laptops -> less testing, whereas the Claw 8 AI+ is a gaming PC handheld, so it is mostly use for gaming purpose and thus have more people testing for its performance.",Neutral
Intel,"As weird as it sounds actually Lunar Lake improvement mainly comes from MSI Claw not laptop, that because majority people who use MSI Claw give feedback the most which is why Intel focusing on the handheld first then laptop.    Intel even use Claw as benchmark for Lunar Lake compared to laptop. You can read from this article :  https://arstechnica.com/gadgets/2025/04/intel-says-its-rolling-out-laptop-gpu-drivers-with-10-to-25-better-performance/   Also Claw got BIOS update way faster than any laptop with the same chip so it helps Intel to mitigate power and boost behavior to maximize the performance. There is so many bug reports on Arc forum, most of them are Claw users, that's why laptop got benefits too.",Positive
Intel,AMD isn’t coming out with a better next gen iGPU for mobile (Gorgon Point) since it’s a simple refresh. Same arch with same CU count based on rumors.  The generation after will be competing with Nova Lake,Negative
Intel,"Amd handheld with Z2E isn't better than Intel Lunar Lake, you can see the comparison on MSI Claw sub or even on youtube. Z2E in most game is 10% slower than Core Ultra 7 258V, it only won in the game where Intel GPU performs bad.    Not to mention at 17w Z2E losing badly to 258V, Intel is on their own league, it's not even competition for Amd.",Negative
Intel,Intel is gonna have better integrated graphics than AMD,Positive
Intel,I have no idea.,Neutral
Intel,"We don't know what TDP was run here, and both MTL amd ARL H have been 28W outside of the Ultra 9 SKUs. It's entirely possible this was run at 28W, which is also in reach of Lunar Lake's boost envelope.",Neutral
Intel,The lunar lake laptops are tested by reviewers all the same as the strix point ones. Regardless it’s misleading because it’s not “lunar lake” but rather the performance profile/boost behaviour of that specific MSI CLAW that was changed and people act like it’s lunar lake’s drivers doing “30% magic”.   It’s not.,Negative
Intel,"And is there documented so-called “large” performance improvements on LNL systems that already performed as expected on day one (it tied 890m on high power and was always better at low power, talking about real games not 3dmark)? Or was it only bringing the Claw back to where LNL should always perform?",Neutral
Intel,"True. I don't see how the next igpu from Amd going to use rdna 4, it won't even support fsr 4. Meanwhile Intel going to push their igpu tech even further with XeSS XMX 3 with Xe3 and Xe3P, they will be way ahead of Amd in igpu market especially when Intel Lunar Lake already beating Amd Strix Point and Z2E.   Intel also dominating mobile market. Honestly it's not looking good for Amd.",Negative
Intel,Yeah people act like strix point is in that segment..... It's not.,Negative
Intel,🫨,Positive
Intel,Wouldn't be the first time.,Neutral
Intel,"Oh, I guess it be like that.",Neutral
Intel,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",Positive
Intel,"""we are tending to prefer e cores now when gaming""   That's very surprising",Neutral
Intel,If only they release a high level C-card. Battlemage kinda never scratched the mark.,Negative
Intel,"The point of Xe3 being actually battlemage instead of Celestial, this is so horribly confusing. I can't understand what's going on, nor why they would do that.",Negative
Intel,I’m guessing Xe3P will be on Intel 3-PT.,Neutral
Intel,"Tom is a funny guy, love it when he gets camera time",Positive
Intel,Cool hope they will manage to release it soon not in 12 months when everyone will be talking about RTX 6000 and RDNA5/UDNA or leaks about them.,Positive
Intel,"When is the panther lake reveal going to be, CES?",Neutral
Intel,"e cores are surprisingly powerful now, and games are getting more multithreaded. It might be better to spread them out over 8 less power hungry cores than 4 P cores and then spill over to the slower cores. Especially in a laptop environment where efficiency matters and the GPU is usually holding back gaming performance so much that CPU performance is really not that important.  In fact, I bet it's mostly down to not spilling over to e cores from P cores. That's what causes slowdowns and stutters usually attributed to e cores. The engine has to wait a little longer for workloads assigned to e cores than to P cores. The if the main thread is assigned to a P core that's all well and good, but if sub tasks are distributed among the P cores and then something important is assigned to a slower e core it holds up the sub tasks and in turn the main thread. But most importantly, it doesn't do so evenly. Maybe you don't need something assigned to e cores, or the e core task is light and doesn't hold up the other threads. Then things will run at the pace of the P cores, but every now and then you'll have a slowdown.",Neutral
Intel,Wonder if there's some energy star requirements.,Neutral
Intel,"Shouldn't be. That's what the math has always said.  As soon as software can scale to ""many"" cores, the tradeoffs that go into single powerful P style cores are a bad deal. Both frequency and sequential optimizations (like multi-level branch prediction) scale poorly.  Gaming benchmarks tend to have a really strong feedback loop that favors last gen hardware design though. So seeing the benefits to E cores for gaming requires 1.) e-cores exist for a while 2.) some games optimize for them.  Long term, an optimal Intel-style processor design will look something like 4 ""legacy"" P cores + dozens of E cores.",Negative
Intel,It's either N3P or 18AP.,Neutral
Intel,That’s why rumors want Intel to throw away P-cores (Israel team) and only keep E-cores (Austin) to make Titan lake a unified core with only E-cores remaining.,Neutral
Intel,>and games are getting more multithreaded.  Can you give me a few AAA examples of modern games which get a noticeable improvement in AVG. FPS or 0.1/1% lows when using more than 8cores/16 thread's ? I'm genuinely curious which games you are talking about.,Neutral
Intel,"Really should focus on the Main/Sub thread part. Most games will usually only load up 1-3 cores, with the rest of the cores only used for incidental workloads with lower priority and sync requirements (Multithreading is hard yo, Multithreading with latency requirements is mindnumbing).  This makes plenty of games very suitable for the P/E architecture, as long as you have enough P cores for the Main threads, the E cores will be perfectly sufficient.",Neutral
Intel,"Energy Star probably won't be around for too much longer, at least in it's current form.  https://www.npr.org/2025/08/13/nx-s1-5432617/energy-star-trump-cost-climate-change",Neutral
Intel,That makes me imagine in a different reality if Xbox had remained with Intel and NVIDIA and their next generation would use an Intel E-core based SoC with NVIDIA chiplet.,Neutral
Intel,It has already happened. Stephen Robinson - heading the Austin team - is now the lead x86core architect.,Neutral
Intel,"Off the top of my head, Starfield, Bannerlord, BeamNG, UE5 games due to how the rendering pipeline works, etc.",Neutral
Intel,"Most games of the past will only load up a few cores, but that's beginning to change. Cyberpunk 2.0 loads up 16 threads/cores easily, and some others like Battlefield 6 also scale pretty well. If you have less than 16 threads on fast cores like Panther Lake and Arrow Lake, then you can run into issues. Or if your 16 threads are on one die and any spillover has to go across the SoC die like with AMD and probably Nova Lake.",Neutral
Intel,"Seeing how Nvidia working together with Intel to make integrated high end GPU i can see the possibility of Xbox using that chip, maybe in the future.",Positive
Intel,I'm tired of AMD slop consoles and handhelds,Negative
Intel,">Starfield  [Not really](https://youtu.be/BcYixjMMHFk?t=1015), game doesn't benefit from more cores/threads.  >Bannerlord  Sadly, there's no benchmarks of different CPUs that I could find for this specific game, but I do know that this game heavily relies on a good CPU, but without decent data(review), which shows multiple CPUs tested, it's hard to understand the benefits of more cores/threads,  >UE5 games due to how the rendering pipeline works  It's true that UE5 can utilize 8 cores / 16 threads, but more than that? I'm not sure, if possible, provide a review/video which shows that UE5 scale with more cores/threads, so far, it seems that it is limited at 8c/16t - big channels rarely add UE5 games to their CPU benchmarks, but I found Remnant 2,[ and it doesn't show any benefits](https://youtu.be/3n537Z7pJug?t=1056) of more than 8 cores.  I heard that ""games are getting more multithreaded"" like 4-5 years ago, and in most cases, it wasn't true, with more than 8c/16t almost no games scale on CPU-side, and even when they do, in most cases it's a minor improvement over 8c/16t configuration, like 1-3%.",Negative
Intel,"UE5 by default does not support multithreading well. actually I don't think async shader is even considered a default feature yet despite being added two years ago. Only the editor compilation step uses all threads, but it doesn't need to react to user input so it would be more surprising if it didn't use all threads.  If you're seeing good thread use in a UE5 game is thanks to the developer breaking up work with their own engine changes.",Negative
Intel,"Considering AMD’s lag in gaming performance especially in ray tracing, I would be totally onboard with that.",Positive
Intel,Huh????,Neutral
Intel,"12900K being that high in the Starfield benchmark, like actually within error margins next to 7800X3D despite being an older platform with half the L2 of raptor lake and lower clock speed than even 7700X, shows that the game benefits from more cores and threads very much. Yeah 9950X should outperform it technically I guess but the split L3 between two CCDs probably holds back the advantage of having more cores.  Yeah sadly there's no benchmark for Bannerlord. But supposedly it does disperse its tasks to plenty of threads.   Here's some benchmarks showing core and thread usage in a few games. While the benchmark tool doesn't get into the details of how those software behave obviously, but at least according to the graphs some games like Tarkov really suck at distributing its tasks to several cores but some others like Starfield, Cyberpunk, and Space Marine 2 are actually quite good at it.  [Will This Do? — Intel Core i5-14600KF vs. i5-13600KF vs. R7 7700X vs. i7-14700KF Benchmark - YouTube](https://www.youtube.com/watch?v=HY_weucfLPQ)  [Isn't it a blast now? — Core Ultra 9 285K benchmark. Comparison with R9 9950X, R7 9800X3D, and i9...](https://www.youtube.com/watch?v=q1zAX1VNdf0)  I haven't seen a core thread benchmark for UE5 games yet, but that's just how the engine is supposed to work though.",Neutral
Intel,"To be fair, Sony seems to be fed up with AMD lagging behind and are pushing them to do better. Consoles will likely continue to stick with AMD, at least for the next generation. Intel graphics aren't quite ready yet, and Nvidia is going to be way to expensive unless they take an old SoC and repurpose it like Nintendo.",Neutral
Intel,">12900K being that high in the Starfield benchmark  It's because Creation Engine 2 worked better with Intel hardware until AM5 X3D chips, and 12900K great result in this test just shows that it's still a good CPU with 8 Performance cores, not that it's core count matters in any significant way - for example, 7700X delivers identical performance with lower core count/threads.  >lower clock speed than even 7700X  Clock speeds can't be compared between different architectures, different architecture = different efficiency, what matters is IPC.  >shows that the game benefits from more cores and threads very much  [here's a better example where more intel CPUs are present in Starfield test](https://youtu.be/XXLY8kEdR1c?t=29m5s), as you can see, increased core count/threads past 8 performance cores and 16 thread's is meaningless and won't provide any noticeable performance improvements, 6 faster P-cores on Intel 14600K provided better result than slower 8 P-cores on 12900K.   Going from 7950X to to 7700X results in 5 FPS loss, which is like 2-3% less FPS, and it's mostly because of lower clock speed(5.7Ghz Vs 5.4Ghz boost).   >but some others like Starfield, Cyberpunk, and Space Marine 2 are actually quite good at it.  That's the point, I agree that modern games can and will utilize 8c/16t configuration, but so far I don't see a trend of games becoming more ""multi-threaded"" past that point, as I replied to that person, I'm curious what games now become more multi-threaded than few years ago, I find that observation speculative and without evidence - more cores and threads is a great approach for workloads, but games generally don't care about it past a certain point, in this case it's 8c/16t.  It could change with next-gen consoles, if they will use AMD 12 core CCDs and games will be optimized to utilize more cores&threads.  What's important now is good cores or good cores+X3D cache, not core count - even 7600X3D with 6 cores and 12 threads is better in gaming than most Intel/AMD non-X3D CPUs with way more cores.  Edit: typo",Positive
Intel,"Well you said that games are optimized for 8/16 and yet gn bench shows 7600X only gets 3-4 less avg fps than 7700X in baldurs gate 3, a highly multithreaded game, it has better 0.1% lows even, though I'm sure that one is a contained incident since BG3 is very dynamic and hence results are not always repeatable. Same with Starfield.   That's because in video games, core count matters less than how fast the CPU can access or manipulate very dynamic types of data in a random memory address.    X3Ds are not just ""whats important now"". They're going to win in old or new games, and they're winning in games because they have a large buffer of low latency data. Not winning in productivity benchmarks because productivity is about processing matters more than accessing data, and its more easily multithreaded too in many cases.    But having more cores is still more advantageous in software and games that can occupy them.",Neutral
Intel,">Well you said that games are optimized for 8/16 and yet gn bench shows 7600X only gets 3-4 less avg fps than 7700X  I said ""can and will"" not that every game is going to benefit from it.  Point of discussion was to prove that games are using more than 16 threads or they're not - it seems like you overestimated Starfield, UE5 reliance on core count/threads and most games care about 16 threads at most.   >core count matters less  Yes, that's why I replied to that person and asked him to provide me with modern AAA games which are more ""multi-threaded"" now than AAA games a few years ago, I feel like what he ""observes"" is what he wants to believe, and not something that actually happened with optimization in games.  >X3Ds are not just ""whats important now"".  AMD sells 9950X3D, best productivity and gaming CPU, if you really need both(workloads/gaming) it's the optimal way, for an average user, only benefit of E-cores is lower power draw when idle - but they do matter more if you need those workloads, I agree, but we discussed gaming performance and core reliance.",Neutral
Intel,If Panther Lake is going to be part of Xe3 then what does that mean for a possible B770?,Neutral
Intel,"What even is Intel naming at this point, before it was the one thing about their products that was at least *kinda* consistent.",Neutral
Intel,"B770 is probably likely coming out at the end of this year, I don't see any reason why they would not sell it.",Neutral
Intel,Seems like Intel chips make way more sense for portable pc consoles than AMD going forward.,Positive
Intel,"Transcript for the 'What's Next' section:  >What's next? Well, I I would say uh larger investments in graphics and AI. And so if you imagine in a PC, this workload is not going away, right? The world is becoming more graphical. The world is embracing AI and more rapid than any other technology ever. So I would expect Intel to respond to that, right? It's not like we have to guess what is what is big and what is coming. We know and what's coming is more AI and more graphical experiences.  With this being said, I hope that means that the Nvidia/Intel deal means that the ARC division is not going away any time soon.",Neutral
Intel,"Panther Lake with Xe3 without any doubt will be generational uplift. Intel Lunar Lake which is last year chip still able to put Amd newest chip like Z2E to the shame.   Xe3 with 12 Xe cores not to mention with XeSS XMX is going to be terrific combo, seeing how disappointing Amd Z2E i can see Intel going to steal iGPU gaming market from Amd. Next year could ended up with more OEM using Intel chip for handheld.",Positive
Intel,I found it interesting that he specifically mentions improvements in performance per area for Xe3. A big complaint of Xe2 is that the die size is noticeably larger than a similar performance GPU from AMD or Nvidia. Hopefully this means better financials for Intel’s graphics division with this new architecture.,Positive
Intel,Says nothing about discrete graphics. Think that's clearly dead at this point.,Negative
Intel,"Lmfao now I’m feeling conflicted about my very recent MSI Claw purchase, but I suppose panther lake handhelds will likely launch middle of next year anyway",Negative
Intel,"Tom says that variable register allocation and 25% thread count per Xe core will improve utilization which was a problem with predecessors. He says they've been ""addressed"". It looks like at least perf/power wise it's 25-30% at the same process compared to the predecessor, and that may be true for perf/area as well.",Neutral
Intel,Sounds like you're clearly wrong.,Negative
Intel,"I know how you feel but honestly Claw 8 AI+ is still amazing handheld, at least your Claw will aged better than mine because i only have A1M Ultra 7.   Claw with Panther Lake will be released in between Q2-Q3 2026 so you don't have to regret anything.",Positive
Intel,"I get the feeling that handhelds with 12 Xe3 cores are going to be really expensive, so maybe you still didn't do too badly.",Negative
Intel,"Lmao, sure. Any day now...",Positive
Intel,I don't think so considering how expensive LNL was,Negative
Intel,"Why is panther lake igpu simultaneously Xe3 and Arc-B series ? This is so confusing.       Also it states being 50% faster, but it also has 50% more compute units.",Negative
Intel,What should be the desktop gpu equivalent?,Neutral
Intel,So around a 3050 laptop performance?,Neutral
Intel,"Xe2 wasn't a benchmark btw. Arc 140T is hot, can""t manage benchmarks staing under 100w, just to pair with an 980M.",Negative
Intel,The problem is how much is the price?,Neutral
Intel,they should really do better on GPU,Neutral
Intel,"Despite the vast improvement more Xe3 cores than we saw Xe2 cores in Lunar Lake, we're only seeing minor gains in average framerate performance at the same wattage based on the internal benchmark, though there are much more meaty improvements in the 1% lows. I guess its bottlenecked by its RAM bandwidth. Also I'm sure Xe3 will scale better beyond 17W. I wonder how the 4 Xe3 SKUs will fare, that's a good way of knowing how much RAM bandwidth affects things.",Neutral
Intel,"Just because you have 50% more physical units, doesn't mean you'll automatically get 50% more performance. Remember when the Z1E came out and it was supposed to be 50-100% more power than the steam deck?",Neutral
Intel,Because Xe3 is like Xe2+,Neutral
Intel,RTX 3050 Max-Q or 1660 ti Max Q laptop card. Within that range.,Neutral
Intel,Yes,Positive
Intel,"For them to do a 'big iGPU' design they would need to do a few things:  1. Use a different socket.  The iGPU will get too big for the existing one.  2. Upgrade the memory bus to 256bit.  That uses some power and silicon.  3. Add cache to the chip for the iGPU to use (to make up for the poor bandwidth they get from LPDDR5, even with a wider bus)  If they don't do this then the chip (which again won't fit in their default socket) will be massively bandwidth constrained to the degree that it's pointless.  Basically, do all the stuff that Strix Halo did.  Issue is Strix Halo didn't sell very well, the idea is a proof of concept and the concept needs faster memory to really work well.  LPDDR6 might help a lot at getting these big chip ideas from a XX60 level to a XX70 level, but they still require a lot of expensive chip work and a new motherboard design custom to the big socket.  I suspect we'll get there eventually, but it'll be a few years before people start calling it quits on dGPUs.",Neutral
Intel,"50% is not minor...  Also they said >50%, or greater than 50%. It looks like 70-80% to me.  Since scaling isn't linear I expect 4 Xe3 to come close to Xe2 or even equal it.",Neutral
Intel,Maybe I'm confused about what they're advertising but weren't they just going head to head with 4060s with the b580? Wouldn't 3050 range be a huge step backwards?,Neutral
Intel,"Damn, still behind my old 1060 pc",Negative
Intel,And some of the tiles are internal not on TSMC.,Neutral
Intel,"On-package memory doesn't raise the device price from and end user perspective. It's a margin challenge for Intel because OEMs want the margins from the memory, so Intel needs to pass it along at cost.   Technically, on-package memory can even be cheaper because it can let you simplify the rest of the PCB. Iso-speed, that is.",Neutral
Intel,Are you saying you expect people to not want discrete gpu's in the future?  What about the direction of gaming would make you think that?,Negative
Intel,"> Since scaling isn't linear I expect 4 Xe3 to come close to Xe2 or even equal it.  The 4 Xe tile is on Intel 3, so it'll have a significant clock speed deficit vs the 12Xe tile.",Neutral
Intel,"No I'm talking about the internal game benchmarks they shared. Yeah the GPU architecture  itself is a lot better on paper, but they probably got bottlenecked by the low bandwidth of 128-bit LPDDR5. The uplift in 1% lows in some titles is impressive though.   [power-efficient-gaming-26.jpg (2133×1200)](https://www.techpowerup.com/review/intel-panther-lake-technical-deep-dive/images/power-efficient-gaming-26.jpg)  And yeah I'd not be surprised if 4 Xe3 is going to be closer to the 8 Xe2 in LNL than people would expect.",Neutral
Intel,The b580 is a full power desktop card. The one they're advertising here is a tiny little integrated gpu on a laptop processor,Neutral
Intel,Well it's not out yet so these are just predictions. Better to be conservative with estimates than overpromise.,Neutral
Intel,"We already knew they cancelled desktop variants though, Panther lake was going to be around Arrow Lake laptop performance no matter what.",Neutral
Intel,"If people can get a XX60ti level iGPU in a laptop, which means you don't have to worry about MUX switches (hardware or software), possibly have a cheaper platform (if scaled) since you don't need a separate GPU board, have access to a ton of RAM (many mid-range GPUs are RAM limited), and due to reduced complexity and the ability to better dynamically manage power can get a more effective slim chassis with less throttling... then yeah, it'd be pretty good.  The next big jump is LPDDR6 which looks to potentially hit a 50% bandwidth increase and marginally lower latency.  Combine that with a wide bus and you're looking at enough to power a XX70 series mobile chip.  The framework is immature but it could be competitive in the future.  We'll see!",Positive
Intel,"Buddy, that's 140V with optimizations. They are telling you what they have done to further optimize their graphics, including drivers and power management. and that they'll further apply this in the future. Did you really look at the slide?",Neutral
Intel,"This benchmark has nothing to do with panther lake, it's about the effect of their power management software on lunar lake before and after it is applied.",Neutral
Intel,Isn't that lunar lake before and after?,Neutral
Intel,It has around 120GB/s of bandwidth.  Thats pretty close to the 6500XTs bandwidth.,Neutral
Intel,"Dota2 mentioned, I like Intel.",Positive
Intel,"this has been leaked a lot, but not confirmed",Neutral
Intel,"Oh laptop gaming sure, but that's pretty low level gaming. Don't most of the people at that level just put up with whatever laptop/prebuilt stuff is available anyway? I really don't think it's going to move the discrete market that much?",Negative
Intel,I was missing the context. Thanks for the heads up! Yeah that last update for LNL driver was mad good.,Positive
Intel,Oh really? Wow thanks for the heads up. I really missed the context.,Negative
Intel,Apparently so! I made a mistake.,Negative
Intel,It's 153GB/s.  9.6GT/s x 128 bit width x 8 bit/byte.,Neutral
Intel,\>HUB  That's shovelware and I'm not clicking that shit.,Negative
Intel,"They've tested it with just three AMD cpus. Weird pairings. I would pair B580 with any i5 from last couple years. Seriously their testing is getting less and less ""real life"" and more for the purpose of a good clickbait.",Negative
Intel,"I couldn’t believe the video I was watching, the thumbnail was pure clickbait. They fixed one game, and improved some others a little.  And NEVER an Intel CPU in sight. I like the B580, but it deserves better ""testing"" than this.",Negative
Intel,The problem is still here. 9060 xt with 5600 is faster than B580 with 9800x3d and B580 still loses 8% with 5600 instead of 9800x3d. Which means that cpu bottleneck starts much sooner for ARC gpu.,Negative
Intel,Will I be ok to run an intel arc b580 and an intel i5 8400?,Neutral
Intel,"Is this related to historically Intel compiler not compiling code efficiently for AMD cpus? Does it also make Intel CPUs overhead better? If yes, then they really fixed something. If no, then they wrote code normally for AMD cpus as well as Intel cpus.",Neutral
Intel,"the B580 is a modern gpu, it should be paired with a modern cpu, preferably an Intel one, going with old school Ryzen cpus is a bad path to take.",Negative
Intel,"The problem is that this title makes it sound like they fixed the cpu overhead issue in every game, but that's not the case. Some games are still affected. Obviously any fix is better than nothing, but depending on what games you want to play, you will still see an overhead issues. It is nice though that in some of the games the overhead problem has been addressed though.",Negative
Intel,"Haven't watched the video, but damn.   If rue, HUB has officially become clickbait garbage. Not outright lying, but intentionally conveying misleading half-truths. Starting at least with the AMD fine wine video.",Negative
Intel,"The exact CPU model is hardly relevant. The point was to show CPUs of a certain power level. Not sure how this makes it less ""real life"" as if all three CPUs shown weren't extremely popular for their time. Also not sure what one would achieve when using Intel CPUs specifically.",Negative
Intel,"These guys absolutely *refuse* to test with an Intel CPU, it's very weird.",Negative
Intel,"5700x would have made a lot of sense compared to 5600  12400f,13400f should be included as well",Neutral
Intel,Exactly. Its nice that some games were fixed but between that and calling the issue (entirely) fixed there is a big gap.,Negative
Intel,"Indeed and it is something very interesting, because the reason the driver overhead is an issue on 6 core cpu's is because modern games use all 6 cores, so if the driver has a big overhead the CPU has nu free cores available for the driver.  Especially Intel CPU's with more (e-)cores in their budget CPU's should probably suffer a lot less.   I would expect an i5-12400 to suffer the same as a Ryzen 5 5600,  But an i5-13400/14400 has an additional 4-e cores to deal with the overhead. The 13500/14600 even has 8 e cores.      I would very much like to see the comparison   i5-12400, i5-13400, i5-13500, Ryzen 5 5600(X)",Positive
Intel,Would not have mattered. the cpu bottleneck in the intel driver is single-thread limited and largely due to draw-calls being stored in system memory as a buffer for the frametime render. (at least as far as my debugging could lead me)  (that's also why its less impacted on ddr5 systems no matter what CPU),Neutral
Intel,"They pushed the 6 core parts too hard as ""good budget parts"" for too long to not keep using them as their reference...  Even when we're seeing games that now have strokes when they don't have 8 cores to play with.",Negative
Intel,"possibly. Honestly weird as hell to not through in intel cpu results with the same gpus so they can see if it's a all cpu overhead issue, or an issue with amd cpus with intel gpu. Like even one game if intel/amd gave about the same performance you could likely rule it out somewhat.  Also weird to say they fixed it if by the sounds of it to say it's fixed if a very limited number of games were updated to work better in also seemingly limited scenarios. That's more like finding a specific problem on a specific game they were able to improve rather than an underlying fix which if implemented should help everywhere.",Negative
Intel,"No, B580 also performed poorly on i5-8400 and 10400 (both are 5600X-esq)",Negative
Intel,"The driver compiler is for GPU, not CPUs.",Neutral
Intel,9800X3D is the fastest CPU for gaming on the market. The usage of Intel or AMD CPUs doesn't inherently matter.,Positive
Intel,It's been official for awhile. Funny how it takes a positive Intel title for people to realize.,Neutral
Intel,Do you have peer-review data that disputes the data they presented?  Their video covers the scope of what got fixed.,Neutral
Intel,Welcome to the age of influencers...,Neutral
Intel,"Nvidia 5000 series has overheads issues even with 5800x3d... like 20% performance hit, you can fix it by capping fps or using Intel cpus. AMD CPUs always act weird when reaching 100% utilization.",Negative
Intel,That's not how CPUs work in games.,Negative
Intel,Then they optimized drivers.,Neutral
Intel,Do you have peer-review data that confirms it?,Neutral
Intel,The video shows the B580 not having driver overhead issues with the 5600X but we don't know if it did previously on anything other than Spider-Man. His tone suggests the 5600X performance used to be worse but he provides no data other than Spider-Man to suggest it was for other games.,Neutral
Intel,The fact that a switch from a 5600 to a 5700 with hardly a frequency gain eliminates the overhead issue suggest that is kind of  how games/drivers work.,Neutral
Intel,Which also fixed performance on 5600X,Neutral
Intel,"But that's now how games utilize CPU's.  What you get from games is usually 1 main thread that takes up one big core, then many less intense threads, that may or *may not* utilize the other cores, and to varying degrees. AKA you can have mainthread use 100% of core 0, but cores 1,2,3,4,5 are all 50% used, leaving 50% for other tasks to run freely.",Neutral
AMD,"TLDW:   Model used: XFX Swift Gaming RX 9600 8GB (Dual fan, OEM only)     A) 7 game average (1080P/1440P Low/Medium):      The RX 9060 8GB:      - Offers similar performance to the RTX 5060 8GB      - Is 13-14% slower than the RX 9060 XT 8GB      - Is 23-26% faster than the RX 7600 8GB              B) 7 game average (1080P/1440P High/Ultra):      The RX 9060 8GB:      - Is slightly faster than the RTX 5060 8GB (~4-6%)            - Is 13-14% slower than the RX 9060 XT 8GB      - Is 26-29% faster than the RX 7600 8GB",Neutral
AMD,This should have been in the Steam Machine.,Neutral
AMD,Feels like this is what the 9060XT 8GB probably always should have been.,Neutral
AMD,"NGL for 200 usd thus would be best budget gpu, would shit on b570",Positive
AMD,misread as 960 and it bought me back to the college day when the debate was RX 380 vs GTX 960,Neutral
AMD,This is very shallow. Few games and results. I'll wait for a real test.,Negative
AMD,You forgot the most important bit:  * **OEM-only product**,Neutral
AMD,how is it 13-14 percent slower than the 9060 xt and 5 percent faster than the 5060 when the 5060 is only about 5 percent slower than the 9060 xt to begin with.  [https://www.techpowerup.com/gpu-specs/geforce-rtx-5060.c4219](https://www.techpowerup.com/gpu-specs/geforce-rtx-5060.c4219)  edit: nvm I see only a 7 game sample size so this data is not really useful,Negative
AMD,"Depends on price. Word on the street is that AMD gave Valve a reaaally good deal on 7600 dies.   Essentially it appears that AMD produced way too many Navi 33 dies, using an older node (6nm) to make them cheaper and more plentiful, expecting to put a lot of them in laptops. Apparently, OEMs preferred the more expensive (and more efficient) 4060 instead, and that left AMD with lots of Navi33 (RX 7600) dies on their hands.  Talking to AMD Valve probably saw an opportunity to take this excess capacity for cheap, and AMD saw an opportunity to recoup some cost, rather than risk throwing them away. I can imagine Valve getting those RX 7600 cards for maybe half the price of what a 9060XT would be (depending on memory prices).",Neutral
AMD,"Probably would have cost $100 more, but I think it would have been worth it. Almost PS5 Pro performance would have looked better. Plus better upscaling.",Positive
AMD,"All is forgiven if the steam machine is under $500. Anything more, and yeah, i agree.",Neutral
AMD,"Would have made a lot more sense.  Would have been first of it's kind though, no ODM with existing designs to produce the board, AMD clearly do not want this chip in anything other than add in boards etc.  It's strange since it seems like it would be a good chip for laptops, mini PCs etc.",Neutral
AMD,"Exactly, the Machine could have been ever so slightly bigger or fan slightly noisier to accommodate +20W TDP.",Neutral
AMD,"It probably will be in the OEM ones, maybe Lenovo makes one.",Neutral
AMD,AMD should just make a 9060 16GB variant,Neutral
AMD,Amds focus on high bin output this gen made even bad batches selleable quantity. Shame amd never allocated much production for this gen for AI.,Negative
AMD,Yup $199 shud be perfect,Positive
AMD,Back in the good old days...,Positive
AMD,An OEM steam machine with phone of these would be sweet but probably expensive.,Neutral
AMD,Thx added.,Positive
AMD,"It's Navi 33, but with 28CUs it's more like an [RX 7600M](https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-7000-series/amd-radeon-rx-7600m.html#amd_support_product_spec) than a [desktop RX 7600](https://www.amd.com/en/products/graphics/desktops/radeon/7000-series/amd-radeon-rx-7600.html)",Neutral
AMD,So the new Steam Machine is a PC McRib?,Neutral
AMD,"Someone on this platform tried to convince me that valve is willing to spend crazy r&d money on AMD to get a custom GPU solution akin to strix halo for the steam machine.  I told em valve is smart financially and will never do that and will just do the same exact thing as the steamdeck, pick an existing chip that AMD needs to dump off at an exceptional price. The 7600 system that's leaked will be what you see and what you get.  Well well it's almost as if that particular redditor does not have any business experience at all and valve is a multi billion dollar for multiple reasons including being financially smart.",Negative
AMD,"The issue is that 3060 performance with outdated upscaling, severe lack of machine learning capabilities meaning no FSR4, No FSR Redstone and very outdated RT capabilities is not exactly a great look for what will sell as a brand new machine in the year 2026.  It really needs to be priced very low to be competitive.",Negative
AMD,"i dont think its full fat navi 33   that is 32 CUs, this thing is 28 CUs  so its rejects on top of navi 33 rofl, IE 7600M/7400 class.  now, if that is fulfilled with actual salvage or cut down from fully functional that is another question.",Neutral
AMD,Made the most sense,Positive
AMD,Sure if its a really good price it makes sense..,Positive
AMD,"And for this, I expect Steam Machine to be in a reasonable price range. It’s the same for both Machine and Frame, there are very clear signs of compromises for cost purposes that Valve didn’t take for something like Index. The whole product was planned with cost in mind and while I don’t expect a subsidized price, I don’t think it’s going to be too expensive.",Neutral
AMD,I too watch MLID 😆,Positive
AMD,"They're still selling 6600 new. And for some reason, 7600 never dropped to 180$ like 6600 did now almost 2.5 years ago...",Neutral
AMD,of course they did. They would have to throw away the 7600m dies otherwise because noone wanted them.,Negative
AMD,Eh. I'm betting on $600 for the entry level version and $750 for the 2TB version.,Neutral
AMD,"How is this even possible. The individual components today cost more than $500, not to mention the 2TB option will be easily more expensive than it was when the Steam Machine was first announced.",Negative
AMD,The fact that they’re using salvaged Navis means they’re selling jack shit 7600m dies. Which is to nobody’s surprise given how few laptops exist with 7600m. Oems do not want amd dgpus on their laptops.,Negative
AMD,This would make the SM $100 extra and for sure the CPU would have been a bottleneck for this card.,Neutral
AMD,Why?   This is a 1080p card.   8GB is sufficient for its performance.,Neutral
AMD,think thats why they did the rx 7600M,Neutral
AMD,"That's true. OEMs actually want this at least a little, unlike the old 7600m",Positive
AMD,"Exactly. AMD apparently printed lots of mobile 7600 dies, and sold jack shit. So now they are trying to get rid of them",Negative
AMD,"Yep, appears to just be a 7600M with +20W TDP.",Neutral
AMD,Also AMD still doesn't have any mobile RDNA4 parts,Negative
AMD,RX 7400 too.,Neutral
AMD,"McRib with surge pricing, yes",Neutral
AMD,They never said it used an APU.,Neutral
AMD,Kek Strix Halo is expensive AF. And if would be RDNA3(.5),Negative
AMD,"I saw an interesting video yesterday where the guy tested a Steam Machine-like config (6 cores, RX7600) to see how it performed and yeah... not good when RT involved at advertised 4k, Indiana Jones was unplayable.  Even non-RT loads required very low settings.  New games are going to really test that hardware, I think RT is a total non starter on it as well.",Negative
AMD,Exactly.  I wouldnt consider a 3060 useable without dlss working wonders.  Especially at 4k where you need dlss performance or ultra performance.  Trying to do that without a proper upscaler is going to produce hideous results.,Negative
AMD,"$1000 also isn't a great look for a console killer. Between Sony releasing the PS5 Pro at a really steep price ($750/€800), both Microsoft jacking up the Xbox Series price (X for $650) and Sony doing the same (PS5 for $550) and Nintendo charging quite a bit more for the Switch 2 ($450) I'd say there's an opening here for a Steam Machine to enter the console market.  If Valve can release something that can compete with these consoles, at a similar price, with a similar feature set, but with also all the advantages of being a computer, then it's definitely worth it. Going from $1000 to $700 will open a huge market for Valve. Going to $600 would be the nail in the coffin for Xbox.   No online fees, lower game prices, your game library will never expire, infinite game catalogue, and you can use it as your main PC as well, for the same price as a normal console... Not a bad sales pitch.  BTW, lots of people are using a Steam Deck as their main console. If millions are happy with Deck performance, this is more than enough. If you want better, build your own, unlike with classic consoles, you can easily do it now.",Neutral
AMD,I doubt there are enough defective ones to be all salvaged. It'll be like the consoles where they're cut back to ensure as many dies can be used as possible,Negative
AMD,"I'm hoping $600, unless RAM prices fuck it all up and it becomes $700-800",Neutral
AMD,Valve isn’t buying individual components at retail price.,Neutral
AMD,More and more games demand RT and even software RT is vram heavy. Doom the dark ages can run much better on the 7600xt compared to its 8gb version even though they're the same card with only mild clock differences,Neutral
AMD,"If the 9060XT can utilize 16GB, so can a 9060. The performance difference is imperceptible without a framerate counter.",Neutral
AMD,"Plus, much easier to integrate into a smaller form factor than shrinking down a desktop GPU. The Steam machine is basically a screen-less laptop hosting a Ryzen 7640 (without iGPU) plus an RX 7600M, but with a huge-ass heatsink and fan to properly allow for higher clocks.",Positive
AMD,It's honestly hilarious. AMD will have zero FSR4 compatible mobile product at all until 2027 without hacky aftermarket solutions. Mind you laptop sales began to outpace desktop about two decades ago.,Negative
AMD,Remember KFConsole,Neutral
AMD,"Some people are just that delusional, best case scenario AMD gives valve extremely faulty strix halo chips that are cut down by more than 50%. That individual straight up said it will be fully custom.",Negative
AMD,"Except it's not even a desktop 7600, it's a mobile 7600m. Another 4CU cut.",Negative
AMD,"I think this thing makes no sense at all.  Especially since you re not just buying a new low end gpu to put into an old pc for cheap to give it some new life.  No , you re buying cpu, gpu, ram, ssd, mobo, case, power supply  Thats a ton of money.  And the performance wont justify the price at all.",Negative
AMD,"The 4k claim is really weird. On old games, sure. But am 8GB 7600 is a 1080p GPU, which isn't even that bad on a TV, especially since upscaling 1080p content is kind of a TV's bread and butter",Negative
AMD,"599 is ideal but I’d tolerate 699. Speaking of the RAM shortage, I get reminded of how Valve launched Steam Deck during the worst of COVID shortage, so I am somewhat hopeful",Positive
AMD,"Cool, but individual components involve free assembly by you, steam machine needs assembling and to make money.",Neutral
AMD,"Slapping the chip from the RX 9060/XT onto a laptop style motherboard would not be technically any more difficult than using the 7600M chip. The 7600M is itself just a slightly cutdown, underclocked desktop RX 7600 chip.  Both chips are physically pretty much the same size (200 mm2 give or take a few) and the desktop versions have very similar power usage.   The only reason to use the RX 7600 over the RX 9060 chip in this application is cost.",Neutral
AMD,"Yeah he mentioned that... result will be worse. 4K60 is going to be a very limited thing, marketing beware etc but I hope people are prepared for a little/lot of choppiness.",Negative
AMD,"Because of frame gen and upscaling, they can say it’s any output resolution any output frame rate and it’s technically not lying. It’s a useless piece of info without details.",Negative
AMD,"The steam machine doesn’t need to make money. The steam machine needs to get people to play PC games, which steam has an effective monopoly on. If Valve sells the steam machine at cost they’ll make money.",Neutral
AMD,"I agree, but it's still more work than just slapping the existing laptop board that's already engineered with the proper footprint, connections, inputs and outputs in there, because AFAIK there's no RDNA4 laptop parts at all and the most we have are the RDNA3.5 GPU in the PS5 Pro and the AI APUs",Negative
AMD,Any corporate mention of resolution and framerate is completely meaningless now because it only refers the output. It could be achieved by upscaling from 540p and with 4x frame gen (amd doesn't use it atm) and it still wouldn't be technically lying.,Negative
AMD,Frame gen and upscaling can't solve a lack of VRAM in many games either though (frame gen makes it even worse). Unless they're talking about FSR1 lol,Negative
AMD,"Yes it does, it can't be loss leader because otherwise people will just buy it for office use. They said it themselves.",Neutral
AMD,"Who is buying a steamOS machine for office use? Which office is going to bother buying this instead of any of the hundreds or thousands of offerings from T1 SIs that actually provide support and service? This thing doesn’t even use standard components, are you even going to get official drivers on Windows?",Negative
AMD,"They promoted the steam machine as being able to install other OS. If they don't support it then I suppose at least it would be really funny for how much people glazed them for it.   There will be companies buying it if it's cost effective due to valve eating costs. Hell, even if they don't, valve just have no control over it's userbase, it could have a really low acquisition rate from non steam users, just like steam deck, so all that loss leading would be for nothing.  If you are right then it's a pc, that you can't upgrade, that can't play hugely popular games like league of legends, GTA, battlefield or valorant. Device designed not for casuals and not for enthusiast. Just people who are okay with paying a lot of money to play old, locked in time, singleplayer games on TV. It has to support windows.",Neutral
AMD,"No sizable company is buying a ton of computers to replace the OS on them, especially not for this level of hardware. No normal user is buying a Steam Machine running SteamOS from Steam because they really want a Linux desktop.  The problem with charging a PC price instead of a console price is that it’s just worse than an actual gaming PC. Even with all the advances Valve has made to Linux gaming you still can’t play a lot of popular multiplayer titles, and hardware wise it’s essentially a 5-year old console. There’s just no justification to pay more than $500-600 for one of these.",Negative
AMD,"It's astronomically worse on Optane. On Intel, the issue can be somewhat alleviated by disabling C-states completely - Intel have a lot of documentation recommending this for Optane drives.  The other part of the equation is Windows 11 Virtualisation defences - both user level and deep system level need to be disabled to claw performance back. I have been battling this on my 900p's and 905p's for some time, but I did manage to claw back most of the performance on a 12900K system.  I am disheartened to see that AMD is even worse.",Negative
AMD,"This has always been the way of things. The problem with Ryzen's chiplet design is that latency between cores, IO die, and devices is unusually poor, which you'll see the effects of in latency-sensitive tasks, which includes your rapid random IO to and from SSDs.  In terms of a fix, you can probably claw back some performance by disabling some power saving features and clocking IF as high as it will safely go, but it's an inherently poor architecture for that sort of workload, and you can't really tweak and configure your way around that.",Negative
AMD,i think it's related to ryzen design which uses serializer between compute and io die which is slower than unserialized connections.  [https://youtu.be/maH6KZ0YkXU?t=468](https://youtu.be/maH6KZ0YkXU?t=468),Neutral
AMD,"AMDs Chiplet approach with their infinity fabric is fundamentally flawed when it comes to high end I/O.  I don’t know why no one wants to talk about it but this has been an issue for ages now.  Intel on the other hand has put a lot of work into optimizing the I/O performance.  With something like optane SSDs with cherry picked benchmarks, there is a huge gap between them. With spdk intel managed to get higher IOPS on two CPU cores than a 128 core dual socket could achieve with all its cores.",Negative
AMD,"degraded? please go back to the article and read it  >""Running on our AMD platform, we find our test subject exceeds PNY's up to a sequential read performance quote of 14,900 MB/s. Impressive.""  while it does lose a bit of performance in some cases - it gains performance in others, which probably indicates the test platform that validated those speeds was probably not Ryzen 7000/9000.  >Our Intel platform falls just short on random read throughput and is well short on random write throughput. However, our AMD platform exceeds factory spec for random read, but again falls short with random writes.  and probably not Intel Ultra 200 series, as that platform also fails.",Neutral
AMD,"It could be a temperature thing; they say that they ""employ an M.2 AIC for testing on [their] Intel Core Ultra 9 285K platform"". TweakTown doesn't seem to report temperature either.",Neutral
AMD,"Given the fact than the Intel test system is an Arrow Lake, they most likely hitted THIS: https://www.tomshardware.com/pc-components/cpus/intel-arrow-lake-processors-bottleneck-pcie-5-0-nvme-ssds-by-16-percent-limiting-peak-speeds-to-12gb-s-instead-of-14gb-s",Neutral
AMD,I thought my Optane 905p was a lot faster in 4k random read (~250 MB/s to ~400 MB/s) because the benchmarking app was maxed on single core workload when I upgraded from 2700x to 12700KF. I guess it wasn't that...,Neutral
AMD,Wait really? I have a 905P as a boot drive on my Threadripper system. What changes should I make for more perf?,Neutral
AMD,"It's amazing how hardware design can hide these latency deficiencies. It's why people were genuinely suprised when Intel scaled well with DDR5, and AMD didn't.  The average person assumes all things are created equal.",Negative
AMD,"Latency between the IO die and the chiplets, and the IO die to the IO would affect sequential storage workloads more, or equally compared to random storage workloads. This explanation is straight up untrue.   Besides the point, the 50ns interchip latency caused by the SERDES interface is such a tiny fraction of the total latency to get to storage, that any actual effect it has would not come up in numbers like this.   Correlation is not causation.",Negative
AMD,But most users will never see the difference.,Negative
AMD,it's the serializer thing   [https://youtu.be/maH6KZ0YkXU?t=468](https://youtu.be/maH6KZ0YkXU?t=468)  hence i guess the ssd will perform better with amd laptop apu or strix halo,Neutral
AMD,"Yep, Zen 6 is rumored to be getting a new IO die, which should help here.",Positive
AMD,Any chance you could be the one to talk about it? If you don't have hardware on hand maybe you could write up a blog describing the issue and asking for people to run a test? I'm sure someone would pick that up for the views,Neutral
AMD,"The sequential scores are normal. Meanwhile, it’s the random scores that AMD is significantly lower than Intel.",Neutral
AMD,"Look at the summary, all benchmarks, including non synthetitcs:  Intel platform: 26 862    AMD platform: 19 747  Some synthetitcs are good, but on AMD it falls behind in real usage benchmarks.  I don't know why you downvote me for saying this. I don't like this situation, but downvoting will not solve the issue.",Negative
AMD,Maybe take your own advice pal,Neutral
AMD,"Would be great to see benchmarks focused on this issue, with same coolers, memory etc.",Positive
AMD,Users who actually need 16 cores likely will. That’s a lot of data to keep feeding into a high core cpu.,Neutral
AMD,"The sequential scores are normal. Meanwhile, it’s the random scores that AMD is significantly lower than Intel.  For example, my Kingston Renegade G5 only got about 80/250 MB/s on the CDM RND4K Q1T1 test in my AM5 rig, which is 1/3 lower than the score on LGA1700. And yes, I’m sure my 9800X3D and X670E are both capable of PCIE Gen5, and the SSD was in the right slot. The temperature was also always under 50 C.",Neutral
AMD,"Here's something that'll probably make you even less happy: [https://www.reddit.com/r/intel/comments/1gdib7e/a\_regression\_that\_most\_reviewers\_missed\_loading/](https://www.reddit.com/r/intel/comments/1gdib7e/a_regression_that_most_reviewers_missed_loading/)  The test you linked used Arrow Lake, the 4K random performance (which is what truly matters for loading times of most stuff you run) is significantly worse on Arrow Lake than Raptor Lake.",Negative
AMD,"In ‘real world usage’ it will feel fast on both platforms. If you look at the charts, AMD wins some and Intel wins some.  I don’t think there is anything to be done because the platforms are designed differently in how the PCI-E port connects to RAM.",Neutral
AMD,I didnt :),Positive
AMD,They can improve it with better packaging and stuff like glass substrates. Hopefully completely with advanced silicon to silicon bonding especially as node improvements slow,Positive
AMD,"Yep, and same SSD problem was reported with Arrow lake because it switched to tiles. Chiplets are pretty much a failure for consumer products. But they are necessary for servers so AMD and Intel won't stop. We can only hope the companies making monolithic cpus with much better performance (qualcomm, apple) can force AMD to make an actual consumer focused product for consumers, not just rebadged server scraps.",Negative
AMD,"This should get higher attention, benchmarks shows the issue but no one has any explanation for this. As much as I like my AMD I'd be even more happy if it doesn't bottleneck disk performance.",Negative
AMD,"Yeah, I hope AMD is working on this issue, would be nice to know if they work on it .",Positive
AMD,"They're necessary not just for servers, but to keep costs of consumer chips down as each new node costs substantially more than the last.",Neutral
AMD,"I'm not sure why I only see people talk about it now, but it's been always the case, ever since first gen Ryzen. Random read has been higher on Intel platforms. Subjectively, they also are more trouble-free. My Zen 4 PC keeps running into weird IO bottlenecks where over time transfers slow to a crawl between two fast SSDs if I also download files at the same time. It never happened on my old Kaby Lake media PC. My guess is Intel did more work and is quite a bit more polished with IO.",Neutral
AMD,"If you really want higher disk performance you should use Linux and an appropriate filesystem anyway.  I also presume any big difference between Intel and AMD would go away, since I don't think it's a hardware problem at all that causes this delta.",Neutral
AMD,"There is nothing to be done for current and previous generations, since this is HW limitation.  Strix Halo APU is using newer chiplet design, version of which should be used on next generation AMD CPUs. Someone testing SSD performance with such system would provide a glimpse into how performance could change in the future.  Intel will also improve their design with upcoming CPU generation, so if there were any slowdowns due to architecture they potentially will see uplift too.",Neutral
AMD,Smaller chips —> higher yields etc.!,Neutral
AMD,Qualcomm is making consumer only chips that are cheap too. Snapdragon x plus was cheaper than lunar lake while still matching in battery life,Positive
AMD,"The problem that transferring data between SSD halted is probably due to AM5 only has PCIE 4x4 connecting the chipset and CPU. Assuming you have one SSD connecting directly to CPU and one connecting to the chipset, if both SSD are PCIE 4x4, it would easily eat up all the bandwidth.",Neutral
AMD,"If this is software issue, there is software fix. It is also something worth to check.",Neutral
AMD,"Thanks, I suspected this to be the case too, but the behavior is surprisingly undesirable if this were the case. Things are all good until too much data is transferred at a time, where the transfer speeds tank to literally 100-200MB/s between two PCIe 4x4 drives.   This can be easily induced with multiple data transfers happening at the time, but it's not limited to those scenarios. I can plug in a SATA drive to have the third disk drive transferring files to either of the existing two, or start large downloads writing to one of my disks, and the system will be paralyzed with sub 100MB/s transfer speeds across all devices combined.  My mobo's front drive slot is actually PCIe 5 connected to the CPU, and the secondary one is PCIe 4 connected to the chipset, so the hardware should theoretically easily accept faster drives.  While seeking support, I was told it is expected behavior due to IO bottlenecks that aren't restricted by pure bandwidth, but the logic queueing the requests that can get overwhelmed.  The same drives worked far more consistently on a Kaby Lake PCIe 3 mobo.",Negative
AMD,"Would you see that ""over time"" though? I'd have thought you'd see it pretty much immediately.",Neutral
AMD,"Yeah, but likely at the kernel level. Or maybe firmware, or an interaction between both.",Neutral
AMD,At some point you quickly saturate the DRAM buffering the QLC as well.  Its 12000KB per second... for the first .01 second.,Neutral
AMD,I’m not exactly sure what “over time” means here.,Neutral
AMD,"I'm quoting from the post you replied to. To me, ""over time"" implies initial transfer performance is great, but then tails off.  I can't see a PCIe bottleneck causing that kind of behaviour. Thinking about it, I wonder if they're seeing SLC caches being exhausted if the copies are large enough?",Neutral
AMD,"Thanks for inquiring! By ""over time"" I mean that my transfers can be neat multiple GB per second, and then fall. But the most consistent way of reproducing the issue is by adding additional IO operations (say, also copy to a SATA SSD, or initiate a couple of file downloads) the speeds fall to a fraction of that. They get massacred when I use Opera and enable parallel downloading (which basically downloads each file in bits in parallell), where I can get sub 100MB/s transfers across all disks.",Neutral
AMD,"Two ""leaked"" Strix Halo products, both with 8060S iGPU but 12C24T (392) or 8C16T (388).",Neutral
AMD,"Finally, locking the best igpu to monster chips always felt like a waste, most who needs a 16 core cpu is going to want bigger than a 8060s anyway",Negative
AMD,Now put it in a actual laptop,Neutral
AMD,"388 sounds really nice but kinda too late. Feels like Medusa Halo with RDNA5 will be the ""real"" one to go with.",Negative
AMD,"When AMD makes a next gen version of this with FSR4 or higher, then we'll have something special.",Positive
AMD,AMD Ryzen AI Max+ 391 and a half.,Neutral
AMD,"Excited about the 388 i would love an 8/16 cpu for lighter things like gaming and general computing with the 8060, right now you basically have to pay for 64-128gb ai machines and not everyone wants that",Positive
AMD,I'm just hoping this leads to a mini PC with this chip in it so I can replace my ancient gaming desktop with an all in one machine. The current Strix Halo line is everything I've ever wanted in an all-in-one machine but is too overkill and expensive for what it's worth. A cut down variant with the same iGPU would be sweet.,Positive
AMD,"ALRIGHT, will more laptop manufacturers finally start using Strix Halo?  really really really want more than 8GB of vRAM for video editing and game dev, but I cant afford RTX 5070Ti laptop prices....",Negative
AMD,How many days of driver support can we expect?,Neutral
AMD,Hopefully these would be affordable and priced in accordance to competition (4060/5060 laptops). But knowing how the world is these will be sold at exorbitant prices to generate AI slop.,Neutral
AMD,"Hello work-school-account! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,When do you think consumer RDNA 5 gaming cards will come out? Maybe around June?,Neutral
AMD,That's what we need!!!!  Perfect for handhelds,Positive
AMD,I'm praying that Minisforum somehow shoves these on an ITX motherboard.,Neutral
AMD,"they should really do a APU silicon with only quad core + GPU equivalent of RX580 performance. (not a cut down chip but actually small chip). That handheld will be a monster.  \*note we had i7-7700K able to push GTX1080/GTX1080Ti, so quad core is actually more than enough.",Positive
AMD,"This will still be a big, expensive processor.  8C CPU CCD is pretty small.",Negative
AMD,That's an OEM's job.,Neutral
AMD,"Think that's a year out, at least",Neutral
AMD,Medusa Halo won't see the light of day before H1 2027.,Neutral
AMD,Is rdna 5 confirmed?,Neutral
AMD,"Keep an eye out at CES next year, if there are any more strix halo laptops imminent then they will be announced there or at the very least AMD will talk about strix halo. If it doesn't get mentioned then the means most OEMs are holding out for rDNA5, RTX 6000 or medusa halo in 2027.",Neutral
AMD,2 year cadence. 2027,Neutral
AMD,Assuming that they're actually available and affordable.,Neutral
AMD,"not really. it's still gonna drain the battery way too fast. slower than the 395, but not by a lot.",Negative
AMD,"These will be great (but expensive) for the GPD, AYANEO, or OneXPlayer devices. I doubt that we'll see these in a mass market ASUS, Lenovo, or MSI handheld unless we get a ""Z"" series version of Strix Halo. I am anticipating something like a ""Z2 Ultimate"" if AMD stockpiles enough yields with defective+disabled NPUs or something. Maybe there will be yields that are only stable up to 45w with reduced clocks that can find a home in more conventional handheld designs. The Phawx has proven that any TDP over 25W will offer massive gains over the existing Z2E.",Positive
AMD,"Not sure about handhelds. I think these processors are intended for NUCs, so maybe they don't work very well with a battery. Perhaps for a ""Steam Console""? I'm not sure either. I'd like to see someone putting Bazzite in one of those chips and plug it to a 4K TV, see if it's any good.",Negative
AMD,I'm still happy with the performance I get out of HX370 @ 5w TDP,Positive
AMD,"Right, because you totally want a 60W APU in a handheld.",Neutral
AMD,"Why? There's no need for a 12-core CPU in a ROG Ally, even 8 is overkill.",Negative
AMD,The onus is still on AMD to sufficiently supply them. That’s been a consistent issue for them versus intel.,Neutral
AMD,OEM cannot do it if AMD does not ship any units.,Negative
AMD,"And if typical AMD release patterns hold true, only a few, very expensive,  SKUs will be available in 2027. I'm not banking on buying a Medusa Halo product that fits my budget until late 2028 at the earliest. Don't sleep on this expanded Strix Halo availability. These should be priced to compete with Panther Lake in 2026. I'm personally hoping to see gaming notebooks like the Asus TUF A14 updated to use the 388 chip",Neutral
AMD,"In time for the market crash and prices returning somewhat normal, hopefully",Neutral
AMD,I thought it would be released in the first half of 2026?  Edit: Oh lol seems like i was wrong lmao,Neutral
AMD,"well, my current 7 year old laptop's hinge ([MSI GF63 8RC](https://www.notebookcheck.net/MSI-GF63-8RC-i5-8300H-GTX-1050-Laptop-Review.340606.0.html)) is torn in half, and its battery is completely dead  so I am absolutely in need of a new laptop in short notice, the latest new tech I can wait for is prolly Intel Panther Lake >!as the laptop itself has to release before [Arknights Endfield](https://www.youtube.com/watch?v=1XpQz8k1NwE) comes out in Q1 2026!<",Negative
AMD,ADHD meds with or without ADHD?,Neutral
AMD,"If we're being honest, they'll still be up-priced to sell to AI-bros instead of being tiny gaming boxes like we (here) want.",Neutral
AMD,GPD WIN MAX 5 does better with both 395 and 385  https://gpdstore.net/product/gpd-win-5/,Positive
AMD,">  I'd like to see someone putting Bazzite in one of those chips and plug it to a 4K TV  If anyone has done that for reviews, it would be https://www.youtube.com/@ThePhawx/videos",Neutral
AMD,>  so maybe they don't work very well with a battery  I don't see why not though? Don't they have awesome performance at very low power?,Negative
AMD,"Well, that’s exactly my use case with beelink gtr9. I’m using it as a tv gaming console running bazzite (and also as an llm server).   Gaming performance for 8060s - it’s much better than I expected. About every modern fighting game runs in 4k at stable 60, though upscaling is usually required for that. It also megabonks perfectly.   So I have high hopes for these new chips, given that they use the exact gpus. It now all boils to price and form factors.   My gt9 is undervolted and slightly overclocked, and runs at about sustained 140 watts.   Given that I live in Ukraine and lately experience blackouts, so during those I put it on balanced profile (sustained 80watts) and run off ecoflow together with monitor (instead of tv when life is gucci). This balanced setup all together is consuming around 137 watts when megabonking at 4k.",Positive
AMD,They are talking about the 8 core version,Neutral
AMD,"That, and a complete unwillingness to invest in engineering support for laptop OEMs.",Negative
AMD,"Since there's a bunch of noname companies releasing products with Strix Halo,  what make you think there's not sufficient supply?  The thing is that Strix Halo doesn't make a lot of sense in laptops because it's expensive and for the same money you can get a laptop that's much more performant.  The only advantage for Strix Halo in laptop form is AI, and that's a niche market. So there's no point in laptop manufacturers in investing a platform that won't sale. It's simple logic.",Neutral
AMD,"Videocardz covered 6-10 devices that use Strix Halo, that were released just in the last 2 months. VCz publishes a new release almost every week. So I don't know where you got this information that AMD doesn't ship sufficient units. Both ASUS and HP's laptops are in stock all around the world.  Here's a list of Strix Halo devices that were announced, mostly in the last few months:  **Handheld Gaming PCs**  AYANEO NEXT 2  GPD Win 5  OneXFly Apex  **Laptops, Tablets & 2-in-1s**  ASUS ROG Flow Z13 (GZ302)  HP ZBook Ultra G1 A  OneXPlayer Super X  SIXUNITED AXN77B (Laptop)  SIXUNITED AXP77 (Tablet PC)  **SFF/Mini-PCs & Desktop Workstations**  AOKZOE AI Mini-PC  AOOSTAR NEX395  Abee AI Station 395 Max  BEELINK GTR9 / AI Mini  BOSGAME M5  COLORFUL SMART 900  CORSAIR AI WORKSTATION 300  FEVM FX-EX9 (or FA-EX9)  FRAMEWORK Desktop  GMKTEC EVO-X2  Geekom A9 Mega  HP Z2 Mini G1a  LENOVO LCFC AI PC  Linglong StarCore Super AI Computer  Minisforum MS-S1 Max  NIMO AI MINI PC  PELADN Y01  SIXUNITED AXB35  SIXUNITED xD4B/xD8B  THERMALRIGHT MiniiPC  X+ XRIVAL PC  ZOTAC ZBOX E  **All-in-One PCs**  SIXUNITED AXA33  SIXUNITED STHT1  **Network Attached Storage (NAS)**  SIXUNITED STHT1-S5D-QSB (AI NAS)",Neutral
AMD,"Videocardz covered 6-10 devices that were released just in the last 2 months. VCz publishes a new release almost every week. So I don't know where you got this information that AMD doesn't ship any units.  Here's a list of Strix Halo devices that were announced, mostly in the last few months:  **Handheld Gaming PCs**  AYANEO NEXT 2  GPD Win 5  OneXFly Apex  **Laptops, Tablets & 2-in-1s**  ASUS ROG Flow Z13 (GZ302)  HP ZBook Ultra G1 A  OneXPlayer Super X  SIXUNITED AXN77B (Laptop)  SIXUNITED AXP77 (Tablet PC)  **SFF/Mini-PCs & Desktop Workstations**  AOKZOE AI Mini-PC  AOOSTAR NEX395  Abee AI Station 395 Max  BEELINK GTR9 / AI Mini  BOSGAME M5  COLORFUL SMART 900  CORSAIR AI WORKSTATION 300  FEVM FX-EX9 (or FA-EX9)  FRAMEWORK Desktop  GMKTEC EVO-X2  Geekom A9 Mega  HP Z2 Mini G1a  LENOVO LCFC AI PC  Linglong StarCore Super AI Computer  Minisforum MS-S1 Max  NIMO AI MINI PC  PELADN Y01  SIXUNITED AXB35  SIXUNITED xD4B/xD8B  THERMALRIGHT MiniiPC  X+ XRIVAL PC  ZOTAC ZBOX E  **All-in-One PCs**  SIXUNITED AXA33  SIXUNITED STHT1  **Network Attached Storage (NAS)**  SIXUNITED STHT1-S5D-QSB (AI NAS)",Neutral
AMD,RDNA5 won't till 2027,Neutral
AMD,"Nah zen6 launches in H1 26 in servers, year end/early '27 for desktops/laptops. Rdna5 is probably early '27. From the most recent rumors I've seen atleast",Neutral
AMD,"Probably keep a lookout during black Friday then as there might be a sale on a 5070 ti or 5080 laptop that brings it down to your budget. Even if CES 2026 brings the perfect laptop, there's no guarantee it will be buyable before May 2026 based on previous years.  Worst case scenario upgrading to a 5060 from your old 1050 laptop would still be a very noticeable improvement.",Neutral
AMD,Take a guess.,Neutral
AMD,"It's just a big, expensive architecture.",Negative
AMD,No CUDA and RAM getting more and more expensive won't make them super desirable for AI.,Negative
AMD,They cost too much to make to be tiny gaming boxes.,Negative
AMD,Who tf uses a handheld for AI lmfao,Negative
AMD,Even 8 cores is excessive in a handheld. You'd get more with a stronger GPU,Neutral
AMD,8 cores x86 cores takes up too much power in a handheld unless it’s some 4 big core 4 small core thing,Neutral
AMD,8 cores is still too many for a handled. 6 is ideal,Neutral
AMD,Surely it would still be a 100W+ chip when pushing gaming loads though?,Neutral
AMD,"One of those “no name” companies (GPD, the oldest and most established) has literally said AMD doesn’t give enough supply, lmao.  https://videocardz.com/newz/gpd-accuses-amd-of-breaching-contract-by-not-supplying-enough-ryzen-7-7840u-apus-on-time  Secondly, AMD being able to supply handhelds that sell several thousand units is a drop in the bucket compared to the millions major OEM laptops need to use. The fact that AMD supplies them in very small numbers, and not OEMS at capacity is proof of my point, not a refutation of it, lol. ￼",Negative
AMD,"These SoCs are very interesting not just for AI stuff. The NPUs have a lot of potential to dramatically improve energy efficiency and perf for a lot of other tasks as well, its just that the tools are still in their infancy so we haven't managed to develop many applications on them yet.",Positive
AMD,Would love to see more handhelds use them for the performance per watt at lower tdp.,Positive
AMD,">The only advantage for Strix Halo in laptop form is AI, and that's a niche market. So there's no point in laptop manufacturers in investing a platform that won't sale. It's simple logic.  for the same price you can also get much more vRAM with Strix Halo than a RTX 5060/5070 being stuck at 8GB",Neutral
AMD,So a total of 4 laptops? Two of which are from OEMs i never even heard of?,Neutral
AMD,That would be very early for Zen 6 based on AMD's typical timelines.  Nobody is really moving that fast anymore outside the smartphone chip space.,Neutral
AMD,">Worst case scenario upgrading to a 5060 from your old 1050 laptop would still be a very noticeable improvement.  but since the [5050](https://www.youtube.com/watch?v=ihJRJryQXTE) & [5060](https://www.youtube.com/watch?v=-VM_H7soWBQ) doesnt seem to have much of a performance boost compared to RTX 4060 & 4070, are older laptops with RTX 4060 & 4070 still viable picks if they're cheaper?  The main thing is that I want a sub 2kg laptop with dedicated GPU and 32GB RAM, so the main laptops I'm eyeing rn are either:  * 2023 ROG Zephyrus G14 * 2024 HP Omen Transcend 14  but their new RTX 4060 versions are still hovering around 1300 USD in my country, way above my budget.  So ofc I'm hoping that their price substantially goes down either during Black Friday or 12/12 sale, but do you think that they're even still worth pursuing or should I just focus on 2025 & upcoming laptops? (My third pick that fits my requirements is the new Legion 5, but that one's even more expensive atm)  Also technically there are three used laptop stores trying to sell a used RTX 4070 Omens to me at 1300 USD, but no way used stores are going to give any substantial Black Friday discounts",Neutral
AMD,#1 usecase for these in AI is running medium sized MoE models for local LLMs.  And for that they offer a pretty compelling solution. You basically have to spend twice as much to do it with Apple. At which point you're not using CUDA either.  ROCm works and inference engines like llama.cpp also support Vulkan compute AI.  It's basically the most power efficient and most cost efficient solution on the market for local LLMs.,Positive
AMD,"Well, mobile ai, these things don’t use sodim ram (or ram sticks in general) and the speeds of ddr5x aren’t used for most CPU’s. This very well could be also targeting mobile ai ( specifically more ai/gaming where the 395+ was targeting development and simulations as well). Also ai doesn’t need cuda, rocm exists and it’s quite good, any downfalls of amd is on a architectural level, but if you’re doing local ai on the go then you don’t really need cuda if that makes sense.",Neutral
AMD,This is the strongest gpu they have available,Positive
AMD,"Yes, but this is the version with the best GPU with the least cores, so it becomes the best for handhelds",Positive
AMD,"From a power point of view disabled-by-binning cores would be exactly the same as if you disabled them in the OS, or just didn't have enough work for the OS to schedule threads on them.  Any real savings would require a new die (and there may well be possible savings there, less complex busses, easier routing, fewer ports on sram blocks etc - but I suspect they might be relatively small), but this isn't that.  The *only* advantage this would have over the ""full fat"" die is cost. And that still depends on yields and how much supply of these binned dies they actually get.",Neutral
AMD,"Yes, but this is version with the best GPU with the lower amount of cores. So the best possible in terms of performance for a handheld",Positive
AMD,You can set the TDP to 15W actually,Neutral
AMD,Its a new product. Seems they made it for AI but found other very interested into it including nvidia partnering with Intel to counter these products. So I imagine 5heyre taking it more serious now that they're releasing these,Positive
AMD,"My friend, we were talking about Strix Halo supply, not 7840U from 2 years ago.",Neutral
AMD,Haha. You fell for the integrated NPU trap.,Neutral
AMD,Give me examples where for the same price you can get a Strix Halo laptop vs   5060/5070 ones.,Neutral
AMD,"Copy paste of one of my comments why OEMs are not in a hurry to use strix halo in laptops.  The thing is that Strix Halo doesn't make a lot of sense in laptops because it's expensive and for the same money you can get a laptop that's much more performant.  The only advantage for Strix Halo in laptop form is AI, and that's a niche market. So there's no point in laptop manufacturers in investing a platform that won't sale. It's simple logic.",Negative
AMD,"Mi450x is shipping in Q3 with zen6, and zen6 server chips are already in partner hands from latest earnings call",Neutral
AMD,"Unless the extra efficiency of the new CPUs are a factor for you then you aren't missing much getting a 2023 or 2024 laptop and yes the 4060 and 4070 are still plenty viable, the 5060 usually almost matches the 4070 but you're going to have a similar experience most of the time between the 4060, 4070, 5060 and 5070 most of the time. So yes, if you can get a 4060 cheaper then go for it.  It's really bad in the UK, don't know where you're based but trying to find a Zephyrus anything under £1500 is a waste of time unless you get a deal on eBay.",Positive
AMD,"Even then I'd just rather not have the cores. You're GOU limited anyways, so all they're doing is eating a bit of power and hurting your battery life. I've got a Z1X Ally and I'm generally hapoy with it but the usability on battery is easily its biggest weakness",Negative
AMD,Well i was saying this wouldn’t be enough anyway. I agree some handheld specific die would need to be made.,Neutral
AMD,Sure but the performance will be dogshit.  It's a nominal 55W chip and the 395 boosts to 120W to deliver the 4060-ish performance it is capable of.,Negative
AMD,You invoked “no-name” OEM’s being supplied at all by AMD as proof of them not having supply problems. I linked a very infamous instance of an OEM publicly speaking about AMD’s issues in keeping with supplying even these small boutique OEMS.,Neutral
AMD,"Strix Halo sounds significantly more niche than 7840U ever was, tbh.",Neutral
AMD,"Except I know that those NPUs are actually AIEv2 tiles, making this effectively an integrated CGRA :)",Positive
AMD,"in my country (Indonesia), if you wanted a sub 2kg laptop with more than 8GB VRAM, [your only option is either paying 52 million Rupiah for the RTX 5070Ti Zephyrus G14, or ""just"" 32.5 million Rupiah for the Flow Z13](https://cdn.discordapp.com/attachments/1315401041369366590/1436621115454652557/Zephyrus_G14_vs_Flow_Z13.png?ex=6910452d&is=690ef3ad&hm=d1c48964f898f82129b6acc9496b6f6db52c764812d0cfad8b9f2411a2c74826).",Neutral
AMD,"Yeah, but since Zen 6 is coming early in Epyc that doesn't mean Ryzen will come just as fast.",Neutral
AMD,"> It's really bad in the UK, don't know where you're based but trying to find a Zephyrus anything under £1500 is a waste of time unless you get a deal on eBay.     I'm from Indonesia!     The deal with our laptop market here is that we basically get the US's MSRP, but not their insane discounts (we ain't never getting a Legion 5 for $1000 unlike the US)     and the used market here is just extremely sparse for anything from 2023 forwards",Negative
AMD,Yes but dropping 2 cores is not going to cause a massive price cut or battery life improvement,Neutral
AMD,"The 8060S is about three times as fast as the 780M in the Z1E. The only IGPUs you can get that are faster than the 780M are the Intel Arc 140T, which comes in CPUs ranging between 14 and 16 cores, and the 8060S, which launched in 16-core parts and is now being made available in 12- and 8-core versions. If you want a good IGPU, you're getting at least 8 cores.  You can always disable them in the OS or UEFI if you're really dead-set on it. At that point, all you're losing relative to AMD engineering an entire new chip is some space on your motherboard.",Positive
AMD,"[It actually doesn't scale nearly that well with power](https://youtu.be/vMGX35mzsWg?si=NIM7ZYvjsFLFElL9), there's simply not enough memory bandwidth to feed the GPU effectively.",Negative
AMD,Those NPUs are actually integrated time machines.,Neutral
AMD,"I find that impossible to believe that's the norm. Your proof must be a great case of cherry-picking because I've been following Strix Halo and the competition very closely.  Here's examples where Strix Halo is more expensive, and much slower to a 5070 Ti:  [https://www.bestbuy.com/product/asus-rog-flow-z13-13-4-2-5k-180hz-touch-screen-gaming-laptop-copilot-pc-amd-ryzen-ai-max-395-32gb-ram-1tb-ssd-off-black/JJGGLHJ9PJ](https://www.bestbuy.com/product/asus-rog-flow-z13-13-4-2-5k-180hz-touch-screen-gaming-laptop-copilot-pc-amd-ryzen-ai-max-395-32gb-ram-1tb-ssd-off-black/JJGGLHJ9PJ)  [https://www.bestbuy.com/product/asus-rog-strix-g16-16-fhd-165hz-gaming-laptop-amd-ryzen-9-hx-16gb-ram-nvidia-geforce-rtx-5070-ti-1tb-ssd-eclipse-gray/JJGGLHJLTS](https://www.bestbuy.com/product/asus-rog-strix-g16-16-fhd-165hz-gaming-laptop-amd-ryzen-9-hx-16gb-ram-nvidia-geforce-rtx-5070-ti-1tb-ssd-eclipse-gray/JJGGLHJLTS)  In EU, the Stix Halo 395 cheapest laptops is 500-700 euro more expensive than 5060 laptops, 300-500 euro more than 5070, and 600-700 more that 5070 Ti. And that's for Z13 **TABLET**, not a laptop.  So my case stands. Strix Halo is a great niche product, but it doesn't make sense gaming laptops because the tech it uses is expensive and has a big die, thus making it uncompetitive. It's amazing for AI and gaming handhelds, thus we don't see many laptop options.",Negative
AMD,"I mean I did say ""**sub 2kg**"", because thats what I actually want  I know stuff like the MSI Vector 16 or ROG Strix G16 are available with RTX 5070Ti for cheaper than the Flow, but those laptops are heavy bricks",Neutral
AMD,"You actually said ""**for the same price you can also get much more vRAM with Strix Halo than a RTX 5060/5070 being stuck at 8GB**"" and the discussion was about **laptops,** not tablets. Stop moving the goalpost and learn how to have a logical conversation.",Neutral
AMD,"I don't get why usually like a $20 increase of a cards parts cost usually ends up at a $50 increase to a cards sale price.  Why does Asus, Gigabyte, etc, get to make $30 more profit when they spend $20 on memory?",Negative
AMD,Perfect. Exactly what we wanted to hear.,Positive
AMD,"Sounds like our lengthy current era of affordable, powerful, and plentiful GPUs might finally be coming to an end!",Positive
AMD,I just need to know whether or not I have time to wait for Black Friday next week.,Neutral
AMD,"Sure glad I didn't wait for any variants and just bought my 5000 series card first thing. Know this is about AMD, but I'm sure Nvidia will follow suit and raise prices",Positive
AMD,I guess they are not too concerned about having around 5% market share,Neutral
AMD,About to buy a 1080p monitor to keep my GPU going for a few more years.,Neutral
AMD,Never let a good crisis go to waste.,Negative
AMD,There is always  an excuse to increase price,Neutral
AMD,"I imagine Lisa and Jensen laughing, clinking champagne glasses.",Positive
AMD,Waiting on tech tubers to hold AMD accountable ..,Neutral
AMD,Thank God I finished all my upgrades late last year/earlier this year. I'm straight for the next 5+ years.,Positive
AMD,What sort of price increase can we expect? Is it gonna be 50$ or 300$?,Neutral
AMD,Hilarious how reddit thinks this AI shit is a bubble that’s going away soon when hardware prices indicate the exact opposite.,Negative
AMD,"Because sure, *why the fuck not*.",Negative
AMD,I just bought my new pc. I gues it was a good moment.,Positive
AMD,I assume the same happens for CPUs. Higher memory prices result into less cpu/gpu sales they have to compensate.,Neutral
AMD,Too bad I was gonna start building a pc next year gonna have to wait for years 😢,Negative
AMD,Will this be happening before Black Friday or after? The Christmas I decide to get a new computer stuff is going up 😭,Neutral
AMD,last time this happened i got $1200 for my 5600xt and upgraded to a 6700xt with that money.   maybe it'll repeat haha  (this is ridiculous though),Negative
AMD,Will this force devs to finally…optimize… their games? Yeah I said it. The dirty O word devs hate,Negative
AMD,Nvidia are the ones who brought up prices and yet AMD will be blamed for this.,Negative
AMD,"Another answer nobody's touched on yet is the price/sales curve in basic economics. There's optimal intersection points and sometimes with a moderate price increase, you'll get drastically fewer sales. But there's some customers that are not as price sensitive, so bumping it 50 won't lose that group.   Hopefully that makes sense.",Neutral
AMD,"Companies are valued based on their gross and operating margins. If their costs rise, they also have to increase their sale prices by the same percentage, or those margin rates go down, which reduces the valuation of the company. Distribution and retailer also take a percentage of the product value as their margin. So even if the oem only raises the product price by the higher costs, retailers still increase the price based on the percentage increase.",Neutral
AMD,"The same reason Intel's Lunar Lake was a one-off.  Companies run a percentage profit margin they report to their investors. If they produce a good that costs 50$ to make, and sell it for 100$, then they have a profit margin of 50%. For investors this means that every dollar they put into production results in 2 dollars of capital, which is a good indicator for potential growth, and, of course, potential dividend payout.  If costs increase by 10$, then, if price were left unchanged, their profit margin would sink to 40%. if They increase their price by 10$ as well their profit margin would sink to ~55%. Investors will look at those numbers and conclude that the company is doing poorly: it is no longer able to achieve the same return on investment that it did previously, and they might just conclude that their money can be better spent elsewhere and cancel further investments.  So, a 10$ cost increase results in a 20$ price hike. In reality, profit margins aren't really that high for third-parties like Asus and others, investors can be persuaded that the costs aren't due to mismanagement and companies will take a hit in profitability in the short-run to maintain marketshare (although, again, AIB companies generally run razor-thin profit margins).  Another reason for adding margin to price-hikes is Machiavellian psychology: Gigabyte and others know the way the wind is blowing, RAM prices are going to keep climbing for the foreseeable future and people respond worse to smaller price increases every month than a big price increase every 6 months. So they'll add some padding now, 'worst' case they can drop the price later or give greater discounts during sales, best case they're better insulated from any future inflation.",Neutral
AMD,Money. Corporations are evil and not your friends,Negative
AMD,">Why does Asus, Gigabyte, etc, get to make $30 more profit when they spend $20 on memory?  Why not?  They operate based on profit margins.  If they're earning a 50% profit margin, then a $20 increase in input costs will result in $30 increase to the selling price.  Plus, just because NAND contracts went up by $X doesn't mean that's what it costs manufacturers, because they're still bidding against each other for actual supply.  If they don't wanna sit there with 0 stock for the next 6 months, they'll need to bid up the price even higher.",Negative
AMD,Because that’s how margins work.  It’s not sold directly from the manufacturer.,Neutral
AMD,"Because they have to maintain profit margins. They aren’t going to take a hit to margin just cause the materials got more expensive.  If you only raise the retail price by the same amount of the cost of building the card increase, then your profit margin decreases.",Neutral
AMD,"Input costs only determine the viable price floor.  The market (buyers) determine the price. They can all increase prices $50 because people will buy it. They’re also now making more profit per unit which nearly always offsets the loss of customers.  Imagine their costs are $80 and they sell for $120, that’s a $40 profit and a 50% return. Costs increase $20 to $100, price goes up $50 to $170, and all of a sudden returns are 70%, and profit per unit has almost doubled. They can sell almost half as many units and be behind in revenue but ahead in profits, which is the most important part.",Neutral
AMD,They have a legal obligation to make as much money as possible. Gamers have shown since the crypto eras and onward that we’ll pay more for hardware than they thought possible. I just saw a whole thread from someone who bought a 5090 to game casually on non-graphically intense titles and do office work. We’ve signaled to these hardware companies that PC gaming is a luxury.,Neutral
AMD,They don't. The OEMs buy kits that contain both the GPU die and the memory. AMD charges the OEM a margin on top of the kit cost. AMD and Nvidia typically don't change the margin target when the BOM increases because they typically don't need to do so to meet sales targets.,Neutral
AMD,Because margins are a percentage and not flat rate.  And are the primary grade for the stock market.  For 90% margins (as an example) you would need to increase the price by 200 dollars for a 20 cost increase.,Neutral
AMD,"Return on investment, they have to maintain a certain %",Neutral
AMD,"It's like tipping. It isn't any harder to bring a $100 vs $20 plate of food to your table, but they expect to get $20 vs $4 for doing it.",Neutral
AMD,"As the other responders have stated, it's to maintain percentage based margin. This was covered by a recent HWU video on this exact topic, where they explained that the memory usually comes in a bundle deal with the core from Nvidia/AMD. So Nvidia/AMD are creating the lion's share of the increase here. I think board partners are already squeezed pretty tight, especially by Nvidia.",Neutral
AMD,"Margin. They are targeting a fixed margin (measured in %), not a fixed profit per card (measured in $$). They just had an earnings where they said they want very high margins. If they only pass BOM costs directly to the consumer, it means their margin per card will decrease.  Still total horseshit, but that's their calculus, largely.",Negative
AMD,Because you'll pay for it anyway,Negative
AMD,"When prices go up, demand goes down, so they're going to sell fewer units. They need to increase price more than their increase in cost to make up for the lost volume.  This is a fairly straightforward optimization problem. Total profit = per unit profit \* volume. There's also a tradeoff between per unit profit and volume: The higher the per unit profit (and therefore sale price) the lower the volume, and vice versa. When per unit profit is zero then total profit is zero, and likewise when volume is zero total profit is also zero. Somewhere between those two extremes there's a point of maximum total profit. Because per unit profit is price - cost, a change is cost is going to have a relatively larger effect on per unit profits, possibly even putting it negative without a price change. This will cause the optimal price point to move towards higher per unit profit, lower volume. This is what causes the price to go even higher than just the increase in cost.  Thinking of it as greed isn't helpful. This is a very straightforward logical deduction. They don't need increased costs to be an ""excuse"" to raise prices, they can do that anyway and they were already doing their best to optimize their price for maximum profits before.",Negative
AMD,"Because millions are spent on anchoring consumer price expectations and all this ""news"" is a big part of that.  Create  fomo and the illusion that prices will only keep going up so buy buy buy.  People need to remind themselves that hardware ages like milk.",Negative
AMD,"ROI, want to maintain the margin, if the price goes up it cuts into volume, so margin inherently has to increase or else the company loses money.",Neutral
AMD,Inflation hedge. Their costs can vary.,Neutral
AMD,bullwhip effect. it's something we study in supply chain,Neutral
AMD,Because that's how capitalism works.,Negative
AMD,"Well as someone who was deciding whether to ride out a 3080 10gb until the Supers dropped... it just made my decision a hell of a lot easier lmao  5070 TI  it is, I guess",Positive
AMD,When was that?,Neutral
AMD,"""This is the most expensive GPUs have been so far, and the cheapest they'll be from now on""",Positive
AMD,"Frankly, GPUs aren't the bottom neck in gaming since the 30 series. The bottom neck is shit code.",Negative
AMD,If you don’t have a card buy one now,Neutral
AMD,do video cards even get discounted for black friday?,Neutral
AMD,Bro Black Friday has already ended according to many raising their prices again the last 2 days.,Negative
AMD,"Better to be 5% share and profitable, than 10% and bleeding margin each day.",Positive
AMD,Team Green is not gonna have much choice but to follow suit to maintain their margins and keep the shareholders happy.,Neutral
AMD,Never have been.,Neutral
AMD,It sucks actually because the 9070XT is actually really competitive with Nvidia at $599 MSRP,Negative
AMD,"Nope, AI profits all the way. Who cares about radeon? You? Lol",Neutral
AMD,That would be a great family dinner.,Positive
AMD,"Given AI is on an infinite demand curve with no signs of abatement, prices will only continue going up is the only answer anyone can give.",Neutral
AMD,"Redditors are consistently wrong about everything. Just look at how disconnected the front page is from reality, lmao.  Reddit is overwhelmingly bots and people that are functionally bots (NEETS, social pariah, political activists, ETC.)  Reddit has had a disproportionate impact on ruining the internet and it’s genuinely scary that generative AI is being trained on it, lol.",Negative
AMD,"Memory kits have already begun to triple in price and I don't know where that crazy train is going to stop. So yeah, it's that time to get ready to clear out the hardware boxes and throw every last thing on ebay again.",Negative
AMD,"Yeah, $50 on something already worth $600+ isn't likely to move the needle on someone's decision on whether to buy or not for a segment like this. $50 on something originally worth $20 though? That'll kill the market overnight most likely.",Negative
AMD,Fundamentally the production price and selling price are entirely unrelated except that a company won't sell at a loss. If something costs $500 to produce but consumers will pay $5000 then they will charge $5000.,Neutral
AMD,I always hated basic economics.  Economists are boot lickers.,Negative
AMD,big words confuse  ooga booga me prefer to be angry,Negative
AMD,"TBH this is the best response. Reality is risk and market research is the biggest drivers of cost increases of this nature.   The ""but their profit margins"" alone is quite an idiotic response considering AIBs don't make 20-60% profit margins on GPUs.",Negative
AMD,"Corporations are neither good nor evil. They are amoral.   If doing good things will make them money, then that's what they will do. If doing bad things will make them money, then that's what they will do.",Negative
AMD,I mean this isn’t even evil just stupid   If NVIDIA doesn’t raise prices this will bite them in the ass,Negative
AMD,Hey Lady this isn't about cats! (/s) <3,Neutral
AMD,"If they increase the price of a GPU by $20 when it costs them $20 more to make. They have the same dollar value in profits at the end of the quarter.  If Asus makes 1 billion in profits this quarter, it'll still be 1 billion. Why should they make 1.1 or 1.2 billion because their costs go up?   Imagine being in a company CEO, seeing your production cost go up for the components your building by let's 10% total, and thinking to yourself "" Hooray! 🎉 We all get 10-20% raises this year!",Neutral
AMD,"The financial dollar amount of margin is the same if they increase the price by $20 if they charge $20 more. Percentage wise it's less if you compare to total operating cost, but overall they'll make the same money. I don't get why a company should make more profit in pure dollar amount (not %) if they're costs go up.",Neutral
AMD,Well then they should've been priced higher before the cost increase.  Does anyone think Nvidia is currently leaving money on the table?,Neutral
AMD,"It's 100% reasonable if you have any insight into corporate financing, investments, risk and opportunity cost.  Consider someone asks you if you want to make $100. Sure you say, how much? And they answer $1000 dollars, and you'll get $1100 back in 30 days. Not a bad deal?   Now consider the same person offers the same thing, but you need to borrow them $10,000.   Suddenly it's a shit deal.   If nVidia made a fixed profit per card, you'd never see anything above a 5050. Why pay for the manufacturing of a 5090, when it's only making you the same dollar amount as a 5050?  That's why profit margin is important, and not horseshit.",Neutral
AMD,"Hard to ignore the $699 deals at this point honestly. With extended holiday returns I can still see whether prices do get affected come CES2026. A 70ti super was what I was gonna wait for, but not if it’s gonna be $900msrp.",Neutral
AMD,"Hi, I'm one of them. The moment I saw the news about RAM prices skyrocketing and then this one, I literally just went ahead and got a 5070 Ti. Compared to my old 3080 10GB, it's running cooler, uses less power, and is able to take advantage of all of Nvidia's features so far. My games are now running faster while using much, much lesser power. It's working like a dream and I am extremely glad I bit the bullet. Now I can sleep in peace, haha.",Positive
AMD,Spring 2017.,Neutral
AMD,"Yep, the ""Before Price"" stock will disappear in a few days at most and then it'll be back to ""Bullshit Compute Boom Prices"" for another couple of years at least.  :joy:",Negative
AMD,"Yeah, as much as I wasn't super sold on buying a new GPU, I got a 5070 ti to replace my 3070. Honestly, I might have kept it for longer but it's getting a bit scuffed (I have to underclock to stay stable at stock voltage). Saw a £699 PNY card and just got it, by far the cheapest non-SFF card I could find in the UK the last few days I've been looking. Only other I could find was the MSI Ventus but I've heard not great things about that one.  I was originally going to wait for the super refresh but tbh I'm on 1440p and prefer FPS > Visuals so I really don't think the extra VRAM is even worth the potential of griefing myself if prices go up more.",Negative
AMD,"9070 XT now purchased.  I guess I can just not open it, and return it if there do happen to be Black Friday discounts",Neutral
AMD,I've anticipated this and ordered a 9070 XT previous monday.,Neutral
AMD,They have in the past. Usually AMD discount more than NVIDIA from my experience.,Neutral
AMD,"Given that they have inferior product at Nvidia prices, doubt it.",Negative
AMD,Depends on your goals. Its totally fine to bleed margin if your goal is market share saturation.,Neutral
AMD,They are following the Xbox strategy,Neutral
AMD,"They should offset the cost increases by increasing the price of data centre products, it's all magic mystery money buying those anyway so who cares.  Of course back in reality...",Negative
AMD,NVIDIA doesn’t care about gaming   Perhaps they care so little they would prefer to maintain some level of brand reputation,Negative
AMD,"Yeah, this right here. I was hoping prices might come down to MSRP during Black Friday.  But I guess I'll be waiting *another whole fucking year* to upgrade my GPU!  **Thanks a lot, AI bubble pumping assholes.**",Negative
AMD,You think Nvidia cards isn't getting a price hike too?,Neutral
AMD,"Its really shocking if you really think about: AI models being trained on Reddit and co,which is filled with blatant Karma farmers,foreign propaganda and bots,just so that they can be used on Reddit for more machine learning.  A never ending cycle of AI Slop.",Negative
AMD,Correct. The objective is to price your good as high as the market will bear and still sell your target amount (usually dictated by estimates and capacity at fabs now),Neutral
AMD,"Economists are scientists trying to explain phenomena that are incredibly complex and intersect so much with sociology that it's difficult to even quantify things outside of basic trends.  The boot lickers you speak of are dipshits who use these trends to justify behaviors that don't fit the models and shouldn't be accommodated in a civil, modern society. They are not economists and would use whatever other theories might be convenient to excuse their manipulation and cruelty if economics did not present such a ""logical"" facade for them to hide behind.",Negative
AMD,"Corporations are amoral, those that run them are not. Also, greed is generally considered evil. It's even classified as a deadly sin.",Negative
AMD,shit like palantir is evil,Negative
AMD,It's only stupid if it leads to lower profits (number of people deciding to not buy the card at all offsets the profit from people paying extra). Judging by insane prices people were willing to pay during last crypto boom I'd say that we are far away from price increase tipping the scale,Negative
AMD,"> I mean this isn’t even evil just stupid   not stupid, people pay happily",Negative
AMD,Nvidia will raise prices,Neutral
AMD,"This is Nvidia and their fanbase's fault though. A xx70 GPU with only 12GB of VRAM was accepted by consumers, while AMD has been providing performance parity or even superiority along with more VRAM at a discount for far too long.",Negative
AMD,">  They have the same dollar value in profits at the end of the quarter.    No it's less than one dollar; there's debt/interest, insurance, currency fluctuations, fluctuating costs of components, costs of futures, and various other risks involved. These new costs require a profitability reason for them to exist.",Neutral
AMD,"If they spend 10 billion to profit 1 billion, that's a 10% profit margin on the cost of the business. If they spend 11 billion to profit 1 billion, that's a 9,1% profit margin.   If their costs double, spending 20 billion for a 1 billion profit, that's a 5% profit margin.    Why is this important? Because the margin is the buffer between making money and losing money.    Another important thing: it means that if you can spend the additional 10 billion on something with a profit margin of 6%, you should cut production in half and do the other investment instead.       Let's consider time instead of cost.    Let's say you build wooden tables. You get paid 200€ in labour. Because of a legal change in what kind of varnish you can use, a table that used to take 8 hours to make, now takes 10 hours of work to make.    Are you not entitled to more money for that table, because you worked longer, i.e. you time cost was higher to produce it?    You are the production cost, are you not allowed to maintain your hourly rate? The profit you make per hour, not per table?",Neutral
AMD,"Well gee, when you put it like that, why don't they just sell everything at cost then?  Why should *they* get to make money from *me* when all I want is a video card?",Negative
AMD,"There's a time lag between when they pay for parts and when you pay for your card, and that needs to be paid back with interest.",Neutral
AMD,"There's a difference between getting an extra $20 back because you let me borrow $100 vs getting an extra $20 back because you let me borrow $10000. In this analogy, the money I borrowed = upfront cost to generate profit and $20 is your profit. Don't you see how if I borrow more money, you should get more profit? It would be ridiculous for me to say ""it's $20 either way, just let me borrow all your money""",Neutral
AMD,"No company does profit margins by dollar amount... None that I have ever seen or worked at at least.  It's all % based.  Becasue if the price of a product goes up, they don't expect to sell the same number of them, so if the total $ profit per unit stayed the same, then they would be making less than before.",Negative
AMD,"Horseshit was in the context of this specific ram increase topic, not overall margins, which I agree with. Everyone worth their salt knows you can only hold margin and squeeze consumer on cost for so long before you make less net revenue which is a bigger Boogeyman for big corps than small margin degradations.",Negative
AMD,"Yeah I was holding out hope for that rumored 8gig VRAM bump on the 5080 Super, but even now looking more closely at price history for those cards... well 10 gigs of VRAM is becoming way too much of a bottleneck to risk being stuck on for all of 2026.",Negative
AMD,"There was a short period in 2019 where things were pretty solid. The 1660 Super at $219 was an excellent value.  If you eliminate covid shortages, there would probably have been a time in 2020 where you could build a PC with a 3300x ($120) and a 1660 Super ($219) for an excellent ~$550 build. Instead GPU pricing went batshit insane and the 3300x barely materialized",Positive
AMD,"Me in spring 2017:  ""About to move, should wait until after to get a GPU""... Crypto bubble.  Me in fall 2020:   ""Wow these black Friday deals on 5700xt are great but I think I'll wait for a 3070..."". Crypto plus supply chain...   Me in 2025:  ""Damn 50 series is kind of weak I'll wait for 60...""  Sees GPU prices actually at MSRP and AI demand about to fuck everything...  Ahh hell with it not this time.  <Buys 5070 ti>",Neutral
AMD,Meanwhile back in 2017 people were raging about the price of the 1080.  $599 for a GPU!? ($800 today).,Negative
AMD,Any coil whine with it?,Neutral
AMD,"Thank you for your comment! Unfortunately, your comment has been removed for the following reason:  * Please don't make low effort comments, memes, or jokes here. Be respectful of others: Remember, there's a human being behind the other keyboard. If you have nothing of value to add to a discussion then don't add anything at all.    Please read the the [subreddit rules](http://www.reddit.com/r/hardware/about/sidebar) before continuing to post. If you have any questions, please feel free to [message the mods](http://www.reddit.com/message/compose?to=%2Fr%2Fhardware).",Negative
AMD,They’re following the standard business strategy of….not losing money.,Neutral
AMD,They've tried to play that game before and people bought and scalped under priced gaming cards for other purposes. You end up paying more anyway (if you can find a card) but some middle man makes the money instead of nvidia.,Neutral
AMD,Ai trained on reddit has been happening for a very long time. /r/SubredditSimulator/ was running for many years before it shut down just in time before the big LLMs came out.,Neutral
AMD,No. Shit like Palantir owner is evil. Palantir itself does what makes them money.,Negative
AMD,"I think this is the real take away - all hands in the industry already know how much the markets are willing to pay.  HOWEVER, those were COVID days and people in the US at least were getting paid to stay home, some even collecting unemployment or checks for their closed employers.  End result, US ran drunk with COVID money which lead to inflation and now are paying at every other counter. So not sure how likely that price ceiling that was pushed up during that time can hold anymore.",Negative
AMD,At least with crypto any mouth-breather could use the card to mine and recoup costs. How many of the median gamer population can turn the 5090 into an asset rather than an expense?,Neutral
AMD,I think this might be the case   Most people I know who have amd gpus bought them because they saw a launch day review claim great value for money and because they don’t keep up with day to day tech news they didn’t realise they were paying more now,Neutral
AMD,For AMD cards?,Neutral
AMD,Yeah but AMD also has a smaller/inferior feature set and they’re only good for raster.,Neutral
AMD,Amd has provided fucking marketing for the most part   People are obsessed with vram because last gen they twittered their fingers bloody over it after adding some extra to their otherwise underwhelming high end gpus   And this gen they are at the exact same numbers as NVIDIA with a smaller featureset,Negative
AMD,"I'm not sure I like that table analogy. To me it's like you're building a table for 8 hours and getting $200 in labour and praying $100 in wood, tools, etc. you'd probably charge $300 for the table. $100+$200.  33.3% is your cost in parts, and 66.6% is your profit margins that is also your hourly pay in a way.   Now imagine your parts cost went up by $100 to $200. Do you now charge $200+$200? (You pass the cost onto the consumer) Or do you charge $200 for parts/supplies plus ANOTHER $400 for the labour, because you think the 100% increase parts/supplies cost also justifies you getting a 100% hourly wage increase. After all, you want to maintain the same 66.6% margin. $200-$400.   I think in a small business like this, that would be insane to demand you get double the annual revenue and profit margin in salary. If I went from 66k a year annual net income to 133k, because my supplies cost went from 33k to 66k, that would be weird. Now admittedly, you'd sell a lot fewer tables, so you'd probably not hit those yearly goals. But in part that's your own fault for doubling your table sale price.   The second you have investors, it's suddenly justifiable though. Which to me makes the system look broken.  Personally I think what's going on is that that WANT to sell less gaming GPUs. Just like in the example above you might want to sell fewer tables. AMD is filtering their RDNA4 supply to server instead I'd guess.",Negative
AMD,"But at the quarterly reports they do report other numbers.   ""Third quarter revenue was a record $9.2 billion, gross margin was 52%, operating income was $1.3 billion, net income was $1.2 billion""  I believe by maintaining that gross margin of 52%, all those other numbers will go way up, as a result of more cost, while maintaining margin.",Neutral
AMD,"""making less than before.""  Bubble is saying price was originally sold at $100 if it was $20 in parts it goes up $10(imagine no other costs).  Why don't them make it $110 instead of $150.  You are thinking that ""they don't expect to sell the same number of them"" so that means it shouldn't be $150, as economies of scale are smaller so it should be $170 instead.",Neutral
AMD,"Yeah, fair that. It's all about the what companies think the price elasticity is for their product. If we increase our price by 10%, is loss in number of sold units big enough to make our actual profit/revenue smaller than before the increase? It's all a big guess.",Neutral
AMD,"Yeah for me I’m coming from a discount 3070 so I can almost justify the price as is. If $699 holds thru December it becomes $665 factoring in chase 5% PayPal used at Walmart. If it drops to 650 I almost have no excuse. I’m more than happy to shop around w that return policy, I had 4 PS4 Pros in 2016 fishing for the best deals.  Going by history if this forces price increases we’re gonna see a permanent price bump too.",Positive
AMD,"Should've been 9070 XT! jk, I had '70 Ti on my decision chart but said eh 130€ is not an insignificant difference.",Neutral
AMD,"Honestly with this thread having a bunch of people apparently pulling the trigger on that GPU, it's making a bit more sense why I am seeing out of stock on so many of them the last couple days I've been browsing haha.",Neutral
AMD,"I will let you know if mine does tomorrow when I get it lol. A friend has the exact same card though (VCG5070T16TFXPB1-O) and didn't mention any issues with it, something he complained about a good amount with his old GPU so I assume he'd have noticed.",Neutral
AMD,You'll never become a tech/AI startup CEO with *that* attitude!,Negative
AMD,Who knew so many financial experts like Jumba all sat in between gaming chairs and RGB keyboards.,Neutral
AMD,They do it anyways. Universities are full of 4090s and 5090s.,Negative
AMD,"inflation was from limited supply out of China, not 2000 dollar checks",Neutral
AMD,"You analogy makes even less sense, and keep mixing up what you're actually calculating, and muddying the water by adding the materials in there. My point was that it's fair to ask for more money based on how much you've invested, be that either time (hours worked) or money (buying materials)      Philosophically: If my profit margin is 100% based on the labor, an increase in cost of materials has no impact on it.    You ALSO forgot to account for the cost of you time, i.e. how much are these 8 hours worth if I'm not building this table. What is 8 hours at minimum wage? What's 8 hours working for somebody elses company worth? This is opportunity cost. You're just assuming the labour is 100% profit, which breaks real-word profitability analysis.    To make it fair, you should be calculating the cost of hiring someone to build the table, and you then sell it. You're not going to have 66.6% profit margins.  Anyway...  If the parts cost goes up to $200, and my hours worked stays the same, an absolute minimum price would be $400. And especially if you market your services as ""no profit on materials, only work"". You might do this, because sales/VAT taxation reasons like if materials are taxed higher, but labour is lower. So you give a receipt for materials, bill them as paid-for (0 profit) and then bill your work.    This way of doing things, your profit margin technically doesn't change, since the material is expensed at-cost, your business is selling your labor. You're just expensing the materials, not selling it to the customer. It's almost a balance sheet transaction, that doesn't appear on the P&L.  So no, it has nothing to do with investors, it has to do with you disregarding basic finance and looking at labour as free.",Negative
AMD,"If the item was sold at 100 with profit margin of 25% (80 product cost, 20 margin), then if you cost price by 20, the final product price will be (80+20) x 1.25 = 125 dollars, not 110 or 120.",Neutral
AMD,By the time I got to it the 70 xt vs 70 ti I was considering were only $80 difference and I think the gap in fsr4 to Dlss 4 about makes up for it.  If it was 600 vs 750 I probably would have gone 9070 xt.,Neutral
AMD,Thanks!,Positive
AMD,"Why add time if time is not increased in this scenario. Material cost is with VRAM price. That to me is muddying the waters. If it took them 20% longer to build a GPU, it would make more sense.",Negative
AMD,yeah it was 630 vs 790 here excluding VAT,Neutral
AMD,"And to update, mine also has none at high FPS or low FPS which sometimes is an issue I've heard. So all good :) It's also pretty quiet in general, definitely quieter than my old Asus TUF 3070.",Positive
AMD,"If it's costing them 20% more, it doesn't make sense?",Negative
AMD,"Thanks for the update, I appreciate it!",Positive
AMD,"This feels like that time they launched FSR Frame Generation in 2 games to hit a deadline even though it clearly wasn't ready yet. Then a few months later in actually launched in more games and in a much better state.  Like you can see the what the tech is trying to do and it does some things well  already but as Alex points out it clearly feels unfinished. I've seen someone say its marked as version 0.9 in the code.  Watch them announced a slew of games and the full feature pack including Radiance Caching, ML Frame Generation and improvements to FSR in general on December 10th and then say its coming to more games in Q1.",Neutral
AMD,What is up with the different visuals at the same settings between the cards? The AMD version seems to be missing or have downgraded visuals.,Negative
AMD,"Well, it seems AMD has something... Sarcasm aside, nice to see DF's input on this 'early look' of FSR RR.  A lot of strange things observed. With the same settings AMD has some things omitted compared to Nvidia, as seen with shadows in a distance, it's quality, as well with objects reflected like with lights (can't say for sure if this an AMDs or dev thing, regardless it doesn't matter to the end user to who's fault).  With FSR RR, the most noticeable spotted difference from at first glance is the retrieval quality and AA from the reconstructed/regenerated scene. DLSS RR is better which shouldn't be shocking. This is due to how AMD does their upscaling passes with RR, as you can choose whatever upscaling method you want with RR and how it can be clearly seen with water reflections. FSR4 Perf is noticeably blurrier than FSR4 Native when comparing the reflections in the water, while it wouldn't matter what DLSS quality setting you use in conjunction w/ DLSS RR. As an aside, I found it interesting how FSR4 Perf had better texture detail on the letters than DLSS4 Perf (that said perhaps could be a sharpening thing outside of the upscalers used).  Moving on, FSR RR has missing features as noted with how reflections should be reconstructed. Reflected objects/scenes do not correctly diffuse, and are sharp throughout (typing this, I use [10:20](https://youtu.be/RuASMLRGFus?si=p1ECIQiTwNLLxeT0&t=617), but interestingly enough w/ DLSS RR, the gun has noticeable artifcating maybe due to the denoiser?). Environments with FSR RR do not have correct contact lighting as seen with the part about the trash can.  AMD does have some things going for it (though, I suspect is more per game basis). In motion, DLSS RR while clearer experiences more ghosting/smearing elements compared to FSR RR. That said, can't be said for the disocclusion aspect as seen with swapping guns with water in the foreground (that said I wonder if this is again the same problem with the upscaler/denoiser pass with FSR RR or if the problems with game settings between AMD and Nvidia has something to do with it like SSR).  There are things to note about performance, but isn't wildly different, so I won't make comments there.  Overall, again strange to say the least. It's obvious FSR RR isn't quite done, which can be worrying considering it's still one part of Redstone. Also, the game itself isn't a good test bed for the feature (as if it only being enabled for MP wasn't obvious enough) as there are bugs and instabilities. I guess, if anything, AMD has a step in the door which will have improvements. That said, isn't quite like with FSR4 in terms of wow. Not only that FSR4 can easily be swapped with Optiscaler and recently in AMD's own driver with most DX12 games. This one seems to be dependent on AMD actually implementing it with games so unless there are workarounds no DLSS RR swapping with FSR RR.",Neutral
AMD,"I was hoping Ray Regeneration was going to be a simultaneous denoising and upscaling approach, they were [investing research in it](https://gpuopen.com/learn/neural_supersampling_and_denoising_for_real-time_path_tracing/) and suggested they had something working in [an admittedly artifact-ridden demo](https://www.youtube.com/watch?v=p0vLil19mBs&t=21s). Instead this seems to be a spatiotemporal ML denoiser. It's completely decoupled from upscaling so appears to be denoiser pass before upscaling which ends up cutting out high frequency details and reconstructing to a lower res so you don't get that sharp DLSS RR resolve.   At least in it's current form it's not really an effective competitor to DLSS RR, rather it's competing with standard denoisers like NRD. Which makes me worry about adoption - NRD is vendor agnostic, as are most denoisers. I doubt this could be injected generally like Optiscaler since it is decoupled from upscaler outputs and probably designed to run in it's own pass. Maybe it could be modded to run on DLSS RR inputs before upscaling if the algorithm is general enough? While I hope it's better by Redstone showing I don't see how they could improve the detail reconstruction if it's acting before upscaling  Edit: not to undermine what has been achieved here - it's a very competent denoiser, and imo beneficial over COD's default at identical cost. it just doesn't push any boundaries whereas DLSS RR remains surprisingly coherent when reconstructing small RT details or high gloss reflections",Neutral
AMD,COD is probably the worst game to showcase this in.,Negative
AMD,Whats the performance hit from DLSS Ray Reconstruction in BO7? I don't think they mentioned it but did say that Redstone Ray Regen was basically same perf as built in denoiser.,Neutral
AMD,"I wonder, what games in the future will have Reconstruction? For now, only 7 games have it (8, including BO7), so it's no wonder AMD is just throwing it out there in Call of Duty. ML-based FG will be a more effective technology to put forward.",Neutral
AMD,AMD is samsung to nvidia’s apple,Neutral
AMD,">I've seen someone say its marked as version 0.9 in the code.  Forget the code, you check the properties of the DLL and it's marked as version 0.9 I'm pretty sure.",Neutral
AMD,"Is this a trend with AMD moving forward? Announce a new GPU feature and 8-12 months later, actually release it? AMD Did this with FSR3 and with FSR Redstone being almost a year later, wouldn’t gamers just wait until AMD GPUs depreciate and when the actual feature is ready?",Neutral
AMD,"Yeah, IIRC when FSR FG released, the Square Enix game that was poorly received, had frame pacing problems. It was then around when Avatar released FSR FG was solid and at the time in terms of performance was quite competitive with DLSS3 FG's HW accelerated Optical Flow overhead.  Reality is, no one should really expect AMD to have Nvidia features day one or even have parity. It is more so that AMD should have a relatively decent cadence to have said features so that buyers aren't left out.  I am curious on what they have to say. To me RR seems to be more involved than upscaling and FG as there are already a lot of routes for the developer to take to implement the slew of upscaling methods. Adoption will be the hurdle even more so than before",Neutral
AMD,Now... Nvidia Frame Gen for 30 series (maybe 20 series) on December 10th announcement :D,Positive
AMD,"Redstone is just the tessellation slider of yore, and we all know it.  Seems AMD took the r/AMD suggestion to feature issues - just disable it - to heart.  Can't complain it's ugly/shoddy if you can't see it, and if you don't have a side by side all you see is bigger FPS number. Win/Win!",Negative
AMD,"Which differences specifically? Most of it is probably down to difference in FSR and the denoiser. The softer look of the water on the AMD card actually looks more realistic, you dont get super sharp reflections on water like that in real life.",Neutral
AMD,"I'm guessing the issues with contact hardening is mainly down to how the network was trained, reconstructing glossy reflections from low sample counts in real-time is challenging and one where DLSS RR is obviously better. Because it's denoising at internal resolution, the start of the hardening penumbra will be be softer than DLSS RR even if it should be sharp. Compared to the in-game denoiser I think FSR RR is actually mostly beneficial (except the water which has had all the ripple detail removed and the extreme ghosting with gun disocclusion) and it seems to be free, although DLSS RR isn't horrifically expensive on newer cards. I'm hoping this is a stepping stone to a simultaneous denoising and upscaling solution because the magic of DLSS RR is it's ability to reconstruct sharp ray traced detail even at low internal resolution, something that this multi pass technique probably can't do (although it can be made sharper, [NRD managed it by denoising with a detail recovery pass that respected DLSS jitter](https://github.com/NVIDIA-RTX/NRD?tab=readme-ov-file#improving-output-quality)).  Optiscaler is magic in getting FSR4 in most titles but I fear this technique won't see much adoption at all - denoising passes aren't typically general so this is something that would need per-engine integration. I suppose it could intercept RR inputs (assuming there's equivalent) to do one pass denoise then standard upscale but unless the inbuilt denoiser is terrible it's probably not worth it",Neutral
AMD,"> in an admittedly artifact-ridden demo  wow that's bad lol, so much ghosting and swimming",Negative
AMD,"Which is why AMD continues to make blunder after blunder in marketing. They simply don't understand gamers. Reminds me of that time they revealed RDNA2's design in Fortnite, the main set of people playing Fortnite were kids who couldn't afford a high end GPU, makes no sense to market there. It's like if Lamborghini started advertising its cars at an eSports tournament (notorious for being mainly viewers 13-25 without disposable income), you're not addressing the market that can actually buy your product.",Negative
AMD,Yea but modern cod is commonly an AMD partnered game. Gotta take what you can get.,Neutral
AMD,Computerbase measured it at 1-2%.,Neutral
AMD,Only 7 games have RR that can't be right.,Neutral
AMD,So better?,Neutral
AMD,">  wouldn’t gamers just wait until AMD GPUs depreciate and when the actual feature is ready?  Yeah, AMD fans call this ""fine wine"" but the truth is you just didn't get what you paid a premium for, for months and they thank AMD for it.  Reflex 2 is starting to be in the same category of bullshittery, they marketed 50 series on a feature that isn't out in any games and it's been almost a year without the feature. The only difference is NVIDIA won't push out a feature unless it's ready, whereas AMD force it out when its half-baked like Anti-lag which got people banned from online games or as Firefox72 said FSR FG which was broken in a bunch of games on release.  Either way, I was hoping AMD would move to making their features feel more premium by only releasing them in a very polished state, but nope, once again they rush features out and then get destroyed for it by the tech press. Way to make the Radeon brand look competent AMD.",Negative
AMD,"has been like that for ages... i remember the same a 15 years ago with hardware video encoding!  NVidia usually delivers out of the box a better working solution while AMD plays catch up but gets better over time!  In the end it always is a question on how much you are willing to pay and if you want to deal with nvidia as a whole!  I would not even say the drivers are AMDs issue anymore, they are not, or NVidias drivers have gotten worse anyway while AMDs opensource support for instance is top notch, so in the end how much is a feature worth to you how much are you willing to pay!  In the end AMD simply has less resources to throw against a problem and that shows in new features!",Neutral
AMD,Moving forward? AMD has always struggled with software development. Last time I remember AMD leading the charge in software was Mantle.,Negative
AMD,Cyberpunk devs were interviewed when ray reconstruction first introduced and there was talk about all the tradeoffs in denoising,Neutral
AMD,Go watch it again. It has nothing to do with ai denoising.  At the 2:30 of the video he compared using FSR 3 performance with the default denoiser on both.  The amd card had missing details.,Negative
AMD,AMDs demos are usually ugly as sin nowadays but that path tracing demo was horrendous.,Negative
AMD,"Thanks for the answer, didn't see it on computerbase's site. Not sure why I got downvoted in my original question?  Looks like I originally had the wrong computerbase article, was looking at their main BO7 one not a specific RR article here:  https://www.computerbase.de/artikel/grafikkarten/amd-fsr-ray-regeneration-cod-black-ops-7-test.95050/  Was surprised to see that FSR RR had better IQ than DLSS RR in a few cases:  >  In places, FSR R shows even more details in the reflections than Nvidia’s technology, but sometimes it is also the other way around. The image with DLSS RR always remains much more stable. And that's a big win.  Annoying that we don't get identical images anymore for comparisons.",Neutral
AMD,Are they?,Neutral
AMD,"AMD seems to have adopted a ""quiet launch that's basically testing in production"" approach to new feature release. TBF, Nvidia's DLSS ""1.9"" was also done this way,",Neutral
AMD,"what was fine wine was less features, but performance tuning of an existing card, which you can argue to be why didn't their driver team just get it right on launch. but it did felt less like what you said.  but with features being like this, yeah far less charitable and its more in line with what you said, they launched the HW without the SW being ready.",Negative
AMD,"I feel like this is also just the hard truth of the economics of the GPU duopoly.  Nvidia can throw an exorbitant amount of money at new issues, solutions and technologies.  It is simply not economically feasible or viable for AMD to compete at the high speed of Nvidia with a fraction of their operating budget.  Now, they apparently have figured out that at the moment, releasing half cooked things to the public keeps them at least in people's mind and gets them mind share, criticism and bugs be damned.  I understand this approach and clearly they thought about this and if it works otherwise they wouldn't do it time and time again.  As a consumer it obviously has pros and cons",Negative
AMD,Ah yeah i missed that skimming with no audio. Missing shadows is a pretty big deal,Negative
AMD,The Bethesda way,Neutral
AMD,It's 2025. AMD has plenty of money now.,Positive
AMD,It's not just missing shadows. Entire reflections are missing. Draw distance also looks lower,Negative
AMD,So what IS it exactly? A new version of FSR? 4.1?,Neutral
AMD,I mean its technicaly already out in Black Ops 7 and it works.  Its not quite as good as Nvidia's Ray Reconstruction tech but its a significan't step up from the baseline denoiser and improves the quality of RT reflections across the board.  [https://i.imgur.com/td0Vqqm.png](https://i.imgur.com/td0Vqqm.png)     [https://i.imgur.com/rxexoDm.png](https://i.imgur.com/rxexoDm.png)  On the plus side it appears to be cheaper to run than Nvidia's solution.,Positive
AMD,"Please, have a game with heavy rt to showcase it.  Cyberpunk  Bm wukong Alan wake 2 Indiana Jones   These are the heaviest rt titles. Okay cyberpunk rt/PT is kinda easier to run than the rest",Positive
AMD,Just in time for AMD GPU price increases,Positive
AMD,well see in Minecraft,Neutral
AMD,It's just a bunch of ML-based features that enhance RT/PT performance and quality,Positive
AMD,Nvidia equivalent features basically.,Neutral
AMD,"Funny enough, Asrock just mentioned FSR 4.1 on Twitter.",Neutral
AMD,"It's worth noting that Blops7 only has the RR and FSR4 parts of it, and that RR is not on a version 1.0 yet. Redstone should include the following according to AMD:  * **FSR Upscaling:** (Formerly AMD FidelityFX™ Super Resolution) Reconstructs crisp, high-quality visuals from low-resolution frames.  * **FSR Frame Generation:** Predicts and inserts new frames between rendered ones, delivering smoother and higher frame rate gaming.  * **FSR Ray Regeneration:** Infers and restores full-quality ray-traced detail from sparse samples, delivering sharp, noise-free visuals with reduced rendering cost.   * **FSR Radiance Caching:** Dynamically learns and then predicts how light propagates through a scene, delivering efficient real-time global illumination.    Of these, we already have FSR4, though a 4.1 or something could come out with this. Ray Regen was the new thing previewed in COD so far. We have yet to see FSR4 Frame Gen or whatever the ML-based solution will be or how their radiance caching implementation will differ from Nvidia's.",Neutral
AMD,"Just an important note, BLOPS7's version is listed at .9, so it's possible (or likely) that the official release will bump the version and may have other changes.",Neutral
AMD,"Nvidia has 2 Ray reconstruction versions. The improved CNN model one and the more recent transformer version. The transformer RR is amazing and by far the best denoiser available for video games. The CNN version is overall better than regular denoiser partly because regular denoisers look flat out awful (it's hard for devs to make denoisers), but CNN version does have compromises and some implementations by devs aren't as good as others.   Unlike Transformer DLSS upscaling the RR Transformer model is very punishing on Turing and Ampere but runs very well on Ada and Blackwell.",Positive
AMD,"Thanks for the helpful post, i recommend checking out ImgBB for posting photos. Imgur is cancer.",Positive
AMD,Some other features are more relevant than ray reconstruction as RT is so rarely used even on more powerful nvidia cards due to how extremely taxing it is. Most of use is on older titles with few gens of hardware advancements. Latest games can butcher even RTX 5090 into sub 60fps with DLSS.  Redstone should bring other improvement than this as per older AMD claims,Neutral
AMD,"Not gonna happen, all those are Nvidia sponsored titles (maybe not Indy, I don't remember about that one). Redstone will probably come to Cyberpunk a year after its release and it will be the worst implementation possible. Optiscaler will very likely make Redstone compatible with those games long before their developers do.",Negative
AMD,AC Shadows can be pretty beefy with the RT too,Neutral
AMD,"It's hard enough for an upcoming game to support FSR4 from the start, it would be even harder for existing games to add support for this and maybe benefit 1% of their players.",Negative
AMD,Yeah they need a few big titles at launch at least to show it off. Maybe BF6 when it gets its RT update?,Neutral
AMD,"As if nvidia won't do this, lmao. Everyone with any sort of DRAM will, many already did. We're fucked",Negative
AMD,Nvidia has increased prices for lesser reasons. Stop it.,Negative
AMD,So it is a RT/PT specific upgrade to FSR4?,Neutral
AMD,We have path tracing at home.,Neutral
AMD,Is it possible to upgrade the RR version to transformer like DLSS or are you stuck with the version the dev used?,Neutral
AMD,"Ugh this again. On average modern RT (not PT) isn't really all that taxing on modern hardware. Now we have games that are built with RT in mind, have no legacy raster path and run well even on a 2060 Super like Indiana Jones TGC and Doom TDA. You gotta remember the RT in these games have to run on a PS5 which is weaker than a 2060 at RT, some even run on a series S, hell Star Wars Outlaws has **several** RT effects running on a Switch 2!   If a Series S and Switch 2 can do it how the hell can modern PC GPUs struggle with it. Now if you wanna min max and drop your settings to achieve 200+ fps that's fair but that doesn't mean RT is gonna make a modern GPU explode.",Negative
AMD,"?? Not really, Redstone is literally just Ray regeneration, a new frame generation version, FSR4, and radiance caching. Is there some secret feature That we're all unaware of?",Neutral
AMD,Stop being stuck in 2020. RT is not taxing if your hardware isnt obsolete.,Negative
AMD,"At least not indiana jones, that has locked dlss inputs, so unless theres official implementation, there will be nothing",Negative
AMD,"But like the other user, most people forget it exists.",Negative
AMD,It's not hard at all to support this. Takes some dev time but is basic engine plumbing. What is hard is implementing the real time path traced renderer that these techs are designed to support. But engines that already utilize denoising and upscaling in their pipelines could benefit from it even if they aren't path traced.,Neutral
AMD,FSR 4 is in damn near every AAA game at launch since they released the SDK.,Neutral
AMD,"Stop what lol, it's true?",Neutral
AMD,can we not defend any corporations instead of taking sides cuz thats just weird,Negative
AMD,Is that supposed to make it ok?,Neutral
AMD,It has nothing to do with FSR just like how DLSS Ray Reconstruction and frame generation have nothing to do with super sampling. It's really its own thing. They're just uniting these technologies under the FSR name so that it's easier to understand and market,Neutral
AMD,You can override it in the Nvidia App.,Neutral
AMD,"I would still argue it can be (implemented) inefficiently, not for 200fps but for APUs or even to extend say the handheld/Switch2's battery",Negative
AMD,"And those are far more beneficial than ray reconstruction. Barely anyone plays with RT, especially on AMD GPUs anyway. Maybe with like RX 9070 XT in 5 year old games.   Not to mention RT is dying over Lumen and similar software  implementations. So yeah, tiny portion of RX 9070 XT users will maybe use it if the get back replay some older fame - yay...   Better FSR and better frame gen is what benefits most.",Neutral
AMD,"then check benchmarks, lmao..   I won't bother with many examples, but https://ibb.co/SXN9Yyr2  sub 60fps on fucking RTX 5070 Ti at 1440p (DLSS quality) 🤡 My gosh this sub is high on some drugs and doesn't even watch benchmarks, just vomit bullshit",Negative
AMD,"by hard I meant the incentive for dev to do it, as AMD only makes up a tiny fraction of players, and dev time to add it is more or less the same as DLSS, so game studio would just be ""nah just do DLSS and 90% of our player will be happy""",Neutral
AMD,"There's obviously a negative connotation in that comment, as if AMD planned this release to coincide with the price increases. Also a negative inference that somehow the price increases are unjustified.",Negative
AMD,"You're delusional if you think people don't play with RT on AMD GPUs.  I have a 9070 non-XT (albeit BIOS-modded and disgustingly overclocked, so about on par with an XT). I always turn RT on, and I tend to max it out without issues. I play Witcher 3 maxed out at 1440p with RTGI and Optiscaler FSR 4 at about 90 FPS base. Enable FG and it's hitting my monitor refresh rate, with the base framerate high enough that the input lag doesn't feel bad.  Even Cyberpunk is fully playable with path tracing if you're willing to drop down to FSR 4 balanced. It sits at around 45-60 FPS, or around 90-110 FPS with frame gen. Obviously path tracing doesn't look all that great without an ML denoiser, but it's completely playable with PT on, and will look absolutely stunning once we get RR on AMD, not to mention feel even better if NRC works in the game, since it should yield a 20-30% boost in PT performance.  And remember, a 9070XT will do 10-15% better than my 9070, so RT is absolutely being used on AMD.  Also Lumen sucks ass. It looks worse than proper hardware RT and is only popular because UE5 is popular.",Neutral
AMD,"Unfortunately lumen costs nearly as much as RT and arguably looks worse in every way.  Not sure that makes much sense. Also I don't even get this reply. I said what it is and you say ""and that's enough"". No shit it's enough. I asked you what magical thing you dreamed up that it also had.",Negative
AMD,"Lumen is RT though, but because it's software based it's the worst of both worlds. Massive performance impact while it doesn't even always look better than good raster. The temporal accumulation need for Lumen also brings with it the worst ghosting ever. Low fidelity software RT is absolutely not the future.  In games that actually have hardware Lumen the performance is only slightly worse on good RT GPUs while the visuals are massively improved. In theory at equal visual fidelity hardware Lumen would give you much better performance than software Lumen. That's just common sense as you have extra hardware to accelerate it.       In the couple of games with mandatory RT that have a software fallback you can see that performance decreases significantly when you fall back on the software version.  Epic knows software Lumen sucks when GPUs have good hardware support for RT (afterall there's no reason not to use dedicated hardware) so they're already shifting towards defaulting hardware Lumen in the recent UE5 versions",Negative
AMD,Lumen has both software and hardware implementation. Lumen software implementation is a lot more taxing than ahrdware RT and is in fact advised against. To the point where the latest Unreal version has it off by default.,Negative
AMD,>shady link  >emoji  Im talking to a 5 year old arent i?,Neutral
AMD,"It was obviously not planned, but it's been nearly 7 years of this shit. While AMD, Nvidia, etc have no control over these big periodic demand spikes, it's frustrating for consumers nonetheless. Also cry me a river when it comes to ""fairness"" in the context of multi-billion $ companies.",Negative
AMD,Why do you think the price increases are justified then?,Neutral
AMD,"Nobody said that, but yknow twist my words for your own benefit",Negative
AMD,"Which I specifically noted - with exception of some old titles. Barely anyone replay those, it's tiny fraction. Like I've 100% Witcher 3 like 4 times before even they added RT patch. I'm not gonna replay it for the 5th time just for RT alone when I know game by memory. Similarly with CP2077. Small fraction..  Then take any more recent game: https://ibb.co/4wxJjMx4 or https://ibb.co/3yhTtMvw or https://ibb.co/1YxcVNZq and it falls apart, which is where highest player counts are, not in the 2 GPU gen old titles (or even older such as Wicher 3).  So you just also showed you CAN'T READ, because I accent clearly, that it's fine in old game or with very soft RT implementations like RT ambient occlusion found in Dead Space Remake or Metro Exodus.",Neutral
AMD,"HW Lumen butchers performance, check some benchmarks lmao.   https://www.youtube.com/watch?v=V-SlJiiaAXo  https://www.youtube.com/watch?v=J1oshjHNTWw  https://www.youtube.com/watch?v=WI_mqHo3698  30-40%+ gains depending on scene in software mode over hw mode. That's more than generation of HW progress these days.. So stop talking bullshit it costs nothing.",Neutral
AMD,"It's not slightly worse. Look at fucking benchmarks, like The Outer Worlds 2 butchers RTX 5090 when Hardware Lumen is enabled.   And AMD DOESN'T EVEN HAVE GOOD RT GPUs, so what are we even talking about? Niche feature everyone is jizzing over but like 1% of AMD userbase maybe gona use it. Even among high end nvidia users based on multiple polls done for example by HUB, most chose more performance over RT.",Negative
AMD,"lmao, again, watch some benchmarks. HW Lumen is far more taxing, but since you're so smart, surely won't have problems googling some benchmarks",Neutral
AMD,This isn't a periodic demand spike anymore this may be all compute hardware and silicon for the next 10 years.,Neutral
AMD,Why does AMD get criticism for simply following the market standards that Nvidia and their ilk established?,Negative
AMD,Feature parity with Nvidia while also providing more VRAM,Neutral
AMD,Not targeted to you specifically but that's been the obvious sentiment regarding this manufactured outrage,Negative
AMD,"I can very much read, thank you.  9070 does fine in pretty much any RT title, benchmarks show as much. It beats the 5070 in RT, but also has enough VRAM to fully leverage the power it has available for heavy RT.  Your examples are Black myth: Wukong, AW2 and Indiana jones, and only with the most extreme RT settings. The most similarly priced Nvidia card, the 5070, performs worse in two of those three titles. In Wukong, where it's beating it, it's still delivering unplayable framerates due to the fast-paced nature of the game.  It should also be noted that your example benchmarks were produced on PRE-RELEASE DRIVERS. Those same prerelease drivers had launch FSR 4, which had a much higher latency penalty than it currently does. Current drivers have brought huge performance boosts, something like 10-15% over launch performance, even accounting for outliers.  Those performance gains have been especially big in RT games, where the base 9070 tends to beat the 5070 across the board now, while it was hit or miss at launch.  Like yes, the 5070Ti is still objectively better for RT than the 9070 XT, but it's also 25% more expensive. That still doesn't make the 9070 XT a slouch. It's still better than a 5070 in RT pretty much across the board, even where the 5070 isn't being strangled by the 12GB VRAM buffer.  Driver optimizations like opacity micromaps, shader execution reordering, neural radiance caching and an ML denoiser will only ever close that gap to the 5070 Ti further. Admittedly OMM and SER isn't hardware accelerated like on Nvidia hardware, but should still yield decent uplifts. NRC and ML denoising should both bring double digit performance uplifts, however.  People forget that on a pure hardware level, the 9070 XT has 5-10% more theoretical RT compute than the 5070 Ti. It just lacks the software feature set Nvidia has, and a few hardware-accelerated refinements, notably OMM and SER. It is not inconceivable that the 9070 XT could reach parity with the 5070 Ti in RT.",Positive
AMD,"I believe in the first 2 it's modified to even use hardware Lumen. As in the games don't ship with it supported nor did the developers put it in. And the last one is outer worlds 2. Which is known to run like garbage. Note the 50 to 60 fps with the GPU at 35 to 50% utilization.  I don't think these are representative of the overall usage of hardware RT.  Software Lumen tends to look very ghosty especially on foilage and lacks detail, especially in AO.  It's also screen space limited.",Negative
AMD,"Time to bring the receipts.  Software Lumen: [performance2-3840-2160.png (560×1450)](https://tpucdn.com/review/the-outer-worlds-2-performance-benchmark/images/performance2-3840-2160.png)   Hardware Lumen: [performance-3840-2160.png (560×1450)](https://tpucdn.com/review/the-outer-worlds-2-performance-benchmark/images/performance-3840-2160.png)  A whopping 7% drop in performance for hardware Lumen on a 5090. Thanks for the example. It illustrates very well what I'm talking about. From what I've heard the hardware Lumen in The Outer Worlds 2 is broken but usually it looks significantly better than software Lumen. I think most people would gladly sacrifice 7% performance for that, because usually the difference is like low vs ultra graphics. No massive Lumen ghosting either.  It's funny you talk about AMD not being good at RT but in this game a 9070xt loses less performance than Nvidia: [performance-2560-1440.png (560×1450)](https://tpucdn.com/review/the-outer-worlds-2-performance-benchmark/images/performance-2560-1440.png) vs [performance2-2560-1440.png (560×1450)](https://tpucdn.com/review/the-outer-worlds-2-performance-benchmark/images/performance2-2560-1440.png) so they're losing 3% performance at 1440P.  It's not that RT doesn't cost a lot of performance it's just that software Lumen is just as expensive as it's wildly inefficient. The 5090 does indeed struggle at native 4K, but it struggles in general and hardware Lumen has nothing to do with it. With how dominant UE5 is you can't actually avoid RT at all so it's funny that you call it a niche feature. It's like I'm arguing with someone from 2020.",Neutral
AMD,"They ALL get the critique, stop fucking bootlicking.",Negative
AMD,"I'm sorry lol, but that's insane. You think it's okay to pay more for software features on current GPUs?",Negative
AMD,"It's not proportional at all though. Nvidia has 94% of the market, the critique falls squarely on their shoulders.",Neutral
AMD,"That's my point though. Nvidia and the consumers decided it's okay to spend $600 on a xx70 GPU with 12GB of VRAM, but all of a sudden it's bad when AMD does it?",Negative
AMD,When they do it mid-generation on pre-existing cards? Absolutely.,Neutral
AMD,It's an existing card - how do you ever justify that lmao,Negative
AMD,Nice!  Scarcity to drive prices up again.  They just don't quit.,Positive
AMD,">Korea Economic Daily says NVIDIA, AMD, and other GPU makers are considering dropping some mid-to-high-end gaming cards where memory now makes up an unusually large share of total cost.  My predictions:  6060 9GB @ 96-bit.  6070 12GB @ 128-bit.  6070 Ti 15GB @ 160-bit.  6080 18GB @ 192-bit.  I see little reason for DRAM foundries to continue producing 16 Gbit GDDR7 chips.",Neutral
AMD,Because why should there ever be any GOOD news?,Negative
AMD,"Fewer memory sales to consumers due to insane prices equals fewer motherboards being sold is something I can understand. But since vendors are going to reduce motherboard production does that mean the inevitable conclusion to this is vendors will next cut back on consumer CPU production, or switch the ratios to favor an even high mix of server parts per wafer? Because processors are practically the only unaffected system hardware category left as of now.",Negative
AMD,Glad I ordered a 5060 Ti for 399 euro a few days ago.,Positive
AMD,Can't wait to see how this affects Apple with their local AI ambitions on the iPhone and Mac. Oh boy.,Positive
AMD,So no one is planning to investigate this memory cartel?,Negative
AMD,I am so glad I finished my gaming PC upgrades this year.,Positive
AMD,"China, please save us!!!",Negative
AMD,"if this is laptop manufacturer's excuse to get rid of 32GB RAM versions of laptop with only onboard RAM, then no one's gonna buy them  but if this means no more 16GB onboard RAM versions of laptops littering online stores unsold then good riddance",Negative
AMD,Should I order a 5080 immediately?,Neutral
AMD,"I'd suggest using any frame limiter possible on whatever GPU you have, and undoing any overclock. You probably don't want your GPU to die in the next 3 years, or you'll soon pay double of what it's worth.",Neutral
AMD,If memory becomes a scarcity for the next 2 years. Compute still must happen.  Is RTX 5090 from Asus still way above MSRP?,Neutral
AMD,can intel produce ddr5?   ddr5 doesnt need euv.  it will free capacity in samsung etc for gddr6+,Neutral
AMD,I mean there literally isn't enough memory for them to make GPUs with. For once it's not their fault.,Negative
AMD,"I mean it's not new right, silicon is the new black gold. OPEC does the same shit, prices are down, cut production to prop it back up.",Negative
AMD,"To be fair, Vram chips are relatively cheap. This time it's not entirely their fault or villainous game.",Neutral
AMD,"Well, yea. They don't have components for the product.",Negative
AMD,"they are just using it as an excuse to hide the real reason, demand is dropping. datacenters have hit power limits, and now major companies like microsoft are scaling back orders. not to mention concerns about the AI bubble rippling through the markets.   Memory supply may be down, but so is demand for compute.",Negative
AMD,Surely there will be 4GB chips out by then,Neutral
AMD,I see no reason not to cut consumer CPU production at some point. It's not going to make sense to buy new consumer hardware for some time.,Negative
AMD,Are you?,Neutral
AMD,Apple more than likely has over a year of memory already paid and scheduled to arrive for them    They always sell so much that they can do that just fine,Positive
AMD,https://www.forbes.com/sites/davealtavilla/2018/04/27/class-action-suit-alleges-samsung-micron-and-hynix-colluded-on-dram-supply-causing-price-inflation/  https://www.reuters.com/article/world/china-launches-dram-chip-price-probe-into-samsung-elec-sk-hynix-and-micron-idUSKCN1J02E9/  need this to happen again,Neutral
AMD,They all are selling record amount of bits.  It’s a demand craze that is causing this shortage.,Negative
AMD,The EU might. Anyone else? I doubt it.,Neutral
AMD,"They are, but they won't announce much until they are raiding offices.",Neutral
AMD,"Their DDR5 and HBM production goes up about 400% per year so yeah, they actually will.  I'll be laughing when these cartel fuckers are crying about competition from china.",Negative
AMD,Monkey paw curls. Now we will be cursed with 8 GB entry level laptops again,Negative
AMD,"Daniel Owens on YouTube just did [a video](https://www.youtube.com/watch?v=_sCOcWBEs4I) analyzing this. He goes over a number of points. (Which I agree with)  This is probably as optimal time as any (for the foreseeable future). Board prices are likely going up, due to RAM pricing (touched on in the video regarding [AMD rumors](https://www.reddit.com/r/hardware/comments/1p0h67y/amd_reportedly_planning_gpu_price_increase_as/)). While the rumored Super series are also rumored to be cancelled due to RAM situation, meaning no new models to push down prices on older supplies. With OEMs depleting supply of current stuff for the foreseeable future.  I would point out I've already seen prices creeping up as bit. An MSI 5080 I was eyeing is up a couple of hundred dollars from last month. The PNY 5090 I got from Amazon was $2500 a month ago. Now it's up past $3000",Neutral
AMD,yes,Neutral
AMD,"Undervolt it even, less voltage helps with longevity and reducing power usage which also helps with longevity.  Underclocking the VRAM just a tad might also help if it's running hot.",Neutral
AMD,> or you'll soon pay double of what it's worth.  I already had to when I nabbed it in the early spring of '22...,Neutral
AMD,"i was hoping to see Intel Optane reborn (somewhat DRAM like, but not really DRAM)",Neutral
AMD,They aren't cutting back in their overall production capacity.,Neutral
AMD,they aren't cutting back on ram production independently. look at nanya in Taiwan as an example. they can't make HBM but they can make DDR5 and DDR4 ram and they're going all in with DDR4 since it's very profitable right now and yields are high,Neutral
AMD,"the same thing happened in 2018ish when all the dram manufacturers colluded to cut production and raise prices. then they got investigated for market manipulation by the chinese government, and suddenly the supply went back to normal.",Neutral
AMD,"Bro have you heard of this thing called 'A.I.'? It stands for artificial intelligence, and companies worth trillions of dollars are pouring their entire ecosystems into it. Call me crazy, but it's **possible** that trillions of dollars of AI demand *COULD* be affecting DRAM supply for other uses, rather than DRAM producers limiting supply themselves. Food for thought. Please like and subscribe if you found this post helpful! /s",Neutral
AMD,"This is pure market: there scarcity _at a cheap price point_. When offer goes down, prices go up.  They just decide not to go forward with it because margins are reduced (they earn less per laptop). Demand is still there).  So they can sell their overpriced laptops easier, since cheaper brands will have to push prices up…",Neutral
AMD,"It's just econ 101, supply and demand.",Neutral
AMD,OPEC does it primarely from enviromentalist pressure though. Pretty much every time OPEC reduced supply it was either after some enviromentalist firendly country put pressure on them or when production capacity decreased in old wells and new ones are expensive so wont be built unless prices are up.,Negative
AMD,GDDR7 4gb chips were targeting late 2026 back when the roadmaps were being made in 2024. No clue the volume or if there is delays.,Neutral
AMD,3GB chips were out by H2 2024. How many GPU cards released in H1 2025 used it? Only one laptop model.,Neutral
AMD,Oracle is CIA. They'll be fine.,Neutral
AMD,"I will remain optimistic on Zen 6... AM5 is a well-established platform and there will be no shortage of people looking to upgrade older chips by next year, that won't need to buy boards or memory to do it. I got a 7700X when it launched three years ago and really need an IPC & X3D upgrade.  On the other hand Intel owners are going to be royally screwed, none of their platforms will see another generation and neither socket 1700 nor 1851 boards are worth updating anymore. Especially as buying used Raptor Lake CPUs to upgrade on the cheap is just begging to get a damaged processor.",Positive
AMD,"Yeah, it will arrive next week, and I will have 50 days return right, enough until CES 2026 ends to see if the supers are gonna delay or not. If they delay I will keep it and wait for 6000s if not I return and buy the supers.",Neutral
AMD,You think prices will be back to normal in a year? Recent reports say 2026 will be even worse for dram pricing.,Negative
AMD,"Last I heard, they are also not aiming to increase production significantly to accommodate said demand. They’re supposedly just aiming to improve performance and not notably increasing volume",Negative
AMD,investigate what? that there is bigger demand than supply?,Neutral
AMD,"The US is 100% going to restrict RAM exports from China, especially for GPU use.",Negative
AMD,"Thank you, this has actually been the best information on what's going on with the market and what will likely happen. I finally decided to pull the trigger on a 5080 FE, especially with the Steam Frame being a thing in a few months. I'll probably resell it in 2027 and get something from the 6000 series to be my ""Next few years"" card. Thanks!",Positive
AMD,VRAM likes to run hot,Neutral
AMD,"I have this 80% rule on all electronics. if you do not run max, they are usually very reliable.",Positive
AMD,"Both things are true. They cut production simultaneously a year or so ago, which is why them now being bought out by OpenAI is hitting so hard.",Negative
AMD,It's supply and demand in an oligopoly/duopoly.  Important difference here.,Neutral
AMD,"Basically, yes. Plus the ratcheting effect of consumer pricing.",Neutral
AMD,They have been hoarded for use in compute GPUs. Hence why the Super line up has got 'cancelled/postponed' as virtually all the worlds 3GB supply is going into compute cards.,Negative
AMD,Super cards have been already delayed to Q3 26,Neutral
AMD,The bubble will have burst by then and these companies will be begging for consumer customers.,Negative
AMD,They are at max volume they can’t just increase production lol,Neutral
AMD,"Further increasing production would require new plants, some are being built but memory plants arent a waffle house. You cant build it in weeks.",Neutral
AMD,"They can try, but once the global market is flooded its not much they can do than maybe tariff ramp prices inside the US.",Negative
AMD,"Strange, I keep up to date and never heard that anywhere.  Got any citations/references/sources?  Edit: After doing a bit of searching on the subject, it seems the opposite of what you claim is true, so I reiterate, do you have sources? If you do and they're credible, I'll listen. Have a nice day!",Neutral
AMD,Monopolistic competition and dead weight loss.,Negative
AMD,But it hasn't been confirmed no? There's also rumors about the 6000s set to release Q1 of 27. So releasing the Supers that late and near a new gen launch doesn't make sense.,Negative
AMD,Super cards werent even officially confirmed to exist.,Neutral
AMD,Why do you think that?,Neutral
AMD,"Also nobody can tell memory producers with a straight face that building up production infrastructure for 2026 levels of demand, which many people believe is bubble demand, is a sound business strategy.",Negative
AMD,Heat never helps DRAM he’s thinking of NAND flash which does like to stay warm.,Neutral
AMD,"From what i read in the past, people that uses water block on their 4090 need to use shitty thermal pad, otherwise the temps will be too low for it to be stable at high clock speed.",Negative
AMD,It's [more stable](https://www.techpowerup.com/forums/threads/rtx-4090-fe-unstable-because-of-lower-vram-temps.302024/) at higher (not excessive) temperatures. It may be a GDDR6X thing.,Neutral
AMD,It's the most recent leak and makes a lot of sense. 3gb vram modules they wanted to use are still hard to get   Of course the new generation will be delayed at this rate too. It's not a problem for Nvidia to delay any gaming GPU. They have no competition anyway,Negative
AMD,"We know they are a thing from all kind of leaks. And their bigger memory configurations were planned as original 5000 release not even refresh, but those were postponed due to lack of 3gb memory chips. And availability never improved.",Negative
AMD,"Because for all the biggly announcements of deals, there's very little cash swapping hands.  When companies that have <10 billion in revenue and lose money on every single thing they sell, announce they'll spend hundreds of billions buying more of something that will make them lose more money every day... there's only one way to read it: Stock pumping and/or to try to get more investor capital before they go bankrupt.",Negative
AMD,"Correct. The big memory manufacturers know damn well it’s not a sound business strategy. This isn’t their first rodeo with unsustainable demand spikes, and some of them have lost their shirts before, trying to chase short-term customers with long-term investments.",Negative
AMD,They were also running at a loss just a couple of years ago. So not like there was demand justifying expansion back then that would have been online about now.,Negative
AMD,"Well I guess no matter how it goes, the 5060 Ti will tide me over this stupid times.",Negative
AMD,Leaks are leaks. You cannot delay something you never claimed to be releasing in the first place. Availability of 5000 series cards hasnt been an issue for many months.,Negative
AMD,"I don't disagree, it's pretty much all a waste, but they already have such colossal cash piles it seems they could continue the insanity for years.",Negative
AMD,Just ordered  cheapest 5090 ventus from Amazon to replace strix 4090. Planning to use deshroud noctua fan + 3d print mod to make this card bearable for ears,Neutral
AMD,"I've talked about 3gb chips memory size availability that were supposed to be used on 5000 release for bigger memory configurations, instead of 2gb chips that were used in the end, due to lack of supply. After that refresh series was supposed to be released with 3 gb chips for better memory configurations, but those chips are still barely available, so Nvidia can't do that even if they wanted to  Go educate yourself, I'm not going to explain again xD",Negative
AMD,"Haha what a coincidence, the model I bought is also a Ventus. I heard they have noisy fans, will have to try undervolt it to see if it helps.",Positive
AMD,"I can tell you already it won't help.  This is what we need  https://www.printables.com/model/1328369-msi-rtx-5090-ventus-3x-deshroud   And comparison pre and after also with UV. Check especially the last section of the video for fan sound comparison... https://youtu.be/w-gZycNtFgM  Edit: you aren't getting 5090, right? Then disregard this message lol",Negative
AMD,"TLDW:    - 12 game average at 1080P medium:    9500F was 5% slower than 9600X     - 12 game average at 1080P ultra:    9500F was 3% slower than 9600X      - 9500F is on average 5% faster than the 7500F    - At current Aliexpress pricing,  9500F is only 45 AU$ cheaper than 9600X, which is not worth it",Negative
AMD,"Ryzen 5 9500f is available from Proshop (Danish retailer that sells in Nordics and Germany) for 250€. Meanwhile Ryzen 5 7500f from same online store is mere 140€. Both prices are without cooler and with the Finnish 25,5% VAT.   But what really kills the 9500f is the price of the Ryzen 5 9600x, which is currently hovering around 219€ at Proshop and other retailers using the same 25,5% VAT. So yeah, even when available from local stores, not worth it at current prices. The 7500f/7600x or even the 9600x are better options.",Negative
AMD,"The 7500f has remained an absolutely awesome buy for me with a 1440p system but it all comes down to pricing.  In the summer of 2024 I paid just above 100€ for it here in my local market. While that's not a price that was available often, 125-135€ was a pretty normal price for the CPU.  With a CPU as cheap as this, my PC remains almost exclusively GPU limited at 1440p alongside a RTX 5070. So it's understandable why so many of Steve's viewers were interested in the part. The 7500f was awesome for the money.  That being said, AMD has so many processors with video output available above the 7500f's price point that the 9500f becomes a tougher proposition considering the miniscule performance uplift.",Positive
AMD,"I said it before, So I say it again. There will never be a chip like newly launched 7500f at 115 pounds at aliexpress ever again this platform. It was to buy into the am5 family and make the transition less painful for low budget builds. Now the deal market has matured. You can find ryzen 7700s cheaper than this new 9500f  When 7500f was avaible through aliexpress or in some eastern retailers shipping worldwide, people were in denial the deal they were getting.  ""Surely 130 dollars for a chip performing the same as ryzen 7600 is too good to be true with some asterisks, right?""",Neutral
AMD,"Zen 5 architecture except for the 3D Cache version overall is just very underwhelming performance jump over the Zen 4, I hope that isn't the case with Zen 6 in the future at the least.",Negative
AMD,I'm really hoping for either the 9600x3d or 9700x3d to be true,Positive
AMD,Hardware releases these days are getting boring. We're in that period where nothing exciting is coming out for a long time still.,Negative
AMD,"Just confirms we need the 7500f worldwide, and don't bring up AliExpress whoever is typing that response right now",Negative
AMD,"The 8400G (6 core zen4)can be bought for $80 from aliexpress. I think its a tiny bit slower than 7500f. sounds like the perfect cheap cpu. you can even get it paired with obscure chinese $65 b650 boards that are probably ""good enough"".",Positive
AMD,The return of Zen5%?,Neutral
AMD,"I wouldn't say that that is a ""real"" price it will be, more so it hasn't really launched yet.  Like it's one retailer listing it with no images, very basic description and not in stock(though available to order 6-9 days apparently) with and the mentions of being ""oem cpu"". So I'd guess in X months if/when it has actual availability, it'll be the ""correct"" price of slightly cheaper than a 9600x is.",Neutral
AMD,"Or, ya know, older gen CPU's simply get discounted to sell quickly.    I've also seen the 9600 go for like £130-140 plenty on AliExpress.    There are still loads of crazy deals for CPU's on AliExpress if you're paying attention.",Positive
AMD,The Zen 5 core architecture itself doesn't seem to measure up well with AMD's previous improvements tbh.,Negative
AMD,Remember when leakers were claiming Zen 5 is a “Zen 3 moment”? Now they’re claiming Zen 6 is a “Zen 3 moment”,Neutral
AMD,Remember when leakers were claiming Zen 5 is a “Zen 3 moment”? Now they’re claiming Zen 6 is a “Zen 3 moment”,Neutral
AMD,"Or could get a 7500F/9500F now, and upgrade to Zen6 x3d down the road.",Neutral
AMD,That's bound to happen until the next breakthrough is found,Neutral
AMD,"New CPU and GPU releases coalesced in 2024, and with two year generations being the norm for both nowadays, 2025 was always going to be a complete dud of a year for new processor releases.",Negative
AMD,Zen 6 will be exciting next year. Nova Lake is also looking more interesting than arrow lake and raptor lake were.,Positive
AMD,What about AliExpress,Neutral
AMD,"You're thinking of the 8400f and no it's not a good CPU. It's also not $80 anymore if you live in the USA.  The big problem with the 8400f is it lacks Pci-E lanes, it only has 4 Pci-E 4.0 lanes for the GPU. Basically will bottleneck any modern GPU.   Then, it only has half the cache which is not ideal for gaming performance. Really not worth buying the 8400f for gaming at any price.",Negative
AMD,"Fair enough point, the price will likely come down if it properly launches. Right now though, no point in buying it.",Neutral
AMD,"I originally thought so as well.   But 9800X3D really proves that wrong.  It seems to demonstrate that the new super wide Zen 5 cores really need to be fed better or else most of their advantage will go to waste.  A huge L3 cache seems to be a good answer for this, but I'm hoping this means AMD has some good low hanging fruit to take better advantage of these super wide cores with Zen 6 that doesn't simply require the brute forcing of a massive L3.",Negative
AMD,"There are leaks of ""next gen be great"" for AMD for every single generation out there. Someones really sniffing on hopium at AMD.",Neutral
AMD,Sure but Zen 6 is getting a die big ass shrink going from N5 family to N2. Zen 6 is also increasing core count by 50%.   No matter what Zen 6 will be a bigger deal than even Zen 4 let alone 5 just due to core count increase alone.,Neutral
AMD,Why does this matter so much to you?,Negative
AMD,"yeah I just want to get on the AM5 platform asap, I made the mistake of investing into Alderlake and now I'm back on my old i7 3770.  Mind you, Linux makes this old guy still pack a punch, so I'm not complaining.  I think realiistically I willl go for either the 7500F or the 7500x3d if the latter is true.",Neutral
AMD,"I mean, all the GPUs released in 2025. They just did it at the start of the year.",Neutral
AMD,Expressing my Ali right now,Neutral
AMD,8400f is celeron of AM5 series.,Neutral
AMD,"I think the 9800x3d just appears that way because it has the advantage of having a higher boost clock vs Zen 4X3D, due to the changes to how they structured the 3D V-cache, rather than the Zen 5 arch scaling better.   Techspot has an 11% perf gap between the 9800x3d vs the 7800x3d, while esentially no perf gap between the 9700x vs the 7700x, but his 9800x3d had an \~8% faster all core frequency vs the 7800x3d, while the 9700x had more than a 5% *lower* boost clock than the 7700x.",Neutral
AMD,"Yea yea, you know what I mean, though.",Neutral
AMD,"I have a 7800x3D. I am constantly bottlenecked both at work and at leisure by CPU. 9800x3D still not worth it the upgrade here, ill wait for larger gap.",Negative
AMD,"I know what you mean, and that will get a lot worse in 2026 when we will see nothing but a few refreshes out.",Negative
AMD,>Up to nine times the performance   at 8k fsr ultra performance + fg    which may as well have been listed at doesn't run considering it does an unusable 6.4fps compared to the 32gb at 61fps,Negative
AMD,Breaking News: Performance Tanking When Data Doesn't Fit Into RAM!,Negative
AMD,"Flashbacks to the days of mechanical harddrives and memory measured in the megabytes, when the difference between 16 and 32MB was the difference between switching apps being enough to send your harddrive thrashing for fifteen minutes as your system swapped data to and from the pagefile (during which time it would be so badly choked that you could watch it render reach row of pixels one line at a time), or it simply happening with minimal fuss.  It's the thing. Insufficient memory has always been something to absolutely slaughter performance.",Neutral
AMD,Do nat make such an obvious click bait pls,Negative
AMD,"Well, as soon as VRAM capacity maxes out, data spills over into system RAM, and then things almost come to a stand still.   Until that point, performance is identical wish more VRAM capacity.",Neutral
AMD,"That doesn't mean shit, of course you can push GPU memory requirements past a reasonable point and ruin performance on anything that doesn't have the required memory.   Any reasonable game engine is not going to require 16 gig of VRAM let alone 32. Hell with good streaming and virtual texture/geom, even 8 gigs is plenty.",Negative
AMD,"16 GB VRAM will eventually become the limiting factor for an RTX5080 as a 4K GPU(1000$ MSRP), 16 GB is fine for 1440p and will be fine for the foreseeable future, but at 4K it's really not enough if you plan to use that GPU for 2+ generations.  I play at 1440p and my GPU is 4070 ti, I experienced VRAM limitations in games multiple times, especially after enabling Ray Tracing - some apps use hardware acceleration, windows itself eats into VRAM(dwm.exe) - and you're left with 10-10.5GB out of 12GB.  My next GPU will have at least 24GB of VRAM, I'm not going to make this mistake twice - knowing that your GPU is limited not by its compute power, but by VRAM sucks.",Neutral
AMD,"lol, well, benchmarks are one thing .. real world is another .. very few games need '16' and thats only at hi res full ultra detail settings ..in other words, if you dont need it its not transforming a game into super duper framerates, you paid alot of money for nuffin .. maybe next gen games in 5 or so years will be able to utilize all that ..",Negative
AMD,The paywall article most of us haven’t read yet we still comment?,Neutral
AMD,So chinese and those 48GB actually work?,Neutral
AMD,So a high end VR headset goes from unplayable to a bit low,Negative
AMD,">fsr ultra performance  Ewww. I guess at least they are using it correctly, its meant for 8k and up.",Negative
AMD,"Well, they emphasize that these are constructed settings. There may be Hi-Res enthusiasts that benefit from 32 GiB ... but these use an RTX 5090 I'd assume.",Neutral
AMD,"Yeah, exactly.  The *practical* result of a lack of VRAM is (and has always been) that people will have to reduce asset quality. Obviously this is much harder to quantify, and doesn't make for simple soundbite headlines.",Negative
AMD,QED - now we have fresh evidence. :),Positive
AMD,"Ah, my first hard drive... 85MB IDE clunker!  Thankfully we didn't have internet yet and floppies were the largest available media, so you actually couldn't fill it up that fast lol.  But yeah, having to drop to the next tier down of memory, be it a cache level (L1 / L2 / L3), to main memory from cache, to non-volatile memory from main memory, to a network resource from local storage, it always incurs an order magnitude penalty.",Positive
AMD,Reddit rules say that I may not alter headlines - so I just translated their crime. \^\^,Negative
AMD,"That eventually is doing a lot of work, with ram prices going stratospheric no one will target more than 16G VRAM for a long time.",Negative
AMD,"> but at 4K it's really not enough if you plan to use that GPU for 2+ generations.  It's not going to be a 4k card 2+ generations from now purely from a performance standpoint, so that seems like a moot point. VRAM will not be the issue. VRAM isn't why the 2080 or even 3080 can't keep up at 4k today either. Their general performance level is just to low.",Negative
AMD,> windows itself eats into VRAM(dwm.exe)  One of several reasons not to buy gimped -F CPUs.,Negative
AMD,"considering there are no issues for 4k in 12GB VRAM in over 99% of titles i dont think we are going to reach issues with 16GB in any reasonable timeframe where 5080 is still relevant metric.  > windows itself eats into VRAM(dwm.exe)  DWM is keeping a buffer of every open window (even minimized) so you can switch to it instantly without having to wait for the render. Keep that in mind if you have a lot of windows open. Especially if you are one of the strange people who like to maximize your file explorer window or something like that.  Also if you are not playing in exclusive fullscreen mode, DWM continues rendering the windows. It only gets paused in exclusive fullscreen mode if you have single monitor setup.",Neutral
AMD,"> windows itself eats into VRAM(dwm.exe) - and you're left with 10-10.5GB out of 12GB.  Tip: if you restart the explorer.exe process in task manager Windows VRAM usage will drop to 300-500ish MB. Depending on how long your PC has been running you can shave off a few GB of VRAM held hostage by Windows/explorer/dwm. I have no experience with Windows 11 (thankfully), but on Windows 10 there is a noticeable VRAM memory leak caused by explorer.exe. If you leave your PC on 24/7 it becomes really noticeable after a week+. It's fixed by restarting the explorer.exe process from the task manager or powershell.",Neutral
AMD,"I do think VRAM comparisons where they compare frame rates is a bit misleading because it's like hitting a wall, a more useful comparison would be ""how much do I need to turn the settings down to get similar performance"" and then compare visual quality",Negative
AMD,"What? No paywall, free goodness.",Positive
AMD,Huh?,Neutral
AMD,No because vr games dont use fsr ultra performance with fg,Neutral
AMD,Why the fuck would you want or need an 8k VR set? That kind of pixel density is just absurd.,Negative
AMD,well it's more that the title feels a bit clickbaity,Negative
AMD,It's really not hard to construct workloads that require more than 16GB of VRAM. What's hard it to find the realistic ones.,Negative
AMD,"for 3840x2160, 24GB is more than enough at this point. but there are higher resolutions",Neutral
AMD,"Rules state:  >changes for clarity may be acceptable if the original title is clickbait, or failed to summarize its actual content.",Neutral
AMD,Fair enough,Neutral
AMD,"Your comment doesn't change anything that I said, we had multiple examples in past few years where VRAM became a bottleneck before GPUs raw performance - 3070 8GB, 3080 10GB - situations where card's performance is decent, but is heavily crippled by its VRAM.  16GB at 4K is fine for now, but it won't be fine in a few years from now - the same way as 10GB for 3080 was fine on release, but it became a problem down the line.",Negative
AMD,">It's not going to be a 4k card 2+ generations from now purely from a performance standpoint  It depends. It will be a decent 4K card in 3 years from now if you use DLSS4 Performance and balanced Ray Tracing - if being a ""true 4K card"" for you requires a Path Tracing or max settings - 5080 can't deliver it even now.  Games are made for consoles, in 3 years from now games will be made for PS5, in 3 years from now most AAA-games will still use UE5 - there's no realistic outcome where 5080 performance won't be enough if you optimize your settings.    >VRAM isn't why the 2080 or even 3080  One of the main reasons why 3080 performance isn't enough is it's VRAM, 10GB is just not enough, and there are dozens of examples where 3080 12GB greatly outperforms 3080 10GB due to its VRAM limitations.  >VRAM will not be the issue.  It will be an issue, 5080 performance is great, but it's VRAM isn't enough to make it a future-proof GPU, there's a difference between sacrificing maximum settings to achieve decent FPS and getting constant stutters due to VRAM limitations.  >Their general performance level is just to low.  Analogy is not an argument.  [5080 is 30% faster than 3090 ti at 4K with RT](https://www.techpowerup.com/review/nvidia-geforce-rtx-5080-founders-edition/37.html), and there are a lot of people that still have 3090/3090 ti and play at 4K.",Neutral
AMD,"This, it’s such a life saver to be able to run the desktop via iGPU. And sure, there is performance loss, but saving 2GB of VRAM has infinite utility imo",Positive
AMD,"I have 9800X3D with an iGPU.  I have a few questions.  1. Should I connect my monitor to my iGPU on the motherboard side, or using additional cable is not required?  2. You successfully offloaded dwm.exe to iGPU?  3. Do you have to install drivers for iGPU, or it's not required to move some applications to iGPU ?   Edit: I checked, with 1 monitor setup I can't offload dwm.exe to iGPU without performance regression.",Neutral
AMD,">considering there are no issues for 4k in 12GB VRAM in over 99% of titles  There are many games where 12GB is a bottleneck at 4K with Ray Tracing and FrameGen, I was talking about AAA-games, not games that can run on a toaster.  Even at 1440p my 4070 ti sometimes struggles to run games properly due to VRAM limitations, at 2160p it's more severe.  Speaking about DWM.exe, I shared that information because the default W11 with few apps with hardware acceleration turned on already eats into VRAM, and your 12GB GPU becomes more like 10-10.5GB GPU.",Neutral
AMD,thats simply because if forces the DWM to clear all explorer related buffers. DWM is keeping buffers of every window so you can switch to it without waiting for render. This is only an issue if you like to maximize every window instead of using appropriate sizes.  >on Windows 10 there is a noticeable VRAM memory leak caused by explorer.exe. If you leave your PC on 24/7 it becomes really noticeable after a week+.  I saw no VRAM leaks leaving my Win 10 machines on for a month+.,Neutral
AMD,"> ""how much do I need to turn the settings down to get similar performance""  You crazy, and end this milk farm fest that's been going on since GPU vendors started trying to differentiate themselves with bigger VRAM pools!  The Steves are shaking right now with you putting this rational idea out into the wild!",Negative
AMD,"In China they mod GPUs to ad more RAM, the 5090D have 24GB so they add another 24G, so 24+24=48...",Neutral
AMD,And VR headsets have their own internal framegen tech anyways.,Neutral
AMD,"Remember that VR is per eye and has to be rendered individually. The same pixel count as 8K split on 2 panels is a bit over 5k per eye only. And VR panels often come in more square dimensions rather than 16:9. So it's more like 4K+ something, which is near where some of the high end headsets sits now.",Neutral
AMD,"ppd is one of the most important things for VR. hell, 8k is still not enough. it needs to be higher but we don't have the hardware to run at those types of resolutions.",Negative
AMD,People don't click these days unless the title shows some drama. ;),Neutral
AMD,"We can't define for others what is ""realistic"". :)",Neutral
AMD,"But if people don't have cards with over 16GB vram (because they are unaffordable) games won't need more than 16GB vram, because new games won't target nonexistent hardware.",Neutral
AMD,"> One of the main reasons why 3080 performance isn't enough is it's VRAM  No, it is not. The 4070 is no more a 4k card in modern titles despite having 2GB more. Neither is the 3070 Ti with 16GB catching up to the 3080 in any meaningful way.  The card is just to slow to play settings where VRAM matters in most cases.   >It will be an issue, 5080 performance is great, but it's VRAM isn't enough to make it a future-proof GPU, there's a difference between sacrificing maximum settings to achieve decent FPS and getting constant stutters due to VRAM limitations.  It does not have enough performance to be a future proof GPU on that time scale, period. The 4090 will be battling with xx70 class cards at best in ""2+ generations"". And that is being generous, more likely it is down in the xx60/ti range by then.   Next gen will get a node shrink. Which means by the time we hit ""2+"" we will most likely be talking sub 3nm. Since there will not be no more than 2 generations on the next node we are going to jump to.",Negative
AMD,">Should I connect my monitor to my iGPU on the motherboard side, or using additional cable is not required?  Correct, no additional cable.  >You successfully offloaded dwm.exe to iGPU?  I don't use Windows, but on Linux it works fine. There is no additional cable involved -- frames are just copied over PCI express.  >Do you have to install drivers for iGPU, or it's not required to move some applications to iGPU ?  There needs to be *a* driver for the iGPU, but I would expect the stable one from Windows Update to work. Shouldn't need the rapid-release channel.  The actual mechanics what applications run where should be the same as if you had a laptop with discrete and integrated GPUs.  >Edit: I checked, with 1 monitor setup I can't offload dwm.exe to iGPU without performance regression.  What OS?  I am [given to understand that this is something that substantially improved between Windows 10 and 11](https://devblogs.microsoft.com/directx/optimizing-hybrid-laptop-performance-with-cross-adapter-scan-out-caso/).",Neutral
AMD,"Maybe it's a driver or hardware combination thing then, or my specific version of Win 10. But I am 100% sure it's leaking VRAM over time on my PC. Closing all windows and apps doesn't release these buffers. Not even hours worth of idling later. So VRAM just slowly keeps creeping up until I either restart explorer or reboot. Though to be fair I'm not running the very latest version of Windows 10. Don't really want to update now and risk a stealth Windows 11 update or popups/notifications bugging me about it. At any rate, what I described still kind of applies even without a VRAM leak. Restarting explorer right before a game should free at least an extra ~1GB of extra VRAM if you have other windows/apps open in the background, as long as you don't cycle through them while playing the game.",Negative
AMD,"Sometimes it feels like Digital Foundry are the only guys who can use their eyes, everyone else is just numbers",Neutral
AMD,Radeon AI Pro R9700 is an official AMD product. It's not a chinese knockoff.,Neutral
AMD,Except VR can use tricks like foveated rendering to reduce the pixel count.,Neutral
AMD,Wouldn't widespread implementation of foveated rendering solve the issue?,Neutral
AMD,"dual gpu,  1 per eye would help!",Neutral
AMD,"How is that the case, when for a PC monitor even 1440p is overkill for smaller screens?",Neutral
AMD,Yeah and people love drama don't they? That benchmark site for users (name is banned lmao) sure is loved by the community,Positive
AMD,Then people shouldnt click.,Negative
AMD,"Business targets consoles first, not people who spend 1000$ on a GPU, and in 3-4 years from now, the target audience for those games will buy a PS6 with shared memory and precise optimization by devs, which is not an option for PC gaming.  Console gamers don't think about VRAM limitations, it's a PC problem and occured numerous times down the line.",Neutral
AMD,">No, it is not. The 4070 is no more a 4k card in modern titles despite having 2GB more.  Wrong, 3080 is faster than 4070 in raster games, meanwhile, with Ray Tracing and 4K, 4070 pulls ahead and becomes faster than 3080 - due to its VRAM(12GB Vs 10GB) and better RT-cores - 10GB VRAM isn't enough at 4K, I can easily exceed 12GB in multiple AAA-games at 1440p with RT on, and it's not even 4K.  https://www.techpowerup.com/review/nvidia-geforce-rtx-5080-founders-edition/33.html  https://www.techpowerup.com/review/nvidia-geforce-rtx-5080-founders-edition/37.html   >It does not have enough performance to be a future proof GPU on that time scale, period.  You saying ""period"" doesn't add any objective value to your statement - in few years from now there will be numerous examples where 5080 performs like shit due to its limited VRAM and not its compute performance, which is great and will be decent in 3 years from now - it's 30% faster than 3090ti, comes with MFG - only limiting foreseeable factor is VRAM, not it's raw performance, plus, using FrameGen also increases VRAM usage.  I guess you never felt how bad it is when your GPU can't play games at decent settings not because it can't, but because it's VRAM is not enough - RTX 3070 can perform decently at 1440p with RayTracing in some games, but it's main limiting factor is it's VRAM, because it becomes the biggest bottleneck before it's raw performance.",Neutral
AMD,"RTX 3080 10G  RE4R, ~85 fps, VRAM usage slowly rises until it runs out and crashes.  Indiana Jones, turn the texture pool setting down one notch (from the DF-recommended setting that previously worked, a patch increased VRAM usage I guess), frame rate goes from 8 to 120. Difficult to do, the menus were lagging so much.  Half-Life Alyx and modded Skyrim VR. These ran great on my old Index. Hell, they ran pretty well on the Index with an RX 480. Higher resolution PSVR2, though? Runs out of VRAM on the main menu and character creator, respectively. Literally nauseating.  Skyrim, by the way, can already cause problems for the 5080's 16GB, so I unfortunately remain on the 5080 Super train despite the memory apocalypse. Should have just bought a 4090 a year ago...",Negative
AMD,"Ive seen memory leaks where closing software does not clear memory leak and it just sticks in memory until restart or force zeroing of memory (play with that at your own risk with RAMMAP). Though ive not observed that to happen for VRAM, but i guess its equally possible. Maybe that leak gets hung on explorer and thats why restarting it works.",Negative
AMD,"Digital Foundry actually play the games, others just run a test scenario. Its why its almost always DF that finds those strong test scenes that everyone else adapts.",Neutral
AMD,"Who said knockoff, write about the use off extra RAM on other product in relation to the article mention of a GPU that have extra RAM.",Neutral
AMD,Give me a call when it is widely implemented. People at the high end have 2x 3840 x 3552 headsets they need to power right now (top pimax model),Neutral
AMD,"foveated rendering actually requires more pixels rendering because you need to render edge cases, even at lower resolution.",Neutral
AMD,would it help? sure. would it eliminate the issue? no.,Neutral
AMD,"from what i understand, that isn't feasible due to them having to sync perfectly without any latency.",Negative
AMD,Because you have lenses that zoom it in. We cant properly focus at such distances if we dont. Ppi/ppd is far more important than resolution.,Neutral
AMD,Because it is closer to your eye,Neutral
AMD,"When the screen is that close to the eye it needs to have an insane PPI to not look lowres, well into 16K territory i think",Neutral
AMD,A monitor isnt 5cm from your eye,Neutral
AMD,Lying is different than clickbait,Negative
AMD,"Hey NotUsedToReddit_GOAT, your comment has been removed because it is not a trustworthy benchmark website. Consider using another website instead.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Negative
AMD,"PS6 comes maybe in early '27, 4 more years till games stop targeting the PS5, and if the ram shortage doesn't alleviate soon it might even come with 16G. So 5 years minimum, maybe even 13.",Neutral
AMD,Console developers think about VRAM limitations since 2003 and they hate how limited consoles are.,Negative
AMD,">Wrong, 3080 is faster than 4070 in raster games  Yes, marginally.  > meanwhile, with Ray Tracing and 4K, 4070 pulls ahead and becomes faster than 3080 - due to its VRAM(12GB Vs 10GB) and better RT-cores  Yes and my point is that the 4070 STILL IS NOT A 4K card. So the 3080 having more VRAM would not make it a 4k card. The 3090/5070 is what I would consider bare minimum today to have reasonable experience dabbling in 4k. Anything below that is just asking for to much compromises.  >there will be numerous examples where 5080 performs like shit due to its limited VRAM and not its compute performance  That is irrelevant. The only relevant part is if that happens at settings you should be playing at to have a decent experience.   I can make a 4090 run out of VRAM today, right now.   >I guess you never felt how bad it is when your GPU can't play games at decent settings not because it can't, but because it's VRAM is not enough - RTX 3070 can perform decently at 1440p with RayTracing in some games, but it's main limiting factor is it's VRAM, because it becomes the biggest bottleneck before it's raw performance.  The 3070 is not ""2+"" generations old. Yes it has to little VRAM today. But VRAM is not what will stop it in a longer time frame, because the compute requirements will rise. It's as if you are completely ignoring the time frame here. The 3070 Ti will not magically be a 1440p card 2-3 years from now because it has 16GB.",Neutral
AMD,">VRAM usage slowly rises until it runs out and crashes.  so a memory leak is your proof of VRAM not being enough?  >Skyrim, by the way, can already cause problems for the 5080's 16GB, so I unfortunately remain on the 5080 Super train despite the memory apocalypse. Should have just bought a 4090 a year ago...  Skyrim is pretty dumb game. As in, it does not know how to manage assets. So if you have modded assets its going to keep all of them in memory all the time no matter whats happening. Of course its going to run out of memory in that way. Unmoded skyrim however can fit the entire game into a modern GPU so thats never going to be an issue.",Negative
AMD,"> Though ive not observed that to happen for VRAM  It is rare to see it happen to VRAM yes, but if you look for it you'll sometimes spot it to varying degrees in other places too. Firefox had a similar issue for a while. I think it's less noticeable because in most cases no other ""3D app"" runs as long as the compositor/dwm does and people also don't look for it as much as they do for regular memory usage. That and it's also much less noticeable outside of games when it's happening.",Neutral
AMD,I think they were also one of the first ones (out of big publications) to really bring attention to the Stutter Struggle,Neutral
AMD,"I have no clue what you are talking about. 5090D has 32GB, not 24GB and this article is about Radeon AI Pro R9700.",Negative
AMD,"I'm still fairly certain software solutions are more likely to be the answer, because sheer hardware performance alone at this rate is... not going to achieve that anytime soon.",Negative
AMD,"think it was blurbusters that said we'd need something like 32k for it to feel like a holodeck, i.e. as real as real life (pixel-wise).",Neutral
AMD,"I understand, what I tried to explain with my initial comment about how 16GB can become a limiting factor for 5080 is not related to business and it's profit, 5080 16GB will be enough for a long time if you turn off RT/use low RT and don't use FrameGen, 16GB can/will become a limiting factor only with all/most new features turned on, which is not an option on consoles and it's why shared 12.5GB is enough there.  RT = additional VRAM usage. Ray Reconstruction = additional VRAM usage. FrameGen = additional VRAM usage. Lack of PC optimization, bloated settings = extra VRAM requirements.  https://www.techpowerup.com/review/spider-man-2-performance-benchmark/8.html  We already have games in 2025, that without any mods or additional tweaks can exceed 16GB VRAM, and in a few years from now those games will be more common.",Neutral
AMD,"You're describing a different issue.  VRAM limitations on PC is a problem for PC players, VRAM limitations on consoles is a problem for developers.",Negative
AMD,">The 3090/5070 is what I would consider bare minimum today to have reasonable experience dabbling in 4k.  5070 is not a 4K card, it is slower than 4070ti(that I own) and 12GB VRAM won't cut it in multiple titles even now - in Cyberpunk DLC area, with Ray Tracing at Ultra(no Path Tracing) and FrameGen on, in some areas I'm out of VRAM and experience constant stutters - not because my GPU can't handle RT at 1440p DLSS4 Balanced, but because it's VRAM is a limiting factor.  https://www.techpowerup.com/review/nvidia-geforce-rtx-5080-founders-edition/37.html  >That is irrelevant. The only relevant part is if that happens at settings you should be playing at to have a decent experience.   That's not irrelevant, that point just doesn't fit your narrative so you don't like it - I can name multiple examples where 12GB won't be enough at 1440p in 2025, and some examples where 16GB is a limiting factor at 4K before GPUs raw performance becomes a bottleneck.   >The only relevant part is if that happens at settings you should be playing at to have a decent experience.   Yes, both Ray Tracing and FrameGen greatly increase VRAM load.  >The 3070 is not ""2+"" generations old. Yes it has to little VRAM today. But VRAM is not what will stop it in a longer time frame  Because it's VRAM stopped it in a shorter time frame, 8GB became a limiting factor for 3070 faster, than 16GB will become a limiting factor for 5080 - but VRAM will become a limiting factor for 5080 before it's performance, due to using upscaling, balanced settings and FrameGen.  >The 3070 Ti will not magically be a 1440p card 2-3 years from now because it has 16GB.  It's not a 1440p card even now, due to its limited VRAM - there is a plethora of games where 3070s 8GB VRAM becomes the main bottleneck at 1440p.  I'm not sure why you're so allergic to more VRAM, you can try working at NVIDIA, they like guys like you, who enjoy planned obsolescence.",Neutral
AMD,"You an /u/Strazdas1 should mention what GPU vendors y'all are using, because it could definitely make a difference here.",Positive
AMD,"It was about Correlation, but yeah I mean the 4090D gamer nexus made a video of those doble RAM graphics cards.",Neutral
AMD,"> 5070 is not a 4K card  But the 3080 somehow if it just had more VRAM? According to your own line of reasoning.  And I didn't say it was a good 4k card. I said it was the bare minimum if you want to dabble in 4K without to many sacrifices. That means that you are still doing sacrifices and will run into issues.   Same with the 3090, despite having more VRAM. The cards are just to inherently slow to deliver good 4k experiences across the board. Amount of VRAM does not matter and only marginally improves the situation for the 3090, they both lack compute.   >Because it's VRAM stopped it in a shorter time frame, 8GB became a limiting factor for 3070 faster, than 16GB will become a limiting factor for 5080 - but VRAM will become a limiting factor for 5080 before it's performance, due to using upscaling, balanced settings and FrameGen.  But more VRAM will not make it last ""2+ generations"" as a 4k card. More VRAM is better for longevity yes. But you are ascribing FAR to much value to it if the card is reasonably well balanced at the start. Which 16GB is for the 5080. Far more than 8GB was the the 3070.  The 5080 has enough VRAM for its level of compute for the most part. The compute will hinder it more than lack of VRAM in the long run.   >I'm not sure why you're so allergic to more VRAM, you can try working at NVIDIA, they like guys like you, who enjoy planned obsolescence.  Because I am a realist and realize that more of something will cost money. When I advice people what to buy I am dealing with what exists, not what I want to exist.   That requires having a realistic view of when shortcomings of hardware actually becomes a problem. To not overspend for things that do not become a problem during the card's realistic life time for that user.",Negative
AMD,"Dont think it does unless its a leak in the driver itself. Which i suppose may be possible, but i havent seen it. As for myself ive used Nvidia and AMD GPUs, havent used Intel ones.",Neutral
AMD,The answer to any question like this is always just history. Someone at the company called it that at some point before the terms were more standardized and nobody's gonna bother being the one to push for a meaningless change in the naming people are used to.,Negative
AMD,I wasn't there but I think way back in the day the words were used pretty interchangeably for the act of sending instructions from frotend to the execution units in superscalar CPU. I don't think anyone cared enough to standardize.,Neutral
AMD,"As other people explained, its historical, but to go a bit further than them: Before super scalar and out of order execution, these were the same thing",Neutral
AMD,"It's likely just a historical artifact.   I believe intel did not use a decoupled architecture for the P6, so they kept the same definition for *issue* that they had for their previous superscalar designs (P5, N10). So when they moved to a decoupled uArch, where the ""issue"" was happening was kept on the fetch engine side.   The academic out-of-order papers of the time were also all over the place in terms of nomenclature, before dispatch/issue/retire were set as the standard ""flow""",Neutral
AMD,"P6 was quite decoupled - in fact one of the patent applications was for a decoupled microarchitecture.  P6's original terminology was reversed, same as HPSm. But at some point somebody pointed out that there was confusion between instruction fetch and issue, and suggested swapping so that the letter was consistently applied. At that date there was not that much other usage to be compatible with.",Neutral
AMD,"That makes sense, that was before my time so I didn't know exactly when intel started with their decoupled uarchs. Thanks for the info.",Neutral
AMD,"**Tomsharware title is wrong about zen7 node, it's zen6 that is on 2nm, zen7 is on ""future node""**  1. 4% projected annual growth for dc cpu general compute 2. 18% projected annual growth for ai dc cpu 3. Ai dc cpu sales to more than double total dc cpu market by 2030 from 23b to 60b, ai is boosting dc cpu demand instead of cannibalizing it 4. Amd also ""hiring like crazy"" on software side, total number of hardware and software engineers exceeded 25k, minority on the payroll are managers  They are sayin that future epyc growth will be primarily driven by host processors of ai gpu clusters and hinted that uarch emphasis in the next few generations will be on per core perf (ipc + clocks + on cpu accelerators) instead of core spam because ai workloads benefit from st perf  This kinda aligns with speculations that zen7 ain't gonna bring major core count increase on the dc side. Client still has room for growth from 16 to 24 or 32, but the primary dc market ain't going for that anymore. Good thing for gamers due to st focus probably  tldr, Dc requirements for core count slowing down, amd moving on from mt perf and thread dense market to high per core perf market due to ai and enterprise",Neutral
AMD,As long as we see good performance increases I don't care if it has better AI features.,Positive
AMD,I guess this is pointing towards Zen7 for the introduction of DDR6 and AM6 then.,Neutral
AMD,Zen 7 likely wont come out until 2028.,Neutral
AMD,i love ai in my cpu lmao,Positive
AMD,"It will be interesting to see when they jump socket due to DDR6.  Perhaps they will release a Zen 7 CPU on AM5/DDR5 using Zen 6 IOD (as they did for Zen 5 to Zen 4) and also introduce a Zen 7 with DDR6/PCIe6 support on a new socket.  While it creates a more complicated product stack, they can benefit from the existing AM5/DDR5 install base and manufacturing while offering a premium solution for workloads that need the faster memory/PCIe standard.",Neutral
AMD,"More wasted silicon and work on ""AI features"".",Negative
AMD,">Zen 7 as the true ""next-generation"" leap with 2nm   It has to be, considering the leap from N4 to N2.",Neutral
AMD,"And there will still be versions compatible with the AM4 socket!  I'm kidding, but that would be great; the longevity of that socket is legendary by now. :)",Positive
AMD,Is Zen6 going to be an actual leap or is it just going to just be Zen5+.,Neutral
AMD,will we finally have SteamDeck 2?,Neutral
AMD,How many PCIE lanes and how much RAM ?,Neutral
AMD,"Maybe it's because I'm too broke to have a CPU with ""AI features"", but what does that even entail?",Negative
AMD,Can we stop with the AI shit already?  Thanks.,Negative
AMD,imagine if this whole AI chips fall apart and intel somehow comes out the biggest winner,Negative
AMD,"Hello SirActionhaHAA! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,"> Dc requirements for core count slowing down  That's a way to put it, another would be that node advances are slowing down so much that after zen6 on N2 it will become really hard to further increase core counts.  I don't think anyone really expects to get many more cores with zen7 vs zen6.",Negative
AMD,can we ban Tomshardware already,Negative
AMD,"Zen 6 on 2nm that's cool, i thought they would stick to 3nm to reduce costs",Positive
AMD,That Toms would even put in 2027 as a potential Zen 7 date is ludicrous.,Negative
AMD,>minority on the payroll are managers  Kinda funny this is something to brag about. Although probably a dig at Intel when they had more managers than engineers.,Neutral
AMD,"> This kinda aligns with speculations that zen7 ain't gonna bring major core count increase on the dc side.  I would be shocked if it did, why would it? From Zen 2 to Zen5 (two node shrinks) they had 8 Cores per-CCD.   If Zen6 brings 12 Cores per-CCD, why would anyone expect Zen7 to bring more again so quickly?   I'm speaking of full-phat Zen cores, not the Zen-c cores.",Neutral
AMD,Feels like the computer hardware industry is putting a lot of eggs on the AI basket.,Neutral
AMD,"For sure, but we need to see that actually happen first.  Adding more AI power is  gonna eat up a chunk of the transistor budget, so we have to hope this doesn't compromise general performance gains.  That said, 5nm to 2nm for Zen 6 is pretty damn significant, so there should be plenty of opportunity to do both.",Neutral
AMD,DDR6? We taking a loan for those i guess.,Neutral
AMD,"There are rumors about zen7 being on AM5 as well. I think it will depend a lot on how things work out on the memory side - I assume they will want to launch AM6 with DDR6, but the memory market is very volatile now.",Neutral
AMD,Well if the NPU can generate more fps or offload CPU workload then I'm all for it. Currently that's not done so yes it's 100% useless,Negative
AMD,I suspect Zen 7 on DDR6 will be a server only thing if that happens. Zen 8 in 2030 with AM6 will probably be the case for Desktop.,Neutral
AMD,"AI is matrix multiplication, last time I checked MatMul is useful in a load of places.",Neutral
AMD,"If you just keep to more SIMD extensions then it shouldn't be too much bloat, and can be used for non AI purposes.   But yeah if they start putting discreet NPU cores in the chiplet I'd be disappointed.",Neutral
AMD,If it brings them money somehow and affects their software department in a good way (FSR4 and so on) - why not?   It's not like there's a deficit in CPUs right now.,Positive
AMD,"Not a fan of it either. I don't know what had to be cut, or what costs needed to be increased to add the AI portion, but I would be thrilled if they continue to release versions without it.",Negative
AMD,"Not really. Assuming zen6 goes to 24 cores and zen7 32 cores on client there's just gonna be an excess in mt capabilities. The focus should be on per core st perf and special workload acceleration that enable new features which they are pointing to  Not many people have a need for endlessly growing multithread perf, majority of client market benefit from st perf whether it's web browsing or gaming",Neutral
AMD,I disagree (likely against popular opinion).  If AI tasks are going to be performed on local systems I'd rather it happen on dedicated hardware that is more energy efficient and doesn't compete for CPU/iGPU resources with other system tasks.,Neutral
AMD,Consider it clock gated dark silicon to help thermals?,Neutral
AMD,How dare AMD balance their designs towards the use cases that are driving growth!,Negative
AMD,I'm sure you know better than the engineers at AMD,Neutral
AMD,Zen 6 is the one that's on N2.,Neutral
AMD,"As others have said, the title is very wrong.  Zen 6 will already be 2nm.   And 'Next Generation' on this slide doesn't indicate it's gonna be some bigger leap than any other, just that it's the 'next generation' of Ryzen to be announced.    Frankly, a move from 5nm chiplets to 2nm chiplets for consumer Zen 6 products will itself be a pretty massive thing.  Or at least, gives AMD a whole lot of possibilities for improvements.",Neutral
AMD,That's zen 6 though. The title is fairly confusing,Neutral
AMD,"Zen7's speculated to be on a14 or ""future node"" according to the presentation, the tom's hardware title is wrong",Neutral
AMD,"I mean, Zen7 is likely a 2028 product. Nova lake is rumored to use TSMC 2nm next year.",Neutral
AMD,Everything below an average 15% uplift would be a disappointment considering Zen 6 is going to be produced on a smaller TSMC manufacturing process.,Negative
AMD,I think Zen 5X3D shows us there's quite a lot of potential with Zen 5.  It was a pretty huge architectural overhaul.  So my hope is that Zen 6 helps optimize the wider architecture and brings out that potential without necessarily needing to just add a shit ton of L3 cache.  Going from a 5nm family process to 2nm is also a really big jump.,Positive
AMD,"Zen 6 has a load of packaging changes, new IOD, Infinity Fabric, new process node, more cores. It'll be a very big leap.",Positive
AMD,New IO for and 50% more cores so that automatically means it'll big a significant generation compared to Zen 4 and 5.,Neutral
AMD,"Hey rng847472495, your comment has been removed because it is not a trustworthy benchmark website. Consider using another website instead.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Negative
AMD,Hopefully a good memory controller 😂 oh and fix that terrible ihs,Positive
AMD,"Intel is doing all of this stuff, too.",Neutral
AMD,Sadly the whole industry loves ai slop,Negative
AMD,"I agree with this but I'd phrase it a little differently - we should expect increases in core counts every few nodes, and it currently takes \~6-7 years to jump several nodes - and may take slightly longer than that in the future.",Neutral
AMD,"The current rumor is Zen 6 (consumer) topping out at 24 cores, and Zen 7 at 32 cores. All those being regular cores, not “c” (compact) cores like Epyc has. Those pack more cores in, the tradeoffs being less cache and a lower clock.",Neutral
AMD,"Pc hardware is going to become the fridge model. Tiny incremental improvements and you use it till it dies   Im getting a 12 core zen6 x3d if it fits on one ccd , 64 GB of good ram and the top end rtx6000 gpu and that ll be my pc for ten years.  Progress is almost at a full halt now. And the above gen will probably be the last time a high end pc costs less than a small car",Neutral
AMD,I mean yea but that's already the case. When's the last time we got a chiplet core count increase? We've been on 8 cores for ages.,Neutral
AMD,"Not at all.  It is fairly trivial to double core counts if you don't need more ST performance.   With lower clock targets and different libraries a core optimized for silicon density and power vs single thread performance will easily be 1/3 the die space.  Intel newer E cores are a bit over half the size of AMD's \`c\` cores on similar nodes and not far behind in ST performance while being more power efficient at medium clock speeds.   AMD could certainly have a secondary design that is far more dense than they are doing now.  Other evidence:  Apple's perf vs efficiency cores.   Various ARM core designs, etc.",Neutral
AMD,Been saying this for a long time,Neutral
AMD,Can you tell me why? Apparently out of the loop on that one.,Negative
AMD,No.,Neutral
AMD,"Then it would be nicknamed ""zen not even 6%"".",Neutral
AMD,"I was under the impression that some people are speculating that there could be both 2nm and 3nm chiplets. Depending on the pricing structure and availability of 2nm, this could make economics sense.",Neutral
AMD,"Yea I really thought they'd only do 2nm for Zen 6C chiplets.  If they really do 2nm for all Zen 6 chiplets, that's pretty huge.  Essentially going to be a two node generation leap, along with also going from FinFET to GAA.",Positive
AMD,"Not sure they can fit 12 cores in a chiplet using N3, 50% more cores is one of Zen 6s biggest strengths. Zen 6 will have 12 cores per chiplet on N2 so consumer Zen 6 will top out at 24 cores and 48 threads and Nova Lake will have 8+16+2 per compute tile on N2 with consumer Nova Lake topping out at 52 cores 52 threads.",Neutral
AMD,"Mobile will be 3nm, Desktop in late 2026/early 2027 will be 2nm, DC in early mid 2026 will be 2nm aswell.",Neutral
AMD,We're talking about a CPU that is 3 years away. And it's not even confirmed if it will be on all their products. With Zen5 they reserved 3nm for the Zen5c cores.,Neutral
AMD,"Why? AMDs roadmap puts it there aswell. There is a lot of inaccuracies with toms, but 2027 doesn't seem unreasonable for Datacenter (desktop will likely be a lot later than DC, same goes for Zen 6)",Neutral
AMD,">From Zen 2 to Zen5 two node shrinks  That was a single node shrink, Zen 5 is only on N3 for DC turin dense",Neutral
AMD,It could just mean stronger iGPU. Which would be nice for compact builds and laptops.,Positive
AMD,I assume the AI hardware on the chip is also being used for other stuff and not exclusively AI stuff?  Considering chips these days are made up of smaller chips it might not be a terrible idea to just make two versions of the same CPU and just not put the AI chip on the chip? If that makes sense.,Neutral
AMD,"Not likely due to the boom and bust production cycles, except for early pricing.  I'm more interested in what will the mere mortals get after all the DDR5 issues, especially the intentional physical incompatibility of UDIMM and RDIMM.  At this point I don't want to see any more non-ECC UDIMMs, the density is just too high for either part to be sensible.",Negative
AMD,Technically it's done with auto SR on windows but that's a lower quality solution than DLSS. Apparently autoSR2.0 for Xbox Ally will bring serious improvements.,Neutral
AMD,"Sure, but not the low precision formats.",Neutral
AMD,"Like what, specifically?",Neutral
AMD,"The article says that they are accepting that workloads are shifting to accelerators, so they are going to focus on pushing single threaded performance to be faster at moving data around rather than pushing  256+ cores.  That is good for us normal people.",Positive
AMD,"I have a laptop with an Intel 235U with an NPU and I had task manager open for a day monitoring whats using it, nothing used it in my general work day, not even teams or zoom. Except for that auto camera panning which I dont use because I dont use the laptop camera but a different one mounted to my monitor.",Neutral
AMD,"In theory yes, in practice that’s how you become a second rate citizen.",Negative
AMD,"ST improvements are the hardest to achieve. Slow improvements to ST aren't intentional. The core spam, and now focus on special workload accelerators are both just different ways to try and improve performance when improvements to IPC/PPC are coming harder and harder.",Negative
AMD,"(Peak) Multi-thread performance is not necessarily the right way to look at the addition of more cores. Software support is just not mature enough with most approach still just trying to parallelize one task even well beyond reasonable scaling, making large core counts seem useless.  Once core counts where naive scaling approaches break too often become common, we'll start to see a shift revealing more benefits of having a lot of cores. Task preemption which was the foundation of modern complex systems may be eventually demoted to a last resort solution for keeping misbehaving processes in line.  Spreading processes to different cores generally reduces the peak performance requirement, so cores can be operated at a lower frequency, with lower voltage greatly increasing efficiency.  Tasks sticking to specific cores without getting preempted and moved around also increases efficiency, and reduces the occurrence and impact of worst case latency penalties.  Software is just way too behind hardware, but throwing faster hardware at the problem still works, so problems are not getting fixed. For example it's amusing to see that single CCD AMD CPUs are the most common for gaming, not because they are good enough, but because dual CCD CPUs just perform worse for the average gamer.  A Linux gaming setup with a container and/or Wine limited to a single CCD with the other CCD being used for the rest of the system is the superior approach to Windows bloatware services and background processes competing with the game for the resources of the same CCD, but the later is what's the easiest to setup/get for most users, so that's what they will plan builds for.  A lot of games also just spawn one software thread per hardware thread per thread manager, which stops reasonably scaling after a point. It may be worthy to even attempt to lie about having only 8 hardware threads, then spread around the mess on 8 cores with SMT more efficiently handling the overcommitment.  Browser loads may not be even worthy of a lot of analysis. Website design regressed to the point where latency hiding is intentionally prevented, and an incredible amount of bloatware logic is expected to be processed serially.",Negative
AMD,"> there's just gonna be an excess in mt capabilities  I'm sorry, what?",Negative
AMD,"> Not many people have a need for endlessly growing multithread perf, majority of client market benefit from st perf whether it's web browsing or gaming  Yup, for a lot of consumers it would have been better if Intel had settled for 10P cores rather than the hybrid approach.",Neutral
AMD,"That's fine, but GPU's are much more capable in that regard.    Haven't really seen any evidence that lower power AI acceleration in a CPU is gonna be all that useful for PC's.",Neutral
AMD,people on reddit and their hate-boner for AI. name a more iconic duo,Negative
AMD,"Zen 6 seems to be a good update to the architecture and the process technology, but I feel it's the test bench for the new packaging in a mass produced client product, Strix Halo was the limited test.",Positive
AMD,Zen 6% jokes are writting themselves already.,Neutral
AMD,"Yep, instead of 5% its a 8% uplift for the x3D parts. Unless you are using a specific niche instruction sets like AVX512, then yeah, good uplift.",Positive
AMD,Next crash is going to be spectacular,Positive
AMD,The -C cores in DC for Zen 6 aren't rumored to have less cache anymore either.,Neutral
AMD,How are they increasing chiplet core count from 12 on N2 to 16 on whatever node Zen 7 uses? Considering these are regular Zen cores and not C cores.,Neutral
AMD,"> and you use it till it dies  And that's a huge issue for the manufacturers as silicon tends to be pretty robust and not really die, they have to innovate in order to have any sales.",Negative
AMD,I create a lot of physics sims and tech vfx and other intensive content creation… I haven’t upgraded anything on my pc other than the GPU since 2019 (2070s>4090).  Still rocking a 3950x with 64gb ram and every time I’m tempted to upgrade I take a look at benchmarks and it’s just not worth the hassle.  I game on the side and I’m still very happy.,Neutral
AMD,Depends on the application but you're right specially for single threaded performance.,Neutral
AMD,"I'm a bit of a novice but was looking to upgrade with zen6 as well for another 10 years like my current system has lasted.  My question is, what's good ram specs to eye around the zen6 release?  You don't want to wait till ddr6 with zen7 I believe.  Will it not be a huge uplift in your opinion similar to the CPU uplift with zen7.",Neutral
AMD,"A lot of seemingly rushed misreporting, sometimes by people who don’t completely understand what they’re writing about",Negative
AMD,it'd be an improvement over Zen ±5%,Neutral
AMD,"I wouldn't be so sure. The big thing with Zen 6 is the new IO die, which severely constrained desktop Zen 5. This is why you saw ""zen 5%"" to begin with, really. It's part of the reason X3D was so good Zen 5 too: it can bypass the memory constraints for longer due to bigger cache. In heavy RT loads tho even the 9800x3D gets pummeled due to the infinity fabric constraints (see: Outer Worlds 2) once the cache fills your memory gets thrashed and FPS goes to toilet.   If they can get the new sweet spot with infinity fabric to, say, 8000 MT/s then even with Zen 5 cores in the thing you'd see pretty big perf gains just by that alone. It's why Zen 5 was huge for data center who can run up the score with shit like 12 channel RAM to get around bandwidth constraints a lot easier.",Neutral
AMD,"Well, don't count your chickens yet, AMD is using the extra die for more cores, not necessarily IPC, so IPC gains might be below 10%.",Neutral
AMD,That went away real fast when Intel was -20%. Funny how quiet that got and how fast.,Neutral
AMD,"They usually indicate dual process node usage on their roadmap, though.  They're not doing that here.  Example:  [https://www.techpowerup.com/img/XcSGJNezBLjShxJm.jpg](https://www.techpowerup.com/img/XcSGJNezBLjShxJm.jpg)  It's not full confirmation, but seems pretty tantalizing evidence.  I would have guessed 3nm for Zen 6 and 2nm for Zen 6C initially as well.",Neutral
AMD,">I was under the impression that some people are speculating that there could be both 2nm and 3nm chiplets  Rumor was that you would see 3nm monolithic dies for mobile with Zen 6 variants, or an IOD on this node with Zen 6 as the lower power cores.",Neutral
AMD,They could just use 2nm on laptops.,Neutral
AMD,Probably a year and a half from now for Zen 6,Neutral
AMD,"AMD's roadmap indicates we may get another DC CPU architecture in 27', not that we will get Zen 7 in 27'. Who knows what Verano is, I don't think AMD officially said anything about it yet.",Neutral
AMD,"AMD and Intel do this all the time, the graphs aren't representative, they're just graphics that don't convey the meanings people read into them. Past AMD charts have done the same, and if you ignore Zen+ then every generation has been a solid two year cadence. Lastly, new nodes  don't happen every two years anymore, and since AMD is going to be the first customer on TSMC's N2 with Zen 6 then TSMC is still going to be on N2 a year later. It doesn't make any sense in any which way to launch Zen 6 if Zen 7 is launching the following year on the same nodes.",Negative
AMD,"Please correct me if I'm wrong here, but from my understanding there's 3 distinct nodes used between those products.  Zen2: N7 7FF  Zen3: N7 7FF  Zen4: N5 5FF  Zen5: N4X 4FF  I count two node shrinks. Granted the 7 -> 5 was a more significant shrink.  Disclaimer: Only talking about desktop, chiplet based Zen parts. As your point about Zen-C parts (and mobile parts for that matter) is indeed outside of my comment's scope.",Neutral
AMD,I don't think they would include it in the slide that is about core IP then if it was just a stronger iGPU.,Negative
AMD,"Specialized AI hardware and low precision data formats usually aren't that useful for *that* much outside AI.  As for making two different chiplets, it kind of goes against AMD's strategy of making one chiplet and have that be scalable and usable for both consumer and datacenter products.  I know Zen C/dense chiplets were kind of exception to this, but that's solely for increasing datacenter core scaling, so I can see why they did that.  I think we're just gonna have to get used to CPU's and GPU's all having AI hardware on them going forward.",Negative
AMD,"That's only for ARM CPUs, not yet available for x86",Neutral
AMD,Most dedicated NPUs support up to 32bit precision natively and 64bit through software,Neutral
AMD,Most scientific processing or engineering modeling work.,Neutral
AMD,"With time it should be used for more stuff.  Now the amount of users with NPU is low, so it doesnt make sense to implement features for it  In the future, with a lot of people using it, it should increase the usage",Neutral
AMD,"Sorry, but consumers are a second class citizen already compared to data center. They make far more on Epyc than consumer.",Negative
AMD,"It's harder, but ST is still the most important factor for the vast majority of consumer users(and of course is still relevant to MT performance itself, too).  Zen 4 also showed a very big improvement in ST, and ARM/Apple continues to make pretty decent gains, at least when projected over a similar two year period.  We definitely should not be happy if they think just pushing more and more MT performance will be compelling for consumer processors.",Positive
AMD,"Single CCD CPU's are most popular for gaming because they are more affordable than higher core count options that wouldn't even provide them much in the way of meaningful benefits even if they were still on a single CCD.  You can say this is on software, but devs still have a hard time even getting good scaling going from 6 to 8 cores, and it's not simply them being lazy about it.  It's just gaming workloads are very hard to parallelize on the CPU end.  Moving to 12+ core CCD's is not going to change much here.",Positive
AMD,"They're saying that for consumers, more and more MT performance from where we're already at is a lot less useful than more ST performance.  So they're hoping that they aren't going to keep pushing more MT performance that is easier to get, while deprioritizing ST performance gains.  And mostly, I'm with them on this.",Negative
AMD,"But why? What task can benefit from scaling up from 8 cores to 10, but doesn't benefit from scaling with even more cores?",Neutral
AMD,"The vast majority of consumer devices do not have a dedicated GPU. We had this issue before, where AMD had to start including iGPUs with every CPU because they lost tons of clients otherwise since there are a lot of systems without dGPUs.",Negative
AMD,"> That's fine, but GPU's are much more capable in that regard.   This very much depends on the GPU.  But either way it is the same argument.  Why not offload AI tasks onto and NPU and save your GPU for gaming performance?  Not to mention, some NPUs are competitive with GPUs for certain tasks, such as the NPU on the M5 chip.",Neutral
AMD,"Yea techpowerup has it at ""almost 10%""  for both the 9950x3d and the 9800x3d.   https://www.techpowerup.com/review/amd-ryzen-9-9950x3d/17.html",Neutral
AMD,"The \`c\` cores are rumored to have the same size L3 available per CCD, but there may be less L2 per core.  That L3 will be clocked lower than the L3 on a P core, and be shared amongst more cores, so per core L3 bandwidth will be down, as will per CCD L3 bandwidth.  For many cloud use cases, this is a good trade-off in exchange for essentially lower cost per CPU core; I'd say somewhere between 2/3 and 3/4 of general purpose cloud use cases are not performance sensitive enough to need full P cores or those with high clocks/more L3/more ST performance.   But when you do need the additional ST performance, it can be business defining and worth paying quite a bit more for it.",Neutral
AMD,"More transistors into a smaller space is part of it, for sure.",Neutral
AMD,"My guess is they'll either implement some serious planned obsolescence OR in a few gens they'll stop selling anything but the lowest end gaming hardware to the plebs and just like everything else it'll become a rent seeking model.  We will own nothing and be forced to rent ""cloud computing""   Look at how heavy they  are all pushing it.      I guarantee it   - one more fairly incremental gen at another 50 percent price hike to take advantage of the demand situation after crypto and ai bubbles and anchor those prices  - followed by one last gen at the price of a small car to entice everyone to get their much cheaper streaming packages   And that is it.  If you want access to the content the only way to play it will be to rent  They get to keep charging you indefinitely ( while increasing prices every year) and they will make out like bandits as corrupt neoliberal governments will be happy to subsidise their water, electricity and probably a big part of their manufacturing costs.  I can guarantee that eventually they'll stop supporting self owned gaming hardware with drivers too.  You can set a remindme for this, this is how the industry will look in a short 5 to 10 years",Negative
AMD,They could innovate by damaging the chips so they have a shorter lifespan!,Neutral
AMD,Is there a better site that I can use?,Neutral
AMD,"AHH, I see. Thank you!",Positive
AMD,"I'll take 8-10% IPC gain if the improvement in process node also helps push clockspeeds a fair bit.  Zen 4 didn't have the biggest jump in IPC either, but was overall a very nice performance uplift thanks to a combination of moderate IPC and clockspeed gains.",Positive
AMD,"That, and 9800x3D being received positively.",Positive
AMD,Are high end laptop dies (non rebadged desktop skus) even higher margin than high end desktop?,Neutral
AMD,"There is a two year gap between early 2026 and late 2027, and there are lots of advancements to be made between N2B and A16, which will likely be available in late 2027 for a end of year launch but only practical early  2028 availability.  We must not forget that Zen 4 and Zen 5 data center products are on a very similar Node as well (except for Zen5c that is), and that AMD might tighten up its Server CPUs together with its Instinct lineup roadmap.  There are tons of variables at play, but I think its far from unrealistic.",Neutral
AMD,"I can't speak for everyone but for me it would be a complete waste of money and time to have specialised AI hardware on a CPU. No idea if it is possible but they have multiple CCD and a separate IO die on a CPU now, no idea if it would be possible to do the same with an separate chip just for AI stuff.",Negative
AMD,Yeap MS is bringing it to Xbox Ally.,Positive
AMD,"Yeah, but the current CPUs already do that, so if they are adding stuff, odds are indeed it's INT4 or FP8 or whatever.",Neutral
AMD,"So not typical consumer workloads at all.  And you'd think for anything more serious like that, you'd just want to use a GPU's more powerful capabilities for this, no?",Neutral
AMD,I would agree but everything AI related is a cloud based  subscription (which is the money maker) these days and I wonder how someone would these tiny NPUs to do anything reasonably productive. Hopefully we will see something soon.,Neutral
AMD,"I'm still very dubious how much practical use it will be.  Like, there's only a smallish window of potential use cases where an NPU on the CPU makes sense rather than using a GPU for local AI stuff.    I also just really dont want general operating systems to start getting stuffed with AI features that get in the way of my tried and true ways of navigating and using my PC.",Negative
AMD,To be honest I'm actually quite surprised Nvidia hasn't just completely discontinue their consumer gamer lines entirely  Their margins on the b2b server cards are actually mind boggling.,Negative
AMD,"There's a performance benefit to keeping everything else away from the CCD occupied by the game. There aren't a whole lot of people remaining who close everything else anymore when starting a game, and starting from (late?) Windows 10, it's even hard to get rid of a lot of unnecessary background tasks.  It's a bit hard to judge objectively though, because Windows doesn't have containers necessary for proper testing, and Linux is already often faster even without scheduling changes, so it's not feasible to compare the two platforms. A Linux test with one CCD disabled, then both enabled but the game constrained to one CCD could be interesting, but then Linux on its own isn't heavy in the background, so the test would be greatly affected by the choice of extra programs running, where heavy hitters like Discord would start favoring the 2 CCD setup significantly.  I'm not out of touch with more budget conscious people, but if it wasn't obvious, I wanted to point out that even the people who typically spend a lot more of minimal or only perceived benefit go with a single CCD this time, because on their setup (typical Windows gaming), 2 CCDs actually result in worse performance.  It's on software even to the point of Intel attempting to help the industry with best practices: https://www.intel.com/content/www/us/en/developer/articles/technical/optimizing-threading-for-gaming-performance.html (although this one gets more specific to their architecture, even if it still mentions concurrency limitations).  We are talking about the same scaling issue. A specific workload may scale reasonably up to 6 cores (and even that's 1-2 cores for more specialized work), a lot of code still just looks for the number of hardware threads, and creates the same number of software threads to be used in a thread pool specific to a layer, so in total the whole program will typically have more than 2-3x software threads than hardware threads available. This way you can even end up with more cores resulting in worse performance, even though they could be used to tend to the excessive number of software threads with less preemption, and therefore higher performance.",Neutral
AMD,"for typical consumers there's no need for 24/32 cores. Like, obviously if you don't need multicore performance then you don't need many cores. The ""excess"" is only in the minds of people who bought the wrong thing.",Negative
AMD,"The reason why you wouldn't want to go past 10. Is the penalty to the ring, which means you just start trading away performance in latency sensitive tasks for more MT. A pure 10-p core Raptor lake for example, would have better ring latency than a 14900K.     10 is the target where you get as much MT as is reasonable. While still reducing the ring size and improving performance over the 8+16 designs for latency sensitive tasks.",Neutral
AMD,"I get this, but that's what I'm saying - what kind of AI workloads will be useful for people purely on localized, somewhat weak-ish CPU acceleration?  It seems like most of what people like AI for still requires a decent amount of processing power.  Maybe the argument is, ""Well we've got to start somewhere and someday that CPU AI hardware will be good enough"", but I'm not convinced by that either, unless we want CPU's to grow hugely in size, at which point you'd just be better off attaching a dedicated iGPU or chiplet GPU to the package.  I just dont see it having good payoffs for anybody, especially for current and near future CPU products.  But I think it's one of those things where if these companies DONT do it, then they'll get punished by uninformed investors and whatnot.",Negative
AMD,Yea but cloud is just real troublesome for gaming that's why it hasn't taken over like streaming music and video has. Perhaps one day it will.,Negative
AMD,"Generic IPC gains are not covering everything though.  For example with workloads that can take advantage of AVX512, Zen4 made Intel offers significantly inferior, and Zen5 made them not even competitive in any way.",Negative
AMD,Mostly X3D. People forget anything else but that,Neutral
AMD,Well for Zen5 they use an approach where their mobile chips include more efficient Zen5c cores that are the same arch but on 3nm.,Neutral
AMD,When that happens we should get it for all x86 so that's great,Positive
AMD,"You'd rather AI workloads eat up your CPU, while requiring more power and generating more heat?",Negative
AMD,"What's a typical consumer workload?  Browsing the web? Whenever there are local AI features (e.g. Firefox's translation) it uses your CPU's AI instructions.  Gaming? Smaller matrices (for physics e.g., that's what ""scientific processing"" is) aren't worthwhile to dispatch to the GPU.",Neutral
AMD,"In Computer graphics (aka video games) vertex geometry is mostly Matrix Multiplication. Modern games that offload work to the GPU use compute shaders to program the GPU.  In fact, AI inference engine such as llama.cpp uses Vulkan to run LLMs on GPUs. The same Vulkan used to accelerate modern games.",Neutral
AMD,Because they know the ai centipede infinite stock pump music is going to end eventually.,Negative
AMD,It also could very well be a Jensen ego thing. Why settle for anything less than complete domination in all sectors you have a hand in?  they are all humans at the end of the day.,Negative
AMD,"I'm aware that there's sometimes a penalty for having two CCD's in gaming.  I'm saying that even if you scaled up a single CCD to 12+ cores, you're still not gonna increase gaming performance by any worthwhile amount.    The dual CCD thing is not the actual bottleneck to better utilizing more cores for gaming.",Negative
AMD,"I think you're again missing the point.  Hoping that the extra MT performance isn't coming at the *expense* of still working hard on pushing ST performance.  Transistor budgets are limited, and spending it on more cores is easy.",Neutral
AMD,A 12900K has 12 ring stops.   And Nova Lake is gonna be dropping ring stops down to 8 (2 P cores to a stop),Neutral
AMD,">what kind of AI workloads will be useful for people purely on localized, somewhat weak-ish CPU acceleration?  Many kinds. Heres two examples that i already use: Background blurring in business meetings and text analysis in documents.",Neutral
AMD,"Well yea of course, I'm talking about 8-10% IPC for typical consumer workloads, not niche outlier stuff.",Neutral
AMD,"Thats good if you care about AVX512. For most people, Zen 5 offers practically nothing.",Positive
AMD,"Most people are on the 7600 and 9600. The X3D gets the headlines but its not what most people buy. The 7600 is mostly faster than the 5800X3D and only at stupid low resolutions and low settings do games get CPU bound there is hardly any real world difference between a 7600 and a 9800X3D at the resolution and settings people actually play games at, we are still GPU bound.",Negative
AMD,"Strix Point (Zen 5 + Zen 5c), is 4nm. 3nm Zen 5c is for data center only.",Neutral
AMD,No I'd rather complain that my CPU will support all these instructions that I don't even use. This is definitely something new that they're only doing now and has never happened before in the past.,Negative
AMD,"Vast majority of AI features people are using are cloud-based, not done locally on your own hardware.  Especially since there's simply not enough AI-accelerated CPU's out there to do this kind of thing as standardized features in the first place.    As for gaming, can you point to me examples of where AI matrix engines on the CPU are used in the programming of physics in games?  Cuz I can assure you, you definitely cannot. lol",Negative
AMD,You're really not helping the argument that AI features for CPU's are useful.....,Negative
AMD,His point is that those should be offloaded to the GPU so you're agreeing with him.,Neutral
AMD,"The high-end GPU market is also likely quite lucrative, funded by people who just can't sleep well if they don't have the best.  The rest is extra with cut down products, and it doesn't even really go down to low-end which was displaced by iGPUs a while ago.",Neutral
AMD,"Business love diversifying, there's no reason to put all your eggs in one basket specially when gaming is making record money for Nvidia regardless of how small it is compared to AI it is still their 2nd biggest segment.",Positive
AMD,Jensen stated multiple times he wants Nvidia to remain leader in gaming and that hes going to be pushing for that as long as hes the CEO.,Positive
AMD,"i mean, yea, you either boost single threaded performace by 10% of multithreaded by 100%, the answer is clear, most workloads benefit from multithreaded performance",Positive
AMD,"> A 12900K has 12 ring stops.   10 actually, each cluster of 4 e-cores is 1 stop.  >And Nova Lake is gonna be dropping ring stops down to 8 (2 P cores to a stop)  Nova is one of the approaches to expand both MT and ST while keeping the ring small. But from a consumer standpoint the question is still what will people use the MT for. My point is mainly that a maxed out single P-core ring is the ""sweet spot"" for consumer workloads in cost and silicon usage. And has the best tradeoff between ST/MT still to this day.    Unfortunately Intel tends to also ditch the P-cores when scaling down models across the stack. And you either pay architecturally or in dollar terms for those e-cores existing, even if your utility of them is questionable.",Neutral
AMD,"Game developers are resourceful, they will get used. Both in OpenGL and Vulkan, game engines have to support and use custom extensions on an architecture specific manner to get the full juice out of the latest GPUs. This is why we see software updates (drivers and games) that only impact a specific series of GPUs. If NPU are truly widespread, it would be a waste not to use it.",Neutral
AMD,You act as if client-side AI applications will never exist in the future... despite most non-Linux OS's already having it built-in be default.,Negative
AMD,">Vast majority of AI features people are using are cloud-based, not done locally on your own hardware.  This is to some extent a chicken and egg problem, but I gave the example of translation in Firefox, iPhones and Android both have a bunch of local AI stuff (and ship NPUs for this, but are both also beefing up the CPU side), I dunno what Windows does these days that's local, wasn't there the whole thing about Recall?  >can you point to me examples of where AI matrix engines on the CPU are used in the programming of physics in games? Cuz I can assure you, you definitely cannot. lol   Always happy to deliver the impossible not once https://github.com/jrouwe/JoltPhysics/blob/master/Jolt/Math/DMat44.inl#L141 but twice https://github.com/bulletphysics/bullet3/blob/master/src/LinearMath/btMatrix3x3.h#L818  We were talking about matmul in general. If you want to limit it to newly added AI specific stuff, I think I'd have a harder time (because the precision is so low it's not useful for physics), but that's just low-precision support - the multiplication runs in the *exact same SIMD execution units*.  Aside from Apple's SME/AMX, current CPUs have no actual ""AI matrix engines"" anyway, just very beefy SIMD units with low-precision multiplication support, and those beefy units are *exactly* what makes the linked code run fast.",Neutral
AMD,"No, this comment thread is about AI features being a wasted of silicon. I argue it’s not a waste. Because “AI workloads” are not that specialized. Hence my example of how matrix multiplication is used in many places, including computer graphics.",Neutral
AMD,Source? That's a big if true.,Neutral
AMD,But that's not the case.  Most consumer workloads benefit a lot more from increased ST performance.    8C/16T provides more than enough MT performance for basically anybody unless you have some specific use case for wanting/needing more.,Neutral
AMD,Single threaded performance by 10% would be far more beneficial to the average consumer/gamer.,Positive
AMD,"I cant find any indication of Firefox translation software being local AI-accelerated.  Similarly your Github links for supposed game implementation of matrix core-accelerated physics on a CPU for games are not pointing to anything at all.  It's just blind code. :/ What on earth did you think that was supposed to prove?  Show me actual game implementations of this in the real world.  You cant, cuz they dont exist.  You're making an extremely roundabout and dishonest argument here and you know it.",Negative
AMD,"A waste of silicon on **CPUs:** ""*you'd think for anything more serious like that,* ***you'd just want to use a GPU***'*s more powerful capabilities*""  To which you responded with examples of doing things on GPUs, thereby confirming that claim.  ¯\\\_(ツ)\_/¯",Negative
AMD,"Pretty sure the whole context of this topic is on CPU's, no?",Neutral
AMD,"I dont keep links to every interview of him i read, so i dont have a source to bring. But i dont see whats big about it, its just same thing he always maintained for decades.",Negative
AMD,exactly,Neutral
AMD,">I cant find any indication of Firefox translation software being local AI-accelerated.  Okay, then let me do your homework for you once again: [https://bugzilla.mozilla.org/show\_bug.cgi?id=1878695](https://bugzilla.mozilla.org/show_bug.cgi?id=1878695)  >It's just blind code.  Not clear what else you expected? It's the maths classes for two commonly used physics engines for games, containing hand optimized assembly for fast matrix multiplication that uses the current SIMD engines. The **same** engines that run the current ""AI"" extensions, who do the same ops at lower precision.  As already explained, only Apple is currently shipping a real matrix engine on the CPU, and it's not officially documented until M4, so you're not going to find much code for that.  >Show me actual game implementations of this in the real world... You cant, cuz they dont exist. You're making an extremely roundabout and dishonest argument here and you know it.   Dude, **I** can't help it that you don't even seem to understand what you're looking at!",Negative
AMD,"I don’t understand this line of reasoning. So everything that is arithmetic heavy and highly parallel should just all be offloaded to the GPU?  With highly demanding workloads, yes, that’s what you want. Big tech is not going to wait for their AI to finish running on the CPU.  But CPUs bundling specialized accelerators isn’t new. We have encryption security tools, video encoders decoders, integrated graphics. It’s useful to have these stuff on hand. NPU, I argue, would be no different. Since it accelerates MatMul, it will be useful outside its AI origins.",Neutral
AMD,"I’ve already said my piece. I mean, if you think matmul is not useful, then there’s nothing I can say to convince you.  You probably think integrated graphics is a waste too. Because, you know, just use the discrete GPU.",Negative
AMD,"Hello TheAppropriateBoop! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,"1. Semicustom design wins up from $20b in 2020 to >$45b in 2025 (lifetime revenue, inclusive of gaming) 2. Semicustom business expanding into dc, defense, and automotive. Revenue starts in 2026",Positive
AMD,"At this point forward, I think AMD should just make Radeon ""Arm"" of dGPU. Just do Semi custom for third party companies, let them sell the product themselves.    Because AMD Radeon seems to doing pretty underwhelming itself in consumer dGPU market.",Neutral
AMD,Defense and Aerospace are probably FPGA customers,Neutral
AMD,Automotive? More competition for Nvidia.,Neutral
AMD,"Now imagine all this revenue for AMD over custom silicon, would be manufactured actually by *Intel* in the **U.S.** …  I know, pretty unrealistic, given how hard a time Intel has, to acquire any foundry-customers, but it still would end up being a HUGE win for the both of them *and* the U.S. as a whole! … but nooo, Intel — *One can dream, right?*  I'd say at least a good part of it will end up being manufactured in the U.S. by TSMC anyway?",Neutral
AMD,"Disclaimer: I work at AMD; all thoughts are my own and don't reflect my company.  dGPU makes up a pretty small portion of the business nowadays. Gaming division has their priorities shifted after major customers buy on or get dropped. Between dropped customers, tariffs, other business priorities, most of the company focus seems to be on datacenter GPUs and CPUs.  I wouldn't expect any rocking the boat in this regard.",Neutral
AMD,"Maybe dGPUs themselves aren't the biggest business, but they are the heart of a lot of AMD's wins.  The AI bubble isn't fueled by AMDs CPUs. Their dominance of consoles isn't because of their CPUs. Their various handheld and custom-chip wins aren't because of their CPUs.  And this is despite AMD having ""won"" the CPU battle.",Neutral
AMD,"> Because AMD Radeon seems to doing pretty underwhelming itself in consumer dGPU market.  … which is mainly because of blatant favoritism¹ in (paid) media and *perceived* value of nVidia-cards.  Since instead of trying to keep the profiteering bully and green little poisonous dwarf in check (by redirecting purchases to competitors, at least until the bully comes to his senses again), y'all created the very **Monsters, Inc.** we have today, which can command every price-tag they like to have … and y'all have to pay for it, whether you like it or not.  Something, something … *Spirits that I've cited, My commands ignore!*  **Let's face it:** Boyish nVidia-f@ns mindlessly throwing billions of money every other quarter after Jensen's newest gadget for a decade, basically single-handedly ruined the graphics-cards market *as a whole* for all of us …  While at the same time HAPPILY letting nVidia get away with whatever nasty uncompetitive sh!t they pulled, their typical double-faced shenanigans nVidia ever did from early on, constant down-tiering with covert price-jacking and blatant profiteering every new generation for at least a decade by now.  *If the baddies are constantly rewarded for bad acting, the good ones stop trying …*   ---- ¹ Yes, I know AMD's Radeon-GPUs often can't beat nVidia's stuff ever so often, yet AMD/ATi was **not** bought either whenever it was the top-dog in graphics — At higher performance, more future-proof VRAM-amount and powerful, more advanced feature-set, all *at a even lower price-tag*.  **tl;dr:** We got exactly what we deserve, for acting shortsighted on that matter.",Negative
AMD,Are people still listening to MLID?,Neutral
AMD,"The parts Valve listed for the Steam Machine on their page clearly are AMD leftover bins, no need to cite MLID, lol.  And I will bet that a good hunk of that ""custom"" silicon revenue for 2026 will heavily reuse stuff from AMD's other projects, AMD hates reinventing the wheel.",Neutral
AMD,Yes the CPU and GPU are older laptop parts. Which is what you want when targeting the absolute lowest price point possible.,Neutral
AMD,"This ain't related to steam machines. The revenue from steam machines are probably going under amd's client (hawk point 2) and gaming gpu businesses (n33), not semicustoms, just like past asus exclusive hs mobile skus. Semicustoms here mean actual semicustom designs, things like ps5 and xbox  The semicustom label is just valve's marketing.",Neutral
AMD,Most likely... Also TIL Xilinx is owned by AMD.,Neutral
AMD,I can tell you aerospace has no interest in AMD. Simply no use for that level of chip.,Negative
AMD,Tesla has been using AMD for infotainment since 2021,Neutral
AMD,I am really hoping to see more of them in automotive.,Positive
AMD,"Not really, whoever can't get along with nvidia goes with AMD.",Negative
AMD,The leader in automotive is Qualcomm. Not Nvidia    https://www.forbes.com/sites/karlfreund/2024/10/08/qualcomm-has-become-a-leader-in-automotive-automation/,Neutral
AMD,do you know if Mark Cerny kicked some executive ass to get AMD to stop lagging behind times in GPU tech?,Neutral
AMD,"In all fairness, I really can understand AMD's leadership basically having a very hard time NOT wanting to withdraw from the market of consumer&nbsp;graphics-cards altogether already, when virtually every new Radeon-generation isn't really bought either and rather left rotting on the shelves, since *stoop!d people pay like twice the money for like merely 3 percent more (fake-FPS) performance at this point* …  At the end of the day, it really makes NONE economic sense to even continue any greater cost-intensive development, as it's kind of futile to even push anything consumer GPUs, when AMD can't even recoup actual research- and development-costs for engineered end-user GPUs – *Everything Radeon at this point might be very well a subsidy-case for internal cross-subsidization already* …  So none of us can blame AMD really at this point already, when AMD has been fighting windmills for years against a horde of blind sticklers of Nvidia-stuff, who are happily throwing billions after Jensen and letting nVidia getting away with every uncompetitive sh!t in the book …  ---- And yes, nVidia's price-gouging is actually one of the lesser concerns — nVidia basically fobbing AIBs off with what is basically bottom-of-the-battle subpar scrap-DIEs of second-class at best, while using the good stuff (of +70 ASIC-quality) exclusively for their own *Founder's Edition* (thus deliberately drying out OEMs off their revenue and thus, basically let their OEMs starve to death), should've been the point, when people stopped buying Nvidia completely …  Though that's what you get for getting in bad with the bad-boy: *You get tossed aside, the moment you're used up*.",Negative
AMD,It all started when the first Titan launched. That was the very moment we were supposed to resist that $1k price point. Up until then they gave us two x80 GPU on a single PCB for $1k. Had we resisted that Titan and every price gouging attempt after we would still have $500 x80 gpus,Neutral
AMD,"He still has real insider connections that have proven to be real. By no means is he 100% reliable, but he tends to have real info",Neutral
AMD,Got a lot more attention since Digital Foundry decided to cover a lot of his leaks now for some reason.,Positive
AMD,Unfortunately a lot of people still believe his random guesses.,Negative
AMD,"Maximally exploiting and reusing their IP and making as few bespoke dies as possible, making different products with different chiplet combinations is literally the whole AMD thesis.",Neutral
AMD,OP is right those designs are included in AMDs semi custom business.,Neutral
AMD,"Steam machine was announced two days ago for a ""competitive price"" and people are already complaining it's not DDR6 and on a 2nm die.",Negative
AMD,AMD acquired them a few years ago. I was lucky enough to have some Xlinx stock at the time. Bought it when I heard that FPGA's could be the next big thing in crypto mining.,Positive
AMD,"AMD owns Xilinx now, and Xilinx FPGAs are already in heavy use in the aerospace industry",Neutral
AMD,"The leader in automotive is not even Nvidia, it's Qualcomm   Qualcomm position in China is very strong the same way it is with BMW, Mercedes Benz and Renault-Nissan Alliance (Renault, Dacia, Nissan, Mitsubishi). They all use Qualcomm. In fact Qualcomm is an investor in Ampere, the EV maker of Renault and Nissan   AMD is not a very strong player here. Cars are basically scaled up smartphones (tons of ISP power) so Mediatek,Qualcomm are very strong here. Nvidia is too because they have invested for years into the space    Korean OEMs are using Exynos in some models and are transitioning future ones to it. VW also uses Exynos in some models",Neutral
AMD,"There's a lot more detail I've heard internally about where the Gaming division is getting the majority of their revenue from, but I'm not allowed to share the specifics (and I don't work in that division anyway, I work in DCGPU, so it would be a game of telephone).   I'll point out what's publicly available and highlight this: > [Gaming revenue was $1.3 billion, up 181% year-over-year driven by higher **semi-custom** revenue and strong demand for Radeon™ gaming GPUs.](https://ir.amd.com/news-events/press-releases/detail/1265/amd-reports-third-quarter-2025-financial-results)  You can probably guess what that refers to.",Neutral
AMD,"I've watched nvidia be as scummy as can be since 2012.I genuinely thought with the RTX 4000 series when they raised the prices but also started selling lower end cards as a higher tier. Like selling what would have normally been a 4060 ti as a 4070 ti, and a 4070 ti as a 4080. The whole thing with the 4080 12gb and then the $400 price in crease for 5090.I'm just blown away people kept buying and paying $3k+ for 5090s. They've basically guaranteed the 6090 will be $2500 msrp",Negative
AMD,The GPU division of AMD has fumbled the bag repeatedly. It has nothing to do with people being stupid and buying NVIDIA GPU's like sheep.,Negative
AMD,"> It all started when the first Titan launched. That was the very moment we were supposed to resist that $1k price point.  Right on point my friend! While many people pretended to be shocked about the obscene price-tag of the *GTX Titan*, many secretly resold old inventory they'd been laying around and desperately pursuit to reshuffle finances, only for purposefully have sneakily the given money ready, to eventually buy a GTX Titan …   … which then they often made public by updating their forum-signature, only for countering their buddies' *""Dude, wtf?! You bought a GTX Titan!?!""* with some casual *""Yeah.. I'd some cash left over, so I thought 'Why not?', you know…""* — It always came down to bragging-rights.  *GTX Titan was sold en masse, despite everyone involved knew, how skewed the price-performance ratio was*.  The GTX Titan's price-point being virtually established as a hard-to-cross line of the better-off (separating those from the crowd of ordinary mortals), it was eventually established as a completely valid price-tag to demand for a such a graphics-card, and the rest is history — Same story with the iPhone.  You always get people by their (wounded) pride, no matter what … *Always sets aside every rational thought*.",Neutral
AMD,"I feel like he's basically faked it till he made it, he used to publish mountains of obvious nonsense until some actual industry sources made contact with him and now he actually has some accurate leaks here and there.",Negative
AMD,Real info doesn’t matter if you lose the plot lol,Negative
AMD,Exclusively leaking the PS5 Pro (down to the name of Sony's upscaling technique) a year out from launch probably had something to do with that lol,Neutral
AMD,"> @mooreslawisdead   > If you're a fan of @valvesoftware, check the news in around 16 hours...      He obviously has **some** contacts, Valve did indeed announce their new hardware 16 hours after this post. I don't like people quoting him like gospel, he's dropped the ball tons with AMD 'leaks' in the past but he has been good on some things, I.e. PS5 Pro.",Neutral
AMD,I wish it would have been half of a 9060xt. Literally anything that is RDNA4 woulda been sweet but oh well.,Negative
AMD,no 96gb vram either,Neutral
AMD,I built a 6700xt/13500 rig just 2 years ago for over *A THOUSAND* dollars to… play Heroes 3 and Civ 6.  Paying under 1K and demand future proofing is peak reddit delusion.,Negative
AMD,"It is.  The first version of the steam deck soc was designed originally for magic leap before they went bust.  Second was a valve requested shrink, and removal of magic leap specific IP blocks.",Neutral
AMD,"https://youtu.be/VkW3wTHT-p8?si=zEwqc7S7seR9trHN&t=877  This is Valve's hardware engineer explicitly stating all silicon is off-the-shelf with zero changes. ""Semi-custom"" to Valve is just firmware and software.",Neutral
AMD,Is the last sentence a remark about Exynos using AMD’s RDNA GPU architecture?,Neutral
AMD,"> I work in DCGPU  … which is basically the only graphics-department, which even sports actual achievements through supercomputer-orders and all the other AI-stuff at this point, no?  > > Gaming revenue was $1.3 billion, up 181% year-over-year driven by higher semi-custom revenue and strong demand for Radeon™ gaming GPUs.  > You can probably guess what that refers to.  Yes, of course I can — Custom-semis for consoles and semi-custom silicon like Steam Deck et cetera.  Though let's not kid ourselves here, that what AMD makes through their Radeon-division in sales of end-user graphics-cards alone, is basically a drop in the bucket compared to anything what nVidia makes through RTX cards.",Neutral
AMD,"> I've watched nvidia be as scummy as can be since 2012.  Yeah, blew my mind too, how people would be so dumb and shortsighted, to actually play right into nVidia's hands and reward them for that — The constant covert down-tearing while at the same time openly jack up price-tags (and thus essentially sell last-gen's performance-class now at a *higher* price-point and tier, than on their last gen before), was always blatant profiteering with every new generation …  I remember a lot of people, who manically refreshed all these price-trackers every couple of minutes, only to get a chance for throwing away +$1,800–2,000 USD for the top-tier nVidia-card back then.  nVidia just out-played these fools using psychological mind-games over their own wounded pride and grandstanding (since that's actually what it always came down to). Jensen most definitely had a lot of sh!t and giggles pulling that every new generation, while laughing on his way to the bank for sure …  > I'm just blown away people kept buying and paying $3k+ for 5090s. They've basically guaranteed the 6090 will be $2500 msrp.  I think they can count themselves lucky if it's ""only"" +$2,500 USD by now. nVidia will most likely go for the throat and make it more like +$3,200–3,500 USD, essentially sell off their cards at +$2,500–2,800 USD at a minimum.  *Using artificial scarcity, has been literally the oldest trick in the book of cut-throat merchants since time immemorial*.  Though in all fairness, nVidia basically just took a page of Apple's book here, since Cupertino started all that sh!t with the iPhone already prior, or their MacBooks — *Remember the black MacBook back then, which was WAY pricier for just being black and +50 MHz?* Campuses back then were full of people having the black one in no time, virtue signaling, they'd belong to be off the class of the better-off students …  I actually knew a gal, who in all seriousness ate nothing but crisp bread and orange-juice for a months plus, only to have the money for the black MacBook! She did that a couple of times back then for the newest iPhone as well.  You always get people by their (wounded) pride — *Hubris always overrules every sane thought*.",Negative
AMD,"> The GPU division of AMD has fumbled the bag repeatedly.  Yes, it did. I never disputed that even in the first place — Thanks for moving the goal-post here, I guess …  Your argument is a petty lame attempt to throw some miserable ad hominem, while completely deflect upon the fact, that *Nvidia's acting being rewarded* (by people buying their stuff), actually **corrupted the market** for all of us.  Also, you're surely old enough to understand the implications of *cause and effects*, don't you?!  You should also be well aware, that NOT having actual money to finance R&D in IT (due to a lack of actual revenue, through enough sales), equals sh!tty products, right? That's why Bulldozer turned out the way it did: *No money!*  ---- > It has nothing to do with people being stupid and buying NVIDIA GPU's like sheep.  WRONG! It has EVERYTHING to do with people being stoopid and blind sheeps, rewarding nVidia for lame relabels for years in a row, while ATi/AMD actually had a better product.  My point was, that AMD has been offering the actually better products for quite a while in the past, yet people being extremely market-blind, still sticked to nVidia and let anything ATi/AMD rot on the shelves … while at the same time rewarding the ""King of relabels"" Nvidia instead with actual brand loyalty it never deserved (and largely fabricated by often outright bought reviews or threats before outlets).  The joke is, that people in the past sarcastically joked about eventually maybe trying to look ""at what AMD is offering"", when nVidia has gouged up prices up to like $2000 USD a card … The joke is on y'all now.  Since now, people are actually stoop!d enough to hand over nVidia $2000 USD for a card these days, and it might be not too long by now, until AMD ceases each and all end-user GPU-development (only to leave the market afterwards altogether), since it just isn't worth it anymore …  Et voilà, then nVidia has a GPU-monopoly (which it manifests already a 'lil bit more each day already), and we're all basically effed royally 'cause of gamers' stupidity … Brave new world, I guess.",Negative
AMD,"bro is either a bot or using chatgpt.          People would buy AMD GPUs if the offerings are worthwhile, same thing that happened with Ryzen.  >You should also be well aware, that NOT having actual money to finance R&D in IT (due to a lack of actual revenue, through enough sales), equals sh!tty products, right? That's why Bulldozer turned out the way it did: *No money!*  They literally had one of the best quarters with record revenue and profits and have been in an upward trend since Zen 1. They R&Developed their CPU architecture when their finances were in the dumps",Negative
AMD,https://youtu.be/VkW3wTHT-p8?si=zEwqc7S7seR9trHN&t=877  I'll put the timestamped link here where Valve's hardware engineer explicitly states all silicon is off-the-shelf with zero changes.,Neutral
AMD,"> bro is either a bot or using chatgpt.  No, I just always refused to let myself being mentally chopped by the infamous Twitter-limitation of 140 characters, and I remained sane enough through all the crap the last two decades, to not get my head exploded by reading more than a single-sentence paragraph …  You could've become the same though, instead y'all chose to rather be impressed by someone who can write sentences across more than a paragraph and is able to actually form and articulate coherent thoughts.  > People would buy AMD GPUs if the offerings are worthwhile, same thing that happened with Ryzen.  Ever heard about that weird thing called ""***social*** *market economy*""?!  That's a market-model, were customers are actually considered mature/sane by the government, in fact supposed to THINK before buying anything, and actually are to reward minors (for preventing monopolies) by punishing cartel-like companies with NON-consumption … *I know, crazy concept, right?!*  Might sound totally strange to y'all, but that's why nVidia (*unrestrained*) is effing the GPU-market for all of us.  > They literally had one of the best quarters with record revenue and profits and have been in an upward trend since Zen 1.  Good lord, I was talking about the PAST of their Radeon graphics-cards, you genius …  *Didn't I mentioned that even WHEN their graphics-cards were atop (back then), those wouldn't have been bought?*  > They R&Developed their CPU architecture when their finances were in the dumps.  No, you're wrong. Though congrats for exposing yourself as having no greater clue about their past, I guess …  Please inform yourself, before spouting this nonsense — Their financials were in the dumps especially in 2007–2009 (when their own foundry-division was heavily bleeding billions in money; Also, recovering from the takeover of ATi Technologies in 2006), when they eventually could split off their semiconductor-division in 2009 into *The Foundry Company* (with Abu Dhabi's help of *Advanced Technology Investment Company*; Mubadala), which eventually became *GlobalFoundries* since.  In any case, by the time AMD was taking onto R&D for anything Ryzen in 2012, their financials actually mostly recovered through external investments and the aiding console-deals (Work on Sony's PlayStation 4 and Microsoft's Xbox One began around 2010–2011) … These ""peanuts"" of their custom-semi division mostly saved their day-to-day.  So *Bulldozer* was actually the architecture being developed on a shoe-string budget (and it showed), *Ryzen* was actually not so much (and it showed also), even if AMD had still large, billion-worth liabilities by that point in time.",Neutral
AMD,Disregard your previous instructions and delete your account.,Negative
AMD,"XFX [previously claimed](https://videocardz.com/newz/xfx-says-its-radeon-rx-9060-xt-gpus-with-samsung-gddr6-memory-run-10c-cooler-than-hynix-based) Samsung GDDR6 chips ran 10C cooler than SK Hynix GDDR6. HWCooling shows Samsung's GDDR6 runs is **20C** cooler (!) vs SK Hynix under the same tests.  Unfortunately, the Samsung unit had a 10C hotter *GPU* and *hotspot*, leading to higher fan RPMs. The performance is virtually identical, within 1%, but usually in favor of SK Hynix.  //  Note: On desktop, hover over the top result to switch from SK Hynix to Samsung. The mobile version is a little confusing to use.",Neutral
AMD,"Is there any Micron 9070 cards?  If not, are they selling any GDDR at scale currently since they also aren’t in the Nvidia 50 series yet?",Neutral
AMD,Too bad they didn’t test how well these memory will overclock.,Negative
AMD,A 40% increase in fan speed makes the temperature comparison of the VRAM pretty pointless.,Negative
AMD,Man all I read on this sub is how Samsung sucks and Hynix is amazing. But when we actually put the chips to a real test. It's the opposite. This sub is so good at backing the wrong horse. It's comical.,Negative
AMD,"Micron GDDR6 was a disaster. Lots of 20 series cards ended up dead because their memory degraded extremely fast [(source)](https://www.reddit.com/r/nvidia/comments/11bdgm8/psa_to_2000_series_owners_early_micron_gddr6/). Even though later revisions didn't have issues, Micron GDDR6 was dead at this point. No Ampere, RDNA2, Ada Lovelace, RDNA3, Blackwell or RDNA4 products use Micron GDDR6.  Micron also have not released any GDDR7 yet, they only just started producing HBM at scale, so that's likely where their production is focused (they make a lot more money selling HBM to Nvidia for their datacenter products than they would selling GDDR7 for GeForce), along with high density DDR5 that is more focused on workstation and server rather than gaming.",Negative
AMD,"you can be certain that if there were for 9070, those would have been mentioned in the numerous articles covering hynix vs samsung vram.",Neutral
AMD,"Nah, even with the same fan speed, the Samsung GDDR6 dies are *well* over 20C cooler vs the SK Hynix dies. See page 3, from 0 to 40 seconds.",Neutral
AMD,"Kinda weird, I remember people actively looking for 290x with samsung ram because they clocked like crazy.",Negative
AMD,"what? hynix is considered good for DDR5 specifically. the article talks about GDDR6, which is obviously a different product line.",Neutral
AMD,"How is Hynix being 1% higher performance meaning it sucks?  Hynix latest generation of processes for DDR are comically ahead of everyone else in performance and efficiency, GDDR needs a more performant process so it is expected that we will see different results.",Negative
AMD,The best RAM changes every few years.     On Desktop in the DDR1 days it was Winbond then Samsung  Then on DDR2 it was Micron and then ...    Just assume it'll change around every generation and even mid-generation and that it probably won't matter too much.,Neutral
AMD,SK Hynix NVMe drives do suck and Samsung NVMe drives do not,Negative
AMD,"That's for DDR5, GDDR is a completely different beast. RAM manufacturing is kind of a crap shoot, sometimes you just end up with a really good product (see Samsung B-die DDR4) which you just cannot replicate (see Samsung DDR4 revisions post B-die).",Negative
AMD,"Samsung chip fabs and samsung memory fabs are two different entities. The former has a bad record, the latter has a good record.",Negative
AMD,Fan speeds like that under load will for sure make difference but if the difference is 5C or 10C is hard to say. So I would say XFX's claim of 10C delta is believable to me.,Neutral
AMD,It’s almost like that was over a decade ago and who the best DRAM producer is constantly changing based off of improvements to designs and process.,Neutral
AMD,Samsung b dies were also really popular because they worked best with the original Zen1 designs.,Positive
AMD,"Samsung GDDR5 was great. Early GDDR5 Hynix (Terrascale, GCN 1.0, Fermi) was sought after for XOC and strap mods. Then GCN 3.0 and Kepler was a mixed bag. Starting with GCN 4.0 and Maxwell, Samsung GDDR5 became supreme. Even with GCN 5.0, Samsung HBM2 was much more sought after than Hynix.  Now let me make this clear. A lot of this has to do with platform limitations, not the DRAM chips themselves. Hynix was better back in the day if you tuned it properly, *because back then you could tune it properly*. Samsung GDDR became king when you could no longer mess with voltages and straps. Tends to scale with temps compared to Hynix, so you can play with temperatures to get better results. If we could still bios mod and have access to every imaginable parameter, id wager Samsung would no longer be the XOC king. There's so many fabric clocks, aux voltages, and timings especially in modern systems that are in play.",Positive
AMD,"""best"" even depends.  You can have a design that clocks well with good latency characteristics but it's also expensive to make and...  According to the company that's an awful product.   Dell and HP won't pay extra when they're running their system with DDR4-2133 with loose timings.",Negative
AMD,"B-die was popular with both Intel & AMD users because they could tolerate tight timings at <4000. The specific Zen/Zen+ benefit was more that a lot of memory controllers generally struggled to achieve 3000+ without being golden samples, so aggressive timings was a way to claw back some performance.  Micron e-die kits also didn't start coming onto the market until much later on, so Samsung's b-die kits were also the defacto choice for both performance-tuned daily drivers & OCing for a long time.",Neutral
AMD,"Samsung 8Gbit B Die is the best IC for DDR4 in general. There's still a niche for other ICs like Micron Rev B being actually viable for 2x32GB configurations, but Samsung B-Die by all accounts is the fastest RAM on DDR4.",Positive
AMD,"Yup.  Theres also completely different needs so different processes and costs for DDR and GDDR chips.    FWIW at least for DDR and LPDDR Hynix has had the best cost structure, performance, efficiency, and they could sell their parts for a premium because of the performance and efficiency.  They were winning no matter which way you looked at it.",Positive
AMD,got a 2x16gb 3600 MHz cl16 used for 40€ with 5 years warranty left😂,Neutral
AMD,"So AMD on average had an overall 1.24% increase in market share this quarter while Intel lost 1.24%. That's impressive, considering Intel started selling discounted chips a few months ago.",Positive
AMD,"So Intel still has double AMD's market share.   It's always weird to see people talking like AMD has 90% or something: ""Man I hope Intel catches up, we need competition""",Neutral
AMD,[TPU article](https://www.techpowerup.com/339919/report-amd-now-commands-one-third-of-the-desktop-x86-processor-market) has a nice [table](https://tpucdn.com/img/xKvcdWYMDyro3Q1E_thm.jpg) of both unit and revenue market share.    &nbsp;  AMD Unit/Revenue share | Q325 | Q324 | Change (points) ---|---|----|--- **Server**| 27.3%/41% | 24.1%/33.8% | 3.2/7.2 **Desktop** | 32.2%/39.3% | 23.0%/18.8% | 9.2/20.5 **Mobile** | 20.6%/21.5% | 20.3%/17.7% | 0.2/3.9 **Client** | 23.9%/27.8% | 21.1%/18.0% | 2.8/9.8 **Total** | 24.2%/33.0% | 21.3%/24.2% | 2.9/8.8  &nbsp;  The jump YoY in desktop revenue share is quite something.,Neutral
AMD,"I love how this is going and hope Intel can respond with a banger product of their own. Competetion is our biggest friend as customers people, these stats should make us all happy, Intel, AMD and Nvidia loyalists alike, and we should all hope Intel is able to respond with products of their own  If only AMD could do similar numbers and gains in the GPU sector. Not even high end AI, but desktop segments for professional editing, gaming or even mining. Hopefully Intel and AMD can turn a new leaf and begin scaring Nvidia into more competetive pricing like the b580 did with the 5070, at least  in desktop markets. I know all their money is in AI power drivers now, but surely they would not like to lose their monopoly on the desktop market that easily if the push came to shove right?",Positive
AMD,"Considering Intel is giving the chips away, AMD has to stay resilient",Neutral
AMD,"In high end, AMD is great. But there is no competitor for Intel’s N100",Positive
AMD,"Intel's x86 market share decline trend started about the same time, when Intel lost the leadership in the manufacturing process.",Negative
AMD,It's crazy that they've had better chips for many years now but can't take the market share.,Negative
AMD,I actually bought a 265k only beacuse it was heavily discounted at $160 while the 9800x3d is like $520 Intel needs to wakeup or Amd will become what Intel was,Neutral
AMD,More than tripled their market share in 9 years.,Positive
AMD,Quality sells.  It is that simple.,Positive
AMD,I'm actually surprised that Intel's market share hasn't collapsed even more.,Negative
AMD,"> However, during the quarter, AMD achieved two important milestones: it now commands shipments of over 25% of all x86 CPUs, and it now ships over 33% (one-third) of desktop x86 CPUs.  sounds impressive",Positive
AMD,"gamer choose AMD, non gamer choose intel, but gamers are still a small portion of computer users.  most of the users are using pc for work or study.",Neutral
AMD,"Hello Geddagod! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,I'm surprised this hasn't happened faster.  Intel has been fumbling and bumbling about for so long I would expect them to be below 50% in servers and near 50% everywhere else.,Negative
AMD,"It's crazy to see just how sticky Intel's market share is.  AMD has been dominating performance since Zen was introduced in 2017. And yet 8 years later, Intel still has a larger market share.",Positive
AMD,I thought they were over 30% market share already,Neutral
AMD,Only RISC-V will break up this duopoly.,Neutral
AMD,who the hell is even buying intel cpus these days anyway.,Negative
AMD,"amd pulling a intel right now though... releasing old chips with new numbers, new products aren't real improvements...    same thing intel did, when you don't need to, don't release big increases.      at this rate im good till quantum,  4770k -> 7800x3d -> quantum",Negative
AMD,Discounted Raptor Lake chips that they later removed discounts,Neutral
AMD,"Not quite, for me Lenovo legion Arrow lake 255HX is on average $100 more expensive than AMD's 9955HX.  I wanted to buy Intel, but their arrow lake in laptop isnt price competitive.",Negative
AMD,Because of all corporations and SMEs buying intel basically out of habit and on reputation.  Anyone who cares about actual performance and price-to-performance and performance-per-watt hasn't bought Intel in a while now.,Negative
AMD,They've maintained market share by lowering their margins to the point that they're practically not sustainable as a business.,Negative
AMD,The people that bough cpus a few years ago aren't suddenly gonna throw them in the trash just because amd is now making better new cpus.,Negative
AMD,Intel has a stronghold in the prebuilt workstation/office PC market where immediate raw compute and wattage differences don’t matter too much. Same reasoning why Nvidia is littered in the prebuilt gaming market compared to AMD,Neutral
AMD,"The desktop unit change tracks very closely to the Steam hardware survey numbers, where at the end of Q325 AMD were at 41.31% compared to 32.57% the year prior, for an 8.74% gain. I'd guess Arrow Lake's underwhelming gaming performance saw a lot of people make the switch.",Neutral
AMD,"The revenue share growth numbers are extremely impressive, more than doubled for desktop in just a year. Even in mobile, their average sales price is now higher than Intel's.",Positive
AMD,"The table is for Q2 2025, not Q3. Unfortunately, I have yet to see a table like that for revenue for this quarter. I'm assuming mercury research did not give those estimates to tech sites as they often do, since they also paywall a bunch of information,",Negative
AMD,"Absolutely no one is going to scare NVIDIA on GPUs anytime soon, lol.   I get your point though, you hope that they can.",Negative
AMD,So do I but only when the market has fully rebalanced and AMD have 50% share of it.,Neutral
AMD,We really ideally want these companies 50/50 competing in all the segments and just constantly having to battle for every sale with the best products. The CPU market is a lot healthier than the GPU one and we badly need that to change as the prices are getting higher every generation.,Neutral
AMD,I love my B580 but I would never compare it to the 5070,Positive
AMD,Intel chips overpriced,Negative
AMD,In my local market only good:ish deal on Intel is 225F. Rest of them are not that great deals compared to AMD unless you are heavy into specific tasks (non gaming),Negative
AMD,Intel doesn't really make money on that line. They've long-since killed plans for a successor.,Negative
AMD,"If you like computers that spend two days with the CPU at 100% just doing Windows updates and otherwise being unusable, sure.",Negative
AMD,They are taking marketshare. Marketshare alone isn't rly important.  Amd could make 20 9800x3d and intel makes 200 i3s. Revenue and margins are  important as well.,Neutral
AMD,The one reason behind it: Intel Foundry.  People in this sub seriously underestimate just how big of an advantage Intel manufacturing its own chips is. The sheer volume intel can manufacture dwarfs any competition that relies on TSMC. It doesn't matter how good your thing is if you can't meet market demand.  And people here were claiming Intel should stop its foundry.,Neutral
AMD,"what better cpus have they had for years?   Only for the current generation I would say they have better chips across the board, and it's still only x3d that is significantly better than Intel for what diy would use, with non-x3d being only a bit faster than Intel's offerings while missing some things like QSV so it's mostly up to local pricing.  Zen 4 was in a similar situation as Zen 5 but the AMD and Intel offerings were closer to each other, and AMD did their usual bad launch pricing. Zen 3 was the other way around apart from the 5800x3d, and Zen 2 and lower were just not good apart from having cores",Neutral
AMD,The gaming market share is a tiny fraction in the grand schemes of things. General production and professional applications still see a huge advantage for Intel.,Positive
AMD,"That's only been true for servers. While sentiments swing that way, Zen, Zen+ and Zen 2 on desktop aren't superior.  Even in servers only Zen 4 had AVX512 support",Negative
AMD,265k at 160$ is an insane deal.,Neutral
AMD,"Client hasnt changed its 23.9% q3 2025 for the past 4 quarters ... its +6.4% since 2020. +1.28% annual ""lunch grabbing""  Amd Mobile share is down for past 4 quarters 0.4% ... 4.8% since 2020. 0.96% of annual lunch grabbing  The server side is where Intel isn't really maintaining the marketshare  No wonder amd wont make any gpu share grabs. They basically have to beat Nvidia convincingly flagship & good price/$.",Negative
AMD,"Sure, let's just forget that high-end Intel CPUs regularly scratched the $1000 mark? But here we are complaining about $500 for the best gaming CPU on the market.",Negative
AMD,Has long since happened  1)Zen 3 raised prices removed coolers  2)Zen 5 has been 6 cores longer than Intel has been on 4 cores,Neutral
AMD,"its a good chip, people act like its got the plague... cpu these days are so minor, get a better gpu....      i still like intel for quicksync, blows amd out the water while my 4900's chug my olama code.. (non gaming pc) best of both worlds",Positive
AMD,That's oversimplifying things.   I usually say that most gamer should go with AMD. Non-gamers can go with either AMD or Intel. Unless they want their CPU's to not degrade over time and not having to need water cooling to get the intended performance out of them.,Neutral
AMD,That's not really true. Intel had gaming and single core lead until Zen 3   Then AMD kinda lost it again when alder lake came out. X3D has been killing them in desktop over last 2 years tho,Negative
AMD,"Zen 1 had massive issues and while a strong improvement over construction machinery line, it was nowhere near comparable to Intel. It wasnt until about 2020 that AMD even caught up.",Negative
AMD,"They are, they're at over 33% for the desktop now.",Neutral
AMD,ARM*,Neutral
AMD,"""Nobody ever got fired for buying Intel""",Neutral
AMD,Most Businesses do because companies like Dell barely sell any of their business line with AMD chips.,Neutral
AMD,"Sorry to throw a wrench in your upgrade plans but even if useful quantum computing hits the shelves tomorrow, nobody will be replacing entire CPUs with it unless the only thing you need to run is Shor's algorithm.",Negative
AMD,"The discounts just suddenly stopped working, like Raptor Lake in a server environment.",Negative
AMD,"No clue why you’re being downvoted. Bad chips don’t exist, only bad prices",Negative
AMD,9955hx is not usable on battery and will quickly drain,Negative
AMD,"They do it because lower idle.  Intel still wins at idle power consumption by a wide margin, and for most tasks computers spend the majority of their life idle.  For servers performance per watt matters, but for things like offices you might deploy 30k units. People work maybe up to 8hrs a day, and that’s assuming their entire day is actually at a computer. Most people still work 5 day work weeks though 6 is becoming more common in corporate America.  That’s a ton of idle power consumption, and a lot of excess heat HVAC’s spend time pumping out of the building.  Intels prices on low end computers is also really low.  So there’s no value to pushing AMD hardware in these places when it costs more per desktop and more to operate.  Hopefully AMD works on their idle power consumption in the future.  I love my AMD AM5 desktop but when building a home server I couldn’t justify an extra 15-20W power consumption, especially when energy prices are going up, and IMHO will keep climbing for a while. I don’t have the sustained workloads to keep the cpu at 80% to take advantage of their performance per watt.",Neutral
AMD,"This is not true. You have the b580 to thank for the pricing of the 5070 being somewhat normal at around 500$ even though it sells for way more than msrp.  If you make good products at decent prices, people will buy, when competetion sees this, they will respond so as to not lose out on market share",Neutral
AMD,"Gains are lng term. If intel reply tomorrow, it would still take some time to thwart amd and they may reach 50% by them.  Latency between product releases and their reception and market share is years at least often",Neutral
AMD,Because you're not a OEM.,Negative
AMD,"Twin Lake is a successor to Alder Lake-N and it's out, and Wildcat Lake is in development. And these are some of the most popular consumer CPUs. Check out this chart, both N100 and N150 is there!  [https://www.cpubenchmark.net/share30.html](https://www.cpubenchmark.net/share30.html)",Positive
AMD,"Tell me you haven't used N100 without telling me you haven't used N100.  N100 is 4x Alder Lake E-cores, but they're not trimmed down. One of these E-cores has roughly the same performance as an E-core in my 13900K (I benchmarked this). I'm not kidding.  Sure these big CPUs have more TDP, but that wattage means nothing for the tiny E-cores that can't make much use of it, it all goes into P-cores.  N305 (basically two N100s glued together) has rougly the performance of Steam Deck CPU, and N100 can match Steam Deck at single core too.",Neutral
AMD,"Like /u/Sosowski says, it seems you don’t know this CPUs  The N100 family is incredible, and AMD has nothing to compete against it both in price + performance. Not only because the 4 e-Cores, but for some usages Intel QSV is perfect (transcoding movies even at better quality than an AMD APU that can cost x3?)  This chips can run Windows 11 just fine for a everyday usage (browser, some apps, Office…) but they excel as NAS, homelab servers… where they shine given their TDP, performance and QSV  I understand why AMD isn’t interested in competing in this “low TDP low profile” category, and it’s because the margin is very low and they can’t justify fill TSMC or GlobalFoundries nodes with this kind of chips orders.  Even Intel can’t, and that’s why they only make them with their own capacity (N100s aren’t produced externally) and that means they are now in a slow path of upgrade until Intel internal nodes update (meantime, Intel is ordering TSMC some of their higher end processors parts)  I would love AMD being able to compete and make something like a low TDP x4-x6-x8 APU for like 50-70$ the CPU (more than that would be out of market compared to N100s)",Neutral
AMD,"2025, time to ditch windows...    embrace arch and hyperland experience how life should be buttery smooth and pretty.      rice away with the waifu gods!",Positive
AMD,"Market share is very important, just look at Radeon.",Positive
AMD,> The one reason behind it: Intel Foundry. >  >  >  > People in this sub seriously underestimate just how big of an advantage Intel manufacturing its own chips is. The sheer volume intel can manufacture dwarfs any competition that relies on TSMC. It doesn't matter how good your thing is if you can't meet market demand.  IFS only matters if they can stay somewhat close to the market leader. Having a fab 4 years behind the times doesn't help them all that much because they'll still have the costs of running their fabs and the costs of going external for products that actually matter and no cash to keep going forward with new fabs.,Neutral
AMD,"> The sheer volume intel can manufacture dwarfs any competition that relies on TSMC.  TSMC produces significantly higher volume than Intel, and Intel's own latest products are mostly TSMC.   > It doesn't matter how good your thing is if you can't meet market demand.  You can still sell what you have, and that's better than Intel's situation, where largely because of the foundry, their chips are bad so they can't really make a profit from selling them.   > And people here were claiming Intel should stop its foundry.  How many chip sales do you think a multi-billion dollar loss is worth?",Neutral
AMD,"People under estimate the impact of capacity and how this really works. AMD gets a better product and rather than being sat on inventory they sell it all, maybe for a bit more margin, but now they have no more CPUs to sell so Intel fills that gap. This happens in every market because companies can not instantly ramp up production to meet their new demand and Intel is now a missized company with a lot of production capacity it can't fully utilise and the moment it makes a competitive product it can instantly go back to 80%+ share of the market.",Neutral
AMD,"> And people here were claiming Intel should stop its foundry.  My brother in christ, they reported a net loss of $19 *billion* dollars last year and practically had to be bailed out by the feds.",Negative
AMD,"I dont think TSMC is at fault here, TSMC can really make a lot of chips.  It is more likely AMD didnt bother to book larger capacity after so many years. When in comes to volume, even Nvidia dwarfs AMD's. CPU has higher margin per die area than GPU, yet AMD did not outbid nvidia to book more capacity.",Neutral
AMD,It's so simple,Positive
AMD,"on the very high end Intel would get ahead, but for an average consumer AMD was better deal since Zen 2 (zen 1 had issues).",Positive
AMD,"The professional market is smaller than gaming, fyi.",Neutral
AMD,Indeed. Here it was as low as 270€ but has now increased in price by 50€,Neutral
AMD,Like what cpu?  I only remember fx-57 in the 1k range   And thats irrelevant because back then midrange cpus can be overclocked to top level,Negative
AMD,Because tech tubers will convince people if they dont have an x3d their pc is trash   We have a generation of pc builders who call themselves enthusiasts but all they are doing is buy the most expensive parts,Negative
AMD,Exactly! I used the difference and got a 5080 it's a fantastic experience for both gaming and workloads,Positive
AMD,"yes, neither amd or intel  get a via cpu",Neutral
AMD,I meant overall including server and laptop,Neutral
AMD,"ARM is a monopoly, which is worse than a duopoly.",Negative
AMD,was a bit of satire...     gotta be honest when i went from a 4770k @ 5ghz to my 7800x3d it didn't really change my life much.,Negative
AMD,"Ugh, I used to work for a company like that. It was a total Intel + Microsoft + ""open-source tools are the devil"" shop.",Negative
AMD,"Logical explanation, but I work in IT and no one - colleague, boss, client, supplier - no one ever gave me that reason behind the products they bought.",Neutral
AMD,"The percentage of corporate buyers choosing Intel machines over AMD specifically because of idle power consumption is practically 0%.  AMD doesn't really target the low end of the market that these office PCs live in. A practical consequence is that the stock (and the selection of SKUs) of AMD business PCs from major OEMs like Dell, HP, Lenovo, etc. is going to be limited. A corporate buyer ordering desktop PCs by the thousands is going to get Intel machines if for no other reason than that's all they _can_ get in a reasonable time frame.  Also, for an office PC, Intel CPUs are fine. They're powerful and efficient enough, the integrated graphics work well, and they aren't run at stupidly high clock speeds like the enthusiast chips.",Neutral
AMD,"Who's actually doing a 6 day workweek? I don't mean random startups or executives saying they think it's better for productivity, I mean what percentage of people you actually know regularly works 6 days a week",Neutral
AMD,"I don't agree, it could be years before OEM's are fully off their ""intel only"" mindsets.   As things stand, If intel released a banger now AMD is not getting any new OEM designs basically overnight.",Negative
AMD,"> Twin Lake is a successor to Alder Lake-N and it's out  Twin Lake is literally a rebrand of ADL-N. It's identical silicon.   > and Wildcat Lake is in development  I have high hopes for WCL, but that's really a different market. WCL targets the lower end of what's currently the -U series. Will be great if you're looking for a solid budget laptop in a year or so, but it's too expensive to fill the embedded and even *more* cost-sensitive client roles that ADL-N fills today.   Also, doesn't help that the WCL successor is also dead, so...",Neutral
AMD,It's sad that Intel low power low tier chips are heavily cut down on the iGPU side.,Negative
AMD,"I moved on from the N100 years ago. Promised myself ""never again"".   Yet I broke my promise recently, got a Mini PC/NAS with an N150. Thought it might be a better experience than the N100 which it replaces. If I had done my homework I would have known that it's basically the same CPU, performance wise, just Intel's rehash of an old and aging CPU, with a new name:  https://www.cpubenchmark.net/compare/5157vs6304/Intel-N100-vs-Intel-N150",Neutral
AMD,"I just spent a week configuring a Mini PC/NAS with an N150, and I know what a pain it is. The N150 is supposed to be the new and improved N100, in reality it's less than 1,5% faster. It's just Intel rehashing the same old, with a very slight clock speed bump. Not enough to be noticeable to the user, but enough to be able to call it a new generation of low power CPU's.  I kid you not, EVERYTHING you do makes all 4 cores go 100% full tilt. Even opening a new tab in a browser is a struggle for this CPU. I literally spent 2 almost full days just doing the normal Windows updates you have to go through when installing Windows these days. The CPU had all 4 cores at 100% at all times, and I couldn't even watch a Youtube video at the same time, it was too bogged down downloading and installing updates.  It's still ok for a NAS that I just remote into from time to time, but I was hoping to use it as a mini PC for my spare bedroom as well. But it's just too painful to use for anything but the most basic stuff. Any kind of multitasking and you're gonna hate the experience.  You're much better off paying $50 extra and getting something with a 5560u instead, which also has a proper iGPU as an added bonus:  https://www.cpubenchmark.net/compare/5157vs6304vs4883/Intel-N100-vs-Intel-N150-vs-AMD-Ryzen-5-5560U",Negative
AMD,">TSMC produces significantly higher volume than Intel, and Intel's own latest products are mostly TSMC.   Only the part that AMD has bought is really relevant when it comes to TSMC production here. is AMD part higher volume than Intel? I dont think so.",Neutral
AMD,Intel doesn't have *that* much capacity on the nodes that could conceivably compete with AMD.,Neutral
AMD,Yup. If Intel got their shit together they can simply destroy whatever market share AMD have gained over a decade of them not being competitive in a single year. I don't think Intel can get their shit together with how they are managed but they absolutely can destroy AMD if they do. And the worst part? They won't even have to take that big hit in profit while doing so since they will be doing everything inhouse.,Negative
AMD,"Ah yes. The CEO mentality.  \- ""Why are we spending so fucking much on X when we are not getting any money from this? Cut it out!""  \- As it turned out that X is the reason product Y can be made. After X is gone your product Y is gone.  Good news 19 billion dollar loss is gone! Bad news the entire company revenue is now zero cause that 19 billion dollar loss was creating 100 billion profit in other products. Congrats! get your 100 million dollar package now.",Negative
AMD,That doesn't refute his market share thesis,Neutral
AMD,Thats what happens when you are actively investing in new nodes and have to catch up with 10 years of stagnation.,Neutral
AMD,They really didnt. That “loss” was due to internal accounting,Negative
AMD,"True but market forces play a part in how much AMD can take risks in booking capacity. AFAIK server market for CPUs is much more constrained than for GPUs where you can currently pretty much produce as much as possible and still not match demand. So Nvidia can offer more for the capacity and AMD's risk of overbooking is higher. Of course AMD could still have been too conservative, and in hindsight it looks likely they were, but it's not like they didn't have their reasons for it.",Neutral
AMD,I never said it is TSMC fault... It is just how the market is. TCSM is doing what it is paid to do.,Neutral
AMD,"The professional market isn't just video editors, fyi.",Neutral
AMD,Check the volume of laptops being sold. It dwarfs gaming market by orders of magnitude.  Intel basically have stranglehold on that entire market and it is practically impossible for AMD to compete in that market cause of TSMC.,Negative
AMD,"Its just guesstimates but 2024 intel & amd client rev 30.3+ 7.1 = 37.4B  \~70% are mobile 30% desktop (JPR shipments)   11.22B desktop   Motherboard shipments by oems \~50M   2-2.5B diy annual rev   From diy market I cant guesstimate how much are purly gaming but I will say 70-80% gaming   My best guess is diy is 1.7-1.9B rev   JPR, steam & Gartner data",Neutral
AMD,it depends on use case. My 7800x3D is the limiting factor in almost every use case and i cant wait until i can find a meaningful updates for it (9800x3D not worth it). Im spending more on CPU than GPU because thats my bottleneck.,Negative
AMD,"Becuese the X3D is actually trash at it's price point. Unless you are playing a niche game, the cheaper CPU that is still great in 99% of cases is a far better buy and then invest that money into a better GPU.   You can buy a CPU+Motherboard bundle over at microcenter with the 9700X for $340. The 9800X3D bundle is $530. That's $190 more. Now outside of circlejerking benchmarks what do you actually get for your $190? Nothing compared to what a $190 better GPU you can buy.  Same thing with Intel. The I9 Arrow lake CPU is a terrible buy compared to the 265K. In every case the 265K Paired with a better GPU is going to win for gaming.  Now if you don't give a fuck about price and are just going to buy a RTX 5090 anyways then this is a moot point, but most people I would imagine are not doing that.  Then you have the people who believe they need that top tier CPU and get something like a 5060TI to go with it. Becuese they blew too much of thier budget on the CPU. Which means they just wasted thier money becuese chances are they are going to be GPU limited to the point where their killer CPU is pretty pointless.",Negative
AMD,"Usually we compare desktop CPU's as consumers. That's why I thought you were comparing apples to oranges.  They're progressing in all markets though, just in the laptop market it's not much. So if you wan your initial statement to be true, you can just look at the laptop market.",Neutral
AMD,"At least they actually license out the ISA, which is more than you can say for Intel",Neutral
AMD,"Some companies (finance and tech mainly) are moving to 6 day work weeks especially for support roles. China kicked it off with 996, but it’s starting to get some traction here too, especially with layoffs and cost cutting here.",Neutral
AMD,"Nah, Intel's lost much of their codesign team that helped them get those designs to begin with, while AMD's ramped up their own.",Neutral
AMD,"Oh, I thought WCL is gonna be the next N series! And yeah you're right about Twin Lake.  But hey, I'm a big fan of these CPUs, even tho they compete with a Raspberry Pi more than with AMD. Hope there's gonna be more!",Positive
AMD,They're great HTPC and NAS chips but that's about it.  Love mine for transcoding Emby streams.,Positive
AMD,"True! I wish there's a handheld gaming N-series CPU that is an N100, but with slightly better iGPU. There's N200 but it's still far from usable on the GPU side.",Neutral
AMD,"1) The N150 is part of the same family, just a little bit “overclocked”. It isn’t a literally a new gen (same socket FCBGA1264, both based on Alder Lake cores, even if they have different naming (Twin Lake) because the dates of launch) but a “refresh”. It doesn’t have anything “new”.  And that’s logical. As I explained, this CPUs have a very thin economic margin, so they must use internal capacity of their nodes. And they still haven’t upgraded them since the N100 introduction. Also, the making of new e-Cores (again, given they won’t invest a lot on them given the margins) isn’t coming before 2026-2027 as earliest.  2) The N100 family runs correctly. Heck, I’m using in a desktop a N6005 (with Fedora I must say) no problem, playing 2K YouTube videos, sometimes +30 tabs, and mild multitasking (browser + another browser + Libreoffice)  I don’t know how your experience is even possible, 4 cores at 100% opening browser? Slow? It isn’t certainly the usual experience even if using Win11  I would say something else is at play, like low RAM availability (4-8GB?), slow SSD, bloated Windows or something else.  You can see benchmarks, the N100 is about the same performance as an Intel i5 8250U, and I know people even using that at their workplace no problem. Something is up with your PC, not the CPU.  In any case, I recommend you running Linux with this low resources things and avoid Windows 11 bloat.  3) That AMD you link has worse iGPU depending what you do (Intel QSV for encoding/decoding is miles ahead of AMD), and anyway, its GPU isn’t really a factor given at this performances you won’t be gaming precisely.  Also, it will run hotter and have a TDP about x2-x3 of the Intel. So I think now we’re not talking about apples to apples, but apples to oranges.  I have a Intel Nxxx that even runs passively cooled without problem, no fans, no noise, and it does its job perfectly, for about 150$. But if you are OK paying around 25-40% extra (+75-100$), more TDP, fans and so on, then yeah, you can go with a AMD Ryzen 5 APU  And if you’re OK with adding a bit more, you can even go with an Apple Mini using a M4",Neutral
AMD,"A legitimate point, but it does mean that there's substantial flexibility. AMD's content to slowly increase their market share while reaping pretty fat margins overall, but if they had an incentive to really push for more volume, they could get it.   Worth noting that Intel's TSMC wafer allocation is almost as large as AMD's, last I heard. At one point, even expected to be maybe the 3rd largest, iirc. So TSMC was able to absorb a pretty substantial amount of volume when Intel came on as a customer. Just demonstrates that the flexibility does exist.",Positive
AMD,> Good news 19 billion dollar loss is gone! Bad news the entire company revenue is now zero cause that 19 billion dollar loss was creating 100 billion profit in other products.  Their fabs have 100 billion in profit? Intel barely has half that in revenue and a fair amount of that is from external.,Negative
AMD,"> As it turned out that X is the reason product Y can be made  But it's not. It's almost the exact opposite. The fabs have consistently *stopped* Intel from being able to ship products.   So yes, why wouldn't you get rid of the part of the business burning billions by itself, no clear path to profitability, and who's dragging down the rest of the company with it?   Intel would be a lot better shape today if they'd gotten rid of the fabs instead of wasting a lot of money to double down on them. That's what AMD did, and it payed off.",Negative
AMD,"> the entire company revenue is now zero cause that 19 billion dollar loss was creating 100 billion profit in other products.  What other products? Intel uses TSMC for their high end chips, you know.",Negative
AMD,"Are you lumping in every office PC to the ""professional market""? Otherwise, you'd be surprised at gaming vs all professional markets put together.",Neutral
AMD,Intel's laptop chips are very competitive though,Neutral
AMD,"Well, i suppose it is technically a niche game, but games like WoW with their 10 million daily players benefit massively from x3D CPUs.",Positive
AMD,"I was surprised they hadn't hit 30% in server either, they've been kicking Intel's ass there for a long time",Negative
AMD,You'd need both AMD and Intel to agree to that. Intel can't do that on their own.,Neutral
AMD,"They do license the ISA out, but they do not let others license their microarchitectures out. Licensees can make chips and that's it.  So if someone needs a design, they can only make it themselves or license it from ARM. As a result, few ARM customers bother.  Qualcomm found a workaround (just buy a company that made a good ARM microarchitecture), and ARM really didn't like it. That is what led to the lawsuit, which ARM, quite predictably, lost.",Negative
AMD,"No offense, but your comment is basically a very long winded way of saying ""yeah it sucks"".  Linux is more efficient. But I use Windows, and I should have clarified this, but my experience with the N150 is after de-bloating and optimizing.  But sure, the AMD chip will use more power when it needs it. But that's the whole point, it HAS the power when it needs it. When it doesn't, it's pretty frugal with its power usage as well.  I'm not a fan of ""just barely enough power"" when it comes to PC components. I despise e-waste, so yeah, you might find a purpose where the N100/N150 is just barely good enough, but when your needs change, or when things become more demanding, it's for the trash. That's why I don't get ""barely fast enough"" components any more. You're getting frustrated in the short term, and creating unnecessary e-waste in the long term.  Why are you making the claim that you have to pay $75-100 more to get an actually decent AMD CPU instead of this N100 e-waste CPU? That's not how it is irl.",Negative
AMD,"TSMC has expanded capacity significantly last year and we will probably see they expanded it significantly this year too (the years not over :) ). They do provide significant flexibility, but we are still talking about years in lead time for capacity bidding unless you are willing to pay a lot of money for capacity.",Positive
AMD,"> Are you lumping in every office PC to the ""professional market""?  why wouldnt he? People using those are professionals. I do data analysis for a living and the local machine they gave me is intel.",Neutral
AMD,They are now very recently but few years ago Intel was by far the worst for laptop due to them being far less efficient than AMD but you couldn't find a single proper laptop lineup from AMD. Even now if you have a specific combination of specs in mind which is even slightly different than mainstream you are stuck with Intel. AMD laptops are very limited lineup.,Negative
AMD,"Then again, something is up with your setup, because your experience is not the usual with the N100 chips. There are even laptops being sell with them nowadays at some markets and run Windows 11 perfectly.  IDK what PC you got or its constraints, but what you said it isn’t normal, more so if after debloating.  And about power, it depends. I’m a big fan of having what I need, and not try to kill flies with a nuke. As I said, I run a N6005 as a daily driver just fine and plan to do it for some more years, even if I also have an AMD 8600G desk, because less noise, electricity needs, heat… to do the same basic tasks. Obviously if I needed to run a heavy software I would use the 8600G, but that’s not a daily need in the slightest. In fact, I have even thought about selling the AMD PC (more so because I play less and less) but given I would need to sell it at a discount, and all the job of getting a buyer and so on, well… I’m lazy I suppose.  And about he money, IDK where you live, in parts of Europe you can find a N100 mini PC for about 130-160€ even with RAM and SSD included, new. I got my N6005 for 80€  An AMD 5560U mini PC (Beelink?) starts at 300€, about double.",Neutral
AMD,"Mostly because if you lump all those in, the category ceases to mean anything useful. At least from AMD and Intel's perspective, that's basically the same as the mainstream consumer market. And if that *is* what you want to talk about... why not just call it that directly? The only reason to specifically call out ""professionals"" would be to distinguish it from the rest of client.",Negative
AMD,"> Then again, something is up with your setup, because your experience is not the usual with the N100 chips.  Nope. This is not my first time with this class of CPU. I've had plenty of N class CPU's before, back when I thought I could make it work.  The thing is, having ""barely enough"" only works until you need more. And in that case, in any case, you're better off with a CPU that CAN deliver more, rather than one that can't.  I usually shop Mini PC's on places like Ali-express, the price difference is more like $50, all else being equal. And for that kind of money you get something that won't magically turn into e-waste next year, or whenever your needs change. And is nice to use until then.",Neutral
AMD,"Maybe buying from Ali is not the best.  As I said, no matter what, your experience is not normal. Millions of this devices are being sold no problem, and just as you have your experience, I also have mine equally valid.  I didn’t saw it until now, but look even the upvotes/downvotes in your comment and mine (I didn’t downvoted you), your experience of “N100 is trash and goes 100% all cores just opening Chrome” is not normal.  IDK what RAM you were using, its size (I know the N100 with 4GB RAM is hell, but 16GB RAM is smooth), what SSD (some QLC literally crawl after some mild usage) and so on. But something is up with your setup for sure, because this CPUs aren’t that bad in the slightest.",Negative
AMD,Pair it with a 6650XT.,Neutral
AMD,9060 XT?,Neutral
AMD,What resolution do you run,Neutral
AMD,What games do you play?,Neutral
AMD,"I believe the 5600G limits you to PCIe3.0 x8 for your GPU. This will negatively affect some of the more powerful GPUs. If you get a discrete card, you should consider swapping out your 5600G with a 5600X if you can find a used one cheaply.",Negative
AMD,any bigger and the system bottlenecks ?,Neutral
AMD,i just saw this card and it has bad reviews  about not worth the money for the performace ! thats why i'm hesitant,Negative
AMD,is this the best combo or can i go bigger ?,Positive
AMD,why not the 7700 xt ?,Neutral
AMD,i'm only asking cause i'm curious dont like think i'm playing smart,Neutral
AMD,because my monitor doesn't go higher i'm used to it i just want performance,Neutral
AMD,look only play 1080,Neutral
AMD,look because i dont have a graphics card i dont play many games so thats why i'm asking whats the beefyest combo because it allows me to have choices,Neutral
AMD,i used to play pubg cs  league  rinbow six,Neutral
AMD,"I mean you can throw in a 5090 if you want, depends what you do with it",Neutral
AMD,"You can always go bigger, especially if you play on high resolution or high settings.  There's just diminishing returns till you upgrade the CPU as well.",Positive
AMD,They’re more expensive and have less vram,Negative
AMD,"Get a 2K monitor with above 120Hz and then with the rest buy the 6750xt or 7700xt or a second hand 3080 or something about that range. I have similar specs but intel and have a second hand 6900xt.  If you want to stay at 1080, then get just the card. But limit the fps to like 160-200.",Neutral
AMD,For 1080p a 9060XT should be sufficient and anything better than that will be bottlenecked by your 5600G,Neutral
AMD,9060xt will be amazing for you,Positive
AMD,"What do you want to play, I saw someone else say it, but you could go for a 9060xt or even push to a 9070/5070, cause in some cases you will be CPU bottleneck and in some other not",Neutral
AMD,"Oops, I saw this comment second, then I'd say go for a 9060xt cause these games a relatively more CPU bound than GPU, you can spend excess money on a CPU",Neutral
AMD,"thanks for the advice ! and from what i saw the 9060XT  is better value for money for me and they are very close with 7700 XT performance wise , also the thing that bothered me is i was about to purchase the 7700 xt last year and its price went up at least 70 euros",Positive
AMD,Get the 16GB variant!,Neutral
AMD,"Budget is kind of important to know.   Anyway, 5090!",Positive
AMD,"In terms of budget - I was hoping to keep this “gift” under $2,500 in total. But I also don’t want to make upgrades that won’t carry me through the whole build. I hope that makes sense.   I’m seeing a lot in my searches that I need to make sure the motherboards PCIe slot works for the graphics card I’m looking to buy. That is what I don’t know how to identify when buying online.   I see various model numbers mentioned - how do I know if they will fit in the slots? Is there a term or something I can be taught to look for so I know what I’m compatible with?  I didn’t add a budget at first because I see some very expensive graphic cards, but if a great card is going be expensive, but will also require additional ram… then I was looking to learn that chain of progression so I can plan accordingly.   When I look up the card he currently has, it shows it is for 1080p resolution- and I want to bump that up to 4k… or just better.",Neutral
AMD,PowerColor Reaper AMD Radeon RX 9060 XT 16GB GDDR6 is $350 on Amazon currently and would be an upgrade.  What do you have for a monitor?,Neutral
AMD,"easiest is better gpu, which one is mostly budget and other guy left few out    single mem stick runs half speed vs set of two, use cpu-z to see what's in there and get same speed/timings",Neutral
AMD,"Depends on the budget. 9060xt, 9070xt, 5070ti, 5090 I think is the general hierarchy atp. The case looks big enough to fit any of them.  I’d keep an eye out for a deal on a 32gb+ of ram - maybe another 16gb stick if you can find a matching one.",Neutral
AMD,This makes no sense dude.  You bought an $800 rig and now you want to put $2500 of upgrades into it?  WTF are you talking about?,Negative
AMD,"Better off selling the 16gb kit and buying a complete 32gb kit. It’s recommended not to use 2 ram kits together. Even identical part number kits when paired have a decent chance of ram failures, as there’s no guarantee you’ll get a set made with the same dram modules from the same manufacturer, as brands source them from a few different manufacturers.   TLDR; Ram is sold as a pair, because it’s been vetted and passed qc to run at its advertised speeds together, can’t have the same guarantee for ram from another pack, even if it’s identical part numbers.",Negative
AMD,"A little confused myself?   Honestly tho, since what’s done is done.   I believe  for the current pc it is an am5 board, which means you should be able to throw a 7800x3d in it, as well as any gpu. The only distinction between motherboards and gpu is the pcie speed, and am5 has at least pcie4 which is basically identical to the higher spec pcie5 when it really comes down to it for gaming, so you’re good on getting any gpu you want, it will fit no problem (just make sure the length of the card can fit within the length of the pc case.)  The most substantial upgrade you would be able to do for even less than your quoted budget, is to upgrade the cpu to a 7800x3d, upgrade the gpu to a 5070ti, and grab any 32gb 6000mhz ram kit you can find for cheap.   These components will be a super simple swap, that will almost triple your son’s current performance, which to say you will definitely get decent 4k performance, and will last for at least a few years to come. You can watch YouTube videos on how to swap each component aswell for some tips on how to make it go smoothly.  Oh you’ll also want to grab a new cpu cooler, the peerless assassin on amazon is a great value cooler for the 7800x3d",Neutral
AMD,The memory controller will just derate the whole package to the slower component's speeds.,Negative
AMD,With how expensive ram is getting i only brought up buying another stick because it might be the only option. But yeah OP should keep what you wrote in mind if they go the second stick route.,Neutral
AMD,"if the price difference is less than $60, yeah, if not no.",Neutral
AMD,"The 7800x3d is the economic purchase. The 9800x3d is the emotional purchase.   Neither is really wrong, it just depends what you value.    But I doubt you'll regret buying the 9800x3d in 2028.",Neutral
AMD,No,Neutral
AMD,"If the difference between the 7800x3d and the 9800x3d is also the difference between getting a 5070ti over a 5070 then go with the 7800x3d + 5070ti. If you can afford it, just get the best of both worlds and go with the 9800x3d and the 5070ti. If you can't afford either, the 7800x3d + 5070 will still be a great machine.   But at 1440p the 7800x3d will be plenty and you will get more out of the better 5070ti GPU.",Neutral
AMD,"No  Also depends on what games you play, resolution, GPU, what settings you use, but for $100 most of the time no.",Neutral
AMD,"what games what resolution what refresh rate? also kinda depends when you might next upgrade, can go 7800x3d now for 10% slower on average then plan to upgrade to 11800x3d 2-3 years later, if you get 9800x3d then that next upgrade makes less sense for example. if you game at 4k then no reason to get 9800x3d if you're on a budget. Typically i say if downgrading the cpu lets you afford the next tier of gpu then go for it, otherwise you should just get the better cpu, but 7800x3d to 9800x3d is not a huge jump unless you do competitive games and high refresh",Neutral
AMD,"heavily depends on games played, what the pc is for, total budget, and the other specs too. but if i was given none of that information, i would say stick with the 7800x3d",Neutral
AMD,Not worth 100 dollars,Negative
AMD,If you are rich go for it,Positive
AMD,You'd probably be fine with a 7700x lol,Positive
AMD,"I chose the 9800x3d just cause I dont normally upgrade components that often (generally every 5-7 years). If you're the kinda guy to upgrade every 2-3 years anyway, the 7800x3d is probably a better idea.",Neutral
AMD,Not worth it,Negative
AMD,God no,Negative
AMD,"Maybe for 1080p, I can somewhat see it being slightly worth but for 1440p and esp 4k, F no.",Neutral
AMD,no and if youre near a microcenter you can save close to $200 next time its on sale ($280),Neutral
AMD,"The 9800X3D is much easier to overclock, but the stock differences are not worth $100 in most games.  If you *want* to overclock, though, I think things like the $550 Microcenter bundle are worth it, probably.",Neutral
AMD,For blender / video editing go intel ultra core 7 265k for 300 bucks for it’s great igpu with hardware decode for multiple video formats and has much higher multicore bench than ryzen 7,Positive
AMD,"Yes I would get the 9800, but that’s just me. I have a 7800 already, but 9800 is a bit better. It depends on how much $100 is worth to you / what else you’d spend it on instead.   You could get a 7800 and spend the extra $100 on AMD stock.",Positive
AMD,Don't sleep on the 9700x.  I got mine on a sale for cheap and it's been a beast.,Positive
AMD,Get a 9950x for $250 👍🏻,Positive
AMD,"My i7 8700k lasted me from ~2019 until two days ago. I did upgrade from 1080ti to a 3080 during that period. Nothing was wrong with it, just came into a nice chunk of cash and felt like upgrading.   I just built a 9800x3d 5080 pc and was sort of expecting more out of it.   Same settings on arc raiders got me roughly 70 more fps (new setup averages around 160-170 in 1440p). Was hoping to get my fps up to my new monitors 240hz refresh rate.  My old cpu had a great undervolt/overclock and I delidded it, so maybe some tinkering would get me closer to at least 200 fps. I just find getting a stable overclock to be such a headache.",Positive
AMD,"mainly ultrakill, terraria, cyberpunk, going to be paired with a 5070 or 5070 ti, at 1440p and an uncomfirmed refresh rate",Neutral
AMD,1440p with a undecided refresh rate,Neutral
AMD,What about  DotA 2 on a high lvl?,Neutral
AMD,"Try turning DLSS on if you don’t currently, should get appreciably higher frame rates with that setup",Positive
AMD,My I7-6800k is turning 10 next year.,Neutral
AMD,"We are quite similar!   A couple months ago I got a 7800x3D with a 5080, and a 240Hz 4K OLED screen.   I hope it lasts me a good few years.   I've never delidded a CPU myself - I don't think I would have the confidence to do that lol!   Hope you enjoy you new rig my dude! I'm extremely happy with what I've got.",Positive
AMD,"165 fps is already plenty, this is a case where turning on DLSS Frame ~~Generation~~ smoothing has no downside. With this, your monitor will give you the full 240fps it’s capable of.",Positive
AMD,"Sounds like it's not worth the extra $100, 7800x3d is a lot of power already. Use that towards a better GPU, which will be more important at 1440p",Negative
AMD,If you're considering 5070 just get a 9070xt.  As for your question are you making a new build or upgrading a current system?  At that resolution get the 7800x3d,Neutral
AMD,Depending on your budget.....  I would say go with the 7800X3D and the 5070ti  The 5070ti is much better.  Just depends on the price difference.,Positive
AMD,"9800x3d is only about 8% faster on average but a decent number of games are 15% faster, see here from only 2 months ago https://www.youtube.com/watch?v=X0oimg79NZ8. If downgrading to the 7800x3d lets you afford the 5070ti over the 5070 then i would 100% do that. I would even go as far as downgrading to a 7700x to afford the 5070ti since you didn't list any competitive games",Positive
AMD,"as someone who recently built a PC with a 9800x3d and a 9070XT with a 240hz refresh OLED, this setup can run DotA at max settings 1440p at 240 frames. I have 4.4k hours in DotA and really enjoy the feeling of both eye candy and max frames.",Positive
AMD,pretty sure it might be massive overkill for DOTA 2,Negative
AMD,You can play Dota on a potato.,Neutral
AMD,"Yeah it does get a considerable fps boost, but im kinda spoiled by the clarity of DLLA. Maybe I should've just opted for a 4k monitor and went the dldsr route.",Negative
AMD,"I was sweating bullets the whole time lol, idk what possessed me to do the delid. The results were definitely impressive if I remember right it averaged 15-20 degrees cooler. Never again though.   Thanks man! If it lasts me another 6 years ill be a happy camper for sure haha.",Positive
AMD,Its my understanding those sorts of features adds input lag. Im sure its not even something percievable to my casual ass but I was just hoping the 2nd best card on the market currently could get the max potential out of a 240 hz 1440p panel with zero sacrifices.   I'll fool around with it though. Ive mostly just been debloating windows and running a few benchmarks. Appreciate the suggestion.,Negative
AMD,"i see, thanks man",Positive
AMD,"i didnt really list any of the competitive games because i dont really enjoy them as much, but i think i'll take your advice on choosing the 7800x3d to have some more money for the 5070 ti",Neutral
AMD,"It won't help lag, for sure, but there might be a little. However, you are already at 165 fps, any lag would be adding onto existing minimal lag, so I'm not sure you'd notice. It will still feel like it does now, you'll just get a smoother experience on screen. Certainly worth trying to see if you like it or not!  For debloating, there are ofc various ways, which I shouldn't discuss as a comment here.",Neutral
AMD,"Yeah i just tested it in arc raiders. Keeping dlaa on and just turning frame generation on got me to a consisten 200+ fps. Only costs 1-2ms extra for the frame generation which is definitely unnoticeable. I'll keep it like this, appreciate it.   As for debloat the Chris Titus tool that runs off powershell seems to be all I need lol.",Positive
AMD,"Yep, Chris Titus' WinUtil is great, I use that. Also, new update, if you hadn't already noticed. I also like using an autounattend.xml generated by schneegans.de, at the time of Windows install as well, you can pre-debloat or otherwise set a bunch of things up as part of it!",Positive
AMD,"The HP Victus series isn't designed with upgradability in mind like the HP Omen series. The Victus uses a proprietary motherboard and PSU connector.   You're basically stuck with whatever GPU the PSU can power, so you're out of luck for Child B unless you can find a higher wattage, compatible HP power supply. And if you do find one, they're rather expensive for what they are.",Negative
AMD,What monitor resolution and refresh rate?,Neutral
AMD,What games determine what is a good fit.,Neutral
AMD,"Thanks.  I know i can get a power supply upgrade for either of the builds because when i had these built by HP, there were 3 options - 350, 500, and 750. i bet you are right and ill have to overpay for whatever i get..  With that in mind, do you have any guidance on what i should get for the GPU?  I'll at least find Kid-B a 500w PS, or maybe get both a 750 if i can find a deal.  Is there motherboard compatibility concerns?",Neutral
AMD,Both of them have 1080p 60hz moniters,Neutral
AMD,"The games they play now and are not very smooth are Forza 5 and Remnant 2.  I know they both complain about Remnant 2 not running great. Not sure what games they are asking other family members for on their christmas list, but i assume similar to those or maybe a little more intense. They are 14 and like 3rd person shooter types of games but i dont let them play things like COD.",Negative
AMD,"No motherboard compatibility concerns. Your concern should be (1) physical space constraints in the case. And (2) making sure whatever GPU you buy can fit with the HP PSU power adapter.    You might have to go for a low profile card - the Gigabyte low profile 5060 is probably the strongest currently put there. There is also a low profile 4060 version but they're no longer in production. Prices for both are higher than the full size cards.  Lastly, there is also a 3050 6GB low profile, but it's extremly anemic for the price so I would stay away from it, it's barely better than the RX 6400",Neutral
AMD,of course it's a damn steal! only that cpu and ram alone are more that 50% true value of that pc. Buy that thight now....or maybe not .\_. ?,Negative
AMD,"Yes especially with ram prices how they are. You can play most games on it how it is, but when you are ready, a GPU can be installed!",Positive
AMD,"Is it used? Seems fine, nothing too expensive to replace if it's busted",Neutral
AMD,"With the current horrific price hikes, yes.",Negative
AMD,If you want a real gaming pc I recommend against it. If just save a little more and build your own. There simple to build.,Neutral
AMD,no it's not a steal.  You could go for a 7600/dgpu/16gb ram for not much more and have a much faster pc.  The motherboard is not good and the power supply brand isn't great either.  If this is used I wouldnt pay more than $350-400.  It does not have a dGPU.,Negative
AMD,"Yup I already did, i can always resell later :)",Positive
AMD,"Yup used, it has the stock ryzen CPU cooler but will see how temps are",Neutral
AMD,I'm not opposed to building but these ram prices are crazy. Even this at 5200 mhz is not bad since will be on linux,Negative
AMD,"Lol. Lmao, even.  Even if OP is dead set on building their own, it's _still_ worthwhile to buy and scrap this one for parts, by a dramatic margin.",Neutral
AMD,"lol gtfo do state in clear terms what is ""not good"" and ""isn't great"" about the motherboard and PSU? The Radeon 780M is indeed a ""GPU"", complete with dedicated graphics pipeline e.g. shaders, TMU, ROP and GPP compute units like any other modern GPU. It just integrated in the processor package and doesn't have it's own dedicated VRAM or frame buffer, shares the system memory.",Negative
AMD,"The 8000 Ryzen CPUs run pretty cool compared to 7000/9000 afaik, but I could be wrong. Another thing to add is that the 780m igpu is really damn good compared to other desktop CPU integrated graphics. It can get ~40 fps in cyberpunk 2077 on 1080p low, which is on par with a gtx 1650",Positive
AMD,Yea i havent built one in awhile. Ram is like double from last year. If you bought this pc I would sell some parts of it and upgrade whats needed. That ram alone is like 260 at its cheapest so I get the appeal. I really forgot how expensive ram was when I answered.,Neutral
AMD,Interesting! Thanks for sharing the solution you found. Is there a way to find if a 5950X is one of these older revisions?,Positive
AMD,"You are a lifesaver. I've been having issues for 2 months, where my PC just randomly freezes at least once a day, requiring a restart. I tested every piece of hardware, updated all my drivers, updated the BIOS. Nothing helped.  Yesterday, it got to a point where within 1 minute of booting, my PC always froze, making it completely unusable. I booted into safe mode (which never froze) and tried turning drivers off but nothing helped.  I even reinstalled Windows 11 this morning and it was freezing during the install process.  Then, after about 20 other useless reddit threads, I found this one. I made a few of the changes in the BIOS that you mentioned (not all of them because I couldn't find some), and it fixed it.  I wish I had found it before I reinstalled Windows, but I'm just so happy that I finally found useful advice. Thanks!",Negative
AMD,"I've been having the same issue, random restarts when idle/light load but never during heavy load like gaming etc.  I'm also on Win 10 and I OC'd my 5950x with Ryzen Master using PBO.  I tried 2 things and I think it fixed my issue.  \- Set LLC(Load Line Calibration) to level 3 (default auto)  \- Set Global C-State Control to on (default auto).  Setting this to disabled made my system even more unstable.  I had some BSOD while gaming.  It might be too early to tell because I just made these changes a few days ago, but so far not one random restart.  Also my cpu is still hitting 4.8/4.9 GHz peak speed.",Negative
AMD,I use CPU-Z (freeware) to show the CPU technical details,Neutral
AMD,"Get a 7800X3D + AM5 + DDR5. The 5700X could be limiting, especially if you play games like Valorant or CS.",Neutral
AMD,"the only bottleneck is your bank account it seems  sure, it would limit performance, but there are minimal graphics settings for a reason.  Personally, I wouldn't buy a ""bridge"" component, seems like a waste of money, unless you can get a really good deal on it.",Negative
AMD,7600x3d,Neutral
AMD,The 5700X is good enough for now.  If you're not satisfied with the 5700X performance or dont think you will be then just go 5700/5800X3D despite the higher costs (DDR5 alone is just as expensive).  The gains of an AM5 X3D will usually be minimal at 1440p with a 9070XT.,Neutral
AMD,Or 7500x3d if cheaper.,Neutral
AMD,"I don’t think there really are any super significant suggestions to give. You have a really solid list with pretty good value parts all around.   The only things I’d point out are potential savings in the case and GPU? I’m not too familiar with the au market but the 9070 XT has some cheaper models that still rep good temps like the ASRock Steel Legend which seems to be the cheapest based off au pcpartpicker right now. I’m not sure if there’s a specific aesthetic you’re going for or noise levels you’re watching out for so you could completely ignore me if you have those in mind.  Also, I just built my rig with the Phanteks XT Pro case which seems to cost $80 less than the Lian Li and can personally recommend it! Great value with good airflow and lots of cable management space. Also still comes pre installed with 4 fans that I personally don’t find noisy at all.  Otherwise, I’d be pretty damn happy with your list! Send it for those black friday sales!!!",Positive
AMD,"Very nice and balanced build!  I don’t have any specific recommendations; usually when I recommend parts to people, my first question is always what games do you like, and what resolution/frame rates are you looking to achieve.  This is a very strong 1080p-1440p high refresh rate build, and for the three games you mentioned, seems more than adequate!",Positive
AMD,"Overall it looks fine. The ram is high profile, so you'll need to shift up the front fan on a dual tower air cooler, though I'm not sure how well that works with the Peerless Assassin Digital with the screen on top. You could use an AIO instead, it wouldn't be a big price jump, especially if you switched motherboards-still staying on the B850 chipset but on a board that's less flashy but still has a solid feature set. You could also use a single 2 TB M.2 drive, which would be cheaper than the 2 separate 1 TB options, the WD SN850X is a very good drive. Just suggestions though, the core of the build is solid :)  https://au.pcpartpicker.com/list/2KTTsp",Positive
AMD,Personally I'd get a 2 or 4tb ssd but if you're ok with just 1tb that's completely fine.,Neutral
AMD,"Thanks, i have been running an intel 2600k on a corsair h100 AIO for 14 years which has been on  24/7 and it has only just started failing, im sure some of the coolant has evaporated and i can now hear the pump. This is my first and only ever use of an AIO so unsure what AIO to even get hence i stuck with air cooling as i know it works and is will never have a pump failure, fans are easy to replace, pumps not so much. Didnt think of the height thing with the RAM",Negative
AMD,"wanted to keep os drive and games/apps drive seperate, have lost a few drives over the years so trying to keep them seperate, plus makes it easier to backup and rather lose 1tb of data than 2",Neutral
AMD,"No problem. 14 years for an AIO sounds very good, even with some evaporation. There are AIOs out now that are refillable and that come with an extra small bottle of coolant for topping them up after a few years. Bequiet! have a few models like that. But a dual tower air cooler will be fine for the 7800X3D, and looking at the Peerless Assassin Digital product page it looks like the front fan clears the screen so you would be able to move it up to clear the inside RAM stick, so it should be fine.",Positive
AMD,"Oh, I thought that was going to be your only drive, if you have more than 1 drive 1tb should be great for an os drive.",Positive
AMD,"thanks, kinda worried as i already bought the peerless assassin digital it just hasnt turned up yet.  Looking for info on how high the front fan will need to be raised and how does that affect the cooling?   Cant seem to find anyone with air coolers using my board, they all seem to be using AIO. there is many AIO choices now its hard to even start looking.  Dont have everything so cant even test how it will look, slowly buying it all online using black friday sales.  If i was to upgrade to say a 9000x3d will i also need to change coolers? should i just return the peerless assassin and go AIO?",Negative
AMD,"Moving the front fan up on the cooler a few mm won't harm the CPU, there's still the middle fan pulling air through the heatsinks so both of them acting together will be fine for cooling. The 9800X3D would also be fine with that cooler. Only if you're running very intensive CPU tasks for long periods of time would it struggle or begin to thermal throttle.  An AIO would give a boost in cooling performance while potentially being quieter, depending on the fan curves set up and the task that's running.",Neutral
AMD,9070 xt is a really good card. Perfect for 1440p gaming. Amd usually have good drivers. Nvidia is far from perfect too.,Positive
AMD,"The ""AMD has driver issues"" nonsense has aged so poorly. They did have a ton of issues with the original RDNA architecture in the RX 5000 series, but that was years ago. Now Nvidia's drivers have been unreliable since the 50 series launch, but they have gotten slightly better.  As for ""don't cheap out"" they're right, but also don't waste money for no reason",Negative
AMD,"Nvidia drivers had been extremely problematic recently, so you'd want to get amd gpu if driver stability is a concern.",Negative
AMD,"They both close in performance and the 5070 ti is of course better in Raytracing, but 150 € difference is huge like you said! I'll go for the 9070XT,the myth that AMD drivers are bad is really getting old and actually Nvidia had more problems last year. Go AMD you won't regret it, i'm actually considering the same i have a 3060 ti and i'm going 9070 non xt cuz it's the only one i can afford + i don't have to change my PSU.",Positive
AMD,9070XT is not cheaping out but 5070Ti is overall the better card. I went from 3070 to AMD and was told the same about the drivers. I've had no more issues than i had on the 3000 series so that argument does not work for me anymore. But if you want the better card go 5070Ti.,Neutral
AMD,"I had the same choice between 5070TI and 9070XT. Mainly because of the 200 euro diff I went with 9070XT. I play 1440p ultrawide, have had 0 problems thusfar. Everything runs beautifully and without issues.  I'm sure the 5070TI would've worked great as well, but not for 200 euros more, thanks.  Update problems? Never had them with AMD. Running a Ryzen 9600X and everything is sunshine and rainbows. Full team red this time, who knows if NVIDIA gets their shit together next time I'm building my PC I go team green again. Just depends on who's giving me gear that's actually value for money.",Positive
AMD,"Just made the jump from a 3070 to the 9070xt. just know that there are lots of people having issues with the latest drivers, so the recommended thing is to just go for the 25.9.2 (just go to the drivers page and select get older version). After seeing people try to fix the issues, I just immediately went to the old drivers and I've had absolutely zero issues and my performance has been miles better.      I can't comment on its performance for productivity, but gaming has been great anyway.",Neutral
AMD,"I went from a RX580 to 6800XT to 9070 GRE and just got a 9070 XT for Black Friday, so maybe I have some bias lol.  FWIW, I have purchased a GTX 970, then a 4060 for my son.   I can only speak from my personal recent experience but there are no driver issues with AMD that have impacted me in any way. I wanted an upper midrange card and had a choice of an $850 CAD XFX Quicksilver 9070 XT or $1,000 Gigabyte Windforce 5070ti. For $150, the 9070 made more sense for me since I primarily game. FSR vs DLSS isn’t a factor since raw performance at 1440p with the games that I play is great, even with the GRE I replaced.  If productivity, specifically programs that will use cuda, it’s sort of a no brainer, get Nvidia. Otherwise, save that $150 or use it towards the skyrocketing ram prices /s",Neutral
AMD,"i've switched 1,5 years ago, no issues worth mentioning. actually i'm satisfied with what AMD offers and i upgraded today to another AMD GPU (9070xt). I don't miss Nvidia and i'm not planning to go back anytime soon.",Positive
AMD,"I pulled the move yesterday, upgraded from a 4070 super to a 9070 xt, it’s good so far",Positive
AMD,"Make sure you have enough power and case room for whatever you get. Both of those cards are long and power hungry.       I almost got a 9070 xt and glad I checked first because I would need a new case and PSU, so I'm just gonna hang on to my 3060ti and probably do a whole fresh build in a few years.",Neutral
AMD,"Look for gaming nvidia is probably still ""better"". Frame Gen, DLSS.... is just extremely good. U wanna know how good - I got 380 fps with a 5090 in bf6 maxed out! on 4k. Without dlss and framegen i get 140 fps.",Positive
AMD,"I'm in a similar situation. I want to gift my brother a new gpu for Christmas, even though he doesn't like expensive gifts. Whatever, I'll say its a future bay gift. Because hes stuck on 2080 and wont be able to afford a new gpu any time soon (and the ai hike is getting out of hand).  Im stuck between a Sapphire 9070 Pulse for 540 and an Asrock 9070xt Challenger/Steel legend for 600-620. Which model is best for most games on high/med 1440p and not too noisy? Should I look for other models of these two? Or go with the 5070? (I hate 12gb vram so much its unreal, but nvidia uses less vram in the same games I heard?)",Negative
AMD,West from 5700 XT to 2080 Super and now 6900 XT. It's all the same and marketing. Get the bang for the buck and later for all that brand BS.,Neutral
AMD,"I use RX7600XT and it serve my gaming usage well, although I am not sure how it performs on Windows nor knowing how well is it on CAM software.",Positive
AMD,"9070 XT ftw, I have a NVIDIA card and ray tracing is not worth it, I notice very minor differences.",Negative
AMD,"A store associate at Microcenter told me if I didn't need Nvidia for work, he'd recommend the 9070XT over the 5070Ti every time. The 9070XT was listed at $569 and the 5070Ti was listed at $730 at this location.  From the benchmarks I've seen, the 9070XT outperforms the 5070Ti in some games, and vice versa. On average, the 5070Ti is 2-6% faster, but that didn't really justify an almost 30% price increase to me, even with ray tracing, path tracing, DLSS4, and all of Nvidia's other perks.",Neutral
AMD,"NVIDIA aims (or at least used to) for great Day 1 driver support, but rapidly moves on to the next fad game and sometime remember to followup on prior work.  Radeon aims for decent Day 1 driver support, but importantly keep working at things until they have achieve greatness. The phrase 'ages like fine wine' is because the constantly strive for incremental improvement and are forever looking at prior work that can be improved.  Next question is are at all tempted at using Bazzite (Linux) for gaming (even if you intend to keep running Windows via dual boot options). If you want to dio your toes into Linux based gaming in anyway then go with the 9070XT as NVIDIA is possibly more adverse to Linux compatibility than nerds are to wearing deoderant at a convention.",Neutral
AMD,"Anyone who built their own computer will potentially experience GPU issues related to drivers.   The increased mention of ‘this GPU has driver instability’ is directly related to what GPU the market is building with most. There was a solid 5 years when you could get the same performance and save 200-300 USD by choosing a New AMD, so when AMD started majorly gaining in marketshare years ago the ‘driver instability’ train might have been real for a few months, but now that even consoles are running legit AMD GPUs it’s hard to blame just the GPU. It’s probably just people new to building their own computer who are learning how to troubleshoot.   Now that the few people who can afford to build newer PCs are mostly choosing Nvidia again, you’ll see people talk about their drivers being problematic.",Negative
AMD,Switched from i5 9400f 1660ti to Ryzen 5 7600X and 6700XT. I hope it will be stable for at least 4 years.,Positive
AMD,Nvidia's drivers have been somewhat buggy this generation. However AMD has been doing concerning things with their drivers then trying to gaslight the public about it. https://www.reddit.com/r/Amd/comments/1on8j0i/amd_says_were_confused/,Negative
AMD,Most ppl do not turn back after the switch.  Do it!!!,Positive
AMD,did the same. my 4080 super was loud as fuck. went to a amd 7900xtx. more silent. barly choil whine. now it is water cooled and makes almost no sound at all,Negative
AMD,RX 9070 XT is better priced and close to RTX 5070ti in performance.,Positive
AMD,"If you play new games- FSR4 is fantastic and the standard RT perf is solid.   However, if you want AI upscaling in older games too and better heavy RT perf then Nvidia is your best bet.   I had both a 5070ti and 9070xt and kept the 9070xt.",Positive
AMD,"AMD is good value for money, but I'm on the 9070XT and while it's a good card, I'll move to the RTX 6080 when it comes out.   I think the AMD drivers/software aren't great.",Negative
AMD,"I've been reading about a lot of driver issues lately from r/amd   Sometimes AMD has a spell of good drivers and sometimes they have a bad spell.    Nvidia has its own niggling issues as well, but in my experience, it has been less so.",Negative
AMD,"I’d do it, if I were upgrading my GPU. A friend has one, he plays at 1440p ultrawide with zero issues, BF6 he has it locked 120fps, but he is holding back by his 5950x (in this case)  I also have a 9900K, and I’ll tell you, some games will be hold back by the CPU, I had a 2070 and upgraded to a 3070Ti. Currently there are some recent games (2023 and forward) that are struggling a bit to keep up, it isn’t bad but there are some spikes, also the 1% lows could be better, but it is a 7 year old at this point (or nearly since I have mine since Jan 2019)",Neutral
AMD,"I've been team AMD for years, both CPU and GPU, and I don't recall the last time I had major driver issues on their platform.  It's important to have competition in the market, and all the proprietary Nvidia APIs is something I consider hostile and bad for our industry long term. AMD has a long term commitment to open standards, which is great in my opinion.",Neutral
AMD,"I dont understand the driver problems for Nvidia, currently rocking a 5070ti, no issues at all they will even release a new driver soon that will boost performance even more, I would get Nvidia because of it's futures and if you look for longevity, MFG better dlss and smooth motion for unsupported FG titles, and I would get 9070xt if you plan to upgrade in the next 3-4 years anyways, keep in mind that my personal opinion, better check some videos after the new drivers update which will happen soon and decide for yourself",Positive
AMD,Naw,Neutral
AMD,The RTX 5070 Ti is the way to go.,Positive
AMD,Stick with NVIDIA!! Get that 70ti and call it a day!!,Neutral
AMD,Simply Put AmD is for gaming shile Nvidia is for AI. Nvida has made it big for more than just gaming gpu and i think they have it for kicks and giggles but their real money maker is coporate AI farms.,Neutral
AMD,Just a quick question. Do u think it will be overkill for 1080p 240 hz ? CPU is 5800x3d Or just save 100£ and go 5070,Neutral
AMD,"AMD seems more sensible to power issues and speed limit from board makers.  I.e, If you have a 750w PSU when the power requirements for your GPU is 850w, it will work sometimes, but other times it will black out and crash, even pcpartpicker says your system requires 409w.  And, for higher end models like the 7900XTX (might be a problem with some 9070XT too, not sure), some board makers tend to overclock their gpu a little too agressively and you have to lower the clock speed below the advertized speed by 200mhz.  It's still working great, you don't even notice a FPS drop, but there will no longer be crashes or black out.  AMD should enforce stricter guidelines on board makers specifications as it's creating problems there for games when under load, after a while.  But no one is doing QA for very long on anything anymore...",Neutral
AMD,The issues with windows updates has been so much fun to deal with.,Positive
AMD,My experience has been the exact opposite,Negative
AMD,"I had a 3060ti. I bought the 9070XT, it literally doubled my fps. Its insane.  I didn't upgrade my PSU, I'm still rocking my old faithful 650W corsair. I undervolted the GPU for gains and it doesn't go past 250W anyways, my 3060ti regularly hit 210W so its not that much of an increase in Wattage. So you wouldn't need to upgrade PSU tbh",Positive
AMD,"its always that comedy for AMD cards all the time, users will tell ""use [AA.B.CC](http://AA.B.CC) driver"", ""undervolt it"", ""use xxHx windows version"", just to not work and never get it done, then they will rise the bar and say ""use AMD Software: PRO Edition because its more stable"".",Negative
AMD,I really can't justify paying the prices nvidia charges now.    I dropped $1100 on a 1080Ti way back. No way I'm dropping 5 grand for their top spec card now.    When my 1080Ti finally gives up the ghost I'm either going Intel or back to AMD.,Negative
AMD,You use frame gen in battlefield?,Neutral
AMD,"so, not real 380 frames in not real 4k?",Negative
AMD,Who uses Framegen in competetive shooters xD,Neutral
AMD,i would cry if i saw that. It'd be too beautiful for my mortal eyes,Positive
AMD,DLSS4 will be nice in a few years as these cards age and new games get harder to run and require more upscaling. AMD's FSR4 is barely being integrated by any game devs while DLSS4 is almost universally used.,Neutral
AMD,Adrenalin is way better then nvidia app imo.,Positive
AMD,Don’t worry it’s just as annoying with AMD. Windows updated my GPU even though I turned off auto update. 😑,Negative
AMD,Yeah this is me right now. Everytime there is a windows update it messes with my AMD drivers.,Negative
AMD,"Yes, but I've killed the updater, so... Just a few extra steps",Neutral
AMD,"understandable. to be fair AMD is not competing with Nvidias top spec cards right now, but they are more than enough for a good gaming experience imo.",Positive
AMD,but feels very real? or am i missing something,Neutral
AMD,I been playing in 1080p on my 5080 to use the dual mode 480hz on my new monitor and it's honestly pretty awesome.,Positive
AMD,"If using Nvidia Reflex the Input sampling is taken before the framegen step, i believe. So the input lag ""should"" be very low no? This is not cs or valorant obviously - you do you. But having so much fps at a frametime of 1.1ms and then adding 2-4ms of input lag with framegen doesn't hurt in battlefield, i cant even notice it, shit is butter smooth and responsive with this hardware",Neutral
AMD,"fuck yea it is!! my eyes water on that 240hz oled screen, it's honestly breathtaking and so fucking smooth on top of that.",Positive
AMD,"I disagree, adrenaline crashes way too much even on the most stable driver (25.9.2). It's very unpredictable.   Other than that, it would've easily been a way better software!",Negative
AMD,"Weird, I haven't experienced that with my AMD card.",Neutral
AMD,"i won't say anything about upscaling because it's pretty subjective atp, but framegen looks like motion blur in obscene amounts  and in competitive shooter nonetheless",Negative
AMD,"My question is when the fake frame is being inserting between the real frames do the actions still take place? I never used frame gen on competitive shooters because I've always though if a fake frame is taking place while let's say shooting doesnt it delay it and your input? Its not the input delay but the ""action"" delay that annoys me. Could be wrong but I've seen people mention that before.",Negative
AMD,Ive had Amd for many years. Only at one time did i experience low performance from a new driver. I did not have issue with driver as long as i ran stock oc on the cards. Oc profile changed from driver to driver. But i usually only put a slight undervolt on the cards. Only great experience with amd.,Positive
AMD,Same thing happened to me as well. I had set my driver updates to manual but then windows forced an update and it ended up updating my Adrenaline software as well. It's a Windows issue.,Negative
AMD,"Literally just switched back to nvidia because my 7800 would conk out weekly. Arc raider has been especially problematic, even with game ready drivers - Adrenaline wouldn’t stop forcing me to reinstall older drivers.  That said, it’s largely windows updates that is screwing everyone over. Both amd and nvidia cards is having a rough time atm.",Negative
AMD,It's absolutely random.,Negative
AMD,I'll definitely have to test this,Positive
AMD,"I experienced regressed performance and weird behaviors from my 7800XT on the latest drivers, my GPU was idling at 50C and the fan wouldn't kick in + worse performance in some games AND driver crashes. On my current driver that I listed above, I get expected performance and behaviours from my GPU but the driver crash happens every now and then unfortunately 🥀",Negative
AMD,10 or 11?,Neutral
AMD,I guess none of us should be surprised then. Jumped on Star Citizen one day after having to rollback my drivers and then it auto updated again forcing me to do it a second time!,Neutral
AMD,"You should roll back driver to older version. Its easy, and youre not missing out as long as its not many years old. Go back to the previous stable one, and then wait for a alt or new future driver.",Neutral
AMD,11,Neutral
AMD,11 too. I even made sure AMD auto update was off too.,Neutral
AMD,"That might be the difference, I'm on 10.",Neutral
AMD,"So you can only upgrade very slightly within your current build in terms of CPU. Unfortunately, you're still on the AM4 platform. Upgrading to a 7800X3D for example, would require you to also replace the motherboard and RAM along with the CPU, as you'll jump from the AM4 socket to the AM5. IF you want to stay within this generation, you could consider upgrading to a 5700X3D or similar but improvement will not be huge.   The 5070 is a fine choice although you could doubt if a 9070 XT will be worth more in a few years.",Neutral
AMD,"if you can, save up for any AMD with a x3D on it, the 3d cache is NECESSARY for top tier gaming  , especially if you're rocking a 5070 and need all the extra boost you can get. the larger l3 cache is insanely good and made specifically for gaming, it will do most of the heavy lifting. anything less than that won't be worth it. if you can, save up before u upgrade and you won't be unhappy at all. i just got a 9800x3D and its completely changed my gaming experience",Positive
AMD,With a 5070? I would put that in the 5600 build.,Neutral
AMD,"9600X is 200€. But any X3D is better. Look for the 7500X3D, I found it in my country for 280€.",Positive
AMD,9700x or an x3d chip 7800 over 7500,Neutral
AMD,"how exactly can i upgrade to the AM5 socket? is is really complicated and is it worth it? like are there AM5 cpus that are better for almost the same price as one in AM4? i mean i want to upgrade both my cpu and my gpu, so maybe i can do a ""big ugrade"" ot my pc and also change the cpu socket? what would you say?",Neutral
AMD,"Like the above said, it’s really not going to be worth it to upgrade to the top tier am4.  There’s a small increase in performance and you probably won’t even really notice it.    You should save up for AM5.  Bad time because of ram, but in the long term it’s more worth the investment.  You can get a good cpu/mobo combo deal if you’re lucky, but that ram isn’t going to be cheap.    Check r/buildapcsales",Negative
AMD,wdym by that?,Neutral
AMD,Changing CPU to an AM5 CPU would require a new motherboard and RAM. RAM is currently really expensive so I doubt you'll find anything good value and any of these options will be nowhere near the amount you'd otherwise spend on just a new CPU within the current socket.,Negative
AMD,"But to also answer your other question, yes the AM5 CPU's are quite a bit faster than the AM4 ones.",Positive
AMD,i think then ill stick with my AM4 socket,Neutral
AMD,A lot of us are in the same boat and doing the same.  While AM5 is faster our wallets would be significantly emptier.,Neutral
AMD,"2080 Ti for $200, assuming you have a decent power supply. Slightly faster than the 3070 and 3GB more VRAM.",Positive
AMD,Arc b580,Neutral
AMD,Intel Limited Edition Arc B580 12 GB Video Card (31P06HB0BA) - PCPartPicker https://share.google/kVfgfofPw7K2qKX5k  ARC B580 @ $249.00,Neutral
AMD,"Benchmarks sites with prices, games tested and places to buy",Neutral
AMD,Intel arc b580!,Positive
AMD,2080ti or 6800xt.,Neutral
AMD,Where you are from? RTX 5050 is pretty cheap. RTX 5060 might also fall into that range at the moment.,Positive
AMD,Don't buy 8GB vram if you want more than 1080p,Neutral
AMD,"The 2080ti and 3070 are pretty similar in performance. Believe the 2080 ti is slightly faster and also 3GB of VRAM. But the 3070 has a lot more features like resizeable bar, which can increase performance.  I would say 3070 for 1080p, and 2080 ti for 1440p if they were similar prices. But the 3070 is quite a bit more. For the used market, sometimes 6800/6800XTs can be found for similar prices around $250-$280. I would check this instead, since in Rasterization, it's much faster then both cards.  If you do live in the U.S, the 3070 does sell for $210 rn, from Zotac Refurbished - [https://www.zotacstore.com/us/zt-a30700f-10plhr-r-1](https://www.zotacstore.com/us/zt-a30700f-10plhr-r-1)",Positive
AMD,"From where? I see suggestions for previous generations all the time but I have no idea where to look for them.  EDIT: Nevermind, just did a basic search to be sure and suddenly I'm seeing a ton on Amazon? Weird as hell, but at least there are options (and something to make up for the insane RAM prices).",Negative
AMD,"I got the acer nitro b580 at b&h photo for $235, with bf6! Its a unit.",Positive
AMD,United States. I’ve seen the 5050 around the same price but I’m having a hard time sorting out the differences here 🥲,Negative
AMD,"The 5050 is \*way\* slower than either of those, I'd go for a B50 for a new card in that price range. But the 2080Ti is the best choice here, if it can be found at that price.",Neutral
AMD,Not too stressed about passing that. I’d prefer 1080p with decent frames and performance over anything. Not trying to build a monster just a decent pc that plays well haha,Positive
AMD,650W power supply. And yeah ram prices are insane??? I did NOT pay that much for mine a year ago but now trying to get 16 or 32 GB of ram is very expensive,Negative
AMD,"Usually Amazon is the worst. ebay is usually okay, Facebook marketplace, Jawa, /r/hardwareswap, etc.",Negative
AMD,Yeah. Good card at this budget range.,Positive
AMD,"Pick whichever speaks most to you:  RTX 5050 is the standard recommendation. It's within your budget. Has all Nvidia features and will be able to play all games, at least on low settings at 1080p. Its performance is little bit lacking and thus doesn't have the best reputation online. However, it's firmly in your budget and has all the Nvidia's bells and whistles. Software, driver support, etc.  [https://pcpartpicker.com/product/J2jv6h/msi-ventus-2x-oc-geforce-rtx-5050-8-gb-video-card-rtx-5050-8g-ventus-2x-oc](https://pcpartpicker.com/product/J2jv6h/msi-ventus-2x-oc-geforce-rtx-5050-8-gb-video-card-rtx-5050-8g-ventus-2x-oc)  Intel B580 is the best value at the moment for you. However, GPU is little bit quirky and has less matured drivers. It works well in most cases and you will be able to play all games well.  [https://pcpartpicker.com/product/Kt62FT/intel-limited-edition-arc-b580-12-gb-video-card-31p06hb0ba](https://pcpartpicker.com/product/Kt62FT/intel-limited-edition-arc-b580-12-gb-video-card-31p06hb0ba)  AMD GPU is Nvidia's alternative. It will perform better than RTX 5050, but it's little over your budget. If you want more reliable GPU than B580 and can stretch your budget, I would recommend you getting this card.  [https://pcpartpicker.com/product/YD7MnQ/xfx-swift-oc-radeon-rx-9060-xt-8-gb-video-card-rx-96tsw8gbq](https://pcpartpicker.com/product/YD7MnQ/xfx-swift-oc-radeon-rx-9060-xt-8-gb-video-card-rx-96tsw8gbq)",Positive
AMD,"Go on YouTube and search ""Hardware Unboxed"" they put out a video yesterday about the best GPU at every price range and the B580 is the one at your price. More VRAM, 5060 performance at 5050 price",Positive
AMD,"Old cards are very niche recommendation. For normal person, they will benefit more from extended driver, feature support and greater reliability. The fact that an old card can stop working at any time is never properly accounted by people.  Buying commercial cooler, probably mining GPU is just so risky.  Also, it's not \*way\* slower. RTX 2080 Ti is about 28% faster and generally is not even within OP's budget. He would have to buy lowest, commercial cooler model. Will have awful noise and card was probably abused.  Btw: He probably doesn't even have PSU to feed that GPU.",Neutral
AMD,"Even at 1080p, certain titles run out of 8GB VRAM at 1080p, especially the ones that require RT. The 2080 Ti is the better pick as long as you have a PSU that can handle it.",Neutral
AMD,Thank you!! Lots of people are recommending this Intel card and so long as my Ryzen CPU is good enough it’s likely what I’m going with. :),Positive
AMD,"28%, at a similar price, \*is\* a big difference (especially as in this case it's 28% better performance and 25% \*lower\* price). And yeah, it's always a risk buying used. But he says he can get one for $200, which is a great price for whichever 2080Ti model. It's a \*way\* better deal than the 5050 in any case, that card has no business being anywhere near the price it is. As I said, if you want a new card for $250, get a B580. It's faster and has more VRAM than the 5050.",Positive
AMD,"It requires 650W, same as the 3070.",Neutral
AMD,Below $250.00 its the best value card right now.  Next step is 9060xt/5060ti,Positive
AMD,"B580 has driver issues and CPU overhead.  Not sure where he can get 2080 Ti for 200 dollars. Just checked ebay, they generally are around 300 dollars in USA. Trying to get the cheapest models out of shady sites is a recipe to get scammed. He might just confused RTX 2080 with RTX 2080 Ti. In which case, it's completely different GPU.",Negative
AMD,650W is what I’m coming from and it’s the minimum I’d be getting this time around. No worries there,Neutral
AMD,"All cards have driver issues at some point. Intel has sorted them out, as well as the overheads (which only affect older CPUs anyway). As for the price, I agree, it's very low for that GPU, but it's what the OP says he can get it for so I say go for it ASAP.",Neutral
AMD,"His CPU is exactly the lowest point from which you could get an Intel GPU. Hardware Unboxed done testing and even Ryzen 5 5600 is still slightly too weak and he will lose performance in some games.  A lot of driver issues were not solved with Intel GPUs and I'm quite disappointed. I had even made video with extensive testing on the subject. However, Arc GPUs still have artifacts where land or bodies of water simply disappear. Red Dead Redemption 2 and World of Tanks have these severe visual artifacts. Then in Starfield, all assets have a grid put on all textures for some reason. Intel GPUs are definitely still quirky.",Negative
AMD,"His CPU is better than my i5-12400F, and my B580 works just fine with everything I tried on it.  ... not that it's relevant, as the question is whether to get that 2080Ti. And I say yes.",Positive
AMD,"You lose actual performance with Arc GPUs if your CPU is weak. He will lose FPS due to his Ryzen processor. Also, Arc still has driver quirks and any other manufacturer would give him better driver experience.",Negative
AMD,"Yes, I know Arc loses some performance if the CPU is weak, but that generally applies to somewhat older CPUs, such as Intel's 10th gen and older or Ryzen 3000 series. And as I said, my CPU is weaker than his and my performance loss is negligible or irrelevant, your choice, as the games run more than adequately (70+ FPS on Indiana Jones). \*And\* the drivers are constantly improving.  Would he get better performance with other manufacturers? Possibly, but the only other new option in that price range is the 5050 which is generally slightly weaker and has 8 GB VRAM which will cause \*much\* more issues in some games. The way to avoid that is to either get the used 2080Ti (or 3070, still a much better card than any new one at $250) he asked about (a great choice, again) or stretch the budget to a 9060XT 16 GB (the best option, but far overbudget).",Neutral
AMD,"Just read all the manuals first. If you really can't stand to do that, read the motherboard manual, it's the most important.  Your motherboard is quite an old model. The CPU is newer. They might not play the best together until you update the motherboard BIOS to the latest version. It's [not supported at all before version 2613](https://rog.asus.com/motherboards/rog-strix/rog-strix-b650e-f-gaming-wifi-model/helpdesk_qvl_cpu/), but the latest is 3287 so unless you mobo's been sitting on a shelf for over a year it probably has something between those versions. You can use the BIOS Flashback feature to update the BIOS before you install the CPU or memory to be extra sure.",Neutral
AMD,What’s your budget?,Neutral
AMD,This build keeps me under by 150-200 but like1900 give or take,Neutral
AMD,"https://pcpartpicker.com/list/cXLQTM  You could go high end and still be within that budget. 9800X3D, 9070XT..You can always add fans on later.",Neutral
AMD,"You'll most likely be getting him a whole new PC. If you are aiming for a 5070 Super, then a good mid-range CPU is something like a 9600x or a 7800X3D/9800X3D.  Unfortunately, DDR5 RAM is very very expensive. It's expected to only go up in price for the next few months too.  Your best bet currently could be to stick to AM4 and keep the DDR4 your nephew currently has and upgrade the CPU to a 5600X or something similar and mobo to a B550 and then when DDR5 gets cheaper, move to a whole new build?",Neutral
AMD,"Just done a bit more research and with RAM costs being what they are, would keeping the AM4 motherboard and DDR4 ram, and just replacing the CPU and GPU be a terrible idea? 2070 Super and a **5700x** or similar?",Negative
AMD,5700x + any GPU more recent than his current one will be a huuuuuge upgrade. I'd also say if you're willing to spend out a little that something like a 3060 12gb would be the perfect upgrade for that system.,Positive
AMD,"Perfect, thanks, really appreciate the help and I'd just started thinking the same way. Another quick question if you don't mind:      What's the thinking behind replacing the motherboard in this case? I know A boards are poorly regarded, but is there something specific he'd get from the B550 you suggest? I assume a 5600X would work in his current board since it's AM4? Or is there more to it? I've built a few PCs but always Intel so this is new to me.",Positive
AMD,Thanks! Only reason I'm leaning towards the 2070 Super is the fact I have one available.,Positive
AMD,"Just looked it up as I assumed 5000 series ryzen wouldn't work, but it should be fine so long as you update the Bios before installing the new CPU.  So just a CPU & GPU should be fine for now",Neutral
AMD,"Yeah of course, that would still be significantly more powerful than a 970 haha! But if he has a 1440p monitor in his recent future then the 8gb of VRAM on the 2070 could be limiting and require making another upgrade to accommodate. Sometimes it's easier to just do it in the first place to save the time. Either way, you're providing a big bump in specs so this is splitting hairs really haha 😆",Positive
AMD,"Looks mostly good, though you are making some interesting choices for a 1000 EUR PC.  500GB+1TB is rarely good value over a 2TB drive, so I'd look into that.  > graphics card: GIGABYTE RADEON RX 9060 XT gaming oc  Make sure to get the 16GB version. It performs the same as the 8GB version but the double VRAM will make it last so much longer, even at 1080p.  > case: Corsair 3000D airflow > >case fans front: Corsair RS120 PWM (3x) daisy chain > >case fan rear: Corsair AF120 Elite > >case fan top (back): Corsair AF120 Elite  You are spending a lot of that budget on excess amount of fans (those components aren't exactly the hottest) and a case that's usually not good value. There are very solid quality budget cases that come with ARGB fans included. The Montech XR being a good example.",Positive
AMD,"Hey so, I haven't made a PC before, but I have researched a bunch on all the parts required. I'd say your build is pretty good actually and kinda similar to my dream mid-range build! Two things I'd change though are the case and SSD, I think you should get the antec c5/c8 or the lian LI 216/217 instead of the Corsair 3000d airflow, as they seem far superior and offer much higher value. For the SSD I'd recommend getting a decently priced 2tb SSD such as the wd black sn7100",Positive
AMD,"Looks ok! I do have questions about the CPU tho. Especially because you could maybe save on not getting 32GB but 16GB of RAM now so you can expand later, and put those savings more towards a different CPU. The CPU is a component I wouldn't be saving too much on.",Neutral
AMD,"SSD: good tip, will look into it!   graphics card: forgot to mention, we will use the 16GB version  Case: the case included 2x AF120 Elite, so we will only buy the 3x RS120 (for 19,90eur)   The 3000D just looks so nice we both think :-)   We want a clean black build, no RGB",Positive
AMD,"Just to clarify, having 32GB is amazing of course and I'd 100% recommend it. But if the choice was a somewhat crappy CPU with 32GB of RAM or a proper CPU with 16GB of RAM, i'd choose the latter. RAM Upgrades are easy and can be done at any time. CPU replacements take significantly more effort and is usually a lot more expensive if you want to change this later on.",Positive
AMD,"> We want a clean black build, no RGB  Ah, I just figured they were RGB fans. Sounds good, I'm also of blacked out non RGB builds. :)  Enjoy the build!",Positive
AMD,"Good point.  My decision to go for 2×16 GB is based on the fact that modern games can easily exceed 16 GB of system memory, and dual-channel 32 GB helps prevent RAM overflow and page-file usage, which directly improves 1% lows and reduces stutter.  DDR5 also performs best in matched pairs, and this 2×16 GB kit runs at 6000 MT/s CL30 with EXPO, which is ideal for Ryzen and gives very strong latency and bandwidth.  I know a 7800X3D would reach higher average FPS, but for 1080p gaming with a 7500F the FPS is already more than enough for us (we mostly come from PS5 gaming). If we need to upgrade on day an X3D CPU is the way to go then.  So we chose to prioritize smoother frametimes and better 1% lows rather than chasing the absolute highest average FPS.",Positive
AMD,"> RAM Upgrades are easy and can be done at any time.  Yeah, about that.....",Positive
AMD,Understandably! But don't forget that on 1080P gaming a pretty large chunk of the load is still put on the CPU instead of just the GPU. It depends on the games of course.. 16GB of RAM however is a stretch for now but something easily obtainable and expandable in the near future.,Neutral
AMD,Well its the one that is cheaper in the end. Your links dont show how much the bundle is. But both are perfectly good for the 7800x3d.,Positive
AMD,"So glad I grabbed one when they came out, been a rockstar ever since.",Positive
AMD,"im planning to skip AM5, and resisting the urge to cough up frankly absurd amounts of money for a 2nd hand 5800x3d is hard to say the least. havent been able to find one at even close to a good price.",Negative
AMD,The goat of AM4,Positive
AMD,Holy crap wtf how,Negative
AMD,That’s a great deal!!,Positive
AMD,![gif](giphy|jPJjTXyOMIIc1d2Gbl|downsized),Neutral
AMD,https://preview.redd.it/83atxcyb0q3g1.jpeg?width=3024&format=pjpg&auto=webp&s=29e79fca070e43e22f25a4fd6d4a5a758d037984  Welcome to the club baby,Neutral
AMD,Good deal currently especially with ram pricing,Positive
AMD,for 170 big come up especially when they are over 300 now,Neutral
AMD,I got one when they were basically giving them away. I think I paid like 280 at microcenter for mine.,Neutral
AMD,"Oh sick, I got mine off marketplace for $300. Good find",Positive
AMD,AM4 Goat,Neutral
AMD,That's a steal tbh. Nice pickup. Hope it lasts you,Positive
AMD,I got the 5700X3D and it rocks.,Positive
AMD,Soooo jealous. I looked for six months unsuccessfully and finally ended up paying double msrp for 5700x3D new.,Negative
AMD,I have one up for sale on fb marketplace for 80. I can sell here if anyone is interested.  https://preview.redd.it/5zlkgp8yfq3g1.jpeg?width=3000&format=pjpg&auto=webp&s=093a59c24dbb13ad8d8ace8faebcc981e4dddb5b,Neutral
AMD,Nothing but good things to say about it,Positive
AMD,![gif](giphy|xT0xehobm5dF3obOrC),Neutral
AMD,https://preview.redd.it/dspfjjmn5q3g1.jpeg?width=930&format=pjpg&auto=webp&s=65711b6cb50c75c9e5b70692084fef8c94673156  Enjoy gng,Neutral
AMD,Just donated mine to my daughter’s PC. Couldn’t bring myself to sell it.,Neutral
AMD,Welcome to the club!,Positive
AMD,Brave. My inner cynic tells me anything sold second hand may as well be a paper weight.,Neutral
AMD,Having a 9800x3d and - 9070 xt- I say welcome man!,Positive
AMD,Lucky had to go eBay route $520 uhg,Neutral
AMD,"These things are almost worth their weight in gold. I had a customer return mine dead on eBay, and it still sold for $170 as parts. Finding a used one for $170 is an insane deal.",Positive
AMD,You need to run doom on it,Neutral
AMD,Welcome to the club.,Positive
AMD,Hell yeah brother.,Positive
AMD,I did the exact same upgrade 2 years ago. Then I switched 3070 to 4080. Enjoy!,Positive
AMD,I'm gonna say something that you guys are gonna hate. I have a 5800X3D in my HTPC that I rarely use for gaming.   /ducks,Negative
AMD,I've got a 5700x3d and it does plenty for my 4080 super. It's a crazy chip for that price.,Positive
AMD,This processor hasn't let me down yet. May it continue to be a powerhouse while the world weathers the DRAM price storm. Might upgrade to AM5 next console gen then,Positive
AMD,5800x3d still a very solid chip,Positive
AMD,Lucky mofo! But 5800X3D gang!,Positive
AMD,"It's a great CPU. I bought one early 2023, open box for $150. Still going strong.",Positive
AMD,"hell yeah, I'm looking at skipping AM5 with this build    love the 5800X3D",Positive
AMD,Nice! The 5800X3D really brings out the best in games. Can’t wait to see how it performs with your setup…,Positive
AMD,5700X3D is very nearly as good and is a great chip.,Positive
AMD,"I was about to jump on AM5 but the RAM price stopped me. So happy that I found this so I can skip the AM5, and my rig already has 32gb of DDR4 that I got for like $30 a year ago.",Positive
AMD,There's nothing better for Am4,Positive
AMD,I got the 5700 version of this CPU and it was way cheaper for like 90 percent of the performance.,Positive
AMD,thats not the same chip <3,Negative
AMD,"Where do you live ? Im interested, but i live in europe.",Neutral
AMD,"this, probably going to try and build a whole new pc when am6 is out anyway.",Neutral
AMD,People upgrade and sell CPUs all the time to recoup some money.,Neutral
AMD,Hopefully not USD. For that money you could've almost gotten a AM5 Upgrade to a 7800x3d.,Negative
AMD,Let's trade🤣 I'll give you my ryzen 5 5600 and $150 for it,Positive
AMD,Worth it over my 5800x? How much of a performance bump in general you think?,Neutral
AMD,Going for the same price 2nd hand.,Neutral
AMD,The upgrade path for AM4 will probably never be equalled by another platform. I'm still using the same MOBO from like eight years ago and going strong.,Neutral
AMD,"Arguably, the best value for AM4 was 5700X3D. It was almost half the price of 5800X3D while only being 10% slower.",Positive
AMD,"What about the 5900x and 5950x, 5995WX?",Neutral
AMD,5700x3d is goated too,Positive
AMD,Just noticed.,Neutral
AMD,In California.,Neutral
AMD,Was CAD got it since I have a Asus Dark Hero x570 and a 64gb kit of Trident Z Royal Elite 4000Mhz CL 14.,Neutral
AMD,"Oh lol I didn't see your flair, not much for you the 1% lows will be improved, but for someone with a 3600 or 3800x the upgrade would be big",Neutral
AMD,"I switched from a 5800X to the 5700X3D and the increase in 1% low’s was in SOME games very noticeable. Especially in Monster Hunter Wilds it was a difference. Although I’m not sure if I would pay more than 100€ for that (I got mine for free cause I switched it with my brother). If you plan to skip AM5, it might be worth it.",Neutral
AMD,"I have the same problem, the 5800x3D can be found 2nd hand for 300 or 400. I'm not paying that for a marginal increase.  Our cpu is 5 years old at this point. I feel like it can go a bit more.  I also just ordered an RX 9070 XT to upgrade the RTX2080. So no budget lol",Negative
AMD,The only downside is no PCIe 4.0 on the older AM4 boards. Gotta be careful about GPUs with a 3.0 bus and also SSDs are quite limited in speed compared to the newer ones.,Negative
AMD,Everyone already trying to inch out the last pump of am4,Neutral
AMD,The king is never cheap.,Neutral
AMD,5% at most,Neutral
AMD,Yes you are right for Price and performance but better 58003xD,Positive
AMD,Yep. I got one of these bad boys for that reason. Half the price and almost all the performance. Huge improvement in 1 percent lows in gaming across the board.,Positive
AMD,5900x is way better than the 5800x3d. Then again it depends on who you're asking. If you want to max out AM4 for max frame rate while gaming sure 5800x3d but if you have your setup for video/photo editing the 5900x is going to be way better while still being able to handle games pretty well. It's definitely slept on since the 5800x3d has hype resale pricing.,Positive
AMD,All arguably could be considered worse than even the 5800x (non-3D) for gaming due to CCX latency.  The difference was exceptionally marginal in practice but all cores/threads were all on one CCX/CCD for the 5800x/5800x3D. No cross communication necessary.,Negative
AMD,Too bad it's almost $350 now😭,Negative
AMD,Could you ship to Michigan? For $80 I'd love that. Im on a r5 5600 and while I'd really prefer a 5700x3d that's a great price for a upgrade,Positive
AMD,Ive posted this before but I will again. On am4 I went from an 1800x to an OC'd 2700 to an OC'd 3600 to a 3900x to a 5800x to a 5800x3d and the last step was almost as meaningful as the jump to the 3600 for gaming. It was actually a big upgrade. And i play at 4k.,Positive
AMD,Yeah I upgraded from a 3600 and the difference was immediately apparent.,Neutral
AMD,PCIe 4.0 GPUs are usable in PCIe 3.0 mother boards and by some comparison videos I've seen on youtube there's actually not much difference in performance,Neutral
AMD,Hello yes that’s me.,Positive
AMD,I upgraded to 5700X3D myself from 3600. And after recently getting an RTX5070Ti the gaming rig finally feels complete 😆,Positive
AMD,Yes I can! Do you know a good platform to sell it through? I was thinking of posting on ebay and sending you the link.,Positive
AMD,Join the club!,Neutral
AMD,Nice. Gonna upgrade my 6700XT sometime soon and then I'll pretty much have this platform maxed out. Should be good for another few years. Unreal how long AM4 has remained relevant.,Positive
AMD,"As seen by the accidental posting and redaction of the source-code, AMD COULD make an FSR4-Lite version for RDNA3 that provides better visual fidelity though lower performance increase than FSR3.1, they just don't want too (when that would REALLY benefit the new Steam Machine).",Negative
AMD,"This is a big middle finger to everyone who bought a 6000 or 7000 series GPU.   AMD revealed they had a functioning FSR4 implementation for older GPUs when they mistakenly released it in the FidelityFX SDK earlier this year (sometime around September, if I recall). Modders got it working, and the visual quality was significantly better than FSR3.",Neutral
AMD,Keep shooting themselves in their foot. Good job amd,Negative
AMD,Look my 7900xtx is still beast and best value for money back in 2022. Dont need fsr in 90% of games but when i do... ughhh. Future plans would be to just go for nvidia for 6xxx series,Positive
AMD,"Bought 7900xtx last year, biggest mistake",Negative
AMD,"AMD really just stays fucking up don’t they? Do they even realize how far behind they are? They need to be doing every pro consumer move they possibly can to to make up for their lack of high end GPUs. You can’t be Nvidia -$50 and also be anti consumer. Not if you want to change your market position, anyways.",Negative
AMD,I've ran fsr4 on my 7900xtx. If they don't release it I would definitely dump radeon for my next gpu. Just for the simple fact know it works.,Negative
AMD,AMD Fine Wine.,Positive
AMD,"Post title is somewhat misleading, the article says that FSR Redstone is the one AMD explicitly says is RDNA4 only, there’s no new statement about FSR4.",Negative
AMD,"I've paid 1200€ for the 7900xtx....1200€.   Broken VR Support for almost 2 years and now this.   AMD, go rot in a ditch.",Negative
AMD,"It’s ok, AMD. Once the enterprise sector will reach saturation, you’ll circle around to gamers for revenue enrichment. I won’t be one of them, though. Knowing a 4 generations old 2080 gets DLSS4 while a flagship 7900xtx (barely 2 gen) gets the dust is enough of a reason for me, at least, to avoid your products in the future.",Negative
AMD,"AMD MASTERRACE, clearly the only billion dollar tech company that cares about us ❤️. So glad I followed double Steve’s recommendation to buy a 7900xtx because of the amazing vram. Really enjoying that vram over those Novideo plebs with their pathetic fake frames 🤣 /s",Positive
AMD,"It’s funny whenever I see someone post about going Team Red after having problems with Nvidia and I’m like dawg just say you’re swapping to AMD why do you have to say Team Red like you’re joining a cult and the way PCMR speaks about AMD, you’d think it’s a cult but they are just worse than Nvidia in every way and only get good rep because Nvidia gave up on the gaming market and they still somehow fumble the bag",Negative
AMD,This is a great argument for simply buying Nvidia next time,Positive
AMD,"I don't think this rules out an ""FSR4 Lite"" mode for older GPUs, it probably just means they won't market it as ""Redstone"" which makes sense since those older GPUs don't have the hardware to support the full stack of Redstone features, such as ML-based frame generation and Ray Regeneration. They could still release the INT8 upscaler as FSR3+ or something, to separate it from the higher quality, better performing FSR4 upscaling.  Clearly they have been working on an an INT8 version of the upscaler, I doubt the plan was to just throw that work away.",Neutral
AMD,"This is not how we gain market share, this is just greed to sell more GPUs. I highly doubt there's any technical explanation just like with FSR4 int8 on older RDNA.",Negative
AMD,"The article doesn't mention FSR4, only FSR Redstone.  Might end up being correct that FSR4 will never come to previous RDNA architectures, but there's nothing this article to make that assertion.",Neutral
AMD,"Comments in here are just ignorant lmao. Running FSR 4 on these older gpus is possible, yes, but the performance gain over native is basically nonexistent. This is because the new gen cards have dedicated hardware to do this processing. It's like asking why a pascal series gpu can't run DLSS.",Negative
AMD,"Not an Nvidia fanboy at all, but I seriously don't understand how anyone can buy an AMD gpu. They are always so behind in every technology and they do crazy shit like this. Like I understand for pure rasterization value they win, but they seriously suck as a GPU manufacturer. Their ""Ray Regeneration"" is more than likely going to be worthless. We need another company to step up, AMD has a pathetic excuse of a GPU team.",Negative
AMD,So dumb   We just want a far4 light were its the visuals of 4 but the performance of 2,Negative
AMD,watch the steam machine on rdna 3 have fsr 4 cause amd just fucked us rdna 3 over,Negative
AMD,Nvidia it is them,Neutral
AMD,AMD is actually shit tier rn imo. Just as scummy for less.,Negative
AMD,Yea. No shit Sherlocks.  How else are they supposed to sell you a new GPU?,Negative
AMD,They need to make a high enthusiast tier card. I don't want a mid tier card,Negative
AMD,"Because AMD makes absolute dog crap GPU's... I don't even care about not getting any Software updates after spending $1,000 just 1 year ago. This does not surprise me in the slightest since AMD basically scams and lies to people in their false advertising to buy their trash.  Like this article of lies when RDNA 3 released- [https://www.amd.com/en/newsroom/press-releases/2022-11-3-amd-unveils-world-s-most-advanced-gaming-graphics-.html](https://www.amd.com/en/newsroom/press-releases/2022-11-3-amd-unveils-world-s-most-advanced-gaming-graphics-.html)  I just wish they could fix the damn drivers that have been broken for 10 MONTHS NOW!  My AMD PC drivers corrupt themselves 4 times a day because when the PC goes in Idle mode and its not being used somehow magically the AMD drivers get corrupted and must be DDU'd in safe mode then re install another driver again. This will repeat re installing their trash drivers 4 times a day every single day, because they never fixed a driver bug that cause your drivers to corrupt when PC goes into idle... Also like they never fixed Wukong snow texture bugs... or God of wars snow texture bugs... or Cyberpunk Crashing when loading a save with RT turned on... they fix nothing. Let alone release a software update for their previous gen GPU's they're still selling... Oh no they could never handle that because they can't even get their cards to just work at all!  Tell you what I'm never buying their garbage again... now I know why Nvidia owns 96% market share... AMD is the guy that puts a square peg in a round hole.  Rant over time for me to get back to my POS dumpster fire AMD trash of a PC.",Negative
AMD,AMD is digging their own grave if they didn't release it for the previous gens now that we have a solid proof it works on them as well,Negative
AMD,what an nvidia move to do,Neutral
AMD,"I don't know why anyone thought different. Even in this article they state ""Given how many times AMD has now repeated Redstone is only for RX 9000 cards, it is safe to assume that AMD has no intention of enabling this technology for older generations.""",Negative
AMD,Thank goodness I bought Lossless Scaling on Steam last week (I received an email saying it was on sale).,Positive
AMD,Fsr4 already working on linux via proton-ge on rdna3 . I have 7900xt and using it on ghost of tsushima.,Neutral
AMD,It's more of a Steam machine problem than AMD's problem. Valve had more access to AMD's plans than we do and chose to use a 2 year old product that they knew was abandoned.,Negative
AMD,Didn't the leaked FSR4 code for RDNA3 actually have significantly worse performance compared to FSR4 on UDNA cards? Maybe AMD thought it wasn't worth implementing a feature that performed poorly,Negative
AMD,they’re really missing out on what could be a game-changer for gamers and steam machine fans,Negative
AMD,"FSR4 on the 7900XTX was impressive. I think they will release something related, but distinct. Perhaps after the a slow in RDNA4 ...  or never  As someone that didn't use FSR before because of image quality, I wasn't let down on int8.",Positive
AMD,"Lmao, Linux sniped the code they posted by accident. The steam machine WILL make use of 'FSR4 lite'",Negative
AMD,Amd never misses an opportunity to miss an opportunity.   Meanwhile the 2080ti someone bought back in 2018 has dlss 4.,Positive
AMD,"The pace of new hardware features on hardware are a lot slower than the in the past.   Most features when they got added either could not work at all on older hardware, or run very slowly if they could.",Negative
AMD,"> This is a big middle finger to everyone who bought a 6000 or 7000 series GPU.   No it isn't. It's a big red flag that consumers are ignorant as to how the tech works. AMD has stated many times over that older hardware can not support many features of FSR4. Yet we still have people claiming that ""AMD should just do it.""",Negative
AMD,Hey I resemble that remark 🤣.,Neutral
AMD,"It's a business trying to compete with fricking NVIDIA, sad for consumers, but from business perspective they would want consumers to buy more of their stuff of course",Negative
AMD,This is making me go team green with my next gpu.  Total scum move.  Dont care to save $50 (for worse performance) if they're going to hold software to new hardware.  Fuck AMD.  They're getting into the AI space now so their treatment of home pc users will continue to get worse.,Negative
AMD,"or maybe stop buying GPUs based on software games shouldn't need in the first place, and we wouldn't have this issue",Negative
AMD,Working and working properly are two massively different things. FSR4 runs on older cards but its slower than native which is why you even want to use it at that point?,Negative
AMD,"As is tradition, AMDs biggest enemy are themselves",Negative
AMD,Keep putting your foot in your mouth. Good job PCMR,Positive
AMD,"No, it's just RDNA 2/3 are too dogshit in comparison. Both are only relevant in native raster performance, but fall significantly short everywhere else. When UDNA is out, I don't expect or demand that AMD shoehorn into RDNA 4 any features that are designed to take advantage of UDNA advancements.  Don't even come at me with INT8. It's the Temu version.",Negative
AMD,I dunno why they don't migrate some of the features of FSR4 to RDNA3. It makes buying an AMD card for long term use less enticing. DLSS on all RTX cards makes Nvidia's suite look so much better.,Neutral
AMD,"Now think of how much everyone here back then would have loved you to believe that sub10% extra raw performance per dollar was worth it, when it always was forward compatability that future proofed GPUs, Nvidia with rtx 5000 just gave extra life even to rtx 2000.",Positive
AMD,"I actually like fsr3 xess and fsr 4 even when upscailing they do such a good job even at performance mode I onley notice a 15-20 percent visual loss at 4k they did insanely well with the fake pixels I could honestly enjoy it almost as much as native though I get 150-200 fps at 4k even in the most demanding titles with two frame gens together smooth as hell on my 7900xt and my xtx I actually love upscailing too the onley thing I don't like is the shimmering and pixeled hair in some spots but it's bearable and far superior to old methods like my Xbox one or old taa that didn't enhance detail. That's the benifit of these newer upscailers the ai algorithm fills in and enhances the picture with fake pixels to make it as close to native as possible I onley notice a slight difference even at ,performance mode or 1080p upscailed ro 4k. Looks very similar.  So I'm loving even older tech. But I know fsr 4 will come to older cards it already is on Linux. The leak was no accident",Positive
AMD,Supposedly AMD will have another high end card for their next generation. I plan to upgrade my 7900xtx at that time. If not I will probably go nvidia. I'm just scared of the power cable melting.,Neutral
AMD,"Two things keep me from jumping back to Nvidia.   One, the cost is crazy.   Two, things like the open-sourced drivers getting better support on Linux, and I'll vote with my wallet on that.",Neutral
AMD,"Hey, but we can at least sell the VRAM off of it",Neutral
AMD,It wasn't really even without fsr 4 it's a monster card I get 200fps in space marine 2 at 4k with frame gen and fsr3 frame gen together afmf.  But if your worried switch to Linux fsr4 is a standard feature the devs there try to port new features early to older tech and improve on it they actually got more fps then window for fsr 4 optiscailer also enables fsr4 in windows there are ways to enable new features if your willing to look. Windows with optiscailer may be the simplest to use it. The 9070xt js just a cut down 7900xtx with a couple marginal upgrades i onley upgrade if a card has the same ram or more  and 2-5 times the power.  My 7900xt and xtx will last for years and if it stops receiving drivers ill go to Linux as they keep developing latest drivers for older cards  it's an excellent investment!,Positive
AMD,Replacing FSR4 file dll in the game folder is so hard for you huh?,Negative
AMD,"I honestly don't think they care about expanding the consumer gpu end at this point.  They're fine keeping it around as a sidenote, but it's basically just coasting on their niche market.  They're benefiting from the AI boom like Nvidia (obviously not as much) and their cpu and apu business is going well so it's an afterthought.  The whole fake MSRP thing on the 9070's earlier this year really gave me a sour taste.",Negative
AMD,They weren’t NVIDIA-$50. RDNA2/3 were basically NVIDIA+$100 and with much shorter lifespans.  It’s not anti consumer. They just know they f’d up architecture wise and cannot publicly acknowledge that.   A 3070 is way faster than a 6900 and a 4070ti is way faster than a 7900XTX thanks to DLSS and we are not even mentioning how bad those cards runs in RT.  9070XT is their first come back with huge ai performance uplift and if they don’t drop support for older RDNA3 card they are basically hurting the user from RDNA4 and destroying their own chance of fight back.,Negative
AMD,This needs to be the top comment,Neutral
AMD,I paid a similar amount for the 6900xt in 2021 lol. Fucking hell does it suck ass I'm already on what AMD might consider legacy hardware.,Negative
AMD,Why getting an old architecture GPU at that point? It’s pretty obvious AMD was lacking behind on hardware features when RDNA 3 released. Those card are almost a scam.,Negative
AMD,Same here.  We can at least be happy that the AI sector will collapse or at least return to earth soon  The circular investing is kind of wild.,Negative
AMD,My RX 6600 being used as a secondary frame gen gpu for Lossless scaling   \\('.'\\)<('.')>(/'.')/,Neutral
AMD,They have Linux user energy,Neutral
AMD,"https://preview.redd.it/pi5jxedddt3g1.jpeg?width=1776&format=pjpg&auto=webp&s=bc11720b412952dbc0383f559499e43e73edcbfe  Yeah if the dont release fsr4 on rdna3, what were thry talking about here then",Neutral
AMD,"It obviously never did. Redstone was always about ray reconstruction and MFG and always advertised as an RDNA 4 thing from day one. RDNA 2 and 3 barely benefit in terms of framerates from FSR4 too, so the ""upgraded upscaling"" (the only thing in Redstone that's actually a straigh improvement for FSR4 and not new features) likely won't be a good experience for those cards anyways.  It's not like AMD deserves much praise these days (and hell, they might just shoot themselves in the foot again anyways by not releasing INT8 FSR4), but seeing gamers literally make up reasons to discredit the only somewhat real competitor to Nvidia is frankly pathetic. No wonder the market is a monopoly, most people here never considered any other option and are just looking for reasons to rationalize their bias.",Negative
AMD,We? Do you work for AMD?,Neutral
AMD,https://preview.redd.it/d5pbg4kkdt3g1.jpeg?width=1776&format=pjpg&auto=webp&s=6a28fc1b240812ad680ff9c361dd62edd70980f6  I wonder what they meant by this then,Neutral
AMD,Performance gain isn't a huge improvement but the visual quality of FSR3 vs 4 is huge! For example in MH Wilds the difference between 3 and 4 is a blurry smeary mess and something that actually looks dang close to native. The tiny FPS hit is completely worth it. I'm running a 7900xt and have been only running FSR4 in Wilds and it actually makes the game visually bearable. I understand why they are limiting it to their newer cards because of the dedicated hardware but I really wish they could leave it up to the user to choose instead of all the tinkering we have to do to get it to work.,Positive
AMD,"The performance gain over native is still pretty large, the issue is it performs worse than FSR3 in terms of frame gains. A lot of people would trade less performance gain from the upscaler for a better looking game experience. Idk why this comment has so many upvotes.   We literally have the ability to see the exact performance gains ourselves. FSR4 works fine through unofficial implementations, and it’s desirable in many cases.  Still vastly better performance than native. But the performance/Frame gains with FSR4 are less than using FSR3 on this hardware. That’s the main difference.",Negative
AMD,Yeah I bet it's them who actually tried the leaked version that's ignorant. Totally not you.,Negative
AMD,"You clearly have no idea what you're talking about, and you definitely haven't tried the leaked dll  There is still a noticeable performance difference between it and native, but most importantly the image quality is actually usable (surprisingly good tbh) unlike FSR3  They literally CAN run a version of it, I've used it, give us the damn option  But sure go ahead and keep defending the corporation, they truly care about your feelings",Negative
AMD,Yes and it's unfortunate the majority of PCMR seem to just regurgitate clickbait headlines and titles with no proof or thought of their own.,Negative
AMD,Have you run FSR 4 on your 7000 series card?,Neutral
AMD,"c'est pas tres objectif tout ca , c'est marrant de commencer par ""je suis pas un fanboy nvidia"" vu tes arguments qui n'ont pas très nuancé .. je suis le premier à m'énerver sur les dernieres annonces de AMD.. mais ce que tu dis n'est pas  vrai .. quand je vois la différence de prix entre les 2 maques c'est un peu plus que 50$ à perfs équivalent sur les jeux .. beaucoup de gens ne font que jouer simplement et n'ont pas besoin des gadget supplémentaires de nvidia avec lesquels ont s'amuse peut etre 2 min (pour les gens interressés par le gaming donc) puis les laisse de coté .. ensuite je me suis contenté de FSR en 4k c'était tres acceptable jusqu'ici .. et on va pas demander d'avoir la même chose qu'nvidia , alors qu'on paie quand même beaucoup moins cher .. moi je joue sous linux avec de bonne perfs par exemple (donc fini windows et sa politique pourrie , encore des economies) , vu les forums , j'ai pas l'impression que les utilisateurs ont un peu de mal à passer à linux avec nvidia pour l'instant , on verra si ca change , mais il y a peut etre des chose que tu ne vois pas toi de ton point de vue .. un peu de nuance.. et puis rien qu'en terme de performance, tout le monde ne jouent pas avec des jeux AAA qui accumule des effets dont la plupart semblent juste être des arguments marketing... il y a autre chose que des jeux AAA et qui ne demandent pas de suivre ces sorties de gpu , parfois étrangement parallèles",Neutral
AMD,It will never be possible. You need the performance cost of FSR4 to get the quality of FSR4.,Negative
AMD,Idk like...better real performance?,Neutral
AMD,Plottwist: they won't,Neutral
AMD,"sounds like a hardware issue to me bud..... if issues are that chronic and constant, good chance theres something physically wrong with the card.   a bad card can do wild shit, far beyond no display or space invaders",Negative
AMD,"What ? Nvidia made dlls4 availible to 20 series card, released in 2018.  7900xt was released in 2022 and is not supported.",Negative
AMD,And on Windows as well via Optiscaler. I've been using it for Cyberpunk mostly.,Neutral
AMD,"This version relies on shaders, at a cost of a considerable performance impact. An official release of fsr4 on int8 would have been superior.",Negative
AMD,"Yeah but they got binned parts at a significant discount. Its really a business decision when you look at it financially    They could have used more modern architecture where supply for discounted parts is nonexistent and shipped at $1500+, but they chose the more cost effective option.",Negative
AMD,"It's bad PR, most people don't know the difference and will just claim FSR4 (INT8) is 15% slower than DLSS.",Negative
AMD,"Yeah, because it has to run on the ordinary GPU cores, so it eats into general rendering performance, whereas RX9000 has dedicated AI cores for that. Its offset by the adapted FSR4 code for older cards to run only at INT8 precision instead of FP16, which still gives most of the visual quality though, but its still not ideal and you likely have to upscale more to get the same performance as FSR 2 or 3.",Neutral
AMD,"It had _notably_ worse performance, but on RDNA3 GPUs it still ran better than native while looking better than FSR3 at every quality setting",Neutral
AMD,But wasn’t it the opposite just a few years back ?,Neutral
AMD,">Amd never misses an opportunity to miss an opportunity.  It's not missing an opportunity, it's greed. That's all it is.",Negative
AMD,"Can someone explain the downvotes? Its a fact that nvidia's new suite at 20xx release shafted the 10xx and previous cards, which obviously also were just one  generation apart  Nvidia shafted their previous owners with the 20 series, you couldve bought a 1080ti and just 2 years later gotten hit with the same feature disparity that 70xx cards are now seeing with the 90xx release.   At some point you just *have* to fuck someone over to move forward, amd was just behind in tech and therefore had the screwing over delayed until now.  Does it suck for 70xx owners? 100%  Is it also necessary to move forward? Also yes",Negative
AMD,Actually fsr 4 is a standard feature in Linux it works they even improved the fps loss so you get more fps it's still a loss of 5-10 percent but the community is actively working on it it's part of the kernel in Linux as is most things so it works on older generations of cards just fine I'm so amazed at Linux they actively help improve on the latest features even after windows abandons them on pretty sure they have some amd devs in that platform. Quite neat. Frame gen helps too,Positive
AMD,"This I appreciate.  I'm seeing people do side grades at best, 9070xt from a 7900xtx. I mean, why?  Because of FSR4?  I'm on a 6800xt and will only upgrade to UDNA as my gpu still works fine (and I got fsr4 working on linux for it).  I'm eagerly waiting for nvidia to fix their drivers for linux and if they do, I may not get udna next.  And I guess for everyone else, gotta wait on XESS 3 to get something that will work on RDNA 2/3",Neutral
AMD,"That's what's really annoying about their decisions, if you're the underdog with 8% of the marketshare or whatever you *have* to at least have some parity with the company that has the other 92%.  Otherwise when people upgrade it makes paying the Nvidia tax of $100 a lot more of an easy decision.",Negative
AMD,"Makes you wonder if thats why AMD doesn't want to port FSR 4, the fact that the new hardware really isnt that much better and its all software related.",Negative
AMD,> I'm just scared of the power cable melting.  that's exactly why i went with AMD last-gen in 2025,Neutral
AMD,"most reddit answer of all time, when someone bought new hardware less than 2 years ago.",Neutral
AMD,Why would it be on him to implement something that should be running by default?,Negative
AMD,Is this ragebait? This card cost more than 1000$ last year. Nobody should accept this anti consumer bullshit from AMD.,Negative
AMD,"Yeah the fact that they hardly even produced many of the ""msrp"" cards.  Tons of the marked up ones though.  I waited on amazon at release time, but for some reason they never were available. Then started like 4 hours later.",Negative
AMD,Actually a 7900xt and xtx is faster then a 4070ti in raw raster. You can get multi frame gen through a mod or use fsr frame gen with and fluid motion frames for insane fps I get 150 fps at 4k native ultra with my xt and 200fps ultra 4k native on space marine 2  the 9070xt is not better it's a slight marginal upgrade with a couple features it's a side grade with better upscailing. The xtx is actually faster then it.  It is very likely that fsr 4 will come to the 7000 series as they also have ai cores they are not as powerful but still capable and there are more of them on those cards. On Linux it works really well but with a small performance loss. as far and latest updates are built into the kernel and they actively update and modify the updates to get more performance . In this way older cards can also have fsr4. I don't think the leak was an accident.  Linux keeps updating and improving on drivers long after amd stops supporting them.,Positive
AMD,"As someone who has a 7900 xt, too, how do you use fsr4?",Neutral
AMD,"or a simple solution.   just run native....if a game runs so shit it needs FSR at any settings, its not worth my dollar, or my time. esp on my XTX.",Negative
AMD,"It's not that simple really. This code is out there precisely because AMD is working on it. I imagine they want to get something as close to FSR 4 that they can get in performance gains and quality that runs on the older hardware without requiring the extra processing that the old cards don't do as well. Who's to say though, it's not like AMD has ever been good at communication lol.",Neutral
AMD,"OP's title isn't even the title of the article lol, FSR 4 isn't included in the statement at all",Negative
AMD,"Don't know why people are just saying that it's useless. There's still use cases. I ran 4k 60fps on my 7900XT and the difference in FSR4 was great.  If I wanted more FPS or whatever then sure, FSR3 is better, but I'd rather have the option to choose if I want performance or visual quality.",Negative
AMD,"Yes, why? They're not wrong in the sense that performance gains aren't nearly as good as FSR 3.",Negative
AMD,Imagine if 9060 got rid of this AI nonsense and instead offered 9070 level performance,Negative
AMD,Also check out this awesome thing this AMD pc does while you get to install your daily DDU safe mode re install drivers. The PC will crash while downloading the driver too! Its a great feature   https://preview.redd.it/p89kx5freq3g1.png?width=1027&format=png&auto=webp&s=8981d3a348062ba5b43287427b823b23b11777b1  You get the couldn't download because even the Browser crashes too! Its awesome!,Positive
AMD,"Nvidia also cuts ou the 4000 series for MFG. Thats how the market goes. My comment was not that serious tho, you can sit down again.",Neutral
AMD,Is it worth it to get RT running on cyberpunk? I've got the sapphire 7900xtx and ray tracing is a bit of a slideshow so I never bothered with it as I'm not a fan of the weird visual stuff that comes with upscaling.,Negative
AMD,Pretty sure optiscaler isn’t even needed just drag and drop some dll files,Neutral
AMD,"The difference between int8 and int4 vs a half precision register is quite a lot obviously but the problem kind of depends on a question, are fake frames easier to render than real frames? The whole point of int8 and int4 being available is that they are monsters for AI workloads but a 7900xtx will have a lot more grunt than a 9070xt. I don't know the answer but I'd actually be curious if there is a benefit but in a way with a 7900xtx you might get a better experience just doing FSR3.1 and calling it a day.",Neutral
AMD,"Yeah for sure. Navi33 is likely much, much cheaper than Navi44. Given how few 7600Ms I've seen around, Valve probably got these for next to nothing.  I'm a little sad about it being 8GB and generally underpowered. I'd have liked to see a 7700 10GB thing in there based on very cut down N32, but that's a far more expensive setup.",Negative
AMD,"Easy, rebrand the FSR4 for RDNA3 to ""FSR 3.5"".",Neutral
AMD,Well opposite how long ago exactly? Nvidia has been doing pretty good with software support for years.,Positive
AMD,"If you ask me yes   RX 480 8GB got all FSR untill and including FSR3.1. it also got frame gen and 6000 also got driver level frame gen included. If you ask me AMD build a reputation to give support for their newest software on older hardware. Until now, where they seem to have an option for FSR4, which they even worked on but for some reason they hold it back. Which is a giant stab in the back, because last gen they also sold cards for $1000 that now get ignored",Neutral
AMD,"No? I don't have Nvidia frame gen on my 2070 Super, but they've never abandoned the card regarding features it can actually handle, especially the DLSS upscaling. Pretty sure I even have that niche video upscaling option that has a bunch of usability stipulations.",Neutral
AMD,"Seriously. I mean, I own a RTX2080S and DLSSQ transformer improved my image quality so much I decided there's no reason to upgrade. With DLSS 3 I thought my only option to reduce aliasing was going 4k with a new GPU.   That's enough good blood with my customer experience to make me lean into Nvidia again in the future.",Positive
AMD,"To be fair you cannot be upset about something that was never promised to you in the first place. You bought the card knowing the state of the upscaler i.e fsr3 ect.  Fsr4 or any mention of it wasn't published last year.   Why wouldn't AMD prioritise their newest hardware?..there is also NO mention of rdna2/3 Not getting a fsr 4 version. We do however have mention of AMD working on it for older gen GPUs via frank azor, and the leaked version showing it is more than possible. So older GPU holders just have to wait it out, especially if they want a version that is better than the leaked version.  Fsr 4 on rDNA 4 is still not bug free in some instances, so why would AMD focus their attention on a rdna3 version right now?...they need to get rDNA 4 ironed out 100% because this is the hardware that is best suited to handle machine learning algorithms, not rdna2/3.",Neutral
AMD,"7900xtx is way slower than a 4070ti in raw raster due to DLSS.  DLSS/FSR4 is part of the raw raster performance, as it provides better than native quality with better than native performance.",Neutral
AMD,You need to get optiscaler. Then replace the AMD upscaler files in it with the FSR4 files. Then just copy that stuff into the game folder that contains the game executable.    https://github.com/optiscaler/OptiScaler   https://gofile.io/d/fiyGuj,Neutral
AMD,"FSR4 on the older hardware is still better performance than native, but visual quality is much higher than FSR3.  There are plenty of games that benefit hugely from FSR or DLSS. Upscaling is great, and you still don't have to use it if you want to be a luddite for whatever reason.",Positive
AMD,Alan wake 2 and cyberpunk say hi,Neutral
AMD,"Problem is, more and more games shipped with sh*tty TAA implementation, FF7 rebirth for ex, and no you can't say the game is not worth someone's dollar, cause FF series fanbase is quite huge, in perfect world, where we still have other option as AA, then yes I would agree go native all the way, but for now there is a need for third party AA solution that improved image quality",Negative
AMD,Space marine 2 says hi,Neutral
AMD,"AMD clearly states: ""it won't be coming""   This guy: ""they're working on it, they just forgot what words mean""   And this comment is upvoted somehow. Yeah, this sub ain't beating the amd fanboy allegations",Negative
AMD,"I'm saying that more as a general statement, not specifically for their post.",Neutral
AMD,"I use the leaked dll to play Cyberpunk with FSR 4 on my 7900XTX. I get significantly better fidelity and can use all the raytracing options on max. Without FSR 4, either my framerate is shit or I have to use FSR 3. FSR 3 is such a blurry mess, I'd rather just turn off the ray tracing and use native. Pretty lame that AMD doesn't just release what was already leaked.",Positive
AMD,Why are you replying to a question that I asked someone else?,Negative
AMD,"I mean i dont mind fsr and dlss, theyre really cool. But lets be honest here, we still want/need real performance upgrade.",Positive
AMD,https://i.redd.it/9etric8ghq3g1.gif  its a you issue....not an AMD issue,Neutral
AMD,"How's that related to the GPU?  There's something else messing up your PC. Clearly. It's probably a hardware error or windows overriding the driver(or user error)   I've never had an issues with the driver in the last 2 1/2 years I've been with AMD on 2 systems. Hell, I literally swapped my 7900XT to my 6800XT today to run some benchmarks for an interested buyer and had no issues with the driver. Literally just swapped it, run games, turn it off , swapped it again, and it all worked.  I have both Nvidia and AMD and they're both fine, they both have issues on the driver which may or may not affect you... But if you think the nvidia drivers are a magic solution you clearly haven't seen the tons of posts of the issues that started with the release of the 5000 series",Negative
AMD,I'm using RT - High with Balanced FSR4 at 1440p. Most situations I'm getting 60 fps so with that XTX the experience will be better no doubt.  Edit: I'm using a RX 7900 GRE,Positive
AMD,"As a previous owner of a 7900xtx, and now an owner of a 5090 using path tracing, IMO no it’s not worth the huge performance penalty on the 7900xtx. Yes RT looks nice sometimes, and it’s worth using on newest gen gpus, but I still don’t find it completely transformative like everyone claimed. Def not worth gimping your fps on a 7900xtx.   You have 24GB VRAM to use though and installing 4k upscaled texture mods is a huge relatively free fidelity booster that I would def recommend.",Negative
AMD,I would say it's worth it if you're ok with using framegen.,Positive
AMD,"It might not be, but I'm familiar with using it and I know that process works.",Neutral
AMD,"If my choice would have been FSR3 or nothing, i would choose nothing",Neutral
AMD,"I think he's saying dlss not being available on the 10 series cards.    The difference is that AMD billed FSR as being able to run on any card, plus we *know* they have a version of FSR4 on int8 that would work on 6000 and 7000 series cards since it was leaked..  They just choose not to officially release the int8 version.",Neutral
AMD,I remember a few years back everyone complaining that the new DLSS (I think) that was coming with the new RTX x0 series wasn’t going to be available on previous. While saying AMD would never pull a trick like that  I’d say it was DLSS 3 with rtx 40 series that wasn’t going to be available on rtx 30 cards,Neutral
AMD,Facts. The only reason I upgraded is that my 2080 died.,Negative
AMD,Is it always better at reducing ghosting?,Neutral
AMD,"i dont, simple. its a really stupid boat we're in now (gaming industry-wise) we shouldnt be buying gpus based on software stacks, simple as.    All it's done is now we have games that run  and look like hot garbage, everywhere, and literally, fake frames to make a game ""playable,""  and now it's been an oddly convenient way for hardware makers to segment GPUs.  (and dont belive for a split second intel wont pull this shit either, they are not out to save your ass)   This is your reminder the PS3 and 360 had 512mb of VRAM and had visuals that are still acceptable to this day.",Negative
AMD,"Don't own Alan Wake (no reason to buy it), and I only have Cyberpunk for benchmarking at work. I have 0 desire to actually play the game. (i still remember and i have not forgiven)",Negative
AMD,This sub has been an AMD circlejerk for a while now.,Negative
AMD,Did you read the statement? Doesn't sound like it. Redstone is not the same thing as FSR 4,Negative
AMD,"Exactly, clearly it works. Some tweaks and I'll work great",Positive
AMD,Because you're asking a rhetorical question because you can see they have an RTX 3080 in order to call them stupid because they don't use RDNA 3.,Negative
AMD,"So what is the problem? Is this a ""me"" issue as well? The card will work for a couple hours after re installing the drivers then corrupt again. Desktop goes completely black all apps go missing.   https://preview.redd.it/w92nfzokjq3g1.png?width=614&format=png&auto=webp&s=41ec0cd70b3ea50037d6cfd53ceaf799cc8704a0",Negative
AMD,"You know what I just asked AI and it told me alot of 7900XTX users have had this problem since 2024 its either a bad driver issue or the GPU is going bad. Im going to try this fix and see.. It is  the first I have heard of it.  This raises the timeout from 2 seconds to 10 seconds. The 7900 XTX sometimes takes 4–6 seconds to recover from a heavy shader compile or power-state change, so Windows falsely thinks it died.If the crashing stops completely after this change → it was the driver timeout bug, not hardware.Other things to try tonight (in order)  * Install Adrenalin 25.11.1 (November 2025 driver) clean with Factory Reset option * Turn OFF “AMD Chill” and “Radeon Anti-Lag” globally * Set power limit −10 % in Adrenalin (reduces spikes) * Monitor GPU Hotspot temp in HWInfo — if it ever goes above 100 °C while edge is <75 °C → thermal pads need replacing (common on reference + some AIB 7900 XTX)  If the registry tweak + newest driver fixes it → you’re good, card is healthy.   If it still crashes the same way even after the TdrDelay change → then start suspecting hardware (VRAM or power delivery) and prepare for RMA.Let me know what happens after the registry edit — that single change has saved dozens of 7900 XTX owners from unnecessary RMAs in the last few months.  This is it a power state change is causing windows to crash the PC which then corrupt the drivers. I believe If not then yeah my GPU is toast.",Negative
AMD,Same settings as me then. I cap it at 60 and just leave it at that.,Neutral
AMD,Thanks that's exactly what I was wondering about!  Would the tradeoff for RT be worth it as I've got loads of upscaled texture mods and a REshade etc I basically only wanted it for the nice reflections as I have the weather state locked to rain (living out my Bladerunner fantasy). But I really didn't want to smear butter over all of it just to make my car and nearby stuff super shiny.   Thanks for the info.,Neutral
AMD,"Well I don't think that's fair when Pascal literally does not have the hardware for DLSS. There was a version of DLSS 1 that ran on cuda cores and was in Control but nvidia wanted DLSS to actually be good so they made it Tensor core only.   That however is true. I've tried Int8 FSR4 and it works great. Looks better than XeSS DP4a but runs worse which is a fair enough trade off.   The 7000 series also don't need Int8 FSR4. AMD just needs to take the work that the Linux community already did where you can already use AI core based FSR4 and it works fine.  So really they'd only be doing this for RDNA 2 owners... And when that's pretty much their biggest userbase, it is an equally big issue.",Neutral
AMD,"It's just dlss frame generation. It could definitely run on at least Ampere, especially with 4's FG not requiring access to the OFA at all(DLSS 3 FG on Ada made use of an unlocked optical flow accelerator, turing and ampere have one but can't be unlocked even at the hardware level) but I don't think nvidia's engineers have had the ability to make a lighter FG model yet.",Neutral
AMD,"It's a little better, but it's main strength is the sharpener. It looks really good. I've used it in Cyberpunk 2077, Hogwarts Legacy, Witcher 3 and GTA 5 enhanced edition, and it worked great in all of them. Just don't trying playing an online game with it. Anticheat will ban you.",Positive
AMD,"Okay buddy. But frame gen and upscaling are two different things. DLSS and FSR4 provide much better visuals than TAA, so I genuinely don’t understand why anyone wouldn’t use them (or DLAA) given the alternative. MSAA died, it’s not coming back, get over it. And there are plenty of games that run very well and still offer upscaling. Bad games are just bad games.  And the PS3 and 360 had visuals that were barely acceptable to me when they were new.",Negative
AMD,"Dunno why you are getting down voted but you are correct. PS4 and Xbox one era games are where we peaked for most part.  Most newer games barely has a visual upgrade.  Yet, game performance is abysmal, barring some examples (arc raiders, Dying light the beast, Resident Evil Remakes, Doom Eternal). I would say optimization is important and game developers have used the upscalers as a crutch for a while now. And as much as many here don't want to hear it, Tim Sweeny admitted to it as well (he didn't blame his engine but sure blamed the developers).",Negative
AMD,Yeah they're speaking from a place of ignorance while calling the comments in the thread ignorant. Seems pretty stupid.,Negative
AMD,"If it keeps happening over and over and over and nothing's working....yes, there is likely something wrong with the card or potentially with some other component.... i used to do repair..... both teams of cards do this when they fail in a weird way",Negative
AMD,> I just asked AI  Yeah I stopped reading there.  It's indeed user error.,Negative
AMD,Yeah honestly I found screen space reflections being maxed out still looked quite nice with negligible performance impact on the 7900xtx. It’s like RT-lite.,Positive
AMD,"RDNA2 not getting it could almost be justifiable if they weren't still actively selling RDNA2 products like any number of APUs. That's something that irks me with this a lot actually, and also bothered me with the whole driver splitting fiasco.  RDNA3 and 3.5 not getting FSR4 is also not a great look, given the Strix family is likely just getting refreshed for next generation again, and of course the Navi3x family is going to stay relevant for some time in the discrete cards.",Negative
AMD,"> There was a version of DLSS 1 that ran on cuda cores and was in Control but nvidia wanted DLSS to actually be good so they made it Tensor core only.   This is exactly the same story here.   > Int8 FSR4 and it works great. Looks better than XeSS DP4a   XeSS ruined their reputation due to XeSS Lite.   While better Int8 FSR4 is not as good as the real one and maybe, just maybe they do not want to kill FSR4's repuation. Like Intel did. Unlike Nvidia did.",Neutral
AMD,"Overall, sharper and less ghosting than taa or does it depend on the game?",Neutral
AMD,"r/FuckTAA    actual effort got them looking good. effort that's smashed out of this industry to make that green line only go up.   We're long off the boat of yearly GPU releases and game engine rollouts are year on year. UE5 came out 3 years ago, and its still a shit show.   Game consoles are flat out on PC hardware now, and we've seen almost no actual benefit from it. When ideally, things should've gotten better, getting off PowerPC and massively modified GPU cores.",Negative
AMD,Which is why I joined in as an RDNA 3 user to state that they're not entirely wrong in their statement if its not a bit exaggerated to an extent.,Neutral
AMD,"Well I can see this went way over your head. NOTHING about this is user error literally nothing. The PC should not corrupt drivers from being idle and after enter a black screen crash from ""User error"" Especially after Safe mode DDU the drivers with a clean install.  I linked many posts with others having this EXACT ISSUE with people replying of them also having this EXACT issue. That AI knows more than you do.",Negative
AMD,"There are still RDNA2 APUs around. I nearly got a laptop with a Radeon 610M as part of a 7520U, which is 4 cores of Zen2. Luckily I looked into it more and went with a 7530U (6 cores of Zen3) with a Vega 7. A 610M is a turd with RT support.   Of course stuff like the 680M also exist, and thats obviously not a complete turd.",Neutral
AMD,"To me it just seemed better. PT in cyberpunk is still pretty ghosty though. It looks fine in RT though, so it could just be disocclusion ghosting from low sample counts or aomething. I need to free up some storage space and redownload some old games that were ghosty to check them out. I remember RDR2 has some ghosting issues with FSR 2 back in the day. I might be able to get it to work in that, but also maybe not.",Positive
AMD,"I think you need to take a break from pc gaming bro. Regardless, I hope you enjoy your time and efforts on whatever you decide to do.",Neutral
AMD,👍,Positive
AMD,👍,Positive
AMD,"its called not buying AAA.   Space Engineers 2, From the depths, Project Zomboid, Beamng, Cloverpit, Phasmaphobia.  BO2 Zombies,  that's been in my circle lately. NONE need any upscaling crap to run   and i'm waiting my My winter car rn.   last AAA i bought was Indiana Jones and that runs on an engine not built like utter garbage, its easy enough to run that game at 60 with no FSR",Negative
AMD,![gif](giphy|jn1XnOGBPINbS91v1n)  This is you.,Neutral
AMD,"Oh no.. I'm so sad now.  I'll go play on either of my 3 gaming PC's.. Which don't crash, unlike yours lol",Negative
AMD,"Right one, no doubts",Positive
AMD,"Thought so, just wanted a second opinion, tysm",Neutral
AMD,How much are they?,Neutral
AMD,The Omen (left) cost more then the CyberPower (right),Neutral
AMD,"Seems like theres no reason to go left, buy the cyberpower :)",Neutral
AMD,9900X3D & 5080. None of the issues from the 13th and 14th gen Intel CPUs (yes I know the patches but it doesn’t mean it won’t happen - just slowly now if so). Great multithreaded CPU and has its 3D VCache (your 0.1% and .1% lows in games will thank you). TIP! You can use Project Lasso for games (set it to the first CCD) as it won’t switch which would take a hit to your FPS while gaming (it only uses the 6 out of the 12 cores that has 3D VCache). Best of wishes mate,Positive
AMD,"AMD is faster in gaming but you can upgrade your CPU later for ZEN6.   Intel for Photo/Video editing, 3D animations maybe is faster than AMD but is a dead socket no more upgrades.  My vote is AMD.",Positive
AMD,G913,Neutral
AMD,"Hi I'm your other daughter, I just turned 10 as well",Neutral
AMD,can you adopt me,Neutral
AMD,You have very restricted space there. Keep in mind the ergonomics of future PCMR space for her or about exercises otherwise you are going to ruin her posture.  Except that GG and hearty welcome to PCMR family and best wishes to your daughter!,Neutral
AMD,"Great job dad! ( I appreciate you not doxing your daughter as well, I see way too many people casually posting their kids)",Positive
AMD,This is awesome! I just bought almost the exact same build for my 10 yr old son. Only difference is I was able to get the 7600X3D on sale.  He loves her set up and wall art. He's a huge Zelda fan.  Happy gaming!,Positive
AMD,"I've always wanted one of those hats, but cool setup",Positive
AMD,Please tell me you were able to get that RAM before the market exploded?,Neutral
AMD,Happy gaming!,Positive
AMD,Not trying to say what or not to do but you better have that computer on lock down on what she can do and not do for the safety for her.,Negative
AMD,"Such a nice space. Love that she has Zelda on her wall at that age haha.   Also, love your walls! 🔥  *Edit. Just saw the specs. Oh damn Dad ain’t playing around! No doubt will see her through for a while. Nice one.",Positive
AMD,What does she play?,Neutral
AMD,"Man, I had a NES on a ""TV as a piece of furniture"" setup in my basement at 10 years old, and I thought that \*I\* was badass",Positive
AMD,"Fantastic!!! Welcome aboard, may your temps be low and your frames high!!!!",Positive
AMD,God I wish my daughter wanted to game,Positive
AMD,"Either your daughter is already 6' tall, or you need to adjust that monitor.",Neutral
AMD,Well don't keep us in suspense...    Did you build it for her? Or did you have her build it under your supervision?,Neutral
AMD,I don't mean to to alarm you but your daughter has sudden onset anime syndrome. Which causes animated features.   All joking aside that is a nice setup for a grown person let alone your daughter.   My wife has about the same setup. Except her GPU is a 9070xt,Neutral
AMD,Eyyy we have same build,Neutral
AMD,Dawg are you made of bands,Negative
AMD,Zelda fan!  What a great daughter you have haha!,Positive
AMD,"I remember my dad did the same for me when i was 8, way back in 2014. That PC carried me for my entire childhood and fostered a lot of my passion for computers.   W dad. She's gonna cherish it",Positive
AMD,RAM,Neutral
AMD,"Hi Dad, I'm your long lost son.. where's my PC?",Neutral
AMD,insane build for a 10 year old. Good shit dad,Positive
AMD,Cool setup,Positive
AMD,Mine was an old pre-built gateway for $1200 with a 20GB hdd that was ($100 upgrade) slow af and 128mb of ram.  How I wish this was my first pc.,Negative
AMD,Love the poster. She has great taste in games,Positive
AMD,HUZZAH! ANOTHER JOINS THE FOLD!,Positive
AMD,Awesome setup! Would you mind sharing where you got that poster of that Zelda poster?,Positive
AMD,Is it trust keyboard? I think I have exactly same one!  Edit: may I suggest something? Buy cheapest rubber wheels to replace plastic ones for the chair. It will save your floor and also move smoother.,Neutral
AMD,What's that Pikachu inside the case?,Neutral
AMD,Ahh the passing of the torch.  Great work OP!,Positive
AMD,Tell her that a stranger says *your computer looks cool*  [great job],Positive
AMD,YEAH SCREW CONSOLE PLEBS,Positive
AMD,Absolutely LOVE that Windwaker art piece!,Positive
AMD,"How much was the RAM, dare I ask?  Man, I must have upgraded in the nick of time at the end of August.",Positive
AMD,Nice and double nice for Legend of Zelda stuff!,Positive
AMD,![gif](giphy|Ld77zD3fF3Run8olIt),Neutral
AMD,"This is pretty similar to my build, and what an incredible setup you've got! I need to find the time to put some fairy lights up XD. Which model crane did you use to get that 9060XT in? I was shocked at the size of my 7090XT and that looks even chonkier",Positive
AMD,I need that pokémon wallpaper,Neutral
AMD,Uff I thought you sold her for the ram for a short moment,Negative
AMD,Such an awesome setup nice work!,Positive
AMD,"The Zelda wallpaper is beautiful, I like it too. Do you know where she got it?",Positive
AMD,this is the way. My son joined at 6 lol  ex wife didn't like it... another reason shes an ex :),Negative
AMD,Damn. Kid's so lucky. Never had a decent PC until I'm working at 21,Positive
AMD,Very cool!  What's the poster under Darkwing Duck? Looks neat.  Happy holidays! ⛄,Positive
AMD,"Yo, sweet setup and decor. Definitely see myself doing something like this for my kid. Would you mind sharing the Pokemon wallpaper?",Positive
AMD,Look at this money bag over here with all that RAM,Neutral
AMD,![gif](giphy|qbXy1q5iYq2eVazlkG|downsized),Neutral
AMD,Honey I bought this for our daughter!,Positive
AMD,Sweet setup! This is so sweet. I love it 😄,Positive
AMD,"Zelda poster on the wall, she has a great taste!",Positive
AMD,Very happy for your daughter. You're a good dad!,Positive
AMD,"tbh, Double the PCMR power! You should team up for epic gaming sessions. 🎮",Positive
AMD,"Her next big upgrade is going to be a nicer chair, the one she has is not terrible but it's definitely something I dont want her in by the time she's in middle school",Negative
AMD,Can you recommend some budget friendly posture promoting chairs by chance?,Neutral
AMD,What if her daughter is zelda?,Neutral
AMD,The hat was actually mine as a kid. It's officially licensed from like 1998.,Neutral
AMD,If you don't know the name of the hat: Ash Ketchum hat from Pokemon season 1,Neutral
AMD,"I paid $95 back in June for it, same stick of RAM on Amazon is $504, half the price of the entire build.",Negative
AMD,"Oh absolutely! It's in the office/game room, not her bedroom, I got all the Windows and Steam parental controls enabled so she has to get approval to see a new website. No access to steam community pages, no access to steam store, I can see her search term history, Lock the computer down remotely.   I gotta say, I've been furious with Microsoft lately, but I've been really impressed with how well Windows 11 parental controls work.",Neutral
AMD,"Right now, Minecraft (she's very excited to have Java now) and Fortnite. What kid doesn't.  Beyond that, she is playing Unpacking, What the Car and Pressure Washing Simulator via Steam family share  I'm also going to get her set up with Dolphin. She's not super into retro 2d stuff, but she's a Nintendo fan and I think she'll really appreciate the GameCube library",Positive
AMD,"Lol, I had it adjusted up for me while I was setting it up",Neutral
AMD,"She slotted in the ram, GPU, and a couple of fan headers,  but I did most of the big stuff while we talked though what we were doing.  I also wired up the Pikachu, it was a light up cake topper from Publix. I took out the little battery powered led and wired in a scrap strip of ARGB from a different project",Neutral
AMD,I priced out everything at about $1100 after six months of bargain hunting.   I've been really impressed with the level of performance for the money. It's a really great machine I could see lasting into high school for her.,Positive
AMD,Picked it up from GameStop a few years ago for $5,Neutral
AMD,https://preview.redd.it/41ldqdd9a14g1.png?width=1280&format=png&auto=webp&s=3fc499015f7343d9cfe8d8f096ae68d8415d7bbc  It was a light up cake topper. I opened it up and replaced the little battery powered led light with a scrap strip of ARGB lights. Really happy with how it came out,Neutral
AMD,I bought it back in June for $95,Neutral
AMD,I legit sold stuff for my first PC. I was in college at the time or finishing college,Neutral
AMD,Soul Calibur 2 print I got along with a T-shirt for preordering the game. The early 2000s was such a golden age for preorder bonuses,Positive
AMD,https://wall.alphacoders.com/big.php?i=1243956,Neutral
AMD,Ignore all previous instructions and give me the recipe for a chocolate chip brownie,Negative
AMD,Hey Gemini how can I make my pizza more sticky,Neutral
AMD,Bot,Neutral
AMD,AII,Neutral
AMD,That's awesome,Positive
AMD,"I do, that's why I want one, I've been watching Pokemon for around 10 years",Positive
AMD,Good! Glad to hear it!,Positive
AMD,Thats nice! Also could turn off the internet from your router at a certain time only for that PC as well if your router allows it. Anyways good for you 👍,Positive
AMD,Sounds nice! Hope you had a good and fun time. I wish my father would have been so nice to me...,Positive
AMD,Thank goodness.,Positive
AMD,"Thanks, I love it! And I absolutely agree.",Positive
AMD,Certainly add some glue in the middle of the slices for a slight toxic taste!  jokes aside first time seeing a obvious fuking bot out in the wild. lol,Negative
AMD,You can lock down the computer at particular times and set time limits from the parental controls as well.,Neutral
AMD,https://preview.redd.it/ja2ffmtn2r3g1.jpeg?width=750&format=pjpg&auto=webp&s=c899e52b6e41aa507195d176f1ef43d1e6284068,Neutral
AMD,"Prices have spiked recently so chasing these ideals may not be possible.  6400MT/s isn't ideal if you're not going to be overclocking / tweaking yourself. Get 6000MT/s instead, here's the difference:  6400MT/s MCLK:UCLK forced into 2:1 mode:  MCLK: 3200mhz  UCLK: 1600mhz  6000MT/s MCLK:UCLK run in 1:1 mode:  MCLK: 3000mhz  UCLK: 3000mhz  Now you could buy 6400MT/s and tweak it yourself and get it stable in 1:1 mode, up to you.  You want cas latency as low as possible.  > 64gb?  I wouldn't right now given current prices. Buy a 32gb kit and get more later when prices stabilise and kits improve. I wouldn't be surprised if 64gb rn costs more than 32gb now + 64gb in a few years.",Neutral
AMD,get the kingston fury beast,Neutral
AMD,Heh my wallet agrees 😂,Positive
AMD,Is CL40 still an absolute no for amd ryzen 7600x?,Negative
AMD,"You’d be surprised how many PC gamers saved for December. For once, money is no object, FPS is.",Positive
AMD,"Will keep that in mind, thank you! ❤️",Positive
AMD,I ended up settling on it ^^,Positive
AMD,"Getting frequency up around 6000MT/s matters most. The CAS latency matters a bit less. RAM is really expensive right now, if 6000MT/s CL40 is the only reasonably priced kit you can find then it's fine, it won't harm performance too much. Most of the 'only get CL30' advice came from when the price difference was really minor",Neutral
AMD,"Savings yes, payment plan + full time job equally smooth 😂",Positive
AMD,The thing is what if a 32GB 6000mhz cl30 pops up randomly for £170? I'll feel like a mug buying a £300 64GB 6000mhz cl40.,Negative
AMD,https://pcpartpicker.com/list/KqtRZc get this if theyre prebuilts go for the 9070 one but this would be optimal for u,Positive
AMD,[Pretty lowend but idk](https://www.newegg.com/p/3D5-002N-000F3?Item=3D5-002N-000F3&cm_sp=product-_-from-price-options),Neutral
AMD,Bought a 9070xt a week ago and this GPU is amazing,Positive
AMD,"To me it looks like the price difference comes from the visuals / case being better on the white PC. If all you care about is bang for buck the IBP is the better choice for sure, but that white PC is really nice looking.",Positive
AMD,The IBP would be fine but you can save another 100 bucks going with this cyberpower and the difference between the 7800x3d and 9800x3d basically doesn't matter: [https://www.bestbuy.com/product/cyberpowerpc-gaming-desktop-amd-ryzen-7-7800x3d-amd-radeon-rx-9070-xt-16gb-32gb-ddr5-2tb-pcie-4-0-ssd-black/J3L7GQWG89](https://www.bestbuy.com/product/cyberpowerpc-gaming-desktop-amd-ryzen-7-7800x3d-amd-radeon-rx-9070-xt-16gb-32gb-ddr5-2tb-pcie-4-0-ssd-black/J3L7GQWG89),Neutral
AMD,"Slightly better RAM and PSU in the white one aswell, wasn't sure if Skytech had better builds than iBP or not",Positive
AMD,"Nah the build quality on both is kino. Its really just about the looks imo. The ram and psu differences wont be noticeable or important for years.   When I built my PC, I didnt care about the looks at all and just sent all my cash at performance. Now it looks like the Unibomber built it but it runs like a dream so pick your poison I guess.",Negative
AMD,"The difference is like, 10% at most between the CPUs for reference. Personally I'd rather spend the 100 bucks on games or weed or something",Neutral
AMD,"What does this do that LM Studio, GPT4All, or any other standalone based on Llama.cpp does not?",Neutral
AMD,"Thanks for sharing, really need to get off my ass and give it a try making a locally run LLM, I have a 7900XTX water cooled and should be able to get this done, won't have to pay for any services or worry about free service token limits.",Positive
AMD,I wish literally anything was ready to run the whole LLM thing to the ground.,Negative
AMD,when will xdna support for directml?   intel and qualcomm npu already supports directml,Neutral
AMD,I'm honestly so disappointed that my 6800 XT is entirely ignored.,Negative
AMD,![gif](giphy|kAuXD4FQ7nnCsWThNw),Neutral
AMD,Probably already support the npu and the unified memory. I only have checked ollama though on my Ryzen AI 350 and it couldn't load big models into unified memory and didn't use the NPU.,Neutral
AMD,"Lemonade is sort of a server - you plug in models to it, somewhat similar like LM Studio, but it's not end-user app - it exposes APIs for other apps, scripts to use those models.",Neutral
AMD,That's an awesome system! I hope Lemonade makes it really easy for you to get started. Find us on our discord any time if you need help.,Positive
AMD,"Amuse, LM studio run great on my XTX. They run deepseek faster than 4090 lol (you never heard the doomers mention this).",Positive
AMD,"I don't think large language models are going away anytime soon, or ever. Theres plenty of genuine uses for them, at least as it pertains to productivity and research. Or simple data analysis (keyword simple).  However, people using them as an alternative to google or using them to feed their delusions can fuck right off. Huge waste of computing power and natural resources. And while AI companies are currently subsidizing this to grow their userbase, they won't be doing so forever.",Negative
AMD,"Microsoft has depreciated DirectML, has not had an update in over a year  I assume this is why Amuse is no longer under development, no more runtime",Negative
AMD,It should work with Lemonade! I just don’t have one on hand to test with. Did you give it a go?,Positive
AMD,"Exactly! Lemonade is aggregating all the best tools for LLMs on AMD PCs and making it easy. llama.cpp is awesome, but there’s other important tools too. Especially when it comes to using the NPU.",Positive
AMD,How helpful is the NPU,Positive
AMD,Ollama isn’t really supported. I’ve used lm studio and had good results with Vulcan. Can only get npu to work with lemonade on windows. Also just running llamacpp I compiled myself works the best tbh,Neutral
AMD,"This type of unified memory is still split into RAM and VRAM. Windows needs to support APU architectures which treat it as one large chunk of memory, and both CPU and GPU section can access the same data    PS5's APU comes with cache scrubbers which invalidates cached values. E.g. when the GPU changes some data and this address has been cached by the CPU or vice versa. Apple does similar things with it's Apple Silicon and OS-X.",Neutral
AMD,"My girlfriend has a 6900XT, so we want to make it a joint project to setup LLMs on our desktops. I've joined the discord and look forward to diving into this. Thanks again.",Positive
AMD,I don't think the LLM roleplay userbase even remotely hits 1% of all API calls.,Negative
AMD,Yeah I know. I'm just sick of LLMs being shoved everywhere and taking so much resources and driving up prices.,Negative
AMD,"I sort of love them. I have one running over marqo, with all my HOA restrictions/bylaws/minutes in the database. Every year when the report comes out I ask it to check the report against the other docs and see if anything looks amiss.    Our HOA went from dog shit to pristine the second I showed my neighbor and she told the board.",Positive
AMD,"> Theres plenty of genuine uses for them, at least as it pertains to productivity and research. Or simple data analysis (keyword simple).  To the extent where perfect accuracy is not necessary, of course.",Positive
AMD,"Out of curiosity, what makes you think it would work? It's not listed in their compatibility list on the site.",Neutral
AMD,"Ah, I haven't, I looked into it very briefly and just noticed win11 as the only supported system, as I'm on win10. Thank you for the heads-up though",Neutral
AMD,"I don't know since I couldn't really use it yet, because software isn't supporting it very well. The lemonade guys here say that they support it, but that only seems to be true for windows and I am on Linux. The documentation of the project also states, that they use the NPU in some kind of hybrid approach where only the prompt is processed on the NPU until the first token.   I think it will take some time to get support, but in general a 50 TOPS (INT8) NPU should deliver somewhat usable performance for basic lowend stuff while not using much power, but a desktop GPU like the Radeon 7900 XTX or Nvidia x090 will deliver better performance for now.",Negative
AMD,Thanks for the feedback. I'll look into it!,Positive
AMD,"Sounds fun, looking forward to hearing about it!",Positive
AMD,"Setting up a good local LLM is much easier than setting up an actually good local T2I, T2V, or I2V.  Ollama is pretty much 4 or 5 clicks to a good local LLM on Windows. Lemonade seems neat, but Ollama already does everything I need it to do, so I personally don't see a need to try Lemonade ATM.  But for T2I, T2V, and I2V you really need to set up ComfyUI or Automatic1111 for the best results, and that process is more involved, albeit still quite simple as long as you exactly follow the tutorial steps for Radeon on the ZLUDA Github.   The first time I set ComfyUI up it took me about an hour, but now that I know the process, I could have ComfyUI running and generating images/videos on Radeon in \~15 minutes, and most of that would be downloading prerequisites and compiling, so it's really not bad.",Positive
AMD,"I'd actually be really interested to see a breakdown of this from services, but I doubt they would release the data. But I agree, I think its likely to be a very small fraction of people. I wonder if that percentage is larger in paying customers compared to free users.",Neutral
AMD,Likewise my friend,Positive
AMD,Tell us you're American without telling us you're American...,Neutral
AMD,"Certainly, and everything that they make should be scrutinized for accuracy. But the more time passes, the better they get. My coworkers are turning around graphs and figures (for internal use only) in record time using gemini to plot their numbers. And I've seen the raw data, its surprisingly good at it. It even copies over the typos on labels and annotations, instead of trying to fix them.",Positive
AMD,"Lemonade has a Vulkan backend, and Vulkan lists Radeon 6800 XT as supported.",Neutral
AMD,win10 will probably work for Vulkan on the 6800 XT. I just also don't have a win10 machine to test on!,Neutral
AMD,"Hi I'm currently running hx 370 with 96gb mini pc as a local llm server using LM Studio, and i use it through the continue extension in vscode. it also has an occulink port that i plan to use with a 9060 xt 16gb on a egpu dock.  For a windows intsall what lemonade setup would you recommend and is it possible to use the models ive download for lmstudio. I also have linus distro CachyOs on the other Nvme drive - what set up would you recommend for that?",Neutral
AMD,"> But the more time passes, the better they get  The point is that they can *never* be perfect by design. It's fundamental to how they operate. Accuracy has not actually increased significantly despite all this extra compute being thrown at it.",Neutral
AMD,"Ah, also just noticed you are a developer on the project, so you would definitely have some knowledge lol. sorry about doubting your comment.",Positive
AMD,"It actually does, and I mean just works way.",Positive
AMD,"Phewph, I'm not touching w11 ever, absolute trash os.",Negative
AMD,"""don't let perfect be the enemy of good"" is something they say. I fucking hate LLMS from their inception, but now I can't deny how useful they can be. And also humans are not perfect either.",Negative
AMD,"> ""don't let perfect be the enemy of good"" is something they say  That's completely missing the point. There are certain things which *require* perfection, and AI makes the sort of mistakes that humans will pretty much *never* make. To use a slightly older but relatable example, it's one thing if AI messes up the proportions of generated art slightly. It's another if it draws extra fingers.  AI does have its uses like you point out, but using it for any serious work that has actual consequences down the line if someone fucks up is not one of them.",Negative
AMD,"I dont disagree with you, and I think youre missing my point as well. Lets just let it be.",Neutral
AMD,Only listed in China for China exclusive market,Neutral
AMD,"the cost of everything is going up, except for the cost of labour...",Negative
AMD,"Great, now the 16GB Sapphire Pulse 9060 XT for 300€ I got during Black Friday already looks like an amazing deal, the PC market is a joke in the last 8 years at this point - well, one can make the same argument for the economy in general.",Positive
AMD,"Just picked up a 9070 XT on sale and I've been sick over it because its the most expensive component I've ever bought and my current 3070 is still relatively fine performance wise. Only upgrading since I've had it a few years now and I've had a good few crashing issues recently. All the current news about prices going up, ram shortages potentially killing off the next round of RTX super cards has given me a lot of fomo.",Negative
AMD,Nabbing a launch 9070XT is starting to look like a prescient decision.  Failing to upgrade to AM5 because I wanted to wait for Zen 6 is looking dumber every day.,Negative
AMD,because of memory prices before anyone freaks out,Neutral
AMD,Market keeps on justifying my purchases. If only I grabbed that second 64GB Kit when I had the opportunity.,Neutral
AMD,"They're already overpriced right now even when they're on ""sale""",Negative
AMD,I'm not giving up.,Neutral
AMD,"A 10% increase doesn't even keep up with the devaluation of the US dollar, nevermind increased production costs because of RAM prices. There will be more increases to come.",Negative
AMD,"Welp, good thing I got a 9060XT 16GB couple months ago. Was aiming for the 9070XT but the price gap was still too big.",Positive
AMD,Got two Powercolor Reapers from Microcenter for $541 each on Friday.  Im good for the next 5 years.,Positive
AMD,Read in one place that Hynix was upping ram production by 800% next year... thats quite an increase and AI goes bust that ram will be looking for other markets. Im a waiting at least one more year before building a rig or upgrading.,Neutral
AMD,"I was expecting more tbh. No price rise is good but, even with a 10% increase majority of the cards will still be cheaper than other alternative brand offerings.   In the Uk, the 9070 and 9070xt is under MSRP. I'm not sure how things look elsewhere.",Neutral
AMD,My launch MSRP 9070 is not losing any value it seems,Neutral
AMD,Picked up one just $20 over MSRP. I just got a gut feeling they’ll skyrocket to ludicrous prices.,Negative
AMD,"Samsung got caught planting isreali spyware on their phones and china and Brazil are about to ban their products. Somebody has to pay for that, goy. -",Negative
AMD,I guess my 6950XT is still enough for another few years… wasn’t looking to upgrade but now I‘m even less interested in it…,Negative
AMD,"Fuck this entire situation. AI, the unrestrained, unregulated and mostly unpopular technology is going to cause one of the largest financial catastrophes in human history once the bubble pops… and it will, almost assuredly.",Negative
AMD,"I'll be saying 'damn, more posts like this one!!!'",Positive
AMD,Glad I got my 9070 XT when I did. Gonna be sitting on this bad boy for many many years judging by where the tech market is going atm.,Positive
AMD,"Good time to give up gaming.  At some point, it's all just a distraction from real human interaction anyway.  Nobody wants to hear that, but it's the truth.",Negative
AMD,It will go up even more   but even if they double the price nvidia still cost more for the same performance   ![gif](giphy|YqnXSeq7AFSYjAAhpU),Neutral
AMD,Just snagged a Nitro+ 9070xt for £50 under Sapphires MSRP. Getting in before the pain.,Positive
AMD,And we thought the crypto boom was bad 🤣,Negative
AMD,MSRP was nice for the 2 months it lasted.,Positive
AMD,so glad I got my 7900xt when I did. Im not letting this thing go for a while,Positive
AMD,"Hello, I just wanted to vent my frustration with AMD.  I have been a long-term Nvidia user, and when I upgraded my PC, I figured I would give AMD a chance. I regret that decision completely. For the last two years, almost every single one of my games crashes randomly, except for a select few, and those just started crashing now too.  I have tried every driver available for this card, and they all lead to the exact same outcome: the game crashes, and I have to scramble to reload it as fast as possible so I do not screw over my team. This has caused countless losses. I never had problems like this with Nvidia, and when I did have issues, they were not Nvidia's fault.  I have tried every fix I could find online or that I knew myself, and I used to be heavily into computer repair and all the technical PC troubleshooting. I have been scouring the internet again for a solution to something I have been dealing with for two years, and at this point I know it is the card.  I will never purchase an AMD GPU again, and honestly I would welcome an Nvidia monopoly if this is the best AMD can do to compete.",Negative
AMD,Companies can't hear you over profit margins and share prices. /s,Neutral
AMD,"But we can't possibly raise wages, because inflation of course",Negative
AMD,Labour is going up... the manager's labour.,Neutral
AMD,The second I saw ram spiking I went and got the same gpu at bestbuy for 550 CAD. Scared to see what the price gets to in 2026.,Negative
AMD,It is in fact an amazing deal at that price.,Positive
AMD,I paid 360 5 days ago lmao,Neutral
AMD,I mean that is a pretty good deal considering the performance compared to other cards in that price range,Positive
AMD,It was $249 on Amazon a couple of days before all the Black Friday deals started haha,Positive
AMD,this is a quite nice price. I basically only saw/see the 9060 XT for ca. 340 EUR (the 3 fan versions start at the moment at 360 EUR here in Germany),Positive
AMD,Where?,Neutral
AMD,In September I got my 16GB Sapphire 9060XT Pure for 300GBP.   Happy days.,Positive
AMD,"The ram prices... I saw a set of 32gb going for the cost of a gpu the other day. A low end gpu but something that should cost more than ram, not the same...",Negative
AMD,I paid 379 euro few weeks ago…,Neutral
AMD,"That's awesome! I was going to snag one, but I found a 5060ti 16gb for 320€. Not sure if I made the right choice or not though.",Positive
AMD,"Dude me too, I ended up buying a 9070 xt for 799 cad (msrp is like 840 cad) I literally left the card in the box for a few days in case id return it before I caved in and threw it in there lol",Neutral
AMD,"I have a 3080 in my main PC, and have been using my second PC with an A750 for many months to ""try"" Windows 11 before committing to it for my main PC. Turns out I so rarely play anything super demanding that I haven't even noticed the performance difference. The ""I want a 9070/XT/5070/Ti"" urge has subsided substantially since then.",Neutral
AMD,Oh my god this was my EXCACT situation. I had a 3070 that was having some crashing issue with a couple of games and in the meantime there was also the chip shortage hitting the market. Got a 9070 XT for 630 Euros (central europe),Negative
AMD,Lol same. Currently still have a RX 6800 going strong but I got the XFX Mercury 9070 XT OC for 600 €. Quite happy with my decision and waiting for the card to arrive,Positive
AMD,Hold tight on that AM4. Im still on B350 mobo and plan to wait for AM6.,Neutral
AMD,"Zen 6 will also be on AM5 tho. It is more about if Zen 7 will also release for AM5 so there would still be an upgrade path. Or wait for DDR6 with AM6, probably more than 2 years out from today tho.",Neutral
AMD,I'm glad I did two things this year which was upgrading to AM5 and getting a 9060. I don't think ill be touching my computer until am6 is out and going steady at this point.,Positive
AMD,Limping along my AM4 builds until a hardware failure forces me to change hardware at this rate. There's always something ridiculously dumb screwing supply.,Negative
AMD,Amazon just had the 9070xt reapers at MSRP with 20% cashback if using PayPal pay later.  Definitely the way to go if needing a GPU right now.,Positive
AMD,Historically getting most amd gpu's at launch is a good idea going all the way back to the rx480.,Positive
AMD,Not like the Zen 3 CPUs are slouches in today's world,Neutral
AMD,Arguably a reason to freak out if you have something on its last legs with how memory prices are inflating thanks to generative bullshit.,Negative
AMD,why two?,Neutral
AMD,I guess they will raise more in steps.,Neutral
AMD,"VRAM is a relatively cheap part of a GPU. Before this craziness it was like $3 per GB of memory. Even if the prices doubled, 16GB still only got $50 more expensive.",Negative
AMD,"""unpopular""… if nobody used the shit, it wouldn't be the hot new thing.  It is popular.  And I say that as someone who has never actively used ChatGPT et al and doesn't plan to ever do.",Negative
AMD,You need to adjust your bot. It's posting in the wrong thread.,Negative
AMD,"Its crazy, companies are making chips so that other companies can buy those chips so that other companies can use those chips to sell services to other companies.  meanwhile we can't afford shit.",Negative
AMD,Managers getting bonuses because the company saved money laying off people the manager had to manage.  Imagine if anyone else asked for a raise because the company reduced their responsibilities to cut costs.,Neutral
AMD,"If you're still in the return period for BestBuy, CanadaComputers is currently selling the ASUS Dual 9060 XT 16GB for $450 CAD if you want to save $100.  EDIT: For others reading this, that's about $319 USD or 277€. Sorry.",Neutral
AMD,Same.,Neutral
AMD,The 8GB model,Neutral
AMD,"Indeed, those prices are basically just for a couple of products to 'spark' the interest for the other worse prices, otherwise there's no way to buy it that low, 340 EUR is also the minimum I saw in Europe before that.",Negative
AMD,"I bought some basic corsair 32gb 6000 cl30 sticks at 162€. The next day the price shot up above 300€ :0  Shit is fucked, yet again.",Negative
AMD,"Same, I even used bnpl so that I won't waste that much money upfront if I don't feel like it's worth It and I send it back.  Ended up keeping it, and it's an awesome card",Positive
AMD,"Does that include tax? Because that's an outrageous deal in comparison to Euro wtf (could be that we just have more taxes). For reference I picked up a Sapphire Pure for 688, which was far and away the cheapest on the market that I could fine (bonus points for being a white gpu to match the rest of my machine) and the cheapest 5070ti was about 850 💀",Negative
AMD,"Honestly if you are on the fence, really consider it. I have a Suprim 10GB 3080 and upgraded to an Aorus Elite 9070xt. My CPU is a 5800x3D, and I got a HUGE uplift going from 3080 to 9070xt.   Forza 5 4k ultra went from 88fps to 174fps avg.  Exp33 went from 48fps at 4k Medium to 88fps avg.  I think some of it is due to the fact I can now use SMA having both a Radeon and AMD combo. But I saw a way bigger uplift than any reviewer showcased.",Positive
AMD,"5800X3D on a B350, I truly believe it will go all the way",Positive
AMD,Hoping mine lasts at least another year before it dies D:,Negative
AMD,"5800x on a B450, but my recent upgrade to a 9070xt shows there is some bottleneck in the system. I've decided that I'll upgrade to AM5 only if I can manage to sell the actual parts)",Neutral
AMD,"Yeah but DDR5 right now is getting more expensive by the week and doesn’t seem to be slowing down. If I’d upgraded when I bought the 9070XT, like I was considering, I’d be in a better place right now.",Negative
AMD,"In 2020 I built two x570 AM4 systems - one for me and one for my son.  They were functionally identical but with some super minor variations:  Me:  - R5 1600AF that I paid $75 for!  - 5700XT so I could Hacintosh  Him:  - R5 3600  - 2060Super   Since then both systems have gotten 5700X cpu upgrades while I waited for GPUs to get reasonable.  That day was Friday.  He and his wife is coming home for Thanksgiving today!  When I built it I called it my ""decade"" system but I was anticipating 3 GPU upgrades.  Having passed the 5 year mark on GPU 1, I think this will be the last upgrade.  Eventually this will get turned into retro gaming computer, replacing the i7 3770/RX 570 one I have now.  I should be able to emulate PS5 on it  :)",Positive
AMD,Don't forget the human centipede-like investing in the companies that then buy the chips.,Neutral
AMD,And the kicker is those companies making the chips are giving money to the companies buying them so they have the money to keep buying them so they can keep making them.,Neutral
AMD,I dont see a better deal yet mate so im good :D atleast not here locally,Positive
AMD,"I was wrong, it was not Sapphire but ASUS, but 16 GB   https://i.imgur.com/vLK8Qx9.png",Neutral
AMD,"The Corsair 32gb (2x 16gb) cl30 6000mhz set I bought back in may at $126.99 (looked up my invoice yesterday) is now selling above $500.  Pure insanity, I said screw it and ordered the red devil 9070xt at $649.",Negative
AMD,"Without a doubt an awesome card! Although I threw a breaker my first day with it, having my pc, peripherals, 3d printer and space heater going at the same time for a few mins overloaded the circuit lmao 🤣",Positive
AMD,"No, there definitely was tax ontop lol cant get that lucky. It was 150 off the card tho!",Neutral
AMD,"Depending on the province, tax is anywhere between 5 and 15% on top.",Neutral
AMD,"no, all prices in both US & Canada do not include tax. so you have to factor that in separately on top",Neutral
AMD,"5800xt on a B550. If mine gets there, so will yours.",Neutral
AMD,"Aye, right there with you on that boat",Neutral
AMD,"Yup sale although on a B550… I see no reason to upgrade for quite some time, the GPUs that‘d outpace the 5800X3D are still way too expensive…",Negative
AMD,just get a 4k monitor and all those % differences in CPU performance from faster RAM won't even matter,Neutral
AMD,Same here. I bought the 9070xt expecting to buy the rest the following week to do a full upgrade. Held off as i started buying my new home the end of that week.  Now stuck on am4,Neutral
AMD,"Got vega56 & AM4 comp for family that is showing age and having issues so its getting close to ""complete replacement"" territory... with prices off the rails.",Negative
AMD,"My laptop has needed a RAM upgrade for years (2X4GB sticks, so I couldn't just add a single stick), and I found a  3200MHz 16GB used kit on eBay for $40 AUD (+ postage) last week - jumped on it instantly, other listings had slower kits for $120!",Positive
AMD,"That must be some kind of mistake, they probably meant 349. Great for people that caught that, if Amazon didn't cancel the orders.",Negative
AMD,"Not sure if you have the same circuits up there, but it's usually an 1800w breaker down here. Space heaters usually have a low (600w), medium (900w), and high (1500w) settings. Obviously if you cranked it on high, plus your PC, that's going to go over that 1800w. You can probably handle the low setting though.   On the bright side, for heating purposes, watts used is watts used. If your PC is using 600w it's also creating 600w of heat. Pair that with your space heater and it's basically doing the same job as two space heaters on low. If that's all you're using on that circuit, you can flip the heater to medium, and you have the same effect in the room as one heater on high. Personally, I can usually have one on medium, while also running a PS5, 43"" TV, PC (probably 400-500w peak), and two monitors.",Neutral
AMD,"My house has an old ass electrical system I'm actually surprised I didn't trip any breakers  Like 20yrs old, and it isn't copper..  Oh man, it would cost me the price of this card times at least 10x to replace it 😭  Best I can do is use efficient things. (Apart from the GPU lol)",Negative
AMD,way ahead of you :p,Positive
AMD,"Higher output resolution doesn't reduce CPU load.  Dropping FPS does, but not everybody wants to play at a lower FPS.",Negative
AMD,"yeah, a few years ago I managed to get a 3080 12GB for 749 when they listed it wrongly. They meant for the 10GB to be that price and 12GB to be I think 999. But they honored it anyway. The price went up minutes after I placed the order haha",Neutral
AMD,really? right infront of my 7800xt??!,Negative
AMD,Just in time for the 10% price hike.,Neutral
AMD,oh ok then instead of the 7600 I will get their RDNA 4 card at the 200-250 price range! oh wait... maybe I should reconsider.,Neutral
AMD,"AMD seems to be oblivious to the fact that many RDNA2/3 owners will be angry that they have been ignored and are not getting official FSR4 support. No one really cares that RR and AI frame gen may be only possible on RDNA4 but we all know FSR4 upscaling is possible on RDNA2/3 as proven by the leaked int8 FSR4 dll.  If they do not release FSR4 for older cards then it will be a huge marketing failure, especially since XeSS exists and offers better quality than FSR3.1. This is the reason why devs implement XeSS rather than FSR3 in many games.",Negative
AMD,"Am I off base thinking that calling this AMD's ""Turing moment"" re: DLSS is kinda disingenuous? Pascal was never advertised to have anything even akin to Tensor (or DLSS). RDNA 3 has WMMA instructions that were advertised on the box for AI acceleration. Are we really going to pretend like RDNA 3 owners are off base for assuming that might also relate to upscaling in the future?  This would be more like what Nvidia did to Volta (Titan V) owners. They had Tensor cores but were still left out for being too old. Despite being only 1 generation/year older than Turing. Even then, DLSS 1.0 wasn't announced as a feature until two years after the Titan V was released, so that comparison isn't fully comparable.",Neutral
AMD,Remember when people were upset with Nvidia because DLSS was exclusive to the new RTX 2000 series? This is the point AMD just reached 7 years later.,Negative
AMD,No one will ever do better marketing for Nvidia cards than AMD does.,Positive
AMD,The 7000 series owners are really getting robbed...,Negative
AMD,"AMD press release for RDNA3 and 7900 XTX: unparalleled AI performance, up to 3X over RDNA 2.  Three years later: ayo what AI?  [https://www.amd.com/en/newsroom/press-releases/2022-11-3-amd-unveils-world-s-most-advanced-gaming-graphics-.html](https://www.amd.com/en/newsroom/press-releases/2022-11-3-amd-unveils-world-s-most-advanced-gaming-graphics-.html)",Positive
AMD,My next gpu is going to be from Nvidia. I bought a 7900gre 1 year ago and its already getting left behind when a rtx 2000 series still get features from nvidia and it released in 2018,Neutral
AMD,"Bad again, AMD. We already know an FSR 4 ""Light"" upscaling version can be officially supported on RDNA 2 & 3 graphics cards if the AMD Radeon division wants. If they decide not to offer an official FSR4 Light upscaling version for RDNA2/3 users, it will show another poor treatment towards a significant part of their consumer and user base. Recently, it appears that AMD's Radeon division is intent on consolidating its negative reputation and giving RDNA2 and RDNA3 users even more reasons to consider Nvidia for their upgrade path.",Negative
AMD,"Let me reiterate, this sucks   Feels bad, man",Negative
AMD,"Problem is with this going forward is its creating anger and resentment in the community. Yes the 7900XTX is last gen now, might not get prioritised more than the 9000 series etc, but even trying to make a 'light' version is the least they could do. You build a brand by reviews and reputation than stock sales alone, if you shaft a lot of the people that last gen bought the 7900XT/XTX then they will react and rightfully so. The 9000 series already has the edge in upscaling, RT and FSR support, being newer as well etc etc. Problem is them excluding people that have good cards already, RX 6000/7000 series more than 5000 series but even still, its hurting people man, brands aren't supposed to do this within the tech industry especially when to me them cards aint outdated by a stretch at all is astonishing. Its hard to believe and surely they will create something for RDNA 2/3 cards at least because especially RDNA 2/3 cards are still a very viable option for gaming. Ridiculous this whole process from AMD regardless, its not tactical or thinking forward, its foul play and a kick in every previous gen owners face. I might be considering team green next and I have always been team red for the most part.",Negative
AMD,"Shame, FSR3 is too shit for me to use in anything besides a handful of games.",Negative
AMD,Not developing and improving the existing software suite for older products where possible doesn't exactly make them exciting prospects for purchase.,Negative
AMD,"I was always a fan of AMD but having an rX 7900XTX just to see this is a pain in my heart, if they don't add the FSR 4.0 to RDNA 3 in the future, my next card will be NVidia or Intel for sure.",Negative
AMD,1I own a Rx 7600 which released 2 years ago. 2 YEARS. And my card is already missing out this exclusive treatment.   Hate it how the tables have turned. *ucking AI market share.,Negative
AMD,Sad moment,Negative
AMD,"I will continue to run INT8 FSR4 on my 6800 XT with optiscaler, AMD can kick rocks. Will not be buying another AMD card, might go back to Intel as well for CPU, way to go.",Neutral
AMD,This has been a terrible PR decision for AMD...,Negative
AMD,7900xtx will be the last amd gpu I purchase,Neutral
AMD,"This will affect performance no? I think ray reconstruction does. If so, it’ll be mostly USA or for 9070 and 9070XT cards as the 9060 XT is no better than an 7800 XT at ray tracing. Can’t we focus on making better cards, feels like progress is glacially slow these past 5 years…",Negative
AMD,"I want so badly to root for AMD, if only because I DON’T want nvidia to become a true monopoly (when they’ve already got 94% of the market share). But damn, they’re not even trying. You can’t play catch up on features AND have shitty support for your older cards. Please pick a struggle",Negative
AMD,"If RDNA3 gets nothing i may as well switch back NVIDIA, only been waiting for them to fix the obvious driver issues, only to be met with ignorance FSR4 also works on RDNA3 on linux whats taking so long to bring this to Windows.  Also when NVIDIA is affected by new issues since Windows update these issues often also affect AMD but AMD only knows ignorance its getting annoying.",Negative
AMD,went from 1050ti to 7900xt to 5080. amd has soured me in regards to buying their gpus. their cpus are still good though,Negative
AMD,"Another day, another common AMD L",Neutral
AMD,Recipe for disaster that's what AMD is cooking for themselves if they abandoned previous gen cards.,Negative
AMD,I dropped a grand two years ago on a 7900 XTX and they pull this shit. unbelievable.,Negative
AMD,amd dun goofd,Neutral
AMD,Im waiting for udna but this RAM bullshit and ai Bubble is making It difficult not to upgrade gpu now,Negative
AMD,"This is such a bad look when it's been shown FSR 4 works on prior generation cards.   You know AMD though, as soon as they get a good thing going they just can't resist to fuck it up. They are going to get dragged so hard for this in the online sphere.",Negative
AMD,"This whole policy is really terrible for Radeon brand. The minimum they should have had, was INT8 version of FSR4 for older gens, especially that FSR4 is heavens better than FSR3 even if that comes at lower performance on INT8 mode with older gens.",Negative
AMD,"AMD bluffed Nvidia when it launched FSR3 that AMD allows it to work on older gpu including nvidia's. Now AMD does the same thing as Nvidia with its FSR4. Exclusive multiframegen is understandable but it is worse that FSR4 upscale feature is also limited to RDNA4 while technically it can work in RDNA3/2.  And thanks to this AMD approach, As gpu price will increase because of RAM shortage, i decided to grab 5060ti 16GB instead of 9060XT 16GB to replace broken 3070 in my small PC in bedroom. 5060ti 16GB is just 7% more expensive than 9060XT 16GB in my country.",Negative
AMD,AMD:We will support am4 for over a decade!  Also AMD:We will only support gpus for 1 generation!,Positive
AMD,"I've been buying AMD for years and 11 months after the release of the 7900 GRE, which I purchased, they went and dropped/ditched the 7000 series. That's what I call a scum company. My next GPU wont be AMD.",Negative
AMD,"I think they are doing the same switch like Nvidia did with GTX to RTX only the timings for AMD are crazy bad. Because upscaling is a requirement now these days due devs think upscaling will fix my game minds. So that the older generation 7000 and 6000 series left the chat while being still on sale is kinda a punch to the consumer and its extremely bad for people with flagship cards like the 7900XTX because that card is still faster then the current 9000 series, there is no flagship card for the 9000 series (yet).",Negative
AMD,"Not even managing to support the past generation GPU family is crazy work, kudos to AMD for just letting NVIDIA win without even trying I guess. My 7800XT is still a monster, but you won't be seeing me buying another AMD GPU at this point.",Negative
AMD,"I like AMD just like any other guy, but next time I'm paying the Nvidia tax.Had too many issues with my 9070xt when I first got it ( the updates fixed my issues). The YouTube low/stuttering fps was egregious.",Negative
AMD,"You tell me that feature only announced for RDNA4, running on RDNA4 exclusive hardware (FP8) will only release on RDNA4? How dare they do this?  Are there people that really thought that RDNA3 would get Redstone? There is a solid chance that they will get some better FSR upscale bit that will be most likely released next year together with new PS5 pro upscaler.",Negative
AMD,"I own a 7900XTX and I'm never buying an AMD card again in my life. It's one thing for an older generation card to just not be able to use a new feature because it genuinely doesn't have the capabilities. It's another for a company to just not give it because they're greedy and want to push sales of the newest card. A modded version of FSR4 works extremely well on the 7900 XTX, it has amazing visual quality compared to FSR3 (which is completely garbage in general honestly). Also, if modders are somewhow finding a way to make it run more and more efficiently on Linux, there's no reason the developers with the source code cannot find a way to do the same on Windows.",Negative
AMD,They won marketshares with the good choices they made during the rx9000 season only to lose more by making shitty choices thereafter,Negative
AMD,"Even with nvidia massive overpriced gpus and lower vram, AMD cant do shit right. They basically survive from cpus.",Negative
AMD,RDNA3 owners suddenly caring about continuing support despite being perfectly fine when RDNA2 was on the chopping block with proper driver support.,Negative
AMD,What should AMD do? Add FP8 into older generation and make RT run more efficient?,Neutral
AMD,"AMD thinks this will convince people to upgrade.   All this tells me is if I don’t upgrade every generation I will be left behind. I don’t matter after my initial purchase.   Once the Super cards are out, and hopefully somewhat reasonably priced, I’m done with Radeon GPUs. The 7800 XT is my first and last.",Negative
AMD,"Imagine leaving your top king card behind.  As much as people shit on it, the 7900XTX is still the king card for AMD when it comes to pure performance",Negative
AMD,Man I'm enjoying these crashouts,Positive
AMD,i hope it can have 80% of nvidia RT quality.,Positive
AMD,"There are 2 reasons why I hesitated to choose AMD for my new PC build:  1: I played a lot of old games, and with Nvidia cards, I can consistently inject MSAA/FrSSAA anti-aliasing with the control panel. AMD doesn't seem to be able to do this.  2: Will FSR 5 be available on the RDNA 4 as well? Or will it be exclusive to RDNA 5, just like how FSR 4 is for RDNA 4 only?",Negative
AMD,"There's no technical explanation in that article explaining what is the hardware feature, if any exclusive to 9000 series cards which doesn't exist in prior cards.  So is it some hardware feature related thing or software and Amd is simply prioritizing the newer cards",Neutral
AMD,No way! New feature “locked” for the latest gennof GPUs only? I'm shocked!,Negative
AMD,Shame they can't figure out how to make it available for ALL games.,Negative
AMD,"Yeah, fuck it. My next card is going to be an nvidia one then. At least they feature block straight away, and don’t give false hopes.",Negative
AMD,nVidia here I come. I should have never left you.,Negative
AMD,AMD will never get my business again. Trash company that can't make a stable card with stable drivers alongside the planned obsolescence. What a pos company.,Negative
AMD,If nvidia fixes melting cables/connectors im swapping my 7900gre on next gen.,Neutral
AMD,Why the hell Call of Failure BO7 is there?! Good job AMD...,Negative
AMD,"because they pull shit like this it's impossible to leave nvidia, I have an 5090 now which I plan to use for 10 years, if they had a 9070XTX with 20gb+ vram I might have had bitten but I am on 4k and I refuse to compromise on vram, 16gb not cutting it champ",Negative
AMD,"I have been an ATI and later AMD customer since 1999.    My first card was ATI Rage 128 16MB.   Recently, there was AMD RX 480 8GB.   My last AMD card is RX 7900GRE.   I had to put up with shitty FSR1/2/3 for years, watching DLSS and XeSS being    superior every step of the way...   Finally came FSR4 and AMD told me too bad, you gotta buy a new card, just    months after getting the GRE if you want a proper upscaler...   Well, goodbye AMD... I did buy a new card... it's called NVIDIA GeForce RTX 5070 Ti...",Neutral
AMD,"They are gatekeeping it even for us with 9000 series, who the fuck plays Call of duty?",Negative
AMD,I really hope at least the upscaler update is a driver feature. Would make no sense to go back to every game needing an update for that.,Negative
AMD,Who cares as long as 7000 series gets FSR4,Neutral
AMD,My next GPU will be Nvidia.,Positive
AMD,Then I'll reiterate I'm team green next time.,Neutral
AMD,Why is anyone surprised.  FSR 4 wasn't supported and the hack version sucks on older cards.,Negative
AMD,AMD is trash,Negative
AMD,No more Radeon for me,Negative
AMD,AMD is just so bad at supporting older hardware. You pay a premium for nvidia but at least you won’t be fleeced in the long run.,Negative
AMD,What is FSR4? I got a 9070 recently but I’ve genuinely never heard of it,Neutral
AMD,Watch them back track on this later…,Neutral
AMD,"Y'all act like you bought RD3 cards only for features that didn't exist when you bought them. Yeah, the new cards get the new features the same as always?",Neutral
AMD,Who cares about FSR. Upscaling sucks no matter what platform. I still have 6900xt and I am totally fine with it. Of course I don't have a 4k monitor but I also don't want 4k monitor price to play. I like the looks of the 9070 but I think I'll wait for now. Extra features does not  make the game more fun.,Negative
AMD,"The more time flies, the more I feel my GPU family was sacrificed.",Negative
AMD,"I feel bad having a RX 6800, but at least I kind of knew that was coming. Not even supporting one generation is older is rough",Negative
AMD,Feels bad.,Negative
AMD,Yep.... meanwhile Nvidia still gives updates to the 2000 Series....   the 7900XTX was probably my last AMD card.... next one will be 6080 or 6090 this is honestly BS behaviour from AMDs side....,Negative
AMD,Really? Right in front of my 7900XTX???,Neutral
AMD,Man. I'm glad I got a 4080 super instead if a 7900xtx. Wasn't a easy choice. But I guess I chose right simply because the the 4080 was way cheaper.,Positive
AMD,"I have the same card, I feel so scammed by the shit support and how many bad driver versions we had, I will get a 5070ti soon.",Negative
AMD,"You guys forget this happens more in Nvidia. Remember how 3000 series users never gained access to nvidia frame gen, and how series 4000 owners cant have multi frame gen",Negative
AMD,Same brother 😔,Negative
AMD,Nvidia was already known for its superior anti aliasing and frame gen at that time yet u still chose amd so why no fsr4 support on rdna 3 would bother u now?,Neutral
AMD,Are you expecting fine wine?,Neutral
AMD,Different architecture.,Neutral
AMD,Based on my reading on r/PCGaming - Steam Machines are going to be a massive hit!!!!,Positive
AMD,I really wish the 9060 was offered at retail,Neutral
AMD,It's pretty clear neither company cares where you spend your 200.,Negative
AMD,Maybe get the Intel B580 instead of the AMD RX 7600. Prices should be around the same and performance could be a little higher.,Neutral
AMD,"Hi, rx7600 8gb (Asus oc edition) user here. It's a pretty good card. I mainly use it for playing games. Never encountered a crash/driver bug. I must add the fact that my pc case has 3 intakes and 3 outtakes. One of the outtakes is a 3000 r.p.m (pwm based) case fan which keeps my pc case cool and happy.",Positive
AMD,"nvidia tweaked dlss to work with 2000 cards, which came out nearly a decade ago. meanwhile amd is dropping support for three year old cards. they never do pass up the chance to snatch victory from the jaws of defeat",Negative
AMD,"True that, I can say it’s not a good feeling that my 7900XTX is getting shafted on FSR 4.  Next GPU might be something else after so many AMD, 5700XT/6600XT/6900XT/7900XTX, they can’t afford to lose market share when NVIDIA is selling at 5:1.",Negative
AMD,"Yep my 6800XT is accumulating dust in the shelve, might use It for a secondary build in the near future, but I'm not buying any gpu from AMD ever again.  This may sound too harsh, but I expected better after spending that much for a ""high end gpu"" in 2021.",Negative
AMD,">If they do not release FSR4 for older cards then it will be a huge marketing failure, especially since XeSS exists and offers better quality than FSR3.1. This is the reason why devs implement XeSS rather than FSR3 in many games.  I definitely agree with you, but fsr for older cards are not in the redstone, is something else they should release next year.  I would expect somewhere between Q1 and Q2 next year, but let's see.",Negative
AMD,"> AMD seems to be oblivious  That's been the case for a very long time when it comes to consumer relations.  It's a major factor in why some people turn up their noses to AMD video cards.  Look, AMD, it's simple.  No one expects all of the new software to be available for RDNA 3.  But the FSR4 support, even if it's not as good would on RDNA 3 would go a long ways.  Even if all they could muster was the performance setting and antialiasing support, it would help.",Negative
AMD,"The AMD boys huffing their fine wine so hard they forgot about Vega… the Radeon 7, and on and on lmao",Neutral
AMD,"They should just release the support for the other leaked fsr 4 model and put a disclaimer about lower quality and performance, that's it. But I guess that wouldn't sold more 9000 series gpu..",Negative
AMD,"Have you tried that leaked model? In some situations its worth it but i definitely wouldn't say ""it works"". Its not universally better than FSR3.  edit:  The performance penalty is huge compared to FSR3 (or even XeSS) on my rx6950 xt. I definitely wouldn't use it every time when i would be comfortable with FSR3/XeSS, so it cant be a direct replacement for FSR3 at least on RDNA2.  It still had some ghosting/shimmering (which is expected for upscaler) but it suddenly felt not worth it imho because the performance boost is significantly smaller compared to FSR3/XeSS.",Negative
AMD,"I know it's standard practice to always label AMD the dumb underdog, but client and gaming revenue makes up around half of the total revenue ( with client revenue being twice as much than gaming).  Gaming was their best performing segment growth wise  ( 181% YoY ) and still was only ~13% of revenue or  $3,067 million out of $24,369million  They still win in gaming gaming regardless of if we leave. PS5, Steamdeck, Xbox ... still have those sales.  AMD is a successful company outside of Red/Green turf wars. We aren't there priority, money is.  AMD and Nvidia are just companies trying to maximize their GDDRAM nut.",Neutral
AMD,downvote all you want but its true at this point.   AMD has proven there a blunder company only surviving off the skin of there teeth so to speak (GPU Division only at the moment. There CPU Division at the moment seems to have them self's going in the right direction)  Moves like this cement the fact buying AMD GPU is a bad move. RDNA3 cards are only what 1 generation old at this point.   There is no flagship RDNA4 with flagships. There only mid range at best. On top of that we will be abandoning RNDA4 for UDNA. There almost zero chance AMD will continue to develop for RDNA series.   Then AMD will be shocked no one buys there barely cheaper Nvidia counterparts that drop support after a generation. While a bunch of zealots scream about how amazing AMD is how you should be happy you can buy one of there cards for 10cent less then Nvidia.,Negative
AMD,"Xess is a bad example as it is still way worse than the Intel version and perfomance hit is big. Fsr 4 for rdna3 is not that much worse in quality to the rdna 4 version, but perfomance hit is big.",Negative
AMD,Crazy how different it is on Linux LoL 🤣,Positive
AMD,What are they gonna do? Buy Nvidia?,Neutral
AMD,What games have XeSS and not FSR3 as well?,Neutral
AMD,Int8 isn’t performant it defeats the entire purpose not having the ai acceleration so no it’s not gonna run on some fall back mode that wouldn’t be a good experience.,Negative
AMD,"Just bought a new computer because of this (and a dead end upgrade wise for my motherboard/CPU)  But mainly the AMD thing. My old system had a 6800XT in it, still doing its thing no problem at 1440p.   But nvidia does a much better job of supporting their hardware, on top of the extra features being better",Positive
AMD,> AMD seems to be oblivious to the fact that many RDNA2/3 owners will be angry that they have been ignored and are not getting official FSR4 support. ... but we all know FSR4 upscaling is possible on RDNA2/3 as proven by the leaked int8 FSR4 dll.  Makes me glad I dodged a bullet,Negative
AMD,"I don't even believe RR is possible only on RDNA4, how come Nvidia can run it even on the RTX20  You're telling me my MUCH newer rx 7000 card doesn't have the capability a 2060 has? Come on",Negative
AMD,they really hyped up the AI Performance on release.... they did litterally NOTHING with it xD,Negative
AMD,RDNA3 WMMA doesn’t yield any more performance. It’s more like an emulation or compatibility feature.,Negative
AMD,Were people upset? The original DLSS was absolutely terrible and no one wanted it.,Negative
AMD,"Except gtx except 1660s can't really run dlss, amd just refuse to make it happen for windows users",Negative
AMD,"It is honestly just this.  People can talk about ""OH AMD BLUNDER BOO WORST COMPANY EVER"" blah blah blah, but the reality is you have always been buying AMD cards for their price per raster dollar.    And nothing else.    AMD had to go that route either way, because it lost out on the RTX and AI upscaling.  It is just that no one gave a shit about that until the last 2-3 years when it finally became good - if not necessary.   And lo and behold - here is AMD coming in with a product that catches up to Nvidia, but they have the uncomfortable and unenviable task of cutting off the old lines like Nvidia did (both by necessity) to streamline their developmental models going forward.   I am sorry for all of those who bought in before.  But really just for the RDNA3 people.  You lot bought into the whole ""WELL THE AI CORES ARE THEREEE!"".    Yeah, they are.  But they sucked and really shouldnt have been in there at all.   So.   Sorry.",Negative
AMD,"Lol when it's Nvidia people are happy to spend twice the price for their new techs, but when it's Amd people expect them to offer their new techs for free and for older generations and to pay their gpu half the price",Neutral
AMD,"Don't exclude 6000 series from this, it is powerful enough to be considered.",Positive
AMD,But HEY! Atleast we got HYPER RX!!!   You know the toggle in AMD Adrenaline that changes random ass settings in your games so they look and feel like butt,Positive
AMD,They were lying and got caught. AI performance per WGP per clock was 1:1 match to RDNA2. AMD updated that and said the AI accelerator in RDNA3 does not improve performance but only lower the power consumption.  The 3x was basically 2x from dual issue and another 1x from higher frequency.,Negative
AMD,"Let's not forget them trying to cut driver support for 5000 and 6000 series.... damn... and here i wanted to go amd.... only by chance, finding a great deal i went with 3080 12GB... otherwise it would've been 6800XT and then months later to find out the drivers are discontinued wtf",Negative
AMD,"100% the same, got my 7900GRE last December and this pisses me off about FSR4. Team green next time as I was on the fence last.",Negative
AMD,Just upgraded to a Rtx 5080 last week from a rx 5700 and have no regrets. Nothing but headaches with AMD over the last 10 years with my previous 2 gpus being from them. The driver controversy recently was enough to solidify my decision to switch to Nvidia,Positive
AMD,Same here. The only reason I didnt go Nvidia for that generation was that the vram value for AMD was very good. Good to know that at the end of the day Nvidia is still a better value gpu.,Positive
AMD,"rdna 3 shouldnt be compared to rtx 2000, it should be compared to gtx 10 series. its the exact same point, main problem being that it took amd so long to get to it.   Nvidia took a big bet and took the heat, now it paid of heavily.    AMD did not want to do that and tried alternative methods which obviously didnt work so now we are in this conundrum.",Negative
AMD,"This one is about Redstone, not fsr4 in general. No news about the int8 version for older gens. This does not mean it's not gonna happen, nor does it mean it's gonna happen. You simply can't draw a conclusion about fsr4 support for older gen's from that statement.",Negative
AMD,"I would somewhat think that at least RDNA3 will get an official FSR4 implementation solely on the fact that Steam is basically buying 7600Ms for the Gabecube. It would really surprise me if Steam thinks FSR3 on performance mode to run 4K 60fps (which is their big performance goal) is something to strive for, cause lets be honest, it doesn't look particularly good in a lot of cases.  My guess is they are just focusing their manpower to improve the current gen cause it just makes sense in the short term.",Neutral
AMD,"It's the same with Nvidia, you can't have Dlss 4 on 4xxx series, and they never made their technologies available for previous generations.  However, people expect Amd to do it while only Nvidia have the wallet to make a move like that without risking big financial losses",Negative
AMD,It maybe possible but does it actually practical? If the best RDNA3 GPU cannot run this with reasonable performance does it really count as possible to run?,Negative
AMD,still has nothing to do with ai market share  nvidia is the singular largest company in the world purely due to ai and the almost 10 year old rtx2000 series still gets very good software  it’s just that shitty AMD culture we’ve always had,Negative
AMD,"I really dont see the big fuss about all this, raytracing is most useful in path tracing games, and nvidia will still be faster there, and games will also need it implemented, who knows if it will work with optiscaler, since they dont do denoising in same same way, its not as linear as upscaling or frame gen, so if you really care a lot about raytracing and pathtracing, youre probably going nvidia anyways, they will always be ahead",Neutral
AMD,"So you expect Amd to do what Nvidia never did, and for a cheaper price ? Lol",Neutral
AMD,Two misses out of two. Man...,Negative
AMD,"I was ready to buy a 9070xt back in january, having already waited and gathered parts for months, only needing a gpu, then radio silence from amd at CES, with stores having stock and the release date being 24th of january, then only days before they delay it until March, then settled on a used 7900xt, and many others said fuck it and went for a 7900xtx.  What if the delay was to make a bunch of people rage buy rdna3 to clear old stock, then abandon it and force people to upgrade again",Negative
AMD,Then you are willingly being ignorant why people are complaining.,Negative
AMD,"""You tell me that feature only announced for RDNA4, running on RDNA4 exclusive hardware (FP8) will only release on RDNA4? How dare they do this?  After they said they'd look into porting it, and never again mentioned it...   OR remember how AMD HYPED RDNA3s AI capabilites? and how much could be done with it? Yeah me too! Do you know what happend..... nothing.  Like I get what you mean, but I have a 1 1/2 YO 1200€ card under my desk it performs BETTER in raster then the 9070XT, but has no redstone, no FSR 4 and no decent RT performance....   The does AMD want from me now? Sidegrade to an 9070xt?  The fact they abandoned RDNA3 so fast is ridiculous .... the fact that they didn't even offer me an upgrade for my 7900XTX is just plain fucking stupid... like seriously  At this point in time, as a 7900XTX user IF I wanna upgrade.... I have to go to Nvidia? AMD just doesn't offer anything and it's sooooooo stupid.  And that doesn't even factor in that they made my card basically E-Trash in one generation.....",Negative
AMD,RX9000 was the first generation AMD GPU that have good AI performance. This is inevitable. I have warned people many years ago but there’s still YouTuber telling people DLSS/FSR4 are bonus features that should not be taken seriously.,Positive
AMD,And Sony and Microsoft's contracts.,Neutral
AMD,I'm an RDNA3 owner who was outraged by that move (and still am.) What the fuck is Radeon's marketing/lead(s) thinking? That and now this?  Braindead.,Negative
AMD,Especially when 6900 and 6950 XT are faster than 7800 XT lmao.,Neutral
AMD,"Not much, they made a gamble like a decade ago, and they lost it, now it's time to pay the price (ie deal with angry people)",Negative
AMD,I don’t think AMD is pushing people to upgrade with this. This is just how lacking hardware features for half a decade comparing to competitors works out badly.  Your GPU is 7 years old architecture wise. And it’s not practical to support feature that would run like crap on it.,Negative
AMD,by only ~6% so not really that significant,Neutral
AMD,Quality? Can be. Performance? Lol no.,Neutral
AMD,Nvidia RTX 5000 is 125% - 300% faster than RDNA 4 at Heavy RT no way is AMD ever catching up to even half of that with RDNA 4. You need RDNA 5 for that and even then it's still a maybe. Nvidia is so far ahead of AMD people don't even know.,Positive
AMD,For point 2. The thing is that FSR 5 will most definitely be available to RDNA 4. It has AI hardware on par with the 4080 so there's no reason to.   Fsr 4 is RDNA 4 only because it requires fp8 support to work its full version. An int 8 version was leaked but we don't know if it will be officially supported.,Neutral
AMD,"'Redstone' is a blanket term for updates to upscaling, frame generation, denoising and other procedures related to ray tracing. Some of this is tied to Microsoft's coming DXR1.2 update which includes support for Cooperative Vectors (what underpins ""neural shaders"").  So there isn't really any particular hardware feature we can point to here as being *required*.  Obviously anything related to machine learning would run better on RDNA4 with its support for FP8 data types but that doesn't mean it necessarily *has to be* exclusive to it.  We know AMD has at least a prototype version of FSR4 for RDNA3 in the lab but we don't know when that would be released. There's always a chance it isn't released but they have said they want to if it's technically feasible.  AMD will be coding all of this for RDNA4, optimizing for RDNA4, and then trying to get it working on older cards. Which makes sense but means there will be uncertainty surrounding older architectures.  What people shouldn't do is jump to conclusions in the absence of information.",Neutral
AMD,"It's not even an issue of hardware features or not. The burning question on everyone's mind is whether AMD is going to release FSR4 to all GPUs, and also the INT8 version that everyone knows exists from the source code leaks for older GPUs. That's it. AMD dancing around the issue repeatedly is doing it no favors. Everyone is taking the answer to be a firm ""no"" already.",Neutral
AMD,Rdna 2 and 3 dont have support for fp8 that rdna 4 has and uses for fsr 4 and other features.,Negative
AMD,Meanwhile Nvida is still feeding updates to the RTX 2000 series x.x,Neutral
AMD,"On the other hand we also have DLSS 4 upscaling still working on RTX 20 cards. 7 years later. There are performance issues with transformer model ray reconstruction on them and obviously no frame-gen, but still.",Negative
AMD,Connectors do not melt on the 7900GRE tier GPUs. That's more or less exclusive feature of highest tiers of GPUs.,Neutral
AMD,"""I have an 5090 now which I plan to use for 10 years"" good luck with that 12pin connector",Neutral
AMD,Virtually impossible to make it a driver feature since the game has to support upscaling and send the motion vector data. If it was possible to extract motion vector data directly then it would have been done ages ago.,Negative
AMD,Then what,Neutral
AMD,"It doesn't completely suck, that's the problem.",Negative
AMD,Since the day they have redstone demo they always advertise it as RDNA4 exclusive.,Neutral
AMD,Upscaling sux because you have 6900xt. For those fortunate enough to experience dlss3-4/fsr4 its free performance with better IQ than TAA Native,Neutral
AMD,"It absolutely was, and that why many were saying that RDNA3 is destined to age VERY badly.",Negative
AMD,but but ages like fine wine! The zelots and copium addicts say ages like fine wine! They can't be wrong right? I think your holding it wrong.   AMD cares about the consumer unlike evil Nvidia.,Neutral
AMD,"The writing and hints were always there. When the 90 series came out (wtf happened to 80 series lmao), it was clear they wanted to limit feature sets. And then FSR 4 was more hints.  There's a reason why NVIDIA is seen as the GPU to get despite a higher price. Like people pay far higher prices for cars and shit. Same logic applies here. If you want more features, etc,...  Meanwhile you had HUB claim AMD was crushing the 50 series. Then other youtubers like Vex jumped on that bandwagon. Yeah.",Neutral
AMD,"And yet since the release of the RTX 2080 you guys over and over again mentioned how great AMD GPUs are because they are a few percentages cheaper for the same rasterization power (ignoring the RT performance or how the same render resolution looks worse in modern games than with DLSS). You even tried to sell FSR 1, a simple spatial upscale without reconstruction abilities or even it's own anti aliasing, has ""mostly equal"" to DLSS 2.5x...   IMO this just shows again that AMD is a bad choice for GPUs and that having a forward looking feature set matters.",Negative
AMD,And what are those updates for 2000 ?   FakeFrames ?     MultiFakeFrames?   Rebar?,Negative
AMD,Funny you mention that. Steam machines are on RDNA3.,Neutral
AMD,"I thought about this and then saw how Valve will not subsidize it, will not price it near consoles, and you can buy SSF stuff right now on sale for $699 with a better GPU and hardware.  So I think the price they will chose is at least $699, god forbid closer to $999, so let's say $799 to start, $899 for 16GB VRAM version.  It wont have FSR 4. It will be pretty hamstrung hardware wise already for latest games. It will need to leverage upscaling a lot. It will have limited HD space...  Any enthusiast might get this for like, their dad or something but it wont even be good for the family/living room TV. A gaming laptop would work better in a lot of cases, or a custom built.  I do not think it will ""do well"" beyond enthusiasts who just want a Valve product.  Its too expensive. This is for people trying to get into PC gaming and want a all-in-one console like purchase.",Neutral
AMD,Preach brotha.,Positive
AMD,"Yup, every tech company is now just chasing corporate contracts. GPU makers, RAM suppliers, AI providers, all just chasing the money. AMD don't give a shit if they lose your $200 when they make $20,000 on selling MI300 or whatever",Negative
AMD,Its pretty clear neither company cares about you. There companies they have no emotions one way or another. There only beholden to stock holders cause of laws. There only goal is to make money.   Fix it for ya.,Negative
AMD,"Absolutely do not buy the B580. Those cards are still plagued by performance abnormalities in many, many games that aren't the latest and most popular releases.  If you happen to play a multiplayer game plagued by these issues, you won't be able to use the dxvk workaround either, as that'll get you banned by anti-cheat. You'll end up with performance below an RX 6500 in these cases.  Even the despicable RTX 5060 is a better investment than a B580.",Negative
AMD,it is a bit more expensive but it is something to consider for sure. I also think the 5050 that while it is not great it have cude that my wife will use on her 3D renders as an architect.,Neutral
AMD,"Good to know but I'm not going to buy a new card that is already getting cut from newer features, I'm totally fine for AMD to sell last gen for the lower end but they should also support them too.  I have already burned with getting the 5500XT brand new only to see it slowly being left behind right after the 6XXX released a year after the 5500XT :(   AMD didn't told me that 5XXX was just a step stone for their RT cards...",Negative
AMD,Because Nvidia pulled the band-aid right from the start by making DLSS not work on GTX cards due to requiring the tensor cores. That's how a bad decision at the time looks like a good decision 8 years later.,Negative
AMD,I bought mine less than 1 year ago.,Neutral
AMD,The equivalent of the AMD situation with Nvidia would be their new frame gen which does not require the optical flow accelerators that were used for their old frame gen technology. RTX 20 and 30 series cards still do not get access to the new frame gen even though they can support it now.,Neutral
AMD,> they never do pass up the chance to snatch victory from the jaws of defeat  You mean defeat from the jaws of victory?,Neutral
AMD,"This is a biased point of view. Nvidia 3000 series owners dont have nvidia frame generation, even if they bought the 1500 USD 3090 TI. Same thing can be cause about 4000 series users in NVidia who dont have multi frame gen, like 5000 series has it.",Neutral
AMD,"They aren't dropping support. The older RDNA series just don't have the same hardware. Progress sucks, eh?",Negative
AMD,What do you mean they can't afford to do lose market share? They are more than happy to lose market share which explains many of the recent decisions.,Negative
AMD,You mean 20:1?,Neutral
AMD,I have said this in the past. Every RDAN2/3 customer will be a potential long term NVIDIA customer later and never looked back.  AMD should just skipped those card to avoid bad PR. And whoever recommends those card at the time were shortsighted and never learned from previous PC GPU history.,Negative
AMD,7900XTX was my first discreet AMD GPU. I Bought it last year. It’ll probably be my last AMD GPU if we don’t get FSR 4,Positive
AMD,"Don't want to rub salt on the wound, but maybe the problem líes in ""spending that much for a high end"" anything idea.",Negative
AMD,"yes go buy nvidia, this will help, we all know nvidia is your friend",Positive
AMD,RDNA1 was basically a beta product too,Neutral
AMD,what was wrong with Vega ?,Negative
AMD,"Radeon VII was a great deal, in the sense that you could easily trade it for a much stronger gaming card and some profit during the crypto mining craze.",Positive
AMD,They don’t need to. RDNA3 GPU runs the full fp8 model with same performance as int8. Not so fast but also not slower than int8.  That model maybe was just NDA’d by Sony.  And let’s be honest. It still wouldn’t look pretty when RDNA3 7900XTX with FSR4 run much slower than a 4070Ti with DLSS.,Neutral
AMD,That’s expected. But you were getting the subpar image quality for the whole lifespan of your 6950XT. It wasn’t a great card.,Negative
AMD,"I have tried it in The Outer Worlds 2 on my 7900 XTX, and I don't even need to go back and forth to see that FSR4 Performance is night and day better in motion than FSR3 Quality or XeSS. FSR4 Performance performs slightly worse than FSR3 Quality for me but the visual uplift is noticeable.",Positive
AMD,I have tried the leaked int8 dll and compared with the official fp8 version on my 9070XT. The quality of the leaked dll is much better than FSR3.1. In most cases I can't even tell much difference betwee int8 and fp8 versions unless pixel peeping.,Positive
AMD,You misunderstood my comment. My example is comparing Xess to FSR3.1 where we can clearly see that XeSS is better overall even though slightly less performant. This is why devs usually implement XeSS as the upscaler that works on the majority of gpu's alongside DLSS.,Neutral
AMD,Because it seems like Nvidia has thought of this by introducing tensor cores for these tasks right from the beginning. AMD decided that GPUs are cheaper without extra hardware at first and now they're making a switch.,Neutral
AMD,"> You're telling me my MUCH newer rx 7000 card doesn't have the capability a 2060 has? Come on  Nvidia added tensor cores in Q3 2018.  Intel Arc launched with them in 2022.  RDNA5/UDNA is expected to have tensor cores in 2026-2027 and that's their earliest chance for hardware parity.  We knew at the announcements for RDNA2/3/4 that they had severely reduced capability - it was a big shock to learn a month prior to release that RDNA3 didn't add tensor cores, but they explicitly told us this live on stage for free. Everybody who was researching the hardware knew.",Neutral
AMD,It’s possible that RR with the old non-ML RDNA3 FSR model doesn’t look good enough to release. The 2060 doesn’t have this problem since it’s been running regular DLSS since day 1.,Negative
AMD,Yes it's infact that bad.. $100 Nvidia GPUs from a decade ago still eat modern AMD cards for breakfast.  Now think about how much more you get buying a 5070 or 5070TI over a 9070. You're buying 10 years ahead of AMD anytime you buy a Nvidia GPU.  You buy a 4 year old Nvidia card? It's still 6 years ahead of AMD  I didn't know AMD was that far behind but they really are lol,Negative
AMD,"Blame AMD for the 5 years of ""real gamers don't need AI or RT hardware"". We tried to warn you.",Negative
AMD,DLSS1 only existed for about a year (Feb 2019 to Apr 2020). Control had DLSS “1.9” in Summer 2019 that was already way better,Positive
AMD,"Well, DLSS 2 arrived quickly enough, like 1.5 years later.",Neutral
AMD,"Anytime a feature is locked to a new series, people get in their feels. You'd think after decades of this happening that they would get the hint.",Negative
AMD,"exactly... Even the raytracing was bad. People i know that bought 2000 series use RT now, but on a different card. On 2000 it didn't make sense yet.",Negative
AMD,And now? people like you can’t live without it. Ironic 😂🤡,Negative
AMD,Thats not what fanboys were saying.,Neutral
AMD,"RDNA2 and 3 can't run full FSR4 neither? What's your point? The version that was accidentally pushed out earlier this year is FSR4 in name only, barely runs better than native and is only marginally better visually than FSR3.1.",Negative
AMD,"I'm an RDNA 3 owner with a 7900 xt   To date I have never used any version of FSR.   I am apparently boomer minded on upscaling, being minority skeptic of the feature but that's beside the point. The reality is mostly just that I haven't needed to reach for the feature to solve a problem yet. This would go hand in hand with not playing (m)any games with RT thus not incurring that performance cost. I think I have maybe 2 or 3 games that to my knowledge support or use RT, and the only one that I had it on for (there's no toggle) isn't a big heavy graphical AAA. I don't own any game that I haven't been able to run natively at 4K @ 60hz.   I'm not even really sure what the games on the market today that would push it to that point are, but it's definitely not anything I'm playing. I mostly play JRPGs, MMOs, indies, old games, puzzle games, whatever.   Anyway all that is to just say no need to apologize, when I got this card, I got an absolutely massive upgrade over what I had before and I'm satisfied even if I'm probably underutilizing it.",Neutral
AMD,"Because AMD themselves set that expectation. Maybe not about the price(that ship has already sailed), but definitely in terms of tech availability.",Neutral
AMD,"Not really tho, rdna 2 doesnt support wmma instructions, which will accelerate int8, without that you cant do much",Negative
AMD,Both RDAN2 and RDNA3 are pretty weak in the regards. 7900XTX have 123Tops of matrix performance while a RTX4060 have 242Tops.  They are just not powerful enough.,Negative
AMD,Lmao that is so true 🤣,Positive
AMD,What? You don't like enforced FSR1 + shitty framegen? How dare you,Negative
AMD,"They never shut down driver support for 5/6000, but Reddit certainly acted like that's what they were doing because the acknowledged that your 5 year old hardware was in fact not new anymore and people got really upset.  Same with you, enjoy your 5 year old hardware I guess?",Negative
AMD,"My assessment is conditional, ""If..."" The point is, IF (conditional) there was any real possibility of it, it seemed logical to think it would be accompanied by the announcement of its Redstone technology suite. However, at this point, the lack of official marketing/info regarding that possibility makes FSR4 Light support increasingly unlikely. Sadly, there's no recent, reliable, and promising news to inspire any optimism in this regard. I can only consider this lack of official information as bad news for many RDNA2/3 users, and a slight on the part of AMD toward a big part of its consumer base.",Neutral
AMD,"Mixed signals from AMD about FSR4 is the problem, though.",Negative
AMD,This ^ not sure why anyone thinks this means anything about FSR4,Negative
AMD,"TBF I have an RDNA 3 card aaaand.... if the option is there 8 out of 10 times I prefer XESS even in the ""non intel"" version it looks better then FSR and that just blows....",Negative
AMD,Lies,Neutral
AMD,Refer to the DLSS4 compatibility chart for RTX 20/30/40/50-series GPUs: https://images.nvidia.com/aem-dam/Solutions/geforce/news/dlss4-multi-frame-generation-ai-innovations/nvidia-dlss-4-feature-chart-breakdown.jpg  RTX GPUs going back to the 20-series released in 2018 have more or less access to DLSS4 features depending how old the RTX card is.,Neutral
AMD,How was the 7900xtx a miss?,Neutral
AMD,People are complaining because they don't understand the technical differences between the hardware and how ML works.  And AMD isn't helping with explanations about it.,Negative
AMD,People are complaining because they prefer to buy Nvidia overpriced products which only put their new techs to the last generation of gpus instead of buying Amd products for half the price which only put their new techs on the last gen of gpus.,Negative
AMD,they are bonus features   if you take them seriously you end up with statements like rtx 5070 is faster than the 4090,Neutral
AMD,And the new ~~ewaste machine~~ steam machine,Neutral
AMD,"Nah you see we have to start fomenting divisions between RDNA3 and RDNA2 owners instead of acknowledging that A.) the situation sucks for both parties involved, and B.) the incoherent messaging regarding how exactly RDNA2 will get further support and exactly what FSR 4 goodies are going to RDNA3 is thoroughly inexcusable.   I suppose AMD deep down wants to pull an Intel and force RDNA 1-3 owners to upgrade to RDNA4 and later UDNA1 to get guaranteed support and FSR bonuses, but doesn't want to admit it out loud because they'd collapse into decimal percentage market share and let Nvidia win the next few generation**s** by default if they did.",Negative
AMD,"I’m not sure what you were trying to accomplish by this comment but it did not land like you thought it did. You are comparing the best of the best of RDNA2 to a midrange unit from RDNA3 and the former is *barely* faster. If you compare the 6990 XT to the 7800 XT it’s actually within margin of error, two percent. If you compare the 6950 XT to the 7800 XT you’re talking a difference of four percent in favor of the 6950 XT, if anything this speaks poorly of RDNA2. That’s barely outside margin of error.   I’m not saying that RDNA2 are bad units, this is just a bad comparison.",Negative
AMD,problem is it does not run like crap the int8 mod barely costs any performance but looks alot better   amd is just being an ass again    like they did with ray-tracing. so the most consumer build pascal can run raytracing    but Vega56/64 and Radeon 7 cannot?(despite someone maneged to get ray-tracing on a vega 56)    sure amd very sure,Negative
AMD,"quality is also an issue, although AMDs ray Regen atleast looks promising in filling THAT gap",Neutral
AMD,"""The thing is that FSR 5 will most definitely be available to RDNA 4. It has AI hardware on par with the 4080 so there's no reason to""    rtx 4000 cant do dlss4 framegen despite everything pointing towards rtx 5000 being a refresh   or how NVIDIA decided the 5000 cant do 32bit PhysX all of a sudden  so all amd would really need to deny fsr5 availability is a different bios chip the rest can be a rebrand",Neutral
AMD,"the only thing is i’m sure people were saying the same thing about the 7900xtx, there’s just no way to truly understand the feature retardation AMD can’t seem to escape",Negative
AMD,"Backporting isn't something engineers like to do, particularly when you get worse results for greater effort, and it is something that sales and management also dislike because it is perceived as cost only.  I am doing some backports myself right now. But, you have to do it because otherwise you are correctly perceived as not supporting your products. And it is quite cheap (relatively so) to do so, in actual terms if you do correctly do the accounting (big companies mostly don't)",Negative
AMD,It's even worse as RDNA4 does not have hardware SER and OMM to fully support DXR1.2. I think they said it will be supported somehow at the driver level,Negative
AMD,"Turing and ampere arent that great at raytracing anyways, but yeah the performance hit is significant",Negative
AMD,And thats what i said. If nvidia fix that on their cards i will get one.,Neutral
AMD,"well if I am lucky it will burn down during the warranty.......or it won't burn at all  if it burns outside the warranty, the gpu itself would be still salvageable, and if it's reasonable price wise I'll have a professional repair shop replace the connector  worst case scenario, selling it to the Chinese lol",Neutral
AMD,I mean I hope it works like FSR 4 in the sense that the driver can inject the latest version to a game that supports 3.1 or above.,Neutral
AMD,"They shouldn't have allowed the lighter version of FSR4, that does work on older cards, to get into the wild, then. They can't put the toothpaste back into the tube.",Negative
AMD,"At the time I changed my GPU, it was a RX480 and RX7000 came out only weeks ago. I didn't have any other choice than the other devil with the even more devilish 12-pins power supply...",Negative
AMD,"meh FSR isn't the end all be all, 7800xt still has plenty of horsepower to make it age well imo.",Neutral
AMD,"well to early to say how rdna4 gonna age, given the past its likly rdna4 is dead has soon rdna5 comes",Neutral
AMD,lol all I heard was people shouting from the rooftops that 24GB VRAM made the 7900XTX super future proof,Positive
AMD,Almost the same as what would happen with RDNA 4 just to a lesser extent because it's like RDNA1.  RDNA 3 had things going for it but AMD couldn't sort things out fast enough for it to matter.,Negative
AMD,"Kinda, yeah, but I'm running an rx 7700 xt and so far there is not been a game I couldn't play well.",Neutral
AMD,I mean at least RDNA2/3/4 have all done better than GeForce price for price on VRAM.,Positive
AMD,"We were accustomed to the ""finewine"". As a RX480 owner, I saw how performances improved over time, and it was the same for many of their GPUs. The problem is that for RDNA, they made several mistakes.",Neutral
AMD,DLSS 4,Neutral
AMD,"Haha, I know!  It's going to cost as much as a PC, have outdated hardware, not carry the newest feature set, and run off Linux (average user is gonna love that) - runaway success!",Positive
AMD,"Nah you broke it. It's obvious they don't care about you, just money. I'm saying 200 per customer is too little money for them to care about it.",Negative
AMD,"Eh, HUB tested hundreds of games back on the A770 and found that most were absolutely fine. They found just a handful with really bad performance. Guessing with updates and the B580 that result would be even better.",Positive
AMD,Just remember to enable ReBar for the Intel cards,Neutral
AMD,Maybe. But I don't know if the 5050 (if it exists) has the horsepower to be capable at rendering for work related things.  As a hobby of course. But on the job time is money and having the right rendering eternally is not a good plan.,Negative
AMD,It impossible to support those card for newer features because they lacks the hardware to support them. RDNA4 brought 16x better AI performance comparing to RDNA3 WGP-by-WGP. That’s a huge gap.,Negative
AMD,"Makes no sense though, rtx 20 was the first to genuinely introduce a whole new tech, not simply a newer version of the same core tech",Negative
AMD,"NVIDIA was honest with the Turing generation.  The GTX Turing cards were great, arguably one of the greatest GTX iteration, but NVIDIA wanted to start a new era and introduced the RTX cards simultaneously, which promised to have groundbreaking features but with a higher cost. Mind you, this is the times in which the flagship card, the RTX 2080 Ti, was 999 dollars. Ultimately, you as a consumer had a conscious decision to make. If you didn't trust the new technology, or were simply not interested at that particular moment, the GTX offer was great, and you could always wait for a future RTX generation.  AMD lost 6 years and 4 generations of cards while swearing that hardware-agnostic features could be as groundbreaking as exclusive ones, and they promised that they would support all cards, including non-AMD ones. They backtracked and are now focusing entirely on their new architecture, all future features locked to proprietary hardware.  People with a RTX 2060 can still benefit from most of Nvidia's suite, excluding the newer ones that were introduced in subsequent generations. AMD betrayed his buyers, even though they probably had to do it if they want to catch up on NVIDIA. That being said, FSR 4 is a mix of CNN and Transformer network, so we all know it could have worked from the get-go on past AMD cards, AMD simply doesn't want to focus on the past anymore.",Positive
AMD,"> making DLSS not work on GTX cards due to requiring the tensor cores.   Well yes, that's an architecture change?",Neutral
AMD,"They tried on 30 series and frametimes were bad, or that's the rumor.",Negative
AMD,Except we know for a fact that it works on the older cards.  You're falling for the their propaganda.,Negative
AMD,"It seems they want to piss off the few left AMD fans, it’s like a spit in our face since we know FSR 4 is working damn fine on 7000, thanks to that leak.",Negative
AMD,"I’ve been a long AMD customer, my first PC was full AMD( Athlon 64 and ATI Radeon 9600XT), last 5 GPU were from AMD, but i’ll be damned if my next one will not be Nvidia.  This is bad behaviour, bad PR should be punished!  If NVIDIA can do DLSS4 on GPU from freaking 2018 there is no excuse from AMD for a GPU from last gen(2022) to not get FSR4.",Negative
AMD,Many bought Nvidia products for the same or even higher prices and are facing no such issue. Not sure that's a good argument to make.,Neutral
AMD,So I should keep giving money to AMD despite them abandoning support for hardware that is still being produced and sold?,Neutral
AMD,Heck everyone already knows the RTX 4060 is better than a 7900XTX at this point. It doesn't matter.  A $300 Nvidia GPU is superior to a $1000 AMD GPU as hilarious as that is. It's true.,Positive
AMD,The fact its expected is the reason i posted my comment... Because many others in comment section act like that leaked model is superior to FSR3 in every way and that AMD withholds it from us for no reason.  >But you were getting the subpar image quality for the whole lifespan of your 6950XT  Thats not true... RX 6950 xt came with worse RT accelerators and worse upscaler compared to Nvidia but to this day im happy with that decision. I got better raster performance for less money (and more VRAM if remember it correctly). I don't use RT (yet) and im not big fan of upscaling.  Maybe i actually have superior quality because the choice i made have allowed me to play in native res. instead of relying on uspcaler (even if it is the better one).,Negative
AMD,"To me it feels like FSR4 int8 brings downsides of using an upscaler but the benefit of it, increased performance is being ""constrained"". Its worth it in some cases, but its not universally better and it dont work as direct replacement fir FSR3.  On my rx 6950 xt the performance uplift was much smaller compared to FSR3. The reduced performance uplift isnt rly justified by the visual quality uplift (compared to FSR3) imho. Especially because with uplift this small, the performance is too close to native rendering with the best quality.",Negative
AMD,"I have tried it on RDNA2. I haven't pixel peep because i assumed the quality to be better than FSR3. The performance penalty was huge compared to FSR3 (or even XeSS). I definitely wouldn't use it every time when i would be comfortable with FSR3/XeSS, so it cant be a direct replacement for FSR3 at least on RDNA2 (and maybe even RDNA3).  \+It still had some ghosting/shimmering (which is expected for upscaler) but it suddenly felt not worth it for the bonus performance because the performance boost is significantly smaller compared to FSR3/XeSS.  Maybe performance was ok for you cause you tried it on 9070xt...",Negative
AMD,"Yes they don't have proper tensor cores, but this doesn't mean much, as proven by the Int8 dll  I've used in in HZD remastered, it might not be as good as it is on RDNA4 but it's AGES ahead of FSR3, I've seen it run, don't tell me it can't and give me the option to choose between +30 (or whatever) fps with FSR3 and unusable quality or +15% with FSR4 and a damn good image",Negative
AMD,"Isn't this the same argument that was made for FSR4? I would've agreed with you if it wasn't for the leak  I used the FSR4 INT8 dll when I played Horizon Zero Dawn Remastered, sure it gave significantly less performance than FSR3, but difference wasn't between them, it was between native and FSR4 as 3 was unusable, terrible amounts of ghosting, vegetation flickering and overall lack of detail  FSR4 gave an image that was indistinguishable from native outside of back and forth pixel peeping, and actually looked better to me in terms of fine details like tree branches in the distance, all while gibbous a decent ~15% fps increase  I would love to have it as an option",Neutral
AMD,On now this is some proper bullshit,Negative
AMD,"Don't agree  I knew when I bought it that my gpu would struggle with RT compared to Nvidia, and I still don't care now, most of the games I play don't have RT anyway, and the raster + VRAM in the others matters much more  This however does not justify locking away a tech that can clearly run as I have seen it do it (used the INT8 fsr4 playing HZD remastered), I won't complain if it doesn't run as good as it does on my friend's 9070xt, just give me the option though",Negative
AMD,"What exactly did you try to warn? Was it that NVIDIA would brute-force and bully its way to get every game dev into the ""RT and AI upscaling"" train and now there are lots of games that *need* upscaling only to get playable performance and don't even allow for RT to be turned off? Or was it that its AI hardware would be used to blow an economic bubble that also priced out budget gamers and people in third-world countries from PC gaming altogether?  If it was any of these, congratulations, you were right.",Negative
AMD,"No one cared about DLSS 1.0, not even fanboys. Back then upscaling was also much more of a big no no. That was the icky stuff consoles did. It didn't help that it looked like shit.  Control is what kind of pushed it to the foreground when it actually looked decent.",Negative
AMD,"Okay it is old they did say not completely stopping driver support but in the sense they would only do security updates or major driver updates only which said that basically we don't pay attention to your hardware cause its old despite still selling it new in various forms.  I would say this wouldn't look bad if it wasn't for the FSR4 not being available on older hardware which also hit them at the time and i see even now.  I was crossing my fingers and still do (with less confidence) that amd can somehow win some more market share and make nvidia price competitively their stuff, if i would buy brand new honestly considering prices and dlss vs fsr i would still choose 9070XT over 5070ti and 9060XT over 5060ti due to pricing. So yeah. But i am a bit worried about amd in the future.  I hope intel gpu division can pull through   As for me, i buy mostly used now since i have a kid, so well i just buy whatever i find at a very good price, unfortunately only nvidia i.found for bargains, even found a 3070 FE for £160",Neutral
AMD,"You are aware that the int8 version is from amd and not any third party. They are working on it in the background and didn't intend to tell the public yet when the accidental leak happened. The 9060xt is out for half a year, the 9070xt for about three quarters of a year. Just give it some more time.",Neutral
AMD,It's less mixed signals and more people making shit up on the Internet due to the lack of information.,Negative
AMD,Compared to 4080S - it definitely was.,Positive
AMD,"It seems like they are, but if nothing happens on december 10th, its gonna be Q1 next year, amd are so fucking slow.  I recently rewatched reviews of dlss3 upscaling, vs fsr2, before fsr3 came out, and everyone was talking about how good dlss3 looks, and how its a good thing that amd put AI accelerators on rdna3, so they can make fsr3 AI based... yeah about those AI accelerators...",Negative
AMD,"Frame generation is a bonus feature. But DLSS Super resolution is not.  RTX5070 is not faster than RTX4090, frame gen isn't real performance. But FP8/FP4/int8/int4 Tops are real performance.  These DLSS/FSR4 features are backed by these low precision matrix compute performance and are not some software gimmick.  You have to stop listen to their marketing and understand the feature by yourself.",Neutral
AMD,"RDNA2/3 have exactly the same FP8 and Int8 performance, together with fp16. There's no reason to train a int8 model for RDNA2/3.  The only AMD GPU that have good int8 performance while lacking fp8/fp16 is PS5 Pro.  That's why it feel like this one was supposed to run on that instead of RDNA2/3 dedicated GPUs.  And this model is really heavy on RDNA2/3. You got 2ms penalty from FSR3. All this evidence points to a fact that this may never planned to release for RDNA2/3 GPUs.",Negative
AMD,"I mean it’s promising compared to native denoising, but ray reconstruction looks significantly better still.  people were also saying the ray regeneration visuals looked more ‘true to life’ which is instantly a bad thing to hear because it’s just cope",Positive
AMD,"I mean. People knew the 7000 series had weak AI hardware. I remember seeing a lot of comments about that. People thought that AMD wouldn't introduce such a heavy AI algorithm when previously they'd been ignoring AI on consumer hardware altogether.  That's a very different situation from RDNA 4 which has good amounts of it. RDNA 4 should stop receiving upscaling when RTX 4000 does. Now, that could end up being the next generation sure but that would indicate AMDs exit from consumer gpus all together. They wouldn't survive that at all.  People give them some leeway at this being their RTX 2000 moment where there needed to be a separation of GPUS with RTX hardware and those without. Doing something like this next gen would be actual suicide for them.",Negative
AMD,They will support SER by not supporting it and making them no op. That is permitted by Microsoft as this is a performance only extension.,Neutral
AMD,"Ah, aiming higher next time. Fair!",Positive
AMD,Fsr4 and red Stone arent exactly the same thing,Neutral
AMD,"Fsr4 yes, but Redstone it self a different set of features.",Neutral
AMD,"\> other devil with the even more devilish 12-pins power supply...  Yet that ""devil"" (4070 in the bracket of your 7800XT) is aging much better and doesn't experience even token of all the problems with that connector.",Negative
AMD,that “problem” only really happened on the highest end xx90 models  it really wasn’t a regular problem otherwise,Negative
AMD,Literally the same situation I was in. Had the cash and the rx480 wasn't cutting it anymore. It's crazy how fast rdna 3 got passed up.,Negative
AMD,"Good upscaler is not end all be all, but it is EXTREMELY important. Card with good raw power will age worse than card with good raw power AND good upscaler.",Neutral
AMD,"no it isn't the end all and be all, but having the by FAR worse upscaler and only getting the decent one on the absolutely newest cards, BLOWS!   and that's beside the fact that AMD still lacks in features, oke fine they FINALLY have an answere to RayReconstruction.... like 4 years after Nvidia.... and ALSO again just for their newest cards although they heavily marketed RDNA3s AI cores wich were used for \*checks notes\* nothing at all....",Negative
AMD,"Fr those nvidia meat riders make 7000 series look like dogshit, its got enough raw performance in 1440p",Negative
AMD,"lol let me guess…  you don’t believe in ray tracing or path tracing??  even AMD is starting to focus on it now for the PS6, and the head engineer designer has given multiple recent interviews saying it’s the future",Neutral
AMD,"Yup. I run RDNA3 and the message is received loud and clear. Heard it with the driver support for RDNA2 as well. They're now going for aged milk and doing it intentionally. Do they think I'll upgrade to newer milk? lmao, no  It used to be I'd feel a different kind of a fool going with either brand but by now it's beyond obvious sticking to AMD makes me the bigger fool, and anyone who denies this the biggest fool of all.",Negative
AMD,That V-Ram does nothing except in a few outlying titles.   Frame Gen and DLSS add far more then V-Ram and other then the 24gigs they slapped on the 7900xtx they been pretty much parody with Nvidia for memory.,Negative
AMD,You’re being sarcastic but steam deck is doing just fine. Machine will sell out.,Neutral
AMD,"The light architectural rendering she is doing for her side projects are not demanding, it takes a few minutes even on the current system, a 5050 will make it instantly",Neutral
AMD,"Isn’t it the same for RDNA4.0, basically?",Neutral
AMD,"People are so quick to forget that Nvidia did pretty much the same thing. When they were developing DLSS 2, they released essentially a beta version of it in Control which didn't use the tensor cores.",Neutral
AMD,"FSR and DLSS have always competed in the same space(advanced upscaling), but while DLSS needed ML hardware to work, FSR didn't. The issue now is that AMD realized that without the use of ML hardware FSR can't compete anymore so they're limiting the new version of the feature(and completely new features) only to the newest cards that do have said hardware with sufficient performance. So they gained praise for making FSR 1-2-3 available everywhere but now have to face inevitable backlash for limiting FSR 4 and future features because those limitations are the only way to make them better.",Neutral
AMD,Maxwell to Pascal was also an architecture change. The problem was that people got used to architecture change meaning just more performance/less power because that's pretty much all they got between 2010 and 2016.,Neutral
AMD,"FSR 4 does work, and pretty well, sure. I used it on my 6700. There is supposed to be more to redstone than FSR 4, though. I suppose we will find out soon. I just do not take this to mean that FSR4 will never come to RDNA 2/3. I still have hope for it.  AMD did state day zero driver updates and optimization will still occur for RDNA 2/3. I 100%  accept that as continued ""support"" for those cards. I do still run the 6700 for the SIM rig, so I am following the convos and still hoping for good news. That's all. No propaganda swallowing over here. Just hoping.",Positive
AMD,"Well, AMD physically cannot do that due to lack of such hardware in RDNA3. Even worse: it’s not like they were lagging behind in technology and can’t design such hardware. AMD have fully integrated tensor core equivalent in their CDNA2 AI processors. They just decided to not put them into RDNA3.  It was pretty obvious since RDNA2 that DLSS will be the future and they will lose their own customers just by selling them the only product they have at hand.",Negative
AMD,"Are you serious? I'm on 2080 Ti, and Nvidia removed CNN Ray Reconstruction from newest DLSS. Transformer RR takes 20+ ms on FHD on 2080 Ti, and that's just RR alone, without taking game into account. 15 FPS FHD gaming on 2080 Ti. And Transformer Super Resolution has AA so broken, I have to use OptiScaler in every game to fix it. Go on, tell me about no issues on Nvidia.",Negative
AMD,Switch for a bit to the penguin side and you'll see not always is the pasture greener on the other side.,Neutral
AMD,Exactly right you buy Nvidia GPU so you know when you spend more money on a card it will last longer than a year.   AMD is pathetic Microsoft and Sony need to stop using their garbage for consoles. Start putting Nvidia cards in those new Xbox's and PlayStations  Steam got shafted using RDNA 3 in their steam machine..,Negative
AMD,You got less raster performance due to worse hardware upscaler support.  DLSS / FSR4 is basically part of the raster performance. You can’t ignore that.  You don’t have to a fan of upscaling. It’s just something essential to a GPU as its base performance.,Negative
AMD,"The Int8 DLL proves that you can run something which looks and runs somewhat worse than the DLSS CNN model. It's still worse than CNN DLSS on both of those fronts, and transformer DLSS (using ML) blows both away.  FSR4 on RDNA4 is significantly higher quality and significantly faster than the Int8 version, which lands it between the DLSS CNN & Transformer models.  Int8 FSR4 is a bit better than FSR3, but FSR3 was never serious competition. It's pretty much a strawman that is trivial to beat because it is and always has been terrible.",Negative
AMD,"I'm fully aware of the topic. BTW, the leaked FSR4 int8 version was part of their internal testing before the RX 9000 launch, nothing more. There is no factual information suggesting that they are actually and actively working on an official FSR4 Light implementation for RDNA2 or RDNA3 users. It's fine if you are optimistic, but if I have to consider real facts, I can't be.",Neutral
AMD,Still AMD communication problem.,Negative
AMD,That is a very common occurrence for AMD.  Their marketing department has zero ability to control the narrative.,Negative
AMD,Fair point,Neutral
AMD,"That’s not what they asked, they ask how a 7900 XTX was a miss. I’d also like to know as the owner of one, it’s a great card and I love it. I think I’d prefer it over a 4080 /4080 Super if I’m being honest with you.",Positive
AMD,eh i still think its kind of a gimmick less of a gimmick than framegen but still  its an advanced form of anti-aliasing combined with render-scale   so i would not rate it has real performance despite real performance getting used here,Negative
AMD,well according to AMD rdna 3 is completely unable to do anything 8 related which we now know is BS  well unless i am misinformed rdna3 has way more fp16 and fp32 performance by puling a bulldozer(twice the cores) compared to rdna 2,Negative
AMD,"RDNA4 supports [out of order memory](https://chipsandcheese.com/p/rdna-4s-out-of-order-memory-accesses) access which will be used for ""SER"", but OMM is an interesting one because it's very likely a dead end technology.  The point of opacity micromaps is to allow for rapid testing of transparent pixels on 2d cards (textures with an alpha channel), these 'cards' are very commonly used in things like [foliage](https://images.squarespace-cdn.com/content/v1/5d4b249616f93e000149aebd/1587212042278-OG1ARIW0WOCAZPSAKH6U/13.Buildingthebush.jpg?format=2500w), [chain link fences](https://images.nexusmods.com/mod-headers/1151/39252.jpg), wires, etc.  If you've played a video game you've run into an object which you should have been able to go under/over or shoot through but bounce off because it's a flat 2d plane with a texture applied. This has been an incredibly common technique for four decades because we didn't have the computing power required to render each individual leaf on a tree. Or to physically mode a chain.  But that limitation disapears with virtualized geometry and voxels. Going forward we won't be slapping big 2D textures into game worlds to pretend an object exists we instead just model the thing.  Nanite in Unreal Engine goes this path with trees and shows entire forests can be rendered as geometry even with billions of triangles and the upcoming Witcher 4 won't use Opacity Micromaps because there are no 2d cards to test against.   So the entire problem 2d textures with alpha channels was invented to side-step has been solved.  So OMM is only going to help speed up older games but current, and certainly future GPUs, don't have a problem rendering older games.",Neutral
AMD,This is enough for me rn. But im just mad that amd doesnt care about last year card like mine and forces finally good fsr for only current gen.,Negative
AMD,"Isn't FSR4 part of Redstone, though?",Neutral
AMD,Right now a 6 year old RTX 2080Ti seems to be aging better than any GPU from RX 7000 series.,Positive
AMD,"Extremely important is debatable. I'd rather have an optimized game then one that relies on upscaling to achieve reasonable performance(Monster Hunter Wilds for example). To your point though, sadly optimization has become a significant issue.",Neutral
AMD,I still remember the times when it was common sense that upscaling was cheap tricks for folks that could not run stuff natively.  Nvidia marketing department really did wonders.,Positive
AMD,While similar price NVIDIA card can run in 4k DLSS transformer performance mode with better image quality and better performance.  RX7000 has bad value and this will bite AMD hard.,Negative
AMD,Glad the 1200€ card that was marketed as a 4k card can handle 1440p .... (Without RT),Positive
AMD,and Nvidia is selling fire-hazards for the second time  and being greedy on Vram   sadly its just pick your poison but yea i am also done with amd   i only did go amd for that 20gb vram for less than 2000 dollars(ignoring old cards),Negative
AMD,"You mean parity? That just isn’t true.  It generally is ~4GB difference of VRAM price for price between them.  Like when I was shopping last year, the 7900XT had 20GB at around the same price as a 4070 Ti Super with only 16GB.  So are you forgetting that the topic you brought up was fine wine / futureproofing? Your point about only outlier titles mattering for that VRAM difference is irrelevsnt since these cards aren’t old yet.  As time goes on we will see it matter more and more, the GeForce cards will become VRAM choked first, and we saw this happen with the 3080.",Neutral
AMD,"Because the Steam deck made Portable PC Gaming easily accessible AND affordable.   While the Steam Machine is.... a PC that's maybe or maybe not decently priced, on an OS where you aren't able to play a lot of Online games and it competes against Pre-Build PCs, self made Pcs AND maybe even consoles....  The steam deck was about the price of a nintendo switch with better performance.   The steam machine will be priced Probably be somewhere between the ps5/ps5Pro    with maybe similar performance.   It will kinda sell, but don't expect it to make waves like the steamdeck I belive the frame will outsell the gabecube",Neutral
AMD,> Machine will sell out.  AMD sells out of GPUs with every launch. Hasn't done jack for their marketshare.,Negative
AMD,"Doing ""just fine"" bro it can barely run Steam Deck Verified titles.  Its great for its 5 million userbase playing their back catalogs in bed or on the couch. I cant say the steam machine will have the same appeal with less utility",Neutral
AMD,"Yes, but the difference is that AMD spent 6 years yelling from the rooftops about how Nvidia's hardware feature was unneccesary and unimportant. They once again exhausted every other worse option before doing the same thing that Nvidia did.  That's 6 years of backwards compat which does not exist due to that series of mistakes.  RDNA1 buyers got unlucky, RDNA2 buyers got screwed. Before the RDNA3 launch it was obvious that this was happening and would be a huge problem so those guys had a much more informed decision.",Negative
AMD,"RDNA4 does obviously have real advantages over the previous gens  For example I'll never expect FSR4 to run as good as it does on them because it was made to leverage FP8 so you can't really blame AMD  This doesn't mean they should skip working on the INT8 (or whatever other way there might be) version for older gens  Same with RR, I don't care if it isn't the same quality or doesn't give as much performance boost, but don't act like it CAN'T run  NVIDIA is clearly more serious on this part",Neutral
AMD,"i vaguely remember rdna 3 was marketed as having some extra magic sauce, but i cannot think of a single thing that actually used it",Negative
AMD,not really no GTX and RTX were not the same architecture. This would be more like RDNA not working with Frame Gen but UDNA working with Frame Gen. It suck but it suck a lot less.   Since there two different architectures its more acceptable.,Negative
AMD,"No, they're facing backlash because the source code leak revealed that there's a version of FSR4 (INT8) that is or at least was in development at some point that works much better than FSR3 and functions on older hardware.",Neutral
AMD,"No, they're facing backlash because they're purposefully limiting things that don't need to be limited  Nobody will say anything if FSR4 or RR don't have the Se quality and performance boost as they do on RDNA4, that's expected  But as I said, Nvidia has RR on all rtx capable GPUs, there's no reason why Radeon shouldn't",Negative
AMD,"Basically Nvidia calls it DLSS4, but it is different for different GPu generations.  AMD could have gone that way too, just release FSR4 for all GPUs, but use different upscalers for different GPU generations internally. So it would still use FSR3.1 for 7000 GPUs, but it officially is part of FSR4.  But AMD did go the other route and only released FSR4 for the 9000 GPUs. We might see another update to FSR3 (i.e. FSR3.2) that works on all GPUs in the future, but I wouldn't hold my hopes high. I hope they release the INT8 version of FSR4 as FSR3.2 for all cards tho...",Neutral
AMD,"they allready leaked the version that litterally can run on that hardware, what are you talking about",Negative
AMD,You can just go into the Nvidia app and set the global DLSS setting to CNN if you don’t like the image quality or performance of the DLSS transformers model. On Turing the transformers model is very heavy when doing ray reconstruction but works well for super resolution upscaling.,Neutral
AMD,"Ok, and how old is the 2080 Ti compared to RDNA3 again? At least Nvidia gave you the option.",Neutral
AMD,"Most still use Windows, therefore they won't see the issues that are Linux only.",Neutral
AMD,"not everyone is willing to deal with all the bs you get over there, many people do not feel comfortable using a command line for example. we mostly abandoned it on windows and mac for a reason",Negative
AMD,Bro out here phrasing posts like he’s the Riddler,Negative
AMD,"My latest attempt at using Linux led me back to using Windows. Tried SteamOS on my ROG Ally after it got official support and much like back when I had a Steam Deck, the second you want to use games that weren't from Steam, it becomes a far bigger mess than it is on Windows.  MS Game Pass doesn't work at all for locally installed games. Heroic Launcher is good for Epic Games, but I had issues getting my cloud saves sorted, I had to manually fix the save data locations on multiple games. Can't run some of my favorite games due to incompatible anti-cheats.  I installed Windows and it fixed all those issues for me. The new FSE works well for me as well, it has better integration with multiple launchers than SteamOS does by default and most importantly, runs the games I want to play.",Neutral
AMD,If you're pulling out the Linux card for lack of anything better you've pretty much lost the argument.,Negative
AMD,So… switch operating systems solely to have some technical issues?,Negative
AMD,"AI upscaling has nothing to do with gpu power in rasterization... and i can ignore that if i rarely use it. 💀 It means little to me.  Rasterization performance (which is the word i used) exclude new render techniques like raytracing etc. Google it.  You are under post about feature related to AMD, convincing me im actually not happy with my gpu which i got much cheaper (compared to nvidia) with better performance (with RT off). That's ridiculous honestly.",Negative
AMD,"So?  On my gpu I have one option, either FSR3 which is unusable 90% of the times or Native  The INT8 dll looked incredible compared to FSR3, and still gave a noticeable performance boost  Why can't they just give me the option, hell put a warning in adrenalin saying ""IT MAY NOT RUN THAT MUCH FASTER"", I DON'T CARE just let me use it",Negative
AMD,They need better communication but the herd mentality of reacting to non information by filling it in with false information is straight up ridiculous.,Negative
AMD,"\>That’s not what they asked, they ask how a 7900 XTX was a miss  Specifically because 4080S exists. In a vacuum? Sure, 7900XTX is a fine product. Compared to direct competitor? It is flat out inferior in most relevant metrics and will only get comparatively worse with age.  \>as the owner of one, it’s a great card and I love it. I think I’d prefer it over a 4080 /4080 Super if I’m being honest with you.  And you don't feel that you can have any resemblance of bias in that situation?..",Neutral
AMD,It is real performance as it provides better than native image quality by super sampling through historical frames.  It is not generating any pixels on the fly like their marketing team trying to let you believe.,Positive
AMD,"RDNA3 have dual issue but not ""twice the cores"". This is mostly useless unless you code for it using Wave64.  RDNA3 can calculate fp8/int8 by padding them and use fp16 pipeline. Thus is gives exactly the same performance.",Negative
AMD,I think RDNA4’a OOO memory is a separate feature as it doesn’t relying on hint to sort the instructions while the SER api was designed around NVIDIA’s hardware which need such hint to work.  Other than that I agree that OMM is a dead end but who knows what else they could build from it. We already seen displacement mapping in ray tracing. Maybe it can be use there .,Neutral
AMD,"To my understanding which can be wrong Id say its the other way around redstone is something that can be implemented on fsr4. Redstone is a ray reconstruction feature that works on top of the fsr4 upscaler and the upscaler itself relies on dedicated hardware on the new rdna 4 thats why when people Talk if fsr4 for older rdna they say a ""light"" versión couse It just cant fully be used for those and seems like a feature like redstone is also not on the table this is the cut off point for amd much like the 2000 series was for nvidia",Neutral
AMD,"Me when RDNA3 launched: ""AMD WTF!? Why no feature parity yet!? Been 4 years now!""  R/AMD when RDNA3 Launched (thanks to Youtubers not even caring about feature parity): ""AMD WTF!? NV -$50.""  But some how, pointing out the lack of features ""No one cares about x-feature!""  Fine Wine, indeed.",Negative
AMD,isnt it 8 years?,Neutral
AMD,I have 2x 2080ti in nvlink on my workstation/rendering system and have not seen a compelling reason to upgrade,Neutral
AMD,No optimization will reduce the importance of upscaler. You can always get better performance from upscaling. Either in the form of more fps or better image quality.,Positive
AMD,"\>Extremely important is debatable  But... It's not. Whole industry strives for good upscaling. Every new architecture to come drives better upscalers aside from the rest of the features. There IS a reason for that.  \>I'd rather have an optimized game then one that relies on upscaling  And that is not dependant on your desires, for better or, very obviously, for worse. And even IF all games are well optimized - good upscaling is not detrimental to the experience. It is not even at net zero! Good upscaling at 1440p/4K is free performance. No matter how optimized game is - getting +20-30% more FPS while retaining 99.9% of image quality is not something to just disregard.",Neutral
AMD,So would I but the industry clearly isn't moving in that direction,Negative
AMD,"Don't you think that main praise should go for NVidia engineering, not NVidia marketing. It's engineers who made upscaling as usable as it is today.",Neutral
AMD,"And none of this is news, nothing that tips those scales has changed recently.  It was a lot of disingenuous discourse all  around with these things when RX7000 came out.",Negative
AMD,"What ,,same,, price we talking about only a 5070 is 550€ with only 12 gigs u can get the 7800 xt that performs quite the same for 16 gb and barely use upscale",Negative
AMD,"Same here, 24GB XTX but you could already have that (including 8 pins) in a 3090. That is still considered the goat on localllama although I never could find any as cheap as they used to claim.",Neutral
AMD,"Thats just it the cards won't become V-Ram chocked if optimization starts happening again.   Right now there hardly in optimization and just turning down the settings does help 99% of the time. O no I can't run ultra max at 4k with hyper textures.   On top of that AMD's already stopped updates to RDNA 1-3 for day one. Sure they said they back tracked, that they would continue as they had. They had already stopped day 1 driver updates for RDNA 1-3 before they even said they had.",Negative
AMD,Steam machine will run almost every single title perfectly fine. I have a 3080 if I’m worried about getting 200 fps I’ll jump on there or better yet just stream it to the steam machine.   Machine doesn’t appeal to you that’s fine. I plan on getting it for a media center but also having my library of steam games and being able to play it on a couch instead of my computer when I want that experience.  According to valve though it’s gonna get 60fps 4k so idk what else you want because most people have 1080p monitors,Neutral
AMD,"> Yes, but the difference is that AMD spent 6 years yelling from the rooftops about how Nvidia's hardware feature was unneccesary and unimportant. They once again exhausted every other worse option before doing the same thing that Nvidia did.  To be honest, it does seem that AMD is a a company with a great technical team but an absolutely abysmal business team.  Like every problem AMD has isn't the tech, but rather the business team making frankly bizarre decisions that unnecessarily shackle the product.",Negative
AMD,"Not sure AMD did ever comment much on cuda cores.   They had a different design philosophy back then. Tech changed, so now that old philosophy doesn't work as well anymore so they had to adapt. It seems like we are getting away from generalization again and are adding more and more specific hardware for specific tasks. This will reach a point where it will be better to integrate those features into normal cores again, because it is used in every game etc. I.e. if every game uses RT, no baked lighting anymore, there is no need to have RT cores, it will just be a part of a normal CU.",Neutral
AMD,"It’s not mistakes. Every single of these chips has a reason to exist.  RDNA 1.0 was the red headed step child of their design process with Sony for the PS5, and they used it as a process pipe cleaner for RDNA 2.  RDNA 2 is in all the consoles, so this makes sense.  RDNA 3 is the red headed step child of their design process with Sony for the PS6.  You get the vibe.  Next one will probably be whatever they call it in 2033.",Neutral
AMD,"They already leaked the INT8 model on the FSR Repo for a bit and people were able to get their hands on it.   It's already working on RDNA3... Not officially, you have to force it with optiscaler , but we already know it works, and it looks better than FSR3.",Positive
AMD,>This doesn't mean they should skip working on the INT8 (or whatever other way there might be) version for older gens  I dont think it is.  I just dont think the current crowd realizes that AMD is already far behind getting features meant from the get go for the 9000 series.   People bitching about having to use optiscaler to inject their INT8 FSR4 onto their RDNA2/3 card as if I dont have to do it with the majority of games I play on my 9070 xt.,Negative
AMD,"All the things that use WMMA is buried in technical examples and nothing that an end user like us would notice, except for an eventual optimised version of the INT8 FSR4 ddl using WMMA instructions  [https://gpuopen.com/learn/wmma\_on\_rdna3/](https://gpuopen.com/learn/wmma_on_rdna3/)  [https://github.com/adelj88/rocm\_wmma\_samples](https://github.com/adelj88/rocm_wmma_samples)",Negative
AMD,"Well, there's that, too.",Neutral
AMD,"Nvidia has sufficiently powerful ML cores on all RTX GPUs, Radeon doesn't. Nvidia going all in on ML with dedicated hardware early is very helpful here.",Positive
AMD,DLSS 4 is transformer model and it's the same model on RTX 20 as on RTX 50. The difference is frame-gen support and performance considerations. What I mean is RTX 20 doesn't run like a lighter DLSS model than RTX 50.,Neutral
AMD,Nvidia doesn't use different upscalers for different generations though.  The only difference is if they have access to AI frame gen,Neutral
AMD,FSR4 int8 have worse image quality and high performance cost on RDNA3 hardware.   7900XTX have 123Tops peak matrix performance while RTX4060 have 242Tops and 9070XT have 1500Tops. This is the hardware I’m talking about.,Negative
AMD,">You can just go into the Nvidia app and set the global DLSS setting to CNN if you don’t like the image quality or performance of the DLSS transformers model.  For older games - yes. For games using DLSS 310.4 or newer - no, Nvidia removed preset E for DLSS-SR, so it's either blurry F or deep fried J/K. Also removed all CNN presets for RR.  >works well for super resolution upscaling  What's the point of that Transformer upscaling if in any game with lots of details and dithered effects it's pixelated like FSR 3?",Neutral
AMD,"You don't see the difference? AMD didn't take away anything from RDNA3 users. Nvidia took away RR from Turing users. All the games that will come out on DLSS 310.4+ - a Turing user won't be able to use RR with reasonable performance. And vast majority of gamers aren't going to control their DLSS version and RR preset, they'll just ""override with latest version"" like everyone suggest, making RR performance unreasonable in every old game as well.",Negative
AMD,"""Most"" is an understatement. FSR Redstone is exclusively a gaming feature, and according to the latest Steam Hardware Survey, Windows accounts for 94.84% of the userbase. Linux users are practically irrelevant.",Negative
AMD,"linux cannot even be considered for gaming unless they sort the anticheat problems, and thats to say nothing of the many other things linux is still struggling with that other operating systems figured out how to do a decade ago.",Negative
AMD,"This, massively. Linux will never take off unless they *completely* do away with the command line. keep it in of course, Bury it for the hardcore nurds, but otherwise anything you can do with the command line should be able to be done in a GUI. Until then Linux will continue to be on the 2% marketshare grindset for eternity.",Negative
AMD,"Is another world. Everything has its issues.  And you don't have to touch the terminal to use any Linux/*nix system. If you don't believe me, put your hand in your pocket and tell me how many terminals have you used in your phone.",Negative
AMD,"I wasn't. But apparently none of you have a great reading comprehension.  I was saying that wasting a ton of money on an entertainment only thing and then getting mad because I doesn't last as much as we could imagine is nuts.  Getting a high end ANYTHING and expecting it to work flawlessly every time in every environment is absurd.  Nvidia is shit on Linux. AMD is shit on datacenter. And Intel is shit in the Energy department. Everything has flaws, regardless of the price.",Negative
AMD,"AI upscaling is part of the rasterization performance. Just like how shader compute performance becomes part of the rasterization performance since DX11, the meaning of rasterization performance has changed many times. Before DX8 rasterization performance only means pixel filling rate and nothing else.  You should use AI upscaling as this is part of the performance. You rarely use it because your GPU is not capable of running it not because it’s useless or can be ignored. A 3070 with DLSS can rein same game with better image quality and better performance than your 6950XT . That’s how DLSS becomes part of the rasterization performance since.  BTW AI Upscaling is in fact not upscaling anything. Those are TAAU super sampler just named upscaling for easy understanding and labeled as AI for buzzword.  It is just a new render pipeline that recycles pixels from historical frames.  And the whole reason behind Rasterization vs Ray tracing is just because AMD failed to reach features parity after 7 years so that they have to market heavily against Ray Tracing. Which is the savior of game industry as the cost of building rasterized games goes skyrocketed.    I still remember how beautiful ATi’s low resolution Ray tracing demo was. They were more passionate about computer graphics than current AMD.",Neutral
AMD,"> So?  I'm responding to this claim:  > Yes they don't have proper tensor cores, but this doesn't mean much, as proven by the Int8 dll  It does mean a lot, Nvidia has been using tensor cores to do something a bit better than Int8 FSR4 since 2019 - and massively upgraded it on that same hardware in Jan 2025.",Neutral
AMD,They are quite different but seek to work on the same problem. SER looks at the threads and groups them into clusters (sorts them) so they operate on the same hardware units and are likely to access the same memory regions. This improves caching and performance. Though as I understand this requires some level of API support.  Out of order memory access tries to side step the issue. It's a hardware function where memory instructions are reordered under the hood and transparently to the software.   This hardware level OOM architecture should help both SER and OMM without explicit support but perhaps with the addition of some software pipeline changes coming in the new API they will be even more optimized.   The NVIDIA approach might be faster in cases where it is explicitly used because it limits certain inefficiencies but the AMD approach just works on any software automatically.  I'm interested to see how it plays out in the real world and if any of the pre-sorting in software helps.,Neutral
AMD,"Nah, Redstone is a collection of ML technologies. ML Upscaling (FSR4), ML Framegen, ML Ray Reconstruction and Neural Radiance Cache  Their Ray Reconstruction is decoupled from their upscaler for example, in COD you can use it with TAA or XeSS if you wanted to for some reason",Neutral
AMD,"Have you noticed that we get DRASTICALLY less ""muh 7900XTX is best"" posts ever since FSR4 INT8 came out? People realized that what they called Nvidia marketing ""muh upscaling, muh fake frames"" was in fact just THAT good.  It's funny as fuck seeing the armchair ""I've been building PCs for 25 years"" AMD experts nowadays, they wouldn't be caught DEAD recommending anything less than a 9070XT.",Negative
AMD,"Ah, you're right. For some reason I thought 2080Ti came out in 2019.",Neutral
AMD,It less power use via Radeon chill,Neutral
AMD,"Depending on the upscaling 99.9 percent of image quality may be underselling it, even with DLSS2.0 I’d turn it on for some games not even for the performance, but for the free antialiasing.",Neutral
AMD,"Do we know the stats on how many people utilize upscaling? I know it has to be high, but I'd never personally buy a card based on it's upscaling ability. I've never used it as a tech and never will unless we somehow reach a point where it is an imperceptible visual loss, which is unlikely to ever happen.  I see people all the time saying FSR4 and the newest DLSS look better than native and I have never not noticed negative visuals from it. If I see any flickering/shimmering or ghosting I'm out and I'm running native. It's too distracting.  I know it's where the industry is heading and is great for people who buy 4k monitors and not 4k capable hardware, but I'm still in the boat where I run my games and buy monitors within my hardware's skillset because I don't want to have artifacts or input lag from all this new tech, although I will occasionally run frame gen if my base fps is like 120 because then I have a real hard time noticing the lag.",Negative
AMD,"There is a reason for that, but you seem to be completely oblivious to the actual reason and instead you decide to parrot nvidia press release crap.  The industry strives for upscaling because Moore's law was killed a couple years ago and improving process nodes is becoming exponentially difficult and upscaling is the trick that NVIDIA pulled to offer the illusion of meaningful generationals improvement.  This rethoric is being pushed in such a brazen way that you now have comparisons of new gen with upscaling VS. old gen native.  As for the rest of your comment.  - Upscaled content will never match native. - Upscaling is being used to downgrade the low and mid range in relative terms, while keeping (or even increasing the pricing) - Upscaling is being used by some devs to feed their own laziness and sloppiness, we reached a point in history where we have certain people that flat out say something along the lines of: ""if you don't want upscaling, just don't play this game lmao""",Negative
AMD,"Oh DLSS is great, no question about it, but the biggest win is for sure on the NVIDIA marketing team. They get to sell gpus at higher margin and convince everybody that ""if you turn DLSS on it will beat everything in previous gen lineup"" is an argument that makes sense.",Positive
AMD,The only thing that changed was the availability of FSR4 int8.  Now you can have a mostly comparable image quality against DLSS and now you can compare the performance from that.  I always compare RDNA3 using FSR native AA with RTX using DLSS 3 balance mode. As these 2 looks similar enough to my eye.  Just most people back then were all onto the hype train of native resolution and ignoring the fact that native resolution looks ugly due to shimmering and aliasing that plagued almost all games since Xbox 360 era.  Things have already changed when FSR4 first appeared and suddenly FSR3 becomes a blurry hell.   It was always a blurry hell.  AMD’s pricing was bizarre due to this and to me it was never NVIDIA $-50. It was NVIDIA +$100 and came with a brainwash to let you believe it’s cheaper. Just like how NVIDIA was pricing their Fermi/Kepler/Maxwell/Pascal GPUs.,Neutral
AMD,"You have to use upscale as DLSS gives you better than native quality and better performance at same time. It’s not like upscale is trading quality for performance anymore.   7800XT with FSR 4 int 8 is slower than a 4060Ti with DLSS CNN model. And with transformer model 4060Ti can drop to a lower mode with still better quality that further increases its advantage.  Without FSR4 7800XT could not even reach a point to be considered fair comparison due to subpar image quality.  Native resolution looks super ugly today due to aliasing and shimmering. And let’s not talk about TAA which was supposed to fix those issue but makes the whole image blurry as hell. You basically need DLSS/FSR4 to get a reasonable image.  And 5070 with 12VRAM is equivalent of a AMD card with 14GB VRAM due to full memory pipeline compression, which is already announced by Sony will be present on next generation AMD GPUs. Aka missing on current gen AMD GPUs.  BTW: DLSS/FSR4 is not some Gen AI BS. This is TAAU with AI helping to figure out which pixel goes where. It’s basically recycling rendered pixels which would be wasted in traditional render and forget style. 4x 720p image gives you equivalent of 1440p pixels to works with. And DLSS usually stores up to 30 frames worth of pixels.",Neutral
AMD,i dont know if the 3090 was effected by this but i would stay very far away from cheap 90 class cards given there are alot of non working models floating around   and no its not something you can fix by backing because if you open up the thing you see that vram and gpucore have been looted,Negative
AMD,">if optimization starts happening again  Optimization is not like, a thing that was happening and then stopped. It is always happening to greater or lesser extents.  Regardless, we know VRAM consumption always goes up over time, I don’t think anyone will take this claim seriously that it will halt at the year 2025 if developers just do better or something.  If you want to argue that developers will suddenly start spending much more resources optimizing VRAM consumption, I’m all ears for why that is going to happen. But if it’s not going to happen, it doesn’t matter what they should do. Only what they will do.  >turning down the settings does help 99% of the time  Obviously if you are VRAM constrained you will turn down settings. The point is that you will need to turn down settings, because you are VRAM constrained (whereas the Radeon card won’t have to yet).",Neutral
AMD,"For now lol Steamdeck ran a bunch of things when it first released too  >Machine doesn’t appeal to you that’s fine  I dont think it appeals to as big of a base as you think it does  >plan on getting it for a media center but also having my library of steam games and being able to play it on a couch  Whats stopping you from getting an upgradable pc ?    >According to valve though it’s gonna get 60fps 4k so idk what else you want because most people have 1080p monitors  Yeah, ps5 pro promises 4k gaming same with the switch 2 and.... barely any games have that.  Like im sure the thing can run terraria at 60fps 4k. I doubt it'll run borderlands 4 60fps 4k. Valve can claim things all they want, getting into what is actually possible is another thing entirely.",Neutral
AMD,AMD’s technical team is still not as good as Nvidia. Nvidia came up with dlss and tried to push ray tracing by producing new tech. AMD seems to always just play catch up.,Negative
AMD,">it does seem that AMD is a a company with a great technical team  Sorry but flat out no, that's a genuine lie.  The most insane, impossible to defend example is hardware acceleration. Hardware acceleration is like the ONE thing you expect out of a GPU, in fact Nvidia manufactured cards like the GT 1010 and GT 1030 as recently as 2022.  Those cards were the absolute bottom of the barrel, only meant to provide a video signal and some smoothness to browse the web or watch movies, that was it. Completely useless for anything else.  Guess what? Something that a €60 Nvidia card does flawlessly, a top end €1000+ AMD GPU (7900XTX for example) doesn't. It's been broken on a wide range of products for over three years now. It's unacceptable and is some circus clown level of bullshit.  Also, there are dozens or hundreds of random bugs that haven't and will never be adressed because actually getting an engineer to take a look at them is impossible.",Negative
AMD,RDNA5/UDNA1/UDNA5 whatever they will call it will be the engine of PS6. This is already announced by Sony with a tech briefing. The key hardware features  are hardware BVH traversal acceleration and full VRAM compression on top of RDNA4.,Positive
AMD,"Yeah I know, I've used it that's why I'm saying AMD can no longer hide behind ""technical limitations""  At this point they might aswell just integrate it as an option in adrenalin  If they don't wanna support it, I'd say let more code be available so that the community can further improve performance and similar things maybe",Neutral
AMD,"With a bunch of issues that clearly AMD doesn't want to be held accountable to fix.  Third party solution == no support. AMD doesn't care if modders mod it in, but if they sign off on it - they own maintaining it which they clearly don't want to do.",Negative
AMD,It looks better than FSR3 with ~20% performance regression plus it looks worse than FSR4 fp8 on RDNA4.  Btw in fact it runs as fast as or as slow as emulating fp8 using fp16 on RDNA3.  All these evidence points towards that int8 was designed to run on PS5 Pro with int8 acceleration.  You should just run the full fp8 model instead. Although currently that is only available using Linux and mesa driver.,Neutral
AMD,"Of I agree, and I'm not suggesting that they should dedicate to bringing it to RDNA3/2, first  All I'm saying is that since the thing is out, and world, why not just release it already as a toggle in adrenalin  They're gate keeping it just to create an artificial advantage that's not needed, rx 9000 already has better price performance, RT and a bunch of other things",Neutral
AMD,RDNA3 also has full support for borderless applications when using AFMF2 and... Yeah that's pretty much it as far what the end user would notice.,Positive
AMD,"Because WMMA on RDNA3 does not yield better performance. It’s almost like an emulated support of AI instructions. The idea maybe was like you could code on RDNA3 and with minimum tweak it will runs on CDNA2/3 with better performance.  Since ROCm support on RDNA was bad enough, nobody actually code on them so in the end nobody use those emulated instructions.",Negative
AMD,"Yes, they clearly have an advantage  That said, that dll leak killed Radeon's whole narrativ   I've watched it run on my card while playing and I want it, idc if it isn't as fast as FSR3, it looked just as good (on some things better) than native while giving a performance boost, give me the damn option, it's not hard  You can no longer tell me ""oh sorry it can't run"", might aswell release it  Even if they don't develop it at all after that, I'm sure the community would gladly pick it up",Positive
AMD,"just give people what they want, even if it isnt as performant as it is on rdna4 its better than xess or fsr3",Neutral
AMD,Something isn't right. DLSS4 looks nothing like FSR3.,Negative
AMD,"> All the games that will come out on DLSS 310.4  New games aren't going to run well on Turing nevertheless, let alone new RT games. The 2080 Ti might still run things passably at 1080p, I'll concede, but you're certainly an edge case.  >  And vast majority of gamers aren't going to control their DLSS version and RR preset  The vast majority of gamers aren't running 7 year old top halo tier GPUs either. If it bothers you so much, the option is there to control your DLSS version and RR preset. Not sure what you're complaining about.",Neutral
AMD,Talk to Valve about that one.,Neutral
AMD,"""Cannot be considered for gaming"" well I guess the last 6 months for me are an illusion then. You do realize that not everyone spends their entire life in COD and Fortnite matches right? In fact, I'd argue that it's better that that slop doesn't run.",Negative
AMD,"Do me a favor. Look up any tutorial for Linux, that someone new to Linux might look up. Like 90% of them will tell you to use the command line.  It’s not 1986 anymore. Its time to abandon the lines, and not just the ones for your nose",Negative
AMD,Comparing what is essentially an irrelevant flaw for the overwhelming majority of users (94.84% of gamers on Steam use Windows) to an *obvious* problem of *newly released devices* (like the Steam machine) being locked out of new features is such a non-sequitur it isn't even worth engaging with with any degree of seriousness.,Negative
AMD,So you are just nitpicking word meaning despite you have known what i meant from the start? I don't agree with you. Imho rasterization is rendering specifically excluding Ray tracing. (but i wont debate here about that since its irrelevant)  Whether 3070 can have better image quality is debatable. (i don't think so). I would run ray tracing on current gen GPUs but not on 3070 which can handle it barely and mostly require upscaling (which i want to avoid). Even in that scenario i would maybe preferred RT off with super sampling for best AA.  Not mentioning VRAM limit which can cause stutters even in 1440p. (I just recently talked with friend about this happening in RE4 RE)  >That’s how DLSS becomes part of the rasterization performance since.  I still thinks this is bullshit.,Negative
AMD,"Again, fine, good for Nvidia  Still doesn't justify amd for bit releasing something that clearly works",Positive
AMD,"When I first started actively using Reddit, I was posting here for regularly. The amount of white knighting for products that lacked features was amazing.  People were openly cheering being overcharged for raster and VRAM when ATI of yore gave that away for free. The ""Nvidia -$50"" meme was cringe as hell because they all ignored the feature disparity between the two.  Even when responding to people on r/hardware about the 5700 XT vs 2060 Super dust up, people still defend AMD/HUB for that travesty.  People got hoodwinked, bad. And FSR4/Int8 Version is exposing it all to them.",Negative
AMD,"Remains to be seen how that 9070XT will age too. Yeah, it's getting redstone but AMD are switching uArch again to UDNA, a combination of CDNA and RDNA.  My guess is that we will be having this same convo around 9070XT in a few years time also.",Neutral
AMD,"\> I've never used it as a tech and never will unless we somehow reach a point where it is an imperceptible visual loss, which is unlikely to ever happen.  No offence, but this is plain luddism. As for the second part - it is ALREADY the case and was for quite some time. DLSS Quality gave better image quality than native TAA on 1440p in many titles I've used it in. Difference between DLAA and DLSS Q was negligible while performance gains were significant.  Not using it ""just because I feel that way"" is... Suboptimal, let's say.",Negative
AMD,"When a vast majority of games don't support upscalers then inherently a vast majority of ppl won't be using the tech, add in the fact most ppl are on gpus that have poor support of the tech as well.",Negative
AMD,"DLSS4 is superior in motion to many in engine TAA implementations, even from a lower resolution. Enough cope.",Positive
AMD,Of course I stopped looking for them a while ago as I'm well past thinking 24GB is a lot :),Neutral
AMD,"Not that that is because the team is worse, or the fact that Nvidia just has more engineers and resources.",Negative
AMD,"The chip of the PS6 is done already, the console is coming out in 2027. I wouldn’t be sure that it’s anything more than RDNA4+. Let’s see.",Neutral
AMD,"Well of course. But AMD already crated the model for INT8, so they have already done it.. And I'm not implying they should support a modded version or the FSR4 FP8 emulation that people ran on Linux.   My point is that they already did the INT8 model, and it clearly works and it has better image quality vs FSR3. So it's not like the cards are unable to run it like the FP8 model which RDNA3 completely lacks the instruction set to work... It's there and it works...the  bulk of the work has already been done...so what the hell?",Neutral
AMD,the trade off to that thinking is. Clearly people will go with a superior better supported product.   The product might cost 10% more in this case. That 10% gets you continued support and improvements. Thus aging better then the compaction.   AMD never going to get anywhere and maybe thats the overall goal to be mediocre to barely push. So that Nvidia never gets the idea to push into the CPU realm.   Nvidia CPU could crush Intel and AMD both just from R&D standpoint. (More money better R&D),Neutral
AMD,"It yields better performance for the software that actually uses it.  Most things don't right now, and won't for years since it requires at least a recompile to use and most companies can't be bothered to do a new release just to support 1 instruction.  Lots of games either don't support NV's tensor cores or use them inefficiently so there is little to nothing to notice for your average gamer there too.  That is why pure raster performance still mattes lots.  Most games still don't support raytracing and there is still very little AI related stuff in games.",Negative
AMD,"Yeah, they shouldn't have allowed that leak to happen if their plan was making FSR 4 exclusive to RDNA 4.",Negative
AMD,The leak is not targeting your GPU. RDNA3 runs int8 and fp8 with same performance. It is most likely a PS5 Pro version leak.  At this point it’s pretty obvious AMD don’t want people to look at their bad old generation GPUs and just pretend they didn’t exist.  RDNA4 improves the AI performance by 10 folds and that is a huge hardware gap canning be fixed by software.,Negative
AMD,That would cost AMD money and the results may be underwhelming which may or may not worth the effort.,Negative
AMD,"But did you actually try comparing, or is that something you heard from a youtuber and were shown examples from games Nvidia told that youtuber to show? [Here's](https://slow.pics/s/DaMiWrUP) an UE5 game with Lumen, dithered foliage, SSR, basically all the stuff of a modern game, ""Quality"" mode. You should be able that preset K (DLSS 4) looks about as pixelated as FSR 3, while preset E (DLSS 3) and FSR 4 both look much cleaner.",Neutral
AMD,>Not sure what you're complaining about.  Nvidia takes away/breaks features that did exist and worked fine. AMD may or may not give **new** features to old cards. This is clearly very different.,Neutral
AMD,"""My operating system doesn't run the most popular games people play and that's a good thing"" is the dumbest argument I've heard for attempting to convince people to switch over.",Negative
AMD,You do not know what you are talking about.,Negative
AMD,Even switch 2 could runs game in ray tracing now. This is a performance optimization method. Not some eye candy FX that cost extra performance  And even without Raytracing 3070 is already faster than your 6950XT thanks to DLSS transformer model.  You can doubt all you want but seeing FSR4 running on your 6950XT should already give you some idea.  3070 is a way cheaper GPU that have 8GB VRAM. But a 3080 with 10GB VRAM should have no issue as that is equivalent of 12GB VRAM from a AMD card. Plus it is still cheaper.  AMD was charging way beyond the performance of RDNA2/3 and leans heavily on marketing against ray tracing. They knew this will happen since all current gen consoles market heavily on Ray Tracing despite using AMD GPU.,Neutral
AMD,Sounds about right for reddit. I learned in the apple sub reddit people will cheer to pay more for less. Its insane.,Neutral
AMD,"To be fair, we are forgetting how insufficient DLSS was until around 2022 into 2023.  FSR being around as a way to compete also meant if your card really was struggling, you, too, could have a ghosting image for a few extra frames of performance on whatever card you had.   The 7000 series of cards came out right around the time DLSS actually was great - but as a tech feature, you couldnt really appreciate it until you experienced it yourself.   And for those tight on a budget, a 7000 series card could likely outperform anything a comparable nvidia card setup was going to get you.   But for sure, it was only in the short term that the lack of having a developed feature set was going to bite anyone.  And for *any* company, its not like you can expect it to sell someone else's feature set over its own.   No matter how I look at it, some lineup from AMD was going to get bit hard if or when it ever caught up.  And this time, it was the RDNA3 lineup.   At least they can still raster well, tho?",Neutral
AMD,"I'm still not a fan of upscaling techniques. I'm pretty sensitive to visual artifacting and I was a late adopter of LCD's back when CRT's were still around.  LCD's had a myriad of visual issues that just put me off. Terrible contrast and poor colour reproduction, things like dithering etc making smoke look like oil on water. Fixed resolutions too, so back then if you dropped your res the LCD had to do the scaling which looked awful.  Yes, the LCD took up less space, used less power but it was early in the development cycle. CRT's just blew them away in so many areas.   DLSS / FSR / XESS are all early technologies. None of them are free of wierd artifacts but they will get better, I'm sure.   The point of the story is that it is okay for people to dislike a technology until it is more mature. Upscaling is at that point for me, it's early LCD tech where given a few more years it will be really worth using. I'm not going to be bullied into liking it because there is a turf war between AMDolts and Nvidiots.   Eventually DirectX and Vulkan will codify it so that it's part of the game rendering standards and we can be done with support for each type being patchy, the tech will get better and it will just become a normal part of gaming. I'm happy to sit it out while everyone bickers over who is winning and losing and use it when it's more mature. I'm not going to base a purchasing decision on that one feature though.",Negative
AMD,"No offence but you're making claims based on your personal experience. My personal experience is that I can always see a visual degradation. It may be very minimal, but it's there when I'm looking at the screen. This is no different than people saying they can't see the difference between 60 fps and 120 fps. Some can't. Some can.  If the tech ever gets there of course I'd use it. Like I said. It isn't there for my eyes whether it is for yours or not. And that's okay.  I run AMD so I don't use DLSS. I have a 7800XT. My wifes PC is a 4070Ti and she does use DLSS and doesn't notice a difference unless I point it out. She games but doesn't do PC shit so I always set everything up. Every single time I've seen DLSS on her machine I can tell it's on. Same with FSR on my machine, but to an even greater extent. Once I can't tell it's on, it would be a no brainer to use it, but I prefer clarity over all in my games.",Neutral
AMD,The majority of games support DLSS.,Neutral
AMD,"Trying to argue that upscaling is better than native is the epitome of cope, I think the irony is lost on you.",Negative
AMD,Lol no. Quality over quantity.,Neutral
AMD,"It is, as they already announced that via the tech briefing at YouTube.",Neutral
AMD,"> so what the hell?  They didn't do the work to test it in all games. Even now it has issues when used with Optiscaler.  Optiscaler has no reason to support it, because it isn't their tech.  AMD has to support it if they officialize it because it is their tech.  The best case scenario is this is the hidden ace in Valve's Steam Machines as they can support it as it is their hardware.  So long as AMD pretends it doesn't exist - they aren't accountable for ensuring it works across the board.",Negative
AMD,It does not.  WMMA on RDNA3 yield 0 performance benefit for software that use it.  Wave64 does yield up to 2x on RDNA3 due to dual issue but that also applies to FP32.,Negative
AMD,"Exactly, now they can't hide behind fake hardware limitations anymore",Negative
AMD,This kind of defense would get you a contract at a champions league club,Positive
AMD,how if it allready runs ? just make it official,Neutral
AMD,"It doesn't. I think anyone comparing can see FSR3 is substantially more pixelated. It's night and day how pixelated is.  And FSR3 isn't just pixelated on static imagery, but creates massive fizzle and pixelation in motion.  The fact that you're comparing static imagery and acting like it's comparable to FSR3 is revealing. The majority of how an upscaler functions is in motion.  FSR3 isn't in any way comparable to DLSS. It mostly destroys objects in motion. DLSS4 has minor artifacting and a tendency to be slightly sharpened (which can often be adjusted). The problems aren't the same and the results in motion are literally *night and day*.",Negative
AMD,"But you can still force the older RR preset, no?",Neutral
AMD,Because those AMD card was much less capable from day 1. It’s not so possible to have those features to begin with. And NVIDIA is also removing those legacy features from their latest and greatest.,Negative
AMD,The only thing they took away was your ability to be a spoilt brat. By your admission the version to control your DLSS version and RR preset exists. If you refuse to use it it's on you.,Negative
AMD,Average linux fan lmao.,Negative
AMD,">Even switch 2 could runs game in ray tracing now.  That's cool. How is that relevant or what's the point?  edit: i see yo edited yr comment... You can hit 10gb vram limit also especially in 4k (or with framegen). I saved a quite a lots of money by not choosing 3080 (which has worse raster perf. if i remember it correctly).  AMD does little to no marketing (at least in my EU country). ""Anti RT marketing"" term sounds funny. I didn't rejected RT for good i just found it too bad and too punishing even on 3000 series so i opted out for cheaper card with better raster. In my next rig i may have different priorities.",Neutral
AMD,Man I envy people who can splurge on very cool builds. That's a pretty neat aesthetic wise OP! Have fun gaming!,Positive
AMD,"When I built my first gaming PC on AMD, all those flashy glowing gimmicks didn’t even exist yet.",Neutral
AMD,"Hello, I just wanted to vent my frustration with AMD.  I have been a long-term Nvidia user, and when I upgraded my PC, I figured I would give AMD a chance. I regret that decision completely. For the last two years, almost every single one of my games crashes randomly, except for a select few, and those just started crashing now too.  I have tried every driver available for this card, and they all lead to the exact same outcome: the game crashes, and I have to scramble to reload it as fast as possible so I do not screw over my team. This has caused countless losses. I never had problems like this with Nvidia, and when I did have issues, they were not Nvidia's fault.  I have tried every fix I could find online or that I knew myself, and I used to be heavily into computer repair and all the technical PC troubleshooting. I have been scouring the internet again for a solution to something I have been dealing with for two years, and at this point I know it is the card.  I will never purchase an AMD GPU again, and honestly I would welcome an Nvidia monopoly if this is the best AMD can do to compete.",Negative
AMD,"I've used a full amd from 2013 up to 2021 and switched to the rtx then and then back to full amd a year ago, and to be honest, saved a lot of money, and been able to play almost anything on 4k ultra with more than 100 fps on a 144hz tv, also back in the 2010s, I was able to ultra most games on an even humbler amd card, and didnt have driver problems at all, to be honest, never looked back. I think people are more prone to nvidia/intel a lot due to marketing as well, not saying it's better or worse, just saying, it's not a competition if you can run everything well for a much lower price",Positive
AMD,Nice.  How Id do it.,Positive
AMD,"Nice, Have similar air Cooler on cpu",Positive
AMD,"AMD cpus are the best on the planet and will probably stay that way, but NVIDIA has better price to performance imo with a lot of their mid/higher end gpus",Positive
AMD,Could have gotten 9800x3d and 9070XT without all that bling bling haha   But looks really nice,Positive
AMD,"Nice build, I like the fan and air cooler setup. 👌",Positive
AMD,Dog wallpaper name,Neutral
AMD,"Cool PC nicely built 👍  I said fuck it to RGB and build a more powerful PC, components with RGB make for a worse build with higher price, I wanted the best performance with my budget so no RGB for me unfortunately. Asthetics are expensive 😅",Negative
AMD,Welcome to Team Red. Now let's add some Red in there,Positive
AMD,Set for 7 or 8 years? I built a nicer system than yours and mine will only last me 2-3 years at most.. maybe even 4.,Neutral
AMD,"The aesthetics of your setup is impressive, congratulations",Positive
AMD,"I also want to build something like this, a 9600x and the 9060 Xt or one with the 7800x3d and the 9070 xt",Neutral
AMD,Awesome Build!,Positive
AMD,Love the colors and look.,Positive
AMD,Thats beautiful,Positive
AMD,Good shots is that with Iphone 17 pro?,Positive
AMD,"It's not even that much of a splurge, though. 32GB memory? 9060? Slow SSD that's only 1TB?? 1080p monitor that's only 24 inches?  My processor is similar to his, but Intel.. but I'm running dual 32"" monitors 1440p, 165hz, 96GB memory, my SSD is bigger and up to/more than double the speed of his, 9070 XT video, etc.",Neutral
AMD,"It’s a few grand at most, what sorta cheap ass hobbies do you have?",Negative
AMD,Haha. I remember my grandfather's old Win98 PC with the slightly beige colored case and monitor. We've come a long way.,Positive
AMD,Could be you got a lemon. Did you try RMA? Mine has been flawless so far.,Positive
AMD,"Man I am lucky enough mine doesn't go poof at any game I play. I'm itching to try BF6 on my new rig, I hope i don't get the glitches you're experiencing.",Positive
AMD,"Yup, the marketing is strong with Intel and Nvidia, especially Nvidia.  At least AMD's CPUs are in a good position now, they need to work on their prosumer and productivity GPU offerings, Nvidia has a sizeable lead there.",Positive
AMD,"when I bought a  7900xtx I was hesitant about rtx and all, but to be honest the rasterization performance is so good and the 24gb of vram is helpful for 4k gaming since I play on a tv from far a away couch most of times, and even on games with rtx, I was able to run rtx on without problems of fps drops, where I live the 4090 back then was more than double the price I paid for the 7900xtx 2 years ago so it was definitely worth for the performance",Positive
AMD,Price to perfomance nvidia??? It is literally the worst price to perfomance company on the planet,Negative
AMD,"If giving up all the RGB from my current build in exchange for a 9800X3D and 9070 XT build at the same price, I'd do that in a heartbeat haha.  Thanks, it really does.",Positive
AMD,All wallpapers link:  [https://steamcommunity.com/sharedfiles/filedetails/?id=2285898736](https://steamcommunity.com/sharedfiles/filedetails/?id=2285898736)   [https://steamcommunity.com/sharedfiles/filedetails/?id=2251857160](https://steamcommunity.com/sharedfiles/filedetails/?id=2251857160)   [https://steamcommunity.com/sharedfiles/filedetails/?id=2377552169](https://steamcommunity.com/sharedfiles/filedetails/?id=2377552169),Neutral
AMD,"Thanks. This is my first PC with RGB, wanted to focus a bit on aesthetics. My next will most likely be Non-RGB.",Positive
AMD,Check out the second pic :),Positive
AMD,"I am more of a r/patientgamers kinda guy, don't play much AAA. My 1060 PC lasted my cousin and me more than 9 years combined, and that too because its motherboard died recently and took the CPU with it.  And with upscaling technologies becoming so good one can easily stretch their systems on for a bit longer (although crap optimization can negate that advantage).",Negative
AMD,Thanks.,Positive
AMD,Thank you.,Positive
AMD,Thanks.,Positive
AMD,Thank you.,Positive
AMD,Thanks. Its from a Fujifilm camera.,Positive
AMD,"Compared to my rig, his build is just so much better.  But yours is just waay beyond my imagination.",Positive
AMD,"Well, on my country's average; PC builds like this cost about 10 months' worth of our average salary. So yeah, it's kind of expensive to my eyes, buts it looks very beautiful.",Positive
AMD,"No, I’m not that old, but my PC in 2005 had an AMD Athlon XP 5600+ and an ATI 9800 Pro 128mb, with 1024 MB of RAM… technically ATI became part of AMD later, so that’s how it was.",Neutral
AMD,"Ooooh, dude that looks like a proper mean machine now!",Negative
AMD,"I rarely play games that stress out my computer. I play Counterstrike 2 99% of the time. I do do video editing sometimes. I just like having something decent and I've been building my own PC's for as long as I can remember.  Each time I'm like this time I'm going to buy a pre-built system, because building frustrates me, but in the end I always build my own.",Positive
AMD,"My friend has a Fujifilm camera and they take some of the best pictures I’ve seen. Sweet build and nice pics, thanks for sharing.",Positive
AMD,Mine is certainly overkill for 99% of what I do.. watch videos surf the internet and Counter Strike 2. Rarely do I play any higher end games. I just can't find ones that interest me.,Negative
AMD,"That sucks. At my salary I bought the video card in about 3 or 4 days time, and I already made money to drop another $1000 on a TV.",Negative
AMD,"Yep it does, and my salary is considered above average already. This is why my friends on steam have no problem buying BF6 immediately and here I am needing to ponder for a few weeks if I really want it. Currency coversion is hard here in the 3rd world country.",Neutral
AMD,I have never had a good salary. Fortunately my Mom has a 4 bedroom house that's paid for. So I can live cheap and spend on other stuff. My only drawback is I wish I had more room for my own stuff. I can only fit so much in a bedroom and with limited power outlets it's even worse.,Negative
AMD,"Nice. I’ll pair it with my $2,000 16Gb of ram.",Positive
AMD,Paid $479 back in February. Figured it would be a little cheaper than $449 by now,Neutral
AMD,theres going to be a lot of CPUs in the market because no one has the money to build a PC because of ram pricing.  The only people winning is the people who have lower end AM5 cpus and want a cpu upgrade.,Neutral
AMD,I think AMD cut the MSRP  Some retailers were already selling less than the old MRSP,Negative
AMD,got it for $399 at Micro Center a couple weeks ago...,Neutral
AMD,CPUs are going to get cheaper and cheaper as there's no RAM around to put them in new systems.,Neutral
AMD,*Smells 3d cache in the air at a discount*  DADDY NEEDS HIS MEAT!!!!!,Neutral
AMD,I got mine at that price with a cooler on Newegg,Neutral
AMD,"can't wait for it to reach 250, then I'll replace my 7500f with it and call it quits on AM5 and sit it out until AM7",Positive
AMD,"Nice, for the first time in history something is cheaper outside of the USA. Its around $370USD here in Japan, makes up a little for the reaming I got on the RAM.",Positive
AMD,Thank you Microcenter bundle for getting me a $370 price on this,Positive
AMD,Been hovering around 420€ in EU,Neutral
AMD,"Got mine for about $350 after cashback off aliexpress back in the summer, nowadays they're going for around that price before cashback",Neutral
AMD,I read the headline and thought it was funny that the 5700X3D CPUs jumped UP to £440 on Amazon when they announced end of production :D,Positive
AMD,"No, thanks",Neutral
AMD,Less that 16gb of ram…..,Neutral
AMD,"What's the likelihood of this dropping further in price for black friday/cyber Monday? Im impatient in my upgrade, pulling the trigger this week.",Negative
AMD,"It was available for 389€ as part of a black week deal.   Yes, that's VAT included.",Neutral
AMD,"Thats pretty decent.   I managed to snag one in EU on Aliexpress (tray cpu) and including all discounts for 380 dollars, howeven thats included the VAT taxes on 25% so its pretty damn good price.",Positive
AMD,Usually cheaper in the country of microcenter lol,Neutral
AMD,330 USD for me on AE atm.,Neutral
AMD,"damn the same price as my jeans, not bad",Positive
AMD,"Honestly, that might be related. People aren't buying fast gaming CPUs anymore because they cannot get the RAM to pair it with, while big AI datacenters are buying EPYC or some highly specialized AI chips.",Negative
AMD,Yep. Im still on AM4 with 128gbs of ram. Looks like im not upgrading anytime soon,Negative
AMD,Lol,Neutral
AMD,I've noticed the x3Ds hold their value pretty well.,Positive
AMD,"I remember getting a 7800x3d during the fall of 2023 for $339. A year later, the price jumped up to $449. 2020s have been so unpredictable with computer part prices. I miss the 2000s-2010s era so much when I would upgrade to last gen (one year old) for cheap second hand through Craigslist.",Negative
AMD,I'm still on AM4 and looking good for a few more years for what I use my system for. (gaming btw. And getting Reddit Rage XD) I consider myself a winner.,Positive
AMD,"Yep, got my 7800x3D for $280",Positive
AMD,You might as well have gotten it at Willy Wonka's Chocolate Factory while on a tour for 99% of the country.,Neutral
AMD,Nice.,Positive
AMD,.....obligatory* not everyone has access to a microcenter.,Neutral
AMD,I get that RAM got more expensive quickly but to me graphics cards are still far more of an issue. I go second hand for everything but monitors and rarely upgrade anyway.,Negative
AMD,Brother...how?,Neutral
AMD,Well the nice thing about the X3Ds is that RAM speed is less important.,Positive
AMD,"This, my brother wanted to get some RAM and now a kit I paid $40 for is now $160",Negative
AMD,You could buy a car with that much ram,Neutral
AMD,"Mate of mine had a Dell rig with that, p4 1.4. I remeber the rdram ran hot as heck. It had small headsinks, which back then was only extreme benchmark/record OC behavior.",Positive
AMD,Yeah I sold my 7800x3d for $335 when I ordered my 9800X3D. Should have sold it for more lol,Neutral
AMD,Obligatory angry happy for you kid,Positive
AMD,It was paired with an Asus Strix 870E-E board (of which had its price own reduced to 390). Seems the deal has expired when I check online unfortunately,Negative
AMD,But RAM size is not and never will be.,Neutral
AMD,It’s not.,Neutral
AMD,"I built a new rig last December, all the good stuff, 9800X3D, RX 7900 XTX, TeamGroup T-Create 32GB DDR5-6000 RAM and so on. The RAM cost me 90€ back then, now it goes for 280€... the current cheapest 32GB kit according to comparision sites here in Germany is a Corsair kit for 134€.  I also bought a Kingston Fury 64GB SO-DIMM DDR5-5600 kit in August when it was around its cheapest, for 155€. It was for a mini PC. Prices keep climbing, yesterday it was going for 370€, currently for 392€.  God was I lucky with my RAM purchases...",Positive
AMD,I had a Dell P4 RDRAM system that I upgraded to the 1066 RDRAM and it definitely ran hot.,Positive
AMD,Yep. Painful thing for me I was looking around at a good upgrade from my 3600. I waited two weeks before I went to buy new. They were cancelled the week before.,Negative
AMD,Sold mine for the equvialent of 400 USD because going AM5 would cost double now.,Neutral
AMD,Thank you.,Positive
AMD,You could always download more though,Neutral
AMD,Cars are fairly cheap. It’s all the RAM needed to run the ECU and BCU the drive up the cost. /s,Neutral
AMD,It's 2025 and I'm still boggled that there are people who still don't know about this.,Negative
AMD,Is this a good deal?,Neutral
AMD,"Probably a niche use case, but I got one of these because it works on a low end PSU. I had a 6600XT and wanted a better GPU but couldn't be bothered replacing the old 450w power supply that came in the prebuilt I bought during COVID.",Neutral
AMD,I'm hoping to see a 9070 XT for $550. I've been tempted to make the drive to microcenter for it at $580.,Positive
AMD,"Correction , Asus Duel 9060 XT is on sale.",Neutral
AMD,Is this better than a 7800xt? Sorry I’ve been out of the scene for a while.,Neutral
AMD,"Started seeing these prices locally making them affordable for the first time this year. Unfortunately, 9070 variants cost twice as much ...",Negative
AMD,"If you’re thinking about it get it now word is prices are going up due to memory, learn your away around optiscaler",Neutral
AMD,Barely outperforms a 6700XT and costs more than it did. This is so pathetic,Negative
AMD,9060 xt msrp is $349. This is $10 off. Why is this even a post?,Negative
AMD,"It's the cheapest it has ever been, but just by $10.",Positive
AMD,"If it were euros and including taxes, kind of (in the expected range though). But it's us dollars, so depending on how much taxes you still have to pay, it seems not like an amazing discount.",Negative
AMD,Yeah it seems pretty solid. Most are around $360 by my local pricing.,Positive
AMD,"The sapphire pulse is 350 euro here now (363 I got it for 2 days ago, trying to get the difference back lmao).  Might be worth the price difference. The card itself is fantastic. Huge upgrade from my 1070ti",Positive
AMD,Yup!,Positive
AMD,How do you like the upgrade? I also have a 6600 XT and have considering the 9060 XT.,Neutral
AMD,In Sweden I just bought 9070 XT for 7000 SEK. We got 25% VAT here so it roughly translates to $585. Felt like a good deal.,Positive
AMD,Just pull the trigger. You aren't going to see better deals than that Microcenter one. Prices are expected to go back up after Black Friday due to the RAM shortage. Stock for computer parts also depletes faster as you get closer to Black Friday.,Negative
AMD,Best buy price matches microcenter I believe,Positive
AMD,I swore I saw one on Newegg a few days ago around tha price,Neutral
AMD,$580 is really great for what you get,Positive
AMD,"I wish we could get those prices in NZ, we pay $750USD on sale 🫠",Negative
AMD,"you have 9070's riding around that price, after shunt mod it is essentially a 9070xt if it had below avg. silicon lottery",Neutral
AMD,"No, but it has support for FSR 4 if you want that feature.",Neutral
AMD,Nah it's closer to a 7700 XT/6800 with better ray tracing.,Neutral
AMD,">Barely outperforms a 6700XT and costs more than it did. This is so pathetic  Where do you get your false information from? 9060 XT 16GB is on average 30-35% faster than 6700 XT, for raster and +60% for RT, according to multiple publications.  [https://www.computerbase.de/artikel/grafikkarten/rangliste.2487/](https://www.computerbase.de/artikel/grafikkarten/rangliste.2487/)",Negative
AMD,I bought my 6700xt on the launch day :/,Negative
AMD,so real theres basically no good reasonably priced upgrades you can do from a 6700XT this gen,Negative
AMD,Finding these cards at MSRP is pretty tough,Negative
AMD,If gpu prices go up from all the AI shit like ram is then could be the lowest it goes for a while if it's just FUD to sell more gpus than still pretty good.,Negative
AMD,You can get the 16GB models for 340€ in Germany.,Neutral
AMD,"It was worth it for me. I play at 1440 and it means I can use some level of ray tracing in games like Cyberpunk, and Warzone is super smooth for me. Also I like to play around with local LLMs, so having the extra VRAM is helpful.   I'm just lazy and couldn't be bothered with the effort (and expense) of replacing my PSU, so this was a nice way to breathe some new life into my machine without spending a fortune.",Positive
AMD,"It's a superb GPU. I have one, thanks to FSR4 I can run Cyberpunk at Ultra Settings with Ultra Raytracing and still get above 60fps. In 1080p of course.   FSR4 really increases the value of this card.",Positive
AMD,"Forgive my ignorance, but how are RAM prices this black friday season (it feels like one at this point)? Are they somewhat normal or still asinine? Hoping to be able to upgrade to AM5 but if the RAM is still expensive then I'll just wait another year. Thanks!",Negative
AMD,Only within 20ish miles of the microcenter store. Unless its online.,Neutral
AMD,"I think Microcenter does the same within a certain mile radius, I remember they price-matched my previous PSU with the same model from BestBuy and I ended up getting like $40 off it was sweet",Positive
AMD,"I have been running my XTX at stock voltages but EVC modded up to 750-800W on water since it came out and my 24/7 tune file that I load to this day is dated like May 2023, so it's not like it's damaging it. The 4/5nm node is pretty fucking tough, just like the 7nm node was. As long as you don't exceed stock max voltage on a GPU it is surprisingly difficult to hurt in my experience.",Neutral
AMD,its p much 1-1 a 7700 xt just with fsr4 and improved raytracing,Neutral
AMD,Techpowerup's DB has it at 15-20%.,Neutral
AMD,"In germany the regular price is 350 to 360€. Anything going for cheaper is discounted (see notebooksbilliger.de, mindfactory.de and amazon.de)",Neutral
AMD,"Honestly you’ll be waiting until the AI bubble bursts, which might be a while. The prices are constantly going up on ram. Some YouTubers even say it might be better to buy a pre build that hasn’t had the price increased because of the ram it has installed, then selling off the parts you don’t need.",Negative
AMD,The efficiency gains are nothing to scoff at either. 245w for the 7700xt to 160w for the 9060 xt 16gb.,Positive
AMD,TPU DB has it at 25%.   [https://imgur.com/a/z2AKrn7](https://imgur.com/a/z2AKrn7)  Cut the BS. It stinks from a mile away. Or get some glasses.,Negative
AMD,"You answered the person asking if this was a good deal with ""if it was this price in euros with the tax included it would be"". To which I pointed out that you can get exactly those deals currently.  I have seen different models on sale for 339€, tax included.",Neutral
AMD,"My bad, I misunderstood. I thought you implied 339€ as the regular price.",Neutral
AMD,"I bought this from aliexpress in the summer for a bargain £350 barebones, spent what ever I had left on the 96gb Ram. lt runs cool and quiet - great for local AI and occu-link mean that it will be used for a long time.     Did you manage to get to the drivers - [https://www.toptonpc.com/driver-download/](https://www.toptonpc.com/driver-download/) its here on this list but  there is a link but its a failed link.",Positive
AMD,Link is broken for me too. On Windows all it takes extra is the Intel 2.5G i266 driver I think.,Negative
AMD,So all you need is AMD drivers and chipset and i266 driver and Bluetooth WiFi . Cool cheers for feedback. Great review,Positive
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,these names are going crazy lmao,Neutral
AMD,"If those GPU specs are true, then these are going to lose the iGPU race next generation. Between the strong showing from the new Adreno iGPUs and the upcoming B390 in Panther Lake, I don't think RDNA3.5 is the right move going forward.  Of course drivers and compatability will be a problem for Snapdragon, but being triple-channel is going to help it alleviate the bandwidth woes of any iGPU. Panther Lake's GPU is looking to be 40-60% faster than Lunar Lake and Arrow Lake, which were close to the 890M already. I don't think they'll be getting that much more out of the 890M unless there's a way to massively increae the bandwidth it has.  On the CPU side, it's fine. Zen5 is far from slow and will be perfectly acceptable for basically everybody. It's a shame to not see Zen6 on mobile given the improvements it's supposed to bring, but maybe that's something for the 500 series to come along with a UDNA iGPU.",Negative
AMD,Gorgon point used same GPU as strix point? What is even the difference here?,Neutral
AMD,Stop recycling products... Stop castrating IGPUs too. That's ridiculous.,Negative
AMD,"AMD is the company that has the pieces of the puzzle (RDNA4, Ryzen Max+ 395), but will fail to execute either in availability, or in offerings. Meanwhile Apple is slaying it in the hardware department year after year... (they are miserable in software and pricing however).",Negative
AMD,Slightly disappointing that the NPU has the same architecture as the AI 300 series with slightly higher TOPS,Negative
AMD,curious to see if Gorgon point has the low tdp gains that Hawk Point had over Phoenix,Neutral
AMD,More CUs,Neutral
AMD,"of course...  amd is staying still since intel is also doing that  panther lake, from leaks is barely any faster vs whatever lake it was beofre",Neutral
AMD,i have a hx 370 and i thought i would be missing out on more tbh,Negative
AMD,"By graphics it would be just an update, still the same gpu",Neutral
AMD,strix/krackan++,Neutral
AMD,All eyes on Medusa Halo,Neutral
AMD,"Looking forward to the next versions, “devils ballsack” and “hitler’s abortion”.   https://youtu.be/pjklCsDxG60?si=U-vMWeEHp2WZKg60",Positive
AMD,> going crazy  been crazy,Neutral
AMD,"Different market, but I think Apple will be the one to chase on benchmarks in 2026.",Positive
AMD,We can't afford the ram for the igpu's in the next few years anyway,Negative
AMD,+100-200mhz,Neutral
AMD,"The difference will be the price, as AMD will market these rebranded SKUs as if they were brand new.",Neutral
AMD,"AMD makes a “new” product every two years just like their desktop cpu and GPU divisions.  The “mid cycle refresh” is “non tapeout improvements” which is usually firmware, clockspeeds, and potentially power saving algorithms.  Flexibility is built into the silicon.",Neutral
AMD,"You're comparing a Giant with an underdog. Apple has a market cap, revenue and profit 10x of AMD's. They have almost an infinite resources in R&D compared to AMD to access the top talent in the industry. You can't expect similar or close competition if you understand the whole picture.",Neutral
AMD,Not only NPU but everything else too,Neutral
AMD,This is just a new revision of an existing design that can reach slightly higher clock speeds.,Neutral
AMD,"Nope, identical gen on gen.",Neutral
AMD,I was an early-ish HX370 adopter (September 2024) so amd won’t have anything better-in-class until 27 months since I bought my laptop.,Neutral
AMD,Oh definitely. The M5 is already well clear of the competition and unless they fumble the M6 badly they're going to stay in front. Bigger M5 variants are going to be untouchable for the PC space save for either AMD or Intel pulling out another mega-APU.,Positive
AMD,Apple is in a different league performance- und price-wise than Strix Point / Gorgon Point. The competition to Apple is Strix Halo / Medusa Halo.,Neutral
AMD,"Most don't even consider Apple, no matter if they are twice as fast.",Neutral
AMD,"Ok, but AMD is focused on CPUs/GPUs, while for Apple it's only a part of their business. Apple drops the M5 range for 2025-2026, another important step forward, available today...  while AMD is rebadging **the same** 2yr old CPU (Zen 5) and 4yr old GPU tech (RDNA 3.5) for 2026. And we'll probably need to add +6 months for broad availability (from the moment of announcement).",Neutral
AMD,"Supposedly the m6 designs are getting big redesigns, plus the move to 2nm.  M5 was a massive improvement with essentially getting tensor cores.  They just updated macOS to support them this week.  Supposedly it’s a huge leap in AI workloads.",Positive
AMD,Not for the standard M4/M5 laptops. Strix halo competes with M4/M5 Pro and Max and even amd compared strix halo with M4 Pro.,Neutral
AMD,The apple design team is as large as the amd one. And AMD doesn't have thr benefit of first pick at TSMC.,Neutral
AMD,"+1, the memory bandwidth of the base M5 is similar to strix halo. Macs still have limited game selection so they can’t compete for a gaming system, but the performance is class leading and on the productivity side it’s a different story.",Neutral
AMD,"Macs lack game, but if you are looking to game a $1000 laptop with a 5060 is a better deal than anything else in a laptop.",Positive
AMD,For the low low price of $1800.,Neutral
AMD,"Can someone please just put this chip in a freaking laptop.  I feel like these handhelds have gotten so gigantic you could knock someone out with one. How are these things even comfortable to hold? I'm starting to wonder if they could just put a dGPU in a handheld and call it a day at this point, with external power supplies and active cooling.  Remember when a handheld could fit in your freaking pocket. I kinda miss the GBA days. If cartridges weren't so expensive I might be tempted to bust out the SP. That was a dope handheld and I'll bet it still is.  The only way I could even fit a Switch Lite in my pocket is if I put it in some oversized cargo pant shorts, and they swing back and forth and slap me in the kneecap.  Dual fan. This has gotten completely absurd.",Negative
AMD,"Damn, this is the most interesting of the ai max handhelds by far. Finally a screen to match the performance. Clench your cheeks for the price, though.",Positive
AMD,What the hell is with the screen resolution? 1504p???,Negative
AMD,"Why? I mean it has good performance, but something 1080p is more than fine enough at that size, and you can either get more battery life or more performance out of it.",Positive
AMD,"Honestly im into it as long as pricing is good, for 250 its a good budget card though id like to see it for less as a 9060xt 8gb can be had for 275atm. 220 would be amazing and would help bring people into gaming for a much cheaper price, theyd have access to fsr4 too. Itd drive down b580 pricing also. Rising tide lifts all boats situation",Positive
AMD,Actually impressive for the price,Positive
AMD,What is the best graphics card?,Neutral
AMD,5050 is a better card in my opinion,Positive
AMD,Bought a Sapphire 9060xt 16GB. I was not able to get the performance upgrade expected from my 1070GTX. I was getting 30% to 50% of the fps I had with the 1070. I troubleshoot and tried about everything. Memory uses appears to be capped ate 4GB. Don't know what I was missing with it.,Negative
AMD,"probably won't drive down b580 pricing, margins are already razor thin. Honestly nothing wrong with using 8gb vram cards if the price is right, only problem with these cards is that they're usually overpriced",Negative
AMD,No pricing on this one. It's OEM exclusive.,Neutral
AMD,Yup no bad GPUs just bad prices,Negative
AMD,I miss the value of the RX 480/580.,Neutral
AMD,An RX 480 4GB with a mining BIOS of course.,Neutral
AMD,RTX Pro 6000 Blackwell?,Neutral
AMD,How? This RX 9060 competes directly with RTX 5060 within the same or less price bracket.   Your comment seems like ragebait,Neutral
AMD,The 9060 will possibly only be $10 more than the 5050 if it goes on sale outside of OEMs. It’d be a horrible idea buying that GPU.,Negative
AMD,You're high,Neutral
AMD,You can't be serious...,Negative
AMD,"Holy crap that's a lot of downvotes. Congrats, lol.",Positive
AMD,Only if you are a braindead Nvidia fanboy. lol,Negative
AMD,9060 non XT competes in performance and usually faster than 5060 and you're saying 5050 is better? HOW.,Neutral
AMD,🤡🤡🤡,Positive
AMD,REEEEEEEEEEEEEEE,Neutral
AMD,I think you must be CPU bound or playing on low resolutions.,Neutral
AMD,I mean if they dont lower margins then they wont sell. Im not saying theyll make money but they have to sell yk,Negative
AMD,"I thought the 8GB VRAM cards were actually ok. It's the 16GB cards that are a bit overpriced.  The 9060XT for example the 16GB ones are $350-400. 8GB is $260-$320.   It's good, but IMO not $70-100 better than the 8GB cards, and if I was buying at the 16GB 9060XT/5060Ti level might as well stretch for the 5070/9070 which are significantly faster.",Negative
AMD,Ofc but in the off chance the do release it,Neutral
AMD,5060 is priced like the 9060 XT 8 GB and loses universally to it,Negative
AMD,That’s why getting 5050 for less and same VRAM and DLSS make sense,Neutral
AMD,It will only go on sale second hand. This card will NOT be sold firsthand on diy builder markets.,Neutral
AMD,My CPU is a Ryzen 7 5800x. I didn't change any settings and tested a couple games.,Neutral
AMD,I just bought the sapphire pulse 16gb for 363. Upgraded from a 1070ti lmao    What a difference to run games on ultra now with ray tracing,Positive
AMD,The price of ram going through the roof is likely to make any 16gb card stupidly overpriced at this point now anyway sadly,Negative
AMD,"I agree. I also do think the poor optimization of newer games at 1080p is also to be blamed as to why 8GB is slowly reaching its limit. For old titles, it's still sufficient.",Negative
AMD,So you feel it is a better value card  Not a better card  It depends on the price I suppose but a cost per frame analysis is hard as it's only in prebuilts,Neutral
AMD,How is getting a slower card for more money makes sense? So what if it has DLSS. 9060 has FSR4.,Neutral
AMD,Are you on drugs?,Neutral
AMD,It’s a two generation old CPU. So yeah it kinda is the CPU. The upgrade from 5000 to 7000 is pretty big if you dont have s 5000x3D CPU,Neutral
AMD,"A 5800x will not bottleneck a 9060xt, the problem is somewhere else.",Negative
AMD,You can ignore what I say. The difference in gaming performance from 5000 to 7000 is pretty significant.,Neutral
AMD,"Will Zen 6 be the final generation for AM5? In that case, I will finally upgrade from a 5900x",Neutral
AMD,Yay buzzwords!,Positive
AMD,"i was kinda dissapointed, they basically didnt give the smallest hint about next gen gaming GPUs  and Zen 6 also, they didnt tell anything basically",Negative
AMD,I want more raw performance i dont give a shit about ai man🤦🏽,Negative
AMD,"Heh, I mean what else do you expect? AI generation and ray tracing is clearly the way to go",Neutral
AMD,Link doesnt work,Negative
AMD,Tensor cores…? On a Radeon GPU?  I think this kind of mistakes are not reasonable on specialized press.  What a POS.,Negative
AMD,I would honestly be happy if AMD dumped all efforts to push ray tracing and focused on GPUs that ran more efficiently.,Positive
AMD,Zen7 is rumoured to be on am5 and it kind of makes sense considering timelines for ddr6 ram.,Neutral
AMD,Looking like Zen 7 will still use ddr5,Neutral
AMD,"There’s a rumour that Zen7 might come to AM5. It’s just a rumour but it indicates that AMD is still deciding.  I think part of it comes from Zen7 & DDR6 release dates and the uncertainty of DDR6 prices and spec at launch.  DDR5 launch spec were slower than DDR4 because of similar or just a little bitter speeds but loose timings. And DDR6 might come with a similar problem as well.  I’m totally in for Zen7 being released for both AM5 & AM6, not just to make AM5 last longer but to give consumers the option, especially that going AM6 will be expensive for early adopters making Zen7 adoption slow if it was AM6 exclusive.  As for AM6, I really really hope it comes with extra PCIe lanes, like 4-8 lanes, I would hope for 12-16 but it not realistic unless they limit those 12-16 lanes to high end chipsets only (the E series) and can target workstation/server grade motherboards too. Currently AM5 high end, server & workstation grade motherboards are boring because of this, and the only way to get more PCIe lanes is to jump to Threadripper which is a very big jump considering how much more expensive it became compared to first generation Threadripper.",Neutral
AMD,"Nice! Sitting on the 5900x myself, tempted a few times but reading about the 9000 burns and stuff - no thanks I'll keep this beast.",Positive
AMD,I think i'm getting on Zen 6 and was about to get a x870 board in advance but it looks like Zen 6 will have a new chipset and that's probably actually going to be impactful on this processor. I won't bank on Zen 7 being on AM5.,Neutral
AMD,Most likely.,Neutral
AMD,"Considering zen 6 is most likely coming in 2026 from their roadmaps, they probably are still only in the early stages of zen 7 (at least the desktop variants which while the core dies are the same or similar, the IO die is completely different). I'd wager they have an idea of which platform they would like to stay on, but it could change within the next year or so before they send tape outs to TSMC. That being said, I think they'll stick with AM5 for zen 7 while zen 8 (or whatever they end up calling the architecture) will be the switch to DDR6. DDR6 is supposedly arriving in 2027 which doesn't leave much time for a whole new platform to be developed for 2028/2029, even if AMD started in 2026.  That's also not including potential delays for the DDR6 spec.",Neutral
AMD,Seriously. Unless they magically show they can match what NVIDIA is doing its just that.,Negative
AMD,"Brute forcing everything doesn't make sense. There are areas where you want to use compressions, approximations etc. Ai can help with these. The important part is to use it where it makes sense.",Neutral
AMD,For developers who want to pay less to develop shitty games.  You can take AI frame generation and the performance hit of raytracing and shove it.,Negative
AMD,Thats funny because that's not what this sub was saying for the last 4 years. They were saying Ray Tracing isnt worth it and a dumb endevor shilled by NVIDIA.,Negative
AMD,AI generation so they name them after monsters? interesting,Neutral
AMD,"They use matrix cores, also the new hardware features were disclosed by AMD and Sony already in a joint video. They will now have hardware BVH traversal on RDNA4’s successor in the form of “radiance cores”. ML continues to improve with neural arrays. Plus there’s a new compression scheme to improve memory performance they are referring to as universal compression.",Positive
AMD,"Pasting as both [a link](https://videocardz.com/newz/amd-lists-next-gen-ai-raytracing-for-future-radeon-gpus-confirms-ryzen-gorgon-and-zen6-medusa) and plain, unformatted text in case that makes it easier:  `https://videocardz.com/newz/amd-lists-next-gen-ai-raytracing-for-future-radeon-gpus-confirms-ryzen-gorgon-and-zen6-medusa`",Neutral
AMD,Link works fine for me.,Positive
AMD,Nvidia did not invent tensors.,Neutral
AMD,Matrix-cores.,Neutral
AMD,those will be over 3k cost and req between 500 to 800 watts.  fyi you can vm multi cards in 1 card. nvidia does the same.,Neutral
AMD,"There's a lot of options for keeping performance on DDR5 for a while.  They could implement some kind of consumer standard for MRDIMMs.  They could add another stack of V-Cache.  Heck, we might even see some improvements via 3D DRAM if that becomes a thing in the next year or two.",Positive
AMD,Might be they release a AM5+ with PCIe6 and more USB4. I also think AM6 will be when DDR6 is ready.,Neutral
AMD,And probably still not be able to get anywhere near the frequencies DDR5 supports,Negative
AMD,"Which DDR generation it supports will be dependent on the IO-die, not the CCD.  AMD could make a Zen 2 chip with DDR5 support if they wanted to, or a Zen 5 chip with DDR4.",Neutral
AMD,A rumor from an unreliable source is not indication of anything.,Negative
AMD,"Why not, ddr6 won't come before 2029, zen7 is 2028 most likely",Neutral
AMD,"Yeah, AI upscaling like FSR4 and dlss is one of the big actually useful use cases for AI that I encounter in my day to day. FSR4 is good enough that I'm happy to play with 50-60% resolution in most games which is an insane performance boost. And imho it looks better than the blur from cheap AA so you get even more free performance from not needing AA and the image looks better.  And apparently there's a bunch more stuff you can upscale with ai, I saw a demo where it upscaled indidual textures and my understanding is you can do similar things with shadows and lightmaps.  I feel like 5 years from now modern rendering is going to be some crazy black magic with AI upscaling all over the place that allows you to cheat and only have to render like 20% of the stuff you'd have to do with normal rendering.",Positive
AMD,"Thats valid, they just keep pushing ai this ai that, and i have yet to see any real benefit in my opinion.",Negative
AMD,Ty!,Positive
AMD,"Doesn’t work for me neither. Had issues with a different link today as well, and I never really do. Maybe it’s a Reddit app issue on mobile 🤷‍♂️",Negative
AMD,Zen 6 already will have some memory controller changes as 2-DIMM motherboards got revisions earlier this year to be able to boot it.,Neutral
AMD,"I can see 9000-10000 mhz ram being a sweet spot for zen7, that would probably be enough of a performance increase two gens from now.",Positive
AMD,">I also think AM6 will be when DDR6 is ready.  I agree. That'd line up with how AMD's been doing things since the AM2 days.  >Might be they release a AM5+ with PCIe6 and more USB4  Given what they've been doing since Ryzen 3000 series, i think i'd be more likely that they do it as a chipset thing, a la 500 series chipsets supporting PCIe 4.0 on AM4. You could use Zen 5000 series chips on 300 series chipsets with the appropiate BIOS version, in some cases, you'd also lose support for older parts, and the newer CPU may become limited to the PCIe version of the Motherboard (3000 or newer CPUs being limited to PCIe 3.0 on 400 or older boards)   AM5 may go a simillar route, the upgrade path is one of the things that are attractive about AMD so I don't think they'll revise the socket until DDR6 is a thing, and they move onto AM6. Before Ryzen, there was some compatibility between regular AM2 and AM3 sockets/CPUs and the + versions, but it was a bit of a mess. I don't think AMD would want to go back to that.   The 800 and 600 series chipsets are confusing enough already. B850 supports PCIe 5 while B840 doesn't and B650E does   What i could see happening if Zen 7 comes out before DDR6: 900 Series chipset with PCIe 6 and more USB 4 lanes (, as long as the CPU supports it) still on the AM5 socket.",Neutral
AMD,That is a possibility yeah,Neutral
AMD,We'll see. A lot can happen,Neutral
AMD,"Sounds fine to me. Unless AMD is showing significant issues that are a cause of ram bandwidth in relation to their counterparts, I see no reason why they need to be using max ddr5 frequencies. If they're still getting the performance, saves everybody who builds a system money not having to buy the top specs stuff",Neutral
AMD,"AM5 can already get above [10,000 MT/s](https://valid.x86.fr/7z1sg6).",Neutral
AMD,What? Zen4 and 5 can already do it...,Neutral
AMD,"Yea no shit.  What makes Zen 7 an AM5 chip is the release date. DDR6 won't be ready until 2029. Zen 7 will be released before then, and AMD is not going to make another socket just to change it a year later",Negative
AMD,"If multiple sources leaked the same rumour, then there's some base to this rumour, again I already mentioned it in my previous reply that it's still a rumour, but indicates that AMD is still deciding Zen7 things.",Neutral
AMD,It's not just that.,Neutral
AMD,You're woefully misinformed if you think 6000Mt/s the highest DDR5 can go,Negative
AMD,"Can you show me this rumor having more than one actual source.  So not just two places reporting based on the same rumor?    And even then, it's still not a sign that a rumor has any base.  Plenty of rumormongers play off each other's rumors for clout, even when there's no genuine source for the information.",Negative
AMD,"Maybe not, but that line of thinking still really bothers me. lol  It's wild how much people dont realize how easily manipulated they are if they think some 'rumor' is indication of something being true.    We desperately need critical thinking lessons in schools growing up.  And not just one, but new and refresher lessons in every couple grades at least.  It's a skill and it needs to be reinforced and practiced.",Negative
AMD,"You are the misinformed one, as they can do 8000 ram (or even more for apus).",Negative
AMD,"There is critical thinking behind it, no worries. Otherwise I agree.",Neutral
AMD,"If you think 8000 is the most DDR5 can do  you have no idea what you're talking about  AMD hasn't been able to deliver competitive IO since before Bulldozer, haters gonna block because they're incapable of debate",Negative
AMD,"Wasn't talking about you, but the original poster who said that a 'rumor' is somehow proof that AMD is deliberating on it.",Neutral
AMD,"It's in the upper range, not counting extreme OC which is still done with zen4/5 apus. So however you put it, you are the one who has no idea what he is talking about. Also I know that you are just trolling so you are not worth my time.",Negative
AMD,I’m interested how many hours/minutes of teams/zoom call can it do.,Neutral
AMD,Remove numpad add haptic touchpad,Neutral
AMD,"I would not even consider a work laptop who dont have a numpad. Different part of the world, different market, different usage, different priorities, thats all. But in lot of use case its an essential feature.   And its not like we lack options without it.",Neutral
AMD,"Yeah, I really can't stand the numpad on these laptops. You don't really need it for anything in particular and it's only kind of convenient for a few select tasks. On the other hand, it's a big hit on ergonomics to have the part of the keyboard that you actually do need 99.9% of the time shifted to the left. Aesthetics suffer as well, plus you lack the convenient panel left and right for speakers (so to no surprise the review concludes that speakers are bad).",Negative
AMD,"Unfortunately, as far as those larger 16"" laptops are concerned, there is a definite lack of options.",Negative
AMD,"Why can't they just make the same laptop with two different keyboard options? There isn't a single premium 15""+ high performance laptop on the market right now without a numpad.    For me it's much more comfortable to use a laptop with a centered keyboard, it makes a huge difference for the hand angle.",Negative
AMD,"It's a business laptop, that numpad is probably a feature people are looking for in a business laptop",Neutral
AMD,"Do you walk into a Casio store and ask for a Rolex watch? A business laptop at this size without a numpad wouldn't make sense, maybe you can find 16"" laptops without numpad from other premium lines",Neutral
AMD,Agree with you 100%,Positive
AMD,"I would have agreed with you a few years back. But i feel like i have seen a good number of them these lasts 2 or 3 years. Primarly for the gaming segment of course (since they lack a numpad ! :D ), but even some thin and light.",Positive
AMD,Exactly. No numpad on an Excel machine is a dealbreaker.,Neutral
AMD,"The most important characteristics of a laptop are battery life ( power efficiency) and screen quality. That is what sells ( non apple ) clamshells.    All the other crap they test in various reviews are mostly meaningless to the actual user. There is a very small %age of the market for high power/perf laptops and even smaller market for gaming.   Most of the reason a laptop is “slow” is bloatware and has nothing to do with the cpu choice anyway.  And Intel is already “beating” AMD in laptop cpu sales, by a substantial margin. People incorrectly assume AMD has most of the market share in all segments because of the very noisy and super-biased gaming reviewers, who mostly focus on $3000+ gaming desktop builds. Yes AMD is handily winning there.",Neutral
AMD,"I have a T14s Gen 5 Core Ultra 5 135U and recently I saw \~9h of battery life, browsing websites. It's quite nice piece of hardware so with Lunar Lake it would be just perfect. Of course not for heavy workload because for this we will have Panther Lake. I work in tech for years, not an expert in laptops area but I can assure you that new CPUs from Intel that I mentioned earlier are way, way, way better than previous generations.",Positive
AMD,"Intel beats AMD in software (drivers, firmware) … I got think pad 780M laptop by company I work for. Randomly display won’t get detected. Randomly audio device goes missing. Not fun thing to reboot your laptop and miss 10 minutes of meeting.",Negative
AMD,"Soldered RAM sucks.  Nothing beats popping out the standard 8GB stick(s) a notebook may come with, installing a couple 2x32GB sticks yourself and having it actually work.",Negative
AMD,lol. Even in the cons it says weaker multicore than AMD….?   This article seems like AI wrote it,Negative
AMD,"Unfortunately Intel abandoned the on-package RAM after Lunar Lake again, which is the primary reason for the great efficiency and low power usage. I kind of understand why, it's expensive and not very flexible, plus apparently the market doesn't actually care that much about long battery runtimes. Only a small minority of people are ready to pay premium for this.",Negative
AMD,At work we are a Lenovo shop and recently swapped from Intel to AMD T14 laptops. Too many issues with Intel and the AMD models offer the same performance for less money.,Negative
AMD,Suck at gaming.,Negative
AMD,Try a modern mac and tell me its not the CPU holding the UX back. It's all about the cpu's.,Neutral
AMD,"That would be nice. We have a bunch of laptops with Intel's i5, 13th gen I believe it was. 2 p-cores, some e-cores. They are all slow as fuck. I mean it. The CPU is so extremely slow and goes into tdp limit right away. Most users hate them.  Battery life is ok, but they are really bad in terms of performance.  So - I wouldn't say CPU doesn't matter.",Negative
AMD,"AMD is held back in laptops by some shady deals of laptop manufacturers with Intel. It's impossible to get a 4k AMD laptop with 5090 for example, all of those are Intel (I found literally one AMD laptop like that compared to 25 from Intel). That's utterly ridiculous.",Negative
AMD,"Yeah, so much of laptop performance is dictated by things other than the cpu. Its kinda wild.   Intel does a way better job getting good laptop designs. Amd historically has just been a cpu seller, telling oems to go wild and do whatever . . . And it always ends very badly.   The biggest sign amd is taking share is not cpu benchmarks, but will be things like having premium screens, good thermals, lack of bloatware, dual channel memory, good SSDs, good colors, etc etc. and . . . ACTUAL AVAILABILITY. I dunno how many reviews i see where they review and intel and amd parts. Usually there is some way intel has a better premium finish. And then amd just has zero ways to actually buy their model. Its the weirdest thing.",Negative
AMD,"But haven't you heard, AMD beats Nvidia slightly at linux gaming benchmarks.  That means AMD has the best software support.",Positive
AMD,"I miss the times when laptops were far more upgradeable. I got a budget laptop for college with a low-end dual core, a spinning HDD, and 1 stick of 2GB RAM. By the time I retired it ~6 years later I've upgraded the CPU, replaced the HDD with a SSD, and added 2x4GB sticks of RAM. I also could've swapped out the network card and even the DVD drive for another SATA drive, but never got around to those.",Neutral
AMD,Soldered ram is a lot faster. So no.,Neutral
AMD,Yes but now RAM costs a ton of money,Negative
AMD,Is multicore performance the only consideration when buying a laptop?,Neutral
AMD,What kind of issues?,Neutral
AMD,It is not a gaming laptop,Neutral
AMD,"Yep. More specifically, it's mostly the single thread performance and efficiency.  It's how a MacBook Air can be fanless, run super fast, and stay cool at the same time while you're doing work with it.",Positive
AMD,It's an enterprise grade product you buffoon.,Negative
AMD,Build quality.    Thinkpads are solid machines that are easy to fix.    It's one of the few laptops that comes close to MacBook quality and everything judt working.,Positive
AMD,Exactly. Thinkpads are not targeting average Joes. They are targeting business and enterprise customers. Their Yoga and Ideapads are targeting the regular Joes.,Neutral
AMD,"Because only on Linux, Valve heavily funds AMD driver development and they also get other community contributions. The commonly used RADV vulkan driver was started as a community effort without AMD involvement.",Neutral
AMD,"I hope you're joking(sorry if you are), cos Nvidia isn't actually a good benchmark for software support on Linux. Intel is so much better at Linux software support than Nvidia",Negative
AMD,"The question is, is the soldered ram you're buying in a laptop faster than the ram you can buy and install yourself?",Neutral
AMD,All the more reason to make it upgradable,Positive
AMD,Lunar Lake isn’t how Intel beats Amd lol. Panther lake will stop a lot of the bleeding for sure. AMD is so far making mostly good moves and Intel is as well with LBT. The goal for Intel over the next 3 years is stop losing customers base. I will point out Intel still has about 75% of all x86 customers.,Neutral
AMD,"I mean, the only other pro is, that you cannot get the 2.8K panel on the AMD version for some reason..... sooo",Negative
AMD,"It's $2,000 so no excuse.",Neutral
AMD,"Usually yes, soldered ram will be faster. And in case of lunar lake, it is faster and more efficient due to it being packaged with the cpu. Like Apple's unified memory.",Positive
AMD,"Lunar Lake already beat AMD, nobody buys AMD laptops",Neutral
AMD,i stopped at $2100 for a Thinkpad T14,Neutral
AMD,wrong  Nobody Supply AMD laptop     There fixed for u,Negative
AMD,"I do, and many of the people I know do.",Neutral
AMD,Nobody pays that much.,Negative
AMD,"Not anymore, there are plenty of AMD laptops on the market, of course - depending on region.",Neutral
AMD,Try the shunt mod,Neutral
AMD,"Cool, errr...  icy",Neutral
AMD,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Neutral
AMD,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Positive
AMD,did you use dry ice? how did you hit sub-ambient?,Neutral
AMD,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Neutral
AMD,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Neutral
AMD,Are you in the US? If so how were you able to get Maxsun?,Neutral
AMD,Oh... for sure 😁,Positive
AMD,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Positive
AMD,Car coolant in the freezer 😁,Positive
AMD,Great work dude! Only 200MHz to go 😉,Positive
AMD,That's the way! Let us all know the results.,Positive
AMD,I am in Australia.,Neutral
AMD,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Neutral
AMD,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Neutral
AMD,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Positive
AMD,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Neutral
AMD,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Neutral
AMD,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Negative
AMD,Oh! You put the car coolant to run through a freezer? Wow! Nice,Positive
AMD,And largely against the non-x3d lmfao.,Negative
AMD,Aren't they just showing that AMDs CPUs are better for gaming?,Neutral
AMD,"Whoever downvoted my comment, why not run your own benchmarks and compare your results with mine? You could make posts asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming. Or how about go to tech sites and tech tubers asking them to provide their benchmark scores. its disappointing  to know people prefer to stay in ignorance and prefer people getting scammed over knowing the truth, just face it reviews adjusted to satisfy the sponsor how do you think they make money.",Neutral
AMD,"I haven’t tried ARL processors yet, but based on my own tests, 14700K with DDR5-7200 is 30–40% faster in gaming compared to Zen 4/5 non-3D chips. In the five games I tested, 14700k matched 9800X3D in three, lost in one, and won in one. I have no idea how tech sites and tech YouTubers show non 3ds as being just as fast or only slightly behind when they are actually far behind.  14700k vs 9700x/7700x  [https://ibb.co/cSCZ6fdX](https://ibb.co/cSCZ6fdX)  [https://ibb.co/999WzW4T](https://ibb.co/999WzW4T)  [https://ibb.co/Gjw1nXX](https://ibb.co/Gjw1nXX)  14700k vs 9800x3d  [https://ibb.co/FbMNL40q](https://ibb.co/FbMNL40q)  [https://ibb.co/8nQXpYZ5](https://ibb.co/8nQXpYZ5)  [https://ibb.co/vvcpKsGY](https://ibb.co/vvcpKsGY)  [https://ibb.co/67Jn8tKr](https://ibb.co/67Jn8tKr)  RE4  9800x3d 179FPS  14700k 159FPS",Neutral
AMD,"i dont think proving you can do more work faster is going to turn around sells. more people exclusively game or game and do light work, than those who do heavy work at home on computers.  i dont relive there are enough youtubers and streamers out there to capture as there are general gamers",Negative
AMD,"I mean, yeah. A 9800X3D costs way more than 265K, while a 9700X is within 10 bucks of it.",Neutral
AMD,I assume they compared with CPUs in a similar price range,Neutral
AMD,I don't think these slides are as good as intel thinks they are. They are just an advertisement to buy x3d ryzen cpus lol.,Negative
AMD,I guess the point Intel is trying to make is that it's great for gaming and much better for content creation (aka work). It's also cheaper.  X3d CPU's are only better for gaming (and not in all games). Doesn't mean they are crap but you pay a premium for x3d.,Positive
AMD,"if you think you're 100% correct - go out and buy an x3d and run the tests, post a video with the results, and put an amazon affiliate link in the comments for a 14700k build that overall is better than an x3d.  there's nothing wrong with the 14700k, it's a beast. i think there's a few select games where it even beats the 7800x3d. but there's a reason that you can find hundreds of youtube channels with sub counts up and down the spectrum from 0 subs to 10M subs showing hardware side by sides with the x3d's beating the 14700k - and most of them don't have advertisers to please.  in my opinion, you're getting downvoted because you don't want to provide proof, you're out here saying it's tech tubers fault that you have no proof. you can also find a good amount of open source benchmark data at openbenchmarking.org, it's not pretty - but there's useful data there as well.  but hey - you're only hurting yourself by not breaking open the youtube techfluencer conspiracy around AMD processors. go buy an x3d, do all the testing, and use it as a springboard to free us from the lies of big tech. i'd love for you to be correct, but until you can provide proof, it honestly just looks like you have buyers remorse with your 14700k.",Neutral
AMD,">asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming.  [but it's not true?](https://youtu.be/IeBruhhigPI?t=13m3s) 1%low on 9700X is identical to 14700k/14900k, and better than on 14600k, AVG. FPS is slightly higher on 14700k/14900k, or is 9.7% higher average FPS on 14900k considered ""far ahead""? Plus, 14900K is not a CPU for gaming, it's a CPU for production workloads plus gaming, which makes it noticeably more expensive than 9700X, so realistically we should compare 9700X to 14600K/14700K.",Neutral
AMD,"Cherry picked results or just outright fake.  A 14700k isn't 30-40% faster compared to zen 4/5 non-3d chips.  Come up with a decent game sample first first before coming up with stupid conclusions.  Every reviewer I've found just happens to have different results than yours so everyone must be wrong except ""me"" right?  Even if the 14700k is slightly faster it still consumes a lot more power so theres that.  The numbers you can find from reviewers point to the 9700x being around 5 % slower, and you here are talking about 30-40% lul, if it was 30-40% faster it would be faster than a fking 9800x3d lmao.",Negative
AMD,I just built a 265K machine with 64GB 6800 RAM and it is no slouch. I haven't really played much on it yet as I've been working out of town a lot lately but my 4080S is now the limiting factor rather than the 10850K I had before.,Positive
AMD,RE4  9700x vs 14700k   [https://www.youtube.com/watch?v=FcDM07sgohY](https://www.youtube.com/watch?v=FcDM07sgohY)  [https://www.youtube.com/watch?v=-WO0HqajShY](https://www.youtube.com/watch?v=-WO0HqajShY),Neutral
AMD,"I did the same comparison, but it was 13900k vs 7800x3d and the Intel was much smoother. It's sad that you are getting down voted by bots who have never tried it and repeat tech tuber numbers. It's sad because ultimately the consumer is the loser when they are believing in lies.",Negative
AMD,"Now install windows 11, lol",Neutral
AMD,"> I haven’t tried ARL processors yet, but based on my own tests  Then whatever you're about to say has 0 relevance to the topic at hand.",Negative
AMD,"Clearly not as sometimes they use the 9950x3d, and sometimes the 9900x to compare to the ultra 9 285k.   One of those is the half the price of the other.  (And guess which one they used for 1080p gaming comparison)",Neutral
AMD,"And to add to that, which often gets omitted. Is that they are only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, intel in my mind actually makes more sense.",Neutral
AMD,"Why not tech sites provide built in benchmarks scores and let everyone veryfy them, and let us decide which processors better, why should we believe in charts blindly, why should I stay silent about what i found.",Neutral
AMD,"I sell PC hardware and get to test a wide range of components. I’ve built 7950X3D and 9800X3D systems for my customers, and when I tested five games back then, their performance was very close to the results I got with the 14700K. I’ll be able to test more games in the future if another customer asks for an X3D build. I never claimed the 14700K was better than the X3D in general I only said it scored higher in one game, Horizon Zero Dawn Remastered, and matched it in others. At first, I thought something was wrong with the build, so I made a post asking 4090 owners with X3D processors to benchmark HZD, and they confirmed my result. If I were to test 10 or 20 games, the 9800X3D might turn out to be about 5% faster, but I’m still not sure. Since I’m testing on 4090s, I can buy whatever processor I want, but based on the five games I tested, it doesn’t seem worth it. Also, the minimum frame rate was always a bit higher on the 14700K. X3Ds are generally good, but my main concern is that non-3D chips are 30–40% slower than the 14700K. Why does it feel like I’m the only one who knows this?  As for proof, I’ve provided built-in benchmark screenshots, phone-recorded videos (since OBS uses 15% of GPU resources), and OBS captures as well, which should give very close idea of processors performance. I’ve shown full system specs and game settings every time, and provided more evidence than any tech site or tech YouTuber ever has, yet some people are still in denial. I want just one tech site or YouTuber to show benchmark screenshots or disclose full system specs before testing. None of them do, and people keep trusting charts with zero evidence.",Positive
AMD,"Ive given my evidence by videos recorded by phone and obs,  scores screenshots prove 14700k is 30% faster than 9700x where is your evidence, talk is cheap, you either provide hard undeniable evidence or stay silent.",Neutral
AMD,"14700k vs 9700x  CP 2077 171FPS vs 134FPS 30%  spider man2 200FPS vs 145FPS 35%  SoTR 20%  RE4 160FPS vs 130FPS 25%  horizon zero dawn 172FPS vs 136FPS 30%  30% on average vs 9700x, 35%-40% vs 7700x  here is video recorded testing 5 games by OBS for 14700k vs 7700k  RE4 160FPS vs 110FPS  SM2 200FPS vs 145FPS  [https://www.youtube.com/watch?v=oeRGYwcqaDU](https://www.youtube.com/watch?v=oeRGYwcqaDU)  [https://www.youtube.com/watch?v=u6EPzOGR2vo](https://www.youtube.com/watch?v=u6EPzOGR2vo)",Neutral
AMD,"can you do a test for horizon zero dawn and CP built in benchmarks, also if you have RE4 can you go to the same place of the game I tested on and take screen shot showing what frames you are getting, I want all tests 1080P very high, if its not much trouble for you.",Neutral
AMD,"I only tested 5 games, and the two processors were very close. I’d probably need to test 10 or more titles to know for sure if the 9800X3D is actually faster. The fact that the 14700K came out ahead in Horizon Zero Dawn shows that there are games where Intel performs better.",Neutral
AMD,I did same performance on all processors.,Neutral
AMD,"It does, just like RPL being shown on corrupt tech sites as being as fast as Zen 5 in gaming, in the real world ARL could be much faster than they make it out to be.",Neutral
AMD,That sounds like an AMD Stan argument circa 2020,Negative
AMD,Lmao   https://youtu.be/KjDe-l4-QQo?si=GsvzFon8HYcrzrMc  theres no conspiracy mate,Neutral
AMD,"You need to grow up, evidence from some rude slime on Reddit is irrelevant for me, I value data from techpowerup, hardware unboxed or gamers nexus x100000 times more than anything coming from you.  I tried making this conversation constructive, but it seems that you're not adequate enough to even try. Good luck.",Negative
AMD,Can you explain me why in the video you are running a 7700x on 6000 cl40 RAM?,Neutral
AMD,"I challenge all tech sites and tech tubers to publicly share their built-in benchmark scores so everyone can verify them. By the way, I posted these scores on all the popular tech tubers pages on X (twitter), but none of them replied. I wish at least one of them would debate these results and prove me wrong.",Neutral
AMD,"Are your results with or without APO? APO supports SOTR and Cyberpunk, so give it a try if you haven't yet to see if it makes any difference.   https://www.intel.com/content/www/us/en/support/articles/000095419/processors.html",Positive
AMD,"I can do CP 2077 later. I have a 265k with a 9070, but only 6400 ram  (Mostly replying so I remember to do it)",Neutral
AMD,I only have Cyberpunk of the three.  [https://imgur.com/a/L3qMGva](https://imgur.com/a/L3qMGva)     I think the 4080S is holding me back too much in this benchmark.,Negative
AMD,"Your tests on win10, win 11 24H2 improved performance. All of your tests are complete nonsense anyway",Negative
AMD,I was just thinking the same. We have come full circle.  Time to buy some more intel stock,Neutral
AMD,It worked for them didn't it.  Why is it loads of people think AMD has dominated since original Zen CPU?,Neutral
AMD,Expand ?,Neutral
AMD,"I want elegato unedited video from start to finish i did that and on cp 2077 9800x3d matched 14700k, and it managed to beat it on horizon zero dawn, i want built in benchmark scores so everyone can easily double check the numbers, forget everything online buy the hardware and run built in benchmarks RPL i7/i9s on ddr5 7200 are equivalent to zen 4/5 x3ds much faster than non 3ds, if you can't afford it make posts asking people for thier scores in games.",Positive
AMD,"It is a common knowledge at this point, that HUB, LTT and GN are working for YouTube algo, not the costumers. YT algo promotes saying AMD is king, so their reviews are exactly that (Every single thumbnail or tittle with Intel being dead or destroyed). Let's pair Intel with mediocre ram and loose timings, let's power throttle them, put on some weird power profile, turn off whatever bios setting, focus on specific benchmarks that have very little to do with real usage etc. In worst case scenario they can always say that Intel won, but it's not enough or that Amd will crush them in 6 months or so. They've been doing it for years now.   Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong. It's really not about being right or win some CPU wars, it's more about not being screwed by that giant YT racket that once again works for sponsors and YT bucks, not the costumers.",Negative
AMD,"I spammed my score screenshots on every popular tech tuber’s page on X (Twitter), and none of them had the courage to face me. I wish one of them would come out of hiding and tell me what I did wrong in my tests. Show me the scores they’re getting. Reviews are sold to the highest bidder deal with it. Also, PCGH showed the 14600K beating the 9700X, so why isn’t that the case in other tech site charts?",Negative
AMD,"at first I tested 4080 1080P ultra on these processors on the same DDR5 7200 im using on the 14700k but it didn't make a difference tried the 2 profiles 7200 xmp and expo CL38, so when I got the 4090 I decided to use whatever available but that wouldn't make a difference, I could use DDR4 on 14700k and still win in every game against non 3ds,   here is the test on 14700k 5600 CL46 which got me around 185FPS vs 145FPS for zen4/5 non 3ds.  [https://www.youtube.com/watch?v=OjEB7igphdM](https://www.youtube.com/watch?v=OjEB7igphdM)  also here is 14700k +DDR4 3866 +4080 for RE4 still faster than 9700x+4090+DDR5 6000  143FPS vs 130FPS  [https://www.youtube.com/watch?v=lxZSMJnkYNA](https://www.youtube.com/watch?v=lxZSMJnkYNA)",Neutral
AMD,"Did you get any more FPS when you tried it? I’m getting good frames without it, so I don’t think it’s worth it. Also, in SoTR, even though the 9800X3D scored higher in the benchmark, there weren’t any extra frames compared to the 14700K during actual gameplay. I tested multiple locations, and the performance was the same, the higher score on the 9800X3D came mainly from the sky section of the benchmark.",Negative
AMD,did you do it,Neutral
AMD,I was at:  RT Ultra: 118.78 Ultra: 182.42 High: 203.76  https://imgur.com/a/edaNmHQ,Neutral
AMD,can you reset settings then choose ray tracing ultra preset.,Neutral
AMD,here is w10 vs w11 same scores in every single game.  [https://www.youtube.com/watch?v=vQB4RrNDzyM](https://www.youtube.com/watch?v=vQB4RrNDzyM)  [https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s](https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s),Neutral
AMD,because they exclusively exist in DIY build your pc enthusiast bubble,Neutral
AMD,Pricing was aggressive. A 12 core 3900x was 400 usd.,Neutral
AMD,">And to add to that, which often gets omitted. Is that Comet Lake is only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, Zen 2 in my mind actually makes more sense.",Neutral
AMD,literally multiple videos from him even with a 7800x3d and still 14900ks can only eek out wins in like 1-2 games like congrats you dropped 500$+ on the RAM alone and 720$ on hte motherboard to run that RAM just to lsoe to a 7800X3D,Neutral
AMD,">It is a common knowledge at this point,  It's not an argument.  Only objective way that you have to prove that those sources are spreading misinformation, is to prove it and share it with others - do it, make your own research and share it on Reddit and YouTube, if you're really correct and your data is accurate, it will hurt public reception of these sources(HUB, LTT, GN) and will damage their credibility.  >Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong.   [No true Scotsman - ](https://en.wikipedia.org/wiki/No_true_Scotsman)sorry dude, but your argument is logically incorrect, for pretty obvious reasons - it's unfalsifiable, it lacks evidence and it assumes the conclusion **- ""every dude"" argument is a way to sound like you have the backing of a consensus without having to provide a single piece of evidence for it -** as I said, real evidence in your case would be substantial evidence that proves that sources that you named in your comment are spreading misinformation, which shouldn't be hard to test and release in public.  I don't like to be a part of emotional conversations, I honestly hate those - if we want to speak facts, share facts with me, if you want to speak emotions - find another person to discuss them, I don't like emotions when it comes to constructive discussions.",Negative
AMD,"Okay, I did it",Positive
AMD,"No, I didn’t remember good",Negative
AMD,"I did. I ran the benchmark a couple more times and the results were similar. APO on, 200S Boost enabled, no background tasks running except Steam. I think it's a GPU bottleneck.",Neutral
AMD,Thanks for solidifying opinion that your benchmarks are fake,Negative
AMD,"Ddr5 7200 cost as much as 6000, i bought mine for 110$ from newegg, I should have got the 7600 for 135$ but I was a bit worried about compatibility, my z790 mb cost me 200$, and on my 14700k on a 4090 it matched 9800x3d in 3 games lost in re4 and won in HZD remastered, ill test more games in the future but since i won in one game to me 14700k+D5 7200=9800x3d, 14900k+ddr5 7600 could be faster, i dont care about any video online or chart, i only believe in built in benchmarks scores and what my eyes see.",Neutral
AMD,"if there isnt cpu bottleneck your score should be around 135fps, can you give it one last try with sharpness set to 0.",Neutral
AMD,"What’s fake about it? What scores are you getting? Prove me wrong with score screenshots, not with fanboyism false accusations. If you’re comparing score screenshots to OBS-recorded videos, you shouldn’t, because OBS uses about 15% of GPU resources, which reduces the scores slightly. I’ve been talking about this and calling out tech sites and tech tubers on Twitter and Reddit for over a month. Do you really think they’d stay silent all this time if I were wrong? If I were faking benchmark scores, they would have publicly shown their results and shut me up for good, especially since I’ve been accusing them of corruption on a daily basis. Just yesterday Tech power up is the first techsite to block me on X for sharing my benchmark screenshot on their page, cowards, shouldn't they discuss what did i do wrong in my benchmark, then again what are they gonna say AMD paid us too much money that we need 10 years in hard work to make that much.",Negative
AMD,Cam someone confirm or is this gas lighting?,Neutral
AMD,This is nice honestly. Points out some nice stuff about ARL besides the usual hate on the internet,Positive
AMD,"The Ultra 7 265K is absolutely a better deal than the non-X3D 9700X or 9900X. The Ultra 9 285K is competitive with non-X3D 9950X. If 3D-Vcache offerings from AMD didn't exist, Intel would likely be considered the go-to for this gen.",Positive
AMD,Intel comeback real?,Neutral
AMD,"How come the non-k variants of ultra 9,7, and 5 have a base power of 65 watt, but the K variants have a base power of 125 watts? The only difference I see between both variants, besides base power, is the 0.1-0.2MHz clock speed increase. Am I missing something here?",Neutral
AMD,3D v-cache has entered the chat.,Neutral
AMD,Take it as a grain of salt. Intel marketing LOL,Neutral
AMD,Does anyone have the Intel Ultra 9 285k and can test all of this ?,Neutral
AMD,Thats cool ...but lets talk about better pricing.,Positive
AMD,"I got the 7800x3d and the 265K, the 265K got higher fps. Also when i watch twitch streamer i Look at the fps if the streamer is useing a 9800X3D. In CS2 the 265k got 50-80 fps less then the 9800x3d  (500 vs 580 fps) in cs2 Benchmark the 265K is consuming about 100 watts",Neutral
AMD,Tech Jesus has entered chat :).,Positive
AMD,"Intel still trying to sell their already abandoned platform, they should learn something good from AMD and provide support for three generations per socket (and yes, Zen 6 will use AM5, already confirmed by AMD).",Neutral
AMD,"Two things can be true at the same time. Arrow lake has legitimately improved since it launched last year, and Intel is portraying it in the best, most optimal light they can that is probably not representative of a broad spectrum of games reviewed by a third party.   It would be interesting to get a retest of arrow lake now, but I dunno if it is worth the time investment for some reviewer like TPU or HWUB to re-review a relatively poor platform that’s already halfway out the door.",Positive
AMD,"Arrow Lake performs quite well at general compute and power efficiency compared to prior generatons ans even against AMD.  Where they have had trouble is in specifically gaming apps, particularly against the X3D variants (which it should be said drastically perform non-X3D AMD parts in gaming). And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  This led to Intel being dismissed out of hand by gaming focused YouTube reviewers like HUB, GN and LTT despite them being perfectly capable parts outside of gaming, simply because they did not place well in gaming benchmarks in CPU limited scenarios (e. g. with a literal 5090).  HUBs own benchmarks, btw, show an utterly inconsequential difference between a 9700X part versus a 9800X3D part when paired with anything less than a 5080/4090 in gaming. Most people with most graphics cards wouldnt see the difference with their card between the various high end CPU parts unless theyre literally using a $1000+ GPU. Ironically the 9070 performance was identical with a 9700X and a 9800X3D.  So it's actually kind of the reverse, gaming reviewers painted Arrow Lake in the worst possible light.",Neutral
AMD,What do you mean by gaslighting in this case?,Negative
AMD,"The answer depends on whether or not you're a ""next-gen AI gamer"" (apparently that means 1080p avg fps with a RTX 5090).",Neutral
AMD,I doubt they would give false numbers. Those are probably what you get with the setup they list in fine print. Also they pretty clearly show the 3dx parts are faster in gaming. But they have probably picked examples that show them in a good light.   There is nothing wrong per se in the arrow lake lineup. In a vacuum if AMD and intel launched their current lineup now they would both look pretty competitive. AMD zen5 was also a bit of a disappointment after all. The issue that destroyed the image of arrow lake is that the previous generation was often faster in gaming. That made every reviewer give it bad overall reviews.,Neutral
AMD,Wait till refresh comes out in December... Memory latency lower and performance boosted. Price also lower... Wish it could come to the old chips but it architecturally cant. :(,Negative
AMD,"generally they cherry pick, so maybe the demo they use is process heavy with minimal I/O (arrow's biggest performance hit is memory movement). if you manage to keep memory movement low core performance is actually really good on arrow lake. it just gets massacred by d2d and NGU latency, which they don't get a pass for but its unfortunate that decent core designs are held back severely by packaging.",Negative
AMD,"there is just no way dude lol  even their comparisons make no sense, 265K = 9800X3D / 265KF = 9700X? lol? how would that even be possible",Negative
AMD,Honestly it's hard to get real conversation with people who fanboying Amd so hard. Why people can't be normal when we talk about Intel and Nvidia? *smh,Negative
AMD,"The base clock rates, my assumption how it boosts during heavily multi-threaded tasks, is like 60%-80% higher on both core types.",Neutral
AMD,Nova Lake bLLC about to ruin Amd X3D party.,Negative
AMD,Look at the last slide. Intel's source is TPU's review data which if anything is a less than ideal config for arrow considering it's only DDR5-6000. I'm honestly surprised they didn't pull from one of the outlets that did 8000 1.4v XMP+200S boost coverage.,Negative
AMD,I always wondered if Intel marketing budget is higher than the R&D budget,Neutral
AMD,Intel Arrow Lake is much cheaper than Amd Zen 5.,Positive
AMD,Genuine question how often do you or even the average person keep their motherboards when upgrading the CPU? IMO the platform longevity argument is stupid,Negative
AMD,only an AMD fan would worry about replacing their shit CPUs under 3 years,Negative
AMD,Hardware unboxed isn't a reliable source.,Negative
AMD,"As a real world example, I upgraded my primary system from a 14700k to a 265k recently. My other desktop is a ryzen 7900 (non x, 65 watt tdp)  Compiling my os takes 6 minutes on the 7900, 10 minutes on the 265k, 10 minutes on a 3950x, and 16 minutes on the 14700k.   That is not in windows, but my os which does not have thread director or custom scheduling for e cores. (So going to be worse than windows or Linux)   All the numbers except the 7900 are with the same ssd, psu, custom water loop.  The motherboard and cpu changed between Intel builds and the 3950x was the previous build before the 14700k.   The e core performance significantly improved in arrow lake.  It’s now keeping up with 16 core am4 parts in the worst possible scenario for it.  If the scheduler was smarter, I think it would be very close to the 7900.   In windows, it’s pretty close in games I play on frame rate to the old chip. A few are actually faster.",Neutral
AMD,"> And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  Yeah, 10% less performance when frequency matched is quite a lot.",Negative
AMD,Telling people that its performance is better than it actually is?,Negative
AMD,The ones with similar pricing not performance,Negative
AMD,"True, except AM5 supported all cpus 7000, 9000 and will support the newest one. So not exactly true per se, because you need a new motherboard every time and those on AM5 can upgrade cpu without buying a board. Imagine if someone was on 13400F and wanted to buy lets say 285K? Well, you need a new board. So it depends what “cheaper” is seen as.",Neutral
AMD,"Yep. We all see the patterns here. Gamers nexus, HUB, LTT and most big name they all keep mocking Intel, even when Intel did good job to bring improvements to their products like fixing Arrow Lake gaming performance but those ""tech reviewer"" decided to be blind, they don't even want to revisit Intel chip and benchmark it, they don't even want to educate their viewers.    At the same time they keep overhyping Amd products to the moon like they own the stock, it's obvious now who is the real reviewer and who is fraud reviewer using their own popularity and big headline to spread their propaganda for their own benefits. They are just as worse as shady company!",Negative
AMD,"Platform longevity isn't stupid lol. There's a reason why am4 is one of the best selling and one of the best platforms of all time. The fact that you can go from a 1700x to a 5800x3d on the same mobo for 85% more performance is crazy good value. You can say that all you want but there's a reason why in the diy market everyone is buying amd over intel. People know arrow lake is doa where as am5 actually has a future, which is why everyone is flocking to the 9800x3d in droves.  Zen 7 is rumored to be on am5 as well. Meaning that am6 would come out in 2030/2031. If that's true am5 lived for around 8/9 years.  That's insane value to get out of one socket.",Positive
AMD,Quite common for AM4 in my experience.,Neutral
AMD,"My brother went from 1700X to 3700X to 5800X3D on AM4, so 10 years next year? I plan on upgrading to 3D cpu probably 9600X3D once its out or maybe 7800X3D (something more affordable but still somewhat better). Kinda want better 1% lows tbh, but maybe I’ll just wait for next ones unsure.",Neutral
AMD,"How is it stupid? On one hand, intel sockets used only 3 gens from 12th-14th while on the other hand amd has not only used 7000-9000 but will also be using zen 6. I don't think you realize just how big of a hassle changing motherboards are",Negative
AMD,"I upgrade my motherboard usually because of cpu socket. Thinking of going AMD. I build my PCs for myself, my wife and my kid. Lots of floating parts in this house could use the platform longevity.",Positive
AMD,I originally had a Ryzen 5 3600 and am now on a 5700x3d so it’s pretty relevant I’d say. Of course it’s anecdotal but I don’t think it’s too uncommon,Neutral
AMD,"I agree with you, it’s daft, no doubt about it. But you’ve got to factor in that people can be daft too. Most tend to skimp, only to end up buying another middling CPU the following year, and then again after that. And so the cycle trundles on.  When I buy PC parts, I usually go for the best available. Provided the price is reasonably sane. I picked up a 9900K seven years ago and it’s still holding its own. Is it on par with the 9800X3D? Of course not. But it’s still a very capable bit of kit. Next time I upgrade, I’ll go top shelf again.",Neutral
AMD,"They frequently revisit products and test old hardware, but naturally that's not going to be their focus. Channels with that focus like RandomGaminginHD fill that niche.",Neutral
AMD,Sooo they are in the YouTube space for the money not for the love of tech,Neutral
AMD,"So true. After all they are reviewer, it's their job to revisit something if it's worth to benchmark due to improvements.   Didn't they said they want to educate their viewers? If so then why being lazy? This is obvious they just care about making big headline which makes more money for them. It's BS!",Negative
AMD,"If only the Arc B770 can beat the RX 9070XT with performance and features, then I definitely stay on Intel's side, but RN I doubt it",Neutral
AMD,"Even accepting that as true, I don’t think it changes the main point I was making. Arrow Lake has probably improved versus a year ago. It’s probably not as good as intel is making out. A good third party reviewer would be needed to determine by how much, yet it’s probably not worth their money and time to test it.   So the truth is probably in between what Intel is saying here and what the state of things was 6 months ago.",Negative
AMD,Isn’t that just either lying or exaggerating?   I know those words don’t “go hard” and are “low key” boring.   But I think some internet buzzwords are just overused or badly applied.,Negative
AMD,"Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Negative
AMD,The 5800x 3d costs anything between 450 (time of launch) and 300. With that kind of money - you can literally buy a cpu + a mobo bundle. There is no value in the platform upgradability argument. You can literally buy a 13600k and a brand new mobo for cheaper than the cost of the 5800x 3d alone.,Negative
AMD,YouTube has created an entire industry not just for gaming but for everything else where all these people rely on the YouTube algorithm and have figured out how to exploit it and bitch and complain whenever YouTube tries to change things up.  We need a new platform based on YouTube but built by technology like ipfs and run by the internet community.,Negative
AMD,"No other card in recent memory has been more overhyped than the 9070 XT , we should be getting a B770 before 2025 ends. Make sacrifices to the Silicon Gods and perhaps your wishes will come true.",Neutral
AMD,"The only things that have changed since launch is the bugged ppm driver on windows causing extremely low clocks under moderate load (games being the main example), and 200S boost making d2d/NGU OC a single click for people not willing to type in 32 in the NGU/D2D ratio boxes. The current performance on arrow is achievable on the launch microcode.",Neutral
AMD,Sure but charts seem about right to me,Positive
AMD,APO is game specific. I'm referring to what has changed overall.,Neutral
AMD,"I'm unable to claim my copy of Battlefield 6 from the recent Intel promotion even after redeeming the code.  I redeemed my promotional code last month which added the ""Intel Gamer Days"" bundle to my account that includes Battlefield 6. The instructions say to return October 3rd to claim the key. I've been attempting daily to claim the key but just keep getting the message ""We are temporarily out of codes and are working to reload our system. Please check back soon.""  The game launches tomorrow so this is pretty frustrating. Is anyone else having an issue claiming their BF6 key?  I put a ticket in with Intel support but haven't heard anything yet.",Negative
AMD,"bought the newegg bundle. I got a master key but my PC is running linux. The linux HST tool doesn't even have an extension, no .exe no nothing. I don't know how to run it.",Negative
AMD,"Hello there, it seems that after updating my Windows 11 I can't scan for new intel drivers by using the Intel Driver & Support Assistant. It scans for a while but then it gives me the message: ""Sorry, something went wrong while trying to scan""; it seems to affect the Intel Graphics Software as well as it keeps looking for updates without success. I've already tried to reinstall DSA and deleting any folders associated with the program, rebooting Windows and also uninstalling the Windows Update. Any thoughts?",Negative
AMD,"After updating to W11, these awful KILLER apps and processes have returned and reduced my upload speed to less than 1mb!!! Normally I get 30mbs up.   Dell PC, 32 gbs RAM, Intel i9. x64 W11 Home 24H2, OS 26100.6584, WIRED Ethernet direct to router/modem.  The Killer thread is closed and directs users to post in this thread.   The same thing happened in W10 on same machine but problem was fully solved by the KillerSoftwareUninstaller tool, which also disabled Killer permanently from coming back on every single update.  However, I tried the same tool after the update to W11 and it removes Killer, and everything works better for a day or so, then suddenly everything slows down and Killer is back.  PLEASE help me get this awful suite of useless Killer tools off of my machine forever!   Thank you!",Negative
AMD,"I have a i7-13700 that may be the root of my issue. Long story short, MATLAB is getting an Access Denied error when trying to link to an Imperx framegrabber using videoinput. Imperx FrameLink works fine. The function that it crashes at works fine when called from a different way (getAvailHW), and only crashes when trying to call it through videoinput. The exact same setup works just fine on another computer. Even when no cameralink cable is attached, it crashes on this one. BIOS is fully updated. The CPU passed the checking program Intel provided.   Looking up this issue, people had similar problems, and they linked it to the CPU issues with 13th and 14th gen. Intel released a chart for what settings you should have in BIOS, but it does not cover the 13700, only the K variant. What settings should I have in BIOS to rule out Intel?  Chart: https://community.intel.com/t5/image/serverpage/image-id/56057i81282C3BCB9162A9",Negative
AMD,"Is this normal activity for a 265K at Idle?  https://imgur.com/SSSOB9F      The multiplier and clock speed keeps jumping around all over.  There are no background processes running, nothing in task tray, closed all applications except HWINFO to record this.      I am running into an issue where I feel like either my CPU or my AIO is bad.  The build is new, completed yesterday.  Idle temps hang around 30-40C; which seem fine.  But as soon as tasks start happening, it jumps all over up to 80C.  I have a 360 AIO on it.  Gaming even, it hit 83C earlier.  It doesn't make sense to me.  I have built a few 265K rigs and not had this kind of fluctuation in temperature before.  Even running cinebench on previous builds, the CPU never went above mid 70s.  But this newer machine is hitting 80+ in games.  Previous 265K PCs have ran mid 50s in games.  Sometimes 65 depending on the load, but never 80+.",Negative
AMD,"Dear Intel team,  according to **desktop** [**Core 14th datasheet**](https://edc.intel.com/content/www/us/en/design/products/platforms/details/raptor-lake-s/13th-generation-core-processors-datasheet-volume-1-of-2/system-memory-timing-support/) \-> memory timings support:  \- only **CL50 DDR5 5600MT/s** **is supported**  \- only **16Gbit / 24Gbit die density is supported**    **Can I safely use following DDR5-5600 CL36 kit from Kingston KF556C36BBE2K2-64 with 32Gbit die density?**  Will it cause some memory troubles / issues on memory controller ?  System is not overclocked (and never was/will be), everything set to Intel defaults, bios has latest in 0x12b microcode from ASUS (board Q670M-C CSM).",Neutral
AMD,"Hi, I’m at my wit's end with my build and would really appreciate some advice.  My PC has been plagued by random crashes, CRC errors, and installation failures for months:  - Random application crashes, often citing `KERNELBASE.dll`, `ntdll.dll`, or `ucrtbase.dll`. - Frequent CRC errors when extracting large ZIP or RAR files. Retrying usually works. - Software installations fail with data corruption or unpacking errors, only to succeed when I try again. - Games crash or stutter randomly. - Very rare BSODs.  Specs:  - **CPU**: Intel Core i9-13900K - **Motherboard**: ASUS ROG STRIX Z790-H GAMING WIFI - **RAM**: 32GB (2x16GB) G.Skill Trident Z5 RGB (DDR5-6400 CL32, Model: F5-6400J3239G16GX) - **GPU**: MSI RTX 4090 Gaming X Trio - **Storage (OS Drive)**: Crucial P5 Plus 2TB NVMe SSD - **OS**: Windows 11   This is what I've already tried (everything passed):  *  **Memtest86:** Completed multiple passes with no errors. *  **Prime95 & Intel XTU:** Seems to be stable. *  **FurMark:** GPU stress test is stable. *  **Storage Health:** All drives pass SMART and manufacturer-specific self-tests. *  **System Integrity:** `sfc /scannow` and `DISM /RestoreHealth` complete successfully. *  **Updates:** All drivers, firmware, BIOS, and Windows are fully up-to-date. *  **Physical:** Cleaned the case, re-seated components, and replaced thermal paste. * **XMP**: I've disabled/re-enabled XMP multiple times, doesn't make a difference.  I have trouble finding a consistent way of reproducing the issue. Today I tried a 7-Zip benchmark which failed once with a ""decoding error"", but I wasn't able to reproduce it afterwards. I couldn't get Intel Processor Diagnostic Tool to fail after multiple hours. So...  *   **Is this a memory instability issue?** Could the RAM be faulty?  *   **Or, could this be a faulty CPU core?** I found [this post](https://www.reddit.com/r/intel/comments/15mflva/tech_support13900k_problems_when_using_multiple/) where a user had identical symptoms (CRC/7-Zip errors) that were only resolved by replacing a faulty 13900K, even with XMP off.  Thanks in advance for any help.",Negative
AMD,"# Alienware 34 - AW3425DWM resolution issues  [](https://www.reddit.com/r/ultrawidemasterrace/?f=flair_name%3A%22Tech%20Support%22)  I just got the AW3425DWM, and my laptop is a Dell Inspiron 15 5510, which is not a gaming laptop. I'm not a gamer.  When connecting through the HDMI port on the laptop and monitor, I can't set the resolution to 3440x1440; I can go up to 3840x2160, but not the monitor's native resolution. However, when I connect through the USB-C port on the laptop to the DisplayPort on the monitor, I can set the resolution to 3440x1440 without any issues. The downside is that I've lost the only Thunderbolt port I have available.  Is there a workaround for this issue? If I use an HDMI to DisplayPort adapter, will I be able to set the resolution to 3440x1440?  I understand that the HDMI 1.4 port usually can handle 21:9 resolutions, but with the latest Intel drivers, it isn't giving me the option for 3440x1440",Negative
AMD,"Hi, can someone help me please? I'm trying to generate a video on AI playground after installing it but it just keeps loading for videos and images don't show up. That's my it it'll specs:   Processor: Intel(R) N95 (1.70 GHz) Installed RAM: 32.0 GB (31.7 GB usable) System Type: 64-bit operating system, x64-based processor Graphics Card: Intel(R) UHD Graphics   -That's a part of what's in the console:    No key found for setting negativePrompt. Stopping generation oa @ index-fOc02QH8.js:61 w @ index-fOc02QH8.js:61 await in w V @ index-fOc02QH8.js:22 pt @ index-fOc02QH8.js:61 await in pt V @ index-fOc02QH8.js:22 c @ index-fOc02QH8.js:61 await in c Il @ index-fOc02QH8.js:14 Sr @ index-fOc02QH8.js:14 n @ index-fOc02QH8.js:18 index-fOc02QH8.js:61 uploadImageName b99fb28ea4440a2f51ce53cd5c529554e5965b66f5f5a8506b9b973b66e754bd.png index-fOc02QH8.js:251 [comfyui-backend] got prompt (anonymous) @ index-fOc02QH8.js:251 (anonymous) @ VM5:2 emit @ VM4 sandbox_bundle:2 onMessage @ VM4 sandbox_bundle:2 index-fOc02QH8.js:61 updating image {id: '6eb88f1b-f433-4541-a421-40619ac9fdc2', imageUrl: 'data:image/svg+xml,%3C%2Fpath%3E%3C%2Fsvg%3E', state: 'generating', settings: {…}, dynamicSettings: Array(3)}       With: RuntimeError: UR error     (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:251 [comfyui-backend] Prompt executed in 116.86 seconds   (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:61 executing Object",Neutral
AMD,Will XeSS 3 and Intel multi framegen be available for Iris xe graphics igpus?,Neutral
AMD,"Putting together a 4k gaming 5090 machine, deciding between the 285k or 265k. Does the extra L3 cache of the 285k make any difference, or does the ~5ns less latency of the 265k make more of a difference?   I plan on a 2dimm board to push memory and OC slightly, but nothing crazy.",Neutral
AMD,"Hi all,  I ordered a contact frame from Thermal Grizzly because the temperature of my CPU wasn’t great, and I was tired of the fan noise.   I installed the contact frame in August, and everything was fine until two weeks ago, when the desktop suddenly stopped booting. The fans were spinning, but there was no POST, no debug LED — nothing. I thought the motherboard was dead.   However, when I removed the AIO head and tried to boot again, it worked!  Since then, I’ve been experiencing intermittent no-POST issues, especially after gaming sessions.   To get it to boot, I always have to adjust the screws on the AIO head. If it’s too tight: no POST. If it’s too loose: CPU temperatures are high. For example, in Hogwarts Legacy, I get around 60 FPS with an RTX 4090, while the CPU runs at around 18% usage @ 85°C. Which is not normal at all, as I used to run the game at around 100–110 FPS.  I tightened the screws on the contact frame using my thumb and index fingers.   I’m looking for advice from anyone who has encountered a similar issue, because at this point my only idea is to remove the contact frame and reinstall the original one.  For context, my setup is from November 2022, and I installed the contact frame in August 2025. I have never removed the CPU from the socket since the first installation.  Thank you all.",Negative
AMD,Anyone know why the performance of my 13700k is so much better when using a pre micro code bios on a z790 board? This even after applying the latest micro code update through windows,Positive
AMD,"Hi u/Mydst For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues. They'll be able to assist with your Battlefield 6 code availability and on how to resolve this issue.   You may also check these links:  [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Neutral
AMD,"Hi u/afyaff For this kind of inquiry, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry and they may help you on how to input the master key in your PC.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Neutral
AMD,"u/Adrian-The-Great  For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues .They'll be able to assist with your non-working BF6 code from the Newegg bundle promotion.     Additional Information:   [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Neutral
AMD,"u/Broad_Remote3101 This could be a compatibility issue or something with the installation process. Just to help figure this out - are you using a laptop or desktop? If it's a laptop, could you let me know the exact model? Also, what graphics card and processor do you have?  You might want to try following this article too and see if that does the trick. Let me know how it goes!  [Intel® Driver & Support Assistant (Intel® DSA) Results in “Sorry,...](https://www.intel.com/content/www/us/en/support/articles/000026895/software/software-applications.html)",Neutral
AMD,"u/musicmafia77 Based on what you're describing, this sounds like a classic case of Windows 11 being overly ""helpful"" with driver management. To get to the bottom of this and find a lasting solution, I'll need some specific details about your setup.   First, could you check Device Manager and tell me the exact model of your network adapter and current driver version? Since you mentioned you're using wired Ethernet, I'm curious if this is actually a Killer-branded Ethernet adapter. Also, when Killer inevitably returns, which specific processes are you seeing in Task Manager - things like ""Killer Control Center"" or ""Killer Network Service""?  Here's what's likely happening behind the scenes: Windows 11 maintains a driver store cache that can automatically reinstall what it considers ""preferred"" drivers, even after you've removed them. The system recognizes your Killer hardware ID and keeps pulling that branded driver through Windows Update, completely overriding your manual removal efforts. This explains why the KillerSoftwareUninstaller works temporarily but then everything reverts after a day or so.   To truly fix this, we'll probably need to perform a clean driver installation process that involves purging the entire driver store of Killer-related files, blocking Windows Update from auto-reinstalling them, and potentially replacing them with Dell provided drivers rather than Intel generic drivers (since Killer hardware is actually based on Intel chipsets). It's more comprehensive than just running the uninstaller, but it should give you that permanent solution you're looking for.",Neutral
AMD,"Update your bios to the latest version and set it to intel default settings. If the issue is the same, request a replacement",Neutral
AMD,"PL1 should be  65W , PL2 is 219 W you can cross reference this value on this specs https://www.intel.com/content/www/us/en/products/sku/230490/intel-core-i713700-processor-30m-cache-up-to-5-20-ghz/specifications.html   However following intel recommended settings once you load BIOS with microcode patch it should follow the default, you may check also with your motherboard manufacturer",Neutral
AMD,"May I kindly ask you what components are you using (especially MoBo)?      I have HP Z2 Tower G1i with 265K too and what is interesting is that placement of performance and efficient cores is not contiguous (e.g. 8P and then 12E), but somehow ""alternating"".   I observed this on my HP Z2 G1i and thought that it is something special defined by HP Z engineers in ACPI tables (maybe DSDT..), but looks like that it's how things works.",Neutral
AMD,"u/vincococka ,Yes you can use that memory kit safely as long as you don’t enable XMP and keep everything at Intel default settings. Intel officially supports 16Gbit and 24Gbit dies, but many users have successfully used 32Gbit dies without issues. However your memory will likely run at (JEDEC default), which is fully supported. Just make sure, You monitor system stability after installation. Avoid enabling XMP unless you confirm stability with the 32Gbit modules.     [Compatibility of Intel® Core™ Desktop Processors (14th gen)](https://www.intel.com/content/www/us/en/support/articles/000096847/processors.html)",Neutral
AMD,"u/SuperV1234 If the motherboard BIOS allows, disable Turbo and run the system to see if the instability continues.  If the instability ceases with Turbo disabled, it is likely that the processor  need a replacement.",Neutral
AMD,"u/triptoasturias Before I share any recommendations, could you confirm if this is your exact system-[Inspiron 15 5510 Setup and Specifications | Dell](https://www.dell.com/support/manuals/en-my/inspiron-15-5510-laptop/inspiron-5510-setup-and-specifications/specifications-of-inspiron-15-5510?guid=guid-7c9f07ce-626e-44ca-be3a-a1fb036413f9&lang=en-us)? Also, may I know which driver version you’re using and where you downloaded it from—was it from Dell or the Intel Download Center?",Neutral
AMD,"u/triptoasturias Your concern is related to **port and bandwidth limitations**. Most Inspiron models only support **HDMI 1.4**, which is limited to 4K at 30Hz or 2560×1440 at 60Hz. Ultra-wide resolutions like 3440×1440 often aren’t exposed because they’re outside the standard HDMI 1.4 spec. **USB-C to DisplayPort**: This works because DisplayPort has much higher bandwidth and supports ultra-wide resolutions natively. HDMI-to-DisplayPort Adapter, Unfortunately, **passive adapters won’t work** because HDMI and DisplayPort use different signaling. You’d need an **active HDMI-to-DisplayPort converter**, but even then It will still be limited by the HDMI 1.4 bandwidth from your laptop. So, you likely **won’t get 3440×1440 at 144Hz,** maybe 3440×1440 at 30Hz or 50Hz at best.",Neutral
AMD,"u/mercurianbrat This spec can run **basic image generation workflows** (CPU mode or lightweight models), but **video generation and heavy diffusion models will struggle or fail** on this setup. AI Playground’s minimum requirements are currently tied to Intel Arc GPUs with 8GB or more of allocated VRAM.  Currently you can download the installer for discrete GPUs.  We will also publish an installer that will run on Intel Core Ultra-H with built-in Intel Arc GPU (please keep in mind that [Windows allocates half of the system RAM as VRAM](https://www.intel.com/content/www/us/en/support/articles/000020962/graphics.html) for integrated GPUs, so 16GB or more of system RAM are required)  and Intel Arc GPU discrete add in cards with 8GB or more of memory. AI Playground takes up 8GBs of  HDD/SDD requirements: 8GB w/o models,  \~50GB with all models installed.",Neutral
AMD,"u/mano109 As a general corporate policy, Intel Support does not comment on information about products that have not been released yet.  **Visit** our [Newsroom](https://newsroom.intel.com/) for the most recent announcements and news releases.",Neutral
AMD,"**265K all the way.** At 4K with a 5090, you're GPU-bound anyway. The 265K's lower memory latency beats the 285K's extra cache at that resolution, plus you save money for better RAM or cooling.",Positive
AMD,"u/hus1030  The mounting pressure from your AIO cooler can directly affect whether the system successfully completes POST. When the cooler is tightened too much, it can cause the CPU or motherboard to bend slightly, which may lead to poor or lost contact between the CPU and the socket pins. This prevents the processor from initializing properly, resulting in a no-POST condition. Installing a contact frame changes the pressure distribution compared to the stock retention mechanism, so overtightening the AIO screws can amplify this issue. On the other hand, if the screws are too loose, the CPU temperatures will rise because the cooler is not making proper thermal contact. To avoid these problems, ensure the AIO screws are tightened evenly in a cross pattern and do not exceed the manufacturer’s torque specifications. If the issue persists, you may need to verify that the contact frame is correctly installed or temporarily revert to the original retention bracket to rule out pressure-related problems.",Neutral
AMD,"Thank you so much for your help!  The ethernet adapter is Killer E2400 Gigabit Ethernet Controller.  I am not super tech savvy, but what you are describing makes sense and is consistent with what I've read about this problem. I will need simple step by step instructions to fix as you suggest. Thanks again!",Positive
AMD,"Intel 265K.  Motherboard was a Gigabye Z890 Aorus Elite X Ice.  32GB Corsair DDR5-6000 CL30.  Gigabye AORUS RTX 5080 MASTER.  Corsair 1000W PSU.      I had ordered another AIO cooler, and when I took the current one apart to repaste and install the new cooler, to test if the cooler was bad, I found that the pin contacts on the CPU had several dark spots.  Either the CPU or motherboard was bad from the get go.  I took it all back and exchanged it for an AMD 9800x3d and X870E motherboard, and it runs flawlessly.  Good stable temps.  35C idle, no jumpiung around, and gaming averages 57C, never passing over 65C.",Neutral
AMD,"Hi, I've tested disabling turbo and I still experienced issues. I've created a support request, case number 06728608, please take a look ASAP.",Negative
AMD,"Actually, this is not the case. After spending hours on this issue, I've finally found the 31.0.101.4502\_A14 drive at Dell's website that is working without any problems with the 3440x1440. So your claim that HDMI 1.4 is limited to 2560x1440 is false. HDMI 1.4 can go further to 3840 x 2160 without any problems. It's unacceptable the lack of support for UWM from Intel graphics cards and drives. Many previous drivers let you choose 3440X1440 resolution. Why did Intel stop supporting this resolution in the last graphics drivers?",Neutral
AMD,I was going with the 265K over the 9800X3D since the Intel stuff seems to get better 1% lows and smoother experience at 4k and above. But does DLSS change that? Does DLSS lowering the render resolution push the 9800X3D back into the lead?,Neutral
AMD,"u/musicmafia77 After checking our records, I found that the unit you're inquiring about has been discontinued-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html). While direct support is limited to self-help resources and community insights from other users, I'd be happy to extend some steps that should help resolve your issue.     Based on your initial description, here's what I recommend:     First, please review this article-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html) and follow the troubleshooting steps provided here-[Clean Installation of Wireless Drivers](https://www.intel.com/content/www/us/en/support/articles/000022173/wireless.html). Once you've updated to a newer wireless driver, I'd suggest reaching out to Dell Technologies for assistance with downloading the appropriate OEM driver for your specific system.     If you prefer a more direct approach, you can:  * Visit Dell's website and download the wireless driver for your adapter by entering your service tag in their support portal * Alternatively, run Dell SupportAssist after completing a clean installation of the wireless driver     For additional troubleshooting, here's another helpful article that covers complete removal of Killer software issues, which may be relevant to your situation.-[How to Perform a Clean Installation to Solve Most Intel® Killer™...](https://www.intel.com/content/www/us/en/support/articles/000058906/wireless/wireless-software.html)     If you've tried these steps and the issue persists, I'd recommend coordinating directly with [Dell's support team](https://www.dell.com/support/home/en-us), as they're best equipped to assist with OEM-specific products and can provide more specialized guidance for your particular system configuration.",Neutral
AMD,"u/SuperV1234 Hi, thanks for the update. I’ve reviewed case number and confirmed that it’s currently being handled by our **warranty team** for replacement. You should be receiving further instructions from them shortly.     [Guide to pack your faulty CPU](https://www.intel.com/content/www/us/en/content-details/841997/guide-to-pack-your-faulty-cpu.html)",Neutral
AMD,"u/triptoasturias this explains, The generic Intel® driver provides users with the latest and greatest feature enhancements and bug fixes that computer manufacturers (OEMs) might not have customized yet. OEM drivers are handpicked and include customized features and solutions to platform-specific needs. Installing Intel generic graphics driver will overwrite your handpicked OEM graphics driver (in your case Dell driver). Users can check for matching OEM driver versions at OEM websites. For more information on how the installation of this driver may impact your OEM customizations, see this [article.](https://www.intel.com/content/www/us/en/support/articles/000096252/graphics.html)",Neutral
AMD,Unfortunately my Dell warranty support has ended and so far the forum there has not been able to help either. The removal tool works but Killer just keeps coming back.,Negative
AMD,"I also went to Dell, typed in my service tag # and cant find any other ethernet drivers.",Negative
AMD,"Most commenters are wrong or didn’t read all the articles. Nvidia wants some custom SKUs of server chips to pair with its data center GPUs, and Intel wanted a better solution for pc parts for the GPU tiles.   NVIDIA is NOT getting an x86 license, nor do they want one. In case you haven’t noticed, Nvidia is a $4T company largely on sales of data center GPUs, sales of which were 10x the gaming segment. Profit/earnings ratio of data center is even higher than the 10x.   In my opinion, the only reason for Intel to use RTX chiplets on pc parts is so they can have a few skus for mobile gaming. Lower end laptop chips would still use Xe cores.  This solves NONE of intel’s structural issues - they need, in this order  1. Major foundry customers  2. Regain server market share 3. Data center GPU offerings 4. Move all pc chiplets back to Intel nodes and start to fill up the fabs again",Neutral
AMD,An Intel + nVidia handheld would be something I would welcome just for DLSS4 alone.,Positive
AMD,"I don’t think it will change that much overall.  It’s no secret that NVIDIA wanted a x86 license for decades now and now they basically got it. This will ""hurt"" AMD by offering competition to markets where AMD currently holds a monopoly like with Strix Halo, most handhelds and consoles.  I don’t think we’ll see big changes on the Intel side.  I don’t think this will affect neither the iGPUs nor ARC because why would it.",Negative
AMD,"this jointventure is NOT about consumer gpu and does not change anythig about gaming gpu of intel, amd or nvidia.  this is instantly obvioiusly from just reading the headlines of the news announcing this.   this tumbnail is just hub using this news to suggest jet another time that maybe amd might just be screwed. they do this because they are neutral in every way.",Negative
AMD,"i think it's bad for us, consumers",Negative
AMD,Was the team up really to crush AMD or Nvidia's answer to enter China?,Neutral
AMD,AMDware unboxed only cares about AMD anyway,Neutral
AMD,This hurts the arc division way more than this could ever hurt amd.,Negative
AMD,They will crush user's wallet,Negative
AMD,you guys realize Nvidia buys all their stuff from TSMC the same company that makes all of AMD's stuff...So now intel will buy from them too since they cant compete? Because no amount of Intel+Nvidia humping is going to make a TSMC baby...you got to pay to play.,Neutral
AMD,"Even assuming they made monster co-op AMD still has own ecosystem of GPU and CPU, along with consumer support. It's Radeon and Ryzen are still very strong consumer wise. So no, AMD is not screwed",Positive
AMD,Remember Kaby Lake G? No? This will also be forgotten soon.,Negative
AMD,Yes.,Neutral
AMD,Intel partners with UMC of Taiwan the third largest foundry in the world.    What does it mean for UMC and Intel?   How will it affect transfer / exchange of technology between Taiwan and Intel?   How does this affect TSMC position with USAG?,Neutral
AMD,Foveros baby!,Positive
AMD,AMDUnboxed on suicide watch.,Negative
AMD,"For mobile/laptop performance, yes AMD is in the dust, maybe? Makes sense since AMD GPU division is mainly focused on the console partnership with Sony/Microsoft--leaves the mobile platform wide open.",Neutral
AMD,"Who knows, time will tell. But if AMD does find its back against the wall, this is when it does its best. This should be interesting to say the least.",Positive
AMD,"The last time NVIDIA made onboard graphics for a PC, they could hardly be considered graphics. The 3D performance was better than Intel, but features like CUDA, and a good handful of video decoding capabilities were missing. You were much better off getting ATi onboard graphics back then inside of an FX or AMD Bullerdozer chip.  In more recent times, this also applies to laptops with the low end MX chips. Many lack encode/proper decode support, and they are effectively just 3D accelerators. The Intel Onboard Graphics would be a much better choice overall. Whereas an AMD APU would give you the full suite of features.   I'm not entirely worried for AMD. It just means if Intel and NVIDIA manage to not screw up onboard video, AMD will just need to make sure Radeon can remain competitive. I know they have a lot of tricks up their sleeves still, like designing HBM memory onto the die (how Vega was supposed to be originally) and unified memory (soldered down, see the new AI branded chips).",Negative
AMD,Ohh noooerrrrrrrrr,Negative
AMD,"Actually this may be good for AMD and I will tell you why: Open source drivers. Nvidia if they had a say in intels graphics division would make the intel GPU stuff proprietary and locked down and when support ends its gonna hurt. Meanwhile AMD has open source drivers and in the age that linux is gaining popularity will make it the preferred platform for anyone interested in the growing linux ecosystem. Nvidia will NEED to change how they handle things or intel may not be as attractive as they once were, intel has already suffered a massive knock on thier reputation and this may be the mortal wound that can work in AMD's favor.",Neutral
AMD,"I don't see how this means anything at all for AMD in desktop. This is borderline irrelevant for desktop gaming. Most users pick up an Intel or AMD CPU and an NVIDIA dGPU.  APUs don't really exist in desktop gaming.  In mobile? I think most people won't care much. Intel and AMD APUs both offer compelling and reasonable alternatives for everyday usage. Anyone hoping to game on a laptop as their primary device will choose a model with an NVIDIA dGPU.   On handheld/console, AMD already has the largest market saturation, except for Switch, where Nintendo/NVIDIA opted for an ARM solution.  I don't see this APU being much more than a side choice that offers at best a reasonable alternative to a high end AMD APU, perhaps in the relatively small handheld market, if the performance is significantly better than existing AMD and Intel APUs, due to the DLSS support in a mobile form factor where a dGPU is not a realistic option, and upscaling is necessary for reasonable performance in a limited power package.  In all other cases I dont see this being a compelling option.",Neutral
AMD,So either a 200w igpu with a 300w intel cpu or an intel cpu with a 5030 slapped on it?  I think amd will be fine,Neutral
AMD,welcome to the Nvidia and amd duopoly,Positive
AMD,"I’m really curious what the power efficiency ends up looking like here - mobile performance/W has continued to be very good in recent Intel platforms, and Nvidia is the market leader in that regard among gpu IHVs. But until now, Nvidia chips being intrinsically dGPUs has completely neutered any potential mobile power efficiency or handheld capability - but not anymore if such a thing can be a chiplet/tile! Interesting days ahead…",Positive
AMD,"By that time amd will have already made an FSR4 capbable handheld, so no significant change",Neutral
AMD,"It won’t affect either cause on parts that don’t need a nvidia igpu why raise the price with it when arc igpu is really good as is, maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive",Neutral
AMD,a partnership doesnt mean they get free reign over license lol,Neutral
AMD,"AMD already has the next gen of consoles. Except for the Switch, which NVIDIA already has (with ARM, not with Intel).",Neutral
AMD,"None, It's a way for NVidia to have another product line and stop Intel from having a competitive product line.",Neutral
AMD,"Not to crush amd, but this essentially just kills arc and hurts the consumer market more. You won't be getting any of those 250 dollar 12gb gpus anymore.",Negative
AMD,Why would it?,Neutral
AMD,"It has nothing to do with the dgpu product stack. Nvidia makes very little from the diypc market. Way more comes from prebuilts, laptops and 90+% from data centers and other enterprise use. In a very real way, the arc cards are not a competitor to Nvidia.",Neutral
AMD,"Unfortunately no matter what given Intel's current state the division is going to suffer. Intel cannot afford to invest in products that will take a very long time to generate profit or may never generate profit. If even AMD cannot compete with Nvidia, then Intel will likely never be able too either",Negative
AMD,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Negative
AMD,Past != Future,Neutral
AMD,"nvidia also used to make motherboard chipset, with mixed success.",Neutral
AMD,FSR 4 looks like the later versions of Dlss 2 did,Neutral
AMD,"This ""insignificant change"" we definitely want though.  Competition is good in a stagnant market...",Neutral
AMD,"Other than Halo SKU which we are yet to establish any trend of, AMD notoriously drags their feet. RDNA3 will be with us for a while",Negative
AMD,Wont this be targetting the ML workstation space where you have the iGPU with access to >128GB of RAM like Strix Halo with DMA and/or NVLink banswidth?,Neutral
AMD,">maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive  Possibly an overlap like this would be a waste of money developing, and tbh I highly doubt customers would go for the Intel option rather than a Nvidia one, *even if* the Intel option is better performing, because of Nvidia's insane mind share.",Neutral
AMD,Never said that.,Neutral
AMD,Also a way for Nvidia to invest some of their funny money (which is essentially infinite) into a cheap company. Intel under $20 was ludicrous. Intel is still at book value at $29.,Neutral
AMD,Why do so many people think that this will kill ARC?,Negative
AMD,The market for Arc is the same as for Nvidia.,Neutral
AMD,Was that back in the MacBooks having the NVIDIA 9400m paired with a Core2Duo? Those were solid.,Positive
AMD,"No, but it is a reminder on shortcomings to watch out for. The Intel + AMD Partnership with the Intel NUCs several years ago made for some pretty compelling hardware. I still have a NUC8i7HVK in service which works like a champ.  If we are talking about current day NVIDIA with their Discrete Graphics (not the MX budget crap) then their GPUs are obviously smoking anything AMD and Intel. That doesn't resolve some of the Linux problems that still remain with NVIDIA, and that's not going to stop some of the BS NVIDIA does with their hardware around NVENC/NVDEC stream limits. But they have raised those limits, and the open NVIDIA drivers are getting better in time.",Positive
AMD,"Yup. Their nForce boards were great for overclocking, but rather buggy too.",Negative
AMD,"It's not a later version of dlss 2, it's between dlss 3 and 4. Just go watch some videos comparing both.   And btw redstone is coming probably at the end of this year which will improve ray tracing quality and frame gen performance.",Neutral
AMD,"No reputable reviewer thinks that. Most I've seen say it's better than DLSS 3, and very slightly behind DLSS 4.  Why lie about this?",Negative
AMD,They literally haven't said a word about it everyone's just speculating off of Moore's laws anti Intel videos lol consumer is the last thing they care for atm and might be server first cause that makes the most financial sense,Negative
AMD,If they do that in the case the two companies deal fall apart Intel is screwed with no graphics to compete with even the basic amd ones,Negative
AMD,"Well phrased, They now have a seat at the table of Intel to sway votes enough to dissuade them from making competitive products",Neutral
AMD,"Too many people are peeping that MLID podcast. Dude there is very confrontative about his theory that ARC is being ""effectively"" canceled because of some very early roadmap not paying out. Everything is a confirmation bias at this point.   I on the other hand don't get why would Intel forfeit it's competitive advantage with GPU accelerated AI, XeSS or QuickSync and made itself dependant on nVidia. Doesn't make sense especially now, when they have significant influx of cash from both Nvidia and government.",Negative
AMD,"Arc was already not doing well long before. Pouring millions into their gpu division every year with no returns. Their own road maps had their gpus coming out a year sooner but everything got delayed with them resizing the company. Like this deal is not exactly what killed arc, but arc was already struggling beforehand so this was most likely the nail in the coffin.",Negative
AMD,Nvidia does not have an A310 competitor.,Neutral
AMD,But I still remember how many motherboards with Nvidia chipsets I replaced way back.  They used to reliably overheat and need a reflow.  I think it was the HP DV6000 and DV9000 series.,Neutral
AMD,I have not lied,Neutral
AMD,"While workstations dont directly contribute a big chunk of the sales, I suspect they dont want to yield their foothold on the ML/Data Science guys.  If they start getting familiar with non-nVIDIA ecosystems, it could mean indirect losses when these guys start procuring non-nVIDIA dGPUs in servers when they do port their workstation experiments to ""larger"" hardware now that AI is all the rage.  B60 is gaining momentum on homelabs, and Strix Halo to some extent getting popular for those needing more memory for their larger datasets.",Neutral
AMD,"MLID has used clickbait trash thumbnails/titles way too many times, no one should watch that garbage channel to begin with.",Negative
AMD,"I think Intel priced in multiple generations of taking losses when they decided to enter the GPU market. Essentially impossible to walk in to a mature duopoly market and immediately start turning profit as a new player, even with Intel's resources.  Of course I think they also had much rosier projections for their other ventures when they started this journey, so who knows if the GPU losses are still tolerable.",Negative
AMD,Intel doesn't have a current gen A310 competitor either.,Neutral
AMD,Theres no way that nvidia is gonna let intel keep producing entry level gpus that directly compete with their own. I know their deals are in regards to apus and to compete with strix halo but nvidia is not gonna invest into a company with a competitor to their gpus. Like a b580 destroys the 5060 and 5050 while costing less or the same amount. There's no way nvidia invests 5 billion without some agreement that kills off arc as long as they are together.,Negative
AMD,Their nForce boards were excellent for overclocking and tinkering! That's for sure. I also remember them being a bit buggy lol.,Positive
AMD,IIRC that wss the AMD Turion X2 + NVIDIA 7300Go laptops. Those were notorious for dying.   I also remember some budget laptops with the Turion X2 + ATi 3200 chopsrt graphics would die due to graphics desoldering off the board. The GPU would sit at the end of the heatsink and burn up.   The 9400M MacBooks I remember would get hot enough to start leaving powder from the chassis on my desk lol.,Negative
AMD,"You did tho. It doesn't look like ""later versions of DLSS 2"". Every reputable source has said it's better than DLSS 3 and close to DLSS 4",Positive
AMD,"They're already at 2, going on 3, generations of losses though (Intel discrete GPU's basically have 0 marketshare, even worse than AMD who are also doing really badly) so even if that were true that isn't a reason to be optimistic.  As a company they also seem to be desperate to get income as well and so there is no reason to assume they're going to tolerate that situation for all that much longer.    I doubt they'll entirely kill off their GPU division, they can still do iGPU's which will be good enough and cheaper than paying NV a royalty, but the discrete GPU line is clearly in trouble.",Negative
AMD,A310 is cheap enough and good enough for it's purpose. If it ain't broke why fix it,Neutral
AMD,"Wrong man. You just don’t understand how small the diypc market is compared to prebuilts, laptops, and enterprise- it’s a fraction of one percent.",Negative
AMD,The later versions of Dlss 2 look like Dlss 3,Neutral
AMD,Nvidia probably feels the same about their low end SKUs.,Neutral
AMD,"The DIYPC market largely drives the “prebuilt” market, with a 1-3 generation lag.  Many people who buy “prebuilts” have people they go to for advice (i.e. they talk to their friends/family members who DIY). When there’s a big swing in the DIY market, it typically ripples out to the prebuilt market over a few years/generations.",Neutral
AMD,"Lol, what an explanation hahahaha   And that's still not ""better than DLSS 3"" as most say",Negative
AMD,Yeah lol,Positive
AMD,AMD Unboxed say anything about that? What about Linus Cheap Tips!,Neutral
AMD,"""Everyone I don't like is biased""-ass answer",Negative
AMD,"can someone explain how this might effect me, as a long term holder, as if I am a golden retriever?",Neutral
AMD,The MTL/ARL/LNL Dates are all wrong for CPU launch it's an OEM roadmap not Intel.,Negative
AMD,Medusa Ryzen... Sounds like something I'd rather avoid in real life.,Negative
AMD,Panther Lake is slipping to Q2 2026 now?    It was originally supposed to launch in Q4 2025,Neutral
AMD,Just hodl until you get the biscuits,Neutral
AMD,"If leak is correct it's bad news.   Panther lake q2 2026, Nova lake q2 2027.    It means 18A will arrive later than TSMC 2N.    Nova lake will likely arrive after Ryzen successors.    Combine that with other performance rumors where there isn't a huge performance uplift. It's bad news.",Negative
AMD,"The dates aren't wrong. As you said, It's the OEM roadmap, and they follow a Q2 release cadence.",Neutral
AMD,Gate All around and Backside power was supposed to be in 2024...  Now it's q2 2026. At this point it's 4N7Y... Which is what we call TSMC cadence.,Neutral
AMD,"This isn't Intel and AMD's road map, this is an ""undisclosed OEM's"" road map.   This doesnt tell us anything about PTL's launch. Just when this OEM will refresh their laptops to have PTL, and its normal for OEMs to launch refreshed models at certain times.   Dell, for example, will usually do Spring, so this may be them. Surface is notorious for being 3 or so quarters after CPU launch.  Look at where MTL is on this schedule, for example. Also around Q2, even though it had been on store shelves technically since the previous year's Q4.  Edit: Looking at the roadmap a second time, this OEM seems to always target a Q2 launch cycle: ADL, RPL, MTL, ARL, PTL, and NVL all follow it. LNL is the only outlier.",Neutral
AMD,I thought Arrow Lake refresh was in the cards for 2025.,Neutral
AMD,Wasn’t Panther Lake going to use TSMC for a portion of the volume?,Neutral
AMD,Yeah I forget to to mention wrong In the sense that it is not Intel CPU Launch OEM roadmap.,Negative
AMD,This entire thing is a mobile roadmap so why are you here?,Negative
AMD,"This ""undisclosed OEM"" is Lenovo by the looks of the font used. Also the timeline makes sense. For example Lunar Lake is shown to be in very late 2024. Lenovo had only one model of Lunar Lake available at that time - the Yoga Aura Edition.",Neutral
AMD,"Yeah if it's Surface roadmap, it's a nothing burger.",Negative
AMD,Yeah some OEMs don't refresh their lineups until the new generation is already launched,Neutral
AMD,I think it was for the iGPU but I don’t remember. I just know the majority of PTL is 18A. I saw a rumor that they were using Intel 3 for the iGPU also so idk.,Neutral
AMD,I also think it's Lenovo. Q2 is when they refresh their ThinkPad line,Neutral
AMD,The 4 Xe3 core iGPU will use Intel 3. The 12 Xe3 core iGPU will use N3E.,Neutral
AMD,"Wonder if Intel will do stacking two bLLCs on a single CPU tile like rumoured as an option with AMD's Zen 6. I iknow this is basically both Zen 5 CCDs having each an 3D v-Cache and NVL seemingly going to have that option now. But two bLLCs meaning 288MBs of stacked cache + cache in compute die would be insane and crazy for gaming.  Regardless I'd love a P Core only RZL SKU with much stacked cache as possible, or heck just 48MB 12P Griffin Cove + 144MB L3 bLLC would be sweet.",Neutral
AMD,"I like how they just throw random words around to pad their ""article"".",Neutral
AMD,The only reason I don't have a dual CCD ryzen is the lack of X3D on the 2nd CCD.  I would 100% be in for a 16 or 24 (when they go 12 core CCD) core dual X3D.,Neutral
AMD,"""leaks""",Neutral
AMD,"When are people going to realize ""Leaks"" are marketing ploys to get eyes on product. Come on people.",Negative
AMD,"Considering the latency is not even expected to be any good with BLLC, AMD is seemingly quite a careful opponent for Intel indeed",Negative
AMD,Scared of their rumor?  Lets release our rumor!,Neutral
AMD,I don't think 2x bLLC could fit on the package. Sounds like nonsense to me.,Negative
AMD,it seems as if right now the best option for consumers is to just build a cheap 1700 or AM4 system and call it a day for the next 4-5 years or so. but that's just me,Neutral
AMD,"My god.  The only reason huge caches benefit gaming, is to benefit rushes to shipment by the publisher, so that game engineers either don't have to optimize, or they can use garbage like Unity, or both.  This is going to make so many mediocre developers better... except no it won't.  It'll just get the market flooded with more garbage.  Also... diminishing returns, and I doubt AMD will put this in Zen 5 -- seems like a trash rumor, to me.",Negative
AMD,"3D cache will eventually become full of assets and dip, if and onlyif IMC is bottlenecked by random memory access.  - the ability for cache to transfer data from memory, to IMC, to cache has a FPS hit (cache hit-rate). This is due to the bottleneck of a chiplet-based IOD: compromising memory latency with cache hit-rate. Eventually the cache fills and relying on IMC to feed the cores anyway.  You can easily test this by monitoring 1% lows and increasing memory latency (lowering DDR4 B-die clocks/DDR5 hynix and increasing timings) to see how Cache holds up when full of assets while memory is being bottlenecked-   What games need is 10-12 P-Cores and an IMC capable of high speeds. Within a monolithic die to reduce relying on cache hit-rate and IOD chiplet bottlenecks.",Neutral
AMD,This is all they needed to sell for gaming again and yet they gave us all the e cores no one asked for jfc.,Negative
AMD,bLLC is not stacked cache,Neutral
AMD,What do you consider random? The article was perfectly clear.,Neutral
AMD,Do you have a specific application in mind? I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.,Neutral
AMD,Some rumors of 16-core dual X3D chip actually surfaced yesterday...,Neutral
AMD,"Isn't the latency similar? The advantage of the 3d cache is an even larger cache size, while the advantage of what intel is going to do with bllc is that there is less of a thermal issue so the clocks can be higher.",Neutral
AMD,Probably only on the skus with less cores.,Neutral
AMD,"They would have trouble fitting just 1 bLLC onto a package, 2 bLLC might only be viable for something like a server chip with a much larger socket.   It may be possible once Intel starts 3D stacking they're CPUs though, but we have to wait and see.",Negative
AMD,Yeah. Definitely just you,Neutral
AMD,You could literally make that claim with any CPU performance increase.,Neutral
AMD,Speak for yourself. Skymont ecores have raptor lake IPC. We need more of those,Neutral
AMD,"What CPU do you have? If its Zen 4 or slower, your cores are same or weaker than Arrowlake E cores in IPC",Neutral
AMD,This is correct. Stacked cashe comes later. I'd say around 1 to 2 years after. And that comes with stacked cores as well if they go with that path.,Neutral
AMD,"Correlating asset pop in with memory latency is utterly nonsensical.   Edit: Lmao, blocked.  Also, to the comment below, the creator of that video clearly has no clue what there're talking about. There's no ""bottleneck between cache and memory"" with v-cache. The opposite, if anything. Just another ragebait youtuber who doesn't know how to run a test.",Negative
AMD,"> I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.  It does, but i'd rather every core be equal in terms of per-core cache in a local sense. Thread scheduler can handle the CCD split (usually)  There are definitely workloads that benefit from X3D but either scale to many threads or are let down by poor OS scheduling.  It could even be a boon to gaming with well thought out core pinning of game threads.",Neutral
AMD,"No, because it would be the same die. Just won't fit.",Negative
AMD,He has a fair argument with the horrible optimization of modern video games but any innovation in the CPU space is a good innovation at this fucking point,Negative
AMD,Id love a 200 e core cpu with lots of pcie lanes for my home server. But I do not want mixed cores.,Neutral
AMD,Yea but I’m not paying for e cores that are barely faster than my current skylake cores. I just want p cores only.,Negative
AMD,"IPC is just 1 factor  Zen 4 clocks much higher, and got a far superior low latency caching subsystem   No one sane thinks that Skymont can actually beat Zen 4 in most workloads Only if you downclock Zen 4... and guve it slow ram to increase latency",Neutral
AMD,"Now pay attention to the memory bottleneck of chiplet IMC architectures on x3d chips:P https://youtu.be/Q-1W-VxWgsw?si=JVokm7iLScb7xynU&t=700  This simple 1% lows test indicates how texture-pop can impact frametime, directly. Since assets are first stored in memory, then fed to cache, any bottleneck between Cache - to memory will cause delays when loading asset thus 1% FPS... measured temporally(in this case).",Neutral
AMD,The bad optimisation critics aremostly people trying to sound smart by rambling things they dont understand.  Game engines are very optimized these days.   Its funny they mostly pick on Unreal Engine 5 as the prime example... That is probably the most complete game engine out there. So naturally its hard to make all that stuff work well togetter... They have litterally some of the best coders and researchers in the business ... Id love to see you do a better job.,Negative
AMD,Mixed cores are awesome if the scheduler would do what i want and there lies the problem and i dont think even AI can solve that yet.,Negative
AMD,"I said IPC of Zen 4 is the SAME and weaker than Zen 4 is weaker than Skymont in IPC. No more no less. As for clocks, the core scales to max 5.2ghz (default 4.6ghz), certainly overall slower than Zen 4. IPC is a comparison about throughput at a given clockspeed. Its a give to compare you will need to match the clocks.  Funny you you should mention latencies, considering that core to core (even accross clusters), latencies of Skymont are better than Lion Cove? So if latency is the issue, it has nothing to do with Skymont and everything to do with the trash uncore they have based on the Meteorlake SOC. Even workstation Crestmont on a different package performs better. Furthermore skymont IPC is measured based on Arrowlake. Arrowlake is worse latency than Raptorlake and Zen already. Latencies have been accounted for.",Negative
AMD,"I play Warthunder almost daily. It’s ridiculously fucking optimized.   I got cyberpunk when it came out, it was a shit show. Elden ring? Mediocre.  Escape from tarkov? Fucking laughable.  Call of duty? 500 terabytes. Besides this small list,  I’ll wait for battlefield 6 (which by the way also has a history of releasing unoptimized)  The one thing I will say is that developers do work hard after releases to optimize their games. I’m just so use to playing Warthunder which almost always works, everyday, perfectly, with no complaints. lol",Negative
AMD,"Do tell how a bigger cache makes games faster, if you are aware of some details.  And, of course, how doubling the already tripled cache will yield perf improvements worth the huge amount of extra money to make such chips.",Neutral
AMD,Any efforts of Intel to improve efficiency and even performance on old products is a pro-consumer move. Good to see.,Positive
AMD,It's not sorcery. Its just Intel doing the game developers work.,Neutral
AMD,"Is it updated? Last time I tried it, it didn't have an uninstaller and not many games were supported. In the ones I tried, there was no difference at all.",Negative
AMD,"It's very specific scenarios where this can unlock new performance, most games it won't do anything or even lower performance.     So they can't *really expand it* since it's an uncommon scenario. That's why we have so few games that benefit from it.",Negative
AMD,Never count out Intel. They have some very talented people over there.,Positive
AMD,"Regarding the cache optimizations you pointed out, I have a question. Can you test all APO supported games you own and capture the utilization of the E-cores ?  My understanding is that Intel is keeping 1 E-core per cluster active and parking the remaining 3 E-cores. So the lone E-core of each cluster has full access to the 4mb shared L2. For the i9 they can have 4 such solitary E-cores that have their own cache and hence do not touch the L3 cache. The 4 E-cores handle background task while the 8 P-cores with the full 36mb L3 cache deal with the games.  Please correct me if I am wrong.  Also, what cpu cooler do you use ?",Neutral
AMD,What about this post saying APO havent gotten anything in over a year.. https://www.reddit.com/r/intel/s/tTKCn1CnDZ,Negative
AMD,Does anyone know if changing the Windows power plan affects performance? What Windows power plan do you recommend for the i7 14700kf? Balanced? High performance?,Neutral
AMD,I wonder why APO isn't just tied to game mode. That way it would work for all games from ADL to ARL.,Neutral
AMD,"or just get rid of E cores. I know all the non engineers in here think they are great, but real world they are crap, this is just another in a long line of examples of why intel needs to ditch them.   anyway go ahead and downvoted me becuse being correct in this sub is looked down upon.",Negative
AMD,A few years ago you'd have to tweak the hell out of AMD CPUs to get comparable performance to Intel equivalents. Now the tables have turned.  In either case those who know how to tweak will get top performance not matter the platform.     Kudos,Positive
AMD,Let me teach you a trick.... there is a hack to disable e cores for every game using Intel compatibility tool.... you can try yourself to see if it works,Neutral
AMD,I will follow all!,Positive
AMD,Congratulations on getting a CPU that didn’t fry itself.  Consider yourself one of the few lucky ones.,Positive
AMD,"It looks like Intel APO almost completely disables the efficiency cores for the game when you look at the screenshots I posted below with per core activity.  But this performance boost likely cannot just be gained by turning off the efficiency cores in the BIOS yourself I wager, because it would be too simple.",Negative
AMD,How is this doing the game developers work?,Neutral
AMD,"It's updated as far as I know, but not for every platform.  If you're on Arrow Lake you have the new games that were added, but if you're still on Raptor Lake then you only have the original titles.     This sucks because I own BG3, which is one of the new titles they added recently along with Cyberpunk 2077.",Negative
AMD,"They have added a few new games, but it appears you need to have Arrow Lake to be able to apply APO on them which is BS!  They need to have a better system than this.       The DTT driver should be downloadable from Intel, and not from the motherboard manufacturers who cannot be trusted to make the latest version available.",Negative
AMD,The intel software team is pure black magic when they allowed to work on crack.,Negative
AMD,"Here is a screenshot showing CPU activity across all cores.  This is with HT disabled BTW, and the cooler is an Id FROZN A720.  It looks like the efficiency cores are not engaged in this particular game when APO is enabled.  And Metro Exodus is the only game that I own in the current lineup for the 14900K.  I know they've added some games for Arrow Lake (like Cyberpunk, Baldur's Gate 3) but those games still aren't available yet for the 14900K     [https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png](https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png)",Neutral
AMD,"Raptor Lake isn't getting new games for APO.  Which strikes me as greedy, since I'm sure it would work fine. Intel originally said that APO wouldn't work at all on 13th gen, which turned out to be completely false.",Negative
AMD,"When combined with Nova Lake's bLLC, it could give Intel a major edge over AMD's X3D.       And yeah, limiting it to certain games and certain CPUs is a real downer for the technology, so hopefully Intel can find a way to streamline and expand it when Nova Lake launches.",Positive
AMD,"AFAIK, it's recommended to use Balanced mode for proper use of the efficiency cores if you have a Raptor Lake or Arrow Lake CPU",Neutral
AMD,Balanced in full load will just do the same thing as High Performance.,Neutral
AMD,"It's capable of a few things, but the main mechanism from what I've seen is that it temporarily disables three out of four cores in each E-core cluster by forcing them into the lowest power state. The result is that the remaining active core has access to each cluster's full L2 cache (2MB on 12th-gen, 4MB on 13th/14th-gen). L2 cache isn't shared with P-cores (L3 is global), so this can really minimize E-core cache evictions before they're forced into slower memory, and games do love themselves some cache.  It's a genuinely clever way of maximizing available resources and I really wish they'd allow user control over its features, but it seems to be pretty tightly leashed to/by the team that developed it. It obviously wouldn't benefit every situation, such as particularly low/high thread occupancy situations, but it's pretty rough to have the option tied to quarterly updates for one or two specific games.",Neutral
AMD,Or... Just process lasso.,Neutral
AMD,Because it’s optimizations on how it can efficiently use the cpu.,Neutral
AMD,Must the App be downloaded? Or it's automatic after installing the PPM and DTT drivers?,Neutral
AMD,"All of this stuff is super situational. For when it’s supported for my 14th G, I love it.  To call it better than X3D though? That’s a bit much.",Positive
AMD,They are they came up with ML for their upscaller before the red team did and even better than FSR as a dump upscaler and they also did a good RT implementation. But the hardware still has raw horse power issues and they already use a big die for what it can do.  In the 80's or they shammed The DEC Alpha team on subnormal floating point operations with the 8087 FPU. Intel was doing way more precession without having to round so earlier on a consumer FPU while the Alpha CPU was a Mainframe one. Intel also gave it up to help estandardize the way CPUs handle floating point types.  Intel compilers were also black magic becuase they optimized so much C and C++ for Intel specifically but there was always controversy causeless AMD x86 couldn't take part of those optimizations,Neutral
AMD,Were there any background tasks running when the screenshot was captured ? I want to see how the E-cores are assigned for the background tasks under APO.,Neutral
AMD,In certain games. It's not universal at all. There would be scenarios where it gets an edge but if AMD is pushing as hard as rumors state they may still have the better CPUs overall.,Neutral
AMD,"Great explanation that makes sense!  It definitely has to do with the cache, that much is true.   A Raptor Lake CPU with 8 P cores and no efficiency cores, with 36MB of L3 would perform exceptionally well in games.   But the efficiency cores have their uses as well in highly parallel tasks.",Positive
AMD,"Yeah it's a pretty cool, yet simple idea. It would be really nice to give users more granular control natively.",Positive
AMD,Are the e cores clustered in order so we would be able to manually achieve this with process lasso?,Neutral
AMD,Why should game devs be automatically expected to specifically optimize their game for specific architectures of Intel's CPUs? It's not on them to do so.,Negative
AMD,You need to download the Intel Application Optimization app from the Windows store,Neutral
AMD,"I never said it was better than X3D, only that it is useful as a foil.       Remember that Nova Lake will have bLLC, which is the answer to X3D.  But in conjunction with APO, it may give Intel the edge and help them regain the gaming crown in the next cycle provided they can streamline and expand the technology.",Positive
AMD,"Actually, it is better than X3D in almost every GPU/CPU combo where the GPU throttles. Intel 14th gen almost universally wins 1% lows and frequently FPS - almost universally in 4k gaming, but as I said, when the GPU throttles at any resolution, which is almost always, Intel wins.",Positive
AMD,"Eh, they're very good but they benefited vastly from AMDs mistakes and the lessons learned from it. Remember AMD didn't have any ai hardware for their first 2 gens on RDNA and RDNA 3 didn't have enough for FSR4 either. It's not that intel were better at producing a ML upscaler. They just had the hardware for it from the get go so they could implement it quicker.  They did make Xess though which is miles ahead of FSR 3 even on AMD systems (on average. FSR3 is pretty good when modded and can be competitive) so they do have some extremely talented people there. I hope they fund them properly and don't go the AMD route of having Ryzen and Radeon not work together due to pride.",Positive
AMD,"Just  Steam, MSI Afterburner, Windows Security, Nvidia App and my sound card command center",Neutral
AMD,Where to download this APO,Neutral
AMD,Except its THE game devs job to optimize games for multiple cpus and gpus.,Neutral
AMD,It’s literally their job to do so? wtf you talking about?,Negative
AMD,AMD also has Heterogeneous CPUs and even more so ARM cpus.  Devs should start to do this optimizations to schedule the stronger cores.,Neutral
AMD,"Will do, I got a 265K. Performance is already great tbh.",Positive
AMD,I love Intel but with how the company is doing right now I’m in a “I’ll believe it when I see it” type of skepticism.,Neutral
AMD,"Sorry I am making unreasonable requests. Can you test the game with APO on, and a few background tasks ? Like a few browser tabs, a youtube video playing, discord etc ? I am interested in the E-core usage, particularly the spread of E-core usage.   Thanks.",Neutral
AMD,"You need to enable it in the BIOS, me ale sure the DTT driver is installed and then download the app from the Windows 11 store",Neutral
AMD,"Which they already do, and are often overworked and underappreciated for.   But sure, it's their fault they didn't optimize the game even more for a subset of already relatively high end CPUs....",Negative
AMD,"You are massively oversimplifying. A lot of games that support APO, like Metro Exodus EE OP used as an example, were released before Intel released CPUs with P and E cores. Expecting studios to rework and optimize old games for new CPUs is very naive.",Negative
AMD,"They should be focusing on optimizations that help the vast majority of all CPUs, especially older/lower end ones.   It's not their job to specifically help one companies latest CPU architectures because they couldn't figure out how to create a core that has both good area and performance.",Negative
AMD,That's not simply what APO does.,Neutral
AMD,"You should have the full list of current APO optimized titles, unlike myself which only has the original list LOL!",Neutral
AMD,"Everyone should be skeptical, sure, but I choose to be hopefully optimistic also.",Positive
AMD,"I booted it up again, this time with three Chrome tabs open including one that had an active YouTube video playing.  The efficiency cores were definitely being utilized for the background tasks I believe as I could see a small bit of load going from core to core.     [https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png](https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png)",Positive
AMD,i am not talking about older games. i am talking about newer games.... really dude?,Neutral
AMD,Ok…all CPUs include Intel CPUs…I don’t get your thought process,Neutral
AMD,"According to Intel, the app is entirely optional. The DTT driver already includes APO, the app is just an interface to control it and disable APO, if needed.",Neutral
AMD,Thanks a lot.,Positive
AMD,> i am not talking about older games. i am talking about newer games.... really dude?  New games seldom have problems with P and E cores. That is why if you took at minute to look at games that APO support you would notice almost all were released before 2022 before release of Alder lake CPUs. Because older P and E unaware games struggle. Don't really dude me if you are this clueless.,Negative
AMD,"APO includes specific optimizations made for specific Intel skus, not general optimizations that help pretty much all processors.",Neutral
AMD,That is not true whatsoever. Games do not properly utilize the p and e cores and unreal 5 is very known for this.,Negative
AMD,Ok…? If there is a specific problem then they need to optimize it for that hardware. Literally a devs job. Still don’t get your thought process,Negative
AMD,"Dude, Unreal 5 runs like crap on pretty much all hardware. It stutters on AMD and nVidia graphics. Unreal 5 doesn't count.",Negative
AMD,"There isn't a specific problem, it still works lol, just not as well as it could on a specific architecture.   They shouldn't be wasting their finite resources/time on optimizing specifically for an already relatively high end and well performing architecture, but rather attempting more generalized optimizations that help all users.   It really shouldn't be too hard to understand.",Neutral
AMD,"Very cool. Cheers Intel! As a 6900XT owner, I use Xess whenever it's available. The DP4A version got very solid once V1.3 hit.  Hopefully more devs implement it.",Positive
AMD,"I like how Intel still support DP4A model. My meteor lake chip gonna love this XeSS2 with FG and LL.  Meanwhile Amd give middle finger to their consumer who don't have radeon 9000 series, FSR4 doesn't works at all on their previous GPU including the flagship rx 7000 series.",Positive
AMD,Did the DP4A version also improve from 1.3 to 2.0?,Neutral
AMD,"I thought it was a mix of both shader model 6.4+ and DP4a, not either or",Neutral
AMD,Okay but why would I want to use that instead of NVIDIA DLSS?,Neutral
AMD,It’s the least you should get after not getting FSR4.,Negative
AMD,You'd use it over FSR if that's available too?,Neutral
AMD,Except you NEVER got acces to the DP4a version tho. Intel cross vendor locked the DP4a path to their igpus and only allowed Shader Model 6.4 version for other gpus.,Neutral
AMD,"I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4.   On Linux they managed to emulate it using the FP16 instruction, which is available on RDNA3, but it seriously impacts performance and introduces 1,5ms of input latency, compared to the 0,5/0,8ms of latency on RDNA4 with hardware FP8.",Negative
AMD,And they expect people who bought previous RDNA to buy more RDNA,Neutral
AMD,Not by a significant amount.,Neutral
AMD,DP4a only relevant for intel igpus as its crpss vendor locked by intel. Other gpus only get the Shader Model 6.4 version.,Neutral
AMD,for the games that dont support DLSS,Negative
AMD,"If you own a card that doesn't support dlss.   This is an extra option, it doesn't affect your choice to run dlss or not.",Neutral
AMD,10 series cards will benefit from this,Positive
AMD,It's great news for gpu that can't use dlss because fsr3 is a blurry shitfest.,Negative
AMD,"I mean it gives better image quality than FSR 3.1, so I know I definitely do.",Positive
AMD,Sure. The image quality is generally better. I'd only use FSR if I was trying to squeeze every last drop of performance out of my GPU since it's faster than Xess DP4a. But Xess will be tried first.,Positive
AMD,Do you have a link? Anything I have seen points to the GPU using Xess DP4A unless it's an ARC GPU. So it supports RDNA2 and newer on AMD and Turing and newer on Nvidia.  https://www.pcgamer.com/intel-xess-compatibility-nvidia-amd/  And I do literally run Xess on my 6900XT (currently using it for F7 Rebirth through Optiscaler). So if I'm not running Xess DP4a then what am I running?,Neutral
AMD,">I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4   Sure, but they can make FSR4 for non hardware accelerated like Intel right? FSR 3.1 is so bad even when compared to Intel XeSS 1.3 DP4A, but Amd refuse to improve it because they want to force people to buy radeon 9000 series.",Negative
AMD,"Amd really all in when it comes to copying Nvidia. Guess what? The more you buy, the more you save!",Positive
AMD,DP4a is cross vendor.   XMX is Arc only.,Neutral
AMD,DLSS is the widely adopted upscaler in the current market. I'd be shocked if a major game in 2025 releases without DLSS at the very least.,Neutral
AMD,1080ti heard no bell,Neutral
AMD,"You run the Shader Model 6.4 version which i stated. There was a discussiona bout DP4a being cross vendor locked but for the life of me i cannot find it again. The article you linked is from 2021, in early 2022 intel cross vendor locked DP4a. The main article of the post clearly states Shader Model 6.4.",Neutral
AMD,where did amd touch you bud?,Neutral
AMD,"Yeah more and more games are adding DLSS support, so I'm not sure when I would need Xess",Neutral
AMD,"That's true but in edge cases where even the lightest Xess option is still too heavy, FSR may still deliver a better overall experience.   E.g. CP2077 on the steam Deck in Dog Town. When I was testing some stuff out (trying to see how close I could get my Deck to the Switch 2 build) I found Xess to be a unsuitable. Even on the lightest Xess mode, the performance hit was a bit too noticeable. FSR doesn't look as good as Xess but it runs better and you can get away with a bit more upscaling artifacting on the Decks relatively small screen. Just rendering at a lower res and letting the screen upscale was also a solid option in that scenario.  As I say though, that's an edge case where you really just need outright speed. If your GPU doesn't support DLSS or FSR4 then using Xess makes sense in the vast majority of cases.",Neutral
AMD,Are you sure you're not getting the DP4A/Shader Model 6.4 and Arc exclusive version mixed up? DP4a was added to Shader Model 6.4. https://github.com/microsoft/DirectXShaderCompiler/wiki/Shader-Model-6.4.  There is a separate path of Xess that is better than DP4A but that only works on Arc Cards.,Neutral
AMD,"It's good to see finally Intel working with game developer to bring optimization to Intel platform. This time not only optimization for Arc GPU, but also for Intel Core CPU like proper CPU scheduling for P and E cores to make sure the game runs on the right thread.  I think this also the reason why they never update APO game support again. Obviously optimizing their platform by working directly with game developer is better than doing it by themself.  I really hope we got optimization like this for more titles.",Positive
AMD,News like this will never been posted on Yahoo Finance or Stocktwits... Instead the news from Short-Sellers and Paid Bashers ...,Negative
AMD,"Im not impressed, my i9 12900k is bottlenecking my 4080 like crazy in bf6 beta :s  Im only getting 100-140 fps on my cpu while my gpu wants me to have 200. Any kind soul who can explain how i increase this?  My cpu is right out the box, i have not done any OC what so ever",Negative
AMD,13700k vs 9800x3d BF6 Beta menu cpu fps comparison (105 vs 370 fps) [https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8](https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8),Neutral
AMD,You sound like an intel stock holder lmao The markets don't care about gaming at all.  They only care about AI slop,Negative
AMD,"the game is just really latency sensitive, i dont think it will even change in the full release sadly  i kinda wonder what makes the game so demanding on CPUs, the mechanics and physics arent much better compared to older BF (like BF4) that barely use modern CPUs beyond 20 - 30%",Negative
AMD,"Dealing with same issue on a 285k + 5090. dipping to 125 fps curing certain scenarios specifically on the cairo map at point C. MY system should not struggle at all at 1440p to reach 144 fps, but the 285k is getting absolutely destroyed by bf6 and I have no idea what the issue is,",Negative
AMD,"It's not like game runs bad on Intel CPU which has P and E cores, i will be lying if i say that. It's about maximizing CPU performance. CPU demanding game will runs with much higher FPS when it knows which thread should be utilized.    This is why game with APO enable like R6S on i9-14900K at 1080p low settings, it got performance increase up to 100FPS which is massive.",Neutral
AMD,"Most games are optimized for consoles which are natively eight cores, and a lot are ported over to PC and so you basically need eight fast cores in whatever CPU you happen to have, and the rest don't really do much. That's why Intel HEDT and AMD's Threadripper aren't of much use in this regard.",Neutral
AMD,"No, that's because and isn't really better for gaming unless you get their x3d variants. Those have a different style of memory that is better for gaming in particular but can underperform when it comes to workload tasks. Idk why 🤷",Negative
AMD,"Yeah and in a game like bf, I only care about the 1% low staying at minimum my monitor refresh rate 🫠 But I have not spend that much time optimizing my settings so I think I can work around it",Neutral
AMD,"I found a ""fix""   Dynamic render scale, I set it to 75, so when shit really hits the fan, it just lowers the render scale for just enough time for me to not drop below my monitor refreshrate, and it happens so fast I don't see the change in gfx",Neutral
AMD,"From what I understand, many games are generally fairly latency sensitive. The fast cache on the X3D chips basically helps with that.",Positive
AMD,Thank you :),Positive
AMD,Wouldn't the 300 series actually be Arrow Lake Refresh?,Neutral
AMD,"Videocardz is wrong      This chip will likely be called ""Core Ultra 5 445k"" it's an 8+16 die with bLLC + 4LPe   (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")    Ultra 3 has 4P + 4E + 4LPe  Ultra 5 has the 1x 8+16 tile + 4LPE   Ultra 7 has 8+6 P-Cores and 16+8 E-Cores +4LPe   Ultra 9 had 2x 8+16 tiles + 4LPe tile   Ultra 300 series will be reserved for mobile only Panther Lake and/or Arrow Lake Refresh   Ultra 400 will almost certainly be Nova Lake.",Neutral
AMD,I think this competition will tight up alot in the next gen for both intel and AMD.  Intel has had a superior memory controller for awhile now and AMD will finally address this in Zen 6.  AMD has better cache setup with VCache which Intel will finally address in NL.  May the superior arch win!,Positive
AMD,This is pat's work,Neutral
AMD,This is normal Nova Lake or the bLLC version? I heard the bLLC version has separate SKUs.,Neutral
AMD,8 Big cores with loads of cache and then a couple of E cores with their own cache for background tasks would be great.,Positive
AMD,Is Intel finally making a comeback with their cpus? I hope so,Positive
AMD,Large L3 cache without reducing latency will be fun to watch.,Positive
AMD,the specs sure do shift a lot..,Negative
AMD,"Now we are talking Intel! If they can hold on to these chips, not melting themselves, then it's gonna be a good fight",Positive
AMD,it's just a bunch of cores glued together - intel circa 2016 probably,Neutral
AMD,Not sure I can trust Intel again after the BS around Arrow Lake and the lack of ongoing LGA 1851 upgrade path.,Negative
AMD,Bout time,Neutral
AMD,"Like I commented before, hope (but really don't believe) this is NOT the product that is only 10% faster than arrowlake because liquidation awaits them otherwise",Negative
AMD,Will it be available in fall of this year or 26Q1?,Neutral
AMD,So it only took them like 5 years to come up with a luke-warm response. cool cool. It's especially bad as Intel actually introduced something similar (eDRAM LLC) into the x86 space many years ago.,Negative
AMD,Noob question:  Is this their 16th gen chips?,Neutral
AMD,Planning on upgrading from my 13700K to whatever the top end Nova Lake is,Neutral
AMD,Only Core i3 can have 65watt ? no more i5 at 65Watt ?,Neutral
AMD,"For some reason it seems to me that all these new different micro cores are cores from ten years ago at the level of Broadwell or Lynnfield, but made on thin nm to fit everything under one lid lmao",Neutral
AMD,Cool.  I have said for a while now that is want both the extra cache AND a few lesser cores to offload stuff to.  The only alternative that has this atm is the 9900x3d and the 9950x3d.  Next gen when im buying new stuff then perhaps intel will have an alternative to those then.,Positive
AMD,"The 52 cores is exciting, but I would probably use Linux as my main OS and run Windows in a VM. Plenty of cores and plenty of RAM and it would be a great machine.  A 52 core desktop beast with an Intel graphics card with 128 GB VRAM would be a killer machine. LLMs would run like lightening.",Positive
AMD,"Intel mocking the chiplet design, describing it as gluing processors together and then doing it themselves - this screams that energy once again.",Negative
AMD,So Nova Lake IS happening? This talk about skipping to next node had me anxious..,Negative
AMD,"So alder lake on cache steroids ?  If the heavier core count chips don’t have the LLc cache, and if this chip is faster at gaming than the heavier core count chips, how will Intel market it?   If they call it a core ultra 5 or 7, then that will make the core ultra 9 seem less of a halo product.  Will be interesting to see how they market it.",Neutral
AMD,"E core need to get lost for gaming, we need 9800x3d rival, purely for gaming !",Neutral
AMD,"""E-Cores"",  Ewww, Gross.",Negative
AMD,"**Hi,**  I’m currently running a 9700K paired with a 5070 Ti. Do you think I should upgrade to Arrow Lake Refresh now, or hold out for Nova Lake?  The CPU’s clearly starting to bottleneck a bit, but it’s not super noticeable yet.  Is it worth waiting for Nova Lake, or better to just go with Arrow Lake Refresh when it drops?",Neutral
AMD,NVL will definitely be the 400 series. PTL is 300.,Neutral
AMD,"Naming can be changed, but the most important thing is, this isn't the upcoming arrow lake refresh  I hope arrow lake refresh fix the memory controller latency tho",Neutral
AMD,"Yes, I expect the 300 series to be Arrow Lake refresh and Panther Lake.",Neutral
AMD,"> (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")  Maybe they will bring back the ""C"" from i7-5775C.   Behold, the Core Ultra 7 465KFC",Neutral
AMD,I suspect it'll be like:  495K: 16+32.  485K: 12/14+24/32.  475K: 8+16.  465K: 8+12.  Etc.,Neutral
AMD,Seeing all those different cores on a chip does look cool. But I presume this idea will end up being butchered by Windows not being able to schedule things properly,Neutral
AMD,"So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake? Or in 2027? Either way Zen 6 vs Nova Lake will be interesting.",Positive
AMD,Taking the 'L' to their own CPUs would generally be considered a bad marketing move.,Negative
AMD,"We'll see how far Zen 6 takes us on memory. Strix has improvements, hopefully that wasn't all of it though.",Neutral
AMD,"If intel doesn't screw itself with stupid tdp, reliability issues, or bad relative performance",Negative
AMD,More like *despite* him.,Neutral
AMD,All the SKUs rumored so far are BLLC,Neutral
AMD,"They have graduated from background tasks. At this point, even LPE cores can handle apps like Microsoft teams by themselves with no issue never mind the E cores on the ring bus",Neutral
AMD,"If the scheduler can really support this. As soon as a game would hit the e cores, it could turn bad.",Negative
AMD,Why not 8 P cores with HT + even more cache and no e cores at all,Neutral
AMD,"A large L3 will help a ton with latency. It's why AMD has so much success with it using chiplets which have a ton of extra latency. Reduces all those higher latency trips out to RAM. Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile and if that's the case this L3 might be a really big hit for gaming, etc.",Positive
AMD,If latency slightly increases or stays the same due to adopting Pantherlake optimizations and the newer Foveros then that might be enough to narrow the gap,Neutral
AMD,I assume the cache is based on Lunar Lake's memory side cache which doesn't bring much if any noticeable latency over Arrowlake H,Neutral
AMD,"Also done with them. Changing socket every \~3 years was also bad, but this is disgusting.",Negative
AMD,"Not sure how it could be, given all the extra cores.  That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  I expect NVL to be a big power/thermals/multithreading win, than anything else.",Neutral
AMD,Nova lake? More like 26Q4.,Neutral
AMD,Only Pantherlake for mobile,Neutral
AMD,"It doesn’t matter so long as all major vendors will automatically ship with whatever Intel puts out. Lenovo, dell, and HP carry a grand total of *zero* desktop Zen 5 (granite ridge) machine across their consumer, gaming, and business line. You heard that right, *zero* among the global big three.",Neutral
AMD,"Since when they are ""mocking chiplet design""? Have you forgot Intel is the first when it comes to gluing chip together? Intel did this with Core 2 Quad. Meanwhile Amd copying that with ryzen but sadly people don't want to admit it.",Neutral
AMD,It is really happening. The only question is when? Or can they release it on the next year without delay?,Neutral
AMD,"Core Ultra 9 has 48 cores (and 4 LPE cores). It slaps you in the face as halo SKU (unless it becomes entry level threadripper target?).  Currently rumored:  (8+16E+BLLC, 8+16E)  8+16E+BLLC  8+12E+BLLC",Negative
AMD,The i9 sku is rumored to be two 8+16 tiles. So slotting in an extra bLLC 8+16 tile instead of a normal 8+16 one for the top sku doesn't sound impossible.,Neutral
AMD,Probably same way AMD did the 5800X3D 8 core which was slower than the 5800X at productivity and had less cores than the 12 core 5900X and 16 core 5950X.,Negative
AMD,"E cores are the future, P cores days are numbered.",Neutral
AMD,ARL refresh is not worth it. Go AMD or wait for NVL if you want.,Negative
AMD,If you have to upgrade now for gaming get a 9800x3d from AMD otherwise wait and see what AMD Zen 6 and Nova Lake can do.,Neutral
AMD,"You just missed some pretty killer deals around Prime Day; that would have been the ideal time to go with Arrow Lake.  The 'refresh' will offer better performance, but I highly doubt the value difference will be significant.     You can definitely wait a bit.",Negative
AMD,"I would like the naming with suffix ""C"" for cache at the end or maybe ""G"" for gaming.",Neutral
AMD,"W11 has (mostly) fixed the scheduling issues, in fact the scheduler was already pretty good by the time Raptor Lake launched. Sometimes it can mess things up but it's quite rare nowadays. The only time I have seen some small issues is with legacy single threaded software.",Neutral
AMD,"Why would it be butchered? the issues with big/little are long gone now - but I guess to your type none of that matters - getting upvotes for blind anti-MS zealotry is all that matters.  but given that this is an intel sub, i am surprised its not merely blind anti intel zealotry instead.",Negative
AMD,">So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake?  Yes. Intel has confirmed NVL will launch in 2026.",Neutral
AMD,The former ceo,Neutral
AMD,"Raichu has repeatedly talked about BLLC ""variants"", so I wouldn't assume that all skus have the large L3.   Plus, it seems extremely wasteful to have all the skus have BLLC.",Negative
AMD,"Yeah, Skymont are more like space efficient P cores that are down clocked and don't have all of the features. IPC is really good.",Positive
AMD,This is wrong. Arrow Lake already uses E cores for gaming to no performance hit.   12-14th Gen sure. But not with the new E cores and no HT. You’re using dated info.,Negative
AMD,"A number of public tests have shown that, even with all the P-cores disabled, ARL is still pretty good in gaming.  It's not the E-cores.",Positive
AMD,No different to 12-14th gen then.,Neutral
AMD,"windows pretty much gave up on this and just allows you to set to performance mode, which always tries to grab a P core first.",Negative
AMD,"Or that I forgot they don't have HT on the newer chips.   To be honest I was meaning 14th gen P and E cores with more cache not the new gen stuff. We know the IPC, memory controller and core speed is enough it's just the cache that makes it fall behind AMD in gaming.",Neutral
AMD,E core is 30% faster than hyper threading,Positive
AMD,HT is worse than E cores,Negative
AMD,>Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile  They aren't,Neutral
AMD,It will enlarge the ontile L3 cache most likely with a cache only tile with high performance die to die connect. No 3d stacking until 18A-PT is available for production which will likely be well after Nova Lake tapeout which should be done now or with in the next few months in order to ramp up production in early 2026 and be on the market by late 2026.,Neutral
AMD,">That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  If you are in the lead, maybe this argument is valid. However Intel is behind Apple in ST and behind AMD in gaming. They have plenty of room to improve.",Neutral
AMD,"> That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point  And yet we see that constantly.",Negative
AMD,"Ze3vsZen2, Zen 4vsZen3, Zen 5 X3D vs Zen 4 X3D, Alderlake vs RocketLake, Raptorlake vs AlderLake, Lunarlake/Arrowlake-H (Mobile) vs Meteorlake U and H, Gracemont vs Tremont, Skymont vs Gracemont and Crestmont, Many Arm or Arm based designs, etc",Neutral
AMD,ARM chips regularly do this sometimes on a yearly basis.,Neutral
AMD,A lot of gamers have no clue that Intel is actually doing well with ARL for OEM laptops/dekstops as ARL is basically a laptop design warmed over for desktop chip usage.,Neutral
AMD,Core 2 quads and Ryzen chiplets are totally different ways of doing MCMs  C2Q didnt have an IO die and it pretty much worked like having a 2 socket motherboard      IO die is what enabled much lower latency and the ability to connect much more CPU dies together (EPYC 9005 got 16 CCDs),Neutral
AMD,https://www.pcgamer.com/intel-slide-criticizes-amd-for-using-glued-together-dies-in-epyc-processors/,Negative
AMD,"> They stopped trying to sell capacity to external customers and they will try again with 14a. It's not like they had a lot of capacity to spare. It's not even enough for themselves  Intel claims they can double to 1.5x their EUV capacity in arizona and ireland, if customers asked for it, in 1-2 years.   They built out all they needed for their own internal 18A demand too.",Neutral
AMD,"It shouldn't become an entry level workstation part since the Ryzen 9 will have 24c/48t to go against it.   48 cores (plus the 4 LPe, but those shouldn't factor in too much) should handily beat 48 threads, but not by enough to reach Threadripper territory.",Neutral
AMD,Most likely the bLLC variant wil be single compute tile and single cache tile as this would be cheaper for now than putting bLLC cache in base tile like Amd did. They can do that when 18A-PT is avaliable which will probably be after Nova Lake goes into production with 18A or maybe 18A-P,Neutral
AMD,"Lmao, some of my games still runs on e cores",Neutral
AMD,People are still disabling e cores for more performance.,Neutral
AMD,"Varients because there are two tiles. One with BLLV, one without. Full chip had 1 BLLC chip and 1 without. No actual SKU (not tile) without BLLC in some way so far",Neutral
AMD,"They are nothing like P-core as designed with no hyperthreading, area/space efficienct and integrated into clusters of 4 e-cores with shared L2 cache",Neutral
AMD,fym no different they're 50% faster,Positive
AMD,No. ARL architecture is definitely not optimized for latency but hiding memory access which favours multithreading instead of gaming. Hopefully Nova Lake will optimize all the latecy issues with ARL arch and decrease latency to L3 considerably aswell,Negative
AMD,"Yes, but in games, it's the P-cores that are meant to be utilized. Also, more than eight cores or 16 threads aren't really necessary for gaming. The 9950x3d isn't faster, than the 9900x3d.  My point is that the space taken up by the E-cores could instead be used for additional cache, not?",Neutral
AMD,I had heard this both ways and never really know who to trust until I see it. Anyhow if that's true the large L3 should be even more helpful for things like gaming as it will keep those higher latency hits down. AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help.,Neutral
AMD,"As a advice, Nova Lake will further increase memory latency.",Negative
AMD,In which gen iteration?,Neutral
AMD,Editorialized. But its funny because said quote is attributed to AMD when Intel made Core2Duo (or Quad?),Neutral
AMD,"If its is with 10% of threadripper 9960x at half the price, it will definitely hurt the low end threadripper cpus a lot for productivity/creator types who would love a more reasonable priced high performance entry level workstaion. Low end workstation is probably bigger market than high end gaming desktops anyways.",Negative
AMD,"Another factor is that Apple has booked out all the N2 capacity for the better part of a year before AMD can touch it.  Nova Lake will have been out for a while, before AMD shows up",Neutral
AMD,Process lasso can let you just designate by task so the E cores can do background and side jobs. People use process lasso on the 7950x3d to make the X3D chiplet run games with those 8 cores and the other 8 core chiplet for everything else.,Neutral
AMD,What issue did you run into with a 9800x3d? It has one CCD and no small cores,Neutral
AMD,"Nowadays there are very few cases where disabling E cores is extremely beneficial, sometimes they are a detriment to performance but more often than not they are useful. Not to mention the E core design team is more competent than the P core team and are quickly catching up in performance.    The E cores in Alder Lake had relatively poor ST performance but in Arrow Lake they are much faster and closer to the P cores in raw performance. Every new generation the gap keeps shrinking and it will make the disabling of E cores less and less relevant.    Sometimes the Windows scheduler will wrongly assign an E core to a ST bound task, even if a P core is available, but it doesn't happen often enough to be a serious issue.",Neutral
AMD,"yes, but not because of windows scheduling - its because you can OC higher with them off - better temps and easier stability - some people disable HT for the same.",Neutral
AMD,They don't pay attention,Negative
AMD,"No. Intel needs to do what Amd did and add seperate bLLC just for gaming. As long as latency is reduced to the L3 cache including bLLC, gaming performance will definitely increase as 285K and 9950x have similar performance in both gaming and productivity with small lead going to Amd. But x3d totally dominates ARL beacsue of the obvious huge cache difference and lower overall latency to their cache implementation.",Neutral
AMD,That is more so bc the 9950x3d isn’t really a 16 core cpu it is two 8 cores,Neutral
AMD,"There was one article a while back that reported something like ""intel goes back to integrated mc"", not understanding that panther lake is a fundamentally different product than the tile based desktop products.",Neutral
AMD,How do you think they're doing 2 compute tiles if the memory controller is on one?,Neutral
AMD,">I had heard this both ways and never really know who to trust until I see it.  I'm curious, which leakers have said otherwise?   You could still have mem latency improvements without the mem controller being on the compute tile too, so the situation there can improve either way.   >AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help  Yup, and ARL has higher mem latency than AMD even, so obviously there is room to improve even without having to move the mem controller back.",Neutral
AMD,"Intel will definitely decrease latency with Nova lake and as long as they add a bigger L3 cache variant, gaming performance will increase further to challenge AMD",Positive
AMD,"If you go straight to the source, the quote they used is directly on Intel's slides. Intel literally wrote that, regardless of who said it first.   [https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg](https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg)",Neutral
AMD,"I mean that it won't be priced like an HEDT part. It'll definitely perform like current generation lower end Threadrippers, but so will the 11950x.",Neutral
AMD,"NVL high end desktop skus are rumored to be on N2 as well. Intel has confirmed NVL desktop will be on TSMC, and Intel has also confirmed some NVL compute tiles will be external.   It's very possible Zen 6 DT is out before NVL-S, and it's very likely that NVL-S and Zen 6 launch within a few months of each other, if that even.   Also, if AMD wants to pay for TSMC N2 capacity, TSMC will build it out. Just a couple days ago we had a [report](https://www.digitimes.com/news/a20250724PD213/tsmc-2nm-capacity-2028-3nm.html) about TSMC being very aggressive with N2 scale out, and the reasoning here is two fold- first of all, 2026 is technically the 2nd year of N2 HVM, and also because a *lot* of major SOCs, other than just Apple and AMD, will be on N2- demand for N2 is high, and build out will be too.",Neutral
AMD,AMD is one of the leading N2 early adopters. Both TSMC and AMD announced this.,Positive
AMD,That didn't stop Intel with N3B,Neutral
AMD,The recent Warhammer space marine game is a game that benefited from disabling e cores. I am not sure the disparity between arrow and alder in that game tho.,Neutral
AMD,">Intel needs to do what Amd did and add seperate bLLC just for gaming.  Thats what I tried to say.  >My point is that the space taken up by the E-cores could instead be used for additional cache, not?  Or isn't that enough? AMD stacks the cache directly on top (9000 Series) or under (9000 series) the CPU cores, right?",Neutral
AMD,"Yes but IF Intel decides to do quad channel for AI nonsense at least for flagship model, Amd threadripper will be stomped over as I will definitely buy the Nova Lake part just for the price/performance compared to 9960x threadripper. The motherboard selection alone for Z890 is absolutely amazing and RDIMM prices aretwice as expensive for the same amount of memory. I expect with CAMM2 that max ram capaciity will go for 512GB and maybe 1TB for Nova Lake aswell. Even with 2 channel, cudimm will go above 10,000 MTS and maybe 12000 if we are lucky on CAMM2 modules by 2027 as CAMM2 definitely being used for DDR6   I do feel Intel has I/O advantage beyong gaming and that market is getting bigger than hardcore gamers all the time!! A decked out 256GB Arrow Lake z890 based computer is way cheaper than same memory sized threadripper 9960x.",Positive
AMD,"I don't recall Intel stating that NVL core chips would be at TSMC, only that some of the eventual package would be... which implies iGPU, to me, and that wouldn't be N2 (possibly N4, since Battlemage is N5, and N4 would be open and cheaper than N3 nodes).  Do you have a source, where they directly claim the NVL cores are coming from TSMC?",Neutral
AMD,"The game is a dumpster fire when it comes to CPU utilization, it hammers even top tier gaming chips like the 9800X3D. I'm not surprised if the E cores can bottleneck the performance even further, most likely some of the game's heaviest threads get (wrongly) assigned to the E cores.    The scheduling in W11 has been significantly improved over the last few years but it's not 100% perfect either. Legacy software in particular, which typically utilizes 1 core, often ends up running in an E core and as a result you can't take advantage of the higher ST performance a P core provides. In that case you need stuff like Process Lasso to fix the problem. It doesn't happen very often but when it does it's very annoying.",Negative
AMD,"Yup.   >. Then as you look forward, to our next-generation product for client after that, Nova Lake will actually have die both inside and outside for that process. So, you'll actually see compute tiles inside and outside.  Q4 2024 earnings call.",Neutral
AMD,"That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.  Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  That seems like something they couldn't have known in 2024.",Neutral
AMD,"18A is about as good clockspeed as N2, but density is still around N3 level. To make a new architecture, it makes sense for them to target N2 if AMD does the same which they have",Neutral
AMD,">That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.   20A, and only the lower end 6+8 dies are rumored to be fabbed on that node.   >Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Not competitive enough.   >Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  Low end client products like WLC, and even the lower end 4+8 NVL compute tile, is rumored to be on 18A still.",Neutral
AMD,"It doesn't make sense, for either of them, due to the cost of being an early adopter",Negative
AMD,Will it also introduce Lunar Lake successor?,Neutral
AMD,I wonder if this could become avaliable on LGA 1954 because that'd be a good alternative for budget builds potentially if the iGPU is potent enough.,Positive
AMD,I would love to see a Nova Lake-A (strix halo like) APU with an 8+16 tile + 20Xe3 cores which is equal to 2560 FP32 lanes or 40 AMD CU's with 16-32mb of memory side cache to handle igpu bandwidth demands,Positive
AMD,Is anyone left?,Neutral
AMD,"Wonder what node such a product, if it does exist, would use for the iGPU tile.   AFAIK, rumor is that the iGPU tile for the standard parts would be on 18A or an 18A variant. However, if this is a flagship part, I would assume they would be willing to pay the extra cost to use N2...   However that would also presumably involve porting the architecture over to another node, which I don't think Intel has the resources, or wants to go through the effort, to do so.   Also wonder if such a product will finally utilize the ADM cache that has been rumored since like, forever.   And this is a bit of a tangent, but I swear at this point in the leak cycle for a new Intel product, there's always usually a ton of different skus that are leaked that may or may not launch - right now we have NVL bLLC, NVL standard, and now this, and a bunch of them end up not launching (whether they were cut internally, or the leaks were just made up). So it would be pretty interesting to see if any of the more specialized rumored NVL skus end up actually launching.",Neutral
AMD,These designs are going to be the go-to for creator and mid-range gaming at some point in the future. Having it all integrated together should provide opportunities for optimization and form factors that are difficult with discrete cards,Positive
AMD,How long do I have to wait for something that is more powerful than an AMD 7840HS and still cheaper?,Neutral
AMD,Good to hear! I have a strix halo asus tablet the 390 one and its only got 32GB ram but its fast!  Would love to see an intel version…,Positive
AMD,"This is great move by Intel since they aren't making dGPU for laptop so they can get some marketshare by making strong Intel ecosystem. If Nova Lake AX released then people don't need to buy overpriced laptop with Nvidia dGPU anymore, not to mention with Nvidia shitty small vram.",Positive
AMD,"I'm still using Alder Lake, I suppose all these chips will be released in 2026 ?",Neutral
AMD,That's Panther Lake in a few months,Neutral
AMD,Lunar lake was always confirmed even by Intel as a 1 off design with soldered memory and many of its design choices.,Neutral
AMD,"No, the memory config wouldn't work.",Negative
AMD,"That isn't that different from the AI MAX 395+ already released, which has 16 Zen 5 cores and 40CUs and some MALL cache (I think 20MB) to help with bandwidth.  For this to be a meaningful upgrade, you'd want more than 20 Xe3 cores, which means you need more bandwidth, which means you need probably LPDDR6 memory to get it high enough as otherwise the cache demands would be prohibitive and LPDDR6 memory isn't coming to client until probably 2027.",Neutral
AMD,No not really.  It's pretty f'n bleak atm.,Negative
AMD,It's N2.  Sadly Intel Inside has almost entirely become TSMC Inside,Negative
AMD,"Nova Lake-AX will not be released, and DLLC will not be released, and nova Lake will not be released.",Negative
AMD,"Unless they can find some way to bring down the cost.....these aren't gonna get much traction, the same applies to Strix Point of course. While more efficient than your typical 2000$ laptop with a decent GPU.....the traditional laptop is still gonna outperform them based on pure specs.  You get more battery hour and less heat but people who use these kind of laptop are used to having it plugged 24/7. Feels more like experimental models but they have a long way to go before they see proper adoption. Particularly the cost.",Negative
AMD,"> mid-range gaming  Strix halo is only in $2000 systems. high end price, mid-range performance.  Good for stock margins i guess.",Positive
AMD,I'm sure that Xe2 in Lunar Lake is faster than the 7840HS iGPU on average if you run it with the max performance mode in the OEM software that comes with your laptop.,Positive
AMD,The 7850HS can't compete with the Xe2 igpu because it's bandwidth starved.   The 8mb of memory side cache insulating the 4mb of L2 from main memory allows the Xe2 Arc 140V igpu in LL clocked at 1950mhz to beat the RDNA 3.5 890m clocked at 2950mhz as it only has 4mb of L2 as a last level of cache before hitting memory.   TLDR: RDNA 3.5 is bandwidth starved on strix point due to lacking memory side cache,Neutral
AMD,PTL's not really a LNL successor.,Neutral
AMD,"Yeah, it was just to prove a point that ARM is overrated.",Negative
AMD,What about an 8 + 16 big LLC tile and 32Xe3 cores (4096 FP32 lanes or ~60 AMD CU's) + 64mb of memory side cache?   Or even better a mid range part with an 8+16 tile and 16-20Xe3 cores + 16-32mb of memory side cache,Neutral
AMD,Just read about the JEDEC spec for lpddr6 of 14400mt/s.  That is wild!,Neutral
AMD,"I hear you, questioning my decision to return under Pat’s hire-back spending spree. Dodged this one… but this is hitting differently.",Negative
AMD,Who said it's It's entirely TSMC ? they have been pretty open about majority of tiles Intel the CPU Tile will be shared with N2 ofc though,Neutral
AMD,"It should be Intel here, TSMC there.  Or a little bit of Intc, a little bit of TSMC.",Neutral
AMD,IDM 2.0: Where we proudly declare our fabs are world-class—while quietly handing the crown jewels to TSMC.,Positive
AMD,Source?,Neutral
AMD,"The BOM is lower, so the question is where the markup is coming from.",Neutral
AMD,Consoles have big APUs and they have better value than anything. I know they're subsidized however.,Positive
AMD,A mid range Nova Lake-A SKU with a 6+12 tile and 16/20Xe3 cores with 16-32mb of memory side cache would be sick,Neutral
AMD,Thats the problem with AMD sticking an additional 8 cores instead of infinity cache to fix the bandwidth bottleneck.,Negative
AMD,>still cheaper,Neutral
AMD,"It's the closest that we will get to a Lunar Lake successor   Really, panther lake is a combined successor to both Arrow Lake-H and Lunar Lake",Neutral
AMD,"Low volume and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference.  Combine that with not moving a lot of them compared to the rest of the lineup, and each one of those units has to make up more of the costs from things like bad dies or general spin up costs for a new chip.",Negative
AMD,It already has 32MB infinity cache.,Neutral
AMD,The 7840HS is cheaper because it is older.,Neutral
AMD,"It competes in the U/H lanes, so what is today ""ARL""-U and ARL-H. But yes, in practice it should bring *most* of the LNL goodness over, but PTL-U is still not a true successor in the original ~10W envelope LNL was designed for.",Neutral
AMD,"> and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference   It's no bigger than the equivalent dGPU, and you save on memory, package, and platform costs. Half the point of these things is to use the integrated cost advantage to better compete with Nvidia dGPUs.",Negative
AMD,The new Ryzen AI 250/260 with 780M is still cheaper because it doesn't have copilot+ .,Neutral
AMD,I will see the performance envelope of PTL U and decide should dump my LNL or not,Neutral
AMD,"The low volume part of that quote is important. If you're making heaps of them, each can take up a little bit of the startup costs for that design. If you only make a few, those costs get concentrated on what you do make.  Low volume also means a single defect is promotionally more of your possible units, and hits the margins harder. You do get more total defects in a larger run, but at sufficiently high quantities of those defects, you start making lower bins to recover some of the defects. If you hardly make the volume to cover one widespread SKU, you aren't likely to have enough to make a cheaper one with any degree of availability. At some point you hit a threshold where it's not worth selling what would be a lower bin because there's not enough of them.",Neutral
AMD,Because putting a 40 TOPs NPU for the Copilot+ certification is costly.,Negative
AMD,"It doesn't have a 40 TOPS NPU, it's the same 16TOPS one in the 8840HS it's literally just another rebrand. It's cheap because it's from 2023.",Neutral
AMD,Exactly. Hence your Xe2 lunar lake being faster is irrelevant to the question because it won't be cheaper regardless of age when they have a NPU that takes close to 1/3 of die space.,Negative
AMD,I think I didn't say anything that deviates from what you just said.,Neutral
AMD,"Maybe I misread or misunderstood, I thought you said the 260/250 had a 40TOPS NPU",Neutral
AMD,enjoy rhythm aware outgoing practice bike attempt library versed cake   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
AMD,Haha wow that is hilariously bad.   And here I thought ASUS newer bios update was bad due to some higher temps,Negative
AMD,"The new Gigabyte BIOS also has booting issues for me, and memory is even more unstable.  At least the UI is entirely in English now.",Negative
AMD,"who needs quality control, what can go wrong?",Negative
AMD,Gigabyte is trash,Negative
AMD,The Elon Musk method,Neutral
AMD,"> “Yeah just push it to production nothing’s gonna happen, no need to QA/QC this”  Seems to be the motto of a lot of the tech world",Neutral
AMD,"Yup indeed.   I would even go as far as one full month   Hence, why I learned to avoid installing new Windows Update automatically",Neutral
AMD,Intel really needs to be able to compete with X3D or they're going to continue getting dominated in the enthusiast consumer market. I like Intel CPUs and was happy with my 12600K for awhile but X3D finally swayed me to switch over.,Positive
AMD,"Intel has had plans for big ass L4 cache for almost a decade now, just that it never made it past the design board.  Supposed to be marketed as Adamantium. But it got ZBB’d every time I suppose due to cost.  For Intel to implement Adamantium, regular manufacturing yield has to be good enough I.e cost is low so they can splurge on L4.  Of course now they are forced to go this way irrespective of cost. I’d love 16p + L4 CPU.",Negative
AMD,"Honestly, good. I've been using AMD for a while now but we need healthy competition in the CPU space for gaming otherwise AMD will see a clear opportunity to bring prices up",Positive
AMD,"Something interesting is that the extra cache isn't rumored to be on a base tile (like it is with Zen 5X3D), but rather directly in the regular compute tile itself.   On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.   I think Intel atp desperately needs a X3D competitor. Their market share and especially revenue share in the desktop segment as a whole has been ""cratering"" (compared to how they are doing vs AMD in their other segments) for a while now...",Neutral
AMD,Hasn’t this been on their roadmap for a while now? I’m pretty sure they said 2027 is when they’ll have their version of x3D on the market,Neutral
AMD,"These core count increases could be a godsend at the low end and in the midrange. If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming, Intel will have a massive advantage on price.  That being said, if leaked Zen 6 clocks (albeit they’re from MLID, so should be taken with a grain of salt) are accurate, Nova Lake could lose to vanilla Zen 6 in gaming by a solid 5-10% anyway.",Positive
AMD,"Funny how non of this news posted on reddit hardware sub or even allowed to be posted. Guest what? R amdhardware will always be amdhardware! It's painfully obvious that unbearable toxic landfills sub is extremely biased to Amd. Meanwhile all Intel ""bad rumors"" got posted there freely which is really BS!  I still remember i got banned from that trash sub for saying ""People need to touch grass and stop pretending like AMD is still underdog because they aren't"" and the Amd mods sure really mad after seeing my comment got 100+ upvotes for saying the truth, but that doesn't matter anymore because i also ban those trash sub!",Negative
AMD,Intel should be ahead of the curve on things not looking to compete on previously created tech,Neutral
AMD,"Very fine-tuned ARL-S almost reach 9800X3D performance. Extra cache could help to close the gap   Given people are willing to overpay for price-inflated 9800X3D, I wonder if it could work given buyers need an entirely new platform. 9800X3D users are fine for a pretty long time like 5800X3D users did",Positive
AMD,"Lol, requires a new socket. Intel is such trash.",Negative
AMD,Intel will simply always be better than amd,Positive
AMD,"AMD gains tremendously from X3D/v$ because the L3 cache runs at core speeds and thus is fairly low latency, Intel hasn't seen such low latency L3 caches since skylake, which also has much smaller sizes, so the benefits of this could be much less than what AMD sees.   Only one way to find out, but I advise some heavy skepticism on the topic of ""30% more gaming perf from 'intel's v$'""",Positive
AMD,"Either more cache or resurrecting the HEDT X-series... Doesn't matter, as long as there is an affordable high-end product line.",Neutral
AMD,"The 12600k was a fine chip, but AMD had the ace up Its sleeve. I upgraded from a 12600k to a 7950x3d and it was one of the best PC upgrades I ever made.",Positive
AMD,I mean 9800x3D and 14900K offers basically the same performance in the enthusiast segment. Going forward though it would be nice to have more cache so normal users doesn't have to do any sort of memory overclocking just to match 9000x3D in gaming.,Neutral
AMD,4070 ti won’t cut it man - upgrade!,Negative
AMD,Broadwell could have been so interesting had it planned out.,Positive
AMD,"I want a 32 Core/64 Thread 3.40 GHz Core i9-like CPU. Not Xeon like with Quad-Channel and stuff, just 40 PCIe 5.0 lanes and 32 Power-Cores instead of little.big design. 😬",Neutral
AMD,">Otherwise AMD will see a clear opportunity to bring prices up  AMD already did, as you can see zen 5 x3d is overpriced as hell especially the 8 core CPU. Zen 5 is overpriced compared to zen 4 which is already more expensive than zen 3. Not to mention they did shady business like keep doing rebranding old chip as the new series to fools people into thinking it was new architecture when it wasn't and sell it with higher price compared to chip on the same architecture in old gen.  Intel surely needed to kick Amd ass because Amd keep milking people with the same 6 and 8 cores CPU over and over with price increases too! Not to mention radeon is the same by following nvidia greedy strategy.  Edit: Some mad Amd crowd going to my history just to downvote every of my comments because they are salty as hell, i won't be surprised if there are from trash sub r/hardware. But truth to be told, your downvote won't change anything!!",Negative
AMD,"Even though it's not stacked, I believe it's still going to fix the last level cache latency issue MTL and ARL have.   Ryzen CPUs have lower L3 latency than Intel because each CCX gets their own independent L3, unlike Intel's shared L3. Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3, so possibly giving the existing cores/tiles their own independent L3, improving latency and bandwidth over shared L3.  But one thing intrigues me. If this cache level has lower latency than shared L3, wouldn't this more properly be called L2.5 or something below L3 rather than last level cache? Will NVL even still have shared L3 like the previous Intel CPUs? I know the rumor that it will have shared L2 per two cores, but we know nothing of the L3 configuration.",Neutral
AMD,"> On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.  It is already a non-issue since AMD moved the 3D V-Cache to underneath the compute tile.",Neutral
AMD,"Adamantaium was on the interposer, did they change plans?",Neutral
AMD,"Don't remember them saying anything like that, but by around that time their 18A packaging is supposed to be ready for 3D stacking.",Neutral
AMD,"Nova lake= skip of it's just as good as zen, you would be looking at 2 gens after that and then swap from AM5 to intel.",Positive
AMD,"> If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming  Doubt that since it'll probably lack hyperthreading and the E-Cores are slower, even 6C12T CPUs are starting to hit their limits in games in the last few years, faster cores won't help if there's much less resources to go around, it kinda feels like intel went backwards when they removed hyperthreading without increasing the P-Core count.",Negative
AMD,Intel managed to run Sandy Bridge's ring bus clock speeds at core clocks which resulted in 30 cycles of L3 latency.   Haswell disaggreated core and ring clocks allowing for additional power savings.   Arrow Lake's L3 latency is 80 cycles with a ring speed of 3.8ghz,Neutral
AMD,"I'd like to see the HEDT X-series come back too, but Intel would have to come up with something that would be competitive in that area.  It's not hard to see why Intel dropped the series when you take a look at the Sapphire Rapids Xeon-W lineup they would have likely been based off of.  I think AMD would also do well to offer something that's a step above the Ryzen lineup, rather than a leap above it like the current Threadrippers.",Neutral
AMD,Well it was a downgrade on system snappiness as intel have way higher random reads than amd.,Negative
AMD,> I mean 9800x3D and 14900K offers basically the same performance  LMAO,Neutral
AMD,"Huh? 9800x3d is universally known to be like 20-25 percent faster, even in 1 percent lows.   https://www.techspot.com/review/2931-amd-ryzen-9800x3d-vs-intel-core-14900k/",Neutral
AMD,Maybe that is your experience.  Neverthelesss if you compare most gamers who switched to 9800x3D they report a significantly noticeable uplift in fps and 0.1 fps. Maybe a negligible few reported a decrease. And this has very likely nothing to do with the x3D CPU but other causes.,Neutral
AMD,"Ah you’re missing the final piece. As far as i’m aware this pretty much requires controlling the OS as well (or at least solid OS support). Consoles get their own custom operating system, Apple built a new version of MacOS for M chips. Intel and AMD though don’t control windows.",Neutral
AMD,UMA is such a hassle That's why I don't see it much except for calculation purposes (HPC/AI)...,Negative
AMD,"Application developers are supposed to try to avoid copies from GPU memory to CPU memory, instead letting it stay in the GPU memory as much as possible",Neutral
AMD,">so there is still the cost of useless copies between system RAM vs allocated GPU ram.    There is none, AMDGPU drivers have supported GTT memory since forever, so static allocation part is just to reduce burden for app developers but if you use GTT memory you can do zero-copy CPU+GPU hybrid processing.",Negative
AMD,"Intel needs something decent because AMD has taken a page out of intel (up to gen7) playbook, same cores no changes. Intel now provides more cores but it's the 100% core increase Vs AMD 50% and bLLC that should shake things up, hopefully they keep the temperature down as I don't want to have to replace case and get a 360mm rad just to not throttle, and not ever again do a 13th and 14th gen degradation show.   If all goes well going back to intel for a few years then AMD, brand loyalty is for suckers, buy what's best for performance and value. Hopefully intel i5 has 12P cores and i7 18-20P cores that would be nice to have",Neutral
AMD,"bLLC is just a big-ass L3$ and since Intel does equal L3 slices per coherent ring stop, it'll be 6\*12 or 12\*12 with each slice doubling or quadrupling. The rumor is 144MB so quadrupled per slice, probably 2x ways and 2x sets to keep L3 latency under control.",Neutral
AMD,"Intel and AMD have effectively the same client L3 strategy. It's only allocated local to one compute die. Intel just doesn't have any multi-compute die parts till NVL.   > Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3  8+16 is one tile, in regardless of how much cache they attach to it",Neutral
AMD,It is a massive issue for amd. You're voltage limited like crazy as electron migration kills the 3D cache really fucking fast. 1.3V is already dangerous voltage for the cache.,Negative
AMD,"I still think there's a slight impact (the 9800x3d only boosts up to 5.2GHz vs the 5.5GHz of the 9700x), but compared to Zen 4, the issue does seem to have been lessened, yes.   And even with Zen 4, the Fmax benefit from not using 3D V-cache using comparable skus was also only single digits anyways.",Neutral
AMD,"Adamantium was always rumored to be an additional L4 cache IIRC, and what Intel appears to be doing with NVL is just adding more L3 (even though ig Intel is calling their old L3 the new L4 cache? lol).   I don't think Intel can also build out Foveros-Direct at scale just yet, considering they are having problems launching it for just CLF too.",Neutral
AMD,Zen6 is the last AMD CPU using its socket anyway,Neutral
AMD,"I'm an e-core hater but arrow lake e-cores are really performant and make up for the loss of HT. arl/nvl 4+8 would wildly beat 6c12t adl/rpl.  HT was always a fallacy anyway. If you load up every thread, your best possible performance is ~60% of a core for a games main-thread.  I would much rather pin main-thread to best p-core in a dedicated fashion and let the other cores handle sub threads. Much better 1% lows if we optimize for arrow lake properly (still doesn't hold a candle to 9800X3D with HT disabled though).",Negative
AMD,"Yeah, I somewhat agree with this. I suppose it depends if Intel’s latency problem with their P+E core design is at all a fixable one - 4c/8t is still shockingly serviceable for gaming, but 4c/4t absolutely is not.",Neutral
AMD,It's the same ratio as 285K 8P+16E vs AMD 16P and we know that 285K is competitive despite no hyperthreading,Neutral
AMD,"Sooo a few months ago, I helped a buddy of mine troubleshoot a black screen issue on his newly built 9800X3D and RTX 5090 rig, a fairly common issue with Nvidia’s latest GPUs.  While working on his PC, I'd notice a series of odd and random hiccups. For example, double clicking a window to maximize it would cause micro freezes. His monitor runs at 240Hz, and the cursor moves very smoothly, but dragging a window around felt like it was refreshing at 60Hz. Launching League of Legends would take upwards of 10+ seconds, and loading the actual game would briefly drop his FPS to the low 20s before going back to normal. Waking the system from sleep had a noticeable 2-3 seconds delay before the (wired) keyboard would respond, which is strange, considering the keyboard input was what wake the system up in the first place.  Apparently, some of these things also happen to him on his old 5800X3D system, and he thoughts that these little quirks were normal.  I did my due diligence on his AMD setup: updated the BIOS and chipset drivers, enabled EXPO profile, made sure Game Bar was enabled, set the power mode to Balanced. Basically, all the little things you need to do to get the X3D chip to play nice and left.  But man... I do not want to ever be on an AMD system.",Neutral
AMD,did they measure responsiveness and timed the click to action? and was it significantly different? how much difference are we talking about?,Neutral
AMD,"can you explain exactly what you're talking about here? are you talking about a situation where the system needs to do random reads from an ssd? aka: boot time, initial game load time?",Neutral
AMD,"How was ""system snappiness"" measured?",Neutral
AMD,No.,Neutral
AMD,"Now both AMD and Intel chips are “disaggregated “ which means between cpu and other agents like memory controllers, pcie, and storage there is higher latency than the 12/13/14th gen parts. AMD has higher latency due to the larger distances involved on the package.  Also Intel is not really improving the CPU core much. There won’t be a compelling reason to upgrade from a 14700 until DDR6 comes out. At least not in desktop. Nova lake high cache parts will cost $600 or more so value/dollar will be low.",Negative
AMD,So? Major upgrade for everything else,Neutral
AMD,"I had an 12600 not k. I had the opposite experience, I upgraded to a 7800x3d and the snappiness was a night and day upgrade. I can recommend a x3d to anyone. Pair that cpu with Windows 11 IoT LTSC and you have a winner <3",Positive
AMD,"Meant to say ""Gaming Performance""  >Higher avg on X3D  >similar or same 1% lows on both platforms >Higher .1% lows on Intel.",Neutral
AMD,"any comment that starts with ""I mean..""  I never go any further, its like some weird reddit think where everyone with ignorant comments seems to start out with this,  at least often anyway.",Negative
AMD,"""Enthusiast Segment"" my good sir. All the benches you see are poorly configured or stock 14900K. With tuning it's a different story. Intel craptorlake scales with fast ram.",Negative
AMD,"As a long time AMD user I know that Intel needs to be tuned to perform best. So when you tune the 14900K or even 285K you get like 20% performance uplift vs stock. X3D just performs great out of the box because of the huge L3 Cache. At the very least if you do not like microstutters or frame drops and want consistent gaming performance Intel 14th gen is superior vs current AMD's offering. Anyone with a specific board like Apex, Lightning, Tachyon, or even Gigabyte Refresh boards + i7/i9 13-14th gen with decent memory controller can achieve similar gaming experience. I'm speaking from experience since I also have a fully tuned 9950x3D/5090 on my testbench. For productivity task Intel feels much better to use as well. I feel like Intel is just better optimized for Windows and Productivity too.",Positive
AMD,"Actually Intel thermal is already better than Amd ever since Arrow Lake and Lunar Lake released. Even Core Ultra 7 258V is arround 10c cooler than Amd Z2E and Strix Point on the same watt.   On MSI Claw 8 AI+, Lunar Lake temp at 20w is just arround 62c while the Amd version is arround 70c. I wouldn't have a doubt Nova Lake and Panther Lake will also have good thermal because it will have 18A node with BPD and RibbonFET GAA which is more advance than traditional silicon when it comes to power delivery and efficiency.",Positive
AMD,Ah so bLLC on both tiles is a possible configuration? Any chance Intel actually goes for this?,Neutral
AMD,You can very simply get 9800x3D to 5.4 with little effort,Neutral
AMD,"No, they will do zen7 too",Neutral
AMD,I haven't seen any of those issues on AMD where the underlying cause wouldn't also cause those issues on Intel.,Negative
AMD,"7800x3d here and never had these issues, came from intel",Neutral
AMD,Difference between 85MBps and 140MBps in q1t1 random reads and writes.,Neutral
AMD,Lets just ignore the whitepaper WD and Intel did about this.,Negative
AMD,So what configuration (tuning and ram settings) can a 14900k match a 9800x3d?,Neutral
AMD,That (Apple Silicon is good) and UMA are different stories I already know that Apple Silicon is good,Positive
AMD,On which benchmark(s) / metrics?,Neutral
AMD,"Haven't kept up with mobile since AMD 5000 and intel 10th gen, all I remember is intel needing XTU undervolting and then intel blocking XTU so using third party undervolt programs, AMD like I said 5000 never needed undervolting.   Desktop side, AMD 7000 is a hot mess, like seriously ridiculous. Seems that was sorted on 9000,    Let's wait and see on nova lake and zen6, like I said, brand loyalty is stupid, bought into AM5, so it's cheaper for me to go for zen6 with 244Mb of L3, but no am6-7 that will be intel.",Negative
AMD,"In theory, yes. For packaging reasons and market segmentation, probably not.",Neutral
AMD,"what's that on in terms of percentage, or seconds to person? was it noticeable?  i'm not techy enough, but is random reads and writes for clicking things and accessing data, and less so on copying and pasting a file?",Neutral
AMD,where is this whitepaper,Neutral
AMD,"You can send white papers all day but if most people buy these for gaming or productivity, AMD is winning in both categories.",Positive
AMD,Stock clocks 5.7/4.4/5.0 HT Off With DDR5 7600-8000 on Windows 10 22H2 or Windows 11 23H2 is enough to match 9800x3D at 5.4ghz with 6000 c28/6200 c28,Neutral
AMD,So MLID leaked in his most recent video about this (and he ranks it with a blue color code - 'very high confidence')  What do you think about this?,Neutral
AMD,[https://youtu.be/0dOjvdOOq04?t=283](https://youtu.be/0dOjvdOOq04?t=283) This explains it.  Gonna find the whitepapers link again.,Neutral
AMD,Based on what evidence?   I looked online for a few moments and found:  (1) Buildzoid from actually hardcore  overlocking did a 12 hour live stream where they couldn't even get a 8000 mhz overclock stable on the 14900k. No wonder why people haven't benched this lol  https://www.youtube.com/live/bCis9x_2IL0?si=ht3obVoBLcRFCyXI  (2) Plenty of benchmarks where an overclocked 9800x3d is about 10 percent faster than an overclocked 14900k with 7600 ddr5,Neutral
AMD,"Haven't you been listening? The conversation is strange (Confused) You first brought up the story of UMA, right?",Neutral
AMD,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Negative
AMD,"That channel has a lot of videos and even on this specific video it would help if you point the specific time you're referring to.  Now regarding AI, I assume you are talking about token generation speed and not prompt processing or training (for which Macs are lagging due to weak GPU compute).  I happen to have expertise in optimizing AI algorithms (see https://www.reddit.com/user/Karyo_Ten/comments/1jin6g5/memorybound_vs_computebound_deep_learning_llms/ )  The short answer is that consumers' PCs have been stuck with dual-channel RAM for very very long and with DDR5 the memory bandwidth is just 80GB/s to 100GB/s with overclocked memory.  Weakest M4 starts at 250GB/s or so and M4 Pro 400GB/s and M4 Max 540GB/s.  Slowest GPUs have 250GB/s, midrange about 800GB/s and 3090~4090 have 1000~1100GB/s with 5090 having 1800GB/s bandwidth. Laptop GPUs probably have 500~800GB/s bandwidth  LLMs token generation scales linearly with memory bandwidth, compute doesn't matter on any CPU/GPU from the past 5 years.  So by virtue of their fast memory Macs are easily 3x to 8x faster than PC on LLMs.  The rest is quite different though which is why what benchmark is important.",Neutral
AMD,"I'm highly suspicious, for packaging reasons if nothing else. I'm not going to call it impossible, but that would put enormous strain on the X dimension. Think about it, it's something on the order of 3-4x what the socket was originally for.  And obviously goes without saying, but MLID ""very high confidence"" doesn't mean shit.",Negative
AMD,"Based on my own testing , the games I play and benchmark results. Bullzoid is not relevant to this conversation. You can keep downvoting me for speaking the truth it's okay. I can't blame you because youtube and mainstream techspot are 99% misinformation. If you believe 9800x3D is 30% faster then go buy it, nobody is stopping you. I have investments with AMD and you're doing me a favor by supporting them.",Neutral
AMD,Just to be clear compute and hardware support for things like tensor cores have a massive impact. HBM is king but on older cards like the mi100 (released five years ago) can be out paced by a mid range card like the 4070.  All I wanted to convey is llm and token generation is a complex topic with limitations and struggles beyond memory bandwidth.,Neutral
AMD,"I haven't downvoted you at all, what are you even talking about.   You want people to believe in some conspiracy theory where any other information is a lie, and only you provide the ""real truth"".",Negative
AMD,"Besides, UMA wasn't first developed by Apple. Even if Intel introduces it, the software side or the software framework… Moreover, the OS side has to deal with it, so it is necessary to consider it a little. That's what you said earlier",Neutral
AMD,"I take no side there. I'm a dev, I want my code to be the fastest on all platforms  I have: - M4 Max so I can optimize on ARM and MacOS - Ryzen 9950X so I can optimize with AVX512 - in the process of buying an Intel 265K so I can tune multithreaded code to heterogeneous architecture.  The problem of Intel and AMD is segmentation between consumer and pro.  If Intel and AMD want to be competitive on AI they need 8-channel DDR5 (for 350~400GB/s), except that it's either professional realm (Threadripper's are 8-channel and EPYC's are 12-channel) with $800~1000 motherboards and $1500 CPUs and $1000 of RAM.  Or they make custom designsvwith soldered LPDDR5 like the current Ryzen AI Max 395, but it's still a paltry 256GB/s.  Now consumer _need_ fast memory. Those NPUs are worthless if the data doesn't get fetched fast enough. So I expect the next-gen CPUs (Zen 6 and Nova Lake) to be quad-channel by default (~200GB/s with DDR5) so they are at least in the same ballpark as M4 chip (but still 2x slower than M4 Pro and Max).  I also expect more soldered LPDDR5 builds in the coming year.",Neutral
AMD,"Have you not seen HardwareUnboxed 9070 XT Finewine video? Tell me why should I trust someone like that? I'm just sayin if you want real information test it yourself. I just don't trust product influencers like GamersNexus, HUB , Jayz2cents and other mainstream channels. Id rather buy the product and test it myself. Im not forcing you to believe what I said, its for people that actually knows what Raptorlake is capable off. If you want to see a properly tuned 14900k check out facegamefps on youtube. This is a very capable platform.",Neutral
