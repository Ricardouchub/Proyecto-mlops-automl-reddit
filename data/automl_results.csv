brand,text,sentiment
Intel,Why does this need an article? It's a tweet by an official account praising their own product.,Negative
Intel,"The B580 has 200W TDP, in a perfect world and TDP scales linearly, the B770 would be 50% faster, that would put it around the 5060Ti/9060XT.  If the price also scales linearly, that would be around 375€, seeing that the 9060XT is going for 350€ now, it's gonna be tough competition.",Neutral
Intel,Im really looking forward to panther lake X. 4-4-4 core configuration and Xe3 iGPU with sr-iov is perfect for running a Linux-Windows mixed vm environment without having to get a gaming laptop with a dedicated GPU for virtualisation.,Positive
Intel,I hope the Linux driver support and performance is good in these,Positive
Intel,"Intel ARC needs to maintain their momentum. They have an excellent pricing strategy and genuinely compelling features, it's time they released a card that competes in the midrange. And no, I don't count the A770. As a B580 owner, increased ARC adoption rates will be sure to benefit all cards in the range, so I really hope that intel is committed for the long-haul here. They are not in the position to be burning consumers anymore",Positive
Intel,"Releasing a GPU more than 1 year after the B580 came out seems weird to me. Unless this is a new architecture, or is using Intel's own process, and fabs.",Neutral
Intel,"4070 performance for $350-400, I'm calling it now.",Neutral
Intel,Hopefully they've seen Nvidia and AMD fuck things up by having two VRAM configurations and know not to do that.,Negative
Intel,"300W? Sounds like they're chasing the big boys. Hope the performance justifies the power draw, leaks can be misleading.",Neutral
Intel,"Hello Revolutionary_Pain56! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,They wouldn’t need a B770 or mystery GPU if they actually released more than just a B50 to the masses.,Neutral
Intel,"I don't know what the driver situation is like a year later, but B580 was anywhere between a 4060ti and a 3060 (or less if the driver really choked), so comparing B770 to a single Nvidia point of reference probably isn't the whole story.  Intel has been selling a big chip with a lot of hardware relative to what they charge, so when the drivers work Battlemage can punch way above its price class. I expect the same this time.",Neutral
Intel,"It's been deleted, so it might even be inaccurate.",Negative
Intel,Ad revenue.,Neutral
Intel,Trying to apply logic or rules to the internet is a waste of time.,Negative
Intel,"Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds. That however will mean a bigger die and viability might be questionable (considering they're already massive for the performance).",Negative
Intel,"Price doesn't scale linearly because die sizes make defects scale quadratically. so pricing is the same, 2 50mm\^2 dies are cheaper than 1 100mm\^2 die     However in GPUs there is a fixed cost for every GPU so there is a sweet spot",Neutral
Intel,"As always, TDP is a semi-arbitrary figure and has little to do with what the GPU requires.  Most GPU's of today have heavily inflated TDP's simply to try and juice benchmarks on review day as much as possible.",Neutral
Intel,"The BMG-G31 is supposed to have 32 Xe cores in 8 render slices on a 256-bit memory bus, compared to the 20 Xe cores and 5 render slices on a 192-bit memory bus for the BMG-G21. Unless Battlemage is seriously memory bandwidth-limited, it should be almost 50% more performant.  The only question is die size. If it's 50% larger than the 270 mm^2 BMG-G21, that would exceed 400 mm^2. The GB203 in the RTX 5080 is 378 mm^2 for context.",Neutral
Intel,"That ""perfect world"" of yours seems to violate basic physics though...",Positive
Intel,With tdp of 300 w it better be RTx 5070 or 9070 territory for much low price,Neutral
Intel,"You can choose between high performance and crashes (xe) or low performance and stable (i915), and with Intel firing linux devs left and right I wouldn't expect much improvement any time soon.",Negative
Intel,That would be an amazing value proposition.,Positive
Intel,Rtx 5070 16gb for 380$,Neutral
Intel,I’d be happy if they didn’t gate the Arc Pro B60 behind bad distributors.,Negative
Intel,"So banking on the hope, that *everyone* ***else*** *somehow falls behind by accident*, only for Intel to succeed?  If that's their business-plan (looking at their foundry-woes, it seems it is), that's an awfully idiotic business-model.  ---- Last thing I heard, was redditors moaning about en masse that monopolies are bad. *Which one is it?!*",Negative
Intel,"The B50 is not a gaming GPU and actually underperforms in gaming tasks compared the the B580. They need to have an actual range of cards, not just a budget option, and even more budget option, and a server/workstation GPU. The B770 is essential to compete in the midrange",Negative
Intel,"A year later the drivers are fantastic, seriously not even a single hiccup. Been playing Hogwarts legacy at 4k 60fps with Xess Quality upscaling, and no frame gen.",Positive
Intel,"> Scaling by TDP is not a good metric as they can pack more cores ect. and run them at lower, more efficient speeds.   The problem with this idea, is that this would cost them far more money, as you need more die space, which they already use relatively inefficiently compared to nVidia.  They can't really afford not to use every bit of die space they have for all that its worth.",Negative
Intel,"Battlemage doesn't have the ability to add more Xe cores per render slice, this is something Intel has changed for Xe3. The BMG-G31 will have 128 ROPs, the same as an RX 9070 XT, or more than an RTX 5080.",Neutral
Intel,"FWIW that is not how pricing structures work. It's a bit more complex than that.   E.g. defect rates scale with die size, that is true. But larger dies also have more budget for DFM structures, that can lead to fewer overall functional faults than smaller dies and more variability and binning opportunities. So although smaller dies tend to be on average cheaper, it is not necessarily that 2 dies half the size will be cheaper than the twice as large single die.",Neutral
Intel,I'm using an Arc A770 right now in Linux.  With i915 performance was unusably (for me) low.  With xe it's been fine.,Positive
Intel,"the driver is already open source right? i think it will get better over time on virtue of being open source, but relying on intel to fix it now probably isnt gonna pan out.",Neutral
Intel,"Well it kinda has to be, the 4070 came out nearly three years ago.",Neutral
Intel,5060 performance for twice the price isn't a good deal.,Negative
Intel,"I could see that. Nvidia really bailed out Intel by making the 5070 not much faster than the 4070 without using MFG to cheat lol   Edit: for all the Nvidiots downvoting, [the truth hurts](https://www.techspot.com/review/2960-nvidia-geforce-rtx-5070/#RT-1440p-png)",Neutral
Intel,This seems an absurd overreaction. All I'm saying is they don't do a 5060ti or 9060xt situation where there's a 8 gig model and a 16 gig model.,Negative
Intel,"It’s not but the B50 is the only Arc Pro that isn’t gated behind a bad vendor like Hydratech.    If they can’t properly launch the B60, why should I trust Intel or it’s partners with the B770 or some mystery GPU?",Negative
Intel,5060 is not nearly as performant as the 4070,Neutral
Intel,Spoiler. Yes is a new variant but it is call B70 arc pro series  Not the B770 we want,Neutral
Intel,"Performance wise, how what nvidia series card will it compare to?",Neutral
Intel,Celestial or maybe even Druid was supposed to be out in 2025 at one point.   https://www.techpowerup.com/292208/intel-targeting-2024-for-ultra-enthusiast-arc-celestial-gpus,Neutral
Intel,What niche is this going to fill? Battlemage was already kinda iffy performance wise when the B580 launched. It really had to compete with its increased VRAM and low price. Performance is going to be aged even more now. I imagine it’ll lose to a 5060ti or at best a 5070. Obviously price is king but I don’t see this generation being performance competitive.,Negative
Intel,"I'm sure B770 will launch as a side-effect if G31, but the real reason is for a low cost (relative) 32GB pro card to target AI. (Called B70?)  If Intel can offer 32GB VRAM on the pro card for $1000, it would be hugely popular within it's specific niche. And it's most important job is to get it into the hands of AI hobbyists to build out software support OneAPI in the open source AI community.  G31 isn't going to revolutionize the gaming market. It won't revolutionize Intel's finances. But it's goal is going to be high-VRAM at a low cost to try and build out alternatives to CUDA.   It's just a shame that it couldn't have come at a worse time from a cost-structure POV",Neutral
Intel,Curious to see if they can actually compete this time.,Neutral
Intel,"Oh, Intel, the *timing*...",Neutral
Intel,"As always too late - it's hard to compete when launching mid-late AMD/nvidia generation. Also curious how CPU driver head will reflect on much more powerful card, because it's still not quite resolved based of B580 tests.  Something like RX 9070 has very good pricing, especially in Europe, then RTX 5070 is very popular too. They would need to price it under 500€, likely closer to 450€ to have decent appeal, and I don't know if that's possible.",Negative
Intel,Wondering when we'd start hearing CES stuff.,Neutral
Intel,"On TSMC 5nm with gddr6 memory, theoretically this card could have launched in 2020-2021, not 2026.",Neutral
Intel,"Maxsun edition: 32gb memory, $1299 via Hydratech reseller.",Neutral
Intel,With how insane RAM shit has gone. These stock oils were bought long long ago. It won’t be in stock like… at all. Ever.,Negative
Intel,I thought they had given up on higher-end items. Will the die size be as large as the G10 (406 mm^2 )?,Neutral
Intel,"I might be interested in buying one of these, if it's priced right.",Neutral
Intel,I feel like this would be the perfect 1440p card. Hope that prices won't be inflated af.,Positive
Intel,"Still rocking my founders edition A770 16GB after buying it a month after launch, good to see the project itself wasn’t shelved. Hopefully the wait will be worth it :’)",Positive
Intel,For me the sweet spot would be 5070ti performance with 32gb vram,Positive
Intel,Id sell my 4070ti super and move to this if it releases,Neutral
Intel,"Watch it end up being AI-focused, without graphics output, and part of the Flex family.",Neutral
Intel,"1.6x the cores, but only 1.33x the memory bus with maybe 5% faster RAM is like 1.4x the bandwidth.   So I'd guess 1.5x the B580 at best.  Which puts it below an RTX 4070.",Neutral
Intel,"4070, 4070 Super at best (5070).",Positive
Intel,"b580 looks to use around 170W, this new card has a TDP of 300W so assuming the same efficiency and it uses the full 300W it'll be up to 1.76x the speed putting it between 4070 and 5070.",Neutral
Intel,This must be the “4080 performance” RedGamingTech has been mentioning since 2020 /s,Negative
Intel,"1.6 times the number of cores, 1.5 times the power draw compared to the B580. Let's say 1.4 to 1.6 times faster than the B580. That puts it at approximately the 5060ti to 4070 level.",Neutral
Intel,Nobody knows yet  I guess between 5060 ti and 5070,Neutral
Intel,vaguely 4070-5070 ish,Neutral
Intel,"we're not sure yet, its theorised to be around 5070-5070ti or 9070/xt but if intel does what they do with the b580 it'll completely destroy them on value, let's hope the ram issue doesn't kill what was shaping up to be the best gpu in recent time. the b580 is delightful.  intel 140t/v and b580 gained like a cumulative 20% perf boost this year just from drivers alone",Neutral
Intel,People are saying \~4070 at a lower cost. Intel GPUs are still space inefficient compared to Nvidia (so is AMD) so them dumping a lot of power into the chip is probably the best way for them to keep costs down.,Negative
Intel,"Intel has so far shown to match Nvidia regarding RT performance per dollar but not ""real"" performance per watt against AMD.  As their last flagship was kinda matched into AMD 5700XT and Nvidia RTX 2070, but like 5 years too late.",Neutral
Intel,5060 Ti 16gb to 4070 Super range.,Neutral
Intel,That roadmap was always laughable.,Neutral
Intel,"Realistically? None. This is a long-term play for Intel. They know the real money's in datacenters, but the ramp up to get there now is a decade+ with how complex modern GPUs are. They're just putting a few consumer products out there to establish the product line along the way.",Neutral
Intel,It'll be priced in the 5060ti range but offer better performance. AMD doesn't offer a product at that price.,Neutral
Intel,> If Intel can offer 32GB VRAM on the pro card for $1000  If you can even find the card.  It’s bad enough that I just went with an AMD R9700 instead of waiting for the B60 or its 48GB dual variant to show up.,Negative
Intel,"To be fair, if the Nvidia super launch is cancelled then this could maybe fill a place for the 5070 super",Neutral
Intel,"Where are likely a year and a half to the next Nvidia consumer cards releasing.  Assuming things like memory shortage goes away at that time, or else they might even Delay it even more.",Neutral
Intel,"> because it's still not quite resolved based of B580 tests.  And it probably never will be, since a large part of it is architectural.  AMD never solved the GCN single thread bottleneck in DX11 either.",Negative
Intel,"Not sure how it is mid-late this cycle, new Nvidia cards aren't expected till 2027, with AMD *maybe* releasing late 2026. We're really not even halfway this gen.",Negative
Intel,"Rumors were that it wasn't gonna come out. Something changed in the market to make Intel resume releasing G31.   And I'm convinced that's the fact that B70 could disrupt the market *(the market being the relatively small but growing home AI market - don't underestimate the impact this market can have on software stack support in professional environments)  As for the gaming side of things, I guess we'll find out if the driver overhead is a fixed amount or one that increases with the card's performance.",Neutral
Intel,"Maybe, if it comes out soon it will be a better value than both potentially.",Positive
Intel,Nvidia released RTX 30 Series in 2020 on a variant of Samsung's 10nm.,Neutral
Intel,All about price points.,Neutral
Intel,"Aside from the architecture not existing back then, the tech is a lot cheaper now than it was then. Not using bleeding edge hardware is a good thing for a budget card.",Positive
Intel,"It's Intel, price is the selling point, if they make them not attractive enough then it's DOA.",Neutral
Intel,"No way in hell it'll be anywhere near that with only 32 Xe cores, it's looking like 5060Ti at best, and only if they can get it to scale well",Negative
Intel,"You’ll end up with the same performance on the games Intel optimized the drivers for, worse performance on everything else, and no DLSS.",Negative
Intel,Why? You already have a superior GPU.,Neutral
Intel,The 4070Ti Super is literally a beast of a card. It's between the >RX 9070 and <5070Ti in rasterization and a cutdown of 4080/S,Positive
Intel,That'd be a weird thing to show off at CES.,Negative
Intel,Flex was killed.,Neutral
Intel,"That’s the relatively unobtainable Arc Pro B60 24gb and its unicorn tier 48gb dual model, save for the lack of Flex branding.",Neutral
Intel,It's not going to be linear with power. Core count and especially memory bandwidth are not scaling to that degree.,Neutral
Intel,"that would be an excellent sweet spot for sales. more than most anyone needs to play games, but not excessive.",Positive
Intel,Haven't seen them on my youtube feed in a long time.  I prefer to get all my fake rumors from MLID.,Negative
Intel,That was a rumored XE3 card. Not sure if it'll come.,Neutral
Intel,The rumor when B580 was coming out was that this should be a 4070 competitor. Don't tell people this will be anywhere close to a 5070ti lol; that'll undermine any chance of Intel having success with this card real quick.,Negative
Intel,\>5070ti or 9070/xt   Absolutely and utterly out of the question,Neutral
Intel,They can probably just make the money back for launching the larger die from cranking out 32GB professional cards.     Wouldn't be surprised if what they launch for consumers is a slightly cut down version.,Neutral
Intel,isnt that to prove one being reliable GPU maker to datacenter client; they have to let their product out in consumer market long enough to build that reputation?,Neutral
Intel,"Because ""that price"" is iGPU range.",Neutral
Intel,Intel will also end up being a year and a half out of date shipping another non-competitive entry level enterprise card.,Neutral
Intel,"""Assuming things like memory shortage goes away at that time""  Oh you sweet summer child. ALL the memory for 2026 has already been bought and paid for by AI data-centers. The 3 companies that make the chips will be operating at 100% of their output. The earliest the memory situation could start to get better is in 3-4 years or about the amount of time it takes to get a fab up and running assuming the companies are willing to pay the 20 billion dollars to pay for a fab.",Neutral
Intel,"And by the time B770 drops, gonna be April / May. It's gonna be mid-gen as fuck",Negative
Intel,"Fuck, it's already been half a decade.",Negative
Intel,And apple launched m1 mac on tsmc 5nm in 2020,Neutral
Intel,"My guess is that'll outperform 5060ti at higher resolutions, but trade blows and maybe even be slightly slower at lower resolution.",Neutral
Intel,If this can match the 4070ti super (>5070) in any gpu bottlenecked game that would be a huge success. Probably impossible given that this only has 32 Xe cores,Positive
Intel,Local llm focused COULD make sense. But yeah not ces likely,Neutral
Intel,Exactly why I think it's more likely than them releasing a B770.,Neutral
Intel,Been waiting for the B60 to be commonly available here and it’s more rare than the 5090 FE,Neutral
Intel,"Yeah, depending on the price it could be the perfect 1440p card.",Positive
Intel,"b770 has shown up in benchmarks with 16gb gddr6 and double the CUs so that's why i gave a range, it means it is going to fall between those probably with lower spec being realistic and higher optimistic.  itd possibly be better for the consumer if it was 5070 spec and 16gb since it would be a killer affordable 1440p card with potential to do entry 4k and the community needs value right now.  if it competes with high mid range it will probably cost a ton. if they can do this card under 500 its going to be insane value. my wife and i think it'll be 500-600(msrp) but of course scalpers will do their thing",Neutral
Intel,Really all they need to do is provide a better card than the 9060xt and the 5060ti. If it's $370 it will crush the competition.,Neutral
Intel,AMD's iGPUs on the high end were like 4060 equivalent last I remember hearing. But the price was also reflective of that.,Neutral
Intel,That's because only Apple  has early access to TSMC most advanced node.,Neutral
Intel,I’d bet that equivalent generation Quadros or Radeon Pro cards are far easier to find at any price much less near MSRP or even introductory MSRP.,Neutral
Intel,"I mean honestly a 5070 is fully playable even at 4k,",Positive
Intel,I keep wondering if Apple will get everything together to actually make a gaming alternative.  They are throwing enough die space at it seemingly.,Negative
Intel,"The total worldwide gaming market is $400 billion, about equal to Apple's yearly revenue, and only 4x Apple's yearly profit.  They will invest the minimum in the one gaming cash cow they have (iPhone) and ignore the rest, as usual.",Neutral
Intel,"Apple iPhones run games better than Android. You will need to manually tweak your Android device a lot to match performance. I doubt they can compete against laptop gaming, both in terms of price and performance (battery doesn't matter here), but they are better over Android in that space.",Positive
Intel,"They make excellent gaming devices, that's more on developers avoiding them for whatever reasons. The closed shop nature doesn't help either, they really should keep the closed stuff as a premium brand and release a new brand of more open arm hardware. They could probably make a killer handheld at a minimum.",Positive
Intel,Apple is a large player in the *total* gaming market through iOS,Neutral
Intel,I was thinking of proton on macOS that correctly uses metal.  Maybe that could happen?  Granted it adds x86 compatibility to the mix but it’s not like they did not already address that….,Neutral
Intel,"Interesting results. If this is representative for consumer laptops, Panther Lake is a much bigger upgrade than most here, including me, expected. But it almost seems too good to be true somehow.",Positive
Intel,is Geekbench a CPU or a GPU benchmark?,Neutral
Intel,"Hello LastChancellor! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,How does this compare to the Snapdragon X2 Elite?,Neutral
Intel,4 pcores  8ecores 4 lpcores..,Neutral
Intel,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",Negative
Intel,"Is Intel just ""squeezing the toothpaste"" again ? Even a low-frequency single-core 288V gets 2,700+ on Geekbench, while the 285H gets 2,600+ in single-core and 14,785 on multi-core. Therefore, TL;DR: I don't see Panther Lake being a huge improvement over the current Alder/Arrow Lake pairing. We will have to wait and see the power consumption, though.",Neutral
Intel,"I’m sorry, but that’s awful? Only 9% better single core when it has a better node and a newer architecture? Compared to what Apple and Qualcomm achieve every year, that’s pathetic",Negative
Intel,"Probably because GeekBench 6 only scales to a certain point, where more cores won’t help with improving performance compared to improving core IPC",Negative
Intel,"Fr, I really need to get a new light laptop (bc my old one's hinge is broken), but starting to feel  like I'd be better off waiting for Panther Lake than compromising with a bulky gaming laptop....",Negative
Intel,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",Negative
Intel,cpu,Neutral
Intel,"Probably one of the worst benchmarks out there for multicore tbh, I’d be more curious about the cb r24 scores",Negative
Intel,"Panther Lake doesn't bring any major changes to the cores. It's mainly about bringing node shrink, redesigned SoC, and new iGPU.",Neutral
Intel,"It’s obvious that this is the viewpoint of an outsider. Professionals would never look at it this way. Professionals first evaluate a processor based on its specifications, features, and process technology. Lunar Lake and Arrow Lake are *not* using some outdated process — they use TSMC’s then-most-advanced N3B node, Intel’s first time adopting it. Meanwhile, Panther Lake uses Intel’s own **18A** process.  Based on the current benchmark results, Intel’s 18A appears to outperform TSMC’s N3B by at least the same margin that **Intel 4** trailed behind N3B — which is an astonishing result.  Every day you hear people saying how much TSMC has advanced, how far ahead its processes are, how “outdated” Intel’s nodes are, how AMD’s processors using TSMC have excellent efficiency. These kinds of statements have been repeated endlessly over the past decade.  Yet today, Intel is using its newest process node to **clearly surpass** TSMC’s top process from just one year ago.",Neutral
Intel,"the real test will be how many watts the X9 388H needs to achieve its scores, because the 285HX needed like 90 watts to achieve its scores  so if the X9 could hit its scores while on its base TDP (65 watts) then thats a \~40% increase in efficiency, not bad",Neutral
Intel,"But it does that while clocked almost 6% lower, so the IPC gain is actually decent. Especially considering most people expected Panther Lake to be a side grade because of the small architectural changes on the cores.",Neutral
Intel,"For the use cases of PTL, Geekbench (which is mostly consumer focused) is a good indicator.  It doesn't assume that its workloads are perfectly parallel, it assumes some threads are used more heavily than others, so its value in nT is influenced by its 1T.    If someone is using this for rendering or other highly parallelizable workloads they might want to look into a subtest or into an alternative benchmark, but for typical consumers it seems like Geekbench is a good approximation of their experience.",Positive
Intel,"Geekbench’s scaling has always been problematic, and the differences between architectures are huge. The benchmark is very friendly to ARM and least favorable to AMD. Even a random Apple M5 chip can easily score close to 20,000.  Therefore, cross-architecture comparisons using Geekbench scores have little real meaning. The only valid reference is **same-generation, same-architecture comparisons**, such as between the 285H and 388H.  Based on the actual results, the improvements are about **9% in single-core** and **21% in multi-core**. Given that the 285H scores around **22,500** in Cinebench, I estimate that the 388H should be able to reach roughly **24,000**.  But the key point is **power efficiency at lower power limits**. For example, if the 285H needs around **80W** to reach 22,500, then the real question is:  * How much power does the 388H need to reach 22,500? * How many watts does it take to exceed 20,000?  This is what really matters. If the 388H can achieve 2**0,000 at 35W**, **22,500 at 60W**, and **24,000 at 80W**, then that would represent massive progress. It would also strongly indicate that Intel’s 18A node indeed offers significantly better energy efficiency than TSMC's N3B.",Negative
Intel,"Is macOS not an option? Because IMO, MacBook Air is *the* thin and light laptop to get, hands down.",Negative
Intel,"Couple of subtests leverage some new arm vector instructions and get huge scores, but those have limited influence to the overall score. Apple is better across the board, though the difference isn’t as big as the overall score suggests.   One difference is that since geekbench is distributed as binary it’s compiled more directly for apple architectures specifically while others use more generic targets. But that has very limited effect.",Neutral
Intel,But they have a gpu compute test too,Neutral
Intel,"Geekbench claims it's much more realistic than those multicore tests that scale nearly perfectly with tons of cores, and I think that's a fair take. It's not as if they didn't know how to create a benchmark that scales like other nT tests do, geekbench 5 nT does that.   I wouldn't call it worse, just different.",Negative
Intel,Geekbench runs common workloads as they are commonly implemented. It gives you a score on how well a multi core implementation of that workload would actually run in that CPU.   I think that is far more useful than some perfectly parallel workload measuring max power and core count.,Positive
Intel,"I have carefully compared the various models across Geekbench, PassMark, and the differences between Meteor Lake, Arrow Lake, and Lunar Lake. If my judgment is correct, the theoretical peak performance of the 484 in Cinebench R23 should reach around **24,500**; the 285H scores **22,500**. Compared with the 285H, it should be easier for the 484 to achieve high scores because its power requirements are significantly lower than the previous generation built on TSMC N3B.  Its peak performance will not be extremely strong because the frequency is not high. IPC is likely improved by around **10–11%**, but clock speeds drop by about **6%**. Overall, that means single-core performance should only rise by **4–5%**.  The improvement will be most noticeable in Geekbench. Since PassMark single-core also shows gains, the IPC uplift and resulting single-core increase should be quite certain. If Geekbench were the only source, it would still be questionable, but PassMark is more solid and has higher reference value.  Overall, in terms of peak performance, the uplift is average—around **10%**, close to that figure.  However, the real key is the **efficiency gains**. I believe they will be excellent. Compared with the 285H, which requires **65 W** to reach **20,000** points in Cinebench R23, I estimate that the **388H** may only need **40–45 W**.  I also estimate that the Cinebench R24 score should fall around **1300–1400**. Compared with Qualcomm’s X Elite 2 at **1950**, there is still a significant gap—but the two products differ drastically in scale.  Overall, Panther Lake’s greatest achievements lie in several aspects:  1. **Energy efficiency** — likely the best among all x86 products. 2. **Performance per mm²** — excellent. For example, the 484: if you look at its die shot, the total area of the CPU (including the CPU tile’s 4P and 4 LPE cores and all caches) is essentially equal to the die area of a traditional monolithic 8-core design. That means the 484 uses the same silicon resources as past 8-core chips, yet **no AMD mobile 8-core processor surpasses it**, either in raw performance or efficiency. 3. It also offers better performance-per-area than Qualcomm’s processors. The X Elite 2 has **18 cores**, including **12 “very large” cores**—similar in size to Intel P-cores—and **6 large cores**, each larger than Intel’s E-cores. The die area of this chip is **2.5× larger** than Panther Lake 484’s.",Neutral
Intel,"I mean, it does bring SOME changes to the cores, both are a next generation, it just isn't a more radical change like will be happening with NVL.  A mid single digit improvement is still pretty decent.",Positive
Intel,">The benchmark is very friendly to ARM and least favorable to AMD.   How so?   >The only valid reference is **same-generation, same-architecture comparisons**,  Geekbench is nice because it explicitly allows cross ISA comparisons. You don't have to take my word on it either, Intel and AMD themselves have used geekbench before to compare themselves to the ARM competition.   Same thing applies to spec and cinebench 2024.",Positive
Intel,My old 14900hx gets 35k multi core in cinebench r23,Neutral
Intel,"What's with the Cinebench fascination? At any rate. Geekbench 6 runs a raytracing test, and the 388H leak shows it at 29700 points compared to a 285H scoring 25300 points. That would place the Cinebench R23 scores at about 20% higher for the 388H  https://browser.geekbench.com/v6/cpu/15500755  https://browser.geekbench.com/v6/cpu/15474224  At any rate, the reason Geekbench doesn't scale perfectly with more threads is because a lot of workloads hit scaling limits due to Amdahl's Law, or memory bandwidth limitations. This applies to SPECint and SPECfp results for multiple threads as well.",Neutral
Intel,"lemme know once UTAU and Fighter Maker 2002 works on Arm macOS    (my point is that I work with a lot of old abandonware apps that barely even run on x86, so there's no chance in hell they gonna work on macOS)",Negative
Intel,"Thus useless to compare high CPU core counts.  If you actually need more than 8 cores you also have workloads that scale much better than Geekbench 6. It's especially dumb to claim this CPU is close to a 16 core, 32 thread zen 5 cpu based on Geekbench...",Neutral
Intel,It runs for far too short a time to reflect accurate multi-core performance.  People don't get a multitude of cores to run a task for a few seconds.  They do it for tasks that take minutes or hours to complete.  I'd argue it spends too little time on single-core tests as well.  I don't trust it to provide any useful information about anything other than transient performance.,Negative
Intel,I agree but the problem is it's being mindlessly used to compare MT scores as in this article.,Negative
Intel,Bro there's no need to spam this same comment like 4x in the same post's comment section T-T,Negative
Intel,"Why's the scaling ""problematic""? Its nT scaling is by design because GB6 is trying to replicate common consumer workloads which are rarely embarrassingly parallel. If you wanna see how well nT scaling for rendering is, there's cinebench for that.",Neutral
Intel,"was your 35,000 score achieved with power consumption above 100W? Can you try it now at 80W and see how many points are left? Also, limit it to 40W and check if it can reach 20,000 points. Because I estimate that the 388H has a chance to hit 20,000 points at 40W.",Neutral
Intel,"Ok, a simple no would have been fine.   Seems like those extremely old apps would run on any old POS x86 machine, if anything harder to run on modern hardware but hey what do I know. Best of luck.",Negative
Intel,"I mean, shortness is more of a problem if a device can cool itself properly or not rather than a problem of the CPU itself, unless said CPU in question is impossible to cool in that form factor",Neutral
Intel,"It provides useful information about the chip itself to real computer architecture enjoyers. Idk if gb6 changed it but geekbench has historically correlated with spec scores. Longer running programs like cinebench test the whole system including the thermal solution but geekbench gives a much better view into the pure performance of the cpu itself (and the associated memory system :/). Besides, you can always slap on a bigger cooler if thermals are that limiting.",Positive
Intel,Geekbench correlates with SPEC really well while taking a fraction of the time to run. Making it run for more minutes changes nothing,Negative
Intel,"It's being used for comparison because that's what we have. AFAIK, this is the *only* 388H benchmark we have",Negative
Intel,That looks like an AI post to me,Negative
Intel,It has been going hayway since SME just like GB5 had issues with AES Skewing results,Negative
Intel,Fair enough but I'd rather they kept something similar to GB5 multicore test in addition to their new 'more realistic' one.,Neutral
Intel,"Frankly, I wouldn't buy one for gaming, though I must admit Battlemage is pretty sweet for video editors thanks to 10-bit AV1 and 4:4:4 chroma on the HEVC side + you also get two codec engines (at least on the B580 with the same G21 core).  For perspective, you'll have to move up to Nvidia GB203 (RTX5070Ti), or better, to get your hands on two or more NVENC engines for the same 10-bit AV1 + 4:4:4 H.265.  If I was a serious video editor, this is *the* graphics card I would get.",Positive
Intel,The main bit that intrigues me about these ARC GPUs is their Linux gaming performance & how they compare to their windows performance.,Neutral
Intel,Not in the same system the 1080 ti was in.,Neutral
Intel,"One interesting data point is he's testing with 7500f. We have no comparison with contemporaries or higher end CPU to examine CPU bottleneck, but it's a realistic scenario and system for the card.  Interesting how that 1gig made all the difference in TLOU2",Positive
Intel,"idk why, intel gpu is so expensive in my country like bruh that gpu perform worse than cheaper nvidia/amd. those sucker trying to scam buyer just cause ""intel"" name in it.",Negative
Intel,Yeah intel's quick sync is very good at video editing and streaming as well. Even preferred over nvenc in streaming (no idea about video editing),Positive
Intel,"I would and I did (Intel B50 gpu).  So far, zero regrets and zero issues on linux.  Edit: Fedora for those that are curious.",Positive
Intel,Rumour has it Linus torvalds uses an Intel card because he wanted something on a budget that could drive dual 6k screens.,Neutral
Intel,"For desktop use Intel on Linux is great, but gaming performance and compatibility is horrifically bad.",Negative
Intel,"it's the retailers, they don't sell well and need higher margins",Neutral
Intel,Its not a rumor lol he did a video with Linus tech tips and specifically requested they put a b580 in the PC they built him  [link to video](https://youtu.be/mfv0V1SxbNA?si=jT_3dFy1H40vrVjk),Neutral
Intel,"Performance is a little worse than on windows, but compatibility is not horrifically bad. It's pretty much the same as on windows.",Negative
Intel,"I believe Linus Torvalds wanted an ARC Pro B50, but settled for the B580 because that's what LMG could get their hands on",Neutral
Intel,can you imagine a bunch of nerds whispering about which graphics card an old man uses?,Negative
Intel,"I'm curious as to how much worse. I've considered an upgrade to a B570 due to them being seen for £150 new, putting it into used RX 6600/6600 XT territory, but if the Linux performance of say a B580 on Linux falls closer to either or, then it's probably not a worthwhile choice over the used AMD options for me.",Negative
Intel,"TLDW:    GPU Models Tested: MSI Shadow 2X RTX 5050, Intel Arc B580 FE      16 games average:    1080P, High-Ultra Settings:     Native TAA: Arc B580 is 14% faster, 23% faster at 1% lows due to higher VRAM        DLSS 4 Quality vs XeSS Ultra Quality: Arc B580 is ~11% faster     DLSS 4 Quality XeSS Quality: Arc B580 is ~20% faster     DLSS 4 Balanced XeSS Balanced: Arc B580 is ~15% faster     DLSS 4 Performance vs XeSS Performance: Arc B580 is ~14% faster",Neutral
Intel,"""There was a time, about a decade ago when the $250 price tag offered solid products, but the world has changed""  Yep, inflation. $250 in 2015 money is $342 in todays money. And you can get a very solid product at that price tier, the RX 9060 XT is $369 on Newegg.  GPU prices haven't gone up, you money is just worth way less.",Neutral
Intel,"5050 really has no right to exist at the price it does. B580 is obviously being sold at near cost or even a loss however, it's not exactly a fair comparison but that doesn't matter to consumers.  If you just want to game then I can't see any reason to consider anything else at this price point.",Negative
Intel,I'd still probably go nvidia here as I don't trust intel's compatibility with older titles and the like.   Still it would probably be better to spend $20 more on a 9060 xt 8 gb or $50 more on a 5060 than either of these.,Neutral
Intel,"If UE5 games generally run this poor on Intel GPUs, there might be trouble ahead as there are lots of those games in the pipeline.  You still couldn't get ~~more~~ me to buy an Intel GPU, even if I was desperate for a cheap GPU right now. I'd just adjust my settings.",Negative
Intel,"The B580 is decent enough, but it might be better to just save a bit more and get a 16GB 9060 XT for $350 or something. That card is likely to last 10 years flat at this point, and it will definitely last at least 5.  And yes the 5050 is not good. Getting something with a half-decent iGPU would be a better use of your money at that point.",Negative
Intel,The biggest issue is that he did not test PCIE 3.0 vs 4.0 vs 5.0. Those GPUs are very likely to go into budget builds or as upgrades to older motherboards like the B450.,Negative
Intel,Imagine spending $250 on a GPU when you could literally just save $100 more for like a 100% percent more performance.,Neutral
Intel,Wait isn't xess a lower resolution per quality setting?,Neutral
Intel,"yeah there is a reason why there are 5050s for 210  the thing is a sub 200 dollar GPU, which matches it capability and vram well, its more or less the I want to step up from igpu deal",Neutral
Intel,Maybe for the low end but high end I can’t even buy a card at msrp outside of America.,Negative
Intel,"*ignores that this is a 50 tier product and should be compared with the 950 and 1050*  This kinda of ""but but inflation"" virtue signalling I'd very unhelpful to these kinds of discussions. It's as if you're saying people should stop complaining gpus are several times more expensive than they used to be with the actual low end market completely destroyed.",Negative
Intel,Tech is supposed to beat inflation. Look at monitors or TVs or SSDs (before now) or CPUs or ....,Neutral
Intel,All the tech tubers are just turning into old men shouting at clouds. They will probably all be replaced by younger people living in the now soon enough.,Negative
Intel,"First of all 250 euros bought way more gpu in 2015 than 360 does today. And the lower end and midrange gpus were much less cut down vs the high end chipa today.  A 5050 sits where the 750ti did when the 980ti was out. Now you get entry level performance for mid end prices  Have wages actually increased that much? Because that is the only useful measure of ""inflation"". Everything else is just corporate profits   If prices for everything go up but wages don't then that leaves less money for frivolous shit like ram and storage and laptops and consoles, not more.  Even in my country where our wages are automatically indexed to match inflation, our purchasing power has dropped because the actual cost of living isn't properly represented in whichever calculation is used for the inflation number.  Houses have gone up by 100+ percent since 2015, rents have gone up by over 60 percent, grocery prices have more than doubled, utility prices have risen sharply, public transport has more than tripled in cost.  Minor expenses like clothing or a tv you buy every ten years have stayed flat, but that isnt what people are spending 80 percent of their income on.",Neutral
Intel,"Shh, everyone knows that prices only go up on luxury goods due to evil corporations, after all how will people live without their computer not being 800% faster than last year?",Negative
Intel,"> Yep, inflation. $250 in 2015 money is $342 in todays money.   People really need to stop using CPI. I can bet you that GPUs don't make it to the market basket. Yes, your money's value has fallen but not by that much.",Negative
Intel,You're getting downvoted for speaking the truth.  The RTX 5050 should be a $150-180 GPU for the price and value it offers but unfortunately people are gonna defend the price tag that the card was set for by Nvidia,Negative
Intel,"For a while you could get them for $229, which would be more acceptable vs a 5060 for $299, making it the same FPS/$. But the 5060 is actually the one on sale right now for only $30-$35 more. 30% faster for like 12% more money.",Neutral
Intel,"I'd say that the cheapest new GPU that I'd blanket recommend with no ifs, buts and caveats is the 9060XT 16GB, everything below that either struggles with outright performance, VRAM or software issues like Arc.",Neutral
Intel,"Intel checks all the right boxes on paper (generous VRAM, decent pricing compared to competitors, an alternative to the duopoly) but the recent CPU overhead stuff coupled with the crapshoot that is trying to play older games and it just isn't worth it",Negative
Intel,"Yeah they cover this at the start of the video https://youtu.be/lLe5AP6igjw?t=229   XeSS 1.3 shifts everything down a tier, so their quality scaling ratio is everyone elses balanced ratio.  Older versions of XeSS match DLSS/FSR scaling ratios.",Neutral
Intel,"I know quality is, not 100% sure about others. dlss quality preset uses higher resolution than XeSS and FSR quality presets",Neutral
Intel,Its fine for people who need a dGPU but not a beast for work. think stuff like CAD or Photoshop. It will also be fine for people who only play competitive multiplayer games.,Neutral
Intel,Nvidia cards are bellow MSRP here in eastern europe. AMD cards slightly above MSRP.,Neutral
Intel,"They are all selling below MSRP in the UK. £979 is MSRP for a 5080 and I can buy 3 in stock models for less than that price without much searching, at scan.co.uk.  If you are in South America its probably your countries insane import taxes, protecting their home grown GPU market lol.  29 upvotes from children who have not bothered to check or do any kind of reasoning.",Neutral
Intel,"> Tech is supposed to beat inflation.  And it does, wtf are you trying to claim?  $100 CPUs these days run circles around 6700K which was the flagship in 2015. A B580 is faster than a GTX 980 Ti, which was the flagship card of 2015.",Neutral
Intel,"It does. For the price of a 1993 CRT TV, you can get a flat-screen LED thrice the size and with 10 times the resolution.  SSDs? A 2 TB nvme is a fraction today than a 128 GB Sata one was a little over a decade ago.   What actually changed is inflation, and that the buying power of today's middle class person decreased significantly relative even to the 2000s.",Neutral
Intel,Gpus are way more expensive to produce,Negative
Intel,"> Tech is supposed to beat inflation.   It does, despite wafer prices increasing the last 10 years.",Neutral
Intel,Wrote a fucking who? The fact that TVs are cheaper in nominal terms than they were 15 years ago does not mean it has to be the same thing with every other tech product. TVs are not products manufactured necessarily on cutting-edge expensive nodes.,Negative
Intel,"It does. For the same amount of money, you get way better GPU(unless your braindead thinks the gtx970 has same performance of 9060xt)",Positive
Intel,"It does, but also, inflation has been extremely bad for 5 years.",Negative
Intel,"You are just making shit up at this point. NVIDIA GeForce GTX 760 (2013) release price: $250.   That was a shit card, arguably a worse product than the 9060 XT is today, when you compare it to contemporary rivals. How do i know it was shit? I had it.",Negative
Intel,TSMC inflation is FAR higher than CPI. You are half right,Neutral
Intel,"according to US bureau of labour staticstics that measures the CPI it includes  all personal computers (desktops, laptops, tablets) and related equipment (printers, monitors, smartwatches, smartphones). It does not look at GPUs specifically, but the effect of that will be visible.",Neutral
Intel,I doubt ppl are gonna defend the 5050 considering a 5060 or an 8GB 9060XT is not much more and a fair bit faster.,Negative
Intel,"i mean, the 5050 off of amazon rn is 210, so it is getting there as a sub 200 dollar GPU for improving over iGPU right",Neutral
Intel,I pretty much but there's nothing else people *trust* in the category because apparently Arc cards are for professional nerds or something whereas I haven't had a real bad driver issue in over 2 years with my A770  People are also conditioned to fear older gen GPUs so 6xxx and 7xxx parts are sitting on shelves waiting for blowout discounts. People would still rather spend more on a basic nvidia from the 5000 series.,Negative
Intel,B580 user here. I coupled it with a R5 5500 and as of the recent updates the card just seemed to run much better vs when I got it last July.  There was a video before which also revealed that the CPU overhead is now being addressed in subsequent updates.  https://youtu.be/gfqGqj2bFj8?si=PyAfB2NhqZKWWVXY  I’d say it’s getting better and that I’d recommend it over a 5050 since the overhead is now fixed/negligible.,Positive
Intel,Intel is in their second GPU generation. Its going to take a lot longer to catch up with the institutional knowledge and practical application in videogames that the others were developing for over 20 years. The CPU overheard was not an issue in Intel iGPUs and Alchemist because GPUs never got fast enough to matter. It is only now that they noticed that issue since the GPU is far enough to create it.,Negative
Intel,And that only in terms of native resolution and does not mean equal final image quality.,Neutral
Intel,\>dlss quality preset uses higher resolution than XeSS and FSR quality presets  Not exactly. FSR and DLSS are evenly matched in internals at all quality presets.,Neutral
Intel,I’m in Australia lol. The msrp for the 5090 is 1999 USD which translates to 3011 AUD The cheapest 5090 is 4800 AUD. That’s not even close at all to the msrp…,Neutral
Intel,"It sure is funny every time all those 6700k/8700k era CPU's pop up on used parts sites or FB Marketplace and still expecting close to initial prices.  Who even buys them anymore? At least a Q6600 has retro value ,but those are just obsolete.",Negative
Intel,"Not if you consider how the workloads being run on them have also changed. A GTX 970 ($450 inflation adjusted) would have run 2015 games better than how a RTX 5060 Ti ($429) or 5070 ($550) run 2025 games.  In other words, demand for performance has outstripped performance improvements, and those improvements are not felt as much.",Neutral
Intel,"A 100$ CPU in 2015 would easily run 2015 made software. A 100$ CPU in 2025 would barely run the electron JS slop. This includes Windows.  In 2015 my run-of-the-mill laptop (cost 300-400 bucks at that time USD equivalent) instantly opened the control panel when I clicked or file explorer even with a old HDD. Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  Yes yes yes it is very good on benchmarks but I don't stare at benchmarks all day. I use my computer for things you do at computer. Don't force my CPU to crunch how much digits of Pi it can compute.  A 980 Ti can easily run top 2015 games. Now? My laptop barely runs modern AAA games without looking a blurry mess. I simply can't fucking understand how you people look at the glorified motion blur and call ""yup it is the pinnacle of computer graphics"". How the fuck majority of modern AAA games look any better than RDR2 can anyone fucking tell me?",Neutral
Intel,"Why aren't you comparing relative buying power of 2014/2015 vs now then?  $650 got you what in 2014, a GTX 980TI?  $330 got you what in 2014? How close to the top end are both these things?    That's $900/$455 today, thereabouts.  What does $900 get you today?  Does that buy you anywhere near the top end?  And how does that product compare in relation to others above and below it?  Because the $330 product in question ($455 today) got you about ~75% to top end performance for ~half~ MSRP of the 980TI.  How does a $455 product of today square up relative to the top end?   Why don't we throw in a GTX 980TI vs a GTX 280 comparison while we're at it.  Make things really interesting.  I'll let you fill in those blanks (along with the $330 card in question) hoping you actually learn something in the process here.  The bar is very low, try not to trip.    The underlying point that user was making was pretty obvious if you read the comment they responded to.",Neutral
Intel,"cmon man, give him some slack, he just made shit up cause it's convenience for his argument.",Negative
Intel,How is it comparable? The 9060 XT is a very good card for 250. Ideally it'd be around 200 or below but for 250 you get a card that's a bit overkill for even 1080 P gaming.,Positive
Intel,In quite a few countries the 9060 XT is at or below the RTX 5050s MSRP.,Neutral
Intel,Then why did the comment above mine get multiple downvotes? It's Reddit and that's how it goes unfortunately,Negative
Intel,6x and 7x are priced far too high for old stock and are poor value compared to nvidias 50 series. They really haven't had a good price/performance low-mid end card since the 6700XT which are extinct at retail.,Negative
Intel,"I think they must have to go into every game, and adjust that t fix it, because it's not a universal fix it seems. Maybe per-game optimizations .",Negative
Intel,yep. XeSS 1.3 is closer to DLSS 3 rather than DLSS 4 in terms of image quality. Its good enough to game on in my opinion.,Positive
Intel,"> Who even buys them anymore?  The best SKUs on sockets have always demanded a premium in the used market. Since that's where people upgrading old machines will go.  And many machines from OEMs are not readily upgradable with just new boards/CPUs combos. Since they use custom form factors etc. So it's either a in socket upgrade or replace the whole machine. The socket 6700k is on is also especially affected by the ""premium"" factor. Since there's no lower end SKU with 4C/8T. You either get the 6700/7700 variants or are stuck with lower thread count.",Negative
Intel,"> would have run 2015 games better than how a RTX 5060 Ti ($429) or 5070 ($550) run 2025 games.  I think your memory is impacted by the expectations at the time. And the problem of reviews often using older titles inflating numbers, ffs some are still benching with GTA V to this day.   The [970](https://tpucdn.com/review/nvidia-geforce-gtx-1060/images/witcher3_1920_1080.png) couldn't even get 60 fps in witcher 3. Which was released in 2015.  And the performance it got in Witcher 3. Was not much better than what the 5060 Ti got in [Black Myth Wukon](https://tpucdn.com/review/msi-geforce-rtx-5060-ti-gaming-16-gb/images/black-myth-wukong-1920-1080.png)  Which even including 2025 titles. Is one of the hardest/heaviest titles with the worst performance. You can expect much better performance in almost every title. Just like the 970 was doing better than it did in Witchers 3.   But to argue that we got a lot better performance back then in the games releasing at the time, that is just false.",Negative
Intel,Yes 3.5GB of memory in 2015 was soooo much better than 12GB today /s,Positive
Intel,"All the GPU makers are betting on you using DLSS/FSR/XeSS as part of your usage to play games. Maybe even frame generation along with Relex, and all the other tech they ship GPUs with. They used to only rely on you using regular AA techniques.   If you ignore all those options you have today, and pay like it's 2015, it might be worse a lot of the time. If you use those options, you're generally way ahead of where a GTX 970 would fall. So it depends if you're willing to adopt new rendering tech, or rejecting it.",Neutral
Intel,"No, not really, in 2015, you happily accepted 45 FPS on not the highest settings at 1080p",Neutral
Intel,"This don't sounds like CPU problems at all, more like either Win11 is a vibe coded pile of bugs, or ACPI problems.",Negative
Intel,"> Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  Something is wrong. I keep reading people's experiences of stuff like this and I haven't experienced it, I'm not doubting it but I'm so curious as to what is wrong.  In particular I read a lot of people saying Windows Explorer takes forever to open etc",Negative
Intel,"> In 2015 my run-of-the-mill laptop (cost 300-400 bucks at that time USD equivalent) instantly opened the control panel when I clicked or file explorer even with a old HDD. Now? It needs few second to run settings, it lags when opening notepad, it is disgustingly slow while navigating file explorer on my 1000 dollar ""gaming laptop"" with NVMe SSD.  That's Windows for you.",Negative
Intel,it does not matter how close to the top GPU is. its a completely useless comparison.,Negative
Intel,I'm not sure what you're saying. The 9060xt is $250 only in 2013 money. They are arguing it's better value than a GTX 760.,Neutral
Intel,"Ppl up/downvote kinda randomly, doesn't really mean much post can go from +/-20 to the opposite real quick sometimes.  Anyways It's at +8 currently was at +something(2 maybe?) when i commented so who cares.",Neutral
Intel,"Nvidia and AMD do a lot of per-game optimization in the driver as well. In some cases very brutally, for example Nvidia is known for grabbing all games DX12 drawcalls and rearranging them in driver because the way game handles it is inefficient.",Negative
Intel,I remember upgrading to a 970 in 2016 and still being unable to max Witcher 3 at 1080p60 but got close enough,Neutral
Intel,"funnily enough, GTA 5 Enhanced Edition can be quite a benchmark for ray tracing nowadays. But it took to this year for it to be released. I think we can consider it a testbed for whats going to be implemented in GTA 6.",Neutral
Intel,"Bringing up 1080p no RT Wukong benchmarks sort of makes the point for me: the only way these cards look comparable is if we pretend features and standards are the exact same they were a decade ago.  High res and high refresh monitors were a luxury in 2015, now they are a dime a dozen. RT was not a thing in 2015, now it is and Nvidia marketing really wants you to use it. It's like you're comparing Witcher 3 on Ultra settings to Wukong on Medium or High settings, and acting like it's apples to apples.  The moment you take modern displays and features (including DLSS to be fair) into account, it paints a picture where technology has moved on, developers and players would love to move on, and GPUs are struggling to make that jump.",Neutral
Intel,"if you run out of memory today the game swaps textures and continues running, it just looks uglier.   If you run out of memory in 2015 it starts using the superslow 0.5 GB and everything breaks.",Neutral
Intel,Nvidia is certainly expecting DLSS+FG to be the typical use case. The vast majority of their benchmark and marketing material is with those two.,Neutral
Intel,"I've got nothing against DLSS, I use it whenever I can, but sometimes it's just not enough to bridge that gap.  Another user brought up Witcher 3 and Wukong as an example of a graphically advanced 2015 game vs a graphically advanced 2025 game. The 970 would get 50+ fps on Ultra settings Witcher 3. Max out Wukong on a 5060 Ti and no amount of DLSS will make that card stop crying and screaming.",Neutral
Intel,"Like, Debian has no issues running on a n150 with multiple docker containers without instantly spiking the cpu to 100%.",Neutral
Intel,"I found a way to sort of kinda make file explorer slow. But its really a perfect storm thing. Have multiple screens, one of which is running in HDR and another in SDR. Have the file explorer tree open. Have a HDD, slower the better.  When you browse folders it refreshes the tree. When it refreshes the tree it asks connected devices if they are online, including the HDDs. Now move the window back and forth between your screens. When the explorer moves into HDR screen, it gets redrawn. Same when it moves to SDR screen. I suspect but cannot confirm there is a bug where the old instance is not cleaned correctly. So now when you browse it asks all devices if they are online 10 times. 100 times. At some point youll start noticing actual delays in opening folders.  Works even better if you havent restarted for a month.",Neutral
Intel,"the opening notepad thing, if you use taskbar it has a bad habit of not actually opening notepad until it finishes the online search for apps called notepad or whatever you typed. Disabling online search in start makes it fly really fast.",Negative
Intel,The 9060 XT 8GB is currently retailing for 250$ in many areas. I'm saying that the 760 isn't an ARGUABLY worse product. It is a worse product for it's time straight up.,Negative
Intel,"Fair point, but isn't lower and competitive prices good for us?",Neutral
Intel,">  and acting like it's apples to apples.  Apples to apples would be comparing W3 performance for both cards.  Wukong even without RT is a CONSIDERABLY more advanced game graphically than original W3.   >High res and high refresh monitors were a luxury in 2015, now they are a dime a dozen.   And? Better monitors showing up doesn't change the laws of physics and basic economics. It doesn't make scaling with die shrinks suddenly increase. With your logic the 970 was a terrible deal vs some early/mid 2000s GPUs that delivered better FSP at 1024x800 in games released at the time.   And before you start harping on about die sizes. The die in the 5060 Ti is actually more expensive than the die used on the 970. Wafer price increases more than compensates for the size difference.",Neutral
Intel,"Looked it up. Wither 3 got 52 FPS at Ultra settings, no Nvidia Hairworks turned on, for a GTX 970. Wukong gets 42 FPS at the cinematic preset native resolution, which is actually intended for cinematics, but developers allow people to enable anyways. As Digital Foundry has said, they maybe shouldn't.  Gets over 70 FPS if you turn the preset down 1 notch to high. No upscaling, or frame generation, or hardware RT, which is like what Nvidia Hairworks was for Witcher 3. It's really not hard to get Wukong to run at 90 FPS on a 5060ti with some minor tweaks.",Neutral
Intel,According to TPU review maxed out Wukong with DLSS got 42.3 fps. Not exactly the 50 fps you remmeberr for witcher but close. Heres a link to the review: https://www.techpowerup.com/review/black-myth-wukong-fps-performance-benchmark/5.html,Neutral
Intel,"That's very interesting thank you, I can definitely see why I haven't experienced it.  You genuinely wonder how Microsoft are testing these days.",Positive
Intel,"> With your logic the 970 was a terrible deal vs some early/mid 2000s GPUs that delivered better FSP at 1024x800 in games released at the time.  Except in practice from 2005 to 2015 you got considerably more advanced graphics *and* higher resolutions *and* generally higher framerates too. Now either you pay up or you gotta pick one.  As for the rest of your post, it's more of a digression. All I said is GPUs are failing to keep up with the rest of hardware and software, not that there are no valid reasons for it.",Negative
Intel,"> As Digital Foundry has said, they maybe shouldn't.  hard disagree. As someone who does not have a lot of time for videogames and often end up playing older games with newer cards, those beyond high settings are great as it allows me to make use of my newer card and make the old game look better.",Neutral
Intel,"I disagree with comparing RT to Hairworks, when the visual impact as well as the emphasis put on it by Nvidia is so much bigger. I also disagree with using 1080p as a reference for Wukong, when high res and high refresh rate monitors are as cheap and plentiful as 1080p was back then.  Imagine you went back to 2015 and told the GTX 970 guy he's supposed to play his games at 2005 resolution and turn off antialiasing, how do you think he'd react?",Negative
Intel,">All I said is GPUs are failing to keep up with the rest of hardware and software, not that there are no valid reasons for it.  Why complain about something that there are valid reasons for lol",Negative
Intel,"It just makes such a small difference in UE5, it's really not worth losing 30% performance over for this engine. They would agree with you for a lot of other games, and Avatar Pandora kind of has a hidden setting, they'll maybe make available in menu at some point. Right now you need to modify a config file to enable it. Maybe they just need to wait until the final patch of a game to show those settings, years after launch, or just name them ""next gen"" or ""experimental"" with the setting below called ""ultra"".",Negative
Intel,"You can use DLSS and frame generation to play at higher resolutions. That's their intent. Especially UE5 games, because TSR was developed by Epic for a reason. The games on UE5 are really never intended to be run at a native resolution. I don't tell people to run UE5 at a 2013 resolution, but I also don't tell them to have the 2013 mindset that everything has to be run at native resolution, and that's the only way to play it. 1440p Balanced DLSS should give you around 50-60 fps without frame generation.",Neutral
Intel,"The performance loss does not mater for future (re)plays.  The config settings are usually hidden because during testing there were instabilities found that they didnt think was worth fixing. There were some games that had settings beyond ultra with names like ""Extreme,"" ""Nightmare,"" or ""Insane"".",Negative
Intel,"3050 performance at half the power, can't argue with that.  I do wonder how long it will be before the low-end dGPU disappears entirely from laptops, suspect these new chips probably aint it due to price but it's presumably coming.",Neutral
Intel,"While Geekbench's OpenCL isnt super popular for GPU testing;  I still cant believe a mainstream integrated GPU is actually competing with laptop 3050Ti, and these results still aren't with final optimizations!  by the time Panther Lake comes out, Intel claiming that its as fast as laptop RTX 4050 might actually be true lol",Negative
Intel,Are these better than Ryzen igpu? How do they compare to RTX 4050 AND 5050 in non demanding game?,Neutral
Intel,"I'm excited sure but if the claims are true how can anyone here expected these to be reasonably priced and sell well? If it competes with a laptop dedicated GPU it will be priced 2-3 times that laptop. The 285H isn't even that popular and its expensive, hope the 385H is affordable AND the claims are true.",Neutral
Intel,"The score is slightly above a GTX 1650 Super which in turn is slightly above a GTX 1060 which is barely matched by current most performing 128 bits iGPU (Lunar Lake / Strix Point)  Even if we expect 50% improvement compared to 890M, that's still half the performance level of a full power mobile RTX 3060. (roughly half a PS5 too)",Neutral
Intel,This would still no where be close to M4/M5 in single core and GPU,Neutral
Intel,"Hello 6950! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"The problem with buying a device relying on Intel graphics drivers, is that you're getting a device relying on Intel graphics drivers.  I'd rather not.",Negative
Intel,"iGPUs have already killed off the MX series, suppose it’s a real possibility other low end dGPUs also get killed off",Neutral
Intel,"More likely what is considered a low end dGPU will shift, just as a current low end dGPU would have been an high end model a decade ago.",Neutral
Intel,"iGPUs is why low end desktop parts dont exist anymore, its going to do the same for mobile parts.",Neutral
Intel,"It'll happen eventually but they have a few issues.  First is memory bandwidth, they'd need to put a wider bus on it to address which is costly.  Second is the memory itself, they need something with higher bandwidth (but that might be addressed with LPDDR6).  To address the bandwidth issue they often need to put on extra cache, the 12Xe3 core is basically maxing out the bandwidth of the 'standard' chip, and if they went to say 20Xe3 cores (which would be 4060+ level) then they would need a bigger memory bus and more cache (Strix Halo put 20mb of MALL cache to try and address, and a 256 bit bus) and that is more design work.  They'd also have an issue with the socket - the APU would be too big to fit into a standard socket and would need a custom one which means a custom motherboard which increases costs.  Another is heat - while more efficient overall, all the heat is in one spot so the computer needs to be thoughtful about moving that out.  A think that in 2028 big APU solutions can compete well against XX50 series and most XX60 cards.  It isn't happening before then however.",Negative
Intel,Apple's M5 already beats the 3050Ti though.,Neutral
Intel,I mean the early Iris pro onboard GPU’s traded blows with gtx 650m at a lower power draw. It’s been done before. Still nice to see but nothing ground breaking,Neutral
Intel,We will have to wait for the actual laptops to release & see how they do in real games; performance in synthetic benchmarks does not always line up in real tests.,Neutral
Intel,PTL should have comparable or possibly even lower production costs than ARL. There's some room for optimism.,Neutral
Intel,this GPU is also coming into the 355H btw (more specifically there's a Ultra X7 version of the 355H with the full 12-core GPU)     and iirc last time the 255H was pretty popular,Neutral
Intel,"Way we should think about it in my mind is that it's got 12Xe3 cores, and 1 Xe2 core = 2 RDNA 3.5 cores.  Strix Halo has 40 RDNA 3.5 cores (though with a bigger bus and enhanced cache) which is kind of like 20 Xe2 cores (maybe a bit more), and Strix Halo basically ties with a 4060.  If you assume a 5-10% improvement from the architecture going Xe2 to Xe3 to offset the bus and cache (which isn't *as* limiting with only 60% of the hardware), then you're probably at about 60% of a 4060 power, which is more or less where this looks to fall.  4060 mobile passmark = \~17,400  4050 mobile passmark = \~14,300 (81% 4060)  60% 4060 mobile passmark = \~10,500  3050 mobile passmark = \~10,100  That puts it pretty much at a 3050.  With a little sprinkle of cutting edge upscaling and a bit of frame gen it'll be just fine for light titles, but won't be a dedicated monster.  It's just a good solid chip for light-weight long-battery general purpose use that can play some titles on the side (or play older titles with confidence).  I suspect it'll review pretty well, since the reviewers tend to be excessively games-focused and this will appeal, but it's not a games machine - it's a surprisingly zippy long-life chip that's a good fit for all-day laptops.  EDIT: 8060S (Strix Halo) is \~18,000 for reference.  EDIT 2: 890M (strix point's top end) about 8,100, so the PTL 12Xe chip will smoke that.  I'm excited about it, it looks like a great chip!",Neutral
Intel,"It is not 2023 anymore. Intel Arc drivers work quite well, and Lunar Lake graphics are already pretty solid in the low power category. No need to keep repeating this nonsense when you haven't even used the devices.",Negative
Intel,"Only sticking point seems to be cost I suppose, implementing a truly competitive iGPU looks like it will be expensive if Strix Halo is anything to go by.  I think the Intel/Nvidia partnership might at least come close to making it a reality to have a 50/60 tier integrated GPU in a laptop that isn't much more expensive than one with a dGPU currently... in theory it could/should be cheaper.",Neutral
Intel,Not really? Even optimistically speaking this won't be close to a 5050 laptop when accounting for bandwidth limitations in real world tasks that use the cpu and gpu at the same time.,Negative
Intel,"It's a good point. There really isn't any truly low end dGPU options out there now as far as I know, Nvidia abandoned the GT line and someone up there mentioned MX is dead etc.",Negative
Intel,I’m assuming the caveat to posts like this is “running a version of windows/linux”,Neutral
Intel,"and the 780M when fed enough power also comes within range, and the 890M also beats it.",Neutral
Intel,"It could beat a 5090, it would still be useless until the bootloader is open.",Negative
Intel,Wasn't the previous Intel igpu really good for games and efficient?,Positive
Intel,"Even if what you said was true, I know enough about Intel to know that even if it worked great, they'd find a way to fuck it up eventually. That's what they do.",Negative
Intel,">if Strix Halo is anything to go by.  I think Strix Halo skews how expensive a competitive iGPU could be. Strix Halo was very low volume and was also, imo, overbuilt. I think it's definitely possible to have a more cost competitive big-iGPU SoC",Neutral
Intel,Prices for Strix Halo are rumored to fall in the new year; I'm curious to see if it ever reaches 1000-1500 dollar laptops like I've seen suggested it could. Seems like a ridiculous idea right now.,Negative
Intel,And in gaming.,Neutral
Intel,I mean...technically Asahi Linux exists for Macs. Though not the M5,Neutral
Intel,Then there's the 8050S & 8060S,Neutral
Intel,"How so? It is not even beating, at best barely even with 3050 35W (non-Ti).",Negative
Intel,I'm sure thats the first thing that comes to mind when people purchase a thin and light.,Neutral
Intel,"The Arc 140V like in Lunar Lake 268V? it's about 3050 Mobile (35W) sort of performance in real tests. There's a 140T in some chips too, I think it's less powerful than the V.",Neutral
Intel,Strix point is the better comparison,Neutral
Intel,"I hope so. I was a little surprised by the price but as you say, they clearly have a market for that thing and it isn't particularly price sensitive.",Neutral
Intel,they are not really comparable to the traditional APUs,Neutral
Intel,These are 256bit bus devices and have even fatter GPUs .....,Neutral
Intel,"Wait, is the 780/880m close to the 3050? I had heard that it was close to the 1050 at some point... As an 780m owner, it sounds like I can suddenly play way more games than I imagined?",Neutral
Intel,yes that one,Neutral
Intel,"Right, but Strix Point suffers from the same problems that PTL-X will: 128b bus chocking the memory bandwidth.  If iGPUs want to truly replace entry level dGPU, there needs to be wider bus variants",Neutral
Intel,"Might seem so, but mobile 3050 is way slower than let's say 1660 laptop.  Still, you can play many games, just at lower res and settings. You can look up benchmarks/tests for handhelds with this gpu (or ryzen z1/z1 extreme)",Neutral
Intel,"No doubt , I meant in terms of availability",Neutral
Intel,Why can't they have a low core count CPU but still keep the full iGPU?,Neutral
Intel,"While this has pretty much been known for a while, you really do get the feeling when looking at these frequency and power stats that Intel hoped for more from 18A.  It's a good node and a good chipset, I think it's targeted and designed well and will do well in the market, but it doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.",Positive
Intel,"6 different top models with just 100mhz between them is so stupid, that should be 1 or MAYBE 2 models for that core configuration  But just hope the 356H is cheap then",Negative
Intel,Is this desktop or laptop?,Neutral
Intel,">16C (4P + 8E + 4LP)  I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  Two clusters make sense. What am I supposed to do with three?!  Unless I'm mistaken, AMD seems to be sticking with full fat Zen 5 cores in both Strix Halo and ""Fire Range,"" and frankly, it sounds like Intel is just trying to keep up in the core count race with its plethora of 'baby' cores.  Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.   I use my laptop for convenience and my desktop for compute heavy work.",Neutral
Intel,"16 cores, but only 4 of them will be fast. The rest will be slow cores.",Neutral
Intel,"Hello Geddagod! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"Nice. My a770 still slaying 4k ultra gaming in 2025. No stutters no freezing just responsive beautiful gaming. Ofcourse it’s paired with 14900k so my igpu technically has identical specs but I keep temps down this way. Also have an RTX 50 series. I love arc graphics. Bang for buck AT HIGHER RESOLUTIONS — that’s where it’s at. And I’ve had 4090 (sold), amd 9070(sold). Never would I pay what they are asking for a 5090. Just because I wasn’t 100 percent thrilled with price per FPS with the 4090. Once you have had all of them you will see Intel is the way to go with 4k gaming(unless you like paying thousands of dollars). Arc doesn’t have shadows and artifacts . RTX does. Bad. Blurs my picture even on best settings possible.",Positive
Intel,Upselling,Neutral
Intel,IMO it's probably not economical for them to do it.   Gaming laptops/handhelds are already niche enough as it is -- doesn't make sense for them to package both and for OEMs to carry both for just for a ~$1-200 delta between the units. Those who want the full GPU will just pay for it.,Neutral
Intel,"You gotta remember these CPU’s don’t have multi-threading. AMD gets 16 threads out of their 8 core z1 and z2 extreme but with these core ultra’s you get one core and that’s it. A full on physical core is better than a virtual core, but the same amount of cores with multi-threading is much better than just a same core-count CPU without it. Intel are offering up to 16 cores which is 16 cores/16 threads, but generally should offer better multi-core performance vs an 8-core 16-thread. If anything it’s nice Intel have the ability to improve core count offerings without significantly raising the power requirements.   I for one veered away from the MSI Claw 8 AI+ as a standalone PC solution because it lacked the multi-core performance I needed with it only having 8 physical cores and no multi-threading. A 16-core offering is exactly what I need.",Neutral
Intel,Because nobody would buy it.,Negative
Intel,"yeah, sucks cause they said handhelds was a priority for them",Negative
Intel,"Not great, not terrible. In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.",Negative
Intel,According to jaykihn0 it's a heat issue: BSPD and transistor density is flattening the VF curve on the high-end. The ST power curve is substantially improved at lower frequencies over Lunar Lake and Arrow Lake.,Neutral
Intel,"If anything, seems strictly worse than N3, even compared to older iterations. That's not great for a node that was supposed to go toe to toe with N2. Especially if the production cost is more in line with the latter.",Negative
Intel,"18a is initially launching with its lower power variant, which is why the only nova cpus that will use it for the compute tile are the laptop ones. The higher power variant comes later",Neutral
Intel,"Jaykihn, whose tweets these leaks are based on, says that the design characteristics cannot be extrapolated to the node itself.   So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.",Negative
Intel,"> […] It doesn't seem like it improved on N3 and I know that they were targeting better frequency and lower power than they got.  Well, we already had rumors of Intel struggling to hit actual intended frequency-goals, no?  Though especially the actual performance-metrics of the node will be quite sobering I think.  The power-draw of these SKUs is supposed to be a TDP of *officially* 25W – In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4) and 65/80W (4+4+4 and 4+8+4) respectively.  So it must be seen, if these parts are any more efficient in daily usage, or just a side-grade to Lunar Lake.",Negative
Intel,"Panther Lake is pretty much a pipe cleaner, so it should still improve a bit.  Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD. If that is the case, it will only really be allowed to shine in high powered desktop and server chips, while power limited chips won't be able to reap as many of the benefits of 18A, since laptops tend to target the efficiency sweet spot, rather than clocks.  My understanding is that 18A will scale much better with extra power than N3 and N2. Once you pass the sweet spot, 10% extra performance on N2 might take 30% extra power, while 18A should continue to scale much more linearly for a good while longer.  So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones, especially in use cases where SRAM density is less of a concern. AFAIK Intel CPUs aren't as cache-dependent as AMD, so SRAM density might not be the highest priority, whereas slugging it out with AMD and trying to beat them on frequency might?  From what I've heard, Intel is still just about matching AMD in terms of IPC, while Arrow lake is very efficient, just not great for gaming. If Intel wants the CPU crown back, simply pushing more power could go a long way in clawing back the deficit, assuming 18A does indeed scale well with more power. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.",Neutral
Intel,> 6 different top models with just 100mhz between them is so stupid  They've always done that because of the OEMs who like a million segments.,Negative
Intel,Laptop.,Neutral
Intel,Since when is any user determining what cores they want to use and when?,Neutral
Intel,It makes sense to have three as the low-power cores prioritize keeping the power consumption low for battery life but to achieve this they aren't attached to the ring bus which hurts performance so 8 regular E cores are also used,Neutral
Intel,">I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  From a user's perspective, CPU's are a blackbox and the inner-working details are irrelevant. What matters is results. They want, typically, a balance of performance, battery life, heat / fan noise, and cost, placing greater emphasis on one of these categories over the others.  The idea is that P cores = performance optizied  E cores = Area optimized  LP-E Cores = power optimized.  The theory being to have different core types focus on different parts of PPA.  E cores and LP-E cores are the same microarchitecture. Just LP-E cores are off-ring so the ring (and rest of the cores) can be powered down in light-load scenarios.  Intel's biggest problem is that looking at each of the cores, the P cores perform their designated role the worst. 200% larger than E cores for \~10% more IPC and \~15% more clockspeed is a pretty rough trade off - and by the time you've loaded all P cores and need to load E cores too, that clockspeed difference diminishes hard.  Intel's better off (and all rumors point this way) to growing the E cores slightly and making that architecture the basis of the new P and E cores (like Zen vs Zen-C)",Neutral
Intel,"> Now, I'm sure those might be useful in some niche use cases, but I'm speaking from an average user's perspective.  You are the niche use case. The average/median user is using H-class laptops.   These are meant to beat Strix Point and its refresh, which it will do and at better economics for Intel than Arrow Lake H.",Neutral
Intel,"And with all the issues Windows already has with the scheduler, I fully expect it to never be able to properly utilize the different cores. I expect a similar situation as with previous versions, where it's sometimes necessary to disable the slow cores to eliminate stuttering in games, because they sometimes offload tasks to them which slows the whole game down to a crawl.",Negative
Intel,"> I can't say I'm entirely sold on the idea of having three core clusters humming inside my laptop.  > Two clusters make sense. What am I supposed to do with three?!  Ain't these Low-Power Efficiency-Cores aren't even usable by the user anyway in the first place (and only through and for Windows' scheduler to maintain)? As I understand it, LPE-cores are essentially placebo-cores for marketing.",Negative
Intel,Darkmont should really not be that far off Cougar Cove. Certainly not *slow*.,Negative
Intel,The E-Cores are plenty fast for most use-cases by now and there are 8 of them in there. Aren‘t they even ahead of the skylake IPC by now? 8 will be doing a great deal with a good clock frequency.  Only the 4 LP-Cores are kind of weak. But they aren‘t attached to the ring-bus and really only used for offloading low req Backgroundtasks and an idling machine. So that you can turn off the faster cores when they aren‘t necessary.  But for performant gaming and productivity it is basically a 12 core machine.,Positive
Intel,I hope there's a 4+0+4+12 for handhelds but they might not consider adding a whole new tile configuration packaging line worth it over just giving the OEM a discount.,Negative
Intel,I find that weird. Who buys a 20 core CPU for GPU performance equal to like an RX 580?,Negative
Intel,"Yeah as of now only MSI looks committed to using Intel for their handheld, and not only are they not that successful but they had also branched out to AMD for another line of their gaming handheld.",Neutral
Intel,Gaming handhelds are niche but not gaming laptops (relative to gaming desktops). Steam hardware survey reveals a large number of mobile gpus very high up in the charts.,Neutral
Intel,Both Apple's M series and LNL use decent iGPUs for their 4+4 parts.,Positive
Intel,"This is a minority use case, but is a very valid one. I run a home server (/r/homelab) and could use some extra GPU power for video encoding and some basic AI stuff. Though this is just a home server so I don’t want to put in a dedicated GPU due to the desire to not spend all of my discretionary income on my power bill. Right now I have a 9th gen i7 that is good enough for the basics of what I am doing, but I’ve been keeping my eye out for a replacement. I’m not alone in that need, but I do get this is a minority group. I’m sure there are other use cases though. Saying no one would buy it is wrong.",Neutral
Intel,"IIRC Arrow Lake's compute tile is fabbed on N3B, which is pretty much TSMCs earliest N3 node for mass fabrication.  Its other tiles are on a mix of mature 5nm- and 7nm-class nodes.",Neutral
Intel,"> In defense of Intel, Panther Lake is pretty much a pipe cleaner product for 18A whereas Arrow Lake was manufactured on a fully mature N3 node.  Arrow Lake was the pipe cleaner for 20A which in turn was deemed unnecessary to move into production because everything was going so well that they could use that groundwork to get a head start and move straight to 18A. That's right from Intel's mouth. Now the goalposts shift once again.",Neutral
Intel,"> Not great, not terrible.  For what it's worth, I'm glad and somewhat relieved, that the never-ending charade of 18A (and ultimate sh!t-show it eventually again amounted to, after their blatant Vapor-ware 20A-stunt) finally is about to come to some lousy end, after years of constant shady re-schedules, even more bi-weekly Intel-lies and basically +2 Years of delays again.  So it will be a bit of … 'consolation' I guess, not having to constantly read that 18A-sh!t in every darn Intel-news.",Negative
Intel,"I'm curious to see the core's power curve itself. Or the core+ring, I forget what exactly Intel measures here for their software power counters.   Intel claims PTL has outright lower SoC power than LNL and ARL, so if the power savings are coming from better uncore design and not the node gains themselves....",Neutral
Intel,"> BSPD and transistor density is flattening the VF curve on the high-end   He doesn't make this claim. BSPD in particular was supposed to help the most at high voltage, and the density does not appear to meaningfully differ from N3, which it still regresses relative to.    Best case scenario, the node, for other reasons, can't hit as high voltages as N3, and the top of the curve is just capped. But you wouldn't expect thermals to be such an issue then.    Doesn't seem to be any evidence of the core-level VF curve benefiting from the node either.",Neutral
Intel,"Jaykihn attributes only density to the supposed heat issues, not BSPD. Which makes sense because the process guy and Stephen Robinson both talked about having to space out the signal and power wires when working with BSPD.  If they prioritized density, then it is possible that frequency could not reach the theoretical maximum due to crosstalk or heat issues.",Neutral
Intel,"18A has always been low density, but expected to compensate for it with very efficient frequency scaling past peak perf/W. It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Wait for the desktop/server chips before you call it. With fewer thermal constraints and much higher power budgets, they should be able to push well past the perf/W peak, where it should continue to scale well and for a long time before it starts hitting severely diminishing returns.  I'm not talking 300W monster CPUs, but the scaling between a 65W and 105W 18A CPU should be more significant than on current nodes, where you might gain like 10-15% additional performance from that 50% power bump. I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.",Neutral
Intel,> So it is less likely that the clocks are limited by 18A. It is just what was decided for this particular product.  Why would they suddenly decide to underclock it to such a degree?,Neutral
Intel,"Tbh, LNL battery life (note: not the same thing as loaded efficiency) with -U/-P/-H perf levels and market reach would still be a very good thing. It's just a shame to see Intel finally straighten out their SoC architecture only to be hamstrung by a subpar node.",Neutral
Intel,">In reality, these parts are allowed to draw as much as 55W (2+0+4 or 4+0+4)      Intel's existing 2+8 parts have a PL2 of 57W, so seems to me OEMs are permitted to target the same PL2 they've been targeting on U series.  >and 65/80W (4+4+4 and 4+8+4) respectively.  Which is a fairly large drop compared to existing H series. ARL-H PL2 is up to 115W, PTL-H PL2 will be up to 65/80W",Neutral
Intel,">So while it may be an N3 competitior in low power chips, it should be much closer to N2 in high powered ones,  NVL-S being external adds serious question marks to this.   I wonder if post NVL-S they go back to using further iterations of 18A though for even desktop, if they feel confident enough that they can hit high enough Fmax (even throwing away power and density) on those compute tiles.   Maybe for RZL or Titan Lake in 28' (going off memory for codenames lol).   >especially in use cases where SRAM density is less of a concern.  On paper, Intel has caught up to TSMC (even N2) in SRAM density.   >AFAIK Intel CPUs aren't as cache-dependent as AMD,  Generally speaking Intel uses way higher capacity caches than AMD, AMD uses smaller but faster caches.   >. A 10-15% frequency advantage would go a long, long way towards closing gaps if it comes as a relatively small power cost.  This seems *extreme*. Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?",Neutral
Intel,"I actually suspect otherwise - speculation on my part, but I suspect that they were running into unexpected process issues when they pumped the frequency and/or voltage higher so they had to scale back performance.  It almost feels like a process bug that they discovered when making the actual chips.  They've also talked about kind of weak yields on 18A on their latest earnings call.  That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  If they have a 'double upgrade' opportunity in both fixing the discovered 18A issues and implementing the originally planned 18AP improvements then it could end up being a somewhat bigger jump.  Don't think it'll be an N2 beater though.  18AP *might* beat the best of N3 in some applications, we'll see.  For that you'd want to look ahead to 14A where the CFO mentioned on the earnings call (and he was somewhat realistic in other areas so it's more believable here) that 14A has been a positive surprise and they're actually pretty excited about it.  It's possible that if Intel gets more experience on High-NA versus TSMC that they might start having an edge later on.  Maybe.",Neutral
Intel,"> Then again it has long been said that 18A is supposed to be a relatively low density, high frequency node due to the very good voltage control enabled by BPD   And yet we see the exact opposite here. The leaker claims these go to to 65W, even 80W TDP. There's plenty of power to hit any ST boost the silicon can handle, yet it *regresses* vs N3B ARL. And that's with a year to refine the core as well.    > If that is the case, it will only really be allowed to shine in high powered desktop and server chips   Server chips are low voltage. Even lower typically than mobile chips. High-V only matters for client.    > My understanding is that 18A will scale much better with extra power than N3 and N2   There is no reason to believe that at this time. Notice that Intel themselves are using N2 for their next flagship silicon, including desktop.   Edit: Also, for the ""pipe cleaner"" rhetoric, remember how they cancelled 20A claiming 18A was doing so well and ahead of schedule? Even more obviously a lie now.",Neutral
Intel,Having to roll the dice on the scheduler doesn't make things better.,Negative
Intel,"Since release of Windows 7, which allowed users to easily set which cores are used on per application basis in first party software (task manager).",Neutral
Intel,"It's been a thing for a while as a means to circumvent the Windows kernel's shitty scheduling, especially on processors with asymmetric core or chiplet arrangements. Tbf, Windows has slowly begun to catch up and run cores a little more efficiently, but some people still swear by apps like Project Lasso for manually controlling their process affinity.",Neutral
Intel,"Users aren't ""determining"" anything because they don't have to.   On smartphones, they've virtually zero control over their own hardware.   And on the x86 side, I don't think people are buying big.LITTLE hardware in droves, and even if they are, I doubt the multiple core clusters are their deciding factor.   And if I had a big.BABY CPU, I’d definitely be tempted to play around with it, and yes, I don’t see the big idea behind having three clusters, aside from marketing gimmicks and artificially inflated MT benchmark scores.  From what I've heard, Windows Scheduler has trouble dealing with just two clusters as it is. On Linux, the experience is, at best, tolerable.  At the very least, I've certain issues with Intel marketing these CPUs has having ""16 cores."" I know a lot of 'normies,' first hand, who have fallen prey to this deceptive marketing tactic.",Negative
Intel,"> E cores and LP-E cores are the same microarchitecture   In this case, they're even the same physical implementation. 100% identical to the compute cores. Also, for MTL/ARL they were neither power nor area optimized.",Neutral
Intel,The average user is using U series,Neutral
Intel,">, which it will do and at better economics for Intel than Arrow Lake H.  Doesn't seem like this will be the case till the end of 26' though. At least from the earnings call a few weeks ago.",Neutral
Intel,"The average user runs apps like Teams, entirely off of the LPE core of Lunarlake and Pantherlake",Neutral
Intel,"LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  4 Darkmont LPE, in therory, should be a significant improvement.",Negative
Intel,"They are definitely more than usable if it’s anything like LNL, which basically defaults to them.  If it’s more like ARL or MTL, it’s placebo except for S0 sleep.",Positive
Intel,"The LP cores in Meteor Lake/Arrow Lake were too weak to be usable as multithreaded boosters, this isn't the case with Lunar Lake and Panther Lake will be no different",Negative
Intel,"Windows are supposed to be using these LPE cores for lighter task such as web browsing or Word documentation or idling so that the they won't have to activate the rest of the cluster -> Reduced power draw and therefore good battery life. Meteor Lake originated with these but they were so slow they were pretty useless outside of idling. Lunar Lake also had 4 LPE cores functionally and we see how good those cores are, and these Panther Lake are supposed to be built on that but with additional 8 E cores for people that really value multi-core performance and to also solve the one major weakness of Lunar Lake, if it is even that major in the first place.",Neutral
Intel,>Aren‘t they even ahead of the skylake IPC by now  They're definitely ahead of Skylake now. Cougar Cove IPC vs Darkmont is gonna be like 10% better,Positive
Intel,> Only the 4 LP-Cores are kind of weak   Not for LNL or PTL. They're not crippled like the MTL/ARL cores. Should be full speed Darkmont. That's like 12th or even 13th gen big core.,Neutral
Intel,"The LPE cores being weak is probably the point anyway, or at least partially. It doesn't need to usurp an insane amount of wattage to get to full speed because it doesn't scale well, but it stays fast enough at lower wattage to default to that power profile -> Improved battery life.",Neutral
Intel,They are ahead of Raptor lake P cores and Zen 4 in IPC under 40W,Neutral
Intel,Honestly I wonder if the 4 e-core cluster on the compute tile is outright more power efficient than the 4 p-cores.    I wish some reviewers would do power efficiency testing with different core count configurations enabled on LNL and ARL. Just for curiosities sake.,Neutral
Intel,"At this point, I just hope they have a handheld chip with just 8 e cores (ensuring the CPU tile is as efficient as it gets) and a decent GPU. Maybe in time for an Intel + Nvidia chip, which could be a dream handheld chip.",Positive
Intel,Gaming laptop with iGPU should be niche. Most people just get Nvidia in their gaming laptops.,Neutral
Intel,Apple gets their margins in upselling memory and storage they don't need to worry about the added complexity of more SKUs for the sake of price laddering,Negative
Intel,"Perhaps fully mature was an overstatement, but it wasn’t the first product on N3B, the node was already in HVM for over a year at that point IIRC.",Neutral
Intel,"We all know that's a lie and 20A was a disaster. Why repeat it now? 18A may not be perfect, but it exists and these chips are comming soon.",Negative
Intel,What delays? 2025 node in 2025?,Neutral
Intel,">BSPD in particular was supposed to help the most at high voltage  Says who?  BSPD improves signal integrity, lowers IR Drop, and helps improve density, at the expense of heat dissipation.   Idk how a technology that is known to increase heat density was supposed to *improve* fMax",Neutral
Intel,"> but expected to compensate for it with very efficient frequency scaling past peak perf/W   Quite frankly, the only people ""expecting"" that seem to be people on the internet unwilling to admit Intel underdelivered with 18A. There's been no objective reason to believe that was even a target for the node. If anything, the exact opposite. This was supposed to be the node where Intel focused more on low voltage for data center and AI.    > where you rarely push past the best perf/W due to thermal and battery constraints   That is not the case. ST turbo in -H series laptops has always been high, nearly on par with the desktop chips. At the power budgets being discussed, there's plenty available to hit whatever the silicon is capable of, and Intel's never been shy about pushing the limits of thermals.    And again, when you adjust for the power/thermal envelope, you *still* see a clock speed regression vs even ARL-H. The best possible outcome for Intel is actually that they can't hit the same peak voltage but look ok at low to mid.    > Wait for the desktop/server chips before you call it   Server is typically low voltage, or mid voltage at most. It's lower down than even laptop chips. And what desktop? Intel's using N2 for their flagship chips for NVL, which should really have been an obvious indicator for how 18A fairs.    > I'd expect 65W to 105W on 18A to yield 20-25% due to the benefits of BPD.   Intel had numbers in their white paper. BSPD, all else equal, was maybe a couple percent at high V. It's not going to produce dramatically different scaling, and again, we have direct evidence to the contrary here.    In general, Intel's had a lot of process features over the years that look a lot better on slides than they end up doing in silicon.",Neutral
Intel,"> 18A has always been *Xyz* …  There's always some excuse for Intel's stuff the last years, to accidentally NOT perform as expected, no?  > It was always going to struggle in laptops, where you rarely push past the best perf/W due to thermal and battery constraints.  Hear, hear. Isn't that all to convenient …  > Wait for the desktop/server chips before you call it.  … and when is that supposed to happen? Nova Lake will be TSMC's N2.  What *Desktop* CPU-line will be on Intel 18A anyway then?",Neutral
Intel,Give reasons why you claim this is an 'underclock' that was done 'suddenly'.,Negative
Intel,"Of course, Lunar was quite competitive, yet it was mainly so on performance due to Intel basically sugar-coating the living benchmark-bar out of it via the on-package LPDDR5X-8533 RAM. That OpM for sure *majorly* polished its efficiency-metrics by a mile …  Though looking back the recent years, that's how Intel always masked rather underwhelming progress on their process-technology – Hiding the actual architectural inefficiencies and shortcomings behind a invisible *wall of obfuscation* by only ever deliberately bundling it with other stuff like newer PCi-Express versions of 4.0/5.0 or newer, faster, crazy high-clocking RAM-generations.  So Lunar Lake while very strong, was mainly so due to being propped by OpM, and TSMC's processes of course.",Neutral
Intel,"The article claims it's ""TDP"", which would typically refer to PL1. Might be leakers getting their terminology mixed up, but I wouldn't necessarily assume a large drop in power limits.",Neutral
Intel,AFAIK Intel was at 165W in mobile back then …,Neutral
Intel,"> Intel has claimed BSPD is only what, high single digits at best improvement for Fmax IIRC?  [6%](https://web.archive.org/web/20240901235614/https://www.anandtech.com/show/18894/intel-details-powervia-tech-backside-power-on-schedule-for-2024/2)",Neutral
Intel,"I wonder if the issues Intel's facing with 18A partially explain TSMC's very conservative approach to adopting ~~it~~ BSPD.  I image in reality, 18A (and AP's) fMax limits are almost entirely the reason for N2 in NVL-S",Neutral
Intel,"> That makes me optimistic about the improvement opportunity in a more mature 18A if they can chase down these issues, and perhaps even more in 18AP if they can clean up these bugs.  There's no doubt that given enough resources they can fix the node to the point where it's good enough for HVM in high performance chips. It's what they did with 10nm after all but it took them several years to go from poorly yielding Cannon Lake/Ice Lake to okay Tiger Lake and good Alder Lake.  If we take the talk about 18A ""margins"" from the recent analysts call to be a euphemism for something like node yields and a stand in for when it will be ""fixed"" then we could be talking 2027 before 18A gets to that point. A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.",Positive
Intel,"Yeah I know they talked about yield issues, but everything indicated that it was not a case of manufacturing defect density, but rather something else.   That would imply issues with hitting target frequencies, which as you say, they should be able to iron out in due time. That is as long as it isn't a fundamental issues, which I have a hard time believing, given how unconcerned they seemed about it.   I'm fairly confident it will be ironed out before Clearwater Forest hits mass production.",Neutral
Intel,"There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery. It offers better voltage control and less current leakage, leading to higher efficiency at any given voltage, but especially past the point where current leakage starts becoming more of an issue, i.e. at very high frequency.  A pipe cleaner running at low frequencies is to be expected. Wait for the desktop chips and you will see P cores hitting well over 6+ GHz advertised all core boost, possibly 6.5 GHz. I wouldn't be surprised to see some users hitting 7 GHz stable.",Positive
Intel,The modern CPU cores schedule all cores equally with E core priority because nowadays the difference between E core and P core is less than between AMD X3D CCD and higher clocking CCD,Neutral
Intel,"Windows Scheduler has come a long way but I mean come on, this is pretty pedantic. The commenter above is clearly not using a tool to assign cores to specific tasks they are just being annoying.",Negative
Intel,4 skymont already seem pretty good in LNL,Positive
Intel,> LPEs are supposed to very low power tasks but the last gen Crestmont LPEs were just too weak to actually do anything.  Makes you think why Intel even went all the way to integrate those and waste precious die-space doing so …,Negative
Intel,They could be useful if they could check your email and stuff while the laptop is effectively sleeping.,Neutral
Intel,PLT's LP-E cores take after LNL. ARL/MTL's LP-E cores design is dead,Neutral
Intel,I hope those are powerful enough to eventually act as the low-power booster these were once supposed to.,Neutral
Intel,> Meteor Lake originated with these but they were so slow they were pretty useless outside of idling  The last info I had on the back of my mind was that these couldn't even be associated by the user and were basically reserved for Windows itself. Is that still the case now?,Negative
Intel,"Yeah after being reminded that the LPE cores are also Darkmont like the E-Cores I looked up the layout in detail and the differences is simply the P and E cores are on a large Cluster with L3 Cache and a fast ring-bus, but the LPE cores are seperated from that Ring-bus and don't have any L3 Cache (just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster).     They are for sure also much more limited in power-budget and frequency, but they can be very fast in general. The main usage is being able to disable the large cluster in Idle or low cpu usage to heavily reduce power draw.     The communication in between the clusters is much slower. So anything that relies on synchronisation and communication between cores is only really fast (latency-wise) if it is kept within the same cluster.      The LPE cores therefore can be very fast actually, but only if the task stays within the cluster. Modern demanding games will therefore 99% soley run on the large cluster, because they would run slower if they spread across all of them. The small cluster could be used to offload background tasks to themselves (if the powerlimits aren't hit), which could at least improve 1% lows and fps stability.",Neutral
Intel,"It's not just more power efficient, but also much faster at low power which you kinda want since you'd be mostly GPU bottlenecked. The good thing is that they've made that part of their thread director tech.",Positive
Intel,Yes it is. Memory uses power and IPC of LPE should be past Zen 3,Neutral
Intel,Intel makes e-core only CPUs (N100 etc) but at this point they're based on several gen-old e-core design. They're good enough for what they are so they might not update them for a while still though.,Neutral
Intel,"even 4 e-cores is enough, dont forget we use to pair up i7-7700K with GTX1080/1080Ti.  4 e cores + even stronger iGPU probably a better combo for handheld.",Positive
Intel,Apple absolutely does price ladder their SoCs.,Neutral
Intel,"True. But the node itself may be kinda mid. It seemed much, much more complex than N3E, and it's not as if the node was any sort of highlight in the products Apple used it in.   Going back to A17 reviews and such, there were serious questions presented about N3B vs N4P.",Neutral
Intel,I'm fairly sure that's what u/ProfessionalPrincipa was also implying.,Neutral
Intel,> We all know that's a lie and 20A was a disaster. Why repeat it now?   Probably in response to all the people assuming there must be some kind of upside.,Negative
Intel,"It was supposed to be a 2024 node. And given the perf downgrade and admitted yield problems, seems more like they're only delivering the originally promised metrics in '26 or even '27.",Negative
Intel,"> What delays? 2025 node in 2025?  What del— *Seriously now!?* Where you living under a rock the last years by any chance? o.O  18A is neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: *'cause Yields again!*) will be 1H26 and most likely even end of first half of 2026.  So it's a 2H24-node, which Intel is only able to offer actual volume basically +1–2 years later. That's called *»delay«*.  **Edit:** And not to mention the actual massive performance- and metric-regression u/Exist50 pointed out already. 18A is basically 20A in disguise. Since 20A wasn't actually knifed, but just relabeled as ""18A"" instead.",Negative
Intel,> Says who?   Intel. It was part of their own published results about PowerVia on the Intel 4 test chip. The gains range from ~negligible at low-V to around 6% at high-V.    Also explains why TSMC is not adopting it in the same way.,Neutral
Intel,Nova Lake is full on N2?,Neutral
Intel,"It's literally a regression from the prior gen, for a node that was supposed to be ""leadership"", mind you.    I don't know why it's hard to acknowledge that 18A is simply underperforming.",Negative
Intel,"Nah, credit where credit is due. LNL made a ton of fundamental design changes that PTL should also benefit from. Yes, they also benefitted a lot from both having an actually decent node and on-package memory, and no, they're not on par with the likes of Apple or Qualcomm, but merely making ARL monolithic on N3 would not have delivered these gains.",Positive
Intel,They definitely have their terminology mixed up. There's no chance PTL-H has an 65W/80W PL1,Negative
Intel,Which was never the PL1 but rather the PL2.,Neutral
Intel,"Didn't TSMC delay their BSPD node too? Ik they changed it from appearing in N2P vs A16, but unsure if there was a timeline shift in that as well.   Honestly, what's going on with N2P/N2/A16 seems to be kinda weird, timeline wise.",Negative
Intel,2027 just seems like hitting baseline parametric yields. Being able to get to an actual improvement in terms of performance is a whole other thing. Not an easy road.,Negative
Intel,">A planned 2024 product delivered in 2027 is a big yawn and another stake in IFS.  How it can be 24 product in 27 when it was 25 node in 25 with products on shelfs in january 26, rofl?",Neutral
Intel,Intel themselves claim they won't be hitting industry acceptable yield till 2027 for 18A.,Neutral
Intel,"> There is every reason to believe 18A will scale well with more power. That is literally the main point of backside power delivery.    No, it's not. The main long term advantage of BSPD is to reduce the pressure on the metal layers as they do not scale like logic does. Better PnP is another advantage, but secondary. No one cares much about high-V these days.    > A pipe cleaner running at low frequencies is to be expected   As a reminder, ARL-20A was supposed to be the pipe cleaner, and they claimed they cancelled it because 18A was doing so well! PTL was supposed to be the volume product, a year after 18A was nominally supposed to be ready.    > Wait for the desktop chips   What desktop chips? PTL-S was cancelled long ago, and for NVL, they're moving the good compute dies (including desktop) back to TSMC on N2. A decision which should demonstrate the node gap quite clearly...   Also, ST boost for -H chips is in the same ballpark as desktop ones. They're already at the high end of the curve.",Neutral
Intel,"They aren't the same though, hence the roll of the dice to see if you actually get the performance you paid for.",Neutral
Intel,Because they still improve battery life under very light loads.,Positive
Intel,Yeah I think you had to utilize tool such as Project Lasso. I tried to assess the LPE cores to HWINFO64 and it was rather painful to use so I understand why these cores are never used otherwise lol.,Negative
Intel,"Yeah. The efficiency and overall battery life will probably comes down to how Windows manages threads, which seems to be pretty decent now with Lunar Lake seeing as they got pretty good battery life with the same design philosophy.",Positive
Intel,"> just access to a ""memory side-cache"" accessible by the NPU and small cluster but not the large cluster  It's also accessible by the large cluster, fyi.",Neutral
Intel,"For those games back in the day, yeah. But modern games can and do use more than 4 cores.   Depends how low end/power you want to go, of course. 4 cores + eGPU cuold be enough too depending on your games: [https://youtu.be/XCUKJ-AgGmY?t=278](https://youtu.be/XCUKJ-AgGmY?t=278)",Neutral
Intel,"The e-cores are unlikely to be hyperthteaded though. 8 e-cores gives you 8 threads just like a 7700K did, though none of them share cores. It feels like a sweet spot for a handheld in 2025 imho.   This is while AMD competitors use 8c/16t of full Zen. An 8 e-core solution would remain weaker but more than sufficient on the CPU side, while allowing enough power for what truly matters (a capable GPU). If paired with an Nvidia iGPU, that could be the perfect handheld chip, if their partnership produces one like that.",Positive
Intel,"Those ""people"" are either bots or trolls. No sane person thinks Intel is crushing it with 18A.",Negative
Intel,"Whats the name of the close the gap initiative?  5 nodes in 4 years... announced in 2021  2021+4=2025  You can even read 3rd party articles from 2021 talking about 18A in 2025.  https://www.tomshardware.com/news/intel-process-packaging-roadmap-2025   >Intel didn't include it in the roadmap, but it already has its next-gen angstrom-class process in development. 'Intel 18A' is already planned for ""early 2025"" with enhancements to the RibbonFET transistors.",Neutral
Intel,">neither a 2025 node (but was once supposed to be 2H24), nor does it really come to market in 2025, but the bulk of it (read: 'cause Yields again!) will be 1H26 and most likely even end of first half of 2026.  How it aint 25 node when there will be products on shelves in january 26?",Neutral
Intel,FMax at same power consumption is higher true. But the temperature is just too high. Both temperature and power consumption are important considerations,Neutral
Intel,"High end NVL is N2, low end is 18A. At least for compute dies.",Neutral
Intel,"That's what we know so far, yes. At least the performance-parts. The lower end is supposed to be 18A, I guess?",Neutral
Intel,"> Nah, credit where credit is due.  I said verbatim, that Lunar Lake was quite competitive, and I meant it, unironically. It took long enough.  > LNL made a ton of fundamental design changes that PTL should also benefit from.  I haven't disputed that —  The modular concept was surely ground-breaking for Intel, and urgently needed!  Though, it's kind of ironic, how Intel all by itself proved themselves liars, when it took them basically +6 years since 2017, for eventually coming up with only a mere chiplet-copycat and their first true disintegrated chiplet-esque design …  For a design, which *in their world-view*, Intel was working on their tiles-approach already since a decade, which is at least what they basically claimed when revealed by 2018 – I called that bullsh!t the moment I first heard it. Surprise, surprise, they again straight-up *lied* about it …  They most definitely did NOT have had worked on anything chiplets/tiles before, if it took them *this* long.  Just goes to show how arrogant Intel was back then, letting AMD cooking their Zen in complete silence since 2012/2013, for their later Ryzen. *Still boggles my mind, how Intel could let that happen* …  > Yes, they also benefitted a lot from both having an actually decent node and on-package memory […]  In any case, we can't really deny the fact, that Intel basically cheated on Lunar Lake using OpM, eventually creating a halo-product, which ironically was quite sought after, but yet expensive asf to manufacture.  If AMD would've been to cheat like that (using OpM) in a mobile SKU, while dealing some marked cards in such a underhanded manner, it would've been ROFL-stomped Lunar Lake …",Neutral
Intel,"Willing to believe that, though then I have to wonder why it would be so hard to cool at such a substantially reduced PL2. Also if/how they could pitch this as an HX replacement.",Neutral
Intel,"Yes, I think it was 45- or 65W-parts PL1-wise. The spread was still obscene and outright insane.",Negative
Intel,I'm excited for 18A-P. Intel subnode improvements have always seemed to bring decent uplifts (Intel 10SF being a notable example).,Neutral
Intel,"The difference is no longer stark. As I said, its now smaller than between 2 CCDs in the most popular CPU line ever made in actual raw performance. ie it's not something entirely unique at the scale the differences are",Neutral
Intel,"How even, if these weren't even used with MTL!?",Neutral
Intel,"Yup, pretty much paper-cores for marketing-reasons alone basically.",Neutral
Intel,We already have games tested on Skymont E cores. They are very fast,Positive
Intel,"Yes we know modern games can use more cores, but having smaller amount of cores means saving power budget. Those power consumption saved are better off use to juice iGPU.   Lets not forget the E-Cores here are skymont, not skylake. Skymont is definitely more powerful cores. The exact same reason why i5-7600K is more power than i7-2600K.  given how iGPU for handheld are no way near GTX1080 yet. I think it is better to pump more juice for better iGPU.",Neutral
Intel,Go to the intel stock subreddit lol,Neutral
Intel,"You'd be surprised how many people are willing to take Intel at their word on it.  Just look at previous threads here. Every time Intel posts nonsense slides, lot of people come crawling out of the woodwork.",Negative
Intel,"They are doing about as much as the ""sane"" people expected from 18A.",Negative
Intel,"> 5 nodes in 4 years... announced in 2021  It was supposed to start with Intel 7 in '21 and end with 18A readiness in H2'24.   https://img.trendforce.com/blog/wp-content/uploads/2023/10/17144859/intel-4-years-5-nodes.jpg  And now in the way end of '25, we're getting something more like what they originally promised for 20A, *if that*, and now they're saying yields won't be ""industry standard"" until as late as 2027. It's a disaster by any objective standards. Their failure with 18A literally got the CEO fired...",Neutral
Intel,"> How it aint 25 node when there will be products on shelves in january 26?  It seems, you don't actually grasp the concept of the term »difference«. *2025 and 2026 is actually NOT the same!*",Neutral
Intel,so the igpu is propably tsmc too?  do we know if thats because of poor performence or poor manufacturing capazity?,Negative
Intel,I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   I think they'll probably have ARL-R for HX,Neutral
Intel,"Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage. Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.",Neutral
Intel,"The LP-E cores were a failure in MTL/ARL's design.  They're very useful in LNL and that trend follows with PTL/NVL.  They're definitely not just ""placebo cores"" - LNL uses them more often then the P cores and they can be activated and used in all core workloads as well.",Neutral
Intel,"They were, just not as often as Intel would have liked.",Neutral
Intel,"For MTL and ARL, yes. For LNL and PTL though, I haven't test them out personally but seeing the battery results of LNL chips made me think that the LPE cores of those things seem to be legit.",Neutral
Intel,"Oh year, right, I think I missed (or forgot) that. Darkmont should be even better but I'm guessing not by that much.",Negative
Intel,Kinda making my point.,Neutral
Intel,"Bro xd  Node readiness and product readiness and product on shelves are 3 different date  Node must be ready before product, product must be ready before being on shelves.  Officially 18A was ready around Q2 25",Neutral
Intel,"> so the igpu is propably tsmc too?  18AP, iirc. Good *enough* (or at least, assumed to be when originally planned) not to be worth the extra cost of TSMC.  > do we know if thats because of poor performence or poor manufacturing capazity?  Poor performance. Intel can't afford to be constantly a node behind. It's just too big of a gap.",Negative
Intel,"> I imagine all core boost is probably easier to cool than single core turbo boost if heat density is an issue.   But that's exactly it. They said 65W vs 80W was in relation to cooling challenges, but either is more than sufficient for max ST boost. Idk, maybe reading too much into too little information.    > I think they'll probably have ARL-R for HX   At one point it sounded like they wanted to pitch PTL as a partial replacement. Guess it's not good enough though.",Neutral
Intel,"> Intel H parts and later HX have always been repurposed desktop dies at lower clocks and voltage.  Of course, I already knew that. Binned Desktop.  > Uncapped they could pull almost 100w even in the quad core era, and over 100w starting with 6 core coffee lake-H.  Well, up until 2018/2019/2020 Intel actually didn't really uncapped any of them and those parts were hard-limited to only pull their max 45–65W TDP.  Only later on when they were trying to hold pace with AMD, Intel insanely increased the TDP to pull to insane numbers (90W, 135W, 165W) … Other than that and prior, you had to resort to modded BIOS.  So until even Apple burned their thick skin on some i9 (**Cough* i9-9980HK in the 16"" MacBook Pro 2019), you didn't had such insane TDPs to begin with anyway (except for BIOS-mods).",Neutral
Intel,"As said, I owned a MTL-machine back then, and back then you couldn't even associated these cores after all, since you couldn't pin anything on them – So yes, *back then*, these were placebo-cores and they were in fact pretty much 'on paper-cores for marketing-reason' and basically useless.  Seems Intel managed to improve their performance a lot and make them actually useful, which is good!",Negative
Intel,"Yes, MTL's LPE-cores were basically a dud, LNL was fairly workable and PTL will be hopefully potent.",Neutral
Intel,haha,Neutral
Intel,> Officially 18A was ready around Q2 25  Which is still at least 4 quarters *past* its due-date actually …,Neutral
Intel,"That's not strictly true, the (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  https://www.ultrabookreview.com/27215-asus-rog-g703gx-review/#a5  >That comes with a few drawbacks, though, and one of the most important is the noise development. Asus provides three different power modes for this laptop, which you can use to juggle between performance, thermals, and noise:  Silent – CPU limited at 45 W and 4.2x multiplier, GPU limited at 140 W, fans only ramp up to about 40-43 dB in games; Balanced – CPU limited at 90 W and 4.2x multiplier, GPU limited at 140 W; Turbo – CPU limited at 200 W and 4.7x multiplier, GPU limited at 200 W, fans ramp up to 55-56 dB in games.  It was always up to the laptop OEM to configure the power limit and there was no hard cap enforced by Intel.",Neutral
Intel,"The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, negating the entire point of them.  In addition, MTL/ARL had the LP-E cores on the SoC tile which made them functionally useless for full nT load.  LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks *can* stay entirely within the LP island, *and* they're on the same compute tile, so they're also used in full nT workloads.",Neutral
Intel,"> That's not strictly true …  Yes it actually is.   > The (stock) PL1 was always 45w but the PL2 was far higher than 65w in many intel-H laptops.  Dude, you're basically confirming exactly what I wrote, by literally picking a **i9 9980HK**-equipped notebook of 2019, which is even exactly the very infamous SKU I was talking about …  I was talking exactly on (among else) that very CPU, even Apple couldn't tame and had to undervolt (still without being able to prevent the later sh!t-storm) until eventually abandoning Intel altogether.  ---- All I'm saying is, all the years prior with only quad-core, Intel \*never\* (nor any OEMs for that matter) went above and beyond the official TDP – The only lone exception from this, were those super-bulky Schenker Desktop-replacements.  That only suddenly changed by 2017–2019, when Intel suddenly increased core-count in mobile swiftly from Quad- to Hexa- and ultimately Octa-cores, while pushing the TDP in quick succession  into insane territory.  *That was the time, Asus took about a year to manage applying liquid-metal en masse, for Intel-notebooks!*  You are right with your sample here, yet you only confirm what I said before. Only past quad-cores it was.",Neutral
Intel,"> The idea is that because LP-E cores are off ring, you can power gate the rest of the cores when you're doing light tasks, getting much better battery life.  Do you happen to know, if that (ring-bus related) was the same principle on MTL?  > The failure of MTL/ARL's LP-E cores was that there was only 2 and they were very weak, so in practice, even a single web page would turn the ring back on and move the process back to the main cores, **negating the entire point of them**.  Yeah, talking about throwing some completely untested sh!t onto the market, irregardless of the fact, if the product makes sense or not — A true classic, I'd say. Just Intel being Intel.  Since dropping those LPE-cores on MTL, would've actually made the mask and thus needed die-space *smaller*, which in turn would've actually *increased* their yields \*and\* in return profit-margins already …  But muh, benchmark bars and ""AMD has moar corez!!"", I guess.  > LNL/PTL/NVL have much more powerful LP-E cores, 4 of them instead of 2, so common tasks can stay entirely within the LP island, and they're on the same compute tile, so they're also used in full nT workloads.  Is there any greater penalty (latency or cache-flush-wise) for moving threads off the LP-island to P-cores?  *Thanks for the insides so far though!* I can't really see through anymore to all these constant arch-changes.",Neutral
Intel,"I recently picked up an MSI Claw 7 AI+, and have been pleasantly surprised with the current state of their drivers. I would absolutely be interested in a Celestial GPU if they have anything targeting 9070~ or higher performance level. Here's hoping!",Positive
Intel,I'm glad they are doing these followup videos. Gotta give Intel credit where due - their cards are getting so much better over time.,Positive
Intel,The Lunar Lake handhelds would be really popular if they were just a little cheaper.,Neutral
Intel,They said their high idle power is an architecture issue so they can't fix that,Negative
Intel,God forbid Intel supports Day 1 GPU drivers longer than 5 years,Neutral
Intel,"true but thats not really that big of a deal, and can be fixed with some tweaks [https://www.reddit.com/r/IntelArc/comments/161w1z0/managed\_to\_bring\_idle\_power\_draw\_down\_to\_1w\_on/](https://www.reddit.com/r/IntelArc/comments/161w1z0/managed_to_bring_idle_power_draw_down_to_1w_on/)",Neutral
Intel,So just like AMD then.,Neutral
Intel,"They usually do tho? We dont know about their dGPUs yet because it hasn't been 5 years.  Their Xe Laptop gpus are 5years+ and are supported as of this month.  No idea why you're hating on intel GPUs mate, i'm used to everyone cheering the underdog not the otherway around.",Negative
Intel,"I think that guy's a misinformed user that is saying that AMD 5000/6000 series won't get driver updates anymore lol, when AMD just said that those gens aren't slated for game optimizations that use their ""latest technologies"" (FSR4 at the time of writing) anymore. Those gens still will get driver updates but AMD gave up on trying to implement FSR4 on them, like the leaked scripts earlier this year had performance reduction from 20% to up to 30% FPS to use FSR4 for 6000 series cards since they don't have the hardware and just using brute force software for FSR4.   I'd agree that my 6700 XT never getting FSR4 sucks, but the misinfo circling around is that we'd never get anymore driver updates lol.",Negative
Intel,"so the A, B, C, (etc) Series is just branding? and they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name? … but Xe3 plus will be enough to call it C series.",Neutral
Intel,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",Positive
Intel,Everything about Celestial I've heard seems to be regarding mobile. Is there actually going to be dedicated discrete graphics?,Neutral
Intel,The Intel driver downloading a shader cache for games from Intel sounds very noteworthy.   I'm really skeptical tying GPU drivers to a manufacturer's online service.,Neutral
Intel,"They don't care. These will be mid tier chips at best. The RTX IGPU ones will be their flagships. These are still damn impressive though, just have to make that clear.",Negative
Intel,"The mega brains of this community really struggle with product naming lol, its so funny to watch supposed intelligent people straight faced say they can't work out what product out 6 to buy because of their product names, most of them are in different market segments so you can't even accidentally buy the wrong one.",Negative
Intel,"They may divide ABC depending on ISA/hardware capabilities used for example. Like AMD used ""RDNA 3.5"" on mobile as full RDNA 4 would pull more power due to more silicon dedicated to AI (including FSR 4 -\_-). RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4. So Xe 3B will be better than current but Xe 3C will do better at DXR and/or AI frame gen/whatever new tech they added.",Neutral
Intel,"This will help you to understand: https://www.tomshardware.com/pc-components/gpus/intels-xe3-graphics-architecture-breaks-cover-panther-lakes-12-xe-core-igpu-promises-50-percent-better-performance-than-lunar-lake  This is not brand new architecture, it's refinement on battlemage to work more efficiently. Thinks of it as Xe2.5 but for some reason marketing is calling it Xe3.",Neutral
Intel,">they dont think they have enough performance improvement in Xe3 to warrant a jump to a new branding name?      No what he said is that the Arc B Series hasn't been around for all that long and the B580 has garnered a very positive reception so they felt in terms of branding and marketing moving in Panther Lakes Xe³ into the B series brand is a better marketing move than throwing away the B series branding and starting the C series branding now. In others words they want Panther Lake to ride on the momentum and positive vibes the B series branding has garnered.   Xe³ is a Xe³ based architecture like Xe³P is architecturally but there is more to selling a product than the engineering, keep in mind there's strong rumors that there will be a B770 launch, Intel also just released the B60 and later this year will provide a big software expansion to Bxx cards so the B series brand is still full of life. Xe³P will launch much later and at that point Intel feels there will have passed enough time to leave the B series branding behind.",Neutral
Intel,An alternative explanation could also be that Celestial dGPUs *are* launching as Xe3P and they would want the launch of C series iGPU and dGPU to be around the same time and the same architecture.,Neutral
Intel,">so the A, B, C, (etc) Series is just branding?  Wasnt it always so? Xe is the architectural names, ABC was just branding for customers.",Neutral
Intel,"No public announcements yet, but logically it would make all the sense for Intel to launch them, and do so in larger quantities so people in the market can get them at MSRP. Battlemage proved there is a lot more demand than there was supply, and it'd take too long to ramp it up once it became clear they were received well. With proof of a market test pass, they've now got a chance to do it right.  They are excellent value products that would allow Intel to get a chunk of that market, given they are available in sufficient quantities at intended price points.",Positive
Intel,Is shader caching on first use a real problem? Sounds like bored people needing something to cry about again.,Negative
Intel,>RDNA 3.5 has a lot of ISA and part of hardware changes that went into RDNA 4  Do you have a source for this? I vaguely remember 3.5 wasn't really bringing particular changes,Neutral
Intel,That's exactly what everyone's new GPU generation is.  An entirely new architecture is extremely rare for any company.,Neutral
Intel,That's not what Peterson was talking about context wise when he addressed this in the video.,Negative
Intel,"They would be extremely stupid not to continue to release discrete graphics cards, even if they decide to release 2-3 gaming SKUs per new generation; entry/budget, mid-range, high-end, etc.  - they win. until they keep iterating up, maturing driver, increasing install user base, improving software, until the point they can target higher segments (enthusiast tier)  Gaming GPUs remain the flagship component that pushes the limits of silicon design, features like ray tracing, AI upscaling, and real time path racing all debut in gaming before trickling into pro and AI workloads. Every research report shows that demand for high-end GPUs is still strong. Steam HW survey shows the most popular GPUs are still mid-range gaming cards, showing that discrete GPUs remain central to the PC ecosystem.  And yes, gaming GPUs are now marketed as AI accelerators too, Nvidia and AMDs GPUs both highlight AI compute as much as gaming.  Furthermore, if you look at the Steam HW survey, nobody gives a CRAP about 4k/8k gaming, meanwhile 1440p is growing! Nvidia, the leader in GPUs has been marketing for 4k gaming since 2013 with the 780 Ti/GTX 980 era, then 2016 with 1080/1080 Ti, 2018 with 2080/2080 Ti, 2020 with 3080/3090, and marketing for 8k gaming since 2020 with 3090, 2022 with 4090, and this year with 5090. So from 2013 to 2025 Nvidia has been marketing their GPUs as ""4k ready"" and ""8k ready"" but reality is that majority of gamers don't blame games at those resolutions. 4k adoption is only ~2-3% of users, and 8k is virtually nonexistent. So all Intel has to do is really focus on 1080p Ultra and 1440p High/Ultra gaming, above entry‑level (B570, A580) and below enthusiast (future B770/B780?) and keep optimizing and improving ray tracing and XeSS.",Neutral
Intel,[AMD RDNA 3.5’s LLVM Changes - by Chester Lam](https://chipsandcheese.com/p/amd-rdna-3-5s-llvm-changes),Neutral
Intel,"There are more to that, if you look at RDNA 4 and blackwell for example they had changes to the SMs, enc/dec, etc.   Here it using basically the same cores, but changes are made to better utilize those cores. I think this sentence summarize it the best:  ""Intel classifies Xe3 GPUs as part of the Battlemage family because the capabilities the chip presents to software are similar to those of existing Xe2 products"".  Now I'm not saying it's a bad thing, Intel has a lot of inefficiencies with Battlemage, a lot of time you expect it perform better judging by the amount of hardware. By going this route the basically giving pretty good improvement only year after initial release with basically almost the same drivers. It's just not a new generation but something in the middle between a refresh and a new generation.",Neutral
Intel,"Not rare, but more like it only happens when there's a paradigm shift in graphics that necessitates it.   GCN -> RDNA or Pascal -> Ampere are the most recent ones.",Neutral
Intel,"its not extremely rare. It happens once every 7-10 years or so when they redesign it for significant changes. but yeah, most generations will be refinement.",Neutral
Intel,"In the end it is all matter of marketing, filter it for a moment and understand what it's. As he said ""leverage all the good work from battlemage"". As I read it and looking the architecture it's battlemage with some enhancement that will come to future product celestial. Something like RDNA 3.5. I think they messed up the naming - they want to present it as something new and different (it's in some aspects) but it's not the real next gen in their roadmap.",Neutral
Intel,By discrete iam assuming you mean client graphics SoCs? The margins are not enough on those - you make much more money on Pro cards/ HPC.,Neutral
Intel,"No this is not the case. The decision is purely branding. Xe3 and Xe3P are part of the same generation architecture wise unlike Xe2 and Battlemage, he addressed this.",Neutral
Intel,It's this. IGPU's will be for gamers and DGPU for data center. They basically both have said this :) People just don't notice anything. What do folks think RTX IGPU's on intel chips are gonna be for. LIKE LOL!,Neutral
Intel,"Then why he said ""when we move to our next architecture...""? Meaning they are not the same (Xe3 and Xe3P), also look at the slide they're grouping Xe2 with Xe3 under battlemage.  What you say contradicts their statements, and the article. The decision is of course marketing but nowhere they say Xe3 and Xe3P share the same architecture. This is why I said before it should've been called Xe2.5 or something of that sort like with RDNA 3.5.",Neutral
Intel,Next architecture is just rearrangement of existing cores into more effective set up. No contradictions.,Neutral
Intel,"When we will see deep dive into X3P celestial we will see what have changed. Until then it's just speculation of the differences between Xe3 and Xe3P. My bet there will be more changes than mere ""rearrangement of existing cores"".",Neutral
Intel,"Love a good TAP interview, hopefully he'll do some podcasts too once the embargo lifts.",Positive
Intel,"I like how TAP said ""expect even bigger investments in graphics in the future"". I hope this means what I think it means, because Intel \*must\* continue heavy investment in their own graphics. Accelerated compute is the future.",Negative
Intel,MLID must have an aneurysm seeing the guy still employed at Intel,Negative
Intel,"Tom's an absolute gem. It's so frustrating to see the most passionate and effective communicators locked up behind embargoes and NDAs, so fingers crossed that this marks the next wave of tech exploration and deep dives. Always nice to see him pop up in a random GN or PC World video.  Credit where it's due, Lex (Alejandro) has also always been attentive and forthcoming in his Reddit interactions, even factoring in the usual embargo/NDA constraints. Hopefully he'll dust off his u/LexHoyos42 account; I'd love to see some mini-AMAs as Panther Lake trickles out.",Positive
Intel,Igpus not discrete gpus,Neutral
Intel,"I would not attempt to read much into claims like that. This is marketing, not an investment roadmap. I'm sure if you pressed them on it, the lawyers would have some words.   Remember, they've yet to even *talk* about dGPUs past BMG. And the big-iGPU NVL is reportedly cancelled (likely in favor of the Nvidia partnership).",Negative
Intel,Hahaha I can hear Tom saying “fucking Tom Peterson has been *lying* to you” as you say that,Negative
Intel,Problem is we don't know the real roadmap is for Intel. I really want them to put out a roadmap like they used to. But things in flux probably :s.,Negative
Intel,I think only way for intel to enter consumer GPU space is to start via iGPU at this point.  They just need to make bigger faster iGPU to make Nvidia RTX xx50/RTX xx60 redundant.,Neutral
Intel,I don't think partnership Nvidia iGPUs will be ready until TTL/HML circa 2029/2030.,Neutral
Intel,"Agreed, but then similarly I would also not attempt to conjecture on their roadmap outside of an investment roadmap... lack of a public roadmap update on graphics in a while is certainly saying something by not saying anything, but it's also not saying cancellations, etc...",Negative
Intel,There are just too many Toms in tech media. Tom of Moore's Law is Dead needs to thin out the competition.,Negative
Intel,I don't think even Intel knows what the real roadmap is anymore. They're so far behind on the previous ones that it's not surprising that they're not releasing new ones.,Negative
Intel,We are not competing with higher end GPUs. It seems they want to capture the budget segment and then move up the ladder.  But I don't like that iGPUs has more priority than dGPUs. Hope celestial comes into life in future,Negative
Intel,"Probably, but no sense offering an in-house big iGPU for just one gen. It needs to be a long-term commitment.",Neutral
Intel,"I mean, the layoffs tell a story. The dGPU group, and client graphics SoC in general, was gutted even before Lip Bu came on. To say it's likely a skeleton crew is probably doing it a favor.",Negative
Intel,You stole what I was going to say... take my upvote.,Neutral
Intel,"thats how they are doing it.  iGPU is the one that killed Nvidia MX, or GT xx30 series. So their next target is the RTX xx50.",Negative
Intel,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  One thing seems clear, this solution will be cheaper than standard AI data center products.  Hopefully, Intel can get and maintain footholds in medium-sized AI research market",Neutral
Intel,No way that power usage is at idle. Even if there was only one performance state like Tesla GPUs they wouldn't consume that much power.,Negative
Intel,that's insane vram density,Neutral
Intel,"This thing only has a 1Gb Ethernet port? I don't know much about the use case for this thing, but that seems surprisingly low. Simply from the use case of uploading training data to this I feel like something faster is necessary.",Negative
Intel,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Is there a fork of chrome that runs on gpus,Neutral
Intel,"Now do it with pro dual version of b770s, 64gb each. Could be a far more economical inference solution than what AMD is providing",Neutral
Intel,but is that faster than a single 5090?,Positive
Intel,Is this enough VRAM for modern gaming?,Neutral
Intel,Nvidia: ill commit s------e,Neutral
Intel,"> For AI training, there is probably no limit (to enough memory), although it's unclear whether splitting the memory pool into 24 or 48 GB banks is better than using 100 GB or even 200 GB on a single card.  If they rely on PCI-E for interconnect, bandwidth will be anemic between cards for training purposes. Might be ok for inferencing, but for training, you need the bandwidth.  That's not unclear at all.  This thing is probably not useful for anything, but serving many users with small models for inferencing.",Neutral
Intel,"I doubt we'll see any UALink this year, but perhaps  on the 160GB  Crescent Island card next year.  Intel hasn't announced anything, but it seems obvious.",Neutral
Intel,"You think Intel is going to maintain these footholds any longer than they would need to?  This push of their dedicated GPUs into the professional space, is just a mere attempt to get rid of the unsold inventory of ARC-GPUs at the highest possible price-tag right from the beginning – Dumping those into the enterprise- and datacenter-space, in noble hope that someone clueless might bite upon the Intel-brand, and then dip into it at high costs *at non-existing support software-support* Intel pretends to deliver some time in the future.  Since as soon as the bulk of it was sold, Intel will cut the whole division and call it a day, leave anyone hanging dry.  That's what's planned with the whole professional-line of them – Buying this, is a lost cause as a business.",Negative
Intel,"I think they've just taken the 400W per card TDP and multiplied it by 16 to get the 6400W figure, so I think they're trying to say minimum power draw at full load, not at idle. ""Minimum power draw"" is definitely really odd phrasing for that number. Surely you'd think they'd say ""Total GPU TDP"" or something less confusing",Neutral
Intel,I don't think servers are supposed to stay idle for long.,Negative
Intel,"> the 768 GB model uses five 2,700 W units (totaling 10,800 W), while the 384 GB version includes four 2,400 W units (7,200 W)    And an insane power consumption.",Neutral
Intel,You basically always put in dedicated NICs in those kind of servers. The onboard ethernet is rarely used.,Neutral
Intel,Could that make it very cost effective for any particular use cases?,Neutral
Intel,"Even the fancy NVIDIA interconnects are a bottleneck for training performance, as are the memory bandwidth and even L2 bandwidth. One architects the algorithm around that. A tighter bottleneck is obviously not ideal, but it might still make sense depending on the price of the hardware.  As long as a copy of the full model and optimization state fits on the 24GB bank the PCI-E interconnect isn't really too limiting for training. Small models, like a 1B parameters voice model, will naturally fit. The larger the model you want to train, the more complex the architecture might be, but there are ways to train even models over 100B parameters on such hardware, like routing distributed fine grained experts. Not that I think it's a good idea at this point.",Neutral
Intel,It's VideoCardZ so they probably got information from a tweet or Reddit post and copy/pasted without doing any thinking whatsoever.,Neutral
Intel,"I assumed that would be the case, I just didn't see any room for expansion. It's pretty crowded with 16 full sized cards.",Neutral
Intel,"I don't really see any, unless you can find a workload that will fit the card and multiply that workload for 32 users, but as each chip is performance wise less than a 3090, it has to be a fairly light workload.",Neutral
Intel,"1B might work for a voice model, but useful LLM models ARE 100GB+. Smaller models are fun little toys.",Positive
Intel,At least its a human hallucination and not AI hallucination.,Neutral
Intel,Can also be bad translation.,Negative
Intel,"according to the datasheet theres two free pcie 4.0 x16 slots on one version and two pcie 5.0 x8 slots on the other one. Both say ""support high speed NIC"" so I assume there is at least enough room for that",Neutral
Intel,"I wonder if these might make good controllers for ai powered robots. Something highly specialised, like a fruit sorter or something. I don’t know. I’m just hoping Intel can find a buyer because it’s good for the market to have more competition",Neutral
Intel,"It's a thing where the next generation of this card would be a viable competitor for the generative AI crowd, but not this one, where it can't compete with a 5 year old 3090.  For robotics, you just want inference. There are much better options focused on low power, small form factor NPUs custom made exactly for that, and they can be paired with small SBCs like a Raspberry Pi.",Neutral
Intel,No word on OMM so I guess that's reserved for Xe3-P. Wonder what other changes that architecture XE3-P will have.  The new Asynchronous RT Unit functionality in Xe3 is very cool and now Intel also has Dynamic VGPR. Neat.,Neutral
Intel,"It's interesting they lump it in with B-series, given the architecture is pretty distinctly different from Xe2 used in Battlemage. Note that BMG/Celestial names have been explicitly used for dGPUs, however. They're not synonymous with Xe2/Xe3.",Neutral
Intel,"It would be nice if Intel released a small, budget friendly, passive cooled, PCIe powered dGPU with multiple QuickSync units, made specifically for AV1 encode and H.265 4:2:2 chroma with 10-bit depth.   It's unfortunate that one has to cough up $2,000+ (at least around here) for an RTX5090 to have three NVENC units at their disposal, and the 5090 still can't compete with a passive-cooled Apple Macbook that draws a couple of watts at peak.   You know things are crazy when Apple is the only company that caters to your specific needs!",Neutral
Intel,"Hello Dangerman1337! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Very bleak chance that Intel will continue with discrete gpu line up. Pretty shit decision tho,Negative
Intel,"So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In&nbsp;card under this precisely defined term.  Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – *In favour of Nvidia-GPUs?*",Neutral
Intel,Xe3P needs that feature parity with RDNA 5 and RTX 60 overall since its a 2027 product.,Neutral
Intel,"hopefully that means NVL-AX is not getting axed. at least, the 24 core die. heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch. super weird, not believing too much into it. curious about Druid, people seem optimistic, idk why lol.",Neutral
Intel,yeah super weird -- but that does bode well for the next dgpu being based on Xe3,Neutral
Intel,"Arc Pro B50 was pretty interesting to me, it was a modern 75W card that isn’t gimped.",Positive
Intel,With a board with lots of PCIe x16 slots you can stack A310's or A380s and get along fine. I do 3 transcodes of av1 great on a single one right now.,Positive
Intel,The reason they'll never make this is the same reason they don't put SR-IOV on consumer cards.,Negative
Intel,Can you do this work on a $600 Mac mini? Or do you need something more expensive.,Neutral
Intel,"Yeah prob, but by how much. Looking forward to the post CES reviews and Xe3 better be a major leap over Xe2.",Positive
Intel,They will if they have xe cores that gen. They'll only fully kill it if they abandon that core development entirely to exclusively use Nvidia graphics tiles,Neutral
Intel,">Their non-show is basically code for: *“We're canceling everything dedicated, and our eGPUs we consider now* ***d****GPUs.”*  What in the world are you talking about. This roadmap says nothing of the sort.",Negative
Intel,"No, they are saying the GPU in Panther Lake is Xe3, but instead of classifying it as a new-gen architecture, they are saying it's just an extension of Xe2, so they've put it under the B series product family.",Neutral
Intel,100% but I fear this is another botched gen so it will at best be on par with current gen offerings.,Negative
Intel,"Intel buying chips from Nvidia won't be as cost efficient as them developing their own chips. Intel can't abandon GPU development in the short term or it'll be left behind in the long term (Nvidia can easily sell their Intel shares and pivot away from Intel leaving Intel with nothing).    If Intel will stick to making dGPUs and expand to competing in gaming with said GPUs they benefit greatly from the experience and talent that dGPU development yields, this makes dGPU development a no brainer. If AMD can afford to make dGPUs Intel definitely can as well.",Neutral
Intel,"> heard rumours that they resurrected that one Celestial config for a late 26 - early 27 launch  Would be very interesting if so, though I'd be surprised if projects get resurrected in this spending climate. Competitiveness might also be tight in '27.",Neutral
Intel,Celestial was based on Xe3p.,Neutral
Intel,"Then that can't just be a minor tweak, but perhaps it's just market segmentation.",Neutral
Intel,I don't think Mac minis support hardware AV1 _encoding_. Apple only recently supported hardware AV1 _decoding_ so their devices aren't the best suited to AV1 media,Negative
Intel,> They will if they have xe cores that gen  Why do you believe so? They made iGPUs without dGPUs before.,Neutral
Intel,"They won't Nvidia tiles are for specific market segments, they aren't a one size fits all solution.",Neutral
Intel,That's not what my colleague's say,Neutral
Intel,It wouldn't be a Helpdesk_Guy comment if it didn't include insane made up bullshit to disparage Intel.,Negative
Intel,"Exactly. *Good Lord are y'all shortsighted!* Are you falling for their shenanigans and typical tricks again?  Crucial is, what Intel does NOT mention – That is anything future dedicated graphics, as in *Add-In&nbsp;cards*.  They're purposefully declare PTL as de-facto dedicated GPU, yet any thing of future **d**GPUs is ABSENT.",Negative
Intel,"Congrats, you looked at the pictures. We all did. Now look at it *again* …  Now explain to me, why ARC alchemist is separated (since it's a line of dedicated graphics-cards, duh?!), while Battlemage gets PTL's X^e Graphics iGPU-tiles put alongside, despite such tiles classically does NOT count as a graphics-card but rather a iGPU (no matter how large).  The actual answer is, that this on the slide was surely NOT done slide by accident, but it figuratively puts PTL's X^e Graphics tiles of PTL's iGPU *on par* with their dedicated Battlemage-cards, which (at least in Intel's eyes) shall from now on signal a ""continuation"" of  Battlemage itself. Thus, there won't be any further dedicated graphics-cards like Celestial as classical Add-In cards, which still is way overdue anyway …  Since if not so, then WHY are PTL's iGPU-tiles labeled as ""Intel ARC B-Series""?! *You see the virtue signalling?*    You're also aware, that Intel recently dropped the Intel Iris&nbsp;Graphics theme and now calls all iGPUs ""ARC""?",Neutral
Intel,">Intel can't abandon GPU development in the short term  Well Intel now uses the same architecture across both their iGPU and dGPU, so even if their dGPU products get killed, iGPU demand(indirectly, through Core Ultra processor demand) can fund continued GPU architecture dev. Intel still has the majority market share in the iGPU market, so they have the demand to justify continued R&D into GPU dev for iGPU alone.",Neutral
Intel,Xe3p is a significant architectural advancement says Tom Petersen.,Neutral
Intel,"Care to elaborate where I was blatantly wrong and it didn't actually turned out mostly or even exactly as I predicted prior when popping the prospect of Xyz, often months to years in advance?  A single example would be sufficient here, mind you.",Negative
Intel,"They did not declare PTL as a de-facto dedicated GPU. By definition it's *not* a dedicated GPU and they never said it is.  They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs. You're trying to fill in missing info with a nonsense interpretation.   What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".",Negative
Intel,"So much buzzwords, yet it sounds like a stroke.  You need help.",Negative
Intel,"Alchemist had both desktop discrete and mobile discrete. Intel's roadmap isn't a 'graphics card only' chart, it's an IP roadmap. That's why Panther Lake's Xe3 iGPU shows up under Arc B-series, they are saying Xe3 is an extension of Xe2, where as Xe3P (Celestial) is the next Arc family.",Neutral
Intel,"> A single example would be sufficient here, mind you.  Sure, here you go:  > So in other words, Panther Lake now Intel classifies as a dGPU (or at least throws it deliberately into the same bin of dedicated Battlemage), despite under 'dGPU' literally everyone ELSE understands a dedicated graphics-card, as in Add-In card under this precisely defined term.  >Their non-show is basically code for: “We're canceling everything dedicated, and our eGPUs we consider now dGPUs.”  >So ARC as a line of dedicated graphics-cards, can now considered to be basically buried – In favour of Nvidia-GPUs?",Neutral
Intel,"> They did not declare PTL as a de-facto dedicated GPU.  Except that's exactly what they did – look at the damn pictures my friend, Intel purposefully group it into the same category as Battlemage's dedicated GPUs and Add-In graphics-cards and thus, iGPU-tiles as on par as dGPU cards.  > By definition [e.g. Panther Lake's iGPU-tile] it's not a dedicated GPU and they never said it is.  \*Sigh I *know* this, well all do! Yet Intel wants to pretend, that PTL's on-die eGPU X^e Graphics tile is now basically declared, what *Intel* now considers a **d**GPU. The implication is strikingly obvious here.  > They just didn't specify anything about a future, unnamed (if it exists) dGPU at an announcement event for their new mobile CPUs with big iGPUs.  Ex-act-ly! You really don't get it, don't you? They deliberately leave everything what *classically* would count as a dGPU open and without ANY path forward, meanwhile their X^e 3-tiles are now considered dGPUs by Intel.  > You're trying to fill in missing info with a nonsense interpretation.  No, I don't. I'm just one of those being able to decypher Intel's typically backhanded tricks and try to explain that they're virtue signaling the worst: ARC as a line of dedicated graphics-cards is dead and won't get a follow-up, instead, Intel now considers it replaced by iGPU X^e Graphics-tiles (like the one in Panther Lake).  > What typical tricks? ""Here's our new mobile CPUs. We have a new GPU *architecture that we're gonna be using in the big iGPU SKUs"".  Their trickery here, that they basically announce the discontinuation of ARC graphics-cards through the back-door.",Negative
Intel,I wonder why they're using such naming. If Xe3 is just Xe2 enhanced then why not call it Xe2.5 or Xe2+. And if Xe3P is a new architecture why not call it the real Xe3?,Neutral
Intel,"Just wait and see. I sadly have a horrific on point streak to remain in the right with anything Intel, even if I call things months to years in advance.",Negative
Intel,"The implication here is incredibly obvious:  ""Intel Arc B-Series consistes of Battlemage discrete graphics *and* PTL iGPUs"".  That's the implication here. It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL",Neutral
Intel,Because it's most likely still the same foundation. Xe3 just have additional optimizations and HW blocks along the lines of OMM (confirmed) and whatever new tech Intel decides is worth pushing for higher end parts.    But perhaps it's more of a RDNA 3.5 situation. Some early Druid tech backported to XE3P.    Yeah I have zero clue just guessing xD,Neutral
Intel,"> It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL  Of course not, since it would instantly tank their reputation and will kill all further ARC-sales?!  Do you actually think, that Intel is that stupid to plain announce the death of ARC-series graphics-cards? Do you think they'd even sell a hundreds cards after such a announcement? Who would by a dead-end line of graphics-cards, which won't even get any further driver-support and still have disastrous drivers like Intel's ARC? NO-ONE!  Of course they virtue signal it through the backdoor here, to preserve the sell-off the of the rest of pile of shame of their B-series graphics-cards as long as possible (leaving users with none support afterwards), and *only then* pull the plug afterwards when they went through their inventory …  Since when are you following Intel? Intel has always done so, and only announced a sudden discontinuation once the inventory was sold (leaving customers in the open with no further support overnight), despite virtue signalling a continuation all the time (to preserve a proper sell-off).  ---- Wasn't this exactly the same story with **Optane** back then? Each and every question upon a future support was answered in the affirmative, only to pull the plug overnight again as usual …  I'm not another piracy of cons here, it's Intel pulling the same Optane-stunt with ARC – Happily selling off their cards, knowing darn well, that they will pull the plug, yet refuse to actually play with open cards again.",Negative
Intel,"> > It explicitly calls BMG dGPU cards ""discrete"" and does not do so for PTL >  > Of course not  Great, we're all agreed that you're making shit up when you said  > Panther Lake now Intel classifies as a dGPU",Negative
Intel,Just letting it be known that the [original source](https://www.reddit.com/r/IntelArc/comments/1nomuhk/driver_built_xess_frame_generation_might_be_on/) is over at r/IntelArc,Neutral
Intel,"Intel ""ExtraSS"" frame extrapolation tech has been in development for quite a while. I really hope it really does come with B770. Will be exciting to see how the implementation differs from MFG.",Positive
Intel,Could be? Wasn’t one of the lead developers for Asahi Linux’s graphics stack hired by them for this specific purpose?,Neutral
Intel,"Yet more evidence Intel is likely not scrapping arc, as if we couldn't guess they know the trajectory of past nVidia semicustom affairs.",Neutral
Intel,"Nice to see. Still pretty happy with my Arc card, good enough for game and resolutions I’m playing and the AV1 hardware encoder is just excellent.",Positive
Intel,Wonder if well be getting ray reconstruction andmaybe even a transformer model soon.,Neutral
Intel,we are witnessing the downfall of pc gaming in real time,Negative
Intel,Why should they? They are going to buy NVidia GPUs for everything now.,Neutral
Intel,"Hello brand_momentum! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,This could be awesome more completion The better,Positive
Intel,Doubt anyone would use it for practical application. FSR Frame Gen is awful to use.,Negative
Intel,"Multi-Post News Generation, with three articles interpolated per source.",Neutral
Intel,I'm excited for Intels new GPU,Neutral
Intel,"Microsoft has been way too slow in the DX12U era, and Kronos is even slower. Microsoft's new APi that integrates AI upscalers isn't even out yet and it doesn't even cover Frame Gen or AI denoising. By the time MS moves to integrate those they'll be behind on something else.",Negative
Intel,"They aren't, though. FSR Frame Gen works on Nvidia Hardware",Neutral
Intel,Honestly we are gaining more from proprietary features than we lose from them being locked behind certain brands. Currently them being proprietary gives Nvidia an incentive to push the envelope and the head start they have isn't that big so they're incentivized to keep making new features   If everyone was required to share their new features then the incentive for innovation disappears,Neutral
Intel,Competition *,Neutral
Intel,Obligatory article quoting reddit post quoting another article quoting original reddit post.,Neutral
Intel,"Fake frames, fake articles! /s",Negative
Intel,Sometimes I prefer vendor solutions today than waiting for microsoft for years like Direct Storage,Neutral
Intel,"I mean, surely it depends on when they have to share it. If they can release with the feature exclusively first, but then have to share at or after release, then it can still help the current generation of their cards until it's time for the new gen.",Neutral
Intel,"DirectStorage 1.1 (the actually usable version unlike 1.0) was supported by Windows 9 months after PS5 release. They werent that far behind on that, noone just was interested.",Neutral
Intel,"patents expire after 15 years, they will have to share it then.",Neutral
Intel,Nvidia controls 90% of the discrete GPU market on PC.   Anything proprietary that they make essentially becomes the standard going forward.,Neutral
Intel,"Huh?  1. RT in games is now done through DirectX APIs, which are vendor agnostic.   2. For GPU features, NVIDIA usually introduces them through a DirectX standard, they are just usually the first vendors to support it until others catch up.   3. RT is limited because current gen consoles are not good at RT. AMD took multiple years to adopt RT or AI acceleration cores.",Neutral
Intel,I actually have not seen an open source or even cross vendor solution with any reasonably quick uptake. Its a theoretical myth at the moment,Negative
Intel,the issue with RT isnt standard. Its that consoles/amdGPUs simply didnt support it for away too long.,Neutral
Intel,"...It's down to 15 now? I thought it was 20.   Although. Software is more copyright than patent anyway, and copyright is, like, 70 years after the author dies, or 95 after publication for corporate works.",Neutral
Intel,"Depends on country/industry. Copyright also depends on country, but for most civilized word its authors death + 95 years (thanks disney). Copyright however has many exceptions to it, well, at least in the west. In Japan it does not and technically you can get sued for saying Zelda is fun because you broke copyright by saying Zelda without permission. Its why Nintendo is able to get switch reviews removed from youtube.",Neutral
Intel,The DirectX RT spec and SDK came out in 2018. The blame lies entirely on AMD. Imagine Nvidia waiting for AMD to agree,Neutral
Intel,I would imagine intel has to at least get celestial out the door before cutting off their GPU line. it's been so long in development that it would be more of a waste of money to cancel it then finish it and see how it sells. at least getting it to the public means some kind of ROI and a solid position in the lower end of the GPU market that AMD and Nvidia are abandoning to the used market.,Negative
Intel,"Okay then, what's the Xe GPU roadmap looking like then?",Neutral
Intel,"Hopefully Intel still thinks it needs discrete GPUs to compete in DataCenters and such.  Also GPUs for the ""edge""",Neutral
Intel,I can't claim to know the future or fate of Arc but I think if Nvidia wanted to kill it they could just price it out of the market instead of buying a stake in a company with a broadly uncertain future,Negative
Intel,They barely lived on before the deal.,Neutral
Intel,"It'll live on our hearts, yes.",Positive
Intel,Lol if you believe that I have a bridge to sell you in Brooklyn,Neutral
Intel,"You have to look at this from both sides.  This would give NVidia the ability to offer OEMs a tightly integrated on SOC gfx solution vs a separate discrete gfx package as it is now. This would lower costs for NVidia's customers as well as improve performance and Intel's cpu's get to ride that train.   Intel has a long track record of using integrated gfx so it's certainly right up their alley to take this on.  Could Xe gfx coexist with NVidia? Yep, I think so.  Xe could be the budget igfx option and iNVidia is the premium integrated solution. That's basically how it is now with Intel Arc/Battlemage & GTX.   On the DC side, Intel has absolutely no gfx solution at this time and won't have anything for at least two years.  Intel already sells NVidia when a gfx solutions are needed so tighter integrated products will benefit both sides in that DC space as well.",Positive
Intel,"What's weirdest to me about this deal is. They specifically mention NVIDIA chiplets being packaged with Intel CPUs.   Now, with Meteor Lake and Arrow Lake, the CPU and the GPU are different tiles packaged together. But for Lunar Lake, the GPU was included in the same tile as the CPU, called the compute tile, because that was more efficient.   Supposedly, Panther Lake is using a different tile setup, with the GPU being separate again, or with the memory controller being on the tile with the CPU, but.   Either way, it feels to me like relying on separate NVIDIA chiplets for the integrated graphics is a bit of a limitation to how the overall architecture of the SoC can be designed.   My understanding is that, if another company buys Intel, that messes up the cross licensing agreement, and Intel might lose x86-64. So, even if NVIDIA keeps buying Intel stock, the most they can get is 49%.   And NVIDIA is too big now for Intel to buy, so the other way around wouldn't work. And even if NVIDIA wants to keep Intel's fabs open, I doubt they want to run them either.",Neutral
Intel,Wonder if this investment is purely to keep intel going to avoid a monopoly issue in a few years.,Neutral
Intel,AMD is not going to release a next gen gaming gpu it just leaked and that’s what I’m worried about intel arc are ramping up.,Negative
Intel,"There's also a better then not chance this alliance would have gone the same way as most other nVidia semicustom things by the time Druid tapes out anyhow, and the leadership at Chipzilla likely know this.",Neutral
Intel,"While that might be true in the long term, it's not necessarily true for whatever quarter the execs are caring about at any particular moment.",Neutral
Intel,"Probably a chronologically changed version of what is known now.  No one with any savvy expects this alliance to last with nVidia's track record, so carrying it on makes sense in both being ready for the inevitable burnt bridge and possibly pushing it back.",Neutral
Intel,It's looking for 6 years of Xe3 in integrated graphics I'd guess.,Neutral
Intel,"Always selling out actually, they just can't produce that much",Neutral
Intel,And I have another if you think nVidia is capable of keeping this deal running for that long...,Neutral
Intel,Intel will hopefully split their fab business from the rest of the company either way.,Neutral
Intel,"Im not sure what you are implying, but nvidia partnerships have historically not broken from their side as far as I know.   Apple gave them the boot, not the other way around.   AMD are the ones who rejected the Nvidia-AMD merger back in 2005.   Tesla kicked nvidia out as well, because they did not want to keep paying them to develop the tech.   If you are referring to AIB Partners, then that can hardly be called an actual partnership. That's more of a contracted labor and distribution agreement. Even then, XFX only got blacklisted because they were a premier partner who were seen as jumping ship when the going got a little rough with nvidia stalling between 2009-2010.",Neutral
Intel,> they just can't produce that much  because they are losing money on them,Negative
Intel,Trade bridges.,Neutral
Intel,Intel's arc is dead with or without nvidia deal.,Neutral
Intel,"Microsoft went with Nvidia for Xbox and it ended in a lawsuit.   Sony went with Nvidia once, and never again.  Apple ceased to bundle Nvidia GPUs because Nvidia sold them (and HP and Dell) huge quantities of faulty 8000M GPUs.",Negative
Intel,There is a reason why every nvidia partnership ends up breaking apart pretty quickly...,Negative
Intel,"Nvidia seems to make working with Nvidia too difficult for their partners to continue. It's the business version of someone emotionally bailing, and abusing you until you leave them.  They did that with smartphones, consoles, Linux, Apple, and likely countless other projects I didn't follow in as much detail.",Negative
Intel,"That really is up for debate. That they are losing money on them when you account for RNE and RnD amortized by unit, sure. That I have no doubt about.  But that is not the same thing as losing money on each card sold based solely on the bill of materials and distribution cost etc. Which is what actually matters when looking at if a product is ""sold at a loss"" or not.",Negative
Intel,"Hardly. Even after the bubble, Intel needs in house GPU, for both APUs and data center... which means they'll make a gaming version if the work is already mostly there.",Neutral
Intel,"In the case of Intel and Nvidia chipsets, it was Intel that decidedly ended the relationship, paying Nvidia off to exit the chipset business so that they could make integrated graphics.",Neutral
Intel,"Yeah, if nVidia has an Achilles' heel, it's their company culture's near total inability to do semicustom without fuckery, and it's the reason AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops.",Negative
Intel,Well yes. But you do need to amortize the tape out and dev. costs over the production life cycle of the product. And with low volumes I'm not even sure AMD is accomplishing that. AMD is saved due to the console business funding dGPU tech development.,Neutral
Intel,"They've already cancelled every data centre GPU after Ponte Vecchio (which was forced out by DOE contract) despite reassuring us Rialto Bridge and Falcon Shore were on track as little as months before the chop, and their next is apparently Jagure Shore in 2028.   I have 0 hope that will actually happen.",Neutral
Intel,">AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops  And how much money is there in your ""everything else outside of ..."" vs Nvidia's entirety of consumer prebuilt desktop/laptop?",Neutral
Intel,">Well yes. But you do need to amortize the tape out and dev.   If they are selling above the manufacturing and distribution cost, they are amortizing those costs. Might not be much, but it beats not selling them.  They might still be ""losing"" money on them when all costs are included. But it would still make sense to make more of them and sell them, since each one sold still plugs that black hole of externalized costs. And would not be the reason for lack of availability.",Neutral
Intel,"*looks at effectively every current gaming system that isn't a PC, Intel claw, a phone, or the Switch*  A fair bit, and frankly, even if it's not that profitable, it defines more of the dynamics of this sector then you'd think if you're a PC type.",Neutral
Intel,"Not only is it not profitable for the chipmaker, the revenue isn't even anything to brag about looking at AMD's gaming section of their financial statements. 1.1 billion for Q2 2025, and that includes whatever miniscule sales there were for Radeon dGPUs.",Negative
Intel,This is the card Intel should have released instead of the gaming version(s). Took them 2 years,Negative
Intel,Far more than I expected them to come out at. Damn.,Negative
Intel,"Not worth it at $599, slightly worth it at $499  This has 1/2 the memory bandwidth of a 3090 and 70% the compute performance.",Negative
Intel,I’m getting one when it releases in Australia,Neutral
Intel,Thats great and all but when will there be stock? (Canada),Positive
Intel,Pytorch libs are getting better for intel and amd support but there are still a lot of 3090 cards on the used market and those don’t need hoops to jump through to get compute working.  If I was handed a budget this low by a boss I’d just ask them to buy a lot of 3090 cards.,Neutral
Intel,I'm disappointed.  My order was canceled,Neutral
Intel,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"The gaming cards require significantly less robust driver and application support. Releasing them first makes sense when they still needed to work on said driver and application support, the timeline likely wouldn't have meaningfully changed.",Neutral
Intel,b580 should've allowed SR-IOV upon release. That's all the card really needs to sell.,Neutral
Intel,"Even at $599 it's significantly cheaper than a used 3090, comes with a full warranty, and can be bulk purchased new.  If you just need the VRAM, which is what this is targetting, I think it can definitely make sense.",Positive
Intel,"This is worth it at any price point.  I am getting tired of gamers not understanding that SR-IOV support and 200W are very important to some people. If there were a 24GB AMD card out there for 200W and $600 I would buy that too regardless of SR-IOV support.  Neither Nvidia nor AMD have a card at this price point with this feature set. You are looking at $600 for 24GB of VRAM and SR-IOV support. AMD's SR-IOV offerings are in the ""thousands of dollars"" and Nvidia's cards in this segment do not meet the 200W and 24GB of VRAM targets. The closest thing is the P40 and it's a decade old, has way higher power consumption and is about to be end of lifed.  Please tell me the card that exists in the same segment as the B60. I will wait.",Negative
Intel,"This is $150 cheaper than NVIDIA's competing A2000, which has less VRAM.  3090s do not get certified drivers, which are a business requirement for certain workloads.",Neutral
Intel,The 3090 does not have ECC on it's VRAM nor certified drivers,Neutral
Intel,Totally not worth it.,Negative
Intel,it's worth it for enthusiast collectors who want to own a rare Arc GPU. Especially if Intel abandons dGPUs. I'm sure Raja Koduri will buy it.,Positive
Intel,"I'm more looking forward to the B50, but obviously local pricing is everything.",Positive
Intel,"Yeah, the website now says: ""This GPU is only available as part of a whole system. Contact us for a system quote.""",Neutral
Intel,"It has SR-IOV, certified drivers and other professional features...",Neutral
Intel,"Graphics cards are in fact seperated into two broad categories, consumer and professional, used by all the main manufacturers. They have completely different approaches to their design, different use cases, different drivers, different memory etc.",Neutral
Intel,">The gaming cards require significantly less robust driver and application support  Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  It was like that since A770 release. When some games straight up don't work or have unplayable bugs, you are literally paying for own suffering.  That said, B50/B60 are good, though i wish B50 worked with ""Project Battlematrix"" that is Intels NVLink. B60 was also expected to be for 500$, not 600$, but it is what it is.",Negative
Intel,Both nvidia and amd only take a few months between releasing gaming and pro lines of new gpus. Intel managed to do the same with their first gen (only a few months). How come the second gen takes 1 year!?,Neutral
Intel,"...Do integrated graphics support SR-IOV? Do any of Intel's iGPUs support the ""pro"" version of their drivers,   Like, obviously iGPUs have less compute power and less bandwidth than anything dedicated, but. Part of me feels like iGPUs should actually have the features of the ""pro"" cards, given they aren't replaceable, with the actual pro cards providing performance.",Neutral
Intel,"Are used 3090s that expensive?  Here in Chile they go for about 550-600USD. Got one (luckily) at 450USD some weeks ago for AI/ML, without the box and such but it works fine.  Asked a friend from Spain and it seems there they go for about 600 EUR there.",Neutral
Intel,Do not underestimate the lack of CUDA.,Negative
Intel,"Bought my 3090 for MSRP 4.5 years ago and honestly would never have imagined that it would hold 50% of its value.  I want to get another one when they drop to $200 or so, so that I can finally be disappointed by SLI and make 14 year old me happy by finally clicking the bridge onto two GPUs and firing up a benchmark, but who knows if/when that day will come.",Neutral
Intel,A used 3090 is only $100 more.,Neutral
Intel,"It barely makes sense, at $600.  3090s are $700 now and the new 5070Ti Super (faster than a 3090) is expected to come out at $750 soon. And you’d have CUDA for either the Nvidia 3090 or 5070Ti Super.   I strongly believe that $600 is the price Intel set to ward off the scalpers. The niche of the B60 is pretty tiny at $600.   It’ll make a lot more sense at $500, which I suspect is the “real” price a month or two after it goes on sale.",Negative
Intel,Used 3090's are cheaper than this lol. Did you check the sold prices on Ebay at all? I can literally buy them at buy it now prices that are less than this.   94+ upvotes lol. 3090's sell for about the same price as 4070 supers as they have the same gaming performance.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1,Neutral
Intel,yeah it's got a good feature set for its price. people looking at it from a gaming product frame of reference rather than a professional product frame of reference are doing it wrong,Positive
Intel,"SR-IOV alone makes this a substantially different offering than a used 3090. If you know, you know.",Neutral
Intel,Atlas 300i duo,Neutral
Intel,"Also 3090s are $200 more, are used, consume twice as much power (400W card BTW), and do not have SR-IOV support.",Neutral
Intel,"It's cheaper than Nvidia's RTX Pro 2000, but it's also slower, but have more VRAM.",Neutral
Intel,Businesses aren't buying these cards these are destined to hobbyists home labs. The extra money to buy a CUDA supporting card is trivial for businesses they aren't going to buy Intel.  Certified or not the 3090 works wonderfully with all AI software out there while Intel does not.,Neutral
Intel,Yeah it was very scummy imo,Positive
Intel,"> R-IOV, certified drivers and other professional features  Did Intel ship the SR-IOV part? Last I read (been a while) it was ""coming soon"".",Neutral
Intel,It was meant to be a joke. Not so funny I guess.,Negative
Intel,"B580 needed to get out very early to beat RTX 5060 and RX 9060 to the market. Nvidia's and AMD's cards outperform it in gaming raster performance thanks to having been in the market for much longer. They have the more efficient architecture and software for gaming. It wouldn't be great for marketing if Arc came out at the same time with similar pricing, yet got beaten in the FPS benchmarks.  But productivity cards could wait. Arc's always been very competitive in productivity tasks. Maybe because Intel has more experience in this department. So reliable driver is the focus here.",Positive
Intel,">Only if your marketing plan is to sell 0 gpus because who will buy gpu with bad driver and application support?  Good driver and application support is not possible to deliver without a significant install base of your product in the wild. PC configurations vary wildly and usually driver issues are about some bad luck combination of hardware, driver versions, and application versions.  The incumbency advantage in this market is immense. Devs will validate that their stuff works on all manner of configurations including an Nvidia GPU, so they get the perception of having bulletproof drivers.",Negative
Intel,AMD and Nvidia have been having extensive driver support for years  ARC is a much newer platform requiring more careful hand holding,Neutral
Intel,Why does this matter?,Neutral
Intel,"Some iGPU do support SR-IOV, I think 12th generation and newer. Motherboard, driver, and OS dependent.",Neutral
Intel,Was seeing one sold locally recently for $800-$900. They very much are still expensive here in the US.,Negative
Intel,"Yup 600 Euro is about (or slightly above) what used 3090s are sold for here (Sweden) as well. I would rather buy a significantly better card used than a worse one new, if sold at approximately the same price point.",Negative
Intel,"Used site jawa.gg shows 3090s priced from $750 to $950 (and of course, some people asking $999 or $1200)  https://www.jawa.gg/shop/pc-parts-and-components/geforce-rtx-3090-KNRXB30U",Neutral
Intel,Used market is freaking insane. It is better to grab new or open box.,Negative
Intel,Over here used 3090s are sold for 500-600€.,Neutral
Intel,"Well sure if you specifically need cuda then this won't work, but there's also a large subset of people who just need vram for as cheap as they can get it. And for that purpose this Intel card is interesting at least.",Neutral
Intel,Dude that feeling is so worth it though. I did that same with 2 Titan Xps a couple months ago and the build just looks awesome. Great performance on pre 2018 / SLI supported games as well.,Positive
Intel,Just keep on digging the hole man.  A 5070Ti or 5070Ti Super or 5090Ti Super Extreme Titan do not have Support for SR-IOV. You do not know why people want these products.  > Muh niche  **Gamers** are the niche. Have you been living under a rock for the past 5 years?!,Negative
Intel,the super gpus are not expected to release soon?,Neutral
Intel,"Why have you posted this same garbage about a 5 year old used card so many times in this thread? It's a valid point, to some people I'm sure, but it's like you're weirdly obsessed with making it until someone hears you.",Negative
Intel,"....are you mistaking the listings of broken cards as new, working ones?  because when you take those out you'll see pretty quick that all of the 3090's here are selling for FAR above $600 lol.",Negative
Intel,Used 4070 Supers dont sell for nearly $700+ on the low dude.,Neutral
Intel,Yes running business of used cards is how its done.....,Neutral
Intel,no fee SR-IOV makes this a substantially different offering then AMD or Nvidia.,Neutral
Intel,Typically run at 250W though to be fair.,Neutral
Intel,"They are less not more, no idea why people are making up used prices when we can check ebay sold listings.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1",Negative
Intel,not on the Vram like professional cards,Neutral
Intel,Q4.,Neutral
Intel,I thought it was funny ¯\_(ツ)_/¯,Neutral
Intel,The b580 is still good at $250 and it’s only recently in the past month the 5060 and 9060xt 8gb dropped near or at $250 in some instances.  It’s a good card at 1440p aswell and it has 12gb of the other 2 8gb cards. Only problem I have it is the windows 10 drivers being abysmal dog shit if you’d want to play any current game with stuttering and crashes.,Positive
Intel,"Yeah, apparently a year’s worth of it",Negative
Intel,Because they're super late to the party.,Neutral
Intel,"Wow, 800-900USD after 5 years is more than I would have expected.",Positive
Intel,"No one on hardware swap is willing to go above $600 for a 3090, trying to sell one now. This reminded me to just put it up on Ebay.",Negative
Intel,I have no idea why people make up stuff like this when we have ebay sold listing search.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  Most recent one sold for $439.00 including delivery.  Again why are people making up used prices?,Negative
Intel,For what exactly do they need vram without cuda ?,Neutral
Intel,"Look at my profile. I have 0 comments in gaming and hundreds of AI comments. What do you know about running inference locally? Why do you think I mentioned memory bandwidth? Gamers don’t care about that. I haven’t played a game in years.   This is not a good deal for AI running consumers at this price, and a mediocre option for enterprise.",Negative
Intel,Gamers are not niche. They are 20 billion a year business.,Neutral
Intel,"False, they’re releasing in december or jan. So about 3 months from now",Neutral
Intel,"> Typically run at 250W though to be fair.  At which point we have to derate the performance. You know, to be fair.",Neutral
Intel,"The 3090 Ti did, but the standard 3090 did not.",Neutral
Intel,"It's a really good general purpose card. Perhaps not the best card if you want to squeeze out the most fps out of games, but it performs really well in a wide range of tasks including gaming. And yes 12GB VRAM really comes in handy.  It can easily be found for under $250. B580 is still cheaper than 5060 and 9060 XT and even RTX 5050 and RX 7600, especially in the UK where you can find a B570 for only 20 pounds more than RTX 3050. That's way cheaper than RX 6600, it's a no brainer. But the prices could only go down thanks to economy of scale mostly. It's been produced for a while, that's why.  Also one of the big reasons it's still selling now is thanks to good publicity. Imagine if it came out as the same time as RX 9060 XT. HWU would tear a new hole regardless of how much of an improvement it is over the Alchemist cards and the impressive productivity performance. Those guys literally don't care about anything other than pushing a dozen more frames in a dozen personally chosen games. Gamer Nexus and Igor too to some extent. It would be bad publicity.",Positive
Intel,"If intel felt they could have released this safely earlier, they would have",Neutral
Intel,People do not understand AI.  People buy up used 3080s and use 4 of them in an AI Rig to run MoE models. 96GB of VRAM can run you basically anything that's not the full 500GB models like Kimi K2. The power consumption doesn't matter because they're making more money off of the inferencing than the cost of electricity and it's cheaper than cloud providers.,Neutral
Intel,"Where did you find that? FB? Im saving for a GPU here in chile too, was aiming for a 5060ti 16gb but the model I want is 630k in ... Idk the top result in solotodo",Neutral
Intel,3090s will be much less attractive when the 5080 super and 5070ti super launches.  If the 5070ti super is 750- 800 with 24gb for a new faster GPU with a warranty it hel used market will have to shift.  The 3090s have nvlink but I'm unsure how much that matters for most people.,Neutral
Intel,https://www.ebay.com/itm/156720971107  That's a 3080 not a 3090 lol.,Neutral
Intel,"rendering, working on big BIM / CAD models, medical imaging,...",Neutral
Intel,CUDA isnt the only backend used by AI frameworks,Neutral
Intel,"I use opencl for doing gpgpu simulations, this card would be great for it",Positive
Intel,You mean your 0 profile history because you have it private?,Neutral
Intel,"And AI isn't the only use for a workstation card either, jfc.  3090s and 5070 Ti Supers don't have SR-IOV  3090s and 5070 Ti Supers don't have ECC VRAM.  AutoDesk for example will tell you to go pound sand if you try to get support and you aren't running a workstation GPU with ECC VRAM. So for businesses running AutoDesk software in a professional capacity, or businesses needing SR-IOV, you could offer them a 3090 or a 5070 Ti Super to them for $100 and it still won't be an option for them, because for their needs they might as well be paperweights.",Neutral
Intel,I thought we were talking about actual hardware that was available in the referenced country (US) and not a banned Chinese product that may or may not even exist and if it existed would cost nearly 10x as much as this product.,Negative
Intel,Spoken like a true gamer.,Neutral
Intel,last leak on them is that they’re delayed. Haven’t seen any reports since then that they’re releasing in q4,Negative
Intel,You still get about 85% performance compared to stock settings.,Neutral
Intel,"Of course, but that’s the point. It took them an excessive amount of time to “feel” like they could have released this",Negative
Intel,What are they doing that's making them money? Or are theu selling the compute somehow?,Neutral
Intel,"People do not understand Ebay sold listing search showing them selling for way way less than the values you guys just pulled out of your assholes?  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  They sell for roughly the same price as 4070's as they have roughly the same performance, AI don't matter gaming is still king in this market.",Negative
Intel,"Sip, marketplace.  Si buscas por RTX 3090 vendidas, es una PNY 3090 a 450K, y todo perfect.  La 5060Ti es más lenta que la 3090 para juegos, la 5070 es un poco mejor o igual.",Neutral
Intel,">rendering  Rendering what? Because if you're talking from a 3D art perspective, all our software is leaps and bounds better with CUDA or outright requires it. You can't even use Intel GPUs with Arnold or Redshift, so good luck selling your cards to a medium scale VFX house that heavily use those render engines",Neutral
Intel,>CUDA isnt the only backend used by AI frameworks    It is not the only backend for all AI frameworks; it is the only backend for the majority of AI frameworks.,Neutral
Intel,It is the only functional one.,Neutral
Intel,"It's the only one that's always well supported and expected to work with decent performance.  We're months after RDNA4 was released and ROCm still doesn't have any winograd kernels for it, which means 3x3 convs, the most common kernel size, are 2 to 3 times slower than they could potentially be...  I've close to no experience with OneAPI and SYSCL, but I'd imagine AMD should be ahead considering they've been playing the game for way longer… And if ROCm is a nightmare then I can only expect the experience with Intel to be as bad, if not worse.",Negative
Intel,"Oh good, I’m glad I set that private. You can literally google the username anyhow and see 1-2 example posts that underscores my point.",Positive
Intel,0 people are using autodesk with an intel arc gpu lmao,Negative
Intel,"You know nothing about the ai market, huh?  https://www.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/",Neutral
Intel,"Oh no, im part of 60% of world populatioin. how terrible.",Negative
Intel,Which is 36% higher performance per watt.       ... to be fair.,Neutral
Intel,The reason they didn't is because it would have been worse to release it earlier,Negative
Intel,Software Development,Neutral
Intel,Why did you spam my replies about cherrypicked results. The cards are worth $800-$900 and the listings you linked literally prove htat besides a few outliers that are $600-$700 to tank the prices.,Negative
Intel,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",Neutral
Intel,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",Neutral
Intel,"Depends on what software you use, but your right that NVIDIA with Cuda is safe bet when is comes to rendering. I use Enscape and that works great in my AMD gpu",Positive
Intel,Which ones?,Neutral
Intel,"Intel is a bit better for common models as they actually pay frameworks and model makers to optimize for it, unlike AMD.  I use flux and sd models and it’s much better on my arc than my Radeon 9600Xt",Positive
Intel,Private profile = Complete troll.  You're not an exception to this rule.,Negative
Intel,I too like to promote Chinese propaganda which isn't available in the United States due to the whole AI cold war going on and the fact that Huawei products have been banned in the US for what? 15 years?,Negative
Intel,"Right man, my point is that it shouldn’t have been",Negative
Intel,EILI7? How are software engineers using GPU ~~imprints~~ inference on their home machines for software development? What am I missing?  E: a word.,Neutral
Intel,"Hell, one of them was just the cooler without the actual GPU.",Negative
Intel,"En ese claro entonces tal vez valga la pena esperar a la 5070 Super, que supuestamente tendrá 18GB de VRAM.  Sobre comprar usado, todo bien en general, pero siempre hay que tener ojo y ojalá siempre probado. Yo fui a la casa del vendedor para revisar y todo bien, pero igual puede ser peligroso, por eso revisar bien los perfiles.  He comprado hartas 3090s usadas por FB Marketplace.",Neutral
Intel,for example every local image and video generation system I've seen.,Neutral
Intel,"you’re wrong about cost, availability, performance, and just plain wrong overall lol",Negative
Intel,"The Chinese are busy making hardware, not propaganda. These are old cards that have been around for a while, I can assure you they are quite real.",Neutral
Intel,According to whom? With what evidence?   Intel spends more on r&d per year than nvidia and amd combined   Do you think they are just being lazy?,Neutral
Intel,"The many hours of trial and error getting things to work, and building expertise with said labor would be my guess.  This isn’t home software engineers buying up all the inventory, its people getting around sanctions.",Neutral
Intel,Getting good LLMs crammed onto GPUs for coding is a popular topic on /r/locallama.,Positive
Intel,And the one directly under that was the actual PCB...,Neutral
Intel,Im running Comfy UI + Flux on my Strix Halo right now without issue 🤷‍♀️,Neutral
Intel,Propaganda is 90% of chinas economic output though.,Neutral
Intel,"Having same issues, having 40-60% decrease in fps across games. Along with stutters, and fps drops. Hard to run even easy to run games like roblox and LoL. Insane that a windows update caused this somehow.",Negative
Intel,"I think I have an restoration point, but damn it sucks",Negative
Intel,It's working perfectly fine after I rolled back the latest security update.,Positive
Intel,"Make sure you have resizable bar turned on inside your bios and hopefully you got a pretty solid CPU. Other than that, this is one of the better gpus for 1080p gaming.",Positive
Intel,"My current cpu is 5 5600g but I'll upgrade it next year probably, for now it's not that bad ofa bottleneck I hope. I will turn resizeable bar as soon as my new mobo comes, I got the Asus Rog Strix B550-A gaming. Also my monitor is 1440p but from what I've seen, this gpu is perfect for that resolution.",Positive
Intel,no need to be very solid. an entry level cpu suffices  i'm using 225f and most of the time during gaming the gpu is 100% and the cpu is 33%,Positive
Intel,"Oh man, I know how you feel. Sucks to live in a country which gives you the ""3rd world tax"".   In mu country, a 5080 will be ~$2,200. I bought it in the U.S. Amazon store, shipped it into this 3rd party company's warehouse which will ship it to me. Takes 3 weeks in total. But, at least, I paid MSRP for the card + shipping and tarrifs that totaled $1,200.",Negative
Intel,"Late but I chose a B580 and have absolutely 0 regrets. Very stable performance, have been playing some lighter new games at 4k 60fps, and I know I'm going to be futureproofed vs the other cards in its price class due to the 12GB of VRAM. Yes the 9060XT 8gb is technically a stronger performer, I will trade a few frames today for longevity over time, which should also translate into much stronger resale value on the B580 in the future.  But it seems you are finding the 9060Xt for actually significantly cheaper, like $70. That difference might make it worth it to go with an 8gb card. It's not going to be unplayable of course, but I think you may have to upgrade a bit sooner than with the ARC",Positive
Intel,"I feel the B580 to in quite a sweet spot at the moment and I kind of don't see a reason not to go for it. It may not be the best card out there (neither is the 9060 8GB), but in 2025 I would 100% go for the 12GB card over a tad more performance as that will last longer when games start to get more demanding VRAM wise. Sad to see the prices you have to pay for one at the moment, but I also see them rising here and hovering around $325-385 while most of them were under $300 last week (I'm not in the US btw).",Positive
Intel,I would take Rx 9060 Xt even 8 GB over intel arc 580.,Neutral
Intel,"Hey man, fellow Brazilian here. I know things are tough down here. I have not tried it yet, but everywhere I see right now, Intel is the king of cost x benefit. You can also help yourself by selling your card for about 600-700 reais, making your new card ""cost half"" if you go with the arc.  In the u future this will also help when you want to upgrade so your new card ""costs less"". I would go for the 12gb card because with the current landscape, games are demanding more vram and because of AI prices will shoot up.  If you need any help, dm me and I can try to help you out",Neutral
Intel,"Yeah, can't even do that here... It's possible that you would have to pay 92% more when it gets to the country or something...",Negative
Intel,"Yeah, it's possible that I'll stay with the GPU for some time... So, the 12 GB having more longevity, even if the 9060 xt has more raw power... Maybe the safe thing is to get the B580.",Neutral
Intel,"Right now sure... But what about longevity? I said that it's possible that I'll have this GPU for years, so... Won't 12 GB age better than raw power with 8 GB? Especially at 1440p.",Neutral
Intel,"Oh, I actually want my card. I have a home server and an NVDIA GPU would be nice for some stuff like transcoding. Also, I may be building a PC for my brother soon, so maybe the 1650 Super will go well in his PC for now. And if I buy another GPU in the future, I can pass the one I'll have for him too.",Positive
Intel,"Maybe I am wrong and the 9060 turns out to be the card that lives the longest, but if I had to make the call in that budget range personally, it would be the B580. Like you I don't really see a reason to go 8GB anymore. If Intel pulls out of the GPU market tomorrow, then I was wrong. If 8GB is not enough anymore by some release in June 2026, I was right. If you somehow could push the budget to the 9060 16GB, then that is a no-brainer as well. Difficult, but I do feel the B580 is very compelling.",Neutral
Intel,"I think raw power will come in more handy right now, and 8 GB is not the end of the life. Or spend a bit more on 16 gb.   You will need to make sacrifices when raw power runs out too.",Neutral
Intel,"Oh... It's not ""a bit more"" it's 1 THOUSAND reais more... At least...",Neutral
Intel,It is hard for people not here to understand that a difference of 180 USD is almost one month minimum wage for us,Negative
Intel,"I would take 8 GB 9060 Xt. Not a great choice for new AAA titles, but will rock everywhere else",Positive
Intel,I have a hard time affording upgrades too. This is why I would get the cheaper GPU which is also faster in most cases.   Is it possible to bring GPU when you visit Brasil?,Negative
Intel,"Makes sense. Thanks for the input, but ""not a great choice for new AAA"" titles makes me scratch my head, because I'm seeing the card being able to run new demading games on Ultra with more than 50 FPS and some games like Doom Dark Ages with more than 100 FPS max settings... You may have higher standarts, but that's absurd triple A performance for me...",Positive
Intel,"I am gaming at 4k if possible on RTx 5070   My last card was Rx 6650 Xt with 8 GB, and Rx 9060 Xt is much better. I think it is a good card, unless you run into rare cases where ram is not enough. So many great games where 8 GB will be plenty",Positive
Intel,"Makes sense, thanks for the input!  I agree, a lot of games that 8 GB is enough, but like, how the market is going, it seems like 8 GB is not being enough even today.  And I'm scared of having the card in 2 years or even more and being extremely limited by 8 GB.",Negative
Intel,"This would allow you to reuse your current RAM.  [PCPartPicker Part List](https://pcpartpicker.com/list/zCY6t7)  Type|Item|Price :----|:----|:---- **CPU** | [Intel Core i5-14600KF 3.5 GHz 14-Core Processor](https://pcpartpicker.com/product/zyyH99/intel-core-i5-14600kf-35-ghz-14-core-processor-bx8071514600kf) | $209.99 @ B&H  **Motherboard** | [Gigabyte B760M GAMING PLUS WIFI DDR4 Micro ATX LGA1700 Motherboard](https://pcpartpicker.com/product/NMWJ7P/gigabyte-b760m-gaming-plus-wifi-ddr4-micro-atx-lga1700-motherboard-b760m-g-p-wifi-ddr4) | $137.99 @ Amazon   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$347.98**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-12-15 14:54 EST-0500 |",Neutral
Intel,"Probably looking at motherboard, ram, and CPU upgrade since they all have to be done at once. I’d go AMD, 7000 series or higher (am5 motherboard platform)  Then, I’d upgrade the power supply to 650 minimum, to prepare for the last thing  Which is the gpu. Yours is still beyond ok, so this is last.",Neutral
Intel,Because of memory prices if you can find a cheap 14th generation CPU and B760 DDR4 motherboard reuse your other parts. If money is no issue probably others have mentioned something AM5 for future upgradability.,Neutral
Intel,"First try updating your drivers, could just be a simple fix.  Could it also be possible that the psu is overloaded? Hopefully someone with a little more knowledge on here can help",Neutral
Intel,"With FG yes you'll most likely be able to get 140+ FPS.   I'd just go with 2x16, but it depends on pricing.",Neutral
Intel,I play it with my 4070 It get around 90-150 fps with all cinematic setting with dlss quality and framegen You can go with high or epic setting and use native instead,Neutral
Intel,I have a 7800X3D 9070xt and I believe I get a consistent 140 or so at almost max settings. I haven’t tried the new max settings they released though.,Neutral
Intel,"What about 1440p on low settings? Can you reach 140+ FPS at 1440p on normal or low, with or without DLSS/FG?",Neutral
Intel,yes almost 170+fps all the time,Neutral
Intel,"Motherboard should not have any issues with any graphics care, those are always plug and play and completely interchangeable for modern hardware. Between the cards it really depends on the price, you should always include that along with the primary games you want to play.",Neutral
Intel,"If they are the same price: 5060 Ti 16GB.  If the 5060 Ti 16GB is more than about 10% more expensive: 9060 XT 16GB.  Both are superior to the B580, in performance and drivers.",Neutral
Intel,5060ti 16 Gb if you care about Nvidia features or if it not that much more expensive than 9060XT.  If conditions above are false - 9060XT 16 Gb.,Neutral
Intel,"The 5060Ti 16 GB is the best, the 9060XT is usually the best value (right now it's about 60 Euro cheaper, and therefore the better choice). Only go for the B580 if your budget doesn't get to those other two.  No worries about the motherboard, any PCIe card will work with any motherboard with PCIe slots (meaning all of them in the last 20 years), the only possible issue is if your PSU isn't powerful enough or doesn't have the power connectors for the GPU (yours is well over the requirements and should have no trouble whatsoever).",Positive
Intel,9060XT 16GB,Neutral
Intel,This.,Neutral
Intel,"Hi there! Thanks for the comment.  We ask that posts and comments be in English so they can be understood by as many people as possible. Translations on Reddit are client-side, and not all apps or browsers support auto-translate. Currently many users (and moderators) aren’t able to read your {{kind}.  Could you please submit a new comment in English?  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/buildapc) if you have any questions or concerns.*",Neutral
Intel,"If you're on a 5800X3D, I literally might wait until AM6, it's a terrible time to buy into a new platform.  But if you do it, go find one of the million people around here who can't afford DDR5, and sell them your 5800X3D. You're sort of wearing the price-performance crown by *not* having to buy DDR5 at the moment, but if you want to light some $ on fire, just make sure your chip finds a good home.",Negative
Intel,"At 4K, the difference between the 5800X3D and 9800X3D is very minimal. [TechPowerUp shows a difference of 2% on average](https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/20.html).  I wouldn't do the upgrade for your case as you are paying a significant about for an almost negligible improvement in performance,",Neutral
Intel,"Generally speeds between 6000 Mt/s and 8000 Mt/s with the same first word latency are *slower* than just sticking with 6000 Mt/s. Around 7800 you get to parity with 6000 MT/s, and 8000 MT/s finally starts to show benefits over 6000 Mt/s. However very few Ryzen CPUs are capable of 8000 MT/s, and 6000 MT/s with slightly lower CL timings will beat any 8000 MT/s RAM available.  That said, with the current state of DDR5, you often kind of have to go with whatever you can find. And faster kits often have multiple XMP / EXPO profiles, usually including one at 6000 MT/s that'll generally be preferable. But if the Kingston memory is less expensive, go with that instead as we're generally talking fractions of a percent difference in performance here for a 9800X3D.  ...  Here's an explanation for why anything over 6000 MT/s is slower.  Ryzen CPUs are a chiplet design, meaning the CPU cores and the I/O + Memory controller live on separate chips connected by a data bus they call the Infinity Fabric. By default, Ryzen CPUs sync up the memory frequency, memory controller frequency, and infinity fabric frequency so that they're all running the exact same speed. This is known as FCLK 1:1 mode. For DDR5 6000 MT/s this is 3000 Mhz.  And yes, DDR5 6000 is actually 3000 Mhz, not 6000 Mhz. This misrepresentation of their speed as twice their actual frequency is a legacy marketing thing related to what DDR stands for; Double Data Rate. DDR sends data twice per frequency tick, which set it apart from ""single data rate"" RAM that existed before it, so it was marketed listing it's frequency at 2x it's actual frequency to try to communicate its relatively performance benefits when compared to the previously existing types of RAM. More recently companies have started using the more accurate MT/s (Mega Transfers per second) label instead of Mhz, but not all.  Back to Ryzen CPUs. For AM5 CPUs, the infinity fabric is limited to 3000 Mhz, so when running RAM *faster* than that, it drops to running the infinity fabric at half the speed of the RAM, so 6400 MT/s RAM has the bus running at 1600 Mhz vs the RAM's 3200 Mhz. This is FCLK 1:2 mode. This means that while the bandwidth continues to increase with speeds above 6000, the effective latency ends up being higher once you go above that speed and the bandwidth has to increase substantially before it can offset that hit to latency.",Neutral
Intel,I made the jump from a 5600x3d at 4k and it wasn’t worth it.,Negative
Intel,"There's no reason at all to get a 9800X3D at 4K, even if you use performance mode upscaling. You can very likely keep that 5800X3D until at least AM6 or even AM7.  I would at the very least wait until (if) RAM prices go back to normal.",Negative
Intel,Not worth it. Your computer is capable enough as is.,Negative
Intel,"In certain conditions the 9800X3D destroys the 5800X3D, I agree at  native 4K there's not much of a difference if you're using a 5080 (keyword truly native 4K), but for instance Resident Evil 4 Remake with high ray tracing enabled at 1080p maximum settings runs at 75-90 fps in the cabin fight (with dozens of villagers with torches surrounding the cabin) on the 5800X3D + RTX 4080, but with the exact same GPU and a 9800X3D the game never goes below 125-150 fps in that test, I repeated it multiple times and also noticed that when you arrive to the village in the chainsaw guy fight the 5800X3D is limited to 120 fps at best when all the villagers are after you with dips into the low 90 fps, however in the same fight the 9800X3D is sitting comfortably between 162 and 188 fps. I repeated it multiple times, same GPU drivers, same Windows version, same game settings.  This is just an example, I've noticed similar gaps with Spider-Man (ray-tracing enabled), Ratchet and Clank (also with very high ray tracing enabled) and multiple other titles, once ray-tracing is enabled the 9800X3D is in a different league. I play on a 1440p monitor and usually enable DLSS Quality since it looks phenomenal while yielding increased performance.",Neutral
Intel,"I just recently went from 5800x3d to 9800x3d with a 5080 at 1440p some games no gains, some games small gains and some games massive gains in fps. But what i mostly noticed how much smoother all games ran on the 9800x3d.",Positive
Intel,"Wait, some great new chips from AMD and Intel coming out next year, RAM should be cheaper as well. At 4k , you won't see much improvement going to a 9800x3d.",Positive
Intel,It will likely give you a 20-40% uplift based on your description of playing at low graphics settings and DLSS performance at high refresh rates with a 5080. Whether that is worth $1100 is a decision you have to make.,Neutral
Intel,"Just curious, with good hardware like that, why are you playing on the lowest setting?",Neutral
Intel,It is for a game like Tarkov. Not for anything else imo.,Neutral
Intel,"What is your reason for wanting to upgrade?  Are you not able to hit 240 FPS at 1080p with your current setup?    A new CPU, mobo and (DDR5) RAM will cost you a pretty penny, so unless there's a significant performance issue you're experiencing I would think twice.",Neutral
Intel,"1) He’s playing with a 5080 on the lowest settings and wanting 200+ fps, that alone will see uplift with a 9800x3D.   2) 20 game averages don’t really tell the whole story because there are games which would see anywhere from 20-40% uplift for 5800x3D to 9800x3D. They are just more few and far between, so they tend to average out.",Neutral
Intel,"For Ryzen 9000 CPUs they also added the option to unlock the infinity fabric and force a 1:1 mode regardless of the RAM speed. This seems to work reasonably reliably up to about 6400 MT/s, with some people claiming to have gotten it to work at 6600 MT/s. But this isn’t guaranteed, as even 6000 MT/s isn’t always going to work. This can see sizable performance benefits on non-X3D CPUs, but still quite minimal benefits for X3D.",Neutral
Intel,Thanks for the detailed explanation.,Positive
Intel,"Your heart's in the right place, there's little reason to spend crazy prices right now for an AM5 setup if you have a 5800X3D, but it's not even close to ""no reason at all"" territory.    The 9800X3D offers dramatically better 1% lows, which attributes to the ""smoothness"" of how a game feels.  It's strictly a better CPU, all other things equal.",Negative
Intel,I only play on the lowest settings for comptetive games.  First reason is visibility for me. High and detailed effects make hard to see the enemies.  The second reason is to reach the refresh rate of the monitor.,Neutral
Intel,"No, [it does not offer ""dramatically better 1% lows"".](https://youtu.be/aYYVz4q-Rt8?t=1176) There will be a slightly bigger difference the more you rely on upscaling, but not to the point where the 9800X3D offers *dramatically* better lows. Especially considering OP has a 5080, not a 5090.  The 5800X3D will be absolutely fine for 4K for at least another 3-5 years.",Neutral
Intel,"Gotcha, makes sense",Neutral
Intel,"Minimal 9% improvement in 1% lows are very significant. It doesn't mean the 5800x3d is bad or unusable, just strictly worse.    You've also been unfortunate enough to pick the best case scenario for the old AM4 chip, while op is actually focused on 1080p ""competitive"" performance where the better CPU does matter substantially more.",Negative
Intel,"OP is using performance mode upscaling. It's not the same as 1080p native.  Also you're looking at the wrong graph; OP has a 5080: It's a 1% difference in 1% lows, not 9%.  And I never said the 5800X3D isn't strictly worse, obviously it is. It will still be perfectly adequate at 4K for several years, and the 9800X3D will not give him a meaningful upgrade.",Negative
Intel,In 4k gaming the setup will become obsolete sooner as graphics improve.   And you will get lower fps right from the start.   1440p seems to be the sweet spot.,Neutral
Intel,If you go 1440p you’ll have to update your gpu less and in the age of AI that may be a good thing as prices are absurd for things like memory etc,Neutral
Intel,"If you're not aiming for 150+ FPS, then an X3D is a waste of money and you could look at better GPUs instead.   The 7800X3D/9070XT combo is very well suited to very high FPS 1080P/1440P.",Negative
Intel,4k if you dont play competitive games and 40-60fps is fine  1440p if you do,Neutral
Intel,"4K is generally fine on it, especially with FSR 4, but 1440p is this card's sweet spot",Positive
Intel,"I have a Ryzen 9 5900X CPU & an RX7800XT - I run a triple 1440p Eyefinity setup.  So I often run:   \- 2560 x 1440p (16:9) on a single monitor   \- 7860 x 1440p (48:9) on triple-wide  My setup can handle:    \- 1440p REALLY well on 90%+ of the games I play.  Mostly 90fps+    \- It can run a ton of eyefinity ultrawide with at least 60fps      The Eyefinity setup would be  11,318,400 pixels   A single-monitor 4K would be 8,294,400 pixels  The system you are describing would handle any of the above...but I prefer 1440p monitors because they are cheaper and I really don't personally feel I benefit from the increased resolution on a single monitor.  I have 3 39"" 1440p monitors...got 'em for $299 each.  So much more bang for my buck than getting a single 4K.     Also: if you are interested in getting a triple-monitor setup, ultrawide, or 4K - ""Lossless Widescreen"" is absolute magic for games that are below 60fps.",Neutral
Intel,"i have the same build.  i think you should buy a 1440p monitor.   the 9070xt is an ""entry 4k gpu"", i watched tests and it was about 60fps on average in 4k for games that require more hardware.   i put it together about a week ago. this is my first build (i got help).   you have to change a lot of things in the bios to make it run well + the amd adrenaline.   to give you an example, with 1440p basic settings (in game) average about 280fps in warthunder, i haven't had much chance to play anything else.   and a tip, if you want to install windows 11 on it, the december and november gpu drivers won't work well, but they will on windows 10",Neutral
Intel,Unpopular opinion but I think anything above 120fps doesn’t matter that much. Which this card is capable of achieving max settings with upscaling and frame generation.  Frame generation 4K > 1440p native at same fps.,Neutral
Intel,Ah yes. While typing my post I also tought of that. While the 9070xt now gets decent frames with 4k currently. In a few years it might not.,Neutral
Intel,Yes. As of these comments im most likely to go now with the 1440p monitor. Better furure proofing as well as a smaller cost. 4k sounds so nice but I need to think rationally.  Thank you,Positive
Intel,I see. What kind of gpu then would be good? Doesnt the 9070xt beat the 5070 ti which is more expensive?,Neutral
Intel,Thanks. But im swayed rn to 1440p due to thoughts about future proofing. In a few years those 60-40fps might turn into 50-30. That would not be good.,Neutral
Intel,"Yes thats what ive heard too, 4k sounds so tempting tho. But it seems 1440p is the best option. Thank you!",Positive
Intel,"Interesting perspective, thanks! So what you are saying is that I wouldnt have much trouble running in 4k, but 1440p offers much better value currently?   Personally I dont really like the wide or curved monitors and was thinking of a single 27” monitor. I do have a monitor alredy and probably will just allocate it to be my second monitor for discord or whatever on the side.",Positive
Intel,"Hmm, interesting. The [hardwear unboxed video](https://youtu.be/aWfMibZ8t00?si=wK-wVoiKNX7G35ms), which I think we both saw, showed the 9070xt performing at 59 to 74 fps at 4k on avarage. But I think with upscaling it could do 70+ fps in many games.   Could you also elaborate about tinkering with bios thing? Did you have to do it? Why was it neccessary? What was the issue and how much did it effect?  I will run windows 11. Are the drivers just made based on windows 10 and are not yet updated for 11 or what?  Thanks!",Neutral
Intel,"It really depends on the quality and FPS you want to target.   X3D is excellent if you're aiming for 200+ FPS. Here you'd typically use lower settings and resolutions. For example a 1440P 240+Hz monitor.  The 5070 Ti averages slightly better than a 9070XT in raw rasterization power: https://www.tomshardware.com/reviews/gpu-hierarchy,4388.html  But a 5070 Ti is significantly stronger if you're going for high quality with RTX. It's also has an advantage with DLSS for higher FPS with 4K.",Positive
Intel,"Nope, the 5070 Ti is better, but the 9070 XT is a better bang for your buck.",Positive
Intel,yeah  thats perfectly fine,Positive
Intel,"*""So what you are saying is that I wouldnt have much trouble running in 4k, but 1440p offers much better value currently?""*  Yes.      I really wasn't a fan of the curved monitors either...thought they were a gimmick...until I decided to try one.  When you have three of them, you get a good (but still subtle) ""wrap-around"" effect that is really immersive on eyefinity.  I'm with ya on the ultrawide monitors though; simply because 1/2 of what I do is better on single-monitor and usually you'd need some 3rd party software to setup smart frames; and they are way less bang-for-your-buck.  BUT: If you are only planning on getting a single 27 monitor, then you will be good with either 1440p or 4K.",Neutral
Intel,"I can't give a complete answer to the BIOS thing because I'm not an expert. We worked a lot with RAM, we couldn't even enter the BIOS afterwards, so we had to reset the BIOS (but maybe it was just because of my MSI B650 Tomahawk WiFi and Crucial Pro OC Gaming 32GB kit, maybe you won't have this problem), but I just wanted to say something that if you want the best performance. (delay, expo, unlock factory restriction and others)  you can install the driver on Windows 11, but my problem was that when I tried to open something, it wouldn't open anything except folders, which means I couldn't even turn it off, only with the power button on the computer case.  If you don't have a video card, it will be difficult to do anything with it, or you will have to delete the driver somehow. It worked without a driver  Just out of curiosity, how much did you buy RAM for if you already bought it?",Neutral
Intel,"Well, then I might consider trying out a curved one. But having three seems quite exessive for me atleast.  Also a bit unrelated but I will then have two monitors, the new and the old one, and im thinking, should I connect my old second monitor to motherboard and use its integrated graphics to minimize the strain on my gpu? Or is it just not that big of a deal?  Thanks!  edit: fixed a typo",Neutral
Intel,"Did this BIOS hassle result from an upgrade to your previous system, or was it a completely new build with all new parts?  For me, I have a very old pc (1060ti and comparable other parts) and Im going for all new parts for this build. I WAS going to buy a bundle deal with ram, cpu and mobo for 749€ but typing this comment out, I went to check the details of it and the deal is gone. Im guessing due to unstable market, very unfortunate and this looks like ill probably have to spend some extra cash.   When I pull the trigger finally on this thing I will probably post about it on some sub.  But for now I dont even know what I will be paying for all of this. It will hover around 2000€ for all parts.",Neutral
Intel,"You'd definitely want to run them both from the GPU. If you are playing on one, and something like watching youTube on the other, then the additional pull on your GPU to watch YouTube is minimal.",Neutral
Intel,"I had to put together a completely new build, I didn't have a computer before, I only had a laptop and a PS4",Neutral
Intel,Alright. Thanks!,Positive
Intel,I see. Thanks!,Positive
Intel,"What do you want to do with your GPU? Are you just overclocking/undervolting for the fun of it or do you pursue a specific performance goal? Without having a goal in mind, it's hard to make a recommendation.",Neutral
Intel,You won't sell a 7900 XTX used for 800 euros,Neutral
Intel,IMO if you want switch to nvidia get 5070 Ti for around 750€ and keep the change or get better cpu/psu/cooling with spare money.,Neutral
Intel,"If u want to switch to nvidia do it, but maybe get a 5070ti then do a safe overclock. Its cheaper.",Neutral
Intel,I think you need buy 5070 TI It will be best solution in your case,Positive
Intel,100%,Neutral
Intel,"I am just looking for good performance in games and efficiency, also I feel like my 7900 XTX is using so much power, i tried lowering power limits or slight UV but still get crashes… My Monitor is LG Ultragear 1440p 240hz, and when I play single player games with gorgeous graphics I don’t want to turn on FSR because it’s horrendous, which means im running native and getting less FPS on High/Ultra settings",Negative
Intel,"Well I feel like it should cost roughly 700e, I bought it about 8 months ago. But cheapest one selling used where I live goes for 800-850e…",Neutral
Intel,"I have ryzen 7 9800x3d, bequiet 1000W psu, deepcool aio, bought everything together. Now I feel like only thing I could change is GPU",Neutral
Intel,"I mean, the 5080 isn’t exactly any better on the power front.  355W nominal for the 7900 XTX versus 360W nominal for the 5080 - they’re both juiced to the gills.    The 5080 is about 15% faster so you would basically be paying about 400 EUR to swap feature sets to NVidia and a reroll on the silicon lottery.  It sounds like your 7900 XTX basically exactly meets AMD spec given the lack of headroom on your sample, which makes sense for the 7900 XTX given how hard it’s being pushed for those last dregs of performance.",Neutral
Intel,"Try to filter by ""sold items"" on ebay. That tells you for how much they're actually being sold, compared to the prices sellers wish for.",Neutral
Intel,Where do you live?,Neutral
Intel,"I have XFX Merc 7900 XTX, performance is fine but I think  maybe rtx 5080 would give me a but more performance and looks like RDNA 3 would not be getting FSR 4. My main concern is these driver timeouts that I have been having last 2-3 months. Happening frequently, every days it bound to happen. Sometimes on stock settings with no UV or OC. That whats driving me crazy… I wasn’t planning to switch before driver issues started happening and itself really annoying when you play multiplayer games…",Neutral
Intel,"I live in Serbia, there prices are different. For example here used 5080 doesn’t even exist to be bought yet 😩",Negative
Intel,Serbia,Neutral
Intel,If you’ve tried a bunch of things for troubleshooting the timeouts you could see if you can RMA the card.  Some of them just have issues for whatever reason.,Neutral
Intel,I didn’t think about that really. But will see how it goes. Thanks for the recommendation,Positive
Intel,I don’t know how good Serbian consumer protection law is but it’s definitely worth a try.  Better than trying to sell a lemon.,Positive
Intel,I sold a 3080 for $260. I think you can get a 3070ti for that price.,Neutral
Intel,"Your current GPU is the $100 option.  To double your performance you'll need something on par with RTX 4060 or RX7600 or RTX2080 or RX6650XT - all out of your reach.  RTX2060 Super will give you 50% uplift for $200.  ARC is still hit and miss, but A580 is around the same as 2060 Super. RX6600 is the same category.",Neutral
Intel,"You will be best off navigating the used market. $100 is a pretty tough number to work with that would likely limit you to sidegrades or very small increases - closer to $200 will be able to get you a really nice upgrade, and there are a few decent choices in-between.  I'd be looking at a 3060 Ti personally - they're a really good used option that can be found around the $175-200 range right now. If you can find a good deal, the 6600XT is another solid choice to consider, though it'd be a smaller uplift and they're not much cheaper than the 3060 Ti from what I've been seeing.",Positive
Intel,"If you're rocking 4 sticks of ram, sell 32bg of ram then buy a better GPU. It sounds troll but with current ram prices this might be the best option to increase your budget  32 gb total is more than enough nowadays",Positive
Intel,Id say look for a new b570 or a used 2070 super.,Neutral
Intel,"honestly, why not give the older 1080 Ti a try. it may not have the fancy features but it does go raster for raster with the 4060.  just basing this off of benchmarks from Passmark, the score for the 590 is **9315** and the score for the 1080 Ti is **18607**.   (The 1080 is in the $150 range currently)",Neutral
Intel,"Gaming @ 1080p around $100-200, shoot for an AMD RX 5070xt from ebay. Buddy got one for $69 a few weeks ago. Just depends on how lucky you get.",Neutral
Intel,Someone got a good deal. I'm mainly seeing 3080 10 GB models for around $300 atm - really nice used option right now for sure.,Positive
Intel,He can get a 2070 Super for $200. Has to negotiate. I got a full 3080 PC for $615 with a 10700k.,Neutral
Intel,"That's ingenious, actually.",Neutral
Intel,"Feels like this needs the caveat that this goes for a prebuilts using proprietary parts like garbage from HP or Dell.  Most prebuilts that come from small to medium sized SIs and shops are built using off the shelf parts, which are functionally identical to a custom PC.",Neutral
Intel,"I mean it really depends. Hp is notorious for giving users zero freedom. If you hadn't bought a hp, you probably would have been fine. You just don't buy hp, that's the advice you should give",Negative
Intel,You honestly could’ve just stuck with the 3070. It’s still a pretty competent card,Positive
Intel,Your first problem was thinking the 3070 was the weak point of the pc,Negative
Intel,"Just did an upgrade myself, it's exactly that. Use it or don't buy it. It's absolute dogsh\*t for upgradability. I swapped everything except for the CPU, RAM and SSD. After you disassembled everything you find out how cheap and pre-made the junk is that's in an Omen.  We learn.  Oh and HP: showing how modular your Omen system is on your site means nothing when almost all your goddamn parts are OEM.",Negative
Intel,Upgrading most prebuilts is fine.  The lesson is that you NEVER buy HP or Dell/Alienware prebuilts. They are hot garbage.,Negative
Intel,The pc was perfectly suited to arc raiders without upgrades.,Positive
Intel,"Hey man, I went through something very similar with an omen 40L that I've upgraded in the last 2 years. From mobo, to psu, to case. Had to do alot of weird stuff to make it work, HP is all proprietary, unfortunately, so you have to get creative.",Negative
Intel,"I actually did something similar. I got an omen HP as a starter on ebay for like 400 bucks, a deal. I slowly started upgrading parts like SSD, Ram, then finally a CPU. Until I accidentally bent a pin or two on the motherboard. I ended up switching it the motherboard but couldn’t get it to work. After so much testing troubleshooting I figured maybe it was the motherboard which exchanged it but still nothing. At that point I just finished buying everything else like the case, fan, power supply and then the GPU. Never again buying again Omen or even recommending anything close to it.",Neutral
Intel,"I started off with a prebuilt. It was a pretty good system for the time and the only things they flaked out on in my opinion were the PSU and maybe the motherboard. From there I was able to upgrade a few parts and it's still in use almost 10 years later. Prebuilts can be a good initial investment, I would just avoid hp",Positive
Intel,Good point. I've never dealt with prebuilts before. A relatively expensive mistake to make.,Negative
Intel,God damned HP. A 2023 model that costed around 2000chf new,Neutral
Intel,Not for 4k,Neutral
Intel,Do you disagree?,Neutral
Intel,We keep telling people and they fall to listen every time,Neutral
Intel,"Hp makes their devices as locked down and usually proprietary as they can, just to force you to upgrade once its not enough anymore, hoping you go for them again giving them more money  My experience with msi is amazing though, i have a prestige 14 ai evo (not a gaming laptop ik) and its uefi is about as locked down as their desktop motherboards, so not at all. I feel like its truly my device and not controlled by a company",Negative
Intel,lol then you should have built from scratch from the start. Is this some European brained shit or what?,Negative
Intel,If it's 4K you're after then why bother with all these shenanigans? Could've have built a PC yourself.,Negative
Intel,I'm writing this comment on a computer with a 3070. It's still going strong after nearly 5 years. I play Cyberpunk and Star Citizen regularly.,Positive
Intel,"with a 7700 cpu the 3070 is definitely the weakpoint, people just get very defensive because their old parts still ""work"".",Negative
Intel,"Wanting to run 4K off a prebuilt isn't inherently a bad idea, but a 2nd hand prebuilt is an odd choice to start off with, and as someone else mentioned, especially from a large company like HP. Does feel like there wasn't much consideration of goals vs. available resources before making the purchases.   And if 4K wasn't always the original intent, then a 3070 would be absolutely dandy to run Arc Raiders at 1080 or 1440 with a few settings tweaked to hit goal frames.",Negative
Intel,"It would be a great deal (considering the resale value of the 3070) if only HP hadn't rejected the new 5070, . You'd never be able to build anything similar on a budget of 850 CHF.",Positive
Intel,"I much prefer 4K for office work and I genuinely believe that 4K DLSS quality looks much better than 1440p native. The 5070 is a perfectly capable card for that purpose.   Also, there is nothing wrong with a second hand prebuilt, especially if there is an extended warranty.   Of course knowing how things turned out I would go for a custom at this point. But that's the goal of the post - to act as a warning.",Positive
Intel,"The rx9060xt 16gb would be fine for those games at 1440p.  Having the 7800x3d would be fine for getting a GPU upgrade in 3-6:years.  Something like a r7-7700 and rx9070xt would be better right now, but the additional gaming headroom of the 7800x3d may be nice later with a stronger GPU.",Positive
Intel,"Completely irrelevant, but there’s a 9070xt on sale for 560 at micro center if there’s one near you",Neutral
Intel,"I would recommend a 5070 or 9070 if you can find them around the $500 price range, those will give you much better performance.",Positive
Intel,"If you haven’t gotten the 7800x3d yet, I’d highly recommend something like a 9600x or 9700x (7600x and 7700x also work) and going with a 9070xt or 5070ti. Pairing an x3d with a “mid range” GPU like a 5060ti or 9060xt isn’t the best way to spread your money",Neutral
Intel,"If between the two, the rtx 5060ti slightly better than rx 9060xt in almost all games. Not that much big different. If u have the money go for rtx5060ti. I have rx 9060xt for me its more than enough.  Edit forgot to mention the rtx have better ray tracing",Positive
Intel,"Just make sure of getting the 16GB version.   Look for those specific games benchmarks in YouTube, as for example, ""ARC Raiders 9060XT 16GB vs 5060Ti 16GB""",Neutral
Intel,The 5060 ti is 40 percent faster than the 9060 xt in arc raiders(very heavily nvidia favoured title). E33 doesn't have fsr 4 native only dlss 4 so yeah I think the 5060 ti is the easy choice here.  In most games they are pretty equal though with about a 4-6 percent average lead for the 5060 ti,Positive
Intel,"I have a 5070ti (Gigabyte) and a 7900XTX (Sapphire Pulse). IMHO, the 5070 has a better pack of features than the 7900. The performance is similar, with a tiny variation in specific games. NVIDIA app has a ton of customization and QoL features that tip the scale for me.   Now, regarding GPUxCPU... Always split more of your budget towards a video card than you do towards CPU. As you climb resolution or graphic configurations, the performance is heavily tethered down to what your GPU can handle.   There's a great chart by Tom's Hardware in the [following link](https://www.tomshardware.com/pc-components/gpus/cpu-vs-gpu-upgrade-benchmarks-testing) that'll be a good representation of what I'm saying.   You can never go wrong with a GPU upgrade when in doubt.   PS: Wait a little for RAM chips to get cheaper. When the AI bubble bursts, hopefully the chips will plummet xD.",Positive
Intel,"I would save a bit on the CPU here (7600(X)/9600(X)) and go for the bigger GPU here. But it depends what games you plan on playing in the future, graphics heavy go bigger GPU, competitive heavy go CPU.",Neutral
Intel,I would recommend 5070ti,Neutral
Intel,"I got the 5700x3d on sale when they were clearing stock, fits the rx9070xt perfectly for 1440p",Positive
Intel,With 16 GB? I'm not sure if that's possible to find this cheap,Neutral
Intel,"Thanks! I found a 5070ti for around $850 and I'm considering pairing it with the 9600x, seems like the benchmarks are pretty good",Positive
Intel,"Heck of a chip, but unfortunately a 5700x3d used costs about what a 7800x3d costs new these days.",Negative
Intel,"If you want 16GB, get the RX 9070, the 5070 has only 12GB.",Neutral
Intel,"Yeah you’ll get more mileage out of that combo than the previous one. CPUs contribute less than GPUs and even less the higher your resolution is. At 1440p or 4k, you’re going to be mostly reliant on the GPU so your cpu doesn’t make as much of a difference",Neutral
Intel,Yeah you had to get it during the cleareance sale last october. I upgraded from a 3600x for 150 dollars,Neutral
Intel,"[https://www.amazon.com/GIGABYTE-Graphics-WINDFORCE-GV-N507TGAMING-OC-16GD/dp/B0DTRC7782?th=1](https://www.amazon.com/GIGABYTE-Graphics-WINDFORCE-GV-N507TGAMING-OC-16GD/dp/B0DTRC7782?th=1)  Am I missing something, I'm pretty sure this is 16 GB",Neutral
Intel,"In 1440p, an x3d GPU will give up to 20% more FPS compared to a 9700x.  It's only in 4K where they're almost dead even, and most people aren't playing at 4k.",Neutral
Intel,Amazing upgrade!  That 5700x3d will last ya 3-5 years easy,Positive
Intel,That says $800 for me so that’s def not in your range,Negative
Intel,"That's the TI version. RTX 5070, RX 9070, RTX 5070 ti, and RX 9070 XT are all 4 different cards.",Neutral
Intel,"Yea that's true, but if I'm downgrading the CPU a bit I might choose this",Neutral
Intel,"You can do what I did and go for the ryzen 7 7700x and pair it with the 5070ti, my limit was actually my monitor as it doesn’t go above 1440",Neutral
Intel,"5600X is a good buy, but for the GPU I'd consider something a bit more beefy if you can swing it. The 5060 Ti 16GB or 9060 XT 16GB are my floor for 'new GPUs' especially if you're going to be using the upgraded PC for as long as you've used this current one - current-gen games are having trouble with 8GB GPUs especially on PCI-E 3.0 systems if you choose to keep your current motherboard.  If you do get an 8GB GPU, grab a used 6700 XT as a holdover, it'll cost less and perform better than a new RTX 4060.  Power supply is fine for a lower-end GPU upgrade, and there's little reason to replace your motherboard if it's working fine. Put that money toward the GPU upgrade.  EDIT: Also, buy the CPU used if you can. If you're paying 'new 5600X' prices, the 5800XT is only a bit more and should perform better enough to justify the price increase.",Positive
Intel,Ryzen 5600 / 5600x + RX 9060xt 16GB / RTX 5060Ti 16GB would bring you a huge upgrade. You could keep the motherboard (but BIOS update might be needed). For this combo 500W PSU should be enough.,Positive
Intel,"How much money can you spend on the upgrade?  With all the parts you are considering upgrading, I suggest you consider going to AM5.  The AM4 platform has served well but upgrades are limited and AM5 is the present and future.  There will be at least one more gen of cpu's on AM5, likely 2 gens (Zen6 and 7).    Ram is expensive now but if you bite the bullet, you'll be on a platform with a bright future and lots of cpu's to choose from.  I would not spend a lot of money on AM4.  Consider moving to AM5.",Neutral
Intel,"You must install the latest compatible BIOS update with your old Ryzen 5 1600 installed. The best, most budget-friendly, and effective upgrade path for you is to go for the Ryzen 5 5600 and focus your remaining budget on the GPU. A used RX 6700 XT often give better raw performance than the 4060 for a similar or lower price point on the used market. Take a moment to install all your games on your 250GB Samsung SSD. If you run out of room, a cheap 1TB M.2 NVMe SSD.",Positive
Intel,Thank you very much.  Do you have any general advice for buying used PC parts?,Positive
Intel,"Do you think a RX 6700 XT 12GB will do fine? I play only on 1080 and I don’t care that much about beautiful graphics, let alone raytraycing. My priority is smooth fluid gameplay and high fps. Its about 110€/$ cheaper than the 9060 16gb",Neutral
Intel,">I would not spend a lot of money on AM4. Consider moving to AM5.  OP has Ryzen 1000 series, upgrading to 5000 will be already a day and night difference for him, no need for AM5.  Although AM5 offers an upgrade path, AM4 upgrade has far better price / performance value compared to whole new system on AM5.",Neutral
Intel,"I did it roughly like that. I got a used RX 6700 XT for 230, a 5 5600 for 110 and a 500GB for around 60, just because I got enough space and dont have that many games. Thanks for the advice with the BIOS! I've heard that aswell.",Neutral
Intel,"Buy from sellers with a decent number of reviews, and /r/hardwareswap is where I get almost all my used stuff anymore.",Neutral
Intel,"I agree that moving to a 5000 series cpu will provide a substantial performance improvement.  My point is if many other parts are to be upgraded, spend the money on a platform with a future.  Moving to a 7600x will provide an even larger performance boost.    If only dropping in a new cpu, perhaps staying on AM4 makes sense.  A move to AM5 is likely to happen at some point, why not now.  Even though ram prices are high, other parts are reasonable and available.  Who knows what price and availability will be in the future..  I suggest spending money on a platform with a future.    It really depends on what OP is planning to spend and how extensively they want to upgrade parts.",Positive
Intel,"5800x should be perfect for a 3080 and GPU usage should be high in most situations.  really maybe only 5% of games could truly max out a modern 8 core 16 thread cpu like a 5800x at 1080p, so to constantly be hitting 100% load to a point of noticeable stutter and low GPU usage doesn’t seem normal.   What are temps and clock speeds like? What are the specs of the entire system? Is XMP enabled?",Neutral
Intel,"Did you update bios, chipset drivers, etc?. There is a 3 generation jump between those CPUs",Neutral
Intel,"What's the temp, and what motherboard do you have? If temps are fine, it might be a VRM issue. You can run Cinebench with hwinfo64 and look for something that's off. High VRM temp or CPU throttle usually means a VRM issue if the CPU temp is fine.",Neutral
Intel,"5800x will  get very hot,  way hotter than your 2600x, it needs a beefy cooling solution so you don't thermal throttle it. Did you use the same 2600x cooler with the 5800x? what temps and clock speeds are you seeing with the 5800x?",Neutral
Intel,"What motherboard do you have, and what are your temps?",Neutral
Intel,Time for a fresh install of windows,Neutral
Intel,Look at the basics.  What speed is it running?  How hot is it getting?  Do you need a bigger cooler?  Thermal paste applied poorly?   All cores running?,Neutral
Intel,"Suspect some conflict in bios setting, the 5800x should be considerable more powerful than 2600x.",Neutral
Intel,The processor new or used? Is the gpu was from before or also new update to the cpu? If yes is the gpu new or used?  Need to know ur mobo model. Probably bios not updated.  Usually all it take is a fresh window to fix it.,Neutral
Intel,BF6 should play well with that set up. Video benchmarks display well on YT. I found this similar user on reddit who repseated the ram. Maybe try that if you haven't.  https://www.reddit.com/r/pcmasterrace/s/MlqxIgPt9z,Neutral
Intel,"It does in BF6. My friend with 5800x and 5070ti has low GPU usage too as the CPU can't push enough. On the other hand with 5800x3d and 5070ti I'm on a high GPU usage and got 40+ fps more. We are both using ultrawide monitors.   That game is really heavily bottlenecked by the CPU, and it looks like the 3D cache brings massive improvements there.",Neutral
Intel,"I have updated the mobo bios, but I haven’t tried updating the CPU driver. I think windows auto installed the driver I will try to do a manual update later.",Neutral
Intel,"I think it sits around 60-80°c (140-176°F). My mobo is a B450-F. Cinebench seems to be fine, but I’m not really sure what to look for. It scored around 14800 for multithread. I can’t really check anything now as I noticed it was only reading half my RAM (8Gb should be 16Gb) and now I’ve tried reseating the RAM it won’t boot to bios, just giving me a yellow/orange led. Right now I’m in the process of reseating my CPU, my RAM and resetting CMOS.",Neutral
Intel,Nah I’ve upgraded my cooler to a Thermalright Phantom Spirit 120 SE. Usually around 60-80°C (140-176°F) with clock speed sitting around 4700 or more.,Neutral
Intel,B450-F. And around 60-80°C (140-176°F),Neutral
Intel,"Urgh, I hate that you’re probably right.",Negative
Intel,"Yeah that might have been it. I reseated the RAM like a dozen times though, after noticed it was only reading one stick. After the first time my PC wouldn’t even boot, I was getting a DRAM issue and ended up tearing my whole PC apart, reseating CPU and redoing thermals, resetting CMOS and it still wouldn’t boot. The last time it finally worked, but only 1 RAM stick and then finally both read. No idea what the real issue was, but likely RAM issue. In my experience it’s always the bloody RAM.",Negative
Intel,"Yeah I’m running my game in 3840x1080, but even windowed at 1920x1080 doesn’t make any difference. BF6 was completely unplayable on the 2600x all the audio started going choppy, and the fps was really stuttering.",Negative
Intel,"Are you saying their 5800x is hitting 100% load at what im assuming is 3440x1440?   5800x in general is too slow for a 5070ti. It’s going to have low GPU usage in basically all scenarios even without maxing out. If your friend had an equally as powerful CPU (to your 5800x3d) like a 7600(x)/9600x, they’d likely be fine too.",Negative
Intel,no it won’t go the the website for your mobo and get all the drivers,Neutral
Intel,"3840x1600 here, while my friend has 3440x1400. In short, same GPU, but he only gets 80 fps due to the cpu, while I am at 120-140. Yes, changing graphic settings has almost no impact.  He ""solved"" it by enabling nvidia framegen. The 3080 doesn't have access to it, but perhaphs the game has FSR framegen you can use? I don't remember.",Neutral
Intel,"His CPU stays at 60%-70% circa in BF6. Generally, he is GPU limited in almost all games - this was the first case in which the GPU had such a lower usage.  Yeah, I upgraded earlier than him when the 5800x3d were still produced/in stock. When he started thinking about upgrades even the 5700x3d was no longer available. However, the 5800x was dirty cheap, so it wasn't a bad deal.",Neutral
Intel,My mobo website just lists the compatible CPU with the minimum required bios version. I made sure to update to the latest version before installation. https://rog.asus.com/au/motherboards/rog-strix/rog-strix-b450-f-gaming-model/helpdesk_cpu/,Neutral
Intel,https://rog.asus.com/au/motherboards/rog-strix/rog-strix-b450-f-gaming-model/helpdesk_download/   This is what i mean you need to update the drivers for chipset etc,Neutral
Intel,"Yeah thanks I got it now, everything is working. I think it ended up being a RAM issue. My PC was only reading one stick.",Negative
Intel,"If you have any plans for a future GPU upgrade, swap the PSU for MSI Mag 850w.  But overall, I don't see a problem with this build. It's solid for 1080p+60FPS gaming.",Positive
Intel,"Overpaying for cpu cooler unless you care about keeping it for potential upgrade but if that was the case you would likely get a better motherboard. The money spent towards case and fans would be better spent towards a case with more upgradability and built in fans like the montech xr or montech 903 airmax. If you don't care too much about upgrading to a higher end gpu in a similar tier to the 9070, you could change out the psu to like an Adata XPG pylon 550w which is actually similar quality but alot cheaper.",Neutral
Intel,Nothing wrong with build,Negative
Intel,"Just posting this comment for future builders: wait until you have enough money to buy everything at the same time. Why?  * So you don’t get stuck with a single component that is hard to find or is having a price spike, leaving you with an unfinishable build * you won’t be able to return first set of components to the store you bought them from, if something didn’t work. Most stores have 14-30 day return policies. Returning to the store is a lot easier than doing a warranty claim with the part maker.  * You can’t test that everything works together until you can fully assemble the PC. See previous point about return policies.",Neutral
Intel,"All looks good. Except be quiets arent really good case fans. Get thermalright cl14s or something else, anything really. If u wanna get a better psu and save somewhere else, just get one cl14 and maybe a cheaper thermalright cooler. I bought a used thermalright gf1 750w for 55 eur, so u can look into that as well.",Neutral
Intel,What is the total allotted budget? And location?,Neutral
Intel,The thermalright royal pretor is as expensive on aliexpress (here in Belgium last i checked) but waaay better at cooling.  I'm using it myself on a 7800x3d and my idle is 30°c and gaming it goes 55°c. I haven't done a stress test yet though. (might boot one up rn)  I would also recommend you not to skimp on the psu but everyone has their reasons and preferences.,Positive
Intel,"Above build is okay.  If your budget allows you to change the motherboard to a B650, then do it",Neutral
Intel,i'd recommend using intel 225f with b860 mobo. no need to buy an extra cpu cooler as the 225f comes with one. overall that'd be cheaper than what you planned and intel platforms are more stable and reliable,Positive
Intel,"First build I’ve ever seen on this subreddit with an intel gpu  Looks pretty good to me, I would change out the PSU, you should be going for gold rated or higher",Positive
Intel,"Dawg, I was already gonna use the 550w version of that PSU, but switched over bc the 650w one was gonna let me upgrade the GPU down the line. My total wattage is like 380 lmao",Neutral
Intel,"Actually, what would be a better mobo?",Neutral
Intel,"800€. I went over that, but I can just about manage with this. I'm in Lithuania",Neutral
Intel,"Well, the PSU is ok, right? Like, it works according to reviews",Positive
Intel,"I think I was clear when I said ""IF"" you want to upgrade.  Upgrade isn't only from B580 to something like 5060 Ti. It could be a 5070Ti or 5080 too.",Neutral
Intel,It's quite a bit more but also a board with excellent power delivery and the feature set and io is improved. It's the b650 eagle ax for 50 more euros.,Positive
Intel,I cant. Due to no option for Lithuania in pcpricetracker.,Negative
Intel,"Thinking about it. It really depends on you.  Are you upgrading next year, go for the 850w one. Also if it is not a big upgrade. You want some headroom and also keep that battery at it's sweet 50% mark.  If you are upgrading waay later like 2-3 years. Just get this one and save up for a good psu  Edit: your psu has most important safety features but is nothing to write home about. With a warranty of 5 years. So you should be fine.  It has everything you need but constraints you in wattage and only one 4x4 connector. (if you plan on upgrading your motherboard for a better cpu)",Neutral
Intel,Look at you contributing to the post,Neutral
Intel,Not the way I woukd word it but he is right. If you buy a good psu it will last you multiple builds. :),Positive
Intel,"Yeah, but if I wanted to upgrade my GPU, I probably wouldn't immediately jump to the latest and greatest",Neutral
Intel,"True, but what is the ""sweet 50% mark""?",Neutral
Intel,more of a contribution than saying “go for gold bro”,Neutral
Intel,"Well sorry that I'm not your childhood friend and I don't know your preferences by default.  Because there people here in this sub who upgrade from 1070 to 5080. And you know it's indeed ""POSSIBLE"" to jump from a low end card to the latest and greatest.",Neutral
Intel,At 50% capacity the psu will run most efficiently and also stay optimal longer,Neutral
Intel,"Well, sorry ig",Negative
Intel,"Each GPU chip is unique. Some are better at overclocking or running cooler, causing stock performance differences.  It's called the Silicon Lottery",Neutral
Intel,What did you need 6 B580s for? AI farm?,Neutral
Intel,"The best card here is about 2.3% better than the worst. I'd chalk that up to run-to-run variance especially with only 3 runs each. I'd be willing to bet some of these individual cards had a similar spread in results.  This is entirely normal. No two cards are entirely identical. One might have a tiny bit better cooler mounting or need a tiny bit less voltage to hit top clocks, and that one will be a tiny bit faster.  If you were to overclock or undervolt and chase a record, that top performer is the first one I would try first, but they are all so close that any one of these could be the technically best one.",Positive
Intel,Any idea on how the temps vary with those scores?,Neutral
Intel,"That doesn't look like too much of a variance to me, I bet nvidia and amd cards would act the same. Not all chips are equal, the silicon lottery determines what you get",Neutral
Intel,Seems within margin of error,Neutral
Intel,pretty close. was thinking of a sparkle b580 as my upgrade as well.,Neutral
Intel,"Neet data. Not many people test a bunch of the same card against itself.  I know with cpus they talk about the ""silicone lottery"" - extreeme overclockers would look for the best CPUs and get a few percent extra performance. I'd guess thats whats happening here.",Neutral
Intel,"Furmark is known for a *lot* of run to run variance. You'd need to do something like ten runs, discard the outliers, and average them.  It's not very useful as a benchmark, doesn't run long enough, which is why you won't see many folk using it as a benchmark: It's a stress test.",Negative
Intel,"So we're talking about about Standard deviation of 78.2 and mean of 9187.6, which means less than 1% deviation from the mean for about 84% percentile of the cards. While sample size isn't to big to draw any conclusions and tests should be more thorough and controlled, this is not high variance.",Neutral
Intel,Average variation +/- 66 with maximum of -113  Max is a 1.22%,Neutral
Intel,thanks. now i can tell if mine is good or bad. wiat wait wait... no ya didn't benchamrk it on a core ultra 9 100series. thats a laptop only chip!,Positive
Intel,"NO ONE NOTICED THE CORE ULTRA 9 ""100"" SERIES?!?! I think he meant 200.",Neutral
Intel,Fucking ai people and their retarded local llm bs,Negative
Intel,"And not just GPUs but CPUs, vehicle engines, electric heaters, gas boilers, light bulbs, batteries etc  Basically anything will have a “tolerance” where the “output” will vary between x and y and is not a constant.",Neutral
Intel,"and also if you get a higher tier gpu then youre guaranteed to get a really good chip out of this lottery, such as gigabyte aorus xtreme cards",Positive
Intel,Yeah I was planning doing some local llm inference,Neutral
Intel,Good question.  I wonder if they are being thermally restrained or power limited.,Neutral
Intel,Very good GPU IMO the sparkle one looks better and clocks higher than stock,Positive
Intel,Hmm ok while I still have the cards not deployed in anything what benchmarks should I do like what exact testing are y'all looking for,Neutral
Intel,Yes I benchmarked it on my mini PC it is a mobile chip,Neutral
Intel,"What I do with my money and my hardware should not upset you. When people say ""AI people,"" we are referring to large corporations and data centers buying up all the GPUs and RAM and using the AI to ruin our lives even more. What **local LLMs** are doing is to bring that power back into our hands, the consumers. The everyday person. Would you like it so that all things to do with AI belong to Google and OpenAI, or would you not want a world where us, the consumers, can fight back with our own models?  if you dont know what to say about a matter its best to enlighten yourself before shitting on something",Negative
Intel,"And also humans, it's called genetic lottery.... Usually pcmr members don't fare too well on that. 😥",Negative
Intel,"Nvidia bins their chips into very tight bins. Board partners aren't even allowed to sell a chip as overclocked when Nvidia deems it to not be, so there's less of a lottery going on than there is in other places.  That being said, there will still be sample variance.",Neutral
Intel,"Why is this guy getting downvoted? Local LLM inference is really cool and you get full control over every part of it. Maybe people are thinking ""AI bad"" but the random hobbyist buying $2k of hardware is not the same as the large corporations buying the entire world's DRAM supply.",Neutral
Intel,Why go for Intel and not AMD or the one who shall not be named? Is Intel best?,Neutral
Intel,what?!? how did you fit the gpu?,Neutral
Intel,"Yep, they have 2 chip models with one permitting factory of and one not. And the ones permitting also tend to get you higher speeds  But for example if you buy an aorus card, chances are you can't go any higher because the chip couldn't make it to be an xtreme model. On the other hand, an xtreme card probably will go up a bit extra",Neutral
Intel,Nuance is too hard for most people.,Negative
Intel,Most pple dont care they are like robots them selves they hear ai and immediately down vote some pple even believe that someone buying a few GPUs directly means they are taking away from gamers people dont care as long they have something to hate on,Negative
Intel,"From what I saw when I was looking, you can get decent amounts of vram for not too bad, I grabbed an intel b580 for my home server cause it was mega cheap and it can rip transcodes pretty ez",Positive
Intel,as others have mentioned currently the best bang for buck is intel for vram and also i got all of these cards for super cheap around 200-240 each also i dont want to support nvidia at all they can go fuck themselves and as for amd there is very little llm support on their cards,Negative
Intel,"that is 6 b580 carts at 12GB each right now they are selling for like 260 USD each at some retailers so that is 1,560 USD all before tax that is.  And you get 72GB vram to use as long as you have a system for using this.  And 3060 12gb are going for about 300-350 and the 5060 ti 16gb is going for about  400-450  then the 9060xt 16gb is going for about 380-450.  so the 3060 12gb is out, it is way to much  Then lets say he got 5 9060 XT 16gb at 380 that's 1900 and that would be 80 GB of vram or 4 cards at 1520 with 64GB of vram.  Then their is the 5060 ti 16gb it would be even more.  With Vulkan API working with different LLM stuff it is easy to use just about all cards now, aside from the compile process if you want to get maximum performance. Other wise LMStudio will get you going with Vulkan fairly decently.  EDIT: My mistake you can get the B580 right now for 239.99 from B&H photo.",Neutral
Intel,Nvidia tends to be the most performative for AI but also most expensive. Intel is the best bang for buck if you're going for high vram counts. It'll be slower but at least the model fits. Amd isn't really a player in the AI space yet,Neutral
Intel,How about an EU chip since you declared people should be independent of american ones hmm,Negative
Intel,beelink gti 14ultra look it up it has a full pcie slot,Neutral
Intel,"It is hard to predict how chips will overclock. We've had generations where CPUs were apparently binned very conservatively, so even way down the range chips would overclock like crazy (Intel Core 2nd gen) or allow you to unlock whole extra fully functional cores (AMD Phenom triple cores), but we've also seen generations where most headroom was already 'priced in' (Intel 13th/14th gen) or arguably even exceeded. We've seen generations with more and less headroom on the GPU side as well.  That being said, now that consumer GPUs are competing with AI chips, they'll want to squeeze every inch out of each wafer, so I suspect they'll bin things as tightly as possible.",Neutral
Intel,"Yeah thats a fair point, some generations can be liklier to overclock than others",Neutral
Intel,"The B770 feels like it is going to disappoint people. People want a 5070 TI competitor, but it seems more like a 5070 competitor, which could be great for the market.   Their tech suite is starting to feel a bit dated though. Frame gen is fine, but no RT denoiser like AMD and Nvidia is something that could use resolving, but the biggest is XESS XMX slipping into a relatively distant fourth place behind DLSS transformer, FSR ML, and DLSS CNN. A new transformer based architecture could really deliver a boost they need.    As for panther lake it is promising. An efficient yet reasonably powerful architecture with ML hardware for XESS is quite compelling in the gaming space with laptops and handhelds, but also low power laptops. Great encoding and decoding with WiFi 7 is quite nice too. Seemingly it will deliver better performance than stuff like the z2 extreme in the handheld market which is something we desperately need since it was such an incremental jump. Hopefully more big manufacturers consider Intel.   Nova lake is a lot of question marks right now, but it might be reasonably competent which is what we need from Intel. Even if they can’t match the 10800x3d or whatever they might be really viable competition for something like that 10700x or if they managed to do a really big jump the 10950x3d mega chips. I really want to see Intel succeed.  It isn’t good that AMD are starting to creep into the Nvidia style position for high-end CPU. Much like Nvidia in the GPU market has basically sole dominance over the 5080 and upward calibre of product. AMD is kind of dominating when it comes to their x3D processors for high-end gaming performance. That is never a position you want companies to be in.",Negative
Intel,"There is a terrible situation in the CPU's, AMD is dominating, and I will never trust Intel while having the government funding them publicly. Wish Nvidia did something cool in that, but I doubt that, they will probably make some shady movements there too.",Negative
Intel,"You might not trust intel, but I don’t know many people who would care about government subsidies and funding if they offered good value for money CPUs.",Negative
Intel,"If they could hit close to 5070 performance and price it at $399, they might just go from 1% to 2%!",Neutral
Intel,holy shit. these cards are not cancelled?  are they still coming?       theres still hope for Celestial?  https://preview.redd.it/daxvcoxft06g1.jpeg?width=300&format=pjpg&auto=webp&s=8b3dba05fcdcfb82799679baccf7b3c3b05073f6,Negative
Intel,"Over 50% more power, so it should be over 50% more faster.  So slightly faster than the 5060ti and 9060xt.",Positive
Intel,"Spicy.  Nice to see a unit with some power.   Really, their target only needs to be match the 5070/5080/9070XT range.  If they can do that and actually get volume built, they'll sell a ton.   I've other very interesting thing they could do is go heavy on vram, not because it's needed for games but because it's needed for bigger at home AI models.   Drop a 32gb just for fun, or larger and let people run bigger models than can even fit on 4090/5090 cards.  Create a cheap alternative to the pro cards.  Don't even worry about speed.  Just gotta fit model sizes.",Positive
Intel,"Lotta people hoping for 5070 performance who will be pretty disappointed. I'm not saying this will be 5060Ti level necessarily, but probably best we can hope for is something between 5060Ti and 5070 performance but with 16GB of VRAM, in the $350-$399 range.",Negative
Intel,Going to be a very interesting card. In the age of expensive GPUs...,Positive
Intel,"At that TDP it better be very, very close or better than RX 9070 XT... Ok, fine RX 9070, non XT.",Positive
Intel,Hmm for 300watts isn’t that close to 9070xt and 5070TI power usage? Does intel in general use more power? Just from efficiency standpoint and power usage that seems where it may land unless it is power hungry and inefficient compared to competition? Any info is appreciated. Trying to line this up as well 👍.  Either way glad to see it’s finally coming out. I wish they announced it sooner though would have tried to get it over the 9070XT to be honest.,Neutral
Intel,I’ll be watching this with interest. I love my b580 and I’ve had 0 issues with it regardless of whatever crap about them gets spouted online. Never had a crash or driver issue related to the card and the driver update have been coming hard and fast bringing improvements   If these are priced right I’ll be upgrading and replacing my wife’s 3060 with my b580,Positive
Intel,Motherfucker I just bought a B580 because I thought this thing was a myth!,Negative
Intel,I’d sell my 4070s to support them if they can hit the same amount of performance level. I know sounds redundant as hell but need to have more competition in the gpu space,Neutral
Intel,"Why do I suspect that even if this exists, it exists in the area of the curve where power usage explodes and performance stagnates?",Negative
Intel,Gross.  You guys know higher tdp is bad right? It’s literally a measure of power consumption.,Negative
Intel,"The B580 is 4060 performance at almost 200 watts, it's probably gonna be 4070 levels.",Neutral
Intel,"""5070 Performance!""",Neutral
Intel,Just from sales standpoint 1%->2% means 100% more sales. Which is allright ?,Neutral
Intel,They can make it $150 and it still won't increase market share.,Neutral
Intel,Well if Nvidia pulls a crucial and stops selling consumer cards then Intel has a shot!,Neutral
Intel,"At 300w TDP it better be, but definitely not at that price.",Neutral
Intel,"Can't wait for first qtr of 2027 to see them!  Honestly quite annoyed with this launch, they are seemingly having issues at this point. Battlemage has been out for a whole year with no news, only leaks of the 700 series cards.",Negative
Intel,"Judging by the late timing maybe they were ""uncancelled"" =p",Neutral
Intel,"That's assuming performance scales linearly with power consumption, I'm not saying it doesn't but that's not really something we can verify without some hard data.",Neutral
Intel,Oof I'm hoping around 5070,Neutral
Intel,I'll take that. The speed bump that comes with fitting everything in vram for AI outclasses raw power by a mile,Neutral
Intel,With the price of memory right now dont count on it.,Neutral
Intel,Honestly if it’s 399 and hitting 5070/9070 performance. I’m buying in a heartbeat,Positive
Intel,"Not to ruin your day, but selling your 4070S just to ""support"" them is IMO a stupid decision.  The support you ""provided"" is completely negligible, literally a water droplet in a lake. In exchange, you get a far worse experience in your performance, software, power efficiency and compatibility.  All of that just so that you can feel right in your own PoV lol. Selling it doesnt change the fact that you already gave Nvidia your money.  You will be of bigger help by just keeping your 4070S and instead recommending Intel GPUs to your friends and other people who want to build a new PC (assuming Intel nails the pricing right). Spread the word instead, and dont sabotage yourself.",Negative
Intel,But with TDP performance goes in pair. NVIDIA nailed it and they are low TDP high performance. Thats why they are 90%. But to get some traction you have to compete in performance much more then in TDP. Thats what most games care for - FPS.,Neutral
Intel,I doubt you're the only person here who was smart enough to put that together u/flatroundworm.,Negative
Intel,4070 and 5070 are extremely close.,Neutral
Intel,"""4090 Performance!""",Neutral
Intel,"Why not? As others have mentioned, their market share is already successfully increasing. There's no reason to think a new desirable GPU wouldn't continue that trend.",Negative
Intel,"Exactly.  If they were smart, they just build what others don't and build something no one can currently compete with.  Build big and take the sales.   There are three challenges.  One, it needs to be good enough to be on par with the 5070/5080/9070XT.  It needs to be a viable alternative, not 30% worse but on par, however they need to do it.  Two, they need to build a feature no other manufacturer has.  This is the vram part.  Go big.  Make it do something no one can compete with for a period of time   Three is the bigger challenge, but it might or might not matter to certain people.  This is cost and efficiency.  If the cost can be competitive to the 5070 and 9070 XT, they will get considerable sales, not like 1%, but as many sales as they can build for.  The second part efficiency.  If they can be highly efficient, they could market a budget option for AI use, not necessarily fastest but efficient for the work load.  This too could promote a significant sales volume just because it costs less at scale.   But, if very little is as achievable, you can always have a feature no one else has, and you can still sell quite well.",Neutral
Intel,If it's 32 Xe cores (up from 20 on the B580) that'd likely put it somewhere more like a 6800XT or 5060Ti :(,Neutral
Intel,It prevents someone else giving Nvidia their money while propping up intel - if more people did it like this mad lad it would make a difference,Neutral
Intel,"I bought the card used lol , no way I’d pay over msrp . But your statement stands true .",Neutral
Intel,It’s closer to 4070ti. It’s a 23% improvement over 4070. Putting it a little slower than 4070ti but faster than 4070S.,Positive
Intel,"""4100 Performance!""",Neutral
Intel,"It doesnt change the fact that the 4070S was already sold. Nvidia already got their money from that unit. Selling your 4070S DOESNT RETURN what you already gave to nvidia's pockets lol.  You can think of it as **paying for someone** else to get a 4070S.  That is the thing that people like dude above is missing.  In the end, you just sacrificed your own experience for nothing. Its a ***Lose Win*** situation.  I aint gonna be stupid to sabotage myself just for a temporary feeling of being right and morally superior, a feeling that I did the right thing, because in the end, its inconsequential, and I just fucked myself in the process, and what is waiting for me in the end is just regret lol.  You might as well ***keep*** the 4070S, keep the better perf, software, efficiency etc etc and ***just focus on preaching*** to people to buy Intel GPUs (assuming its good value). ***Thats a win win scenario.***  Also, you dont want to mindlessly give Intel your own money. You want them to earn it. Buy Intel GPUs not because you want a 3rd player, but because you know its a good product and helping out a 3rd player in the GPU market is just a nice bonus.  If Intel half assess the B770's pricing and launch, and you still bought it just ""to support the underdog"", you are just incentivizing Intel to continue half assing their approach",Negative
Intel,"Damn I was thinking mobile, those are like 10-15 percent",Negative
Intel,"Private profile, shilling. Jog on buddy, no one knows what the card will be like.",Neutral
Intel,"You keep repeating the idea that “the 4070S was already sold, so nothing changes,” which tells me you’re stuck in the sunk cost fallacy. Nvidia getting money from the original sale is irrelevant to the next decision. What matters in economics is marginal demand. When someone sells their 4070S and chooses Intel for their next GPU, they remove one potential new Nvidia sale (the buyer of the used card may otherwise have purchased new) and simultaneously create one new Intel sale. Those marginal shifts are exactly how incumbents lose revenue and challengers gain market share. Sunk revenue doesn’t grant Nvidia permanent immunity from switching behavior.  A strong second hand market absolutely hurts Nvidia’s future sales. It lowers the effective cost of GPU ownership, pushes price sensitive buyers into used units instead of new ones, and reduces Nvidia’s pricing power. This is standard industrial organization theory used goods cannibalize new goods. Selling the 4070S does not “give Nvidia money,” it gives someone else a path to avoid paying Nvidia at all. Then replacing that card with Intel means Intel gets fresh revenue while Nvidia gets zero. Combine both actions and the net effect is Nvidia loses one new sale, Intel gains one new sale. That is the literal mechanism by which competition grows.  And pretending that “one person doesn’t matter” ignores how collective marginal actions scale. If even a small percentage of consumers switch from buying new Nvidia cards (~€600 on average) to Intel instead, the foregone revenue becomes massive. For example, 100,000 people doing this is €60 million Nvidia doesn’t get and €60 million Intel gains. That’s not “inconsequential”, that’s enough to change pricing strategy, product roadmaps, and R&D priorities. Markets don’t change because one individual defects they change when many individuals do, and that only happens if people stop rationalizing inaction with the sunk cost excuses you’re using.  Your whole argument hinges on a single transaction worldview, as if each GPU purchase is isolated. But the GPU market is dynamic. Nvidia’s future behavior depends on demand elasticity, competitors’ traction, and consumer switching. Selling your Nvidia card and buying Intel is a strategic choice that weakens incumbent dominance, strengthens pricing competition, and signals that alternatives are viable. Yes, you shouldn’t blindly buy Intel if the product is bad but when Intel is competitive, shifting demand is exactly how you incentivize innovation and better pricing across the board. In other words you don’t “sacrifice yourself,” you just refuse to keep subsidizing the monopoly.  In short, your logic collapses because you ignore marginal effects, second hand market cannibalization, collective action, and basic microeconomics. The 4070S sale is sunk the choice of what you buy next is what actually shapes the market.",Negative
Intel,"So what does a private profile got to do with it?  Or did you just ignore all my points.     The fact that said the word ""shilling"" is clear indicator you didnt understand what I said.  Did you want to go full Ad Hominem Fallacy by using my own profile against me while not doing anything with my statements LOL?",Negative
Intel,"Give it time and that guy will switch it up and come back with ""consumers don't matter anymore to Nvidia, only data center"". These corporate bootlickers are very narrow minded.",Negative
Intel,"The fundamental point you have is that a hypothetical card will be worse than the 4070 super, which is silly to say because we just don't know.",Negative
Intel,"Which still doesnt change the fact that itll be less efficienct and have weaker software. The 4070S draws 220W, and has access to a multitude of features at far greater availability and support, that is Nvidia's biggest strength that even AMD stans wont deny.  It still doesnt change the fact that you already gave money to Nvidia.  You could use your own statement against OP and yourself. Why would you sell your GPU when we dont even know how good it is? Lol. You are dumping a good card for an unknown one.  And I already said the following:  >You might as well ***keep*** the 4070S, keep the better perf, software, efficiency etc etc and ***just focus on preaching*** to people to buy Intel GPUs (assuming its good value). ***Thats a win win scenario.***  >You will be of bigger help by just keeping your 4070S and instead recommending Intel GPUs to your friends and other people who want to build a new PC (assuming Intel nails the pricing right). Spread the word instead, and dont sabotage yourself.  Tell me, wheres the shilling here?",Neutral
Intel,are you buying in the US?,Neutral
Intel,Nah he said he's in Canada in the post,Neutral
Intel,Nice find. That would be my budget build choice and I'd snap it up at that price!,Positive
Intel,"Saving for a 9060XT is best, it's usually between 250-300 for the 8GB model. It's better than a 5060 at least. And on older platforms AMD will perform a lot better due to their lower CPU overhead, as well as this the B570/B580/5050/5060/5060TI all have PCIE X8 interfaces, which doesn't help especially on PCIE 3.0, especially when running out of Vram, the 9060XT is the only current generation X16 low end card from any of the three.  Go used below that, there's really nothing compelling below and even at that pricepoint you could probably get something better used or continue saving for a 9060XT 16GB since the extra vram will help it last a lot longer although it's a lot more money.",Positive
Intel,"I would personally look for something with a wider bus width than 128-bit.  That being said, maybe a 6700xt or a 3070 TI.  The 10400 is going to bottleneck a ton of cards.",Neutral
Intel,Very informative post. Thank you. Think I will save for the 16gb model.,Positive
Intel,But if I'm playing on high/medium settings no higher than 1440p with a max fps of 60 (my TV) the bottleneck shouldn't be too bad right? I realize my stuff is older but I feel like I can still get a pretty big bump upgrading the gpu.,Neutral
Intel,you should be fine at 1440p.  I would not really worry about getting the latest and greatest card though.,Neutral
Intel,"I know Ram is crazy expensive right now, but you'll seriously lose a lot of performance running only one stick of ram in a Ryzen system. AMD CPUs don't like running single channel. Other then that congrats and welcome back!",Negative
Intel,I lack the funds to buy hardware.,Negative
Intel,That GPU doesn't play well with that CPU my brotha.  Those Intel Battlemage GPUs don't like AM4.  Perhaps try switching to a 13th or 14th gen Core i5 and a cheap DDR4 motherboard.,Negative
Intel,Always something new to learn. Is it gonna bottleneck the system badly or I could buy another module in a couple months?  Thanks mate!,Negative
Intel,It’s tough for most people nowadays. Hope you are able to soon.,Negative
Intel,Why exactly? Rebar issues?,Neutral
Intel,![gif](giphy|eRtdjiQ07r2rm2vik2)  There’s no way I can find an i5 in the price range of the r5. I’ll let you guys know how’s it going in a couple days. Wish me luck and thanks for the heads up.,Neutral
Intel,"Not the end of the world, but surely dual channel would be a lot better for performance. I'm not sure if it is a good idea waiting, consindering the actual ram pricing issues, but your on ddr4 and on a budget, so it should be ok. Enjoy your build!",Positive
Intel,"Good to hear.  About the RAM…I know a guy, so I’m probably gonna be fine.",Positive
Intel,"Remember that AM5 is not your only uograde option. Depending on the type and quality of your motherboard, you can upgrade all the way up to a 5700x3d or 5800x3d, both of which are beasts for gaming. Or you could always try to trade your 5600 for a 5600x or 5700x if you don't want to invest too much into it. CPU power matters more in lower resolution and high refresh rate scenarios, but at 1440p your current set up still has plenty of life left.",Neutral
Intel,"5600, 32gb ram, and 5070 will be a good gaming pc. You should be gaming nicely. Much better than A750!",Positive
Intel,"this will work perfectly, you dont need a new cpu to handle a new gpu, the one you already have can handle it perfectly fine.",Positive
Intel,9070 is much better,Positive
Intel,"Lower end AM4 motherboards have only PCIe 3.0 slots, syou might want to check that.",Neutral
Intel,"Thank you guys, I'm a technician, have been for years, did my own research and its all good but then, everywhere you look at the Internet its someone telling you, you need the latest and greatest for something new to work 🙄 Im gonna put it down to mass obsession from those that can afford very expensive rigs 😂",Positive
Intel,"""Gift""",Neutral
Intel,"I have pci4, thanks though 😀",Positive
Intel,"yup. Influence the gift, after all you're partially funding it as you say.",Neutral
Intel,"No, it was bought like I said and after I paid back into a bit  because I thought it was too expensive to spend but its whatever as I said gifted and I'm grateful for the gift.",Neutral
Intel,"I used to support Arc GIS where I work. I've since moved to a different area of support, but back then, we used what were classified as engineering laptops. It had 32gb of RAM, 8-core CPU (Intel I7 11850H). Mind you, that was back in 2018/2019. Not sure what they recommend now. But, with that machine, it ran fine. But Pro might require more resources, but my company at the time had just started to use ArcGIS Pro as opposed to ArcMaps, etc. But they were using the engineering laptops at that time with what I stated. So maybe you might be good if that's what you have? I would say these days, that's probably the bare minimum.",Neutral
Intel,I could run gis well on an old ass laptop while only running gis and made due on a lenovo thinkpad before. The later confused the teacher so much but it did work. So ngl your computer is probably more than fine. Save often either way.,Positive
Intel,Refer to ArcGIS's [system requirements](https://pro.arcgis.com/en/pro-app/latest/get-started/arcgis-pro-system-requirements.htm). You probably should've checked *before* buying your computer.,Neutral
Intel,"That’s one thing I learned in class, save as often as you can. We switched from Arcmap to ArcPro, and our desktops that we have struggle with the switch. So, I built I decent gaming computer just for this reason.",Neutral
Intel,"Eh, I’ve built my computer to specifically to be upgraded just in case, if it can run most high end games I’m sure I could be fine. Just wanted to see what people have used ✌️😗",Positive
Intel,Can’t talk for arcmap. We only used pro. But honestly i don’t even know what makes the difference at this point- like in the computer labs some machines were always wonky and for the life of mine it doesn’t make sense that a 10 year old lappy worked better than some of these lab desktops. Pure madness.,Negative
Intel,"You don't mention the PC actually turning off of or crashing. does it crash or not? the only problem you mention having is Arc Raiders logging you out back to the main menu which is not a hardware issue outside of potential Internet problems. A hardware issue would either crash the game, the OS, or shut off the PC, not send you back to the main menu of Arc Raiders.",Negative
Intel,"youtube is not any kind of stress test.   run a ram stress test tool, or pull the ram and test each stick (or half in various configurations)",Neutral
Intel,"Make shure you airflow is not obstructed, if your GPU or CPU temps is high - repaste it.",Neutral
Intel,Yea it was fully turning off but instantly turning back on. I reinstalled drivers and the game seems ok for now but ima clean up the pc a bit and get some new thermal paste going soon,Positive
Intel,Plan on doing that soon!,Neutral
Intel,It may be your GPU or CPU shutting of before reaching a higher temperatures. I had the same. Had to throttle the GPU and undervolt it.,Neutral
Intel,![gif](giphy|14gHyPIqhVDJLO),Neutral
Intel,At first I thought this was the back of the PC… what a mess.,Negative
Intel,This is why fire insurance is hard to find in California,Negative
Intel,Dear god,Negative
Intel,The GPU bend and the open PSU without a fan right above the GPU exhaust make me uncomfortable.  Was there no way to swap to a SFF PSU and avoid all of this gore?,Negative
Intel,Mods should ban this guy from the sub immediately.,Negative
Intel,https://preview.redd.it/9chjhodc0m6g1.png?width=405&format=png&auto=webp&s=4f66666be2dc5eb91fa9ec8cd9c1ea3e983110ef,Neutral
Intel,what in the fuck,Negative
Intel,"Post has to be rage bait. That card is going to die, or something is going to catch on fire.",Neutral
Intel,![gif](giphy|iH2IldVkqeLuJ7eJ0L),Neutral
Intel,"Just because you can, doesn't mean you should",Negative
Intel,God that poor GPU,Negative
Intel,Make sure you have renters/home owner insurance that covers negligent fire damage.,Neutral
Intel,"No, I don’t think that you did.",Negative
Intel,Is... that the power supply on the right..?,Neutral
Intel,good documentation for the future house fire,Positive
Intel,That power supply isn't lasting long without a fan.,Negative
Intel,![gif](giphy|5qoRdabXeT4GY),Neutral
Intel,can't even tell wtf I'm looking at,Negative
Intel,![gif](giphy|6Uqr0IDWkzhBu|downsized),Neutral
Intel,This will probably blow up.,Neutral
Intel,"There is a reason that PSU's have shrouds, and WTF are you doing to your GPU? Like dude if you can't do something right don't do it at all.   This is nasty and worse yet dangerous.",Negative
Intel,"Please, try to enclose that PSU a bit, these capacitors could easily kill you or give you a painful shock",Neutral
Intel,I love these posts. A real morale booster for my own building skills. Thanks op!,Positive
Intel,"get a new case mate, this is horrid",Neutral
Intel,SIR AND OR MADAM:  YOU ARE NOT APPLE  YOU CANNOT JUST REMOVE THE SHIELDING FROM THE MOTHERFUCKING POWER-SUPPLY AND EXPECT NO CONSEQUENCES! (said unshielded power supply in the iMac Pros also made removal of the screen EXCEEDINGLY dangerous),Negative
Intel,Hey pal.   Delete this before I do.,Neutral
Intel,Please tag as NSFMR.  I thought this was the back side…,Neutral
Intel,Not even Ai could be this bad.  ![gif](giphy|dw7lCpFmsyfS0),Negative
Intel,I've gone blind,Neutral
Intel,Rip your future self when this thing either catches fire or that gpu dies. Yikes man,Negative
Intel,That gpu sag gives me anxiety...,Negative
Intel,Everything hurts when I look at this,Negative
Intel,A full size case is like $40. Please for all of us.,Neutral
Intel,What in the actual fuck? That's asking to short/burn/explode. Especially with the naked psu.,Negative
Intel,The secondary side of that PSU ain't gonna last long with zero airflow and CrapXon caps.,Positive
Intel,![gif](giphy|G5JoAjEBtfoTm|downsized),Neutral
Intel,I don't know how someone has the interest to be on this subreddit and yet also think THIS is okay.,Negative
Intel,![gif](giphy|iGAXf0OlUUMYo),Neutral
Intel,Post an update when your home burns down.,Neutral
Intel,is there a r/pcgore or something? because this belongs there,Neutral
Intel,https://preview.redd.it/45a3nv520o6g1.jpeg?width=1079&format=pjpg&auto=webp&s=b245e7b4662cada3aebf067b617ea8e72675bb68,Neutral
Intel,Is that electrical tape? Holly mother of pearl. Mr. Crabs.,Neutral
Intel,![gif](giphy|13ONldD8IuWBGM),Neutral
Intel,Just dont touch the PSU capacitors and you'll be alright - maybe.,Neutral
Intel,"Bro what are you doing, the GPU isn't in the correct slot.",Neutral
Intel,going without a case might have been the better call here...,Neutral
Intel,typical alienware build,Neutral
Intel,"Me and OP were stood next to a burnt down house. With this PC and a hand with electrical burns, and still weren't found out.",Negative
Intel,https://preview.redd.it/i9maivrrim6g1.jpeg?width=1169&format=pjpg&auto=webp&s=234209e8fa44f0790cb4da2956f112ee6957481e,Neutral
Intel,"OP, this is awful, you will kill your GPU overtime.  And GPU has to be in first x16 slot closest to the CPU, that's not debatable. If you can't, you HAVE to get a new case.",Negative
Intel,Troll I guess,Neutral
Intel,Modern art masterpiece,Neutral
Intel,"Ooofff, sag, wrong spot, cooling...all gpu nonos.... Time for a new case wow.",Negative
Intel,"This post sent me on a journey.   Read the title, then looked at the photo: Happy to see there was some type of cable management, I assumed it was worse before and this was some sort of redemption post. Wild how at least my brain was filling gaps to the story.   ... Then I looked at the bottom of their tower and saw a GPU. Here is where I got really confused and really worried. Thought to myself this has to be a rage post. FINALLY realized this was the front of the board instead of behind. Amazing, thank you for the ride, and especially these comments.",Neutral
Intel,"Everything I see in this picture is wrong, but somehow I love it",Negative
Intel,"What, and I can’t stress this enough, the fuck",Negative
Intel,https://preview.redd.it/mmoncs7x4n6g1.jpeg?width=194&format=pjpg&auto=webp&s=729b8db50b34069e8b218bea9f00cd78f209d697,Neutral
Intel,Thank you for making me feel better about my own builds.,Positive
Intel,https://preview.redd.it/lo8nurj3en6g1.png?width=482&format=png&auto=webp&s=256d347922f8cf12705c622153b233815600fad5,Neutral
Intel,Looks like half the garbage that comes into my workshop.,Negative
Intel,"There is a reason for all the warning stickers on PSU's, even once the computer is off those chunkier capacitors can remain  spicy for a while.",Negative
Intel,"The hell. Im broke, but even i want to buy you a new case!",Negative
Intel,"That PSU should be replaced with a new one. I am not trying to be funny, but you put yourself and others around in danger with that naked PSU.",Negative
Intel,hey buddy what the fuck,Negative
Intel,https://preview.redd.it/wi1d624nup6g1.jpeg?width=640&format=pjpg&auto=webp&s=4164c9435980c2b6ae272a5111f9b150321532e2,Neutral
Intel,https://preview.redd.it/pyhenj1r0q6g1.png?width=1080&format=png&auto=webp&s=46128495d14592b05cff9a37a4a9e7d7f2725b18,Neutral
Intel,https://preview.redd.it/pfqqpt9o4y6g1.jpeg?width=1125&format=pjpg&auto=webp&s=4fa470ae307117b7d74f57c00c713b45ce9e7137  (Uncropped for aesthetics),Neutral
Intel,https://preview.redd.it/k36ta6e3u17g1.png?width=1440&format=png&auto=webp&s=8af3b3420e9aad542e6cd636e9179aaaa37c2730  I'm still processing too many wires all over the place The GPU looks sagging  Wait what's that I circled? Is that a PSU!?,Neutral
Intel,Perfection.,Neutral
Intel,![gif](giphy|17RaL7HOgI1CE),Neutral
Intel,![gif](giphy|q6M5DgteMilEc),Neutral
Intel,I see non-perpendicular lines. NON-PERPENDICULAR LINES.  Seriously though - that looks a tad too stressed to last you long.,Negative
Intel,This looks like team 17s attempts at trying to fix hell let loose,Negative
Intel,Looks great!,Positive
Intel,Why?,Neutral
Intel,Rage bait,Neutral
Intel,https://preview.redd.it/6yuiqk6yum6g1.jpeg?width=1170&format=pjpg&auto=webp&s=1156d3d7fdfbe0f9b8e3b6e7b15debcc0d8fbb01,Neutral
Intel,![gif](giphy|xvRQ0ag9DhIffjs2wv),Neutral
Intel,![gif](giphy|ep78UZy5FVbfN6mhCU),Neutral
Intel,"That arc card is a great one, had it till I was saved up enough for a 9070xt then sold it to my friend",Positive
Intel,This is how things catch fire. Also that psu looking super sus,Negative
Intel,I somehow have trouble breathing after seeing this,Neutral
Intel,Cable management my friend..... Cable management.,Neutral
Intel,https://preview.redd.it/qei8osb9wm6g1.png?width=853&format=png&auto=webp&s=595c810e5397fdb0e74dd2df5d2fffa0b08cf6f8  The ad came in clutch the perfect time lol it was in comment 🤣🤣,Positive
Intel,https://preview.redd.it/01pjnxnkwm6g1.jpeg?width=1170&format=pjpg&auto=webp&s=4a5d8796d2b73b848665d338ff85aa9c02b04fdd,Neutral
Intel,Looks really bad my friend.,Negative
Intel,That GPU is fighting for any space down there,Negative
Intel,https://preview.redd.it/y2camvqxwm6g1.jpeg?width=1169&format=pjpg&auto=webp&s=0c751673a001a49f4843b599b2593b017ee0811c,Neutral
Intel,That look and sound naughty,Neutral
Intel,Wtf,Negative
Intel,If sits it fits .,Neutral
Intel,is this a car,Neutral
Intel,Cursed image,Neutral
Intel,https://preview.redd.it/0i23ru9lym6g1.png?width=723&format=png&auto=webp&s=7596053e595adbfc16eb02b879e67d8b9be9c275,Neutral
Intel,GPU sagging harder than Soulja Boy in 2008,Neutral
Intel,Good arc on the Arc,Positive
Intel,What airflow?,Neutral
Intel,That looks like my old setup,Neutral
Intel,Dude just buy a prebuilt next time lmaoooo. That psu one day is going to short the fuck out and that pci-e slot is prob gonna work for 6 months then stop working.,Negative
Intel,Looks good.,Positive
Intel,![gif](giphy|l9FAwJOXYprFe),Neutral
Intel,Yikes!,Neutral
Intel,![gif](giphy|ukGm72ZLZvYfS),Neutral
Intel,What the fuck,Negative
Intel,One way or another this thing is catching fire,Negative
Intel,GPU sagging like an old woman,Neutral
Intel,I mean come on. Cases are so cheap. This is ridiculous.  Edit: I just see now seen the psu open and exposed. Wtf are you actually doing??,Negative
Intel,...what is happening??,Neutral
Intel,That...that's not optimal.☹🤦‍♂️,Neutral
Intel,"This is unsafe, and you're a dumbass.",Negative
Intel,https://preview.redd.it/33poilaa2n6g1.png?width=1344&format=png&auto=webp&s=4e5a67b59cf45b129dbfe0fde125f1e466ed9333,Neutral
Intel,"You're going to reach into your pc one day, touch the wrong part of the power supply and kill yourself.  Whatever the fuck it is you did, undo it immediately.  Putting all your parts into a cardboard box is better than this.",Negative
Intel,https://i.redd.it/jl3r76zf2n6g1.gif,Neutral
Intel,"In 2 weeks  ""Guys I built my first PC but it just died on me. I did everything right but its all ""insert manufacturers name"" fault for selling bad parts""",Negative
Intel,"Either your house burns down or you die, there is no inbetween",Negative
Intel,![gif](giphy|U8MKkup2H6QOttBLkh),Neutral
Intel,"Would be better off with no case.  OP lay all this out on a flat surface, point a box fan at it and enjoy.",Negative
Intel,THE GPU SAG HOLY,Neutral
Intel,https://preview.redd.it/c49vtvdy3n6g1.jpeg?width=1077&format=pjpg&auto=webp&s=bc036e0bf62e3d03fc9521bdbb58932eecfd4a42  Homie fix that ASAP💀,Neutral
Intel,"Just because you can, doesn’t mean you should.",Negative
Intel,![gif](giphy|R5m5vZcaOr4l2TWFIK|downsized),Neutral
Intel,Wtf!,Negative
Intel,This is traumatising,Neutral
Intel,![gif](giphy|JUIYjVeZPHxjWR7rmX|downsized),Neutral
Intel,what a shit,Negative
Intel,Airflow will be great in sure…,Positive
Intel,Is your power supply open????????,Neutral
Intel,"Mods can we remove someone’s building rights, is there a way we can make OP only buy prebuilt from now on",Neutral
Intel,Airflow = zero.,Neutral
Intel,open psu is the cherry on top for me,Neutral
Intel,It ain’t right,Neutral
Intel,![gif](giphy|ukGm72ZLZvYfS),Neutral
Intel,https://preview.redd.it/1pj24tf5an6g1.png?width=315&format=png&auto=webp&s=eb2cde356b05696f732e5e232ed2c75dec53faf5  There are no words to describe the horrors I'm actively whitenessing.,Neutral
Intel,This is a mess.,Negative
Intel,This angers me in ways I haven't been angered before,Negative
Intel,How's the thermals?,Neutral
Intel,Wtf were you thinking?!,Negative
Intel,WTF happened with the PSU?,Negative
Intel,"I was going to ask what the heck is that thing in the front with all the cables. Then Iit hit me seeing the capacitors and where a cable went.  Dear God. There are some evil things in this world. But you, evil can't touch you.  I think I would have settled with a no case pc until I got a better case. And PSU.",Negative
Intel,Air flow?   Never heard of her.,Negative
Intel,not enough space for that GPU to sag further.,Negative
Intel,So.evkdy will soon receive that gpu for repair. My goodness.,Positive
Intel,![gif](giphy|BkfAhfmX0Ppn2),Neutral
Intel,Looks good,Positive
Intel,N ot gonna lie mine kinda looks like this lol. Need a bigger case.,Neutral
Intel,OP ditch the AiO and go for an air cooler.,Neutral
Intel,Did you cut up a fullsize power supply to fit it into a micro itx build? Why not Take Something smaller Like a Bequiet sfx3 that fits without exposing highvoltage parts to dust and debris?,Neutral
Intel,"The gpu? Yeah it’s gonna be awful toasty , look at the psu!!!🙄",Negative
Intel,But… why?  This seemed way more complicated than buying proper case for the components.,Negative
Intel,The longer you look the worse it gets,Negative
Intel,https://preview.redd.it/4f2h54czjn6g1.png?width=457&format=png&auto=webp&s=ef7e361ec8a97213fc34de9a4acdd3834923ee0a,Neutral
Intel,"The longer I look, the worse it gets…",Negative
Intel,hell yeah! DIY power supply!,Positive
Intel,What is all that on the right side? This is making my brain hurt just looking at it.,Negative
Intel,![gif](giphy|ukGm72ZLZvYfS),Neutral
Intel,What the fuck is this?,Negative
Intel,🧢,Neutral
Intel,Dude what the hell,Negative
Intel,"It's not stupid if it works, right?",Negative
Intel,Please get a GPU support bracket,Neutral
Intel,"Op can’t choose to either go sff build or regular. Decided to combine both and oh lawd, what a mess",Negative
Intel,![gif](giphy|26DN5pBQzhqgAufPq),Neutral
Intel,and i thought my microATX case i had was a really tight fit,Neutral
Intel,This is a shitpost.  Right?,Neutral
Intel,"Dear God, no, no!  Nooooo!",Negative
Intel,is that a open psu???,Neutral
Intel,"That's great. Your GPU is sagging.  Also, did you have to open your PSU in order to get it to fit? Why? I mean, I open my PSU all the time, but this is just stupid and dangerous.  At least CapXon and Teapo caps are much much better today than they used to be.",Negative
Intel,This gotta be rage,Neutral
Intel,I’ll be the first any day to admit my cable management is all fucked off but this is absolutely unacceptable.,Negative
Intel,![gif](giphy|JZ253GbwPFAdOrrZFH),Neutral
Intel,A case isn't that expensive bro,Negative
Intel,https://preview.redd.it/kgfkm79gsn6g1.png?width=500&format=png&auto=webp&s=ac48e27737115a92d1d33ddb3ebdad76e8f1f2fc  PLEASE NO OH MY,Neutral
Intel,It was excellent,Positive
Intel,Op… what the hell is wrong with you,Negative
Intel,It's like those people who refuse to throw away those ragged shoes with holes and tears all over them,Negative
Intel,"You really need a larger case, and a PSU that isn't open, a gpu not sitting on the bottom and BENT, a case with enough width as well to run all the cables behind the motherboard tray, not on top of and in front of everything...",Neutral
Intel,![gif](giphy|d46RNstZSnVMQ),Neutral
Intel,That's what she said.,Neutral
Intel,Good Lord. Please tell us that thing has an aluminum side panel and not a window.,Negative
Intel,Hey man what are the temps on the CPU and the GPU under load just to know ?,Neutral
Intel,What’s happening here,Neutral
Intel,https://preview.redd.it/85tu8f052o6g1.jpeg?width=1179&format=pjpg&auto=webp&s=7491a2a58cb8ab3a5211be0570d2059cb777e0ce,Neutral
Intel,https://preview.redd.it/v1phn1sc3o6g1.png?width=194&format=png&auto=webp&s=4dde11ce9fa511867c9dcc44da36006a77f9b460,Neutral
Intel,![gif](giphy|L2qukNXGjccyuAYd3W|downsized),Neutral
Intel,Bruh,Neutral
Intel,Some people just weren’t supposed to build their own… RIP GPU,Negative
Intel,Is this an incendiary device of some kind?,Negative
Intel,The GPU sag is the cherry on top,Neutral
Intel,"Having been on the receiving end of a shock from a PSU, this makes me very uneasy to behold and all I can say is be safe OP. If I were to add anything, for the love of all that is right, get that replaced at the very least! That thing is the absolute mother of all fire hazards!",Negative
Intel,Jesus fuck man I'm trying to eat dinner! Now my wife is mad because I just puked on to my plate,Negative
Intel,You know man space x is looking for skilled engineers such as yourself.  I think you have a solid chance at getting hired.  Hopefully musk sees your build.,Positive
Intel,![gif](giphy|1qTY0QdFd5fsQ),Neutral
Intel,"I don’t know what this is, but I know that I hate it.",Negative
Intel,"Tbh, that's where an Arc is supposed to be. Hidden away & unseen.",Neutral
Intel,I thought that was his mop bucket! That's a PC? Cheers 🥃,Negative
Intel,![gif](giphy|HUkOv6BNWc1HO),Neutral
Intel,![gif](giphy|MAjVWxG6pFmSaGty4Q),Neutral
Intel,This looks like the kind of shit I'd do.,Negative
Intel,Oh boy,Neutral
Intel,this feels like AI and not AI at the same time...,Negative
Intel,That is not fitting. This looks like a fire hazard.,Negative
Intel,What in the holy zip ties!?,Neutral
Intel,Final boss of ocd people,Neutral
Intel,"We build PCs, including gaming machines… And I really try not to pick on other people’s projects because to each their own… However, this calamity is making my eyes bleed. Airflow in this rat’s nest…. It’s going to be hard to come by.",Negative
Intel,I suddenly feel pretty good about my cable management now. Thanks man!,Positive
Intel,"Ah, back to the good posts of PCMR.  ![gif](giphy|SmoCFhZCi1kzu)",Positive
Intel,Sketchy,Neutral
Intel,"Dude. New cases with a nice glass wall....some spare room, and many attachments for fans can be found on marketplace you know? Like...60....40 bucks if your lucky. GL",Positive
Intel,Wuh da fuh am I looking at,Neutral
Intel,Airflow will be an issue.,Neutral
Intel,cable managemen't,Neutral
Intel,![gif](giphy|1T96TRBBGYThC),Neutral
Intel,Just Speechless…. I don’t even know what to say… That poor thing.,Negative
Intel,"https://preview.redd.it/7e20cpr8xo6g1.jpeg?width=3024&format=pjpg&auto=webp&s=9d606d362144cc5910eed501e37ab9dc272f27c2  I get claustrophobic for my pc parts so I like to give them breathing room!! Gaze upon my work and set down that eye bleach, brothers and sisters!!!",Neutral
Intel,Quick someone set a timer so we can see how long until this burns OPs house down.,Negative
Intel,That will blow your house up. 👏,Neutral
Intel,![gif](giphy|84BjZMVEX3aRG),Neutral
Intel,Just buy a new case bro I only say that cuz your going is already sagging,Negative
Intel,That’s not going to overheat.. at all. You’ll be fine,Neutral
Intel,Move this to trash as well Rage bait post OP 🗿,Negative
Intel,Did you turn it on its side and stomp on it to make room?,Neutral
Intel,Uhhh can someone call the fire marshals please,Neutral
Intel,This pisses me off and i don't know why,Negative
Intel,What is happening here,Neutral
Intel,I had a hard time realising this is NOT the back of the pc… hurts to watch honestly,Negative
Intel,https://preview.redd.it/4majbyg99p6g1.jpeg?width=1170&format=pjpg&auto=webp&s=8a6fb46cbf1c1e402a62c8b6c70b4705419624ea,Neutral
Intel,"I feel like ""what in the fuck"" is not getting the upvotes it merits.",Negative
Intel,"Oh OK I see it now. At least some of it. That's a PSU on the right.  Those are usually enclosed, no?",Neutral
Intel,https://preview.redd.it/b7u9upj0cp6g1.jpeg?width=320&format=pjpg&auto=webp&s=d34acf950c2f4659bc9a3818be0b077c21fc7b41,Neutral
Intel,"Yeah, you tore open the psu. That is real smart like. Lol.",Negative
Intel,This a computer Trump would've put together,Negative
Intel,What am I even looking at,Negative
Intel,![gif](giphy|H5C8CevNMbpBqNqFjl),Neutral
Intel,Someone teach this person how to build PC...,Negative
Intel,"My 1st ever custom pc build looked just like that, but ir ran for 5~6 yrs before i changed it 😁",Neutral
Intel,What on God's unholy green earth are my eyes beholden to,Negative
Intel,https://preview.redd.it/swn8ys57mp6g1.jpeg?width=1080&format=pjpg&auto=webp&s=9c24d32f2cb9fc46268eeb458ccb93d23379694f,Neutral
Intel,Very cramped. I expect that GPU to underperform from heat and perhaps fail. Torn open psu is a real hazard. Good luck.,Positive
Intel,![gif](giphy|12rQHIwkWykTRe)  That pc is really fitted each other,Neutral
Intel,"I absolutely love my compact builds, but holy crap man, get yourself an SFX PSU!   Is there a single fan in this system that can actually spin? You were supposed to de-shroud the GPU, not the PSU...",Positive
Intel,![gif](giphy|xTiTnIilwuFFFpf2Cc),Neutral
Intel,I’m not even sure what I’m looking at tbh. I like it,Negative
Intel,Got it to fit? Got WHAT to fit?? Pretty sure their sanity did not fit in there.,Negative
Intel,Was playing Horizon Zero Dawn today. Your PC looks like it's corrupted.,Neutral
Intel,Insert “that’s what she said” joke,Neutral
Intel,Godspeed,Neutral
Intel,I cant imagine after taking the hammer to the PSU the cables are more than twist toes to each other under that electrical tape right?,Negative
Intel,What the actual fuck is going on.,Negative
Intel,Power supply internals exposed??,Neutral
Intel,Ignition in 5... 4... 3... 2...,Neutral
Intel,What a mess,Neutral
Intel,Is that gpu even in? That looks like some major sag.,Negative
Intel,![gif](giphy|zmta6n2Ja2y9VbwRXG|downsized),Neutral
Intel,I'm here thinking that's not too bad for a backside..........,Negative
Intel,![gif](giphy|bEVKYB487Lqxy),Neutral
Intel,"Would be better without the case I think, maybe then the GPU will get some air,??",Positive
Intel,....kabooom   He   he   he  he,Neutral
Intel,"Oh that's gore, that's gore of my comfort character...",Neutral
Intel,Holy shit what a mess,Negative
Intel,Is this a shitpost?? 😂,Neutral
Intel,where did your psu cover go?,Neutral
Intel,"The longer you look at it, the worse it gets.  Holy shit XD  This is either a cry for help, a death wish, a supreme display of idiocy, or a master rage bait.",Negative
Intel,is that the back of the pc also wtf is that gpu doing,Negative
Intel,This is the new era adrenaline junky,Neutral
Intel,Where's the NSFL tag?,Neutral
Intel,My God... It's beautiful,Negative
Intel,Holy fucking retarded hell. With this level of stupidity a house fire is inevitable,Negative
Intel,10/10 no notes,Neutral
Intel,That’s what she said,Neutral
Intel,Whats dizz,Neutral
Intel,"I'm surprised the first thing I looked at was your sagging GPU, not the rat's nest of ketchup and mustard, or the exposed PSU (which can kill you if you're not careful). God, I hope this is ragebait!",Negative
Intel,Anyone want to take bets on how long til it explodes?,Neutral
Intel,OH! That’s a power supply. What the fuck lol.,Negative
Intel,"Great! Now keep doing it  over and over and over again until you get it right. That spaghetti  wiring has to go in the back not be lumped in with the mobo, PSU, and GPU  and Lord knows what. Use zip ties or similar methods. Plenty of examples online to learn from. Come back and show us all a better build than this, You can do it!  ![gif](giphy|faTOHi0omqCMU)  Great! Now keep doing it  over and over and over again until you get it right. That spaghetti  wiring has to go in the back not be lumped in with the mobo, PSU, and GPU  and Lord knows what. Use zip ties or similar methods. Plenty of examples online to learn from. Come back and show us all a better build than this, You can do it!",Positive
Intel,![gif](giphy|rWq6jZ0yNkMit0VfoW),Neutral
Intel,You created this monstrosity just so you could be a little hipster and trick yourself into believing buying an Intel card was a good idea…,Negative
Intel,https://i.redd.it/ylim8bo1mm6g1.gif,Neutral
Intel,I'm not sure what you are trying to do but I applaud you for trying.,Negative
Intel,That poor intel gpu and why tf are you using a exposed psu with no fan? This is  so cursed.,Negative
Intel,This is why prebuilds are a thing....,Neutral
Intel,https://i.redd.it/5hearud44m6g1.gif,Neutral
Intel,I’ll be in my psych ward basement if anyone needs me.  ![gif](giphy|QMHoU66sBXqqLqYvGO),Neutral
Intel,https://preview.redd.it/lqvuhtsm6m6g1.png?width=356&format=png&auto=webp&s=018913c8dde3525dacc6f0e4bb138c5f172a16aa  What in the abomination is that BRUH,Neutral
Intel,![gif](giphy|zQQVpMQvoCleSqOK6h|downsized),Neutral
Intel,https://preview.redd.it/gywx2c1g7m6g1.jpeg?width=1320&format=pjpg&auto=webp&s=f96061ed6a1abd237a4a6b5b594c7e6e58f69ca1,Neutral
Intel,WTF is happening there !??,Negative
Intel,![gif](giphy|KEf7gXqvQ8B3SWnUid),Neutral
Intel,I thought this dude was memeing,Neutral
Intel,that poor GPU is gasping for air lmao,Negative
Intel,That gpu be like  ![gif](giphy|U8MKkup2H6QOttBLkh),Neutral
Intel,Surely this is rage bait??,Neutral
Intel,![gif](giphy|cA0TiRmuetO1szgShj),Neutral
Intel,"that's... that's an interesting curiosity you got here, give us more of it!",Positive
Intel,Yeah that'll be fine. /S,Positive
Intel,Now that’s a quality shitpost,Neutral
Intel,When your house burns down this photo will be exhibit A in your insurance company's refusal to pay the claim.,Negative
Intel,"12VHPWR connectors sometimes melt and even burn!  *OP: Hold my beer!*  ""Announcing the NEW FireStarter 3000 PC!""",Neutral
Intel,why the case even?,Negative
Intel,ragebait,Neutral
Intel,lame,Neutral
Intel,![gif](giphy|YZlQaMesgPIAM)  this belongs in r/pcgore,Neutral
Intel,The gpu sag...,Neutral
Intel,this one will be a toasty machine for sure!,Positive
Intel,"""i can't breathe""   \- your gpu, probably",Neutral
Intel,https://i.redd.it/16wijhffdm6g1.gif,Neutral
Intel,![gif](giphy|h7MJWcymf6o7xIosDC),Neutral
Intel,"Op, why did you stuff this worse than bonny blue!?",Negative
Intel,Where’s the “after” picture…?  ![gif](giphy|xjXok06oc67ObUJwLQ)  # WHERE’S THE “AFTER” PICTURE?!?!?,Neutral
Intel,what am i looking at???????????????????????????????????????????????,Neutral
Intel,"At first I thought ""got what to fit, that doesn't look that bad"". Then I realized I was looking at the front of the case.",Negative
Intel,lol   Arc and it's suffocated in the bottom hehe,Neutral
Intel,I'm pretty sure this is a troll post,Negative
Intel,And gpu still sags,Neutral
Intel,https://preview.redd.it/to0ut7lzem6g1.jpeg?width=480&format=pjpg&auto=webp&s=be36b6badbc219a187d490e7e6ba2d0601afd51b,Neutral
Intel,![gif](giphy|2SBvv58qaLptK),Neutral
Intel,It’s a joke right? Right? Plz say it’s a joke.,Neutral
Intel,![gif](giphy|FY8c5SKwiNf1EtZKGs),Neutral
Intel,did you tape the psu 😭,Neutral
Intel,"Bro, just get a new case. There are some cheap options.",Neutral
Intel,![gif](giphy|1hMhlrWWfXU77iYnBB),Neutral
Intel,W-Why is the GPU located at the b-bottom of the case?,Neutral
Intel,What in the fuck this is the FRONT OF THE CASE!!?!?,Negative
Intel,bomb has been planted,Neutral
Intel,This is a joke right?,Negative
Intel,Either this or spend 30 bucks on a cheap new case. But your way makes way more sense...,Neutral
Intel,https://preview.redd.it/6qsg8p7zhm6g1.jpeg?width=1170&format=pjpg&auto=webp&s=b53952ca8b5bdcab448d175ee569a3bf4d1c249f,Neutral
Intel,I will pray for that GPU,Neutral
Intel,Bro…JUST BUY A BIGGER CASE,Neutral
Intel,Just... Buy a different case. They're the cheapest component lol,Neutral
Intel,An abomination,Neutral
Intel,"Just buy a new real case.  Ffs, you're gonna destroy all that hardware",Negative
Intel,"Sir, I applied your thriftyness, but fear for the neighborhood power grid, and houses.",Neutral
Intel,May I ask what the fuck is this?,Negative
Intel,well i guess cable managment has left the damn chat,Negative
Intel,This may be some of the best rage bait I've ever seen.,Positive
Intel,Wtf am I looking at,Negative
Intel,![gif](giphy|tp4dm1ptNnQ76),Neutral
Intel,Last time I checked there wasn’t a case supply shortage.,Neutral
Intel,https://preview.redd.it/01o2pcw3mm6g1.jpeg?width=1080&format=pjpg&auto=webp&s=421736c9b7fd0ae7c11a0e4048f1fd1b58dac4c8,Neutral
Intel,![gif](giphy|CpAGyBCgY6qrHIKYTt),Neutral
Intel,The video card needs a bracket it's slightly sagging.,Neutral
Intel,Could you explain what the fuck this is,Negative
Intel,![gif](giphy|JtQc9M7l1KUGQ),Neutral
Intel,![gif](giphy|29AxBMcrIt6mWTK95p),Neutral
Intel,My brother in Christ just get a bigger case.,Neutral
Intel,You know you can get another case right ? They just kinda..swap around.,Neutral
Intel,the fuck did bro even do,Negative
Intel,![gif](giphy|l4FGGafcOHmrlQxG0),Neutral
Intel,![gif](giphy|Y54bNi0kU0oj6),Neutral
Intel,r/techgore,Neutral
Intel,I give it 3 years max 😂,Neutral
Intel,Cool deathtrap.,Neutral
Intel,I didn't realize you could deglove a PC... TIL,Neutral
Intel,"Personally, I find the Intel Arc GPU far more disgusting than what you've done to the PSU.",Neutral
Intel,What am I seeing oh God,Negative
Intel,This is a very painful upvote. But here ya go buddy. 'Good job' making it work... for now,Positive
Intel,The opened and in-use PSU is EXTREMELY dangerous. If you touch it while the capacitors still have charge you could die.,Neutral
Intel,https://i.redd.it/1n5wmoazom6g1.gif,Neutral
Intel,"If this was pre-2012, I’d say something really bad.",Negative
Intel,WTF is this piece of shit (casino),Negative
Intel,![gif](giphy|1SldY1L9sEDUQ),Neutral
Intel,![gif](giphy|9IsNfuwCp5sEU),Neutral
Intel,There are mid towers that cost $50. Please get one.     Edit: Jesus christ I just realized what he did to the PSU. What an idiot.,Negative
Intel,H- H- How?,Neutral
Intel,Oh my god,Neutral
Intel,![gif](giphy|IDGNYvFLkJKLK|downsized),Neutral
Intel,Holymoly,Neutral
Intel,Wait you got….WHAT TO FIT???,Neutral
Intel,That's what she said.,Neutral
Intel,![gif](giphy|xUNd9YJwF6ifDUnqNi),Neutral
Intel,This is the stupidest thing I've ever seen,Negative
Intel,NSFMR  ![gif](giphy|StFlcHVnuz6Yy6mxcq),Neutral
Intel,the fuck am I looking at D:,Negative
Intel,"The air is gonna give up trying to enter your case, you built a heater for winter not a PC",Negative
Intel,![gif](giphy|QMHoU66sBXqqLqYvGO),Neutral
Intel,What in the temu build am I looking at?,Neutral
Intel,![gif](giphy|l46CkifT0rN5PmXD2),Neutral
Intel,![gif](giphy|czOM9AjyWfi8w),Neutral
Intel,Makes me feel much better about my tight fit..,Positive
Intel,That GPU is gasping for Air.,Negative
Intel,What on earth am I looking at… is that a PSU without a casing or am I missing something?,Neutral
Intel,![gif](giphy|3o85xnoIXebk3xYx4Q),Neutral
Intel,ITT: People not getting it,Negative
Intel,That's what she said?,Neutral
Intel,This is amazing lmao,Positive
Intel,"first I was like holy crap what is going on there.. but when I figured it out eventually, I thought  that op has got some psu connector problems maybe, but he figured it out and made it work, w/o buying a new one. It may not be pretty, nor safe, but if it works I mean.. then it works.",Negative
Intel,https://preview.redd.it/1p6901x34m6g1.png?width=1236&format=png&auto=webp&s=f24a0d33cede80df2ac2844d155eeaa632b55a36,Neutral
Intel,https://preview.redd.it/7e3twhnsom6g1.jpeg?width=1080&format=pjpg&auto=webp&s=3fa02a1d62b20e9a4e485a7b4ba325f288612d4a,Neutral
Intel,https://preview.redd.it/p2h1hzh68p6g1.jpeg?width=2131&format=pjpg&auto=webp&s=8c6f9a86319fcd0a54ff998429676a25329bac2f,Neutral
Intel,Give me some too 😭,Neutral
Intel,[No! You *drink* the bleach.](https://youtu.be/eqbn7oxXh38?t=16s),Neutral
Intel,https://preview.redd.it/1waouvd1ho6g1.jpeg?width=720&format=pjpg&auto=webp&s=eb5de8ed19c766b61ea0bded6bc04148d4e64719,Neutral
Intel,https://preview.redd.it/wpoxgt41qo6g1.jpeg?width=960&format=pjpg&auto=webp&s=1c1f092f22769319b9b63796689e01001c70c30e,Neutral
Intel,Exactly 😭,Neutral
Intel,https://i.redd.it/zrg0lm4osq6g1.gif,Neutral
Intel,https://preview.redd.it/rq5yzdm9tm6g1.jpeg?width=480&format=pjpg&auto=webp&s=3866e887a1f4f42ff1a52d440a916c7900e1fa10,Neutral
Intel,SAME haha. That card is toast,Neutral
Intel,Wait.... this isn't the back!?,Neutral
Intel,What do u mea....... Oh,Neutral
Intel,Took me a second to realize it’s not in fact the back of the case…,Neutral
Intel,I wouldn’t have realised if not for this comment 😭,Negative
Intel,Had to go back and check 😂,Neutral
Intel,what do you mean that’s not the back..  https://preview.redd.it/t4guzgtewo6g1.jpeg?width=1290&format=pjpg&auto=webp&s=e20390dac1af904d34e480a5186d1886d79af16e,Neutral
Intel,Omg.. I never realized it wasn’t the back of it. What the FUCK!,Negative
Intel,![gif](giphy|3ohhwtftK2lWSFvwUo),Neutral
Intel,Holy shit I didnt realize that was a power supply...,Negative
Intel,"didn't even notice the PSU is exposed like that lmao.   bait/10 OP, congrats.",Negative
Intel,How quaint of you to think that GPU is getting any air to the fans,Negative
Intel,This think will be one of those that run forever,Neutral
Intel,Ohhhhhhh it was taking me a min to figure out what that was.,Negative
Intel,That’s…that’s the PSU?? Oh dear god…,Negative
Intel,Holy crap! I didn't even notice the power supply.,Negative
Intel,"PCLowerRace trying to fit in.  ""How do you do, fellow PC gamers...""  ![gif](giphy|oBwOba7cOph4I|downsized)",Neutral
Intel,![gif](giphy|2wSaulb0fsDydh0IoB),Neutral
Intel,"""No low-ball offers. I know what I got""",Neutral
Intel,How is the gpu your main concern when the psu is unboxed.,Neutral
Intel,"Why do you think so ? Assuming that the >90Degree Celsius protection behavior of the GPU is still working..+ you can’t see if the gpu can/can not/ or barely can get fresh air from under the case, depending on the structure.   Def. not ideal fer sure, especially long term wise. but you simply don’t know many circumstances.",Negative
Intel,Been playing Silksong for hours seems fine. I'm not an idiot.,Positive
Intel,![gif](giphy|NTur7XlVDUdqM),Neutral
Intel,Seriously. I hope their insurance finds this post and denies any claims related to this shit show.,Negative
Intel,![gif](giphy|3oz8xBSs6cGCgqRbCE|downsized),Neutral
Intel,"More like that poor power supply above the exhaust of the GPU…you know, the thing on the right that had its fan and outer casing removed, and connectors removed, AND all the connection instead (I’m assuming) hand-soldered together.  Edit: Also exposed caps on the PSU.",Negative
Intel,In this case I hope he doesn't so he learns a lesson. Because wtf is that???!?,Negative
Intel,Ye just had to mod it a little,Neutral
Intel,Forget the fan he soldered the damn cables himself taped it over without sockets,Negative
Intel,Open power supply on the right with exposed capacitors  What could go wrong,Neutral
Intel,This GIF was made for clusterfucks like this.,Neutral
Intel,Are you sure that a shock _wouldn't_ help OP??,Neutral
Intel,"You won't get the chance, this build will be deleting itself when it blows up the house.",Neutral
Intel,"It’s not even sag at this point. If the GPU is that much out of alignement, the PCB must be bent or cracked.",Negative
Intel,My thoughts exactly,Neutral
Intel,"In the words of Indy ""SNAKES... Why'd it have to be SNAKES?""...",Neutral
Intel,Been on 8 hours no problems,Negative
Intel,Needed a little tweak,Neutral
Intel,Gpu*,Neutral
Intel,"Holy god, I didn’t cop until this comment. I foolishly zoomed in and saw ASRock, and thought not my first choice for a power supply, but okay fine, then WTF is that custom board thing! 🤦‍♂️  That thing needs a couple of different hazard stickers all around it, on it, under it, everywhere.",Negative
Intel,Let's name it the Titanic!,Neutral
Intel,Actually it may just be a standard non modular PSU ripped open.,Neutral
Intel,Works fine ye,Positive
Intel,![gif](giphy|1FMaabePDEfgk),Neutral
Intel,![gif](giphy|3j192fUB8hKvIf8UKm),Neutral
Intel,And me...,Neutral
Intel,My wife bought me an iPad years ago and this was the inscription on the back :D,Positive
Intel,Trollstice  ![gif](giphy|SCoDlK7kpAElHKvSp6),Neutral
Intel,It's an ARC so not a huge loss at least.,Negative
Intel,Jfc…. Me either. I was like “why are so many cords going into that mini mobo”,Negative
Intel,"Well, I *hope* the bottom is mesh.",Neutral
Intel,"Well he couldn't well fit it in there with all that extra metal, now could he?",Negative
Intel,"Why does this matter, after putting the case cover on..?  Not having a psu fan (at least it seems like that) is far from optimal tho",Negative
Intel,">I'm not an idiot.  \- u/Omnicron2, December 11, 2025",Neutral
Intel,>I'm not an idiot  ![gif](giphy|bjB3gtFvREqqr5NAHW),Neutral
Intel,Your post and build show otherwise.,Neutral
Intel,"Heat can be a slow killer, but a killer just the same.",Neutral
Intel,"Nice ragebait, idiot",Positive
Intel,One of the least demanding games too. Play something like red dead or cyber punk and see how hot that shit gets.,Negative
Intel,Dunning meet Kruger lol.,Neutral
Intel,RemindMe! 6 months,Neutral
Intel,"No sorry, you're a moron waiting for the psu to explode shortly and burn your house down.",Negative
Intel,"Bro just buy a bigger case, cases are cheap, any other failed component from poor airflow/excess heat is not cheap lol",Negative
Intel,!remindme 1 month,Neutral
Intel,"Silksong can run on an igpu and a cpu from 10 years ago. If you play anything modern, your pc might die",Negative
Intel,"I didn't see your comment yesterday, so it popped up for me at work. Which made me look at the picture again. It somehow got worse overnight.",Negative
Intel,![gif](giphy|7gNXVIPKkQad0AIIFJ|downsized),Neutral
Intel,You didn’t mod it. You butchered it.,Neutral
Intel,![gif](giphy|THj5QURAqrfyPcblu4),Neutral
Intel,Mod it? You removed the lsu fan because why???,Negative
Intel,It would be for the betterment of us all..,Positive
Intel,Just because it hasn't exploded yet doesn't mean it won't.,Negative
Intel,What happened to the PSU frame?,Neutral
Intel,Lol  She's a keeper xD,Neutral
Intel,Even an Arc didn’t deserve this 🥴,Negative
Intel,Depends battlemage arcs are pretty good,Positive
Intel,Fire is going to be a lot faster.,Neutral
Intel,"On that topic, is 70 C a healthy temp for a 9070xt while at full usage?  And 60-85 C jumps on 9088x3d.",Neutral
Intel,I will be messaging you in 6 months on [**2026-06-11 19:33:46 UTC**](http://www.wolframalpha.com/input/?i=2026-06-11%2019:33:46%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/pcmasterrace/comments/1pk2mzu/had_to_move_things_around_a_little_but_i_got_it/ntio4rl/?context=3)  [**1 OTHERS CLICKED THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2Fpcmasterrace%2Fcomments%2F1pk2mzu%2Fhad_to_move_things_around_a_little_but_i_got_it%2Fntio4rl%2F%5D%0A%0ARemindMe%21%202026-06-11%2019%3A33%3A46%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%201pk2mzu)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,Neutral
Intel,"I got 8 minutes left of my 14 hour shift. I dont know ow how, but I agree in it somehow looking even worse today.",Negative
Intel,"Yeah that's absolutely fine. Amd cpus tend to run hotter anyways. If it's consistently thermal throttling, that's when you might wanna repaste or upgrade your cooling",Neutral
Intel,"Thanks! Makes me realize i feel like a lot of people go a bit overboard with cooling possibly? I have a decently high wattage build for mid range, and literally 3 shitty fans (aside from AIO fans)",Neutral
Intel,"You really can't have too much cooling, and better cooling gives you more thermal headroom for upgrades down the road or overclocking and such. Plus it's a marginal investment compared to the most expensive components so why not spend a little more to future proof it?",Positive
Intel,"My Stepdad had a mid-range Mac in 1993 that he paid over $5000 to buy including the monitor and keyboard.  Based on inflation that Mac would have cost him well over $11,000 in today's dollars. While people bitch about prices in the PC world, we have it very good these days it's very hard to spend over $10,000 on even the most high-end PC's today unless you are doing some insanely custom shit.",Negative
Intel,Ironic that the joy sticks and thrust throttle are probably the only things that have kept their value,Neutral
Intel,The recommendation of QEMM for a system running Windows 95 is a little odd...  I also actually have one of those Aura Interactor things. I got it fairly cheap after a hobbyist electronics chain here bought a bunch of them as surplus. I should drag it out at some point and see if it still works...,Neutral
Intel,"That's really close to the component list that I sent to my mom to build my ""graphic design"" pc for college. I didn't have all the gaming controllers and I had 2 - 1Gb Quantum Bigfoot HDD's and some graphic design software. The price was more like $2500 because a lot of the components were purchased wholesale. By the way, a 21 inch CRT monitor is massive and does not fit on a normal desk, nor does it fit in the trunk of a car.",Neutral
Intel,"i once purchased an ibm original pc for 4000.00 USD  286/12mhz  this shit was advanced.  i also owned that very same matrox card, it was the shit.",Negative
Intel,i pretty much had this system .. minus the stupid peripherals and i opted for 32MB memory \^\^,Negative
Intel,"In 1996 that GPU was nice to have more than anything. Graphics middle wares weren't really standardized at the time so everyone had a proprietary graphics API. Very few games actually took advantage of them, and GPUs weren't that advantageous until 1998 or 1999 with the Riva TNT and TNT2 and the GeForce 256 in 1999.",Positive
Intel,Hold on... a Joy switch? Is that why everything is so depressing now cause no one has a joy switch anymore,Negative
Intel,I love how they recommended this [full ass AV receiver](https://audiokarma.org/forums/threads/technics-sa-gx490.413267/) as a PC part,Positive
Intel,"Hmmm...  Not even a 56k modem.  Guess 56k wasn't a thing until 1998.  Frankly, half this list is ridiculous as it basically has you buy a home theater for sound and has every peripheral imaginable...",Negative
Intel,"I bought a 166 MMX with a 4.7 GB drive, Awe 64, Voodoo 1 and 32mb ram back in May of 97 to name some.  Going by conversions rates for 97, it was about $2100.  It played anything you threw at it. But was outdated and could not play the latest and greatest less than 16 months later. I remember getting 10 fps on some of the larger Unreal levels.  I'm betting this was the same by the time I bougth mine. As Delta Force that came in 97 demanded an MMX iirc",Neutral
Intel,$600 for 16MB of RAM?  Sounds like a steal to me these days.,Negative
Intel,"Good lord technology sucked back then.  That would have been hopelessly obsolete by 2000, just 4 years later. Today we got people gaming happily on their 1080ti and whatever CPU from the mid 2010s.  Interesting to see what best gaming rig of 96 was though!",Negative
Intel,1995 virtual reality is wild.,Neutral
Intel,Nah. The ultimate game machine was the Amiga 500..,Neutral
Intel,But you have the Pro throttle and the virtual pilot pro w/ foot pedals!  Too much added fluff..,Neutral
Intel,"Good to see that they aren't running a Model M keyboard on that rig. The M is a fantastic board, but it isn't the best for a lot of games. A Northgate Omnikey, absolute beast of a board, on par with the Model F in Build quality, and it comes with NKRO. I think they were cheaper than Model Ms as well. Makes me want to break out mine.",Negative
Intel,"What the heck is a ""daughterboard""???",Neutral
Intel,Pre OCZ PC Power & Cooling my beloved.,Neutral
Intel,"A 1995 VR helmet, ninja what?",Neutral
Intel,"It feels weird to me that an external CD-ROM was considered optimal back then, I guess internal CD-ROM changers weren't a thing when this was published?",Negative
Intel,https://preview.redd.it/5em6f1ctwd8g1.png?width=726&format=png&auto=webp&s=38270c9f51c29e40b1f05a15f49f48caf7c1f55f  JFC,Neutral
Intel,"Just today, I saw an old iMac  all-in-one being used as a door stop.",Neutral
Intel,Jensen Huang: “Hold my (whatever ridiculously expensive alcoholic beverage a man like him would drink)!”,Negative
Intel,"Dude... You compare devices that were just invented and that technology was really expensive to produce and ship. I just can't believe people are still comparing this sht and say ""we have good now"". It's like right now could be invented new method of manufacturing something similar to silicone that could be very close or just the same as quantum computer and could be used in home environment. It's the same type of innovation they were facing in that days. You are just like a boomer that say ""oh you just need to save money not to spend it on sht and you will easily afford a house""",Negative
Intel,"If you invested $50,000 in the S&P 500 at the start of 1993 and reinvested dividends, it would be worth roughly $1.35 Million today",Neutral
Intel,"Nah, that PC Power & Cooling case would almost certainly fetch double it's original retail price.  Thing is rare AF.",Neutral
Intel,"I had CH Products virtual pilot pro and pedals back then. Loved them, eventually sold them, and replaced them for a Guillemot Trustmaster wheel, which after a few years was replaced by a Fanatec GT3 RS2.  Now looking to replace that, probably with a Moza R9 or R12.",Neutral
Intel,"I had a 19"" and that was pushing it already on my massive desk. Man, we've come a long way",Positive
Intel,Its reminders like this that make me feel the current hyper inflation was actually overdue. Today you can buy a ballin machine for 4k and that's after 30 years of inflation. Prices should be far higher considering inflation and materials science and a ton of other factors. I shouldn't be able to buy a decent PC for $1000 in 2025.,Negative
Intel,"It wasn't designated as a GPU yet as well. Simply ""graphics card"". It still used the CPU for a lot of calculations too. When I got a TNT2 and later a GeForce MX shit went down, though.",Negative
Intel,"Every gaming PC I've had over the last 20 years has been connected to a full AV receiver for audio, and various configurations of surround speakers.  I can't imagine anything less.",Positive
Intel,"> Frankly, half this list is ridiculous  That's why it's the ""Ultimate"" game machine, not ""very good"" game machine",Negative
Intel,"It didn't suck, it was a very exciting time with rapid evolution where every upgrade was a monumental leap forward vs today's little baby steps.  Here is a clip of another Ultimate Gaming Machine from 1998: https://m.youtube.com/watch?v=xXHN8ybA2kQ  And at the time of recording it wasn't the ultimate CPU anymore with a 50% faster chip on the way.",Positive
Intel,Absolutely didn't suck. It was fantastic. Almost Every new game that came out was ground breaking stuff.,Negative
Intel,Extension circuit for a motherboard,Neutral
Intel,"Yeah this was a bit before that, iMacs would come out like 5 years later.",Neutral
Intel,Yeah I remember playing Counter Strike for the first time with a TNT2. It was nuts.,Neutral
Intel,C'mon! Two *different* flightsticks?,Neutral
Intel,That's what I was saying.,Neutral
Intel,"I lived through that, that's why I'm comfortable hanging out with my PC parts for a long time.",Negative
Intel,"Yeah, one is for throttle, one is for controls. That’s not an uncommon setup.",Neutral
Intel,"Same. I ran a 486 dx2 deep into the pentium 2 era, and games released that would run. I had an upgrade to a pentium 120mhz and then a leap to a PIII 650mhz Dell Pc. That was mind boggling. Tnt2 graphics in it, stunning stuff. Need for Speed Porsche came with my Dell, and that made me a fan of racing games for good.",Positive
Intel,"Same, had a 486SX, was obsolete by the time my dad bought it for me. I had a blast with it though (and at the time had no idea it has been surpassed by so much).",Neutral
Intel,"HOTAS wasn't widely available in the 90s, so double joysticks was the way to go.  Also, talking about 90's joysticks brings back memories of using a joystick to play Descent. Getting a new joystick where I could twist it to control the yaw was a game changer",Neutral
Intel,I remember my buddy getting a 133 mhz while I was still stuck on 90,Neutral
Intel,I imagine how Descent must be in VR. Probably give you epic amounts of Vertigo,Neutral
Intel,HOTOSAS is the new thing. Hands on (throttle or stick) and stick.,Neutral
Intel,"My buddy got a p120 when I was on a 486. Man, the possibilities",Positive
Intel,Imagine if you threw in a little [Sega R360](https://en.wikipedia.org/wiki/R360) as well…,Neutral
Intel,Are you Satan?,Neutral
Intel,I have been accused of that in the past 🤣,Neutral
Intel,"Yeah. It's a decent pairing with a 13400F    However, it's largely wasted at 1080P     If you aren't planning on changing monitor, I wouldn't bother upgrading the GPU yet",Neutral
Intel,"🙏- high five, not please or prayer.",Neutral
Intel,"Yep I was planning on upgrading the montior or cpu next, but would the setup function properly with these current specs? I just wanna make sure its not too heavy for the rest of the build when it maxes out",Neutral
Intel,It'll work in your system without issue as long as it fits in the case,Positive
Intel,"Alright perfect, thank you!! 🙌",Positive
Intel,Just make sure the model you buy has the 12VHPWR adapter   Every GPU with it ships with one,Neutral
Intel,"> Or it is just UE5 problem.  Yes. Loading new areas of the map doesn't utilize 7GB/s of data transfer. It is relatively small asset files loading in.   To see benefits of gen 5 SSDs, you need to be moving sequential data not playing a game.",Neutral
Intel,"For me, gen 5 ssd is a waste of money if your load is only gaming, heck even a gen 4 ssd won't be utilize by the games you play unless the game explicitly support the DirectStorage API and even if they do, they still won't reach 7000 MB/s transfers.  The microstutters you experience is probably because the poor optimization of todays' games. We have a similar build (only that I have a 9070 XT) but i still see those stutters from a poorly optimized game.  One perfect example of this is the Final Fantasy XV Windows Edition (Not a UE5 game but...), even with such powerful gaming hardware like the ones we have, the game still stutters when Prompto takes pictures on the background and no fix was ever brought even to this day.",Negative
Intel,I have a couple gen 4s and one gen 5 on my gen 5 motherboard and like everyone else said you will never notice the difference while gaming the only time Im utilizing the speeds inbetween is if im moving massive files between the two other wise you will never notice it including boot times. I just have a habit of wanting to upgrade gen with gen. like my last pc was gen 4 so i obivously ought gen 4 m.2s,Neutral
Intel,A gen 5 drive will improve load times not fps or game stuttering,Neutral
Intel,My gen 3 only 3500mb/s. Is it enough?,Neutral
Intel,"It's enough, even on a gen 3 nvme, most games still don't utilize those maximum theoretical speeds. I also have some games on my Samsung 970 Evo Plus, and those games run fine.",Neutral
Intel,"From r/radeon   * Ray Caching: Only available in Warhammer40K today, more games next year. * Ray Reconstruction: Only available in Black Ops 7 today with more games next year. * AI Frame Gen: Available in Black Ops 7 today with 40 games by end of 2025.",Neutral
Intel,It's almost 2026 and AMD keeps reinstalling the AMD Install Manager that I do not want and have to keep manually uninstalling. Stop this AMD.,Negative
Intel,What is fsr redstone? and which games use it?,Neutral
Intel,"I got a notification for the update in AMD Adrenalin Edition, but it does not appear in the actual install manager lol",Neutral
Intel,I just tested the release on four machines (76X&78XT/78X3D&79XTX/97X&9070XT/75F&76XT). Every system still suffers from crashing drivers when hardware-accelerated apps are used (Chrome/Discord/etc.).  Please fix. :),Negative
Intel,so can I open adrenalin on this one with a rdna 2 igpu and rdna3 gpu or is it still broken like the last version,Neutral
Intel,<--- Int8 rdna2 enjoyer,Neutral
Intel,did they fix enhanced sync and noise suppression yet,Neutral
Intel,Did this driver fix purple visual glitches with the RX 7700 XT? It's a known bug that appeared after the driver 25.4.1,Negative
Intel,"The ignorance by amd of Rx 7000 users is astounding tbh, but this is 2025 AMD not prior AMD where they would try to appease a larger user base.  It's going to make me rethink my loyalty for future gfx purchases",Negative
Intel,So we cant test path tracing performance yet on Cyberpunk? Lol,Neutral
Intel,"This is a very underwhelming update for RDNA 4 users I get that this technology needs to mature, but they should already be at a point where the implementation is across more wide array of games. My fallen RDNA 2 and RDNA 3 brothers will be remembered. The only reason AMD gpus are still relevant rn is price, nvidia tax is crazy. GG",Neutral
Intel,"Thanks for nothing again, AMD.  Signed, 7900 XTX user.",Negative
Intel,So is there any point to installing this if I'm on RDNA2 and don't have any of the issues that they fixed?,Negative
Intel,This is the worst driver amd made 9060xt for me. 2 games instantly crashes. Indiana jones and silent hill 2. With this driver if you enable ray tracing game hangs and give error.i already report bugs in 25.12.1 and same with 25.11.1 and amd does nothing. every ray tracing titles works ok with 25.10.1 driver and this is bad. amd does not listen users anymore. anyone has any crashes happen like me?thanks...,Negative
Intel,Nothing on Oblivion Remastered crashing? Intermittent application crash or driver timeout on 9000 series when playing Battlefield 6?,Negative
Intel,AMD Software still crashes randomly,Negative
Intel,#AmdNeverAgain Give Fsr4 on rdna3,Neutral
Intel,New update new problems,Negative
Intel,"The adrenalin app just auto updated my 9070xt mid game, now my screen is black with no signal output to my monitor but my music is still playing lol. I waited for 10mins then I had to hard restart my computer for it to say the update failed",Neutral
Intel,Pretty dissapointing ngl,Neutral
Intel,Should I get the RTX 5070 ti or 5080 at msrp? I am currently selling my XTX after radio silent news about FSR 4 int 8 on it.,Neutral
Intel,Please add the broken noise suppression to “Known Issues”.,Neutral
Intel,"If  this driver update keeps crashing my gpu im not leaving 25.9.2 for a while, im also starting to think about selling my gpu and get nvidea, and really black ops 7 why not a real game like cyberpunk i dont want to waste 70 euro for fifa with guns",Negative
Intel,"Can confirm on my 9060XT that Silent Hill 2 is still crashing and Avatar Frontiers of Pandora currently has a bug when FSR4 is enabled where the entire screen starts flashing like a strobe light, shadowy areas seem to trigger it. This is with both games fully patched & up to date.",Negative
Intel,"Let me see - all the new ""Features"" will be available for Cyberpunk 2077 in at least 1 year time and ONLY with RDNA4 ??",Neutral
Intel,AMD NoiseSuppresion still broken. Since September!,Negative
Intel,"Are pink artifacts fixed on RX 7700 xt, anyone ? It was bugged in 25.11.1 driver last month.",Negative
Intel,Where‘s support for 7000 series? Wtf is this dead meat,Negative
Intel,Everything is RDNA 4 exclusive? awesome /s  RIP finewine.,Positive
Intel,I’m on a 6000 card is there literally no reason for me to download this,Negative
Intel,"all i want is to be able to capture clips in my games but for whatever reason amd either doesnt understand im in the game, recognizes the game wrong (battlefield 6 shows as elder scrolls online which i dont even have).",Negative
Intel,It's december and still no FSR4 for vulkan.,Negative
Intel,Is this worth updating to from 25.11.1  Is it more stable?,Neutral
Intel,"I had to downgrade to 25.9.1 to have some level of stability, can somebody confirm that the new driver is safe to upgrade to without it messing stuff up?",Neutral
Intel,Still no fsr 4 support for rdna3 🙄,Negative
Intel,"Guys calm down. RDNA3 being moved to maintenance mode is part of their new strategy, no longer ""Fine Wine"", the new approach is Stale Ale. That way their products remain DOA after launch and people won't keep them very long.",Neutral
Intel,idk why I find it so funny that a specific Roblox game got called out in the patch notes,Negative
Intel,Did they fixed the amd noise supression not turning on?,Neutral
Intel,"Anyone know why Cronos: The New Dawn has been showing [""FSR 4""](https://i.ibb.co/nqW2VMng/Cronos-The-New-Dawn-2025-12-04-02-28.png) for me on a 7900 XT for a few weeks? At first it was 3.1.  I know it can be modded in but this is on a new Windows 11 install and I haven't done any modding.",Neutral
Intel,"Looks like new chipset drivers, too.",Neutral
Intel,"I thought the application freeze fix might have stopped monster hunter wilds from crashing on me but nope still does it (DXGI_ERROR_DEVICE_REMOVED,)",Negative
Intel,/u/amd_vik are you aware of assetq corsa evo vr not working on AMD cards since 25.9.1 ? It displays the left and right eyes out of alignment and therefore fails to show a cohesive single image.,Negative
Intel,so no fsr4 support on Vulcan still? this is getting ridiculous,Negative
Intel,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.   Thank fucking god.,Negative
Intel,Still experiencing 100% gpu usage almost constantly as soon as you boot up BF6 on newer drivers after 25.10.1 and higher temps in general  I'm locking my FPS to 144 but the older drivers is showing better overall temps and less gpu usage for me 🤔  [https://imgur.com/a/ctbMCx7](https://imgur.com/a/ctbMCx7),Negative
Intel,25.11.1 was dog water driver timeout city for me I'm just gonna assume this new one will also be the same.,Negative
Intel,BF6 crashing after a few minutes in game with that driver on a 6800xt,Negative
Intel,"ever since 25.9.2 still same bug is present even now and now it causes even more problems because ML based FSR and FG fails when it happens: Adrenalin app just shuts down randomly even when idle, no errors, no driver timeout, no dx12 trimeout, just adrenalin itself gets shut down in random times. why wont you guys do something about it finally? Seriously its been so long now... im on 9070XT Steel Legend Dark Edition from ASRock, 80% of your users or more report the same issue FIX IT for the love of GOD. I tried everything hoping its on my side but windows reinstall, DDU and AMD cleanup app and fresh driver install nothing helped its still here",Negative
Intel,"Both 25.12.1 and 25.11.1 drivers have the same bug on RX 9060 XT. When my screen goes blank and later i wake up screen, i have two mouse cursors on the screen, until i launch some app and then will second, fake cursor disappear.",Negative
Intel,I hope this fixes the many crashes I've had since the last update...,Negative
Intel,"Still enjoying the piss out of the 7900XTX on 25.9.2. It chews through everything I throw at it at the settings I choose, don't care about new driver releases unless a new game I want to play doesn't play well on whatever driver I currently have installed.",Negative
Intel,"Even tho I have a 9070xt this is still so underwhelming… We waited 6 months and got basically nothing yet. Sorry for all rdna2, 3 users.  Fun fact: Its been years now that the adrenaline software cant be opened, the only fix ist to press win+p and select only main monitor. Than start it, than swap monitor profile again…   Definetly buying nvidia next time, not supporting this big company anymore, which is behind in every aspect. Image you just want to play alan wake 2 (looks beautiful).",Negative
Intel,"ass. no support for rdna2/3, no new features for rdna2/3, rdna4 have only one game that support all of that, redstone framegen almost identical to 3.1 framegen, frame pacing still there.",Negative
Intel,hardware unboxed tested it and frame facing is broken when amd frame gen is on sadly,Negative
Intel,So the HDMI crashing issues should be fixed in this version yes?,Neutral
Intel,Any news on fixing the gpu vram leak issue on bf6? Sorry I’m lazing not reading the patch notes,Negative
Intel,25.12.1 does not even install on my Minipc (780M) + 6650XT eGPU Setup.   I thought I might fix 25.11.X not opening in an eGPU Setup.   Guess I will be running 25.9.2 for another few Months.  God why something always break? I thought it would be better going all AMD for the eGPU setup.,Negative
Intel,"Yeah I'll still be with 25.9.1 until the texture flickering is fixed in BF6, also instant replay just didn't work in 25.11.1 for me.",Negative
Intel,Will this help Warzone not look so blurry on 7900xt? Game is unplayable,Negative
Intel,So there seem to be two links - going through support>picking GPU(9070XT in this case) downloads the 25.21.1 win 11-b.exe file meanwhile going from this release note article it downloads the win11-c.exe . Any difference?,Neutral
Intel,"im using 6800xt the driver page has the win11-a version and article have win11-c version. which one should i choose i really dont know and this ""different builds"" confusing a lot of people",Neutral
Intel,"Genuine question, why all the hype and rush to release this today when it has just two games to showcase the benefits?",Negative
Intel,Jesus how long has that Cyberpunk Pathtracing crash been in the known issues. It feels like it's been more then half a year.,Negative
Intel,Installed with no issues,Positive
Intel,"I can’t play Warzone because I can’t update my bios, there doesn’t seem to be a recent bios update available for my Acer Nitro 5, using Adrenaline. Anyone know if this will help?",Negative
Intel,Doesn't look like they fixed the bug with Enhanced Sync not working properly with Freesync.,Negative
Intel,Any fix planned for 9070 users who cant enable Hardware Lumen on Oblivion Remastered? Game crashes as soon as we turn on the option.,Negative
Intel,Still no fix for Battlefield 6 for those with AMD 6800M GPU. I swear my next setup is going away from AMD if this is not resolved anytime soon.,Negative
Intel,u/AMD_Vik      In 2022 AMD made changes to OpenGL Driver. So since 2022 the extension gl\_ati\_fragment\_shader is missing in the driver. It cause problems in older games like Call of Duty 1 from year 2003. Stutter on some maps and broken water rendering because the games can't use the extension anymore.     Our Community is waiting since 3 years for a fix.,Negative
Intel,in black ops 7 only 25.9.2 driver work better even new 25.12.1 much worse fps drops,Negative
Intel,Very unstable for me (7900XTX). Driver keeps crashing even when I'm just watching videos. Reverting to 25.11.1,Negative
Intel,i just had to roll back to 25.9.2 because 25.12.1 kept crashing my system with poe2   even GGG straight up said don't use 25.10-25.12,Negative
Intel,"After observing you guys for a few days xD, 25.12.1 was installed along with new chipset driver on my system.  To my surprise, unlike previous 25.11.1, Adrenalin interface now runs properly with igpu enabled.  I need to test it out with real games, but for now, I've dodged instant roll back.  FYI, If you're using two or more GPUs, including igpu, on a single system with muti-monitor. Download the C package(1.65GB one including rdna1&2+3&4).",Positive
Intel,"NoUnfortunately, they don't work (( Random crashes remained + In some games, the inability to use frame generation through drivers was added (( Sad ( R5 3600 32gb ram Rx 7700 xt ) Rolled back to 25.9.1 everything works with it",Negative
Intel,"I had a very weird issue:     My PC would just crash when i did an Windows Defender Scan (only Full Scan, it worked fine with QuickScan or other programms like Malewarebytes) like the power was cut. I did a number of things even rollback the chipset driver but that didn't help. Then i rolled back to 25.9.1 + the newest chipset driver and everything worked fine again.   In case somebody had a similiar issue",Negative
Intel,"Anyone having problem with AMD overlay with this update? Somehow not showing at any game even if enabled, if I click to different monitor, it shows up. But when I click back to the game it disappear again.",Negative
Intel,"Error code 182 for my AMD Radeon™ 780M integrated GPU on my Ryzen 7 8854HS CPU.  All other driver updates before 25.12.1 worked fine on my Lenovo Legion Slim 5 Gen 9, but this one says my GPU is incompatible, even though AMD's driver download page is providing [this download link](https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe) to the installer:  https://www.amd.com/en/support/downloads/drivers.html/processors/ryzen/ryzen-8000-series/amd-ryzen-7-8845hs.html",Neutral
Intel,7000 Series web browser glitch? and sound glitch? huh,Neutral
Intel,"Is it safe to update, 25.9.1 is stable for my 9070XT and causes zero crashes with the timeout bullshit from clock speeds going to 3300+ MHz",Neutral
Intel,What does fsr Redstone means ?,Neutral
Intel,"Is this driver more stable than 25.11.1 it was causing driver time outs and i even got a blue screen. I rolled back to 25.9,2 and now im scared to update to this one lol",Negative
Intel,These comments are all over the place is it better than 25.11.1 or not? 😂😂,Negative
Intel,"So in short, still no support for 7000/6000 series, yipee",Negative
Intel,"Idk what happened but after this update my game crashed and then my PC crashed and when I turned it back on AMD Adrenaline disappeared from my PC, completely gone. What did you do lol.",Negative
Intel,For Sale: 7900 XTX - $50 OBO  I know these are no longer desirable due to being left in the dust by AMD after only a few months of real support but hopefully it will be at least a good paper weight for someone.,Neutral
Intel,So now driver frame gen is gone? Unless the game specifically supports it? And the overlay as well? Both are completely gone now after the update...,Negative
Intel,What about the fixes for the 7900xtx crashing all the time?,Neutral
Intel,"«#AmdNeverAgain” Where’s the Christmas gift in the form of FSR 4 for RDNA 3? In the new 2026 year, it might be time to think about switching to Nvidia.",Neutral
Intel,"Toujours pas de FSR4 pour les séries 7000 ? C’est mort. Pour ma part, je n’achèterai plus de cartes AMD. Si Nvidia continue à proposer son DLSS pour les anciennes cartes, alors mon choix est fait.",Neutral
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,"Downloads ""whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe"" for 9070XT, ""whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe"" for 5700 XT  What does it mean?",Neutral
Intel,It took a while for DLSS 4 to get implemented in a good way on 40 series cards too but a version made it there. Give it time. Now if they can just start prodding developers to incorporate that as well it’ll be worth it. Not enough games yet but here’s hoping!,Positive
Intel,Any update the in fact that and adrenaline software is not working when second monitor is connected? Especially using iGPU for second monitor ?,Negative
Intel,Omg I think they fixed the LG oled tv reboot bug,Neutral
Intel,Should i install it directly or should I use AMD cleanup utility first?,Neutral
Intel,some one have problem with instaling?,Neutral
Intel,Does this fix the driver timeouts that were happening with Edge? I had to revert the November update because of that problem,Negative
Intel,Any fix or still need iGPU disabled for 7000 and 9000 cards?,Neutral
Intel,The update is still not showing up in install manager,Negative
Intel,Honestly this software was the bane of my card for the longest time. Not having it anymore stopped so many weird bugs and crashes.,Negative
Intel,Does AMD's Instant Replay record still bug out?,Negative
Intel,Anyone tried the new fsr redstone yet? I am hoping for a big improvement over the old fsr,Positive
Intel,do you guys remove the old drivers before you install new ones? or just install ontop,Neutral
Intel,The path tracing crash STILL on Cyberpunk is absolutely wild to me. Finally AMD has a card capable of playable raytracing but we can't use it on the 'Crysis' of modern times to even test it out.,Negative
Intel,Adrenalin doesn't show this update for me yet lol,Negative
Intel,"> Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby.   Glad for this, it was annoying that we were stuck in 25.9.2",Negative
Intel,Are the issues with SecondLife fixed? Last driver that didn't break textures was 25.9.1,Negative
Intel,9060 non XT 8GB can do the math 7900 XTX Nintendont,Neutral
Intel,"I’m at work, so I cannot check for myself: does this fix the constant crashing in Oblivion when hardware lumen is turned on?",Negative
Intel,Has anyone tested Marvel Rivals on 25.12.1 version of the driver? The only stable driver that works without crashing on that game is the 25.8.1 version.,Negative
Intel,Adrenalin Panel not showing bug still present… :-((((,Negative
Intel,Am I the only one who doesn't have the new option in the drivers for frame generation with a 9070 XT?  https://i.redd.it/7r86hslx3g6g1.gif,Neutral
Intel,"why are there 2 versions, b and c, 1.65Gb and 991Mb, release notes and through the support page, and is it stable or shall i just keep 25.8.1 as any other seems to crash call of duty, regular, other games seem fine,  ryzen 9 7950x3d/rx7900xtx",Negative
Intel,Did it fix the god of war 2018 checkered shadows?,Negative
Intel,"I spent all this time with 25.9.2 on my 9060xt because the following ones were disgusting to me, I will give this new update a chance and let's hope everything improves a little!!",Negative
Intel,Arc raiders crashes are fixed or not?,Neutral
Intel,"Makes my 9070 XT to constant run on 100% load in bf6 no matter if i play or sit in the menu. Cause device hung, graphic glitches and high temps.   Same with all drivers above 25.9.   25.9.1 works flawless with no errors and the load varies depending on the scenery as it should.",Negative
Intel,Noticing in Hogwarts Legacy with the new FSR and FG enabled over a period of like 30 seconds my 9070XT will go from \~250W used and 200 FPS and then drop down to say 120W used and 90 FPS and then after a short period go back up again. With FG disable it stays consistent 140 FPS-ish,Neutral
Intel,"FYI for ""Driver Only"" guys, 25.12.1 still have an issue to install this option.  l've open ticket to support team for last 2 versions. but I can't follow their request to observe the issue.  Don't know how long to keep using extracted file method. lol  Will see how 25.12.1 ""driver only"" perform.",Negative
Intel,oh nice! they fixed the FSR4 Quality Presets artifact issue,Positive
Intel,"When AMD finished Orange, Yellow Green, PurpleStone, can we unlock FSR Infinity?",Neutral
Intel,"Is it worth updating to this latest driver? I am not planning to use frame gen, is the image quality better or are there any fps improvements in games?",Negative
Intel,"Updated to 25.12.1 now, before I was on 25.8.1, have a Rx 6800 XT and a Ryzen 7 7700X. Also updated my Chipset-Driver today. Haven't testet much yet, played now for like 1 hour Space Marines 2, watched some Youtube vids since I updated. So far looks ok. Only thing that worried me first was that I found in my Reliability History, 2 critical entries of LiveKernelEvents of code 1a8. But these were written down by Windows on the time, while I was updating my driver. We will see, if anything happens I will keep you updated.",Neutral
Intel,"Despite the device ID-based driver update blocking set in August, it has worked until now. The windows tried to install some driver on the 6700XT just now, and unfortunately, it also replaced the software itself somehow. threw an error message too.  Manual update would not go through unless i removed the driver update block.   What a sad situation.",Negative
Intel,"Anyone else has problems with CS2/Fortnite? Started happening after i updated drivers to 25.11 My whole PC would randomly freeze for like a minute with the ""AMD software detected that a driver timeout has occurred"" error. Once the PC unfreezes i must kill the game from task manager.",Negative
Intel,Does it fix the arc raiders dxgi crash of the previous driver?,Negative
Intel,"How do I downgrade from this driver?   I’ve tried four different older drivers and all of them give me error 182 – GPU is not supported (RX 9070 XT) during install.   I’ve already used DDU and the AMD Cleanup Utility, but the only driver I can install successfully is 25.12.1.",Negative
Intel,pc started to crash 7900xtx... reverted to 25.11.1,Negative
Intel,Hi me and other people I know. Also forums and Facebook pages . Have had an issue with the frame gen after 25.9.2 . When they released new features we have all had issues where its drops fps and is completely unplayable. Has this been fixed in 25.12.1 I have 7900 xtx 7800x3d. Friend has 9070xt 9800x3d Both have issues. And im on windows 10 he's on windows 11. I used ddu and tried all settings on frame gen and other settings to fix it. Not to mention the drivers where stutters and lower fps without frame gen. Thanks,Negative
Intel,"When I enable V-Sync in the game, I experience lag; it only runs smoothly with V-Sync enabled when I also activate the performance overlay. This problem has existed since driver version 25.11.1.",Negative
Intel,"I have a second card from amd. And both cards have driver problems. Now I have an rx 9070 xt oc. I don't do any undervolting. Everything is at factory settings including the bios. I had 4 driver failures in 7 hours. What good is FSR if the driver doesn't work? It would be good if you finally solved this problem. I can stand it for a while, but if it continues like this, I'm leaving AMD.",Negative
Intel,AMD Wattman settings don't apply for the first time they're set. They have to be changed and applied to a different setting and then to the desired one back and forth to get them to work. I use wattman to set my custom fan curve and it's been glitchy since 25.11.1.,Neutral
Intel,"Wish they would acknowledge the bug where turning on GPU scaling and integer scaling adds more input delay, so for example the mouse movement will feel sluggish.  Been having this issue for 3 months now since nya bought a a 9070 XT",Negative
Intel,"On the RX 7600S graphics card, Adrenalin does not launch at all, and during installation it removed the driver PCIVEN_1022&DEV_15E2&SUBSYS_15131043&REV_60.",Negative
Intel,"How are those with a Cezanne CPU supposed to install this?  Selecting the 5750G from the drivers download page offers 25.21.1, yet none of the 3 variants of the installer support it.  * whql-amd-software-adrenalin-edition-25.12.1-win11-a.exe (Vega, supposedly?) - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-b.exe - nope * whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe (combined? ""Systems with RDNA series graphics products"") - nope  Each of them return Error 182.  Even the minimal web installer, amd-software-adrenalin-edition-25.12.1-minimalsetup-251207_web.exe, only offers 25.8.1.  VEN_1002&DEV_1638 is nowhere to be found in the .inf for any of the 25.21.1 variants.",Negative
Intel,I'm still hesistant to upgrade on this driver until they resolve these driver timeouts hell I'm even on 25.9.1 still experiences time to time TDR's.,Negative
Intel,"Is it worth for my 9060XT to go from 25.11 to the latest, Im having some problems where ghost of tsushima crashes.",Negative
Intel,Should I download the new driver version if I have 6800XT? There is nothing in the patch notes about this series... And if yes - why?,Neutral
Intel,Driver is making valorant run like crap for me idk why .,Negative
Intel,im using an rx6600 and up until today i was fine avg 200fps on r6 today the game says its at 22 usaeg when it avgs 1-4 and now it has major fps drops/tears,Neutral
Intel,This driver constantly crashes call of duty for me. Whatever windows update installs(which seems to be 25.10. something) is the most stable there is. 9070XT.,Negative
Intel,FSR Redstone support? Will my minecraft machine run faster now?,Neutral
Intel,Gonna be able to play modern titles on my HD5750 thanks to redstone !,Positive
Intel,All that build up for dog water. Built my first PC in March and went with a 9070xt full of hope. I'm beginning to understand why AMD is so widely despised.,Negative
Intel,Windows just installed the June   update from AMD. The fck is this,Negative
Intel,I'm not seeing the new update in AMD Install Manager,Neutral
Intel,so in 2028 10 games will have it like FSR4 XD,Neutral
Intel,Anyone knows if it fixed the crashes with Oblivion Remastered and Silent Hill?,Neutral
Intel,"""Intermittent application crash or driver timeout may be observed""  This is not an issue tied to a few games.... is a wide issue for me with a 9060 XT whenever i rise my monitor refresh rate.",Negative
Intel,"Here’s the thing. You either need to address the FSR4/Redstone on RDNA 3 issue with a time frame or I’m just going to refund all my boxed RDNA 4 gpu’s, melt down my XTX’s, and go back to buying Nvidia cards because they’re better in everything but COD (which sucks). People do not support you because your hardware and software are better, they support you because of “AMD Fine Wine”.",Negative
Intel,What a peace of trash. Fsr 4 quality looks dogshit,Negative
Intel,"Ray Caching in 40K?  Not sure how they got this to work on the tabletop in real life but sounds awesome  In all seriousness there are a large number of games in the Warhammer 40K universe, any chance they are saying which one?  Space Marine 2 Darktide Battlesector   Etc",Positive
Intel,"so pretty much nothing for today, shrug...",Negative
Intel,Is there a partial list of the 40 games with the new frame gen? Is it something different from the fg we already have?,Neutral
Intel,There are well over 100 warhammer 40k games. Did they not specify?,Neutral
Intel,they wont,Neutral
Intel,"It's so annoying.  I would keep it if it didn't constantly pop up trying to get me to install ""AMD Chat"" and ""AMD Privacy View"".  I don't want your shovelware AMD, take a hint.",Negative
Intel,"There should be an option during install to exclude it, it can't be that hard to do. Same as you, u/MihawkBeatsRoger , I also uninstall it afterwards.       Notifying u/AMD_Vik",Neutral
Intel,"This.   Why I took it out are my own reasons and quite frankly, irrelevant. It's my PC and I don't want it. So please AMD, listen to me and keep it off.",Negative
Intel,"Focus on serious matters, this is a joke. If you do not want it feel free to install the driver only version, and be happy u have that choice. If you want the full features of adrenalin, well install manager is one of them.",Negative
Intel,Why not just leave it and don't use it?,Neutral
Intel,It's a rebranding of the entire FSR ecosystem. What's new today is machine learning enhanced frame generation for RDNA4 cards. You can enable it in the driver for any game with FSR 3.1.4 or newer.,Neutral
Intel,It adds denoising for Path tracing. In theory it should look way better now,Positive
Intel,All the games that don't use bluestone,Negative
Intel,"Only one , the new call of duty ATM. So if you enjoy shitty games , have at it",Neutral
Intel,"Same, and I'm still on the October drivers",Neutral
Intel,"Delete the amd install manager in windows. This way you can click ""check for update"" button on adrenaline and get drivers from there no time.",Neutral
Intel,You can download it from the website. The app release notification always lags behind the site. This is nothing new.,Negative
Intel,9070xt i see brave or discord freezing and lagging when watching a YouTube video still. I dont understand how hardware acceleration bug hasn't been fixed yet. Wtf are they doing.,Negative
Intel,Yup same here. Had to roll back to October to fix again,Negative
Intel,25.9.1 works on my 9070 XT. Everything after that is a mess for me,Negative
Intel,Tagging u/AMD_Vik  so they are aware of the issues.       I encountered the same problems on my 6800xt. Figma on chrome is causing random BOSD. The system will just restart without notice. Every single web app seems unstable on my system and memory usage is all over the place. Rolling back to 25.9.1 doesn't fix everything but it eliminates 70% of the issues..,Negative
Intel,Oh well. :/  Funny thing is I rebooted my PC again for a Windows update. The first thing that greeted me after opening a web browser was the driver giving up the ghost.  On 25.11.1.,Neutral
Intel,"9800x3d, 6950xt, no issue with either chrome or discord or firefox with hardware accelerated set",Neutral
Intel,"Me too.  Installed 25.12.1, whenever I use YouTube in Full Screen, the whole system freezes, while the sound is still audible, then I have to hard-restart my PC. Happened three times, decided to downgrade to 25.11.1 again.",Negative
Intel,"This should be fixed, I'm not sure why it was omitted from the release notes",Negative
Intel,<--- inte 8 rdna3 enjoyer,Neutral
Intel,"How do I set this up, can't find any info",Neutral
Intel,"I can't speak on enhanced sync, but noise suppression is still busted and not working =/",Negative
Intel,"I'm piggybacking, because I need that info too",Neutral
Intel,Ok I thought I was the only one having the enhanced sync issue because no one replied to any of my posts about it. I use it because then I can lock my fps to 120 (on a 4K OLED TV) and use enhanced sync instead of Vsync because of the screen tearing when locking to 120. Now I have to do the frame lock to 117 which is fine but just annoying me I'm missing out on 3 fps lol it was causing issues in a few games where it would stutter like crazy and it all came down to enhanced sync. I don't use noise suppression so I don't know what's up with that.,Negative
Intel,"Been using it for a few hours with the 7900XTX, so far so good.   Hopefully it's 100% fixed.",Positive
Intel,I hope they fixed it. I will test it now,Positive
Intel,"Did the typical test that I usually do and it didn't show up for me and I'm on the RX 7700XT as well. So hopefully, it's fixed.",Neutral
Intel,"AMD stopped giving a shit about it's fans once the company was saved and they started raking in the money. The change in tone was clear as day. That said, I'll still buy their GPUs because I hate Nvidia far more and I don't see that changing.",Negative
Intel,"yeah my next one will be Nvidia, better features, better performance espacialy with RT/PT   And apperently longer support... and AMD cards in a simmilar performacne bracket don't even cost THAT much less sooo.... jeah I am mad aswell",Negative
Intel,"I agree. AMD has shown poor support for 7000 series owners. If there was a FSR4 int8 leak, AMD should officially release FSR4 for 7000 series owners.  I bought my 7800xt only 2 years ago before RDNA4 cards came out.  Nvidia provides DLSS4 upscaling to their older generations like rtx3000 series",Negative
Intel,looking back rn i think it wasnt worth the 100 dollars gain i gotwhen my 7900xt does consume more than rtx 4070ti and i do have shit features even the antilag+ scam that was one of the main reason i bought the GPU isnt here anymore .,Negative
Intel,And just like that comment and user deleted themselves 😂,Neutral
Intel,"If you're referring to the app crashes with RTPT reflections enabled, we're working with CDPR on a fix",Negative
Intel,Signed /another 7900xtx user,Neutral
Intel,"I came over from NVDA last March, bought a 7900xtx, RMAd it a few weeks ago due to pink/purple pixelation that would randomly happen. Now it's non stop driver timeouts and random performance issues every time I boot my PC or games. I am never buying another AMD card. I'd rather get ripped off by NVDA and not have constant headaches.",Negative
Intel,"Which driver are you currently on? I'm just curious; personally, I'm on 25.9.2, and surprisingly, I have 0 problems, unlike with previous versions. Should I try 25.12.1?",Positive
Intel,"Unterschrieben, Gigabyte 7900 XTX Benutzer.",Neutral
Intel,"Nope. Generally if the driver does not massively increases performance in some game, or you don't have any issues or the issue you have isn't fixed, then it's not worth updating, unless there is some new feature you want.    I reverted back to 25.9.1 (from the top of my head) because with any newer driver BF6 crashes randomly, and neither DICE nor AMD seem to give a damn about it.    And before someone asks, I tried any other fix on the internet for Battlefield and nothing else worked.",Negative
Intel,Same here. Anything above 25.9.2 crashes ray tracing games like Silent Hill  2 and Oblivion Remastered.   Ihr never had a more crash prone GPU than the 9070XT.,Negative
Intel,"Try this, taken from another comment branch https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",Neutral
Intel,Might potentially be fixed by a recent Windows update?  24H2 (and an earlier mini-patch that included this) apparently resolved a lot of crashing for folks.  See [here](https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/),Neutral
Intel,"Yeah I was really hoping they'd have got buy in from a decent number of devs with updates to big RT showcase games like Indiana Jones, Alan Wake 2, Cyberpunk, etc. But Black Ops 7 and Warhammer 40K... and that's it (for the RT features)?",Neutral
Intel,5070ti fs  basically a better 9070xt,Positive
Intel,"I’d wait on 5080ti with more VRAM but these are going to be obscenely expensive knowing nvidia + current RAM prices. Both 5070ti or 5080 are more of a sidegrade than upgrade, not worth the hassle IMO.",Negative
Intel,Get a 5070ti. I never thought I would say that. But this is what is is.. 9 months after release and the drivers are still D.S.,Negative
Intel,What about a secondhand 5070ti?,Neutral
Intel,"I mean, I wouldn't get either. 5070 TI is a sidegrade from the XTX, and 5080 is only slightly better. DLSS and RT would be the only reason.",Neutral
Intel,Sidegrading for an upscaler sounds like a joke.,Negative
Intel,Same boat here. Tired of trying.,Negative
Intel,Thanks for testing. Have you perhaps tested Oblivion Remastered?,Positive
Intel,Finally fixed! It's a christmas miracle!!!,Positive
Intel,I regret getting this 7800xt,Neutral
Intel,"I think Linux developers are doing some experiments As of now, FSR 4 (FidelityFX Super Resolution 4) does not officially support RDNA 2 or RDNA 3 GPUs, even on Linux. However, thanks to Develer’s work on VKD3D-Proton 3.0, there is partial and unofficial support for RDNA 3 under specific conditions.  RDNA 3: Partial Support via Develer’s VKD3D-Proton  - Develer’s VKD3D-Proton 3.0 includes support for FP8 (8-bit floating point), which is required for FSR 4. - This means RDNA 3 GPUs (like RX 7600, 7900 XT/XTX) can run FSR 4 in some games via Proton, even though AMD doesn’t officially enable it. - Global override toggles in AMD’s 25.9.1 driver can bypass the FSR 4 whitelist, allowing it to run in FSR 3.1-compatible games.  I hope they succed it will be a slap in the face.",Neutral
Intel,This has been announced for months.,Neutral
Intel,Yeah AMD refusing to port features to any card released before the 9 series makes supporting them really hard.,Neutral
Intel,Say thanks they haven't demoted 7000 series to only game drivers,Neutral
Intel,"Your best case is your RX 7900 turning into Balsamico, whatever that means.",Positive
Intel,"Its because RDNA 4 added hardware that 3 and 2 don't have. Now before I get kicked to death by angry people, there is a version of FSR Redstone that uses and INT8 path that is compatible and will work on 2 and 3, however that has not been launched today and AMD have not confirmed it will be.   That isn't to say they won't do it, but right now it's not been announced. Perhaps there will be enough noise to get AMD to change their mind or it might be that they want to get it out on their latest cards first before complicating matters with older RDNA support.  Only time will tell",Neutral
Intel,"Bro the AI accelerators completely got revamped, upscaling technique isn't usually the indicator for 'fine wine', it is when non-upscaling raw performance numbers improve.",Positive
Intel,Any card released prior to the 9 series.  Amd could give 2 shits as they chase the AI bubble (jokes on them if I was an exec I'd double down on the consumer market to insulate from the impending bubble burst),Neutral
Intel,sadly,Negative
Intel,"Yep, I go back between 23.9.1 and 25.9.2. I couldn't be happier.",Neutral
Intel,"If it's any consolation, I was on an NVidia card for 2+ years where I wasn't getting the DLSS updates. Then they actively removed features when they went to the NVIDIA app.  Looking at AMD's roadmap, RDNA4 looks like a stopgap anyway until RDNA5 (prob will be called UDNA?) comes out. So in another year and a half I'll be in the same situation with my 9060XT.",Negative
Intel,"Use OBS, replay buffer",Neutral
Intel,Was just thinking of giving a shot for Indiana Jones and the Great Circle - I guess not anymore since FSR4 doesn't work with it..,Negative
Intel,"Microsoft had bugs also causing hanging crashes. Everyone loves to blame GPU drivers immediately, but check this out:  https://www.windowslatest.com/2025/12/09/windows-11s-last-update-of-2025-quietly-fixes-amd-gpu-hangs-haunting-battlefield-6-call-of-duty-black-ops-7-arc-raiders/",Negative
Intel,I also want to know this.,Neutral
Intel,"I'm wondering the same thing, 25.11.1 is still the most stable for me!",Positive
Intel,Wondering too. I bumped back down from 25.11.1 because it was unstable on my machine.,Negative
Intel,Stay on 25.11.1 if you are on RDNA 1 or 2,Neutral
Intel,squash hard-to-find sharp reach memorize fade husky divide subsequent plough   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"~~It messed up my audio, now everything sounds 8-bit. If you're on RDNA4, avoid this update.~~  EDIT: it's not the drivers, after much tinkering I was about to deduce that it was my monitor. So it should be ok to update",Negative
Intel,I can't use anything above 25.9.1 on my 9070 XT,Negative
Intel,forget it they gave u the middle finger move on fuck both amd and nvidia,Negative
Intel,won’t happen,Neutral
Intel,"Fine wine is only a thing for very few and specifics types of wine, typical wine still goes bad over time.",Negative
Intel,What is the source for this or is it trust me bro?,Neutral
Intel,They should just remove this feature. It never worked from day 1..,Negative
Intel,what is the difference,Neutral
Intel,Latest windows update yesterday fixes that.,Neutral
Intel,"Can you tell me if this is also applicable to 25.12.1? There are several (frustratingly unlisted) VR-specific fixes aligned, one of them closely relates to what you've just described",Neutral
Intel,Same here. 25.9.1 makes my problems go away,Negative
Intel,"That was a terrible driver for me also. New one has been night and day improvement, give it a shot.",Negative
Intel,Same for my 9070 XT. Device hung error,Neutral
Intel,"Thanks for reporting, had that once with 25.11.1 + 9070XT (W10) before reverting to 25.9.1 (since then, it never reappeared).",Neutral
Intel,i think your sorry should extend to people with rdna4 cards because this is pretty underwhelming,Negative
Intel,Do you get a firmware update pop up? Is this one?  https://i.redd.it/w46j86mnct6g1.gif,Neutral
Intel,"I'm familiar with this impacting United Offensive, I don't believe we're reintroducing this old vendor specific extension, however. I do have a ticket for the performance issues though; I don't believe this is related to the missing extension.",Neutral
Intel,"Tested for 2 days(1day and 22hrs uptime)  No crash, No BSOD for me so far. Nothing strange.  MS Edge, Google Chrome video playback, youtube...etc all play nice while gaming on main monitor.  Diablo 4, MSFS 2024, Doom dark age, Forever winter(UE5), Witchfire(UE4)...etc All run fine.  Lossless scaling runs fine on spicy vids to all of the above games xD  HWinfo64 and MSI Afterburner, RTSS all run as they should.  (Win11 25H2 uptodate, X670E, igpu(98x3d)+7900xtx+6400 3gpus, 2 monitors, hybrid mode)  Edit) rx 6800 + r7 7700x on win11 25H2, X670E, Single monitor, igpu-disabled -> runs fine.  rx 6700xt + i7 8700k on win11 25H2, Z370, Single monitor, igpu-disabled -> seems good.",Positive
Intel,"25.11.1 had pink artifacts glitch on chromium browsers with 7700 xt but i installed 25.12.1 yesterday and no issue so far, i did not see artifact pink glitches or sound issue so far ?",Neutral
Intel,My 9070 XT hate every driver above 25.9.1,Negative
Intel,I updated to this driver and immediately got a BSOD. Rolled back to October 25.10.2 again,Negative
Intel,offer steep theory scale straight obtainable physical ad hoc selective test   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,thats what I wonder too! Is it more stable??,Neutral
Intel,haha r u fr,Neutral
Intel,la même. C'est scandaleux,Neutral
Intel,"Means that they've created separate driver packages tailored for the specific gens (A rdna1/2, B for RDNA3/4, C - combined fat package that contains both drivers for systems that might have both gens on the same machine (igpu + dgpu) )",Neutral
Intel,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Cleanup utility first, always!",Neutral
Intel,I also wanna know best way updating drivers. DDU kinda annoying but maybe must be done i don't know,Negative
Intel,It's doing it there too.,Neutral
Intel,Never seen any crashes on it with latest driver prior to today 9070xt w11,Negative
Intel,"Just tested it tonight, and for me it's working fine, 9060xt here, windows 11 with the latest update, although i play with the ""medium"" preset which disables ""lumen"", can't say it might work for you but you can try it if it still crashes constantly",Neutral
Intel,Might want to check [https://www.reddit.com/r/radeon/comments/1pjeonb/fyi\_fsr\_ml\_framegen\_requires\_windows\_11/](https://www.reddit.com/r/radeon/comments/1pjeonb/fyi_fsr_ml_framegen_requires_windows_11/) :|,Neutral
Intel,Everything after 25.9.1 is a mess for my 9070 XT,Negative
Intel,nope. I still crash,Negative
Intel,"I fixed my arc raider crashes (mostly in blue gate map load) by running DDU, installing 25.10.1 (down from 25.11.1), and deleting shader caches (dont know if the shader cache delete helped or not). I upgraded to the newest drivers the day after they released and haven't had a single crash since in arc raiders, including w overlay.",Neutral
Intel,"for me, DDU in safe mode, disconnect internet, install 25.9.1 fine for me(9060XT).  I've tried 25.10/ 25.11 and revert back to 25.9.1 with this way. Now observing 25.12",Neutral
Intel,Yes. I downgraded from 25.11.1 because of the crashing. Now been on 25.12.1 all week and havent had any issues come up. You also get proper fsr4 upscaling now.,Neutral
Intel,"I just want to say I think I found the culprit. It also happens on the winupdate one too, because it started crashing all the time.  Core clock boosts itself WAY past what it is declared on the card(I got a Sapphire 9070XT Nitro, supposed to be 3060MHz). Here's the moment before it crashes to a black screen:  [afterburner screenshot](https://i.ibb.co/3YpJFtzM/Screenshot-2025-12-21-030537.png)  The dip in clocks is the moment it crashes. As you can see, it is running well above boost clocks. Hence, freezing in a few minutes, proceeded by a black screen, and a crash. The ups and downs are from me alt tabbing in the graphs, by  the way.   This is with core clock -200mhz applied in Afterburner and no crashes, boosts to just above declared boost clocks. Here the dips in up and down on power are probably me toying around how much exactly -mhz is needed.  [afterburner -200mhz](https://i.ibb.co/YTQfGJtc/11111.png)  All of the crashing behavior so far is replicable in COD, CS2, Cronos New Dawn.  u/AMD_Vik",Negative
Intel,I still have my 5670,Neutral
Intel,"No problems with RX 9070 xt in ARC raiders, i have win10",Positive
Intel,It’s for Darktide apparently,Neutral
Intel,any game with fsr 3.1 fg also has the new fg since drivers override it. it’s also why they stopped versioning fsr. any game with fsr 3.1 should just automatically have any new version of fsr when the drivers update,Neutral
Intel,"It's for Darktide. But it's not even ready for launch there, either.",Negative
Intel,I forgoed any amd software entirely  Use more clock tool  10x better with 0% of the bloat   ^^ helped me get my 4th in world furmark score (7900xtx user),Neutral
Intel,If you want to be in control of what’s on your computer then Windows is not the OS for you,Neutral
Intel,"Dumbest take one can have, since installing only the driver won't let you manage the settings at all.  Which has nothing to do with this useless launcher no one wants or needs.",Negative
Intel,Found the install manager dev lol,Neutral
Intel,"It also removes the *Check for Updates* button in Adrenalin, you have to manually remove the Install Manager to get it back.",Neutral
Intel,"It's garbage, updates only to WHQL drivers, if you want optional driver, you have to check manually on website if this was released. Before they added this garbage manager, you could set in settings to ""main + optional"" - and it was updating as intended. Now it's just useless bloatware.",Negative
Intel,"Because I don't want it.   It shouldn't matter why, it's my PC and I don't want it on. That's good enough reason as any.",Negative
Intel,"Because it's a process running in the background, costing some performance?  It's not going to be a huge difference on it's own, but everything adds up.",Negative
Intel,Thanks.,Positive
Intel,Unfortunately Redstone FG is bugged with poor frame pacing,Negative
Intel,Nice to see the innovation continuing on,Positive
Intel,But only on the 9060 and 9070 right?,Neutral
Intel,Yeah same,Neutral
Intel,"Remember when you could click ""Check for Update"" inside the AMD Software and if there was an update, it would download and install it for you?  Glad they fixed that awful experience, and we have the Installation Manager now.",Neutral
Intel,I remember this mentioned since the  GCN 1.0 days. Lol,Neutral
Intel,"On my end, the driver crashes. Most of the time it manages to recover (sometimes it will crash a few more times before stabilising). Sometimes it doesn't recover (leaving only 1 of my monitors working), so I had to reboot. Then after rebooting, strong chance it'll crash again the moment I open my browser.",Negative
Intel,"\+1 on this. Most games crashed drivers with any newer drivers except 25.9.1, but poe2 i cant play with vulkan or Directx 12 only with Dx11",Negative
Intel,"My experience with switching to amd was so smooth and perfect until 25.9.1. Everything after that just caused stutter issues in games, programs randomly crashing, drivers crashing completely causing my pc to reboot, this is so sad i hope they fix this soon and bring back a stable version asap. Rolling back to 25.9.1 now aswell until that happens.",Negative
Intel,Are you able to tell us what the error code is on the BSOD? I don't suppose you have a kernel memory dmp pertaining to one of these failures over at      C:\Windows\MEMORY.DMP,Negative
Intel,Thanks will give it a try after I finish work,Positive
Intel,"Wait, AMD Customer Support told me that 2 monitors connected to iGPU and dGPU has never been officially supported and that this configurations breaks performance… so they told me bullshit?",Negative
Intel,Any update on three Oblivion Remastered and Silent Hill  2 Remake crashes? A lot of us are still with the September drivers because of them.,Negative
Intel,<--- Ditto,Neutral
Intel,Optiscaler lets you inject it. Do not use in multiplayer games though.,Neutral
Intel,it cannot possibly be this difficult to fix when there’s already community workarounds,Negative
Intel,both are still broken somehow,Negative
Intel,both have been broken since 25.10.1. enhanced sync just makes your display run at an extremely low framerate when freesync is on and then noise suppression just doesn't even turn on. I don't understand how they haven't fixed either of these yet. they haven't even acknowledged it,Negative
Intel,"I have to do the same. My monitor is  240Hz and the TV  120Hz and I have to use Chill, which sometimes will cause stuttering, because enhanced sync always causes stuttering.   Man I'm starting to miss the NVidia setting of just putting vsync on in the driver and everything just working.",Negative
Intel,I did some testing AND as far as I can tell I do think it's actually fixed finally,Positive
Intel,I would continue buying their GPUs if they gave me something to buy.  The XTX has no upgrade path on RDNA4.,Neutral
Intel,"I had Nvidia for years, the main reason I switched was that the drivers went to shit last year. I'm just sick of them in general, too. The 7800 XT I bought has been one of the most trouble free cards I ever had, aside from Adrenalin randomly closing in certain versions.",Negative
Intel,"If I could get my hands on a 5070 Ti I’d happily switch. AMD likes to take advantage of the underdog, for-the-people image whenever it’s convenient but they’ll just as quickly throw us under the bus and fuck us raw once they’ve got the bag.  Is Nvidia a gang of greedy fucks? Sure. But at least the bullshit’s right out front where you can get a good strong whiff of it. You know what you’re in for.",Negative
Intel,"I purchased a 7700 XT and a 7600 8gb I'm March and while I'm satisfied with performance, it would definitely be awesome to have FSR 4 on both cards as FSR 3 and 2.2 (overwatch )leave alot to be desired",Positive
Intel,It's been so long bro :( Hopefully the fix comes with ray regeneration support?,Negative
Intel,"Hey Vik, is there any info for FSR4 Vulkan support?  It's quite sad to see that there still isn't support for it as it has been 9 months by now since the release of the 90 series  Also is there any info about the EAC issue with Star Citizen and the latest drivers?",Negative
Intel,"Amd Noise Supression doesn't work, when I try to turn it on, nothing happens, but in 25.9.1 it works",Negative
Intel,"Hey amd\_vik is amd Aware of the 1 year on going Darktide issues with amd  ( GPU , and specially X3D cpus ? ), and that even the Dev of Darktide ( Fatshark ) seemingly gets ghosted by amd ?  heres some more info specially first links includes a few Dev comments  [https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462](https://forums.fatsharkgames.com/t/investigation-poor-performance-power-draw-issues-impacting-amd-radeon-6000-9000-series-gpus/107462)  [https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f](https://www.reddit.com/r/DarkTide/search/?q=Performance&type=posts&sort=new&cId=4bd6e7a2-8389-4e6d-8f79-d42d57b8562c&iId=eba79a3c-b764-4712-a529-951dc1e87c9f)  [https://forums.fatsharkgames.com/c/darktide/performance-feedback/97](https://forums.fatsharkgames.com/c/darktide/performance-feedback/97)",Neutral
Intel,"Vik, weren't you on holiday leave? xd",Neutral
Intel,Any fixes for the SecondLife issues we've had the last few months? last driver that didn't break textures was 25.9.1,Negative
Intel,Will this update fix some of the artifacting I’m seeing in cyberpunk with fsr enabled?,Neutral
Intel,Also getting driver timeouts in Cyberpunk with RDNA3 with raster or RT. I did not have these problems with my RDNA2 card.,Negative
Intel,"The AMD FSR ML-based Frame Generation option in the Radeon panel disappears in Windows 10.  So I have a question: Is ML-based Frame Generation no longer usable in Windows 10? This option is available in Windows 11, but not in Windows 10.",Neutral
Intel,Can I join if mine's just an XT?,Neutral
Intel,What GPU are you using?,Neutral
Intel,Try reinstalling Windows. That fixed it for me.,Neutral
Intel,"This doesn't work. We are talking about games that crash with or without it, the only difference being the older AMD driver working.",Negative
Intel,I already install the latest update before update drivers its not update related. Vulkan driver is the problem in indina jones and silent hill 2 after windows update 25.11.1 not crashing ray tracing enabled but in 25.12.1 its broken again. So driver is the problem...,Negative
Intel,"I have the 7800 xt hellhound i F love it, tbh i care less about this redstone thing but its frustrating why a 2 year old lineup is abandoned all of a sudden",Positive
Intel,"You need to compile VKD3D with specific flags to enable this, it's not enabled by default. Based on the changelog, it appears that AMD has blocked support in the standard build.",Neutral
Intel,"They said earlier in 2025 they were working on FSR 4 support for RDNA 3, and then it leaked in September with the INT8 version...",Neutral
Intel,"They might as well have lol, they aint getting no new features",Neutral
Intel,They also promised features to the few of us who bought 7900 XTX. Good luck defending them when it's your turn to be disappointed.,Negative
Intel,I expected them not to abandon their king card lmfao. Who does that,Negative
Intel,"Not really, they teased the possibility of including other architectures.",Neutral
Intel,Maybe next time you should read the whole thread before replying.,Neutral
Intel,"It's also related to getting new features in generations other than just the latest one, ""bro"".",Neutral
Intel,"> I'd double down on the consumer market to insulate from the impending bubble burst  If that bubble bursts nobody is going to have much money to spare for consumer goods. That bubble bursting will tank the entire economy along with it.  *Long* term that might work out better, though.",Negative
Intel,further reminder amd is not your friend sadly,Negative
Intel,Same for me but Doom Eternal. I play at 4k and it needs upscaling at that res.,Neutral
Intel,What if I'm on RDNA 4?,Neutral
Intel,Yeah there are no good choices,Positive
Intel,https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html,Neutral
Intel,[https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RYZEN-CHIPSET-7-11-26-2142.html),Neutral
Intel,Adrenalin is for GPUs.   Chipset is for CPU & mobo.,Neutral
Intel,Awesome! I will try that 🤞,Positive
Intel,"No, it's not because it's a driver issue. AMD needs to act up",Negative
Intel,"Yeah im sorry for all of us, already shopping for a 5080 rn…",Negative
Intel,Yes that’s the one. I have no idea where to turn lol,Neutral
Intel,Sad news. Nvidia still supporting old extensions.,Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 8.   Be civil and follow Reddit's sitewide rules, this means no insults, personal attacks, slurs, brigading or any other rude or condescending behaviour towards other users.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",Negative
Intel,oh wow i haven't had any issues yet but that doesnt mean much. 25.11.1 i didnt have issues for a week or so.,Negative
Intel,"Interesting, I’ll test it today. I was crashing non stop on 25.11.1 so hopefully this update fixes it",Positive
Intel,"cod crashes saying driver doesn't meet requirements, so i upgrade to the ""optimised for BO7"", crashcrash crashcrashcrash, roll back one by one with the same results get to 25.8.1 and stable again, crash once every 2/3/4 weeks before this one it was 24.12.1 crazy, a dozen updates every year and only one or maybe 2 are stable enough to enjoy shooting zombies for any length of time",Negative
Intel,Same,Neutral
Intel,6970 here,Neutral
Intel,Literally the one game I don't play lol,Neutral
Intel,"Yay, I own that one",Neutral
Intel,"It's not even out for Darktide yet either. Fatshark clarified that it's experimental and needs more work, so it's not in the live build",Negative
Intel,So it's under the umbrella of the fsr4 override if I understood this correctly. For the fsr2 and 3.0 games I can use optiscaler right? Sorry I just bought a 9070xt coming from nvidia so I need to get used to these things.,Neutral
Intel,I used to do that but a few games can use the FSR4 in driver upgrade.  The enhanced sync was nice too when it worked.,Positive
Intel,"Unfortunately I play games and run software that require Windows so I have it on a separate drive. When I do switch to it (and I update the driver to take advantage of new features), this shit typically happens along with a slew of forced updates.  You are right though, I do primarily run CachyOS.",Negative
Intel,found the linux user,Neutral
Intel,You're talking nonsense.  Engineer managing 2k endpoints and several hundred servers.,Neutral
Intel,That explains why it fucking disappeared,Negative
Intel,I don't think on modern CPU's it is going to hurt performance.,Negative
Intel,I don't like the install manager either (seems a tad buggy still) but performance/memory wise it costs hardly anything.  You can ignore it from that perspective.,Negative
Intel,Wasn't the dude's claim it has been always bugged with AMD,Neutral
Intel,🌍👨‍🚀🔫👨‍🚀,Neutral
Intel,It's barely an improvement.,Negative
Intel,It's branding,Neutral
Intel,"Yes, RDNA4 refers to the RX9000 series.",Neutral
Intel,"Uninstalling the install manager brings back the ""check for updates"" functionality until you update again (and have to re-uninstall the install manager)",Neutral
Intel,I have one of these captures if you want it (error code 0x00000119). I've been having a TON of driver timeouts and BSOD for the past couple of driver versions and I've had to roll back to October to resolve them. Seems like any app that has hardware acceleration enabled causes it and exasperated when viewing the system via RDP.,Negative
Intel,Let us know how it goes!,Neutral
Intel,"I don't know how much of an impact this could have on perf since it's not something I've measured. I personally wouldn't do this, though. With a dGPU installed I keep iGFX off.",Negative
Intel,"performance wise it should only be a couple frames of latency, when doing rendering on dgpu and going out through igpu it'll just copy over the frame buffers.   Main impact is on pcie bandwidth as it'll use up quite a lot there, and to a smaller degree RAM load, so you definitely don't want to run some other dynamic load on the igpu when gaming to overwhelm its pcie link. I think on 7000/9000 it's x8 so it may be fine? But I'm really not sure could be x4 too",Neutral
Intel,"We're tracking a failure in silent hill 2 remake, I believe a fix is aligned to a future release. I'll need to check in with oblivion remastered",Negative
Intel,"Do you have to do that convoluted setup and download the drivers from Limewire, or has Optiscaler wrapped it in to their application?",Neutral
Intel,"So, no official release... ;(",Neutral
Intel,Any tutorial for a noob on RDNA2?,Neutral
Intel,what workaround?,Neutral
Intel,"Same issue with fsr4 on rdna1-3.   It shouldn't be this difficult, it's in a perfectly working state made possible by like one guy's few days worth of work.   And yet AMD just doesn't do it...",Negative
Intel,FUG,Neutral
Intel,Enhanced sync makes games super stuttery even in  25.9.2.,Negative
Intel,So I tried out enhanced sync and the game that I first noticed the issue on is no longer an issue. I've checked like 5 other games and no issues so far. Maybe it's fixed?,Positive
Intel,I have my 5070ti build hooked up to my 240hz Ultrawide Oled because of Multi-Frame Generation. I was hoping that would be part of Redstone but it's not.,Neutral
Intel,"Such a relief, but i am also annoyed because they are ignoring 7000 series... I can literally use FSR 4.0.2 on my 7700XT and it is WAY better than FSR 3.1....",Negative
Intel,I hope it is fixed for me as well 😭🙏. Thanks for the info.,Positive
Intel,yep would have upgraded but with an XTX.... you can cut your vram in 2/3 and have less Raster performance for a good upscaler and better RT performance it's such a stupid fucking problem....,Negative
Intel,"That's not something I'm privy to, but it could be worth reaching out to them to request looking into if they're not already.",Neutral
Intel,"I'm not privy to any of the FSR stuff - that's a different team to mine. I can pass on the feedback.  The Star Citizen EAC issue should be addressed, please let me know how it is.",Neutral
Intel,i still am!   so many fixed issues out of the release notes that I felt the need to stick around and help clear things up in the communities I frequent. I'll go back into hiding again soon,Positive
Intel,"I've seen something like this over at OCUK Forums but weren't given enough data to work with. We've attempted to reproduce a corruption issue but apparently we've not been successful.  Can you give me a step by step breakdown on how to hit this, as well as a clear depiction of the issue?",Negative
Intel,"No, XT peasants needs to form their own group.",Negative
Intel,6800XT.,Neutral
Intel,Some of their marketing said they would like to get it working if possible.,Neutral
Intel,"There are already third party options, but it would be nice to see if Steam Machine drives INT8 FS4 support since it runs on RDNA 3 tech. Let's see what happens in 2026.",Positive
Intel,Yeah there are going to be serious consequences as major retirement funds have invested in all these AI stocks because they have made so much money.,Neutral
Intel,"Give it a try, for my 6800xt it's crashing in almost all games...  ![gif](giphy|QMHoU66sBXqqLqYvGO)",Neutral
Intel,Where does that say rdna 3 is in maintenance mode?,Neutral
Intel,"No, it didn’t 😣",Negative
Intel,"Sorry, out of curiosity, if you close it, it won't let you play? What do you get? Could you send me a photo so I can understand?",Negative
Intel,"I agree. Please can you raise a ticket requesting support for this over at our GPUOpen and ask other end users and developers to upvote it and leave a comment registering their interest? (please share a link to it here if you do) https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues  As far as I'm aware, the impacted titles are: IL-2 Sturmovik: 1946, Neverwinter Nights Diamond Edition and Call of Duty. If there are any others, I would really appreciate you letting us know.  E: I believe it's posted here: https://github.com/GPUOpen-Drivers/AMD-Gfx-Drivers/issues/80",Neutral
Intel,"Just an update - I ended up running DDU and re-installing the latest update and now things are pretty stable, no driver timeouts from hardware accelerated apps either. Could be something to do with the architecture change between driver packages - but doing a complete removal between updates seems required now.",Neutral
Intel,I never seen 1 crash on 25.11.1 although I did use the preview update for windows 11 last week which fixed some amd gpu related crashing and that solved my arc raiders random crashing,Negative
Intel,"Also it's considered pretty poor Raytracing. People are usually snobbs who go ""I can't tell the difference"" when it comes to raytracing as a coping mechanism but darktide is the one game I can see why, it's not very noticeable and not worth it.",Negative
Intel,"yes, 3.1 is where AMD adopted the same modular approach as nvidia so any game at fsr 3.1 or above just runs at whatever latest fsr version your driver supports, which is currently 4 although now the versions aren't numbered anymore",Neutral
Intel,Hell yeah 👍🏻   Impressive you can run that on a 5x86,Negative
Intel,"Since you're already an advanced user, perhaps you could block it from installing by selectively blocking AMD in your hosts or pi-hole? It's not a dumb solution, but it's better than having to deal with push-installs.",Neutral
Intel,I might be an ass but I’m not wrong,Negative
Intel,Sorry  If you’re a **consumer** and want to be in control of what’s on your computer then Windows is not the OS for you,Negative
Intel,"so if you install it, then uninstall it, you're saying the button never reverts back after fully removing it? If that's the case, let u/amd_vik know about it. And provide dmp logs if he requests them. Screenshots of before & after wouldn't hurt either.",Neutral
Intel,"The difference is probably 100 points in Cinebench R23 score on a 9800X3D, which should be scoring 23300 points at stock.  Is that significant? Not really, but what if you're running 5 update managers, Spotify, Chrome, RGB software, temperature monitoring, audio software for an external sound card, mouse configuration software for your mouse, macro software for your keyboard...  I personally prefer to not run anything in the background, because the alternative is death by a thousand cuts.",Neutral
Intel,"Yes, If you mean the bad frame pacing when fps is lower.  I still opt to spent 1-200 hrs of my gaming session with FSR 3 frame gen, 7900xtx.  It's not that bad when the output is close enough to monitor max hz, similar to what hardware unboxed did in thier test.  The generated frame still comes out too early but it has to wait for the monitor's nest refresh which is consistent.",Neutral
Intel,ty,Neutral
Intel,"u/amd_vik it sounds like this person doesnt want the manager to install again, but I am pretty sure you can do custom option to uncheck it. If you do express of course it will put it back sschuler.",Negative
Intel,can you run analyze -v in windbg or fire it over to me via your preferred file sharing method?  I personally like to use https://send.vis.ee,Neutral
Intel,Can confirm this issue is fixed for me on 9800x3d + 9070xt (I had this issue on 25.11.1 and reverted to 25.10.2 until today) 👍,Negative
Intel,"Seems to be working fine, though when I was installing the driver my igpu showed up separately from the dgpu in the installer with a download link. But when re-running it they both show under 25.12.1  Should I be installing some separate older driver for it to keep things like hw accel working or was that just some hiccup?",Neutral
Intel,Oh great will also test after work it’s been headache since last driver update,Positive
Intel,Thank you for taking the time to respond. This has been very frustrating.,Positive
Intel,"I'm sorry to comment directly to you here. Do you have any report about monster hunter wilds performance drops in recent GPU drivers ?    I'm using 9070xt.    I have to use version 25.3.1 to play wilds with no stutters, anything newer gives a lot of stutters in many places.",Negative
Intel,"Yeah you still have to download it on your own, the creator of Optiscaler already said they aren't going to bundle it probably due to the whole legality around it.",Neutral
Intel,"i saw a post that detailed how to essentially replace noise suppresion with the working version in newer drivers, you can probably find it here somewhere",Neutral
Intel,worked fine for me idk,Positive
Intel,still busted for me,Neutral
Intel,"Also from what I can tell, enhanced sync is fixed at least on my end.",Neutral
Intel,Thank you for this! been waiting for a fix with Star citizen.,Positive
Intel,Yeah SC seems to be working for now.,Neutral
Intel,"Bonjour, pour le moment sur Star citizen le problème avec EAC fonctionne pour la 7900XT. Merci d avoir réglé le problème. Bonne fêtes de fin d'année.",Neutral
Intel,That's good to hear. What about Noise Suppression not working since 25.9.2?,Negative
Intel,Hmm let me try. So pretty much having installed the latest driver (25.12.1) I just open SecondLife. I look closely at my avatar/character and my skin looks like this  [https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4](https://i.gyazo.com/9285c648e1163ab0fcc653e1a22ac88b.mp4) (excuse my outfit but just easier to show)  this is how it's supposed to look and also does on 25.9.1 [https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4](https://i.gyazo.com/3fe61911f122ff21eac6af805c69c3c1.mp4)  I've heard that this doesn't occur on linux but only windows (But I don't have linux so can't say for sure)  I think you need PBR / Materials or some reflection on your skin to see the issue.   If you fly up to around 2000+ meters above ground it becomes easier to see  These are my settings [https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png](https://i.gyazo.com/5686c88a62ea2c9ef8f721db34453c90.png)  I have an rx 7900XTX,Neutral
Intel,"Hello! I am actually one of the developers on the client team for Second Life, and I have been trying to figure out how to get in touch - we have found at least one nasty bug on some of the Strix Halo chips with the current drivers.  Can you send me a message here so we can exchange emails?",Negative
Intel,😭😭😭😭,Neutral
Intel,"I had a similar issue with my 6800xt and the other thing that helped was to sit it to fullscreen or borderless and swap back and forth. Now I'm only playing in fullscreen (which is annoying), but it doesnt crash anymore.",Negative
Intel,I have the same card and exactly the same problem. Can't install newer drivers or BF6 just constantly crashes.  I'm on 25.10.2 tho,Negative
Intel,"And it is, and they did, we have the leaked int8 version from September... Just needs official driver implementation now.",Neutral
Intel,Ive had zero crashes for 2 weeks on arc due to the preview update which rolled out to the public this week. You could try underclocking your gpu if it boosts over the limit.,Positive
Intel,"Before the Black Ops 7 (which I don’t own) integration to Warzone, I could click off it & carry on. But since the integration it just closes the game.",Neutral
Intel,"Yes, i have created this github issue.",Neutral
Intel,"Depends on where you look at it. The Morningstar has a bunch of pre baked lighting effects, which make the difference completely unnoticeable unless you study the frames  In mission it makes a pretty significant difference, but wether or not that's something anyone cares about in a fast paced game is another thing entirely.",Neutral
Intel,"They aren't numbered in the sense of like 4.0.2 or like there won't be an ""fsr 5""? Thank you very much btw, very helpful info!",Positive
Intel,Like a charm. :D,Neutral
Intel,"I probably could, but AMD (and any other company, really) should be following the users preference anyways. It is a band aid fix and doesn't solve the problem.  Not a bad idea though.",Negative
Intel,I've been using computers since dos 3.  You're a spanner.  I'm sure MacOS is soooooo much more open.,Neutral
Intel,"I think it's working as intended. If you remove the dedicated install manager, the option to check updates in Adrenalin will reappear",Neutral
Intel,"Having tons of useless services constantly pinging and invalidating cache, in fact, sucks.",Negative
Intel,Well look at the scenario implied. Piling up tons of startup apps is going to have a small impact on a bench testing score. Those synthetic tests dont necessarily translate into a big FPS loss.,Neutral
Intel,"Thank you for the idea, I just tried a custom install during an update, was given 2 choices (update/dont update driver and install/dont install privacy view). After installing drivers, step 2/2 was installing the install manager.    After updating through adrenaline using the custom option, I attempted reinstalling again using the auto-detect, custom install. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.   Installing via the WHQL package, custom install follows the same steps as above. I was given the option of install/dont install privacy view. The driver choice was not selectable and it reinstalled the install manager during step 2/2.",Neutral
Intel,Here you go: [https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw](https://send.vis.ee/download/2b9c553519ec5d1a/#WAbve98Ky-73b6ruGpvyyw)  I did run in windbg but I have no idea how to save the output unless you just want a copy + paste of it here haha,Neutral
Intel,Appreciate the feedback,Neutral
Intel,Thank you for confirming.  That interesting though. I think the most seamless way to support products from both branches is to use the AMD auto detect tool. Can you tell me how the iGPU is represented in Windows' Device Manager?,Positive
Intel,"[https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html](https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html)  https://i.redd.it/3vxsa8yave6g1.gif  If you suspect the installation is incorrect, download the package that includes the IGPU driver using the link provided above. The basic version does not include the IGPU driver, but provides a separate download option during installation.  Anyway, it seems like a lot of bugs have been fixed in this version.",Neutral
Intel,"if you can find it, you will be the goat",Neutral
Intel,"Yeah everything was fine until I tried to play Resident Evil 2 Remake, then the issue was back but not in the other games. This is a very odd issue.",Negative
Intel,Nvm the issue I was having just happened in Resident Evil 2 Remake.,Neutral
Intel,Thank you for letting us know 👍,Positive
Intel,appreciate the info. I'll ask our technicians to check in with the settings you've provided,Neutral
Intel,I can confirm there is no issue in linux. A windows version running under proton in linux has no issues as well.   In the video there is flickering on head and body. I see only flickering on the head (when running it on the windows pc)  But my body has no layers attached - the body in the video usually comes with layers. But all heads have multiple transparent layers. The problem occurs even when that layers are not in use and are fully transparent.   Probably related.,Neutral
Intel,"Hey there, thank you for reaching out!  I don't suppose it would be possible for one of our devrel folks to contact you via a linden lab email address like business@lindenlab.com?",Neutral
Intel,"But he's not talking about ARC. Oblivion Remastered, Silent Hill 2 Remake and Indiana Jones still crash with the update and the latest driver. No crashes (at least with low rear tracing), under  25.9.2.",Neutral
Intel,"So if you click dismiss, the game closes, did I understand correctly? It doesn't let you enter the COD HQ ? I'm telling you this because I too should update the bios, in fact it happens to me too, but I click dismiss and it lets me play anyway.",Negative
Intel,"I just tried it myself and no I still stand by my statement that there's barely much of a change, I guess it's just subjective on this game. Agree to disagree.     I also still stand by the fact that I don't think enough will use RT to warrant it updating to use caching.",Neutral
Intel,"there won't be an ""fsr 5"" because any game with fsr implemented from here on out should, in theory, be compatible with every future version of fsr made, so numbering them isn't as meaningful. they're probably just going to stick with unofficial codenames like redstone for diffrentiation. Nvidia still uses versioning for DLSS despite it using the same system because it's good for marketing and diffrentiation so I'm not sure that dumping the version numbers is a wise decision but it also makes sense",Neutral
Intel,"I agree with you wholeheartedly, but super users do what they do best - sudo that shit. x)",Negative
Intel,"I've never had AMD Chat or Privacy View force install, I hate they show up in the available software to install when updating, but I just dont click to install them lol, just update the gpu/chipset drivers",Negative
Intel,"…Have you no idea what Microsoft has been doing to Windows in the last decade? I use Linux on all of my Desktops and my home server (total control of what software you install), and Apple for my laptop/phone. Say what you want about Apple but they respect privacy and don’t nag you every two seconds about some AI nonsense or OneDrive.",Neutral
Intel,The install manager never showed an update for me; I always have to download the driver manually from the website.,Negative
Intel,"Besides, its not Windows 98 anymore.   Background apps are just not that intensive anymore.",Neutral
Intel,I guess a snippet of the faulting component from the output would work.  This is a minidump. Do you have a kernel memory dump>?,Neutral
Intel,"sorry i missed this, seems it had expired. maybe someone downloaded it before i did?",Negative
Intel,"Right now in devmgr with re-running the driver installer from the site things look like this https://u.numerlor.me/2faMBA . I also remembered adrenalin has full driver details and everything looks fine there https://u.numerlor.me/w1Snxw https://u.numerlor.me/EOclpA so I think it was just the installer being a bit confused.  Compared to the installer on the first screenshot, when doing the actual update (from inside adrenalin) the Radeon Graphics was a separate item, and had a ""Download driver"" or something along those with the link I mentioned",Neutral
Intel,What about the combined exe? It's still available? That will install both gen but was bugged with control panel disappearing on previous driver.  The combined exe is around 1.6GB.,Neutral
Intel,This might be it? Worth a shot I suppose.  Edit: This worked for me on the latest driver  [https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/AMDHelp/comments/1oj27fj/comment/nr9h4ig/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),Neutral
Intel,geenz@ but yes,Neutral
Intel,"Hmm, when I can, I’ll have another look! Thanks!",Positive
Intel,"That's fair. I think the lighting is the only thing that particularly makes a difference. Makes the lighting a lot more realistic, but the reflections are pretty worthless.  But like I said, it's also not necessarily something you'd really notice while playing, since it's not exactly a stop n look around type of game.  >I also still stand by the fact that I don't think enough will use RT to warrant it updating to use caching.  I don't disagree at all. Especially when optimization for AMD hardware in that game has been a substantial criticism since it's release, to the point that ray tracing was compl disabled on AMD cards for a while",Neutral
Intel,"I do not, only the minidump but I've uploaded it again here [https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN\_IxLREw](https://send.vis.ee/download/45e58a7ca188aa6c/#n9lCG59Od0RicrN_IxLREw)",Neutral
Intel,"Yes it should be fixed under that scenario, and the combined package is linked on the release notes:  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-12-1.html  https://drivers.amd.com/drivers/whql-amd-software-adrenalin-edition-25.12.1-win11-c.exe  Kind of guessing here but I believe the '-c' towards the end of the file name denotes a combined package spanning RDNA support.",Neutral
Intel,This worked for me btw - did it a few days ago before these drivers dropped. When I update I'll be using the same method.,Positive
Intel,thanks a bunch. I'll pass this on to my ISV contact and see where we get with that.,Positive
Intel,news ?,Neutral
Intel,"huh, that's odd. Do you have any larger files over at       C:\Windows\LiveKernelReports\WATCHDOG\",Neutral
Intel,"Installed the c one. And seems to be working fine. 780M and 6800 here. Still when selecting a specific GPU for a specific app, both energy saver and performance show 6800. This bug has been forever. And it's probably just a registry key when the driver install. Win11.",Neutral
Intel,I do actually have one in there that's 17MB from a BSOD yesterday caused by the AMD driver,Neutral
Intel,"My PC won't wake up after sleep on previous 25.10.2 so I have downgraded to 25.9.2. It seems that AMD has added into Known Issue in 25.11.1.  >Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.   Ahhh. It is caused by HDMI 2.1... I can't use a DisplayPort on my LG C2 42"" TV sadly so I'll have to stay on 25.9.2 for now.",Negative
Intel,"wait, so the branching did not happen? for RDNA2? I was under the impression that it was already in effect.",Neutral
Intel,"That last known issue is what a lot of us experience. Not fixed, can't use this one either.  Leaving your pc for long enough, like 25 minutes and your system just BSOD quickly into reboot.",Negative
Intel,I just did a clean DDU install to 10.2 last night because something weird was going on. Of course 24hrs later a new driver drops,Negative
Intel,anyone know if the low gpu usage was fixed for Battlefield 6? I had to roll back to 25.8.1,Neutral
Intel,"After install i cant open Adrenalin app, I get starting up for a few seconds and then it's closed and the tray icon is gone, too 😿.",Negative
Intel,I'm going to wait to see if others find it stable before I move on from 25.9.1 I think.,Neutral
Intel,Anyone can tell me if the new release fixes the Adrenalin Panel not showing when trying to open it? I’ve spent 1 entire afternoon try every solutions given by Google but today the problem is still there…,Negative
Intel,"So what's the issue with Cyberpunk 2077? It's been present for quite a few updates now.   I'm asking because I've owned the game since day one but I haven't been able to play it cause my old 1060 6 GB was struggling hard with it, since then I've upgraded to a 7700 XT and for one reason or another I haven't gotten to play it yet but every time I update my driver and check the patch notes it's always a problem with it.   Can anyone with it installed and on RDNA3 tell me if it's playable?",Neutral
Intel,So does this mean Arc Raiders will stop randomly crashing in Windows?,Neutral
Intel,Just installed these zero issues so far!,Positive
Intel,"I dunno what people are expecting from Redstone?  It's on the game devs to implement, Blops 7 has the AMD Ray regeneration element of Redstone baked in.  It's not gonna be some driver toggle and all of a sudden you've got ray regeneration across all games.  Also all the people claiming ""not every game needs an optimized driver, Arc didn't get one"" when the RDNA 2 controversy happened, look, there it is, the optimisations were just late, hopefully it'll fix the crashing some people had in the game.",Neutral
Intel,Anyone know if this fixes the pink artifacting on Chromium based applications for the 7000 series GPUs? Can't check myself because I'm at work.,Neutral
Intel,There was a long delay with the blank screen. Made me a bit nervous,Negative
Intel,At this point i'm sure that cyberpunk will never be fixed.,Negative
Intel,Apparently I'm staying at 23.9.1 because I wanna keep using FSR4 INT8 with my RDNA 2 card.,Neutral
Intel,No fix for being unable to enable Noise Suppression...,Negative
Intel,When does Linux get this,Neutral
Intel,"Unfortunately after updating to 25.11.1 (even with a fresh install after using DDU), on a 9800X3D + 9070XT I am no longer able to open the Adrenalin software at all. It's the same issue as described in this post, caused by some conflicts between having both the RDNA3/4 drivers and RDNA1/2 drivers installed at the same time: https://www.reddit.com/r/radeon/comments/1okhlbw/_/  I had to roll back to the 25.10.2 combined driver which works fine with this setup without any issues, but yeah what a shame. Really hope AMD can resolve this issue shortly in future updates. You would think having both the latest gen AMD CPU and GPU would play nicely with each other, but alas...",Negative
Intel,"Why does the AMD install manager never find the updates for me? The AMD install manager only ever says the AMD Chat is available for install.  To get the updates, I have to uninstall 'AMD install manager' which allows me to manually check for updates in Adrenalin.",Negative
Intel,"Guys, I think I figured something out for those experiencing crashes. My RX7600 was overclocked in the default setting. I created a custom profile that matches the old default and seem to have achieved what appears to be stability in BF6.",Neutral
Intel,i am stable now in BF6 on AMD Adrenaline 25.11.1 ... and the game feels super smooth with FSR on 4k Ultra... so it was the AMD Driver 25.10.2 which was crashing ... annoying :-)  7800X3D + G.Skill Trident Z5 Neo RGB 64GB (2x32GB) DDR5-6000 CL30 + ASUS ROG Crosshair X670E Hero + 7900 XTX + Corsair Shift Series RM1200x,Positive
Intel,"Why is enhanced sync still broken? That was a feature I used to mitigate latency while capping frames to 120 and helped get rid of tearing. Now having it on causes major stuttering in a lot of games. I did come to find out that Vsync can be enabled globally and I'm not sure how long that's been a thing. Since being on the 5700 XT, 6900 XT, 7900 XT and now 9070 XT, I was never able to use it globally but it's been quite a few drivers since I've checked if it worked. I use Vsync and Gsync on my 5070 ti build and notice little to no added latency so I'm glad this is doable with Vsync and Freesync but I did like using enhanced sync and capping the frames to 120 better but this will do.",Neutral
Intel,"It's been almost 6 months and the 9060XT still crashes in DX12 UE5 games, especially with FSR4.  FFS AMD how long is a fix going to take?",Negative
Intel,Windows update keeps trying to update my driver.,Negative
Intel,I'm still having problems with FreeSync stuttering with the RX 7900XTX and this driver. Only when I revert to version 25.9.2 are the stutters gone.,Negative
Intel,No FSR4 on RDNA3 no care,Neutral
Intel,"For how long ? One year already...Cyberpunk 2077 not fixed yet.   ""Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.""",Negative
Intel,Yes thank you AMD for fixing Arc Raiders. I had to revert to 25.9.1 to stop the exception access violation issues. This would happen mid-game and I'd lose my entire loadout. Here's to hoping it works.,Positive
Intel,Awesome no stated support for Outer Worlds 2.... I guess driver timeout while playing it is not a driver problem...,Positive
Intel,hardware ray tracing crashes both oblivion remastered and the outer worlds 2 after 5 minutes to an hour of play and it seems completely random on my 9070xt.   To even get it to last that long I had to set my core clock -300mhz and turn off variable refresh rate and hardware accelerated GPU scheduling,Negative
Intel,"I noticed that this fixed my Geekbench scores     When I got my 9070 XT a couple weeks ago, my Geekbench 6.4 scores for OpenCL and Vulkan were about 185K and 187K.  Then they mysteriously dropped to around 135K each, and I believe it was after updating Adrenaline.  Now I just updated to 25.11.1 and I'm back at 184K and 189K for OpenCL and Vulkan.",Neutral
Intel,"> The AMDRyzenMasterDriverV30 service failed to start due to the following error:   The system cannot find the file specified.  Source: Service Control Manager, Event ID: 7000",Negative
Intel,This driver was way better than the version before it(for me at least).,Positive
Intel,"Since switching to this version, I've been experiencing constant driver crashes. i use RX7700XT sapphire pulse GIGABYTE B650 Gaming X AX V2 with 6000mhz 32 gb ram. I don't know which driver caused the constant pink artifacts when I have graphics card acceleration enabled, but it's getting worse with every update. Why AMD? -.-",Negative
Intel,"Sorry, I'm not very expert, I installed AMD version 25.11.1 from 0, before when I started the PC AMD was already open now instead the icon appears but if I press it says ""Amd adrenaline starting up"" is everything normal?",Neutral
Intel,For me the driver just times out randomly during normal stuff like youtube shorts. Today I opend steam and the driver timed out. That never happend with 25.10.1.,Neutral
Intel,"The Adrenalin Software instantly closes and restarts if I try to click on the ""Record & Stream"" tab (no crash/error report, it simply closes and then restarts in background).       Dunno if it's from 25.11.1 or not, it was the first time I was going to try it. Didn't tried a DDU full reinstall either, just a simple reinstall of the driver but for no use. Guess I will just use other software for recording so whatever but I'm curious if it's really a driver issue since I got no report pop up at all.  Gpu is a 9060 xt 16 gb.",Neutral
Intel,"I am an RX 6800 user, so on the RDNA branch of the driver,  25.11.1 introduced a severe performance regression on a DX9 game (Fallout New Vegas) , with framerate basically getting halved over what I had before  Downgrading to 25.10.2 fixed the issue , not sure if replicatable (I use 70+ mods) , and not sure if it affects any other DX9 games other than Fallout: New Vegas  (My cpu is an intel i5 10400F in case that matters, in New Vegas, on the 25.10.2 Driver, cpu utilization is almost always between 70-100% on the primary core that the game uses, while on the 25.11.1 driver , it was consistently at or below 50% , which leads me to suspect the new driver caused a regression in CPU utilization in DX9 and/or old single-threaded games, downgrading to 25.10.2 completely fixes the issue )",Negative
Intel,"Still no Redstone update. Well, wait for 25.12.1 just started. Hope AMD end the year with a bang.",Negative
Intel,"Brooooo, they didn‘t fix the flickering in BF6 when recording…",Negative
Intel,* Intermittent application crash or driver timeout may be observed while loading a saved game in Cyberpunk 2077 with Path Tracing enabled. AMD is actively working on a resolution with the developer to be released as soon as possible.  * Fucking LOL.,Negative
Intel,25.10.2 completely broke vsync... not even a mention about this in the notes?,Negative
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q4 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1nvf7bw/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,There is new AFMF features too.,Neutral
Intel,Did they fix the crashing for Outer Wolds 2 on 25.10.2?,Neutral
Intel,bf6 fps drop fixed?,Neutral
Intel,What about the cursor lock when pressing hotkey for adrenaline overlay? Is this fixed,Neutral
Intel,"Hopefully they fixed the anti-aliasing this time...  Nah, i'm sure they didn't.",Negative
Intel,"Don't know if anyone else has experienced this in Battlefield 6 on the last 2 drivers but whenever I uses those my GPU usage always stay at 100% load even with 144 fps cap on a 9070 xt. I revert back to 25.10.1 and then it stops doing that, reaches maybe 80% max in menu",Neutral
Intel,"I've been getting black screens since a while ago on my 6750xt, could be after a random alt-tab when gaming or after logging into Windows, on my tv connected via hdmi it looks green but my displayport monitor it is black, what is weird is sometimes  I can win+L and see the login screen again but if I log in it goes black, also while it is black and pc hasn't frozen yet I use an app called chrome remote deskop and I can see and do stuff from my phone, weird. Tried DDU, new drivers, nothing fixed it.",Negative
Intel,How is the driver ? 7700 XT here.,Neutral
Intel,Finally a potential fix for CPU metrics? Look forward to seeing if it’s true!,Positive
Intel,Didn't they just release something already? Now we're getting another like. Do I have to update my rx 9060xt,Neutral
Intel,do yall use ddu for every driver or do yall just update it with the app?,Neutral
Intel,"New AMD update 👏👏👏👏, I'll install it! Send Redstone as soon as possible!!!!! Thanks AMD!",Positive
Intel,I just can’t wait for the instant replay to be fixed. Ever so often when I save a clip the infame notification starts glitching and I know that means the video I’m saving will have graphical glitches as well. It looks like big-ish squares of the image af slightly out of sync with the rest.,Negative
Intel,"There is a bug with Minecraft when using embeddium/rubidium or any forks on the latest driver. Many textures don't render at all. Launching through curseforge fixes it, which is very strange...",Negative
Intel,I'm not seeing this on my 6600xt. Only say 10.2 is available. Do I have to upgrade to that then upgrade to 11.1?,Negative
Intel,>Intermittent system crashes may be observed while using some high-bandwidth HDMI 2.1 displays during display standby. Users experiencing this issue are recommended to use a DisplayPort connection as a temporary workaround.    Well that probably explains the crashes from the last driver whenever I locked my PC but that workaround is not an option. Good to see it's been recognised and being worked on at least.,Negative
Intel,Has anyone else been able to get FSR4 to work again with BF6? Worked for me before the season 1 update.,Neutral
Intel,Think this broke Vulkan in POE2,Neutral
Intel,"Hi [u/AMD\_Vik](https://www.reddit.com/user/AMD_Vik/), other players and myself are having problems using DXVK with latest drivers. From driver timeouts to black screen. I'm particularly having problems with Fallout New Vegas, but there is reports in other games. How can I help in fixing these issues? Examples: [https://github.com/doitsujin/dxvk/issues/4999](https://github.com/doitsujin/dxvk/issues/4999), [https://github.com/doitsujin/dxvk/issues/5204](https://github.com/doitsujin/dxvk/issues/5204), [https://github.com/doitsujin/dxvk/issues/4851](https://github.com/doitsujin/dxvk/issues/4851)",Neutral
Intel,"After installing this update neither Cyberpunk 2077 or The Witcher 3 will launch through Steam anymore. The hitting play just attempts to launch the game and then turns right back into the play button. Only CD Projekt games, no issues anywhere else.  UPDATE: Actual games run fine if launched directly from their install location. It's the CDPR launcher that Steam usually auto opens that broke after this update.",Neutral
Intel,Hope this patch will fix the driver crash while playing Arc Raiders. It crashes in a way that slows my computer so bad and i have to restart it. Temps are fine. Everything is off except image sharpening.,Negative
Intel,I mean Arc Raiders runs perfectly fine even on 23.9.1. Game optimized new drivers are a joke.,Positive
Intel,Hi u/AMD_Vik  Thanks for the VR refresh rate fix. However a bug that I though was related but still isn't fixed are the Beat Saber VR game wall shaders as they are still broken/distorted. Those shaders work on 24.12.1.  Could you investigate this u/AMD_Vik?,Negative
Intel,I never updated to the most recent driver but when I open up adrenalin and the update manager it only shows the previous one 25.10.2  I had to go to the link to get the newest. Does it always work this way?,Negative
Intel,"Wish i could use this software propelry for my 7800xt, everytime i have adrenaline installed after couple hours of gaming the whole pc black screens and gpu driver crashes running with default settings on the gpu. You have to use DDU to get it back running, gave up with adrenaline and installed only the bare bone gpu drivers without adrenaline and installed msi after burned, it has been running for a month just fine under very excessive loads.",Negative
Intel,"25.10.2 already have Terrible Fps spike and stutter in Gaming, this update did not fix the Problem (wth happen amd??).. 25.9.1 is Still the Stable one",Negative
Intel,Did this fix the insane fps drops in 25.10.2?   Reported here https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps and here https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,Neutral
Intel,"Installing this on Windows 10 got me a system shutdown at the first go xD. Luckily, the second one went just fine.  Also, since you seemingly cooperate closely with Activision on CoD, can you fix CoD:WWII constantly crashing? I just bought a month of Game Pass to play the game, but it's basically unplayable.     It'd be nice if you somehow fixed CP2077 situation - FSR implementation, bugs etc. This game is a showcase every single reviewer runs, not Call of Duty...",Negative
Intel,Did AI create these new drivers?,Neutral
Intel,I can't even install it anymore as it doesn't recognize my iGPU (I have an R5 7600).,Negative
Intel,"The new version 25.11.1 still has the same problem that I had with version 25.10.2, that is, if while I'm in the game I press the Windows key on the keyboard, then when I return to the game the mouse cursor no longer works and I can't do anything anymore, which forces me to restart the PC, another problem is that when I open any game the overlay of the active Adrenalin techniques no longer appears in the top right, for the rest it seemed ok, but given the big mouse problem I mentioned above, I am forced once again as it was also for 25.10.2 to go back to version 25.9.1 which to date is the best and bug-free for my configuration with RX 9070 XT.",Negative
Intel,"Drivers fine for me on Arc Raiders so far, not had any issues with AMD drivers using a 9070",Positive
Intel,I'm glad the CPU metrics are showing again,Positive
Intel,"When I install this driver, I can't open the AMD Software any more. The start-up splash screen is shown for about a second and then it closes again. Doesn't matter from where I try to launch it. So it isn't the right click -> open bug.  There is no event in the Event Viewer.  I've reinstalled it, with prior DDU cleaning and disconnecting the internet connection, three times now... to no avail.  Anyone else?  Edit: Reverting back to 25.10.2 and it works fine. I'm tired of all these little quirks and annoyances I've had since I went for an AMD card...  Edit2: Tried 11.1 again and it worked now. The software started... once. The next time I tried to open it I got:   Download failed: Please visit [AMD.com](http://AMD.com) to download the compatible version of AMD Software.    I'm at the end of my rope here AMD... really getting tired of this",Negative
Intel,"Went to do the usual ""Leave AMD Experience Program"" after uninstalling the Installation Manager, but the option is gone.",Neutral
Intel,The update did not help. The problem with the driver crash remained (( ( Rx 7700 xt ),Negative
Intel,Any ideas for when the crashing when playing NBA2k25 is going to be fixed?,Neutral
Intel,"Is there another work around for system locking up?  My PC monitors never even turned off. I woke up this morning and the PC was simply frozen, had to turn off the power supply switch and turn it back on for it to work.  Couldn't even just do a hard reset.",Negative
Intel,"Im still having issues with easy anti cheat, rust game keeps crashing after a few minutes, maybe 2 or less",Negative
Intel,"2.5.11.1 fastest reroll for me to date, well done.   Booted arc raiders which now have support.  Game does not boot, instead I get a message frem arc davs that the driver has issues and want me to reroll to 25.9 😅   What a fucking joke",Negative
Intel,Shits been crashing my system since the update :( sapphire 7900xt,Negative
Intel,"Tested the new driver on 7700 xt, pink artifacts in browser and some weird flickering, some old bugs are fixed but  there are new issues instead, honestly it is not worth to update drivers at all if you find one driver that works without issues.",Negative
Intel,Still weird artifacts on COD MW2 game. Turned back to 25.9.1.,Negative
Intel,"With my Taichi RX 9070 XT OC after updating to the latest version 25.11.1 I still encountered the same problem that I encountered with the 25.10.2, which is that while I play Battlefield 6 press the windows button to go to the desktop and then return to the game my mouse crashes and I can't do anything, the only thing I can do is click ctrl+alt+delec and the mouse works and then I can restart the PC from there",Negative
Intel,"With this new driver, Adrenaline isn't automatically detecting Epic Games Store games (Steam games work fine). I tried with Fortnite, and it only adds to the games tab after launching it, but it doesn't work with ARC Raiders. I tried adding it manually, but I couldn't get FSR4 to activate. Is anyone else experiencing this?",Negative
Intel,"GV-R9070XTGAMING-OC-16GD    I have problems with Graphipcs since day one i bought from amazon.de. There is no driver that prevents some games from black screen and crashing on desktop, also Adobe Effects alongside Fortnite and others. The error is always amd software has detected a driver timeout on your system. Everytime i send logs, they updated drivers every 14days but no driver helped. I also made registry fix with increasing timeout from default 2s to 8s. Tlddelay did nothing, random black screens and app crashes to desktop. Also tried another cable DP instead of HDMI.    What can i do ???",Negative
Intel,"If anyone from AMD sees this. The recent drivers cause The Division 2 to consistently crash. It sometimes happens after 15m. Other times a few hours but is GUARANTEED to happen at some point. When it happens it's a hard reset case and doing so after about 10 times eventually wrecked my boot/login so I had to re-install Windows (bad AMD, spank!).   I assumed it was an issue with thelatest Windows update like the one that broke the other UBI Assassins Creed games a while back but when I installed 25.9.1 (with factory reset) I haven't had a single crash since.     7900xtx & 98003d.",Negative
Intel,Dose Arc raiders works now or not on new release 25.10.1 had prob with that game could not run it must go dx11,Negative
Intel,"Since this driver update my pc is unusable, only one screen loads the other stays black and after the os loads the screen just freeze, I can hear os sound like connecting and disconnecting of USB but the picture is frozen, I have the rx7900xt I tried to completely take the GPU off the motherboard and connecting back (after reinstalling the driver with factory reset when connected to the cpu display port) and it worked for some hours but after shutting down the pc and booting the next day it came back, you're saying the only fix for now is to roll back to previous driver?",Negative
Intel,У меня Мультимедиа контроллер выдает ошибку. Для этого устройства отсутствуют совместимые драйверы. (Код 28),Neutral
Intel,Noise Suppression still broken. 3rd release without that functionality in a row.,Negative
Intel,"Hay un bug que me suele pasar con varios de los ultimos drivers... cuando desintalo los drivers, la pantalla no vuelve, y no me deja saber cuando la desinstalación del driver termino, debo reiniciar la PC. Con RX 6800 XT.  Le eh pasado DDU, pero el error sigue estando.",Neutral
Intel,"Not sure if anyone else is experiencing this, but after this update, Adrenalin acts like BF6 isn't open so I can't force frame-gen thru the driver. On both 25.10.1/25.10.2 and 25.9.2, enabling frame gen in game doesn't work, so I've had to do it through the driver. On 25.11.1, NEITHER are working, frame gen completely non-functional. Tried DDUing/factory resetting 25.11.1, didn't work. Rolled back to 25.9.2 and works normally again...",Negative
Intel,"for some reasons, whatever game i play it either closes itself or looks so bad visually that the games (yes, games) are unplayable so i have resorted to uninstalling all AMD graphics software (drivers and applications) and am going to try to re-install it and see if i can choose a previous driver",Negative
Intel,is the horrible stuttering/flickering (feeling like dynamic hertz and micro stuttering) experience from the 25.10.x drivers fixed? If not I have to stay on 25.9.2,Negative
Intel,"This version, perhaps even the previous one, installs the AMD Adrenalin Edition software even if I select Driver Only when installing the drivers. Can you solve it?",Negative
Intel,"Adrenalin 25.11.1 terminating itself right after launching.  Can't run Adrenalin UI(App, Program...)  Seems like iGPU & multi-monitor related problems.  for more info [https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin\_25111\_not\_opening\_after\_update/?sort=new](https://www.reddit.com/r/radeon/comments/1ox1gd8/adrenalin_25111_not_opening_after_update/?sort=new)  I'm going back to 25.10.2",Neutral
Intel,Anyone else having trouble even getting the software to open since the update?   I've done a clean uninstall and reinstall of the drivers and software twice and Adrenaline won't even open.,Negative
Intel,"Hi u/AMD_Vik  im still waiting more than month legion go 2023 amd vga driver get released update latest for arc raiders,bo7,but asus rog ally x and xbox rog ally x yesterday updated already.is there a chance,we receive an update for legion go?also amd chipset driver very old for legion go.thank you if you answer me 🙏",Neutral
Intel,"Am I the only one seeing this bug in the metrics overlay? there are two ""gpu temp"", one would be that of the cpu,but written wrong. while the other metrics are written right,I already tried a clean reinstall with ddu, but nothing",Negative
Intel,"Anyone else getting per game Settings not being able to be changed? It sticks to just one whenever you click on a slider, this update and the last had it. Apparently older versions didn't and the only other fix is through screwing with the BIOS which i'd rather not.",Negative
Intel,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable,Neutral
Intel,Still not working AMD NOISE S,Negative
Intel,New Game Support: ARC Raiders  I updated to this driver thinking it would be better for ARC Raiders since that is what I am playing right now but my game is crashing if I try to load into the Blue Gate map. I tried restarting my computer and it still happens. I almost lost all my gear because it took me a bit to roll back my drivers to 25.10.2. Come on AMD do better! It's a supported game on this driver! I surprisingly never had crashes in 25.10.2 unless I toggled the Adrenalin game overlay.  Running a 9060 XT and a 5900X.,Neutral
Intel,"I updated from 25.10.2 and saw the bug report tool pop up after restarting. I had no idea what caused it. I launched Battlefront 2 and the entire system froze, with WinAmp trying to play audio which sounded horrendous. I uninstalled Adrenalin, used DDU to clear everything then did a fresh install of 25.11.1.  On restart, the bug report tool showed again. This time Wallpaper Engine failed to show on the second monitor. I quit the program and started it again and boom - system freezes entirely.  So, this driver apparently has big issues with Wallpaper Engine. Uninstalled, DDU'd, fresh install of 25.10.2 and smooth sailing ever since.",Negative
Intel,I still have issues with the combo : 9070XT + PSVR2 + F1 25 in VR.   The image in VR is still bugged. The only version that is working is still 25.4.1,Negative
Intel,Software doesn't open at all for me. Used DDU but still doesnt work. Deleted CN folder from appdata aswell.,Negative
Intel,"After update I can't open the app. It just loads and crashes. I fully reinstalled windows and the error still persists. I can't update drivers or access the config, I can only download drivers externally. Any insight of what it might be?",Negative
Intel,"@AMD_Vik I still do not see CPU metrics after update via Adrenalin software, what to do?",Negative
Intel,"I'm new to this, i'm on 25.10.2 do i update? the only issue i have is fps drops on fortnite but other games i play are alright (r5 9600 igpu) it usually runs at 60 fps on performance mode but since last weeks of last season it started doing that",Neutral
Intel,"Unfortunately, version 25.11.1 does not start with Windows.",Negative
Intel,Is AMD going to come up with another driver soon?,Neutral
Intel,"My RX 7900 XTX now no longer run Frame Gen. The game becomes completely unusable even reporting 200+ fps, it still stutter like crazy.     Without Frame Gen all good, with Frame Gen, completely unusable for me :/",Negative
Intel,Over a few days after installing 25.11.1 on my 7900 gre system I had a few driver timeouts followed by a major screen-freezing crash which corrupted my drivers. DDU'd and rolled back to 25.10.2 and haven't had any new ptoblems.,Negative
Intel,"After installation 25.11.1 (from 25.10.2)  black screens entered the chat. After DDU and rollback to 25.10.2 they stayed, and after rollback 25.9.1 the same... RX 5700 XT. Sadly 😞.",Negative
Intel,"is there 25.11.1 for windows 10? the filename that i downloaded from AMD website is ""whql-amd-software-adrenalin-edition-25.11.1-win11-s"" where usually its filename includes windows 10 along the lines",Neutral
Intel,"they need to fix the BF6 texture corruption glitch, it's annoying af. had to roll back to 10.2",Negative
Intel,Any word on fixing the driver timeouts on the 7900xtx its a bloody joke worst gpu i have ever bought,Negative
Intel,Any of you also have issues with afmf2 and the game not opening adrenalin software or showing performance counter after enabling it?,Negative
Intel,"this shit was fucking with my PC, DDU current drivers and reinstalled 25.10 straight from Gigabyte Program and everything works again",Negative
Intel,getting bsod randomly since 25.9.1 sad..,Negative
Intel,"I started having an issue since the 25.11.1 update with unreal editor where all of my tools menus instantly close, nothing else changed except for this driver update and I've heard of Nvidia having similar issues with driver updates in the past so I think it may be the cause, Going to revert to an older driver and see if it works",Negative
Intel,"I've spent the last few days uninstalling, reinstalling, DDUing, doing everything I could think of to get Adrenaline to start/work. It would show the splash screen and then quit. No way of re-starting it. Couldn't open anything that used Vulkan and got errors. Couldn't install the Windows Store version cause ""driver error"". I eventually used DDU one last time and uninstalled everything AMD and was able to just install the driver through MyASUS. Now I'm able to open all the software again that wasn't starting before. I'll be holding off on installing Adrenaline again anytime soon. Sucks cause I want the features, but I couldn't use the programs anyway. I miss having nvidia.",Negative
Intel,"It seems on the latest Radeon driver that freesync is broken within CS2 when running fullscreen windowed. Freesync works initially when the game starts. But as soon as I alt tab, freesync breaks and I get screen tearing. I rolled back to 25.9.1 and I can confirm it works again as expected. So it seems this is a recent regression. Can we get this addressed please? u/AMD_Vik",Negative
Intel,"Been having issues with VLC freezing and stuttering during playback (video only, not audio) since anything after 25.9.1. Guess I'm gonna roll back to that until it gets figured out.... really frustrating.",Negative
Intel,Substance Designer won't start with this one. Access violation with amdvlk64.dll. Adrenaline won't start either,Neutral
Intel,"Sorry but for me the drive give me crash pop up message every time i boot up my pc. Also just right now i got a freeze, black screen to all my monitors.",Negative
Intel,The worst driver this year so far,Negative
Intel,"Still havent fixed the noise cancellation lmao, guess its another month+ of old version :) Thanks amd, truly doing wonders.",Neutral
Intel,CS2 crashing with driver timeout after tabbing out or watching streams on 2nd screen 7900xtx,Negative
Intel,"When is 25.12.1 coming out? I have read only bad things about 25.11.1 here, so I wanted to skip this one.",Negative
Intel,"Can we use FS4 on rx6000 series now without it crashing now on this driving now, or do i always need to keep downgrading my driver ?",Neutral
Intel,So no redstone yet,Neutral
Intel,FSR AI frame gen??? Didn’t they say that’d it would also have a driver toggle?,Neutral
Intel,Did AMD ever add support for Cronos?,Neutral
Intel,Well Star Citizen will load now!  Now some longer term testing....,Neutral
Intel,Anybody tried this with Anno 117 yet? I’m hoping it helps performance,Positive
Intel,Problemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemasProblemas y problemas,Neutral
Intel,NO LA DESCARGEN ES MAL LAGGGG EN LOS JUEGOS,Neutral
Intel,So are the issues with Arc Raiders fixed? I had to roll back to 25.9.1 because 25.10.1 kept crashing my game. Did they actually fix it?,Negative
Intel,Disabling ULPS seem to fix the crash. My pc crash pretty often when i wake the screen or turn it on if i dont disable ulps with msi afterburner. 9070xt,Negative
Intel,Doom: The Dark Ages does not lauch with the latest driver. I had to rollback to 25.9.2 in order to play. Please fix,Negative
Intel,Last driver crashes Apex Legends every joined game. Im fuckin over amd.,Negative
Intel,Support for Anno 117: Pax Romana - Incorrect. I have needed to downgrade back to 25.9.2 to play Anno 117: Pax Romana without crashing. The last updates have been a joke. I now cannot use adrenaline due to needing to play on this earlier version to be able to play any new games. Does anyone know of where I can express my complaints?  Edit: This is also the same for Arc Raiders.,Negative
Intel,"Here we go again, jetzt stürzt Battlefield 6 wieder ab. Mit dem Treiber davor hatte ich es in den Griff bekommen außer XMP war aktiviert, dann stürzte es dennoch ab.    Also es scheint definitiv ein AMD Treiber Problem zu sein.    Gut das bei euch die Kunden die Tester sind und nicht ihr das übernehmen müsst.  PS: Gespielt wird mit einer 7900XT und einem 7800X3D.      Das war definitiv meine erste und letzte Karte von AMD. So viel Probleme hatte ich mit Team Grün nicht.",Neutral
Intel,Yeah same here LG c5 42inch 😰,Neutral
Intel,"Having system crash issues after putting the PC into sleep mode. Samsung 57"" Odyssey Neo G9. Now I know who to blame.  Reinstalled AMD drivers and changed the settings so that my PC never gets into sleep mode (turns off the screen, but doesn’t sleep or hibernate). This fixed the issue temporarily for me :(  Also from the system crash minidump, it's very clearly an AMD driver issue  **IMAGE\_NAME:  amdkmdag.sys**",Negative
Intel,"I have this but on display port, HDMI works fine",Neutral
Intel,"DDU with full uninstall of all AMD related things and then chipset driver install, fresh GPU driver install fixed the crash from wakeup for me.",Neutral
Intel,"I had to go back to version 25.9.2 but I no longer have AMD Adrenalin. If I try to install it, it reinstalls version 25.11, which crashes my game. Is it necessary to have AMD Adrenalin? I have a 7900 XTX and a Ryzen 9 7950X3D.",Negative
Intel,Could you try a DisplayPort to HDMI adapter? I wonder if it works in this situation =D,Neutral
Intel,I have the same issue with display port but it’s okay with hdmi :/,Neutral
Intel,"Honestly, I plan to make sure my next display has Display Port in it. Mostly for linux though.",Positive
Intel,"There will already be branching inside the code of the driver. This has been the case already, to various degrees, for years. It's not new.  It's just whether AMD wants to formally spread those branches out on a file / compilation level and distribute different packages. And then publicly whether they commit to updating all branches of code or only some.  For any particular bug / feature / optimisation, there will be some cases where it's the same code path for practically all RDNA versions, you fix it once and it applies to everyone. For some, it might be very similar but not the same, just some slight tweaks and what gets fixed in RNDA3 can also be applied to RDNA2. For some, there's some hardware feature of RDNA3 that would make the fix easier there and it will be much more work to adapt the same to the RDNA2 branch. Of course, we have very little outside insight into the exact spread of these cases that AMD wants to pay their engineers to work on.",Neutral
Intel,There is a separate code path for specific things bit both are in those combine driver. Its mostly about certain ray tracing extensions   There are also fp8 and fp16 codepaths    People misunderstand and thought its like pre rdna stuff.,Neutral
Intel,"V25.10.2  here… I have both CPU and GPU by AMD and if you download the specific package, they install drivers only for the specific hardware (and they have different dimensions). For installing both drivers you have to download the AutoDetect package.  EDIT: typo",Neutral
Intel,"I'm one of the 5 people still running a Vega 64 and for years we've had a separate driver ""branch"" despite being able to install new Adrenaline versions. Bf6 beta wouldn't run without spoofing my actual internally installed driver, which hadn't been updated since they dropped support.",Negative
Intel,combined again it looks like 🤷‍♂️,Neutral
Intel,the display team are working on this with priority. Hoping to have this out in the next release. There are two similar display issues on their radar which are both P1.,Neutral
Intel,"what is triggering this? I can let my amd pcs run all day without any crashes. (7900XT,5700XT and 6900XT)",Negative
Intel,"Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  Not criticizing, just curious as to use-case.",Neutral
Intel,You try install last chipset driver ?,Neutral
Intel,"Bummer you're having issues. Hopefully AMD gets it straightened out for you. Out of curiosity, why did you decide to use HDMI on your monitor instead of display port? I thought HDMI was mostly used for TVs nowadays.",Negative
Intel,So it's the driver that's why that happens 😡 and it's not fixed?,Negative
Intel,Thank you for your service,Positive
Intel,"I've had the ""device_hung"" error and the best drivers are those you are on.   AMD needs a lot of help with their drivers...",Negative
Intel,Any update mate?,Neutral
Intel,"No, radeon drivers update cannot upgrade your CPU so that it would stop bottlenecking your GPU.",Negative
Intel,"Had the same issue, just ddu and reinstall the drivers manually, problem now is with my hardware monitor screen not working, I have an aorus board with an internal hdmi, that is not being detected, will try to reinstall chipset drivers",Negative
Intel,"multi gpu? got the same problem with rx 6400 + 7900xtx. plugging in my secondary display to 7900xtx fixed the issue, but what's the point of secondary gpu if its not working properly...",Negative
Intel,"Workaround:  Win+P and disable the second Monitor in iGPU, then start adrenaline and you can enable the second monitor again  software should be okay then",Neutral
Intel,"I'm the opposite, I just want adrenaline app to stop everytime I right click on desktop or open file explorer.... Bruh",Neutral
Intel,"yeah same, my experience with 25.10 was terrible, had to DDU it once to get back my CPU metrics in Adrenalin, then DDU’d it again to go back to 25.9.2 since games were stuttering.",Negative
Intel,Same.,Neutral
Intel,"Not stable, ddu install 11.1 and lag in old dx11 mmo game(BDO)  Same as 10.2, 9800x3d with 7900xtx, i can keep 144fps in town , 11.1like  fps 60-65 and lag spikes  Im go back to 9.1",Neutral
Intel,"Both 10 and 11 are shit with 7xxx series, myself and most others have gone back to 9.1 or 9.2. Might be alright with the 9070, might not",Negative
Intel,I'm playing it on 7900 GRE with FSR4 INT8 with no issues.,Positive
Intel,na it works the problem only occures when using PathTracing and you are not going to play on an AMD card with PT anyways and especially not a 7700XT,Negative
Intel,"I played recently the latest patch of Cyberpunk 2077 with RX 7700 XT with drivers from February this year, 25.2.1 and had no issues, no stutter, no lag, no crashes, the problem is with path tracing or ray tracing something, you don't have to use that even and it lowers fps probably for very little visual gain.",Positive
Intel,If it still crashes set RTX Global Illumination to Static.,Negative
Intel,"pretty sure anti lag was causing my system to freeze up when in arc raiders on 25.10. turned it on and issues, turned it off and haven't had issues since",Negative
Intel,"Optimizing has nothing to do with fixing crashing, generally, that would be bug fixing.",Negative
Intel,"The only Redstone component that will work through the driver for upgrading existing features (like FSR3 FG) will be ML FG, but only for games that have FSR 3.1.4+(and FSR FG), as they’ve already announced in a GPUOpen article quite a while ago.   Ray Regeneration (RR) and Neural Radiance Cache (NRC) require dev implementation since they’re much deeper in engine code/inputs.",Neutral
Intel,"Does BO7 have ray tracing? Where is that feature mentioned, would be keen to read about it 🤓",Neutral
Intel,"Itll probably work through optiscaler, but a game like indiana jones has locked dlss inputs, so you need dev implementation",Neutral
Intel,"And all my USB devices dropped for a while, far worse experience than the usual double screen flicker & it's done.  Edit: seems like that was because of iGPU driver for my Zen4. In the end and after 6+ reinstalls, I'm stopping on 25.9.1 and have Vulkan working again.",Negative
Intel,What is wrong with CP? Just got my 9070 after 10+ years of nvidia :o   Ah nvm i see it.,Negative
Intel,Ugh,Neutral
Intel,"Most likely when your distro provides a kernel update.  I'd give this a quick read: https://www.gamingonlinux.com/guides/view/how-to-install-update-and-see-what-graphics-driver-you-have-on-linux-and-steamos/  Pretty much the default is that you will be using the Mesa driver collection and they were last updated yesterday.  The fixes there do not correlate with the ones in this thread (I think).  As long as you are on a ""cutting edge"" type distro then you can expect that update in the next few days/weeks.  If you are on a ""stable"" distro that doesn't update often, you may be waiting a realllly long time.",Neutral
Intel,"Get what exactly? Features? Bug fixes? Optimizations?   Every 2 weeks there are new Mesa driver updates with bug fixes and performance optimizations. As for features, you have to wait a few months for versions with big numbers (25.2 is soon updating to 25.3, adding Anti-lag 2).   Depending on your distribution, you can get these updates faster or slower.",Neutral
Intel,"Linux doesn’t work like windows. You get updates directly via Mesa stack. When your distro provides mesa package update, that’s when you get driver updates, and they’re completely different from windows branch.",Neutral
Intel,just uninstall it I prefer manual check myself.,Neutral
Intel,So AMDs default driver overclocks and doesn’t reflect that in the values?,Neutral
Intel,Same issues here i underclocked it but this new update just made it worse,Negative
Intel,ok it is still crashing ... complete reboot :(,Negative
Intel,"I feel like that crash is more on DICE's side, since Nvidia users get the same exact crash, although less often.  I tried everything I saw on the internet, nothing really works. Sometimes I can play for hours on end, other time game just crashes randomly after 10-15 minutes.  I am going to try to downgrade to 25.9.1 and see how it fares, since I remember that driver being really stable for me (6800XT).  Edit: been playing for 4 hours, no crash yet. Never had such a long session without the game crashing.  Will update in the next few days.  Edit 2: haven't crashed once, been playing at least 2 hours every evening.",Negative
Intel,Okay.,Neutral
Intel,I’m hoping Valve’s new steam machine will push them on that since it’s RDNA3 based.,Neutral
Intel,The RT/PT reflections leading to CTDs issue? We're working with CDPR to resolve this - it's not caused by the driver software.,Negative
Intel,"Stuff like this is honestly bugging me.  First AMD card and while it is okay to have some driver issues, stuff like that shouldn't be a thing at all. It can't be that some bugs keep existing for multiple months let alone more than half a year.",Negative
Intel,A much more fundamental thing like playing video in a window without stuttering took 3 or 4 driver updates.,Neutral
Intel,"> One year already...Cyberpunk 2077 not fixed yet.  On year, I did not know.  I was thinking of a 9070XT earlier in spring, but choose... differently",Neutral
Intel,welcome to amd,Neutral
Intel,Yeah hoping it works. I guess the point about crashing with easy anticheat was the fix? Doesn't specifically mention but it was with raytracing enabled I was getting the crash. I suppose we shall find out.,Neutral
Intel,"Weird. I was running 10.2 and things were running fine for the most part. I kept getting a pop-up on launch saying there was a known issue with that driver and Arc Raiders though, so I rolled it back to 9.2 and had more issues on that release than the one with the stated problems.  My fan control profile broke and stopped tracking gpu temp so last night I did a clean DDU install back to 10.2.  Of course 12 hrs later the new release drops haha  Hopefully Fan Control continues to play nice if I just update",Negative
Intel,Same. Never even had Ryzen master installed.,Negative
Intel,"I'm receiving the same error in Event Viewer, but I have installed Ryzen Master. Most likely it's also a component of the Adrenalin drivers for system tuning and monitoring.  Registry search shows two keys for ""AMDRyzenMasterDriverV30"" (in both CurrentControlSet and ControlSet001): Computer\\HKEY\_LOCAL\_MACHINE\\SYSTEM\\CurrentControlSet\\Services\\AMDRyzenMasterDriverV30  The ImagePath points to: C:\\Windows\\System32\\AMDRyzenMasterDriver.sys and the file exists. It's valid.",Neutral
Intel,25.2.1 had no pink artifacts on my Pure 7700 xt Sapphire but i switched to 25.11.1 and got it now also and also some flickering.,Neutral
Intel,What is redstone?,Neutral
Intel,What's weird is Black Ops 7 has ray regeneration.,Negative
Intel,"I'm afraid it may be released in January like they drop new major this year like AFMF 2 upgrade, FSR, etc..",Neutral
Intel,vsync issue fixed with win 11 KB5068861 update.,Neutral
Intel,had no issues with vsync on 25.10.2,Neutral
Intel,works fine for me,Positive
Intel,I agree.  I also have this issue  Weird is if you turn on amd overlay it fixes itself,Negative
Intel,"That it did, lol. My only complaint.",Neutral
Intel,"Not a problem on Linux with Wayland. Ever.   Main reason I ditched Windows 11, constant screen tearing and input lag made me go crazy.",Negative
Intel,The new AFMF features were added in 25.10.2 - Still waiting for AFMF 3.0 at this stage,Neutral
Intel,Suggest if you are using powertoys to stop... After their last game update it was crashing left and right. Powertoys was cause. I only crashed once and I believe it was cfeen blanking was cause for that. I was away too long.,Negative
Intel,"Fps drop over time? That's a game issue, it's got a memory leak",Negative
Intel,"What kind of FPS drops? I was having dog shit frame pacing, and hardcore drops to like sub 60 FPS at 1440p medium/high with a 6950XT. I went through every setting and found that anti-lag was the culprit. Once I turned that off it all went away. Then with future frame rendering it got even smoother. Now I can hold 100+ FPS at all times, sometimes peaking over 200 FPS in some scenarios. And frame times are smooth as butter.",Negative
Intel,I’d settle for bf6 going one entire game without drivers crashing the game and freezing pc,Neutral
Intel,"I'm also having this ""problem"" with driver 25.10.2; I haven't tested it with 25.11.1 yet.",Negative
Intel,Crashes?,Neutral
Intel,I have this problem in all games.,Negative
Intel,"Hello, I've been having this issue and I have exactly your gpu and cpu, whenever I played valorant and I alt tabed many times the screen goes black and keyboard become unresponsive but I can still hear friends in discord and they can't hear me, after conctacting valorant support and messing with alot of settings I think  what fixed it for me is to add these in windows defender exclusions : C:\\Riot Games\\VALORANT\\live\\VALORANT.exe   C:\\Riot Games\\VALORANT\\live\\ShooterGame\\Binaries\\Win64\\VALORANT-Win64-Shipping   C:\\Program Files\\Riot Vanguard\\vgc.exe   C:\\Program Files\\Riot Vanguard\\vgm.exe   C:\\Riot Games\\Riot Client\\RiotClientServices.exe   I hope this helps",Negative
Intel,"DDU was only ever for switching GPU families, e.g. Nvidia or Catalyst to Adrenalin.",Neutral
Intel,"Sad to say I am having the same issue. Did you find any workarounds, or just downgrade?",Negative
Intel,"You don't need to make it work on Adrenalin, Battlefield 6 already has FSR 4 support integrated into the game, so you don't need to override, just turn off FSR 4 from Adrenalin.",Neutral
Intel,Works on 25.10.2. I have been seeing ppl said it doesn't work on this driver. It also didn't work on 25.10.1 for me as well.,Negative
Intel,Epic version runs just fine.,Positive
Intel,Cyberpunk GOG last version patch runs fine on this driver.,Positive
Intel,"Hey there, can you give an example of how this looks now versus how it's supposed to?",Neutral
Intel,are you able to install driver only with the newest drivers? When i choose minimal or driver only it still install full version of the software anyway. Only 25.9.1 works perfectly.,Neutral
Intel,Yeah... it can happen. I was already locked outside windows after a bsod with my old nvidia card. I had to use a linux usb drive to fix it through the command line.,Negative
Intel,It's most likely the same as I've described here:  [https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/](https://www.reddit.com/r/Amd/comments/1ow4in7/comment/nop2o04/)  But yeah unfortunately seems like all you can do for now is either roll back to 25.10.2 or disable your iGPU if you don't use it,Negative
Intel,probably because those issues are build specific.  I've not had any freezing or locks on any drivers this whole year.,Neutral
Intel,"The game is booting, this message was for the 25.10 they just didn't removed it",Negative
Intel,I don't know your system specifications but in many cases setting the BIOS to PCIe v4 could help.     Try change in your bios,Neutral
Intel,no I hit alt R opens right away.  And i did an upgrade install over top of 25.10.2 no DDU or AMD clean up utility.  When you say you did a clean install was that just from control panel and remove?,Neutral
Intel,"First time yes, i downloaded with -s letter, but the last time i downloaded smth like -combined(1.6 gb). All two's is for WIn 11.",Neutral
Intel,"To be clear, are you able to confirm that VRR is disabled after you alt-tab? Do you have a display-side OSD to verify?",Neutral
Intel,"Good call, it caused nothing but problems for me and pretty severe. Were talking driver timeouts with black screens and even a couple bluescreens.",Negative
Intel,"Not even 7000s have support for FSR, you'll have to stay on 25.9.1",Neutral
Intel,My 9070 xt crushes while I try to use fsr 4 on new drivers,Neutral
Intel,Why don't you try it and let us know if you can. Would be helpful for lots of us,Negative
Intel,It's in Redstone. Still not out yet,Neutral
Intel,Didn't work for me...,Neutral
Intel,Wait until you see how much your browser's cache is churning...,Neutral
Intel,Why cant you use Adrenalin? I'm using it on 25.9.1,Negative
Intel,I just received a windows extension update for my LG monitor. If you can boot up go check.,Neutral
Intel,The last time I had this problem it was a RAM issue.,Negative
Intel,I have this for my ThinkPad laptop with its internal display. Good to know I'm not the only one.,Positive
Intel,"if it happens, fill in the bug report that pops up. more reports will help AMD identify the issue better.",Neutral
Intel,I have DDUed the driver before upgrading. I still face the GPU crashes during display standby.,Negative
Intel,"Wait, chipset drivers uninstall too? I'm going to have to fix that on my system...",Negative
Intel,Do u reintall already up to date chipset drivers?,Neutral
Intel,You should use the Factory Reset installation in the AMD Driver Installer so it removes the newest AMD Adrenalin and install back the correct Adrenalin driver.,Neutral
Intel,"I don't have a DisplayPort to HDMI 2.1 adapter to try, sorry.",Negative
Intel,doing so (separation) will create a freak out shitstorm part 2.,Neutral
Intel,> People misunderstand and thought its like pre rdna stuff.  It is. They've simply bundled two separate drivers together. RDNA1/2 are still stuck on the same 21033.x branch while RDNA3/4 have moved first to 22021.1009 and now 22029.1019.,Neutral
Intel,Is it combined though?  7900 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov.exe (25.20.29.01 ?)  6800 XT Driver: ......amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-s.exe (25.10.33.03 ?),Neutral
Intel,All it takes is one random Reddit comment with some upvotes spreading misinformation and you will see people posting articles about it,Negative
Intel,"You have two differnt drivers installed and running at the same time, one for APU and the other for GPU? is that what you are saying?",Neutral
Intel,Combined only as far as the two driver packages are now in one file. RDNA1/2 are still stuck on 32.0.21033.x branch. RDNA3+ are on 32.0.22029.1019.,Neutral
Intel,Last driver was branched right? I never updated coz I had just fidled with the files to make FSR4 (INT8 or whatever its called run) and didn't wish to do it again...,Negative
Intel,Can we please have a fix for the 7700 xt artifacts showing? It's been ignored for so long. Using hardware acceleration on chromium (and not just that) causes pink artifacts everywhere.,Negative
Intel,Thank you for communicating,Positive
Intel,Unfortunately happens to me too. So for me it’s a big issue as I can’t update to this driver until it is fixed 😰,Negative
Intel,"I have a dual PC setup, one system with a 9070xt where I have seen this bsod related issue, mostly when toggling HDR.  The other system has exclusivly an RTX 3090 that also has the same HDMI 2.1 related system bsod, but Nvidia driver instead.   It idles, display sleep, 1 in 5 ish chance it locks up or bsods on wake.  Issue goes away using a non 4k 240hz display.     I believe this system crash is deeply related to DSC on Windows.  I only got these two PC bsods when I bought a 4k 240hz display.  Returned a monitor (bad oled) and the issue went away.  Got a new oled a few weeks ago and now I have these bsods again.     Never had a bsod before I got these 4k 240hz displays.  Fresh Windows 11 installs too between both PCs and between my first and second oled.  Systems are both solid and stable.     Linux works fine, but no HDMI 2.1 support thus no 240hz, which makes using the oled pointless imo.   While digging into my issue, I wanted to see if DP2.0 / uhbe20  compatible display would have these issues, but my current display only has HDMI2.1 and dp1.4.  Hopefully someone else had experience with them on 4k 240hz.",Negative
Intel,Thank you AMD my bad for getting upset,Positive
Intel,Thank you.,Positive
Intel,Any word on when noise suppression will be fixed? I would actually lowkey love a right up and why it’s failing. Would be cool to see the technical details if that’s possible. (I’m actually more interested now on why it’s not working vs just getting it fixed).,Neutral
Intel,Thank you!,Positive
Intel,There's a long-time standing issue with Chrome and hardware acceleration. Will that ever be made a priority?,Negative
Intel,Redstone when?,Neutral
Intel,"LG OLED CX and newer are good monitors.  I am using it with my PC and PS3, PS5, Blu-ray 4K Player...  I must say that that was my very first actual driver issue with AMD.",Positive
Intel,Not always true. HDMI 2.1 has more bandwidth than dp 1.4. I have a ASUS pg32ucdm and HDMI looks better and uses a lower dsc ratio. I have a Nvidia GPU in my main PC however.,Neutral
Intel,"Yes, my 9070XT is paired with a 5600X hooked up to my TV for couch gaming. I have a separate PC with a monitor for desktop gaming. My TV doesn't have DisplayPort, very few did when I bought it.",Neutral
Intel,"I currently run a gaming monitor as my primary one, with DP, and a plain old Lenovo office monitor as my second one. It only has HDMI, so i wouldnt be able to use it with DP.  I assume this may be the case for a lot of people. Getting a ""higher spec"" 2nd monitor really isnt a priority for me, as i just need a second screen for productivity when working instead of gaming.",Neutral
Intel,Non pc monitor tvs are sometimes cheaper especially for larger sizes. I’m on lg c5 oled 42inch and it only has hdmi…,Neutral
Intel,"Not universally true, hdmi and displayport depending on the version supported are very comparable.   Dp 1.4 Vs hdmi 2.1 is where hdmi has more bandwidth, before this dp was the easy choice.",Neutral
Intel,"> Are y'all playing on televisions? HDMI isn't really optimal for modern monitors, with DisplayPort being the better spec for computer graphics.  ~~Tell this to Valve, who are about to bring out a gaming-focused mini PC which **only** has HDMI 2~~",Neutral
Intel,"This was true prior to HDMI 2.1 but no longer the case. However, monitors with HDMI 2.1 do still cost more than those with DP1.4 which has similar albeit slower bandwidth.",Neutral
Intel,"Yes, I am on a television. ""HTPC gaming (and turbo tax) from the recliner master race""!",Neutral
Intel,> Are y'all playing on televisions?  Do you guys not have phones?,Neutral
Intel,LG OLED TV as a monitor. I have LG OLED CX which comes with HDMI 2.1.,Neutral
Intel,My game keeps crashing due to the same. I want this fixed ASAP. You'd think they'd test the biggest recent releases before publishing an update.,Negative
Intel,"I am also having a ton of crashes in BF6 RedSec, but I'm not getting any errors. Game just freezes, then crashes.",Negative
Intel,"I got a new pc 2 days ago and i literally cannot downgrade my drivers without amd forcing me onto the latest, even if i download the more stable drivers directly. Got any advice?",Negative
Intel,"I kinda fixed it by turn off XMP/EXPO running the ram the lowest bus, still crash to but 1 crash every 2-3 hrs still better than 15 minutes.",Neutral
Intel,Why does it seem like driver quality/support has gotten substantially worse this past decade? Are we running out of skilled software engineers or is hardware just getting too out of hand?,Negative
Intel,there are multiple reports of lower gpu usage with 25.9 and above.. after going back to 25.8 I have 15%+ fps  I guess the constant crashing with rdna1 cards is also not related to these drivers,Neutral
Intel,"Tried but didnt work, tried the 25.9.1 and get error that no Software is installed tried the auto update to 25.11.1 again and same error with open and shut down again",Negative
Intel,"Thanks a lot mate, i tried everything and adrenalin not worked but after yours advice it's fine. Thank you!",Positive
Intel,"open powershell as an admin and paste this in. (type powershell in the search bar and there will be a choice to run as admin)  Get-AppxPackage -AllUsers | Where-Object {$\_.Name -like ""\*AdvancedMicroDevicesInc-RSXCM\*""} | Remove-AppxPackage -AllUsers",Neutral
Intel,OK thought I was the only one. 25.10 is bad bad,Negative
Intel,"25.10.2 just straight up didn't work for me when trying to play Arc Raiders (constant UE crash when loading into a match), in fact it actually gave me a popup when launching the game to downgrade to 9.2.  Only driver since switching back to AMD to give me issues.",Negative
Intel,Thanks for testing it,Positive
Intel,"I posted before in an arc raiders thread, but I've had zero issues with my 7900gre on 25.10.2. Playing since launch. I don't use any sort of overlays and play in borderless windowed always. Performance is locked on 140+fps native res 1440",Positive
Intel,I thought FSR 4 was only on RDNA 4? 🤔,Neutral
Intel,My thoughts exactly. Thanks.,Positive
Intel,Ohh okay. Yeah I don't use any of that lol. Hopefully it won't give me any issues. Plan on playing it after FF7Rebirth.,Neutral
Intel,I tried that before and amd antilag off. I tried all the suggestions.   Been really happy with Bazzite so haven't been back in windows for 2 weeks,Positive
Intel,I tried that and all the other suggestions at the time. Nothing worked.  I'm running the game in Linux now and have 0 issues. I can't even crash it when I try,Negative
Intel,"There's not much documented about it  Videocardz has an article   AMD launches first feature from FSR Redstone with Call of Duty Black Ops 7, only for RX 9000 GPUs - VideoCardz.com https://share.google/VHJiZgwO6eqkMmHV3",Neutral
Intel,"Well currently none of the FSR 4 tech works in vulkan titles, through Optiscalar or driver override",Neutral
Intel,Pink path tracing and crashing after a few minuets of gameplay. If you don't enable path tracing the game seems to work just fine.,Neutral
Intel,What are you seeing? I played last night on latest driver and everything seemed fine to me. Playing Ultra RT with Auto on FSR4.,Neutral
Intel,"Yes, modern AMD cards use various telemetrics to push past the rated max boost frequency when headroom is available. Even on default. Problem is, not every board runs stable under these circumstances...  This issue is known to AMD but they don't seem to care.",Negative
Intel,"Have you tried undervolting your GPU?   I've played BF6 since launch with AMD drivers before the game ready one and currently still on 25.10.2 and I've had 0 crashes while playing the game.     Since I got this 7900 GRE that I've ran it with 2703MHz min / 2803MHz max and 1010mv, power limit -5%.",Positive
Intel,Would be absolute insanity to release a brand new rdna3 product if its not about to recieve fsr4,Negative
Intel,"But that's Linux based, So while we may get FSR4 in Linux, not necessarily in Windows.",Neutral
Intel,I guess you can't drop any hints as to whether this work with CDPR also involves adding Ray Regeneration to the game 👀?,Neutral
Intel,Fun fact - i am dual booting and on Linux this bug is not existent...:)),Positive
Intel,"On 10.2 the only crashes I was getting was if I pulled up the Radeon overlay while in game. My guess is that the anti-cheat doesn't like it, but as long as I avoided doing so it ran fine for me.",Negative
Intel,Hi. Did you ever resolve this? I'm getting the same error. Thanks.,Negative
Intel,"Yeah, I'm really starting to lose track of what's going on with AMD. I actually love AMD products, but I'm getting fed up. I've been waiting for a fix for over six months, and it never comes. Instead, new features are added, and bugs persist. AMD is making it really hard to continue relying on their products, which is sad. :/",Negative
Intel,https://www.amd.com/en/products/graphics/technologies/fidelityfx/super-resolution.html,Neutral
Intel,It's a thing you can search for on Google,Neutral
Intel,will it work with current version? Or is it based of some pre-release alpha driver for redstone?,Neutral
Intel,"They promised redstone as a feature for 25h2. So if they dont, that could technically be grounds for a lawsuit",Neutral
Intel,ahh i'm on Win 10 so probably why I didn't see it.,Neutral
Intel,Weird. Why is it a Windows fix?   I also got vsync bug in specific games and the weird thing is that I needed to turn on the and perf overlay in order to work,Negative
Intel,"Yes, but was it in the previous WHQL driver ? I'm not sure.",Neutral
Intel,there is an another bug appears in 25.10.2 drvier. 25.10.1 works fine.  [https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows\_25102\_driver\_performance\_for\_battlefield\_6/](https://www.reddit.com/r/AMDHelp/comments/1okvkvk/hows_25102_driver_performance_for_battlefield_6/),Neutral
Intel,It's not only BF6  https://www.reddit.com/r/radeon/comments/1oj98iy/amd_software_adrenalin_edition_25102_release_notes/nm4hh37/  https://www.reddit.com/r/lostarkgame/comments/1oq9ohp/insane_fps_drops_after_the_last_patch/,Neutral
Intel,"I tried everything I saw online: meshes on low, XMP lower/off, chipset drivers reinstall and other stuff. Nothing worked.  I downgraded back to 25.9.1., haven't had a crash in days.  Kinda miss the improvements for AFMF they brought with 25.10 for other games, but eh I'd rather play BF6 without it crashing randomly.",Negative
Intel,"Didn't test long enough to see if it crashes as I reverted back to old driver when I noticed my gpu was permanently at 100% load which it never was before on old driver, doesn't feel like it's supposed to do that with a 9800x3d & 9070xt + 32gb ram, It's moving between 70-95% usage on old driver",Negative
Intel,Either launch with curseforge or rollback,Neutral
Intel,"Damn, didn’t work for me last driver either. I can get FSR4 to work in other games just not BF6",Negative
Intel,Sorry for not replying in time with the pictures but I just saw that on Twitter that Beat Saber and AMD are now aware of the issue. The distorted flickering issue on the walls.  https://xcancel.com/BeatSaber/status/1993629046802882685  However there's another issue. I had not actually tried to use an Index at 90Hz until the other day. I discovered that the latency bug is back for 90Hz mode. As in I have to adjust the photon latency to ~5ms in the Steam debug commands to make it usable but not fixed. Just like in the the drivers before 24.12.1.   120Hz mode still works fine.,Neutral
Intel,"Even the rollback doesn't fix it for me now. It being related to the combination of iGPU and GPU makes sense though. Sadly I actively use the iGPU, so thats not an option for me.",Negative
Intel,You 100 procent sure on this?,Neutral
Intel,I went back to 25.3 official gigabyte latest driver for 9070XT OC gaming and 2 days no crashes for now. Also changed HDMI for DP cable  I will also change in BIOS and completely disable integrated GPU in BIOS,Positive
Intel,"I had auto update on (my mistake), so it did install over the previous driver. I've downgraded for now and it seems to be working just fine.  But no, after uninstalling from control panel I booted into safe mod and used DDU.   Then I downloaded the newest installer from the website. Restarting where needed during this process.   Didn't matter how I tried to access the software. Wasn't active in the system tray, not there in task manager.   Right click on desktop and open from there, click the exe, or go into a game and try Alt R. Didn't open with any of these methods.",Neutral
Intel,Driver with -s letter after black screen and reboot PC tells me that this driver isn't for my graphic card🤡,Negative
Intel,"I had randomly black screens with 24.2.1, this was annoying as hell. Had to DDU the Driver and went back to 23.11.1, after this everything was fine.",Negative
Intel,Does fsr 4 work on 25.9.1? I thought the only driver that works without having to change any files and risk a ban online is 23. 9.1?,Neutral
Intel,"I had to rollback to driver 25.9.1, start up Star Citizen, switch the renderer to Vulkan, exit game, start SC again to verify it's working with Vulkan, then reinstall the latest driver again. Now it works, just don't switch it back to DirectX. No idea who dropped the ball on this one but Easy-Anticheat sure doesn't like the atidxx64.dll.",Neutral
Intel,I'm not sure. However when I install the driver it states that I cannot use adrenaline and gives me a link to the newest driver. Wondering if there is an earlier version of adrenaline I need to install.,Neutral
Intel,What do you mean extension update??? Do you mean lg firmware update or something Ina  windows update? Where do I find this?,Neutral
Intel,They do not.,Neutral
Intel,"Well, I used the system before the latest updates normally, for maybe half a year. Installed every update without DDU. I wanted to triple boot, formated the drive, left windows to install stock updates and drivers, went to install chipset and GPU driver and voila, every wakeup, just before I could type the pin, kernel panic and shutdown. DDU, just the driver, installed again, black screen. Went into DDU again, wanted to see if I missed anything and without reading, uninstalled everything AMD related, went into windows, installed chipset drivers from vendors site (ASUS) and then the latest GPU drivers, and havent had issues. I have a monitor connected via DP and a TV via HDMI.",Neutral
Intel,Dont buy an adapter. Most of them are trash and dont put out the advertised resolution and refresh rate. Not to mention most of them dont support vrr or HDR.,Negative
Intel,The one linked in the release notes is combined. (in the sense that it's two drivers in one executable) (And 1.6GB)  .....amd.com/drivers/whql-amd-software-adrenalin-edition-25.11.1-win11-nov-combined.exe,Neutral
Intel,"AND is taking away one additional driver feature per day, you say?",Neutral
Intel,"Yes, I’m talking about 25.10.2 (posted the other day here: https://www.reddit.com/r/AMDHelp/s/nNunS1QlFX ). If you go to AMD download page and select “GPU” you get a file that has a different dimension from the one you download if you choose “CPU”. If you do the following steps 1) DDU (or AMD CleanUp Utility), 2) install CPU drivers => GPU not recognized or with a generic driver. If the you install the GPU driver you get the GPU correctly recognized and a yellow esclamation mark on the integrated. To have both VGAs working at the same time I have to download the AMD Auto Detect package (the file name ends with “minimal_install), but Adrenalin App does not open.",Neutral
Intel,Thank you for explaining it before the rage baiters go nuts.,Positive
Intel,I've been following this one pretty closely. Unfortunately the fix *just* missed 25.11.1 release cycle; we hope to get this out in the next one.,Negative
Intel,"I have a 7900XTX and have this issue at times. It normally clears quickly on its own, but seeing pink pixels on the screen worried me that I had an issue with the actual hardware",Negative
Intel,"I totally understand. if you're running with an affected display config, you should be fine on 25.9.1 or the 25.10.1 preview driver for BF6. We'll get this out as soon as we can.",Neutral
Intel,"Appreciate you reaching out with this - that's an interesting data point to consider.  In this case, the quirky behaviour with HDMI 2.1 + FRL was attributed to a change made in the display stack, I don't think it explicitly involves display stream compression to repro, just high enough display bandwidth (though I could be mistaken)",Positive
Intel,"Interesting. I've been experiencing random BSODs on my PC too on a fresh Win11 install too, I was also suspecting it was something to do with auto HDR toggle but I didn't know for sure.  I'm running 180hz 1440 LG panel but I don't recall if it's using hdmi or DP. I've also never experienced BSODs until the last week or two, since i did a fresh install and I've had the same monitors/PCs for a long time.",Neutral
Intel,Don't apologise - this one is worth getting upset over. Hope it's fixed for you all very soon.,Negative
Intel,Was yours the DisplayPort config or HDMI? I may have a fix for this ready if you're available test,Neutral
Intel,Can you provide me some context? Are there any posts detailing the issue either here or at the chromium project side?,Neutral
Intel,Already launched in COD 7,Neutral
Intel,"Im currently using dp 1.4 on my 7900 gre.  My monitor has 2.1 hdmi, so you’re saying i shoulf switch to hdmi?",Neutral
Intel,Some of us are using DP 2.1 though. DP 1.4 is a thing of the past.,Neutral
Intel,Are you sure? The steam machine has HDMI 2.0 and DP 1.4 listed in the specifications,Neutral
Intel,"Ah ok, that definitely makes sense if you're using a TV. Hope they straighten it out for you soon.",Neutral
Intel,What I did is get the 25.8.1 drivers shut off the fluid motion frames and in Windows set bf6 to run full power on your graphics card. Try some of those things and see if that helps you.,Neutral
Intel,"I tried to reinstall the chipset drivers, the same error, tried to install the drivers from gigabyte site the second screen worked but uninstalled adrenaline and it ran the auto update that disabled the secondary screen and the issue of no adrenaline opening came back, going back to older version, thanks AMD",Neutral
Intel,With the compiled leaked DLL you can use it on RDNA3 as well.,Neutral
Intel,"Wait until December, there's the 5th anniversary and they'll supposedly drop something new.",Neutral
Intel,For me playing it with path tracing on eventually the game crashes. Had no issues playing with ray tracing: psycho though.,Positive
Intel,Amen to that. Not a single crash since the game released on CachyOS. (9070 XT),Negative
Intel,Thank you! Exciting keen to see what it’s like,Positive
Intel,"It has nothing to do with the lack of vulkan support, even when fsr4 comes to vulkan, the game still needs ray regeneration implementation from the devs, you cant use optiscaler through dlss inputs, like you probably will be able to in fx alan wake 2",Negative
Intel,I suppose is going to be fixed as soon as Amd Redstone go out. Basically is going to add every tecnology that Path tracing needs to work. Off course the devs would have to implement them in the game too,Negative
Intel,"The issue is if you try to use path tracing. Which to be fair, you probably shouldn't unless the miracle of them getting Virtuous to implement Ray Regeneration in Cyberpunk happens.",Negative
Intel,yes i tried it ... maybe it is because of the 7800X3D ... i really dont know... i will wait for the next patch ... should come tomorrow 18.11. ... but i have read so many threads and i am not the only one...,Neutral
Intel,Hmm fair. For me on 9070XT if I had raytracing enabled it would crash most regularly on loading back to the menu after extraction which was annoying but at least not game destroying lol. But yeah annoying enough that I turned RT off entirely.   I hope this means I can turn it back on because the game runs well enough for it.,Negative
Intel,"Yes. So far, so good. I'm not 100% sure what fixed it.      I uninstalled both Adrenalin and Ryzen Master standalone applications. Deleted the ""amdryzenmasterv"" keys. Rebooted.  Then I installed Adrenalin and used the Ryzen Master installer in Adrenalin (Performance > Metrics > Install Ryzen Master).  I think this problem might have something to do with a handshake breaking between Ryzen Master and Adrenalin, after upgrading just Adrenalin.   From now on, I'll probably do clean installs, removing and reinstalling both Adrenalin and Ryzen Master, through Adrenalin Performance tab.",Positive
Intel,"My last two processors are ryzen amd, my last three video cards are amd radeon and next one will be amd also, nvidia has driver issues as well, to be honest.  But yes, some issues are too annoying and updating drivers is lottery at this point. I would advice against updating drivers unless you need it for specific new game or have the latest video cards - rx 9070 \[xt\]  or 9060 xt .  I should have sold the 7700 xt and bought 9060 xt 16gb instead but too late for that now and it has problems as well, so lottery as i said, its a risk.",Negative
Intel,"I see, btw I'm just telling the natural AMD trends on its major update/features on its GPU (kinda). Wish it coming sooner..",Neutral
Intel,lmao chill out dude go touch some grass,Neutral
Intel,Could be grounds for lawsuit… That’s funny!,Neutral
Intel,Because of MPO.,Neutral
Intel,yeah same with 25.11.1 25.9.2 works for me,Positive
Intel,"25.10.2 was the previous WHQL, so also yes :P",Neutral
Intel,"Huh, seems to be running in DX12 for me, as reported by MSI AB, and FSR4 is working",Neutral
Intel,Which driver version and does it still crashing?,Neutral
Intel,OK I will install it now and test it and get back to you. Give me 10 mins.,Neutral
Intel,Yuppp. It does not work on bf6 but it works on ark survival ascended for me. Seems it might be a bf issue. I saw ppl saying they were getting more fps with this driver in BF6 but I think they didn't realize it's using FSR 3.1 and not 4.,Negative
Intel,I'll work with the engineer from that ticket check if that issue has somehow regressed.,Neutral
Intel,We've not been able to reproduce this internally so far. Can you remind me which GPU (was this a 7900XTX?) + connectivity method you're using?,Negative
Intel,Try using DDU to uninstall your current drivers and do a clean install of the 25.10.2 combined drivers if you havent already (you can find it here):  https://www.amd.com/en/resources/support-articles/release-notes/RN-RAD-WIN-25-10-2.html,Neutral
Intel,"Yup just need to say ""No""",Neutral
Intel,"whew thanks, good think i noticed it first before updating. i have 25.10.2 and 25.9.2 here and they both have windows 10 along their filename so i might as well asked.",Positive
Intel,I don't see how it would work on 23.9.1 lol,Neutral
Intel,"Did you use DDU to uninstall the drivers and then reinstall using a driver installer from the AMD website? I found this guide yesterday and it was extremely helpful, specifically step 8:  https://www.reddit.com/r/AMDHelp/comments/1lnxb8o/ultimate_amd_performance_fix_guide_stop_lag_fps/",Neutral
Intel,I did it this morning before the new driver and confirm chipset drivers were untouched,Neutral
Intel,Agreed the variability and stability of adapters can be quite hit and miss. Just wait it out unfortunately and don’t use the latest drivers. At least AMD owned up to it so I can’t be too upset but hopefully they really do fix this soon as new users may not understand what’s happening and think their card is bad.   We are already seeing this in other Reddit post from other users. I have been playing some damage control and telling them to downgrade and suddenly it’s stable for them and they don’t think their card is dead doh. So yeah AMD really needs to fix this soon.  More people use HDMI than what people think and those TVs don’t always have a DP connector at all.,Negative
Intel,"ah, that explains it. Thanks. :)",Positive
Intel,"What if one does not check release notes? V25.10.2 point to different packages, for installing both CPU and GPU you had to download the AutoDetect package (named “minimal install”). Obviously I’m referring to AMD driver download page.",Neutral
Intel,"Okay, as long as they have a workaround for that... I was planning to get a 9060XT, but also keep my 6700XT for lossless scaling frame gen, was worried the branching would affect that..",Neutral
Intel,"What a mess. I have rdna2 and 3 on the same PC, so combined driver is the solution for this? Or auto detect is the only way?",Negative
Intel,"Damn, that's a huge relief to hear. Most of the frustration came from not knowing whether the issue was even acknowledged.   Thanks a lot for the update.",Positive
Intel,What about Noise Suppression not working since 25.9.2?,Neutral
Intel,Finally. The last driver without pink artifacting was 25.4.1  It has been 7 months. AMD Matt acknowledged the bug on [the amd forums](https://pcforum.amd.com/s/question/0D5KZ000011WpJJ0A0/pink-square-artifacting-chrome-discord-and-steam) 5 months ago.  Please let it be fixed with the next driver update. So many 7000series users are on the verge of swapping to Nvidia.  https://i.redd.it/ywrg9at3lp1g1.gif,Neutral
Intel,"what about whatever changed post 25.3.1 for multi monitors? Any driver 25.3.1 and older i can run my main at 200hz, my side at 165, and my third at 60 without issue. Any driver newer than 25.3.1 and running my main at 200hz then playing a game will cause the second monitor (165hz) to flicker, glitch, and freak out the entire time, until the game on the main monitor is closed. Dropping to 165 on the main monitor down from 200hz \*fixes\* the issue but, i paid for 200hz and would like to be able to actually use it again.  (7700xt system)",Neutral
Intel,Hell yeah 🙂 amd I appreciate you see the issues Iv been a fan boy for a long time and I know it will be fixed,Positive
Intel,"Hi. Sorry, I thought this was pretty common as I've had this issue with two different systems and 3 AMD GPUs across 2 generations. [Here.](https://www.reddit.com/r/radeon/comments/1jayump/chrome_full_screen_freezes_after_switching_to_amd/) [Or here.](https://learn.microsoft.com/en-us/answers/questions/3890720/parts-of-screen-freezes-when-using-chrome) Chrome (or other Chromium browsers, I've had this happen with Edge too) in fullscreen, with graphics acceleration enabled.   Best way I can describe it is that very often the program fails to update its display information?? Most of the screen freezes to a certain frame and only small, sporadic parts of it - never the same ones, sometimes more, sometimes less - continue to be updated properly as I scroll or take other actions. Making the window smaller fixes this instantly. It seems to be tied to graphics acceleration as it just doesn't happen at all with it turned off. The issue seems to get exacerbated when the GPU is being stressed.   And in the same vein, Chromium browsers do not seem to mesh well with AMD hardware? I've had plenty of weird behaviors ranging from TDRs, WoW freezing in the background, crashing and restoring itself which do happen less often if I'm running Firefox in the background instead of Chrome.",Neutral
Intel,"Keep in mind this only actually matters if one protocol or the other is **actually** limiting bandwidth for the [resolution * FPS] you want to display on your monitor  For instance, DP 1.4 can do ~25Gbits/s, which translates to roughly 1440p @ 240fps, or 4k @ 120fps  If you have proper cable into HDMI 2.1, it should be able to do ~42Gbits/s, which would instead be 4k @ 190fps  Read up the wikipedia tables for bandwidth specs and resolution / fps / bit depth rates for more",Neutral
Intel,Yeah most people are still have dp 1.4 monitors though. No way I'm swapping my monitor just for dp 2.1 as it's just too awesome. Although these tandem OLED screens look interesting... Hmm lol,Positive
Intel,"lol, listened too much to linus  https://youtu.be/g3FkuZNSGkw?t=261",Neutral
Intel,That was my very first actual driver issue I experienced with AMD.,Negative
Intel,Oh that's nice! I'll look into it when I get the chance.,Positive
Intel,Cool. Thank you,Positive
Intel,I might turn on ray tracing in like a medium or low setting just for shits and giggles or screenshots but I don't plan on playing with it on all the time since I'll also play at 1440p so I don't wanna have a really bad experience 😖,Negative
Intel,Exactly! It's insane. I can't believe running through a translation layer is more stable than running native,Positive
Intel,"but i dont know something is also wrong with 25.11.1 ... AMD Adrenalin randomly vanishes from tray     i dont want to roll back to 25.9.1 :-(    but this version was the last which was good so far      maybe when we have luck amd is able to provide next time a ""perfect"" driver without any issues?",Negative
Intel,"Could be it, nothing hardware wise at all and just software too.     Yeah, you're not the 1st I've read that says they have issues on the game.     If you're crashing and the system reboots it does make me think of some stability issues and not really just GPU, but I'm guessing you've already tried stability tests and using the system without OC+A-XMP/EXPO disabled.     Once upon a time I remember reading some people, on other instances and other games, having crashes like that (complete reboot) due to PSU instability as well.     Good luck!",Neutral
Intel,That's strange. My game defaulted to everything maxed out including running ray tracing and I haven't noticed any issues. The game runs incredibly well.  I've got a 5800X3D & 9070XT as well.,Positive
Intel,"Fair enough, and yeah sooner the better for all of us",Positive
Intel,It absolutely could be.  Europe has some pretty good consumer law and promising features that could impact your purchase decision are part of this law.  It can fall under misleading advertisement,Neutral
Intel,"It's not freezing, it's just that the GPU usage is practically at 100% all the time, even with V-Sync enabled. With the 25.9 drivers, this doesn't happen, and the clock speed varies normally, increasing and decreasing as needed when V-Sync is enabled. These 25.10 drivers are causing the GPU to consume more energy unnecessarily.",Negative
Intel,Fingers crossed,Neutral
Intel,"Thanks for attempting to retest.  It's a 7900XTX with an Index connected via DisplayPort. I am on the latest 25.11.1 driver.  I run a monitor at 4k 120Hz 10bpc with HDR Off, which uses DSC, as my main and only display. I tried disabling DSC in the monitor settings which runs at 4k 120Hz 8bpc with HDR Off but I don't think I noticed a change in latency. I thought that DSC on and off on two different devices might contribute to the problem but I'm not sure.   I have also tried running the Index under a RX480 on another PC and I fairly certain the latency looks different under 90Hz and looks similar under 120Hz. Can't play much to test though as an RX480 runs the Index at a very blurry setting. Getting around to doing this test is what took me so long to reply.",Neutral
Intel,Were you able to find the issue?,Neutral
Intel,"Allright ty, will Install new, any differences in performance?",Neutral
Intel,"im running 25.11.1 on win10 7900xt. no problems besides afmf2 breaking the performance overlay, which ive had for multiple updates now",Negative
Intel,I don't think you understand what I mean.. If you downgrade to that you can get fsr 4 to work on 6000 series without having to change any files.,Neutral
Intel,Thank you for this. This was very helpful. Got adrenaline working fine now.,Positive
Intel,"I wish my LG C4 42"" had a display port. Its my primary monitor.",Neutral
Intel,"Yep, AMD is not so quick in fixing their driver download page, I still can remember when X870E chipset came out but you found only X670 option. The drivers were the same, but man it’s your flagship chipset! Anyway, drivers install but Adrenalin App does not work for me… and have zero time to reinstall Windows.",Negative
Intel,"Don't do that, i'm suffering with both 7900XTX + RVII (and even with RX6400)",Negative
Intel,"I feel like AMD can do a bit of a better job communicating this, despite Vik's great efforts in this subreddit. Just saying: hey guys, we know about this, it sucks, bear with us please can make things better in the long run in my opinion.",Negative
Intel,"We've not observed anything like this internally, and I've personally not seen this at all in the field. This one may be specific to your display combination. I'll talk to a colleague in our display team about what we can do to learn more about that behaviour on your side - we might ask you to capture a special kind of log file during a state when your secondary display is blanking with gameplay on the primary",Neutral
Intel,Can you tell us what content you have on the secondary panel when it starts misbehaving? Is this behaviour affected if you disable VRR on it too?,Neutral
Intel,"I see. I recall the partial display freeze being related to DWM & multi plane overlay in Windows. One of our community testers encountered this and remedied with the overlayminFPS DWORD here: https://www.reddit.com/r/Windows11/comments/1kgp7ar/cause_and_solution_to_windows_24h2_related/?sort=new  If you're getting TDRs, with chome(ium) + gameplay, pass us over a kernel memory dmp and we'll take a look into it",Neutral
Intel,"Oh yeah I noticed that too when watching it, kinda funny of him to say it as it's on screen",Positive
Intel,I've had a really stable 60 fps experience even with path tracing (minus the crashes) but I'm on a 9070 with fsr 4 on quality,Positive
Intel,Can't comment on the vanishing of the tray since I have mine configured to hide it from tray always.,Neutral
Intel,also i have coil whine since this driver 25.11.1. ?!  also in idle sometimes...  very strange driver...,Negative
Intel,Every roadmap since forever has a small print disclaimer at the bottom at the page saying that release times are subject to change.,Neutral
Intel,"I upgraded from 25.9 because it started crashing, it was not crashing until yesterday, also I have limited gpu usage, clockspeed it is working sometimes, sometimes not.",Neutral
Intel,We've still not been able to reproduce this unfortunately. I'll need to check in when I'm back at work next year,Negative
Intel,Didn't really paid attention to it :( but at least my game doesn't crash anymore!!,Negative
Intel,"did you download the same filename with the one i mentioned? i tried downloading windows 11 link and it also gave me the same filename, lol",Neutral
Intel,No you can't.,Neutral
Intel,"I don' think you understand either, 25.9.1 is that driver, 23.9.1 was released in 2023 before FSR 4 was even a thing.",Negative
Intel,"Glad I could help, the crashes on Arc Raiders were pissing me off big style so I wanted to share what helped me.",Positive
Intel,"They are TV's, not pc monitors. Buy the right tool for the job",Neutral
Intel,"I couldn't find the DWM key so I made one, we'll see if that fixes it. Thank you for replying. But please, forward it to someone. There must be people out there on AMD hardware that are not tech savvy and this just breeds bad reputation.",Neutral
Intel,"Nope, that didn't fix it. Chrome continues to forget to update its display. And WoW continues to crash in the background leading to massive stutters when panning the camera around quickly, along with plenty of other odd behaviours.",Negative
Intel,"Yeah, I'm facing the same issue on RX 9060 XT   Is it a GPU driver issue, or a Windows issue that Microsoft needs to fix?",Negative
Intel,since last BF6 Update i had zero crashes also on 25.11.1,Negative
Intel,"That's not how it works. Small print won't save you from stuff like this, you think that they can stop releasing this software for 2-3 years because some small print said 'Subject to change'?   European consumer law would absolutely support the consumer in this case. If you promise a feature by a certain date, you will need to deliver it. If the development takes way too long, you are in for a ride",Negative
Intel,What about 25.11.1?,Neutral
Intel,Yeah same for me. Considering how similair win10 and 11 are under the hood i just went with it. Still absolutely no problems sofar.,Positive
Intel,Can't what? I have been using fsr 4 on my 6700xt for a month now using this tutorial   https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1,Neutral
Intel,"> 23.9.1   Come on man, it's clearly a typo and you know it. That's why he's confused.",Negative
Intel,"Look online for fsr 4 on 6000 and 5000 series, you will understand,    Edit :I will just link you a tutorial to understand...      https://www.reddit.com/r/pcmasterrace/comments/1nmyhpo/fsr_4_on_rdna_2_guide/?share_id=EEC5RH2XmDUZa2DjeRO-5&utm_content=2&utm_medium=android_app&utm_name=androidcss&utm_source=share&utm_term=1",Neutral
Intel,Did you reboot after setting that key? Is the display with chrome still only partially updating?,Negative
Intel,"Haha. Sure thing buddy. If that would be the case, that a small print on a roadmap can't save a company, we would have hundreds of lawsuits in EU. You must be new to the hardware scene.",Negative
Intel,thank you,Positive
Intel,"Uh huh, yeah I did it too. You still have the overwrite DLLs. Hence the flipping tutorial. Completely irrelevant to this topic, anyways.",Negative
Intel,"Not a typo, I was asking about something else and he missed my point...",Negative
Intel,"Yes, it can't.   They could add that they can sell your organs, doesn't make it legal though.   Also lawsuits usually don't happen because of a feature or if they happen, in a very small amount that isn't meaningful. Class action also doesn't exist in Europe, so obviously you won't hear about it like in the USA.",Neutral
Intel,"But flipping the dlls could get you banned from online games , that's why i was wondering if it's finally fixed in this update and we wouldn't have to flip the dlls anymore..",Negative
Intel,"You literally claimed we could do this without overwriting any files, that was obviously false.  Anyway, no. I would not expect AMD to bring FSR4 to RDNA3 anytime soon, if at all.",Negative
Intel,"What a disgusting build, I love it",Neutral
Intel,the content we crave,Neutral
Intel,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",Neutral
Intel,What GPU are you using in your build?  All of them,Neutral
Intel,you're one hell of a doctor. mad setup!,Negative
Intel,The amount of blaspheming on display is worthy of praise.,Neutral
Intel,Brother collecting them like infinity stones lmao,Neutral
Intel,I'm sure those GPUs fight each others at night,Neutral
Intel,Bro unlocked the forbidden RGB gpus combo,Neutral
Intel,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,Neutral
Intel,What the fuck,Negative
Intel,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,Positive
Intel,Yuck,Neutral
Intel,Wait until you discover lossless scaling,Neutral
Intel,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,Neutral
Intel,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",Negative
Intel,Now you just need to buy one of those ARM workstations to get the quad setup,Neutral
Intel,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,Positive
Intel,Love it lol. How do the fucking drivers work? Haha,Positive
Intel,What an amazing build,Positive
Intel,wtf is that build man xdd bro collected all the infinity stones of gpu world.,Negative
Intel,You’re a psychopath. I love it,Positive
Intel,This gpu looks clean asf😭,Negative
Intel,The only setup where RGB gives more performance. :D,Neutral
Intel,Now you need a dual cpu mobo.,Neutral
Intel,Placona! I've been happy with a 6700xt for years.,Positive
Intel,absolute cinema,Neutral
Intel,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",Neutral
Intel,"Brawndo has electrolytes, that's what plants crave!",Neutral
Intel,"You have always been able to do something like this! Though cross manufacturer has not had ""benefits"" until Vulkan in some spaces for games, now lossless scaling, but for anything requiring VRAM or software level rendering, it's been there as an option.   Unironically, ""SLI and crossfire"" I'd argue, is back, but not in the traditional sense.  I did this with a 6950XT and a 2080ti, simply because I wasn't able to jump on the 3090/4090 train in time tho.",Positive
Intel,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",Neutral
Intel,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",Neutral
Intel,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",Neutral
Intel,Team RGB,Neutral
Intel,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",Neutral
Intel,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here: https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",Negative
Intel,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",Neutral
Intel,"OpenCL works on all of them at once, and is just as fast as CUDA!",Positive
Intel,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",Neutral
Intel,Start here with FluidX3D: https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md 🖖,Neutral
Intel,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",Neutral
Intel,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",Neutral
Intel,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (I’m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),Negative
Intel,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,Neutral
Intel,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,Neutral
Intel,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",Negative
Intel,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",Negative
Intel,Thank you so much for the very detailed response!,Positive
Intel,Well worth it!,Positive
Intel,Thank you my man!! Looking forward to run some tests once I get home.,Positive
Intel,That's awesome!,Positive
Intel,"Yes, but SLI is a bad description for it.",Negative
Intel,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",Neutral
Intel,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",Negative
Intel,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",Negative
Intel,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",Neutral
Intel,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",Neutral
Intel,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",Neutral
Intel,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",Neutral
Intel,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",Negative
Intel,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  ​​​  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",Positive
Intel,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",Neutral
Intel,Why are you connecting the monitor to the gpu and not the mobo?,Neutral
Intel,"👍   thanks for the info, this'll definitely come in handy eventually.",Positive
Intel,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,Neutral
Intel,No worries mate. Good luck,Positive
Intel,"For some reason I switched up, connecting to the gpu is the way to go. I derped",Neutral
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,It's alive. Rejoice.,Neutral
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",Neutral
Intel,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",Negative
Intel,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,Neutral
Intel,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,Neutral
Intel,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,Positive
Intel,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,Positive
Intel,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",Neutral
Intel,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",Negative
Intel,I'm fairly sure they use dxvk for d3d9 to 11.,Neutral
Intel,Could just be a cache issue,Neutral
Intel,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,Neutral
Intel,Glad you brought up Nvidia as I didn’t know this had improved until the testing around Arc showed it had gone.,Positive
Intel,"According to the graphs, AMD has slightly less overhead than NVIDIA.",Neutral
Intel,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",Negative
Intel,"Lowest with DX11 and older, but not with the newer APIs",Neutral
Intel,And when is the last time HUB did a dedicated video showing the improvement in overhead?,Neutral
Intel,or it's just a cache/memory access issue,Neutral
Intel,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",Neutral
Intel,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",Negative
Intel,"Intel uses software translation for DX11 and lower, so it does matter for them.",Neutral
Intel,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",Neutral
Intel,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",Negative
Intel,That's not true. Intel's issue is being too verbose in commands/calls.,Negative
Intel,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",Negative
Intel,HUB used DX12 games that also showed the issue.  It's something else.,Neutral
Intel,"The comment to which I am replying is talking about nVidia, not Intel.",Neutral
Intel,"I’m pretty sure HUB doesn’t like Nvidia *or* AMD. They’re calling it how it is, these parts are too damn expensive.",Negative
Intel,That's actually... just worse news.,Negative
Intel,I always dreamt of the day APUs become power houses.,Neutral
Intel,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",Neutral
Intel,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",Negative
Intel,Damn Why is AMD even involved in iGPU,Negative
Intel,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",Positive
Intel,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",Neutral
Intel,almost there,Neutral
Intel,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",Positive
Intel,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",Positive
Intel,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,Negative
Intel,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",Neutral
Intel,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,Negative
Intel,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",Negative
Intel,"It depends on what your goals are for a laptop.  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!   I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",Negative
Intel,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",Neutral
Intel,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",Neutral
Intel,yes its so bad. better go buy some steam deck or ally x,Negative
Intel,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,Neutral
Intel,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",Neutral
Intel,How are they going to feed all those CUs? Quad-channel LPDDR5X?,Neutral
Intel,That's considerably faster than an XSX.,Positive
Intel,>That's tapping on 4070/7800 levels of performance.  What is?,Neutral
Intel,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",Neutral
Intel,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",Positive
Intel,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,Neutral
Intel,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,Neutral
Intel,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",Neutral
Intel,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",Negative
Intel,It's called satire. You're just salty because you're the butt of the joke.,Negative
Intel,throw it in the next steamdeck and I’ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,Neutral
Intel,Praying the blade16 gets it.,Neutral
Intel,"This is the rumor, if you’re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",Neutral
Intel,256 bit bus + infinity cache.,Neutral
Intel,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,Neutral
Intel,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",Positive
Intel,"Yes, it’s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",Neutral
Intel,The rumored 40CU strix halo chip. Not the actual chips released this week.,Neutral
Intel,7500mhz ram and the 780m,Neutral
Intel,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",Neutral
Intel,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",Neutral
Intel,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",Neutral
Intel,Literally where did you see 40-60% uplift at half the power?,Neutral
Intel,> 40-60% performance uplift at half the power  Source?,Neutral
Intel,"i chuckled, then again im not a fanboy of anything",Neutral
Intel,Dont expect 40CUs in a handheld anytime soon,Neutral
Intel,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",Negative
Intel,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.  The system should have an actual TRUE quadcore memory setup.  Many of these systems have currently (and will absolutely continue to have) dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.  This matches my system that runs a 780m with 7500mhz lpddr5x.  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.  This will continue to hamstring these APUs regardless of how many cu they throw at em.",Neutral
Intel,"“Absolute monster”? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.    AMD has a long way to go before claiming “Monster” class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet aren’t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Max… let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",Neutral
Intel,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,Neutral
Intel,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. That’s far from desktop 4070, never mind 4080.",Neutral
Intel,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,Neutral
Intel,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,Neutral
Intel,">AMD has a long way to go before claiming “Monster” class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",Neutral
Intel,"APU is one of AMD’s biggest markets. You are kidding if you think they don’t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they don’t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games aren’t the only thing APUs are used for so PS5 isn’t wholey in the conversation. PS5 also costs monthly to play online and their games aren’t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",Negative
Intel,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",Negative
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",Neutral
Intel,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,Positive
Intel,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",Negative
Intel,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",Negative
Intel,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,Neutral
Intel,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",Neutral
Intel,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",Neutral
Intel,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",Negative
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,Positive
Intel,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,Positive
Intel,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",Positive
Intel,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",Neutral
Intel,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",Negative
Intel,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,Neutral
Intel,"Installs beta software, proceeds to complain about it",Neutral
Intel,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,Negative
Intel,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",Neutral
Intel,What Ghost of Tsushima issue?,Neutral
Intel,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",Neutral
Intel,"This was a graphics card, not a ‘GPU’ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",Negative
Intel,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",Negative
Intel,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,Negative
Intel,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",Neutral
Intel,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",Neutral
Intel,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,Neutral
Intel,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",Negative
Intel,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,Negative
Intel,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,Negative
Intel,The documentation for it would still be in their archives,Neutral
Intel,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",Neutral
Intel,"Pretty annoying how everything follows the same linear fps/price curve, there’s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",Negative
Intel,Wish Arc cards were better. They look so pretty in comparison to their peers,Positive
Intel,Thats actually a pretty solid and accurate breakdown.,Positive
Intel,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,Negative
Intel,3080 still looking good too,Positive
Intel,What they have peaceful then 4k series?,Neutral
Intel,Just get a 4090. I will never regret getting mine.,Positive
Intel,i miss old good times where radeon HD 7970 as best single core card cost around 400$,Positive
Intel,"Damn, the A770 is still so uncompetitive...",Negative
Intel,"It's like the free market priced cards according to their relative performance. How weird, right?",Negative
Intel,How is that possibly annoying,Negative
Intel,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,Positive
Intel,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",Positive
Intel,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",Neutral
Intel,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,Positive
Intel,8gb perfectly fine today :),Positive
Intel,"Ah yes sure, now where did I leave my 1500 euros?",Neutral
Intel,"I don’t mind free markets, I’m just saying the state of the market is less fun now than it used to be.",Negative
Intel,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,Positive
Intel,"Yeah, i like the black super series.",Neutral
Intel,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",Negative
Intel,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",Neutral
Intel,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,Neutral
Intel,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",Negative
Intel,"""The ability to “turn it off and on again” should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",Neutral
Intel,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",Negative
Intel,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",Positive
Intel,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",Negative
Intel,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",Negative
Intel,"Yo, I saw the title and thought this gotta be Gnif2.",Neutral
Intel,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",Negative
Intel,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",Negative
Intel,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",Negative
Intel,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",Positive
Intel,"I’ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, it’s usually a day late and a dollar short. Blender on Linux still can’t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that there’s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but we’ll have to wait and see.",Neutral
Intel,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*™ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",Negative
Intel,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",Negative
Intel,Long but worth it read; Well Done!,Positive
Intel,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",Neutral
Intel,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,Negative
Intel,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",Neutral
Intel,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,Negative
Intel,I’ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,Negative
Intel,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",Negative
Intel,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,Negative
Intel,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",Negative
Intel,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",Negative
Intel,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100ºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",Negative
Intel,Lmao as a recent AMD intern I feel this in my bones. I still can’t fathom just how little effort is put into software stability these days.,Negative
Intel,100% all of this...  Love looking glass by the by,Positive
Intel,How does say VMware handle this? Does it kind of just restart shit as needed?,Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",Neutral
Intel,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",Positive
Intel,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",Negative
Intel,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,Negative
Intel,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",Negative
Intel,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",Neutral
Intel,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",Neutral
Intel,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",Negative
Intel,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",Negative
Intel,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,Neutral
Intel,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",Negative
Intel,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",Negative
Intel,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",Negative
Intel,TL;DR. **PEBKAC**.,Neutral
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",Positive
Intel,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,Positive
Intel,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",Negative
Intel,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  😤😭,Negative
Intel,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",Negative
Intel,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,Negative
Intel,"Thanks mate I appreciate it, glad to see you here :)",Positive
Intel,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",Positive
Intel,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",Negative
Intel,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",Neutral
Intel,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",Negative
Intel,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,Positive
Intel,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,Positive
Intel,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",Neutral
Intel,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,Neutral
Intel,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",Positive
Intel,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",Neutral
Intel,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",Negative
Intel,"Excellent post, very informative. Would take issue with this though:       ""Speaking of VRAM, The drivers use VRAM less efficiently. Look at any side-by-side comparison between games on YouTube between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",Neutral
Intel,"Funny, I saw the title and thought the same too!",Neutral
Intel,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",Negative
Intel,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",Negative
Intel,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,Neutral
Intel,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",Neutral
Intel,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",Negative
Intel,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",Negative
Intel,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",Negative
Intel,ursohot !  back to discord rants...,Neutral
Intel,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,Positive
Intel,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",Negative
Intel,"I dunno man. I’ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I haven’t used anything by team green lately, so it may all be the same shit , different pile.",Negative
Intel,Lol same with me tbh I haven't had any problems 😂 but I guess some do idk 🤷. I have crashed less with AMD than my old  Nvidia card.,Positive
Intel,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",Negative
Intel,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me £540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent £200 more on a second hand 3090 instead of this.",Negative
Intel,"It doesn't handle it, it has the same issue.",Neutral
Intel,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),Neutral
Intel,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",Neutral
Intel,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",Positive
Intel,Me neither. I use a RX580 8GB since launch and not a single problem.,Negative
Intel,Because they're talking absolute rubbish that's why.,Negative
Intel,You are one of the lucky ones!,Positive
Intel,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",Negative
Intel,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",Neutral
Intel,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",Positive
Intel,lol your flair is Please search before asking,Neutral
Intel,"Hey OP — Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,Negative
Intel,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,Neutral
Intel,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",Negative
Intel,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",Neutral
Intel,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,Neutral
Intel,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,Neutral
Intel,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol 🤣🤣🤣🤣   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",Negative
Intel,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",Neutral
Intel,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",Neutral
Intel,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",Negative
Intel,"""NVIDIA, it just works""",Positive
Intel,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,Negative
Intel,What is the AMD Vanguard?,Neutral
Intel,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",Negative
Intel,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,Negative
Intel,You misspelled $2.3T market cap....,Neutral
Intel,"Okay yeah fair enough, hadn't considered this. Removed it from my post",Negative
Intel,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",Neutral
Intel,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",Neutral
Intel,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",Neutral
Intel,This is not a fix. It's a compromise.,Negative
Intel,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",Neutral
Intel,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,Negative
Intel,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",Negative
Intel,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,Neutral
Intel,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,Negative
Intel,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",Negative
Intel,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",Negative
Intel,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",Negative
Intel,The comment I quoted was talking about people playing games having issues.,Neutral
Intel,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,Neutral
Intel,The thing I quoted was talking about people playing games though.,Neutral
Intel,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",Negative
Intel,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",Negative
Intel,"Idk, I don't use Linux",Neutral
Intel,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",Neutral
Intel,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",Neutral
Intel,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),Neutral
Intel,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",Negative
Intel,Because adding a feature for a product literally gives users more control for that product.,Neutral
Intel,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,Neutral
Intel,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",Negative
Intel,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,Neutral
Intel,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",Neutral
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",Negative
Intel,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,Negative
Intel,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",Neutral
Intel,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",Neutral
Intel,*wayland users have joined the chat,Neutral
Intel,You're falling for slogans.,Neutral
Intel,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",Neutral
Intel,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,Neutral
Intel,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),Neutral
Intel,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",Neutral
Intel,Honestly after a trillion I kinda stop counting 😂🤣,Negative
Intel,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",Neutral
Intel,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",Negative
Intel,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",Negative
Intel,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",Negative
Intel,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,Negative
Intel,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",Neutral
Intel,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",Neutral
Intel,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",Negative
Intel,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",Negative
Intel,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",Neutral
Intel,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",Negative
Intel,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",Neutral
Intel,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",Negative
Intel,Oh then just ignore my comment 😅,Neutral
Intel,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",Positive
Intel,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,Negative
Intel,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",Positive
Intel,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",Negative
Intel,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",Neutral
Intel,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,Negative
Intel,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",Negative
Intel,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,Negative
Intel,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,Negative
Intel,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",Negative
Intel,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",Neutral
Intel,"Too soon to tell, but hopes are high.",Positive
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",Negative
Intel,"Agreed, they cannot rest on their laurels.",Negative
Intel,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",Neutral
Intel,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",Neutral
Intel,console gamers know pc’s are better and don’t really complain about upscaling and 30fps.. you’re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldn’t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,Neutral
Intel,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,Neutral
Intel,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",Negative
Intel,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned off   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",Negative
Intel,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",Negative
Intel,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",Neutral
Intel,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",Neutral
Intel,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,Negative
Intel,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,Negative
Intel,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,Negative
Intel,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",Negative
Intel,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",Negative
Intel,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,Negative
Intel,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",Negative
Intel,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,Neutral
Intel,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete. And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",Neutral
Intel,When did they do this switch? I remember years ago when I configured that their windows drivers weren’t being so nice to the card detected in a VM.,Negative
Intel,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",Neutral
Intel,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",Negative
Intel,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",Negative
Intel,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",Neutral
Intel,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,Neutral
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",Negative
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",Negative
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",Negative
Intel,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",Neutral
Intel,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",Negative
Intel,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",Negative
Intel,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",Negative
Intel,Oh and XE also have bug feature reporting.  Omfg!!!!,Neutral
Intel,Nobody is 100% right ;),Neutral
Intel,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),Negative
Intel,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",Negative
Intel,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",Negative
Intel,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",Neutral
Intel,What about using a DP to HDMI 2.1 adapter for that situation?,Neutral
Intel,"2021 my guy, it's right there on the date of the article.",Neutral
Intel,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,Neutral
Intel,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,Neutral
Intel,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",Neutral
Intel,And I guess infallible game developers too then. /s,Neutral
Intel,So you decide what criticism is valid and what not? lol,Neutral
Intel,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,Negative
Intel,"Yup, but do you see them making a big press release about it?",Neutral
Intel,that is not how it works but sure,Neutral
Intel,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,Neutral
Intel,>whine about Redditors.  The irony.,Neutral
Intel,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",Neutral
Intel,learn to comprehend.,Neutral
Intel,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,Neutral
Intel,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",Negative
Intel,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",Negative
Intel,"No, that would be you obviously /s",Neutral
Intel,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,Neutral
Intel,"Yea, given the state of XE drivers every major update has come with significant PR.",Neutral
Intel,Why not ;),Neutral
Intel,Go word salad elsewhere.,Neutral
Intel,"I have replicated the issue reliably yes, and across two different systems.",Neutral
Intel,If discord crashes my drivers.. once every few hours. I have to reboot,Negative
Intel,Discord doesn't crash my drivers  I don't have to reboot.,Negative
Intel,Really love how the 6000 series radeons look.,Positive
Intel,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",Neutral
Intel,That's a good looking line up,Positive
Intel,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",Negative
Intel,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",Positive
Intel,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",Positive
Intel,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,Positive
Intel,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",Negative
Intel,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,Negative
Intel,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",Negative
Intel,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",Negative
Intel,That's suprising. Generally the 70 series and AMD equivalent cards are more popular in Germany. Is 4060ti discounted heavily?,Neutral
Intel,"Always happy to see ARC succeeding. We need a third competitor, and they've got what it takes.",Positive
Intel,Im more confused about how do you guys still buy RX 6xxx series. Only RX 66xx series that are still widely available in my country. RX 67xx and above is just OOS.,Negative
Intel,That 7900xtx sale number is insane,Negative
Intel,That just shows that most people that buy GPU's don't know a thing about them.,Negative
Intel,"4060 Ti is not discounted well at all here.   390€ and 460€ for 8/16GB versions, really bad deal which is why I am absolutely baffled to see it this high in the list. Might be christmas shoppers who just buy the highest nvidia card within their budget without doing any research - that's my best guess actually.",Negative
Intel,best discounts were 6750xt 6800 and 7800xt,Positive
Intel,4060 ti is selling as a surrogate Quadro for AI/ML. $400 for 16gb is not awful in that context and you get full CUDA support,Neutral
Intel,"Intel is putting in the effort on the software front as well. Arc cards are great value, especially with the improvements intel has made with driver updates. With talk about XeSS being integrated into DirectX with Windows 12 (among other upscalers as well) I hope that intel GPUs will be even more competitive.",Positive
Intel,"Battlemage needs to be stable and competitively priced.   First impression matters so much these days, it cannot be overstated imo.",Neutral
Intel,"They're not out of stock there, duh",Neutral
Intel,Yeah the Germans love the 7900xtx in particular for some reason. It was really high up considering its price in last week's summary as well. I guess they don't appreciate the 4090's price.,Positive
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"I have so many (adult) friends who always bought NVIDIA, will always buy NVIDIA and don’t do a single bit of research.   Had one friend even ask me, I recommended the 7900Xt for her budget and she got the 4070 Ti because she didn’t trust my word or AMD  It’s such a big habit issue. Shopping season really showed me that nvidia could literally sell a brick and habit shoppers would buy it",Negative
Intel,">390€ and 460€ for 8/16GB  That equals to $350 and $415 before taxes, a lot lower than in USA ($390 and $450 before taxes)",Neutral
Intel,"They are pretty much gone in Germany as well:  6950XT - only XFX for 599 at Mindfactory   6900XT - gone   6800XT - only XFX for 499 at Mindfactory and Asrock for 580 at various   6800 - only XFX for 435-440 at Mindfactory and some   6750XT - a bunch of models for 373-405 at various   6700XT - a bunch of models for 344-369 at various   6700 - XFX and Sapphire for 298 and 322 from one retailer   6650XT - gone   6600XT - gone     So the high-end is gone except that XFX stock that will probably be gone in the next 2-3 weeks as well, the lower end is gone. Only 6750XT and 6700XT is still there a bit more and of course the 6600 because that has no replacement at it's current 200-220 price, the 7600 is way more expensive at 270-320.",Neutral
Intel,"I know, but i mean, both 6750 XT and 6700 XT still has more than 300 stocks each in single week (if i read the stat correctly). That's a lot of GPU. I thought the stock is already thin globally which in turn make stock in my place nonexist.",Neutral
Intel,"I thought I saw some 8GB models as low as $330 and 16GB models at $400 but even then I would much rather get another GPU like a 6700XT  (you can still get at here and we even had one down at 299€ for a while, think they are back at 330€ tho)",Neutral
Intel,the card is pretty bad if you missed that somehow,Negative
Intel,AMD probably ships leftover to countries in which they know it will sell,Neutral
Intel,"Sure, but I was replying to you saying ""4060 Ti is not discounted **well at all** here""",Neutral
Intel,"Because most people are not willing to pay that much for GPUs that weak. Sure ""discounted well"" is subjective anyways, I got your point that they are below MSRP but I am pretty sure you know what was implied when reading my comment.",Negative
Intel,"As you notice the photoshop version differs, so you can't compare them really",Neutral
Intel,I also another insanely high score:     7900X + RTX4090 [Photoshop score 8995](https://benchmarks.pugetsystems.com/benchmarks/view.php?id=168538),Neutral
Intel,So basically PC games are never going to tell us what the specs are to run the game native ever again.,Negative
Intel,"Well, folks, the game is basically software ray tracing an open world regardless of settings at all the times using compute shaders from what I gather. I mean, what'd you expect from something pushing consoles to the brink? That it would not be equally demanding on PC?",Neutral
Intel,The true crime here is needing FSR to reach these requirements.,Neutral
Intel,"Oh wow, this is probably the first time I'm seeing dual channel listed in the RAM section of specs requirements.",Positive
Intel,im more baffled as to why they are comparing a RX 5700 to a GTX 1070. Id have thought they would have said a RX 590 (instead of 5700) or RTX 2060 (instead of 1070),Neutral
Intel,> Enthusiast   > 1440p [...] with FSR Quality [...] 60 FPS  1440p + xxxx Quality upscaling is just another way to say 1080p you cowards. It's 2023 and enthusiast level is 1080p @ 60 FPS.   Looks pretty though with the amount of vegetation.  I'm surprised that they put the RTX 3080 and RX 6800 XT side by side in a game that has forced ray tracing. I guess that's the advantage of being cross platform and AMD sponsored.,Neutral
Intel,"well, at least the chart is easy to read, not a complete mess",Negative
Intel,This gives me hope that Star Wars outlaws will also have FSR 3. 🤌,Positive
Intel,60fps with balanced upscaling on a 4080/7900xtx … That’s not a good sign,Positive
Intel,It's joever. We are officially at the point where devs are using upscaling as the standard to reach playable frame rates instead of optimizing their games.,Negative
Intel,"If you look in the top right, these seem to be based on ray tracing being enabled? If that is the case, these specs actually seem reasonable.",Neutral
Intel,"FSR and DLSS were supposed to be optional, now they seem to be mandatory in any graphics setup.",Neutral
Intel,Yay finally a game that'll have both fsr 3 and dlss 3 so it'll work perfectly on either gpu brand at launch.,Positive
Intel,"Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. The requirements should all be done without any upscaling or frame generation tech to be upfront and honest about how the game will run. Upscaling will then just be an available option for users to further increase performance.",Negative
Intel,"Actually not to bad for current standards, 1440p 60 with fsr quality on a 6800xt/3080 is great if high includes RT, if it doesn't then it sucks",Negative
Intel,"So with my 6800xt, which should be able to do 1440p high settings at over 60fps easily, I'll have to use FSR just to get 60fps? Great, another unoptimized game headed our way.  So sick of the awful PC ports and such. 6800xt is still a very powerful card yet I'm seeing more and more games require it for 1440p 60fps high or even 1080p 60 fps high. Ridiculous.  The reliance on upscaling tech instead of properly optimizing the game is becoming a problem. And it seems this game has forced RT which is idiotic. RT destroys performance and should always be an option.",Negative
Intel,"If you notice, all the mentioned AMD GPUs are significantly better than Nvidia GPUs, so does that mean bad performance for AMD GPUs?",Neutral
Intel,So i bought an 1200€ card (rx 7900xtx in my region and at the time of launch) and can now enjoy new games at 60 fps with BALANCED fsr upscaling. We love to see it. /s,Positive
Intel,so I'm doomed to use FSR even at 1080p ? what a shame pc gaming has become.,Negative
Intel,"I honestly don't get this upscaling outrage. It's not a fixed requirement. If you want to play native, just turn off upscaling but you will have to turn down settings to compensate. Everyone wants to have their cake and eat it too. People want super low requirements, super high graphics with high FPS.  The thing is game devs/publishers want to highlight all the ""eye candy"" in games to show their advancement and that's how they want to advertise it. They have a certain level of visuals they want to present for the game. So they crank up the baseline and focus on making it look as good as they possibly can and by using upscalers to get to the playable FPS.  Other option is they ditch making games look any better and just keep it restrained to good FPS at native. But then you'll have the other subset of complainers that will complain about the game's graphics looking like X many years ago.  Take Alan Wake 2 for example, The medium setting looks better that some High-Ultra setting games from a few years ago. Running a AAA title 2-3 years ago at 'medium' preset may not equate to running a current AAA title at medium preset.  The games are getting too demanding because they are starting to push the limits visually (partially because on AAA titles, that's something they really focus on for advertising), if your card can run it, great, if not, adjust your settings to match your cards capabilities. If you bought a 6700XT 2+ years ago to play native 1440P on high setting, guess what, it's not always going to play 1440P at high settings as the years go by. You have to adjust your expectations of the card's capabilities as time goes on. Medium preset now is likely better than High preset then. If you want the ""same"" performance you used to get, match the visuals and see how it does.  Seems like the only way to get people to stop complaining is to just halt all visual/graphical progress and stick to limiting the game's visuals so they run well on mid/low tier cards.  People keep throwing out ""OPTIMIZATION"" but on one knows what/how they want that done. WHAT exactly do you want optimized? How do we know the game isn't optimized to the graphics/performance it gives?  What if they just made the ""High Quality"" preset look like games from 2018, would that now be considered ""Optimized""?  (I'm not talking about this game in particular, just in general, it was same outrage w/ AW2 and that turned out better than expected)",Neutral
Intel,I hope they will bundle this game with CPUs/GPUs,Positive
Intel,holy hell so for 1080p you need a 1440p card because not even a RX 7600 is on par with a 3060ti or a 6700XT,Negative
Intel,Recently upgraded GPU and CPU and crazy that newer games are already requiring the newest GPUs to play them. Personally I don't think it's a horrible thing but at the same time it just screams horrible optimization.,Negative
Intel,This game better look like real life with those specs. I does looks beautiful!,Positive
Intel,"Another garbage with forced upscaling at baseline. When upscaling winded up - everyone was ""Hell yeah, high refresh rate AAA gaming""  Fast forward to today: ""FSR performance just to hit 1080p 60fps - yay..""   I fucking knew it.. it was so obvious upscaling will turn into this and replace game optimizations. Why optimizing shit, when you can tell people to turn on fucking FSR / DLSS / XeSS performance??   What a fucking shitshow... And few weeks back when I said I'm not gonna buy AMD GPU again because upscaling is getting mandatory and DLSS is just objectively superior - I was laughed in the face. Every major AAA game onwards will be shoving up upscaling as base line performance - so your typical 60fps marks (and 30fps - yes apparently that is still a thing on PC in 2023). There's no escape from this.",Negative
Intel,Damn. My 5700XT did Mirage fine on Medium/Low shadows. Now its basement quality.,Negative
Intel,This is starting to get scary and not cool. Every stat is not native performance and a 6800xt with fsr at 1440p is only pulling 60fps?!?!? What the actual fuck is going on?!?!? Midrange is definitely on its death bed if this is the new norm...,Negative
Intel,I love it how absurd these things are these days.,Positive
Intel,Really tragic this IP ended up with the most anti-consumer publisher in the world.  Layers upon layers of DRM.  Timed Epic exclusivity. No achievements.,Negative
Intel,Does anyone know if it supports SLI or crossfire?,Neutral
Intel,Source  https://news.ubisoft.com/en-gb/article/G0eJBcH8NcvNO2MPqb1KV,Neutral
Intel,"Hopefully this is another Alan Wake situation where the game performs better than what their system requirements would suggest. With this being a based on a movie game, I don’t have super high hopes that that’ll be the case though. Going from movie to game and vice versa almost always has bad results.   Also, displaying system requirements with upscaling is a bad joke. This is basically saying you’re not getting native 1080p 60fps without a 6800xt or 3080. Upscaling is definitely becoming the crutch a lot of folks feared it would be.",Negative
Intel,I thought upscaling technology is for low end gamers to get more FPS or people who work on 4k monitors and use upscaling to get better frames on them. but it seems to be used as an excuse for bad game optimization. What a complete joke!!,Negative
Intel,They are so bad this days that if the say the specs without the upscaling it would be 4090s all across the board,Negative
Intel,Why in the f*ck is upscaling included on a specs page?,Negative
Intel,no more software optimization and full upscaling  bleah,Neutral
Intel,Now THIS is what I call a reasonable spec sheet for a next-gen game 👏👏👏,Negative
Intel,"Surprisingly good specs. 1080p high 60 FPS on 3060ti/6700XT is what we can expect from 3 years old cards in open world games.  Yes, I know it's with upscaler. But if the footage stays - it's understandable. Ubisoft is always about open world games afterall.",Positive
Intel,"I've never even heard of this game, nor care about it, but these system requirements offend me.",Negative
Intel,seems like graphics reached its eak with ps4 and games dont look that much better naymore and requires 4k dollar pc to run with dlss,Negative
Intel,"What about steam achievements ? :), i want to show off perfect games on my steam profile :)",Positive
Intel,"Telling me my $1,000 GPU will only get me ~1200p at 60fps tells me you don't want me to buy your game.",Negative
Intel,How about support them with games that people actually PLAYS ? Like Alan wake 2 🤣,Neutral
Intel,Fuck FSR and DLSS. This isn’t how they are meant to be used.  Rather than making a good experience better they are being used to make unplayable trash playable. Papering over shit programming.,Negative
Intel,Nice,Positive
Intel,"If Anti-Lag is still locked by then , please enjoy your frame gen input lag.",Neutral
Intel,"The game has a huge graphical potential, it's the right time to release that much potential environment in this era .   But it wasted its potential by considering lower class GPUs there by reducing polygonal complexity and complexity of environment including terrain and vegetation , animal complexity ,etc.....  Imo, it needs to have 2X polygon than what it seems.  It seems ultra poooooorly optimized as per the recommended requirements coz it's a bad graphics.",Negative
Intel,Native gaming died or what ? Wtf they turning pc gaming into console gaming,Negative
Intel,"60 FPS WITH Frame Gen AND FSR balanced? God, that's gonna be a horrible experience.  I really don't like the direction we are heading.",Negative
Intel,4k balanced FSR means 1200-1240p ... lower than 1440p ! And 4080-7900 XTX for that ?,Neutral
Intel,Their supporting FSR 2 and 3? what the hell does that even mean? Unless they plan on releasing b4 FSR3 becomes drops.,Negative
Intel,"Omg, it would be a graphic master piece or  bad optimized thing.",Negative
Intel,First time I see matches recommendations for nv and amd GPUs...,Neutral
Intel,Rip laptop rtx 3060 6gb,Neutral
Intel,"How do FSR 2 & 3 work with tech like ReShade?  ReShade has a pretty cool shader that will make any game into stereoscopic 3D.   When it works it works well, but it doesn't work in every game.  I'm curious because I would like to try it for this, since like 99.97% of Avatar's appeal (shtick?) is the 3D.",Positive
Intel,Upscaled 1080p low settings and only 30fps with A750? At least the game has xess.,Neutral
Intel,*NATIVE* resolution gang ftw!,Neutral
Intel,"I have been wondering with a few releases lately (Forza Motorsport, Cities Skylines) what GPU the dev machines they are using. I have a 7900XT and it ain’t enough to get above 60FPS without variable resolution at 1440p, seems kinda crazy that one of the top 5 cards from the latest gen can’t push enough pixels for these games.",Negative
Intel,Looks capped at 60fps?,Neutral
Intel,"Funny, how it only tells you the settings for FSR but not for DLSS and XeSS even though those 2 are also supported.",Neutral
Intel,How come 6800XT is mentioned? But not the 7800xt? As recommended specs for 1440p?,Neutral
Intel,"Rip my 780m and rtx 4050m. I don't understand why upscaling is ""mandatory"" not ""supplementary"". Do we need a SLI rtx 4090 for native 4k@60hz?",Negative
Intel,Is it using UE5?,Neutral
Intel,"Ubisoft, rip on launch.",Neutral
Intel,guessing no DLSS3 then?,Neutral
Intel,I guess that fsr3 will be added in an update at some point since everything mentions fsr2. It will be interesting to see it running on my gtx 1060 since it seems to be a big upgrade on older cards.,Positive
Intel,"Another game with upscaling baked into all presets. ""Free performance"" they said. I expect frame hallucination tech to be included in these presets in 1-2 years tops",Neutral
Intel,"Can't wait to be ridiculed by Alex at DF again for wanting PC requirements that don't include upscaling.   Upscaling should be an assist, not a requirement.",Negative
Intel,Farewell 1660ti… looks like it’s time for an upgrade,Neutral
Intel,well my 3300x is now obsolete for these new AAA games...,Neutral
Intel,4k ultra right up my alley 😏,Neutral
Intel,fsr3 frame gen but in the specs themselves FSR2 is stated everywhere. What? So there's FS3 FG bu no FSR3 upscaling?,Neutral
Intel,7900xtx will do 4k 120fps with FSR 3 then I guess?,Neutral
Intel,What must one do to achieve a higher rank than an enthusiast? A demi-god?,Negative
Intel,"I think we're at a point in gaming where DLSS / FSR will be mainstream in pretty much every game now. I mean even my 4080 can't get 60FPS 1440p in the Witcher 3 or Cyberpunk maxed out with ray tracing, have to use DLSS to increase to above 60fps. I do think DLSS / FSR is actually good technology, but for those enthusiasts who like to play native, I feel like having a $1200 GPU that can't run 4K or even 1440p ULTRA settings at 60fps depending on the game, is just a cash grab.",Neutral
Intel,Is vrr fixed with frame gen then?,Neutral
Intel,"Will someone please get a message to developers that FSR/DLSS should be *optional* and used only if you want to sacrifice some visual fidelity in order to run 144 FPS+ on high refresh rate displays, and let them know FSR/DLSS should never be *required* to get a 90GB bundle of spaghetti code to *barely* reach a blurry ass 60 FPS?  K? Theeenks!",Negative
Intel,"Yet another AMD sponsored game without DLSS....      EDIT: Nm, it appears this game will offer DLSS thank god.",Positive
Intel,Hoping with my 7800x3d and 7900xtx fsr 3 4k maxed is around 120fps for my tv refresh rate. To be honest at my age I barely notice the quality difference of any input lag.,Neutral
Intel,They recommend upscaling even at 1080p.. disgusting ew,Negative
Intel,pretty much this we all knew they would start using upscaling as a crutch.,Negative
Intel,Welcome to modern times and how you can’t just brute force everything anymore. Or you can go buy a 4090 and not complain about how expensive that is.,Negative
Intel,"Yeah, I'm not crazy about using upscaling techniques on everything to gauge it's performance and requirements. I think more time needs spent on optimization but I could be ignorant for stating that.",Negative
Intel,Thanks to all of you that were screaming dlss looks better than native lmao.,Positive
Intel,"I tend to be much more picky than those around me. A group of people I play with were all playing Remnant 2, and I FSR was driving me crazy, they all have way less powerful computers than I do, and none of them cared at all FSR was on. In fact they were impressed with how well it ran and how it looked. I think there is a general 80% of the PC population that doesn't mess with graphic settings and just wants the game to play. I'm a min/maxer when it comes to graphics where I constantly tweak it until I get the perfect output for me.     I'd love to have real data from games like this on how many people actively change graphic settings.",Neutral
Intel,and a year ago i kept saying this here and i got downvoted to hell   its so obvious that the game engine devs and game companies pressured both nvidia and amd for this because they can release games faster and somewhat unoptimised,Negative
Intel,"Upscaling, once a promising and beneficial tech, now abused by almost every dev cause they are lazy to optimize. :(",Negative
Intel,Nope we as a community abused a nice thing,Positive
Intel,"Alan Wake 2 (and others) is proof that this doesn't matter any more. DLSS Quality looks and runs better than DLAA native rendering as shown a number of times and documented by tech power up and actually observed by those of us playing it  It's about time people stop crying the ""omg no native rendering"" beat as this is a mentality that is irrelevant in 2023. It's pretty clear that too many games come with performance issues and as such ""native"" rendering has the worst performance in those titles. At least upscaling offers a means to get better performance and in the vast majority of titles, better image quality as a result too with intricate details rendered at a higher fidelity thanks to image reconstruction. The likes of Digital Foundry showcase those benefits often in their videos. Plus the fact that all of the consoles upscale yet nobody bats an eyelid, yet for some reason a small portion of PC gamers seem to demand it no matter what, and then complain when it doesn't run as well as they had hoped in various titles.  It's a hard pill to swallow, but it's \[upscaling\] progression and the key driving factor as to why we can path trace and use other modern engine tech with such good performance and visual quality. This is probably the wrong sub to say such things, but this is the truth, and I fully expect pitchforks to be mounted.... But don't take it out on me, point the finger at AMD and ask them why they are falling so far behind.  At the end of the day if it looks good and runs good then nobody needs to bat an eyelid on whether it's got native or upscaled rendering. Everyone benefits as a result. Fact is the vast majority of modern games look and run better when using DLSS with only a handful of FSR exceptions (Callisto Protocol, for example).  Personally I want improved image detail, sharpness and performance, not just AA that DLAA brings to the table since it's just a more modern AA method vs ancient TAA/MSAA/FXAA etc. I don't care if there's no native option, I can just run with DLDSR if I want to render at a higher res and then apply an upscaler to that output to get the best of both worlds.",Neutral
Intel,Didn't take them long to make upscaling worthless.,Negative
Intel,i smell a burgeoning cottage industry of game spec reviewers!,Neutral
Intel,"I've hated upscaling since the beginning.   Now, it's being proven right in our faces that companies are no longer making 100% they're making 45% done games, and then they rely on upscaling and fake frames to do the rest.   Why have a fully optimized game that runs well natively when you can just lower the resolution by 3x then have frame generation help?   Full force ahead to frame generation= Every game will be optimized so badly that you're forced to upscale",Negative
Intel,"It’s literally right there in the hardware specs. 1080p fsr2 quality is 720p input resolution, so at native the game runs at 720p low on a 1070 or 5700.  It is the inverse of what reviewers have been doing with their benchmarks - if you are a fan who really has a stick up their ass about native res, just do the math yourself. They’ve given you all the pieces you need to calculate it out yourself, but that’s not how the game is supposed to be run so they aren’t going to put native res in the official marketing material.  Games are optimized around upscaling now and it is unhelpful and misleading to pretend otherwise. Rendering 4x as many pixels is *obviously* going to throw things off with more intensive effects, and lead to a generally unplayable experience.",Neutral
Intel,"Judging by how games work on 4090/7900 xtx, these cards turn from 4k 60 fps to 1080p 60 fps",Neutral
Intel,requirements have been a joke for over 20 years at this point i dont know why people are surprised.   on the other hand crappy devs using fancy upscaling and fake frames to get the game playable instead of just making a well made game is infuriating,Negative
Intel,yeah this is the new standard,Neutral
Intel,"RT is the future. With this game and Alan Wake 2 using software RT at all times and hardware RT for anything beyond ""low"", RT is the future.  At some point GPUs will be so powerful and game engines will have RT in the bag that RT will be used in basically all indie games...and then suddenly nobody will talk about RT anymore because RT is in everything for years.",Positive
Intel,and with fsr now standard on consoles it(or a similar tech) will be standard on pc :(,Neutral
Intel,"people also forget both AMD and nvidia didn't really offer a generation leap in GPU performance this gen, sure it's popular to call games ""unoptimized"" (I know many AAA releases are, not denying that) but the issue compounds with the fact that game devs couldn't predict a GPU generation to be this bad caused by inflated crypro-boom margins a generation prior",Negative
Intel,"As with almost every game release recently, expect the game to perform a bit better than the requirements.    Sometimes the hardware can achieve 60 FPS most of the time but drops below that in other times, so the devs just say it's guaranteed 30 FPS and call it a day. If you look at the 5700 vs 6700XT, the gap is almost 50% in most titles, this means that if the numbers for the 5700 were accurate then the 6700XT will be getting around 45 FPS at 1080p low (unless the game utilizes special hardware only available on the 6700XT) but the game targets 60 FPS at 1080p high on the 6700XT which leads me to believe the 5700 is gonna perform much better which should leave room for turning off upscaling.",Positive
Intel,"It does state ray tracing in the top right, we don't know if that's included in the requirements or not. I would definitely reserve judgment until we find out.",Neutral
Intel,Don't forget about Frame Gen too. IMO that's the worst part because that means it's 30 fps without it. Imagine the input latency...,Negative
Intel,The actual true crime here is even having fsr to begin with. It should just have dlss,Negative
Intel,Seems like they tried to cover every basis with these.,Neutral
Intel,"Yeah it's kinda weird, even games that had a massive difference of performance when using dual channel RAM never listed it on their requirements :/",Negative
Intel,"I've noticed that many developers do this, for some reason developers seem to have some kind of secret passion for the 1070 lol",Neutral
Intel,It's because the minimum settings is with software RT enabled and thus they're using the fastest cards without hardware RT that can reach 1080p30 FSR2.,Neutral
Intel,"GCN support is over, RDNA1 is the lowest currently supported arch.",Neutral
Intel,"In 2023, RX 5700 performs equally or better than the RTX 2060 Super/2070. The 5600XT is more like a 2060 competitor.   Either the game runs like shit on AMD hardware, or the system requirements are wrong and they just write down the components that they tested with.   Another thing is that the game is doing is software based RT by default and that might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.",Negative
Intel,"They're not, those are the cheapest/slowest GPUs that have 8 GB of VRAM. Clearly game devs are done pandering to people who bought Nvidia's e-waste.",Negative
Intel,What do you mean forced raytracing?,Neutral
Intel,1440 + upscaling looks better than 1080p because you're on a 1440 resolution monitor.   Do you even 1440p?,Positive
Intel,"Honestly RT has a lot of fluff and the majority of which is the most costly for the least amount of noticeable fidelity increase. Try path tracing in Cyberpunk with reduced rays & bounces and you'll still get great soft lighting and GI without most of the performance hit when full tilt. If games only leverage certain aspects of RT then the performance is still comparable, that's also why the 3080 and 6800XT perform similarly in Fortnite with full RT/nanite/lumen settings.",Neutral
Intel,It's actually 960p :(,Neutral
Intel,"can advertize pushing all the triangles and RT if you dont stick with a low resolution, certain things launched WAY before they should have in the consumer space.",Neutral
Intel,Support FSR3.0 and being able to use FSR3.0 at launch gives me a bit of skepticism lately,Neutral
Intel,We'll see in a year.,Neutral
Intel,AFAIK  &#x200B;  It is with RT,Neutral
Intel,Seems pretty good to me given it is at 4K with RT,Positive
Intel,"Likely includes the RT features , its also 4k",Neutral
Intel,"Its ubishit, what do u expect, however, since the game has RT reflections, shadows and GI, then i would say this is based on AMD shit RT performance, so lets wait and see how the 4080 is gonna perform.",Negative
Intel,"I wouldn’t mind so much if it was exclusively a 4K thing, but now devs are leaning on upscaling *at 1080p*, where it still objectively looks like ass and blurs everything in the background.",Negative
Intel,"Everyone asked for this. ""iTs tHe fuTuRe"".  ""LoOks bEtTeR tHan nAtiVe"". ""BiGgeR nUmBeR bEtTer"".",Positive
Intel,and we were called [names that frankly should have resulting in bans] when we said this is where it would lead to. and we still get shat on for saying the same about fake frames,Negative
Intel,"I think the top right table is listing the available features/settings the game supports. Depending on the presets, RT may be enabled by default or it may be a separate option from the presets. I guess we'll find out.",Neutral
Intel,"Nothing will stop DLSS / FSR nor RT in new games at this point.  The problem with the rising hardware requirements is, that it will keep a lot of 1440p/4k dreaming gamers with 1080p for a very long time and FSR doesnt really look good enough in 1080p.",Negative
Intel,I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.,Neutral
Intel,Exactly! It even got xess so Intel users also can use xess,Negative
Intel,">Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. T  its not much different for consoles , like alan wake 2 uses Low to mid settings and FSR balanced on PS5 even on switch titles use scaling and no man sky even FSR and stuff.",Negative
Intel,"I agree with this, that’s not that outrageous if this does include RT. Non RT would be pretty bad to need fsr but that is the quality mode.",Neutral
Intel,This is going to be the exact same situation as with Alan Wake. The game is going to come out and the requirements are going to make sense. People just aren't used to seeing games with mandatory rt.,Neutral
Intel,I would say pressured bc they announced it over a year ago and promised we will get it in september. Thats the problem with games an tech they all make promises and then cant keep the deadline. Just make smth new and when it works then announce it and make the last fine adjustments. If you make promises to early you set yourself up for failure.,Negative
Intel,"Maybe on the lower end, but for enthusiast and ultra... no, they're not ""significantly"" better. These are pretty on-par comparisons (outside of RT).",Negative
Intel,"How so? The 4080 and 7900xtx are neck in neck, same as the 6800 and 3080.",Neutral
Intel,"It's 4k, it will take another 5-8 years to be mainstream.  1440p slowly creeping to 10% marketshare give it another 2-3 years to be mainstream",Neutral
Intel,"Consoles use fsr as well, and not just fsr but often at low precision making the upscale rougher. Its an industry wide trend.",Neutral
Intel,">what a shame pc gaming has become.  Could be worse , Consoles use mostly FSR balanced like in Alan wake 2 the PS5 uses Low to mid settings and FSR balanced.  its likely the trend of going from specially home made engines  catered to the games needs to "" Can do everything , but nothing great"" multi tool engines like UE.",Negative
Intel,"> what a shame pc gaming has become  I mean, PC gaming is in the best state ever. So many amazing games that run so well. Just don't buy all these shitty games, not like there is a shortage.  Tbh this game looks like standard movie cash grab shite anyway. Solid 4/10 game I bet.",Negative
Intel,Which means if all games will require upscaling it will force AMD to improve there's. I'm on board with you though will see way less optimization going forward which is not what we want.,Negative
Intel,"Absolutely agreed my friend. Time to leave pc gaming and focus on building yourself. The gaming industry is corrupted to the extreme, and will continue to do so as long as players continue to praise lazy developers. In the pussy that whole shit.",Negative
Intel,"Mirage is PS4 game, Avatar is PS5",Neutral
Intel,"Update: I must've read that wrongly somewhere! Apparently, it uses their own Snowdrop Engine 😳  Well AC: Mirage is nowhere near a next-gen game. Avatars of Pandora, on the other hand, uses Unreal Engine 5 with hardware ray tracing. So it's only natural it needs more horse power. But it'll look 10x better than AC: Mirage. So there's that 😉",Negative
Intel,Timed epic exclusivity? Aww man.,Positive
Intel,Was guaranteed to happen when up scaling was announced... It's an excuse for less optimisation or to push fidelity more.  I saw a video of this avatar game it got an insane amount of plants visible at all times.,Negative
Intel,You... are.. joking... right..?,Neutral
Intel,Because it’s not coming to steam. It’ll be an epic games exclusive so everyone will pass on it until it hits steam in 18 months. At which point no one will buy an 18 month old game for full price and will wait for a 50% off sale at the minimum.,Negative
Intel,Kind of a messed up opinion.  Maybe games don't look much better than PS4 on your PC but they sure as hell do on mine and that would still be true at a lower resolution and framerate.,Negative
Intel,Ubisoft no longer employs achievements on new titles.  Don't get your hopes up.,Negative
Intel,Try ubisoft achievements :),Neutral
Intel,"It's doing rt at all levels and 4k? Ye... It isn't mainstream yet, heck 1440p is like At 10% market share",Neutral
Intel,"Iam using afmf daily, the input lag introduced by afmf is between 5-12 ms for me.  Doubt any can literally feel this.  I have afmf enabled in all possible games & emulators.  If you think it's bad for you, you don't need to use it",Negative
Intel,Pretty much yeah... Was inevitable that Devs will use up scaling as excuse.  Same for taa being a absolute shoddy aaatleast up scalers do better aa :/ . But even consoles run now heavy fsr like the PS5 uses fsr balanced on WLAN wake on mostly low settings,Negative
Intel,We don't know the settings.  If its like cyberpunk psycho rt on release which 3080 and 3090 easily needed dlss balanced or performance for on 1440p and 4k...    Then I can see the same case in this game for 4080 and similar cards  Also they dont speak about frame gen just fsr2 ( which doesn't exist here because they support fsr3 which does up scaling and frame gen it's not split like dlss 2 & 3),Neutral
Intel,"The fans of avatar, or people that like fantasy games or just games to have fun in.  Luckily games don't need to be for everyone.",Negative
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Really comes up to how far the rt and lumen stuff can be pushed, if it goes to cyberpunk psycho rt levels? At release it needed a 3080 and 3090 with dlss.   So it could need top end gpu of this gen if its a similar case",Neutral
Intel,yup that is why I will play 1440 UW native with a 7900XTX.,Neutral
Intel,I guess they are confused with dlss.  Dlss 2= up scaling Dlss3 = frame gen  So they thought its for fsr3 too. ( as in fsr2 up scaling and 3 frame gen)   But fsr3 can do frame gen or not.  Fsr3 is up scaling and frame gen.,Neutral
Intel,"Likely mid range gpu and a suit yells in the background ""30 fps with up scaling is fine""",Neutral
Intel,"Likely the same, they didn't want to make the graph even more annoying, hence why fsr3 and rt and xess are mentioned top right.  They got anyway already confused with fsr3 and fsr2 ( cause 3 is fsr frame gen and up scaling it's not like dlss where 2 is up scaling and 3 is frame gen)",Negative
Intel,Very similar performance I guess,Neutral
Intel,Ye,Neutral
Intel,They've been using it since forever. The difference is that PC GPUs were 10x faster than console GPUs so there was no need for it on PC.,Neutral
Intel,"It mentions dlss, fsr3 and xess support, doubt they limited it to dlss2.",Neutral
Intel,"Likely, they were confused because dlss 2 and dlss3.  So they thought its fsr2 and 3 too.",Neutral
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,Honestly... Maybe fsr3 frame gen can help you but... A 3300 Was already outdated when it came out.,Negative
Intel,Most likely the upgrade part called FSR 2 and the frame creation part called FSR 3,Neutral
Intel,If they didnt get confused between FSR 3 and 2 then yes fsr 3 roughly doubles fps same for AFMF.,Neutral
Intel,"Nah usually enthusiast in hardware did mean "" max settings no compromises high frames"" but let's say... This changed the last 3 gens sadly.",Negative
Intel,"with AFMF it works , didnt test with FSR3 now.",Neutral
Intel,Reading before raging :),Neutral
Intel,"It is even low quality with 30fps and still needs 5700 ~~XT~~. That is pretty bad :D  *Corrected to 5700, which I think is still a lot for FSR2 at low fullHD, but definitely cheaper GPU.",Negative
Intel,That's with 5700 and 1070.... WHAT exactly are you expecting those cards to do in visually demanding AAA titles in 2023?  People have some crazy expectations. Everyone wants the eye candy but no one wants to pay the performance cost.,Negative
Intel,For 30fps even lol,Neutral
Intel,"Especially recommending FSR.  DLSS at 1080p? Sure why not?   FSR at 1080p? Disgusting, absolutely horrific.  Anyone who downvotes this is a fanboy, end of story. Nvidia's DLSS simply is better at reconstructing an image. At lower resolutions that is SIGNIFICANTLY more important.",Negative
Intel,Or be happy your shitty video card is still supported.,Positive
Intel,and we still get shat on for saying they are or admitting its a bad thing. corpo fanbois are so cringe,Negative
Intel,Even 4090 users use DLSS because it fixes TAA problems. And has a sharpening slider for easy sharpening adjustment.,Negative
Intel,When did I ever complain about how expensive gpus are. I've always been in the category that they are luxury item and vendor can charge whatever they want for em. So you aint never hear me complain about gpu prices.,Negative
Intel,I can't market a game for running with 120fps 4k with raytracing on on a RTX 4060.  But I can market 400 completely identical missions that take up 150 hours of your lifespan and empty out your soul.,Negative
Intel,Yea I know nothing about optimizations and if it could or couldn't fix this or that. I just want them to tell me exactly what I need to run the game at X resolution a Y preset with 60 fps. If that calls for a 14900k and a 4090 then just say so.,Negative
Intel,"Nah, I think we're really beginning to see a trend here with abusing upscaling for free mileage.  Not that upscaling isn't great but...  Apart from RT, on medium settings, I can't really see    massive improvements on visual quality that would warrant such a massive increase on requirements.  UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  We'll probably see this continuing to be a trend for a while, and then as tech novelty wears out, some devs will come out with more optimized and scalable titles.  With hardware prices being as they are, and with 60fps now being highly requested in consoles as well, it's pretty much a given.",Negative
Intel,"People accepted upscaling with open arms, so now devs know they can rely on people using it. Thus they don't have to care about optimizing for native res anymore. Oh you have a 4k card? Okay well we'll make sure it can run the game at 1080p so you can upscale it.",Negative
Intel,"No average consumer wanted to come to this point. The market leader started pushing upscaling instead of just making stronger GPU’s and there’s nothing AMD can do to win them, only join them.",Negative
Intel,Turns out TAA is worse than AI TAA with AI upscaling.   Of course devs are gonna use it to cover their own asses. And they get perf boost!  Let's face it. The FUTURE is AI everything. Nobody is gonna talk shit about frame generation and upscaling when it looks perfect each frame.,Negative
Intel,But it does in many ways.,Neutral
Intel,"I don't know who those people are. I play all my games on native, whether I'm using an amd or nvidia card.",Neutral
Intel,"dlss looks better than native with TAA because taa is so bad, but dlaa/fsraa at native >>>> upscaling",Negative
Intel,you think engine devs can pressure AMD and even Nvidia? Come on man. Be real.,Negative
Intel,"Ever notice how multi-GPU vanished the moment DX12 shifted the onus from AMD/Nvidia over to the developers? It shows just how much effort they put into getting SLI/Crossfire working when developers instantly drop them the moment they have to do the bulk of the work (significantly less, I might add) themselves.",Neutral
Intel,"If I relabeled ""Medium"" preset to ""Ultra"" preset and changed nothing else and told you that you can run this game at Native 1440P Ultra on a midrange 7800XT card and get 60+FPS, would that be ""optimized"" now?",Neutral
Intel,speaking facts my guy,Neutral
Intel,You shouldn't have downvote dood wtf redditors ?,Negative
Intel,People with AMD cards dislike upscaling more because FSR sucks ass lol.,Negative
Intel,"Uhm, me btw...",Neutral
Intel,"> Who actually plays games at native these days, if it has upscaling?  I do.",Neutral
Intel,"I won't judge your preferences, but at least using native rendenring isnt the clown show we have here. Native 4k look the same no matter what gpu render it (performances aside). Now with all the quality preset, we can't figure what quality to expect. 4k60fps at balenced preset? What's that crap even supposed to mean as thoses presets vary from title to title, from NVIDIA to amd. And btw, I'm still using fxaa over taa and fsr when i can, don't ask me why.",Negative
Intel,"You're even seeing this on console, where Spiderman 2 now has RT reflections in every mode. Even Unreal 5's Lumen renderer is a software raytracer that can be hardware accelerated.  RT is the future because there are scenarios that rasterized rendering is just not capable of resolving. Screen space reflection artifacts, light leaking, weird glow on indirectly lit models... I find these so distracting and immersion breaking.",Neutral
Intel,onto absolutely nothing.,Negative
Intel,"Wait what? No performance improvement this gen? Bullshit, RTX 4090 is literally 2 times faster than RTX 3080 and RTX 3090 is only 10% Faster than the 3080  you do the math. RTX 4080 is 50% Faster than RTX 3080.",Negative
Intel,Or the game performs at 50 fps at 1080p low on 5700 but 50fps is a weird number to target and they can't say 60 so they just say 30fps and call it a day.,Negative
Intel,Not if it's using mesh shaders like Alan wake. In that game the GTX 1650 outperforms the 1080ti.,Neutral
Intel,The game is raytracing only with a ridiculous ammount of foooliage.,Neutral
Intel,"I'm so glad this is finally getting more main stream spotlight.  I do a little bit in Dota 2 and the amount of people that simply have a terrible experience because of a single stick of ram in that title is staggering.  Sure, it's an outlier, but a lot of esports/CPU bound games see similar issues.",Negative
Intel,i wonder when dual rank and dual channel will start being listed too. since dual rank does help gaming,Neutral
Intel,Honestly my 1070 still runs great for what it is. It’s in my sons rig now because it couldn’t keep up with 1440p depending on the game and definitely not 4k. Still a fairly solid 1080p card.,Positive
Intel,False.  guy blocked me lmao,Negative
Intel,"I thought that was on Linux, though I might be wrong",Neutral
Intel,1070 doesn't have hardware RT though.,Neutral
Intel,According to the technical director there's no non-raytracing mode. I'm assuming that means software ray tracing for the minimum spec.  Source: Skill Up video - https://youtu.be/v91y87R5iDk?t=332 (5m32s),Neutral
Intel,Spoken like someone who has never upscaled 1080p content on a 1440 screen. The pixel ratios are all off. It's fucked.  1440p is perfect for watching old 720p HD content though.,Negative
Intel,Completely agree.,Neutral
Intel,Damn my brain must have been on auto pilot. I'm so used to 2160p + FSR quality neatly equalling 1440p.,Negative
Intel,I see the 5700 and the GTX 1070 on the minimum specs. Are those cards going to be RT capable for this game? (It's 30FPS LOW with FSR but still.),Neutral
Intel,"Sometimes the DigitalFoundry reviewers use their meme powers for good (eg:fighting shader compilation stutter), but their  >""it's better than native!""   memeing about upscaling seems to have gone sideways.",Negative
Intel,"Dude tells truth, go to Nvidia sub, and you fond every second post someone telling that DLSS looks better then native, and every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote.",Negative
Intel,It's purely on the devs for targeting low performance.   We could be getting 240fps at 4K using those upscaling techniques.,Negative
Intel,"Derp, derp.",Neutral
Intel,Sounds like John on Direct Foundry Direct every week.,Neutral
Intel,"""Avatar will be a title that will only appear with ray tracing. But we're developing the game so that the quality and performance will be scalable.""  Seems I was right.",Positive
Intel,">I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.  Facepalm...  Avatar Frontiers of Pandora very literally uses a custom engine, not Unreal Engine.",Neutral
Intel,Yep. This company learned something from all the failure game launches that happened this year. The game companies can't just implement one gpu company's tech and not the others bc then it'll only work on the one gpu that you implemented the tech for and it won't work on the other gpus from other companies. Meaning for example if a game company only implemented fsr it's only going to work on amd gpus and not on Nvidia and Intel gpus and vise versa is true too for both Nvidia and Intel gpus for dlss and xess.,Negative
Intel,Avatar is RT only. There is no non RT mode.,Neutral
Intel,This is mad with snowdrop engine. It's Ubisoft they don't use unreal...,Negative
Intel,"Yep, I missed that. Good call! Makes sense why the specs are a bit higher then but if it is forced on, that's going to be rough on any one with a more mid tier card.",Neutral
Intel,">  Consoles use mostly FSR balanced  heck sometimes they use performance or less, heck a lot of the time even. Especially with RT or performance modes",Neutral
Intel,They have really long way to go. Take for example Lords of the Fallen - UE5's TSR is better than FSR. Take Alan Wake 2 and you get ton of shimmering on the edges. It's also not like nvidia not improving - so while AMD makes a step forward so will do nvidia..   There's also question how much they improve without dedicated accelerators that both Intel and nvidia is using. Intel jumped late to the party and they're already ahead - which just shows how much AMD cares. It's rally embarrassing considering both XBOX and PlayStation are also relying on FSR - ant that's bigger market than what nvidia has.,Neutral
Intel,I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?  I mean the facts are the facts. I need to play on lower settings till we get a 5070 or 8800xt.,Neutral
Intel,"It uses their own in-house engine, Snowdrop. The last game this developer did was The Division II which was excellent.",Positive
Intel,"Took a look at some gameplay on YouTube, and it does look nice enough, though water looked like ass, and as mentioned already, it's an in-house engine (which is nice to see TBH).",Positive
Intel,You will never see a high production game from Ubisoft ever on the Steam store ever again at initial launch.  They are the anti-christ of the gaming industry seriously.,Negative
Intel,Consoles are the baseline in most multiplat games and they've been using upscaling since forever. The real reason is just that we don't have GPUs anymore that are 10x faster than consoles like last gen. You can't bruteforce high resolutions in games that run at like 720p on consoles with GPUs that are only 2-3x faster.  If upscaling didn't exist on PC the minimum here would just be 720p at the minimum and 1080p-1440p at the max.,Negative
Intel,Next-gen doesn't mean that current gen can run it well. It means it's so far ahead that most curren-gen hardware can't run it properly + top-end hardware can only barely run it well.,Negative
Intel,"Exactly. These paid exclusive games launches only make me care less whenever they actually release.  It's not just Epic, Meta has done it with several games (Moss Book 2) and when a proper looking pcvr port comes a year later it sells bad because people forgot about it.",Negative
Intel,"Is that with significant RT? I assumed it was ""normal"" ultra settings.  Still I can play Far Cry 6 and RE4 with AMD sponsored RT at 4k native with 100fps. This game will have to look amazing to justify what... a quarter of that performance?",Neutral
Intel,"You are correct. I overlooked the part where, under Ultra settings, it said that was with FSR2. Now it doesn't seem as bad. I mean, it's still not good because it's rendering somewhere between 1080p and 1440p, but at least it's not 30 fps without Frame Gen/60 fps with it like I thought.",Neutral
Intel,"It might be, AFAIK neither Nvidia nor AMD still support their implementation of it in their drivers.  But I've tried it on a few games and it is pretty cool to me, much stronger than almost any modern 3D movie (IMO why 3D has gone into hibernation), and I gather that Cameron designed the franchise around 3D.  Plus the Avatar 1 game was in 3D.  FWIW.",Positive
Intel,This is what I am thinking too.,Negative
Intel,"You say that, but they specifically mentioned FSR3, and just DLSS, so i have a feeling DLSS3 will have to go via a mod again.",Neutral
Intel,"i have a believe that even 4C8T will survive for the decade, like the case of that 4790K. But turns out these newer UE5 games are wrecking even newish CPU like the 3600. i mean cmon it's still a 4 years CPU old which i think it's not old yet",Neutral
Intel,Let's hope so. 30fps is far from recommended for FSR 3,Positive
Intel,Yeah I heard it works with that hoping it works with fsr3 now,Positive
Intel,"I've noticed that for some reason newer games look blurry at 1080 even without any kind of upscaling and I'm not sure why, older game look much sharper at 1080p",Negative
Intel,"Just lazy optimisation, that’s pretty much a PS5 equivalent GPU and you can be damn sure that the console version won’t be 1080P 30fps with FSR",Negative
Intel,"No it says 5700. NOT 5700XT, and yes there is a difference. Quite literally the difference between a 1070 and 1080ti. Or a 2070 and 2080.",Neutral
Intel,no no my 7 year old card needs to be able to play at high settings. so does my low end card from today. gotta be high or its just bad,Negative
Intel,Crazy expectation? A game released in 2023 should be able to run at 1080p low 30fps on a low end card like rx6500xt (lowest end modern card you can buy now).  Optimization matters.,Neutral
Intel,"Yup, these cards came out around the time of ps4 I think..that's a long time ago in the graphical quality of games we are seeing.  People should be happy to a point that those cards can even run some of these games if upscaling helps them run that's a bonus as it means they don't need to upgrade if those graphics settings are acceptable to them.",Positive
Intel,"I have a 5700 and it does better at 1080p on these games than what is normally posted. Now it does need quality fsr but its not the worst thing, anything lower than quality is. Bottom line is these are not 100% correct and never have been.  I have zero interest in this game but the specs seem fine even though I wish upscaling wasn't what was relied on for optimization.",Neutral
Intel,"well you dont know what they mean by 30 fps. It could be de 1% lows or the average, we dont know. The easy assumption is that it will be 30 fps average. but that's crazy that a game at 1080p low cant be run by a 5700. It better be a new Crysis.  edit: apparently it uses raytracing only for lighting which would explain the hows and why it crushes the framerate so bad especially on a gpu that doesnt have hardware raytracing like the 5700.",Negative
Intel,"Or in the case of games like Alan Wake 2, you will get unplayable fps if you try to running it at 4k native max settings. Even with a 4090, you will get a glorious 20fps.",Negative
Intel,Assassins creed origins and odyssey side quests/collectibles oh my god,Negative
Intel,Yeah it doesn't give us a actual gauge to understand what we need for set native parameters. I mentioned optimization out of concern that they rely too much on upscaling and it seems like it's the lazy way out of something. It's unrealistic that a usable gaming experience be in a top tier hardware setup that costs $2k+ and top tier hardware of just the last generation are just barely cutting it anymore.,Negative
Intel,I appreciate your insights and opinions. Thank you.,Positive
Intel,">UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  Because they market how ""good"" it looks, not how ""well"" it runs. There's a big influence from marketing perspective to place high focus on ""eye candy"", it's what sells.  The how ""well"" it runs is usually restricted to the small % of us gaming nerds. ![gif](emote|free_emotes_pack|flip_out)",Negative
Intel,console games started upscaling way before PCs .,Negative
Intel,This sub is rife with crazy conspiracy theories. I don't think it was this insane a couple years back.,Negative
Intel,you forgot who owns one of the most popular engines out there?,Neutral
Intel,"I guess technically it would count but then everyone would flame it for looking bad. But seriouly, what happened when Ultra settings ran real smooth on top of the line hardware? Gone those times.",Negative
Intel,downvoted by devs lol,Neutral
Intel,> RTX 4080 is 50% Faster than RTX 3080.  The msrp is 71% higher too.,Neutral
Intel,"Yeah at 1200 bucks and above you got one, that's true but many cards below didn't offer much.",Neutral
Intel,that's stoopid but it's somewhat more understandable. Why a 5700 would struggle so much since it doesnt have hardware raytracing,Negative
Intel,Yep it's like a 10/15% performance uplift in some games but I'd guess they would prefer to keep the requirements sheets somewhat simple.,Neutral
Intel,Why is there a dialog message about unsupported hardware when you try and run a 390X?,Negative
Intel,It can do software based RT just like every other modern GPU out there.,Neutral
Intel,Well then no wonder rx 5700 can't manage 30 fps lol,Neutral
Intel,"Wait so we are finally getting games made only with ray tracing in mind? This is actually great, 2023 is officially the year the new generation started",Positive
Intel,Avatars uses a Lumen like software RT solution.,Neutral
Intel,"yeah they never finished the sentence for some reason. The full sentences is ""its better than native, when native has forced taa!"" which is why dlaa/fsraa at native look so much better than the upscale versions. normal basic TAA is just so bad, that even upscales not using it look better than native using it",Negative
Intel,"The AA side of things might be better, but I swear no matter the resolution textures become a blurry mess even with Quality mode etc, and it irks me that most people don't seem to notice that.",Negative
Intel,Some people seem to have very low sensitivity to added blur. See also *depth of field blur* and *motion blur* being on by default in every game. It's funny how much people rage against FXAA/DoF/Motion blur but ignore DLSS.  Of course only one is tied to someone's purchasing decision :),Negative
Intel,"Yeah, thatsl happened.",Neutral
Intel,">every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote  Well, yes, you are using shitty FSR which is literally unplayable no matter resolution instead of godly DLSS that is literally better than native even at 720p/performance. The only way you could have seen the ever unmatched DLSS for PC kings instead of the plebean FSR is through a compressed youtube video so your opinion is literally invalid. I hope I am being sarcastic enough.",Negative
Intel,"The problem is some (if not most) games have such a bad TAA it might as well be true, and if often cannot but turned off.",Negative
Intel,"""We could"" but not profitable. Just let imagine these games does not exist in our reality.",Negative
Intel,You didn't understand. It's another Swiss knife engine,Negative
Intel,"It is indeed not a great situation. Things like this shouldn't be forced on, and it leaves me feeling like they haven't given the devs enough time to optimize the settings.",Negative
Intel,It works because most consoles players are noobs they don't care about the tech. They just want to sit on the couch and play. I'm hoping we see them start working more on FSR3 and upgrading older games to it. And you are right nvidia will keep moving forward but so will AMD. They may never catch them since they have a head start.,Positive
Intel,AMD just released drivers for Alan Wake 2 that fixed the flickering while using FSR.,Neutral
Intel,The PS5 has a 6700.,Neutral
Intel,">I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?   Not exactly. It's a custom quasi-RDNA2 design (has the improved RDNA2 compute units, but it's missing a few features such as fully-DX12-compliant mesh shaders and VRS-capable ROPs).  For most purposes, it's pretty comparable to a 6700 (non-XT), with the same number of CUs (36), and similar clockspeeds.",Neutral
Intel,"Oh, wow. I didn't know that. But it'll likely be on a similar level as UE5, right?",Positive
Intel,Next gen DOES NOT MEAN that your CURRENT GEN hardware can run it well for sure... Why don't people get that?,Negative
Intel,That reminds me to grab moss 1 and 2. I just got a quest 3.   Agreed though. I’ve been waiting for kingdom hearts 3 to come to steam for so long that I’m basically going to skip it at this point. It’s still stuck on the epic game store.,Neutral
Intel,Far cry 6 and re4 are older and or lower demanding games.  And yes any Mode here does either Hardware or Software rt from. Min to max.,Neutral
Intel,It's a 4 year old cpu yes... But how it was built (4c) was like 10 12 years now old.  6 cores are since a few years already base and more and more 8 cores are standard.,Neutral
Intel,TAA,Neutral
Intel,Temporal anti aliasing.,Neutral
Intel,"TAA is the devil. it forces a blur on every frame, and only gets worse as the resolution goes lower. its why fsr, xess, and dlss can look better than """"native"""" because it doesnt use the horrid plain taa solution so many games do. and why dlaa, and fsraa are so important and need to replace taa completely",Negative
Intel,"Ahh, welcome to r/FuckTAA",Neutral
Intel,Even 4K looks blurry with some implementations of TAA,Negative
Intel,they're giving you the bare minimum until your upgrade!,Neutral
Intel,Because they are built with upscaling in mind so they have a blurry looks intentionally as it works best with aggressive upscaling.,Neutral
Intel,"Not even close, you're completely out of your mind. The 5700xt is 9% faster than the 5700. The 1080ti is **32%** faster than a 1070 and the 2080 is **19%** faster than the 2070.   Source from techpowerup: [https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339](https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339)",Neutral
Intel,"Well the difference is only 256 shaders more on the 5700xt. The 5700xt has 2560 shaders and the 5700 2304. That's 11% more. That's not that much.  The 1080ti has 86% more cuda cores than the 1070, as well as more and faster memory. I would not compare now so haha",Neutral
Intel,"The 5700 and 5700xt is more like a 1070 and 1070ti situation. It's not that big a gap, and you can close it even tighter because the majority of RX 5700 cards can be flashed to a 5700xt bios tightening the gap farther.",Negative
Intel,"The 1070 was released in 2016. Its already the end of 2023. That card belong to ps4 era. Wake up.   People in this sub are delusional, really.",Neutral
Intel,Those are some dumbass expectations. Maybe you should correct them instead of blaming the developers for it.,Neutral
Intel,"The 6500XT offers $200 2016 performance. In fact its often worse because of how gimped it is interface wise. Its one of the worst products released in recent years.  Its not the game developers fault AMD pretty much rebadged the RX 480 4 times and they can't be expected to sink their optimization to that level.  RX 480, RX580, RX 5500XT and RX6500XT are virtualy the same product performance wise. RX 5500XT is the most consistent one of the lot offering a somewhat more modern arhitecture in its 8GB variant. The 6500XT on the other hand while on a newer arhitecture is an unmitigated disaster that is on par at best and sinks even below the RX580 when memory, bus and PCIE interface constrained.  The console baseline is around 6700XT/2070 Super/3060ti level. People better get used to that as its gonna become or already has become the norm for 1080p high settings gaming. The 5700 is upwards of 30% slower by default while also having less hardware features. Its gonna become the minimum norm going forward alongside the 2060's of your time.",Negative
Intel,"the 1070 was 2016, the 5700 was 2019. While both not new its not 10 years old like the PS4 (2013)",Neutral
Intel,This is literally only true if you use path tracing. I think you're trying to insinuate the game is unoptimized when it just scales really high with visual features if your hardware supports it. It straight up runs great for the visual features it uses.,Neutral
Intel,"That was exclusive to console releases, no? And never to the point of replacing AA sharpening and rendering the game from 720p…",Negative
Intel,No I havent. AMD is bigger than Epic.,Negative
Intel,"Right, you'll have different complaints but complaints none the less even though the new Ultra might still look slightly better prior gen games and offer high FPS but people would complain that the graphics are not advancing fast enough, they look the same etc etc.  Yah those days are likely gone but it's hard to say if it's because the game is unoptimized or if the dev is really pushing the limits of what they can do graphically and use upscaling as a buffer because what they have in mind, or what they intend to show is too taxing for current hardware. We won't really know until a game is released  So screaming game is ""unoptimized game, lazy devs"" seems to have just become the norm before anyone's even played it. This is why I've never in my life ever pre-ordered a game. I still don't understand why people do it (esp now that games are digital) but lot of people do. Then again I've never really paid for a ""skin"" or some other visual add-on so maybe that's what attracts people.  I'm not saying games aren't unoptimized, those do exist, they offer nothing more but run like crap. We know what those game are, it's not hard to tell. But then you do have games like AW2, which before release, we were hearing the same exact complaints and it looks amazing and the minimum requirements were on the conservative side. The game looks better at ""medium"" than high/ultra presets of some games, heck you can even argue that at some low settings it looks better than some games at medium/high presets. The presets are all relative anyways. There's no universal settings that define what each preset represents.  I just don't get the outrage on these minimum spec sheets compared to what the game is showing graphically. At least wait until it releases and we have some hard data.",Negative
Intel,"I'm also a dev dood sometimes it's literally a ""foutage de geugle"" I had a friend who talk about how shit devs in EA (the people who's making the engine) code and doesn't optimise",Negative
Intel,"The commenter I replied to said there wasn't a big generational improvement for GPUs, while I said there was. But I agree that 4080 is expensive for what it offers.",Neutral
Intel,It was the same price as a 3090 at launch and offers 90% more performance.  It was definitely a generational leap.,Positive
Intel,It's software ray tracing which isn't accelerated by hardware.,Neutral
Intel,and i guess it can be hard to tell if ram is dual or single rank with just two sticks of it too.,Neutral
Intel,>might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.  I was pointing out that 1070 doesn't have hardware RT either.,Negative
Intel,So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?,Negative
Intel,"Why not make it toggleable though, you can choose other graphics settings so it makes no sense. Most people don't play games with RT enabled",Negative
Intel,Still kinda weird that they put the RX 5700/A750 on the same ballpark as the 1070... Being that the newer GPUs actually perform closer to a 1080(Ti).,Neutral
Intel,Sounds like improperly configured negative LOD bias. If you have Nvidia you can fix it with nvinspector. On AMD there's nothing you can do IIRC if Devs mess it up.,Negative
Intel,"Exactly what I'm telling them all the time, that I can see difference in textures, in hair, fences, powerlines etc. And every time I get downvoted by the army of ""but it looks better then native"". I feel like majority of that sub needs to go get a prescription for new glasses",Negative
Intel,"So if you stopped measuring who's bigger, why do you think I have no access to DLSS? Tested on rig with 4080 on a C2 42"" screen. Once you stop thinking with the tip of your head where probably Nvidia's logo is tattooed or maybe Jensen's face, who knows, you could come to realization that one person have access to more hardware then only his personal rig.  Do I say that FSR is better then DLSS? No! Both of them have their issues, but none of them is better then native. Keep on fanboying mate, instead of providing normal convoy as grownups",Neutral
Intel,I don't think YOU understand what you're talking about.  Did you seriously expect Snowdrop to be something used in like one game and then discarded completely?,Negative
Intel,"The thing is: creating good looking lightning with RT is much easier than with prebaked illumination. I can fully understand, why developers of new games are using RT as the basis of their game.",Positive
Intel,Minimum requirements go up as games get/look better. They probably made this game with the series s as the minimum. Low settings probably looks better than high in most games.   Rt for the last few years has been added on top of already complete games. Now games are being made with rt as the standard and they would have to manually do a ton of extra work to support 7+ year old hardware. It's the same as having to drop support for last gen consoles to be able to deliver a better game.,Neutral
Intel,"And still - superior upscaling makes it worth paying extra, RT on higher end is worth paying extra. AMD even this gen doesn't undercut price by enough, especially in EU where power efficiency also play a role. For example, assuming I'm gonna use GPU for 2 gens, then I save ~90-100€ with my usage just on electricity alone. I know it's over period of 4 years, but still it's price difference that will recoup over time, even if I have to pay extra upfront. AMD is simply no longer a value proposition.",Neutral
Intel,"They did not, lol - that fixes completely different thing, not the FSR caused shimmering edges.",Negative
Intel,Both excellent 👌 Down the Rabbit Hole was another solid one.,Positive
Intel,"RE4 came out this year.  Anyway the difference between us is I don't think any lighting effect is worth a $1,000 GPU using 1080p resolution and 60fps, but I know many disagree. That's life.",Neutral
Intel,"3600 and the 3300x was built with the TSMC 7nm if i recall, that was 2018 cutting edge tech. and somehow in gaming it's already obsolete. i was planning to replace mine with 5700x  but i cancel it because it's still very much capable for what i do now. but now after UE5 i think i'll reconsider it again, maybe even as far as 5800X3D",Neutral
Intel,It is crazy that I see everyone saying that modern games look blurry. I've known for years taa does this but it seems like I see more people than ever dissatisfied by it.,Negative
Intel,I have to turn it off in borderlands 3. Ugh.,Neutral
Intel,r/FuckTAA,Neutral
Intel,"My friend nicknamed it after his eyesight. When he wears glasses he has 20/20, but when he doesn’t he says that he sees the image about the same as without glasses",Neutral
Intel,"Out of curiosity what resolution do you usually game at and at what distance? I find taa usually looks pretty good in modern games, but I'm also playing on a TV (48 inches but I'm not at monitor distance anymore). When I played on my 1920x1200 monitor I HATED TAA, well, not as much as fxaa, but moreso than smaa, and I missed msaa or SGSSAA with a passion. Now on the TV at 4k (lol, usually not on a 6700xt), 1440p, or even 1080p I find TAA can do a pretty decent job paired with varying levels of the Radeon sharpness adjustment in the adrenaline drivers. But again, probably due to viewing distance.  Looks leagues better than FXAA and I prefer it to SMAA now. But again, I'm sitting further away and tweaking sharpening to my liking. FSR2 seems to be a wash most of the time but the performance gain is worth it.",Positive
Intel,You dumbass do understand that even the console version of the recent games are total mess? Alan wake 2 have to run on reduced settings to even reach 30fps on xbox/PS. Callisto protocol used to have frametime issue on consoles. Console ports still targets 30fps on quality settings with upscaling while target should be 60fps with 120fps for performance mode.,Negative
Intel,"Yes, seems you are correct, it was the ps4 pro that came out in 2016, still, I wouldn't expect that to be capable of running some of the games we are seeing now without some serious compromises.",Neutral
Intel,I never said that Alan Wake 2 is unoptimised. I just said that even a 4090 can’t max out the game at native 4K resolution.,Negative
Intel,epic is tencent...,Neutral
Intel,"In terms of absolute numbers sure, 3090 always had a stupid price tho for being 10% faster than a 3080.   In terms of performance per dollar the 4090 even at 2x 3080 performance was worse value since you pay 1600 not 1400.",Negative
Intel,"Yes, you either have to get the spec sheet of the RAM modules (That even then it may not specify if it's a single/dual rank module) or test it in a system and use HWinfo/CPU-Z. Not as straightforward as dual channel that's mostly just using two identical modules.",Neutral
Intel,You don't need RT hardware to do software RT. That's what I'm saying.,Neutral
Intel,"For this particular game, there is a software fallback. Also, even if we did get a game that mandated hardware support for RT they'd still likely build it with console in mind so even RDNA2 GPUs shouldn't have issues",Neutral
Intel,"The first RT/DLSS capable cards are 5 years old now, way older than the current consoles. You are definitely not being left behind if you're only outdated by 1 gen lol",Neutral
Intel,I mean the last non ray tracing cards were released 4 years ago (1650/60 and 5000s from AMD),Neutral
Intel,Games running poorly on computers weaker than current gen consoles isn't exactly anything new.,Neutral
Intel,"No, you have multiple generations of games from prior to raytracing to enjoy.  Games shouldn't be held back forever just because someone wants to game on a GeForce2 MX400.",Neutral
Intel,You are stuck playing older games. Unless someone can find a way to disable it with a mod.,Neutral
Intel,> So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?  A gen old is second gen RT e.g. rtx 3000.  We're halfway through the third RT/matrix generation right now.,Negative
Intel,Yes hah,Neutral
Intel,"The 3090 or 6950xt will very likely run this game extremely well, they're a gen old.",Positive
Intel,"For the AMD GPU, it's probably because minimum will be the max it will support. Medium probably requires Hardware RT.  For Intel, I don’t know. Perhaps it signal not so great optimization for Intel GPUs...",Neutral
Intel,DLSS is very strong in thin lines like fences & hair. Much better than most TAA solutions. Not sure about FSR in general.  An example: https://pbs.twimg.com/media/E6erA6BVkAAuf_M?format=jpg&name=large  Only DLSS resolves the lines properly.,Positive
Intel,"I mean, it's not that black and white. Quality upscaling to 1080 will obviously be more noticeable than 4k. Maybe you *can* tell the difference between 4k native and dlss quality, but people aren't gonna believe you unless you prove it with some kind of blind test, but I doubt you'd be willing to go out of your way to do that.  Even if you could, the benefits of better aa might outweight some negligible drop in textures or whatever it is upscaling reduces. Though that's irrelevant when DLAA or NAA are options too, I guess.",Neutral
Intel,"Test, which one looks better, explain.  https://imgsli.com/MjE3MzEy",Neutral
Intel,Oh man the hair... its so crazy how much upscaling kills the hair..,Negative
Intel,i mean they already arent the smartest bulbs considering they went team green.,Neutral
Intel,Reading comrehension dude.,Neutral
Intel,"I do, but thanks for your interest.",Positive
Intel,That will depend on the user. Power is cheap here in canada so none of thar factors into my buying decisions. And rt will depends on the games you play also not everyone plays tripple a games.,Neutral
Intel,"You need to get 23.20.17.05.  Fixed Issues Intermittent flicker may be observed on some textures while playing Alan Wake 2.  Fixed for most people, also has a nice performance boost of about 10fps from what I'm seeing.  That being said I always prefer native resolution unless absolutely necessary. So far only had to use it in Martha is Dead.",Positive
Intel,Nice. It’s on the list. Thanks man,Positive
Intel,Yes.... 7nm was but 4 cores the actual important thing was obsolete like 10 or 12 years ago.,Neutral
Intel,Probably because TAA is (unfortunately) more prevalent than it ever was.,Negative
Intel,"Nah, most people are FINE with it. Its people on FuckTAA that are mad.  The thing is, you have sharpening filters today. You can adjust the sharpening for TAA. You can make it look oversharpened if you want. There's driver level sharpening for both AMD and NVIDIA. Both also have software sharpening filters.  And DLSS and FSR bot have sharpening sliders.  There's 3 ways to adjust how blurry and how sharp the game looks for everyone. It's a non issue and even Digital Foundry acknowledges the benefits of TAA for game devs vs things like the blur. At the end of the day, these games are looking great...in this era, with exceptions. Not all games are blur. The whole blur thing was like in the past 4 years since upscalers started and TAA became the norm. But with all the sharpening options, and with devs actually used to making TAA not so soft looking, things are definitely getting a lot better for TAA.",Neutral
Intel,Oh my bad if I read that wrong,Negative
Intel,And I'm saying 1070 doesn't have hardware RT so it's just as gimped as AMD cards when doing RT.,Negative
Intel,Still pretty annoying nonetheless. Until it's stable no way should it be forced upon a game when someone's willing to run it normally at native.,Negative
Intel,"and the 2080ti is still winning, best purchase in gpu i made in a long time.",Positive
Intel,imagine being mad that 4 year old cards arent high end,Neutral
Intel,I'm on a 6700XT. 🙇. I just feel very shocked that I would probably need a gpu upgrade soon. With the demands of new games. Also 1440p ruined me.,Negative
Intel,"Technically makes sense, but they can just make RT something you can toggle between Software/Hardware and they also could just add the Vega 56 in the requirement sheet being that was AMD's competitor to the 1070 or they could've added the GTX 1080(Ti)/RTX 2060(Super) into the sheet.  On Intel's side they only have the A580 as the closest GPU to a 1070, even when it's too similar to the A750 performance wise :/",Neutral
Intel,"It is that black and white for me. I can immediately see the added image softness from upscaling, and it's especially prevalent with DLSS 2.5+ unless you enable a sharpening filter. DLAA also has weird issues like blurring textures (almost like it reverts to initial mipmap) while moving, usually at mid-distance from camera. It's too distracting and I have to go back to regular TAA. Immediately goes away.  1440p to 2160p is better than 720p to 1080p, but it's still noticeable. Detail is immediately lost as lower resolution image is scaled to higher resolution. Is it terrible? No. Actually changing monitor resolution outside of native and having all UI elements scaled looks terrible. That was what we used to do and upscaling is much better than that alternative. LCD panels need to stay at native signal resolution and GPU should scale render resolutions.",Negative
Intel,"And you look at static image all the time while gaming? It's different story when movement is involved, that's where you find all the artefacts you get. Static image doesn't tell a thing when it comes to gaming",Neutral
Intel,"Yes, and that's driving me crazy. I really couldn't stand it in Cyberpunk (in other titles it's either hit or miss), that I play it in native 4k instead.",Negative
Intel,"Wouldn't phrase it like that tough. I had that choice too in January, and actually by going 4070Ti instead of 7900XT I would be able to play Cyberpunk in higher fidelity and better frames then on mine, because AMD cooked FSR3.0 almost a year in the oven, and God knows how long it will take for CDPR to add it to the game, if ever considering that active development on game is ended and they are on bug fixing sprint right now. Also AMD needs to fix FSR in general, because how it works with smaller details like hair and fences is awful. But don't get me wrong, I love my card, just feel I git sold on features that would come handy right kow instead of when I don't need them anymore",Neutral
Intel,"Sorry mate, guess I need to read comments early morning instead of late night, just saw that last bit. My apologies. I'm just tired of upscalare wars whatever sub I go, and green team fanboys screaming that ""FSR is shit, DLSS is better then native"" and then throwing around static images for comparison",Negative
Intel,"It's for textures, not object edges from FSR use, lol. Two completely different things",Neutral
Intel,"people echoing the same shit like ""ooh 4C is obsolete, you need to upgrade that geriatric cpu"". 4C is still solid for gaming (ofc no UE5) and literally anything beside doing compute intensive professional workloads.",Negative
Intel,"Sharpening makes the image less blurry, but it doesn't recover the texture detail lost from TAA blur.",Negative
Intel,"""Even Nvidia foundry""   \---   they are like chief proponents of blur-tech and they don't care about how sharp and easy-to-see the image is. TAA flattens the image so you can't see which objects are far and which are near, you have to strain your eyes and mind to determine that which is unbearable to many. Just give us option to disable it even if it breaks some effects (which is just lazy thing to do on the side of devs)",Negative
Intel,"Sharpening fixes blur, not clearity. That is TAA destroys+add significant ghosting no matter the implementation.      Sharpening looks was worse than the normal cleairy of games, it's not the same thing at all.      >Its people on FuckTAA that are mad.  We are mad because it being forced 1 and 2 it's being used to hide lazy effects that break without temporal frame smearing. People are now punished for turning off forced frame smearing.",Negative
Intel,And I'm saying that maybe the GTX 1070 is better at even software RT compared to AMD cards with no RT hardware. GTX 10 series card did get RTX support through CUDA even though they were horribly bad at it. Older AMD cards with no RT hardware didn't get any support like that from AMD.,Neutral
Intel,"You're not forced to use upscaling though. Unless you replied to the wrong person this is about ray tracing, which can be run at native just fine. If your GPU can't handle RT at native res, then just use upscaling.",Neutral
Intel,The minimum requirements for this game includes de 1070: a 7-year-old gpu from 2016.,Neutral
Intel,"I'm not really shocked, and was out there telling people not to buy those cards on day 1 in large part because of their deficiencies in RT performance and complete lack of matrix accelerators (which Nvidia and Intel have both made a large part of their architecture and got shockingly good results with - DLSS and matrix-XeSS are twice as good as FSR)",Negative
Intel,5700 was probably the lowest AMD card they had to test with.,Neutral
Intel,"It also looks better in gameplay, less ghosting, less shimmering, less aliasing.   So which one looks better? You clearly see the difference in textures/fences/powerline, right?  I even made it easier for you, both images are 1080p output.",Positive
Intel,I personally didn't try DLSS even once but FSR 2.0 does have shimmering I notice to various degrees. It was very noticable on fur in Ratchet & Clank cutscenes for example. Overall I am just happy my 5700xt could still keep up with 1440p.,Neutral
Intel,Hmm yeah whatever you want to believe and let's you sleep.  Have a nice day.,Positive
Intel,AMD CAS is aimed to restore detail,Neutral
Intel,"They aren’t chief proponents of anything, TAAU has been popular for well over a decade at this point and it’s the default approach for consoles (checkerboarding has moire and aliasing issues) and digital foundry simply recognizes this is the reality of how the console world has settled on doing graphics.",Neutral
Intel,The problem with Nvidia is they are massively overpriced. One 4080 would probably cost the same as my whole build or more.,Negative
Intel,"Yes I see which one looks better, and as I said before it's a static image. While I didn't use upscaling in this same exact example, I can go by my own experience and tell that it doesn't look better then native let's say in same Cyberpunk or any other game. And I will beleive my own eyes on my own screen then static images tossed around. TBH, in my opinion it all depends on screen and screen size too, as to me that difference between upscaled and native was less noticible on my old 27"" 1440p screen then it is now on 42"" 4k. DLAA tough, that's another story",Neutral
Intel,"I tried both, and neither of them are better then native as people tend to claim. In my testing FSR have big problem with smaller details like hair, fur, fences, powerlines and as you mentioned, you get severe shimmering effect. I really hope AMD will keep on working on it, an try to get away from that. But DLSS also adds shimmer, it's just less visible then in FSR (atleast in my testing on Cyberpunk pre 2.0), and whatever upscaling method I've used I also could see that image is upscaled (lower res textures here and there etc.). I like idea being upscaling tech, it helps prolong life of GPUs, and for more powerful GPUs to run higher settings with more fidelity, but there's still work to do on them as they aren't perfect yet.",Negative
Intel,Yeah but a 3060ti didn't,Neutral
Intel,"In Cyberpunk you are not comparing upscaling vs no upscaling at the same settings though. You are comparing low RT native vs path traced upscaling and at 4K, the decission there is absolutely clear. On my 4k OLED screen, upscaled PT is so clearly the way to go, there is no competition.",Neutral
Intel,"The gameplay looks exactly like the image + your typical TAA/DLSS smearing. And as I said, in my example DLSS also looks better in gameplay, because as you see... native has a ton of aliasing which is super noticeable while playing (shimmering). There are so many objective tests, which show why its most of the time better than native. (up res to 80% instead of 67% and its always way better, even at 1080p)   Well when did you try DLSS the last time? Newest versions are miles ahead of the ""old"" 2.0 version and pretty much always ahead of standard TAA.  DLSS is essentially a way better TAA solution + AI to fight TAAs issues + lower res. Its so much better than your standard TAA, that youre able to reduce the res and it will still look better in movement. Think about it like codecs for streaming... there are codecs which need a way lower bitrate for the same quality than others. (av1)  Everything graphics related is not rendered at full res anyway (shadows/lighting/lod etc...), there is no line between native and upscaling. Talking about native is nonesense, just image quality counts and thats were DLSS or even FSR 2 (some games) gets better and better. There is a reason why consoles use it... it looks better than without it.",Positive
Intel,Confirmed Can it run CP2077 4k is the new Can it run Crysis,Neutral
Intel,It'll be crazy seeing this chart in 5-10 years with new gpu's pushing 60-120 fps with no problem.,Positive
Intel,In 1440p with 7900 xtx with fsr 2.1 quality it doesn’t even get 30fps,Negative
Intel,"Makes sense. Is almost full path tracing, it’s insane it’s even running.",Positive
Intel,good. this is how gaming should be. something to go back to before the next crysis shows its teeth,Positive
Intel,3080 falling behind a 3060? what is this data?,Negative
Intel,wake me up when we have a card that can run this at 40 without needing its own psu.,Neutral
Intel,5090 here I come!,Neutral
Intel,"Playing the game maxed out with path tracing, FG, RR and DLSS set to balanced at 1440p. Over 100fps 90% of the time. Incredible experience.  *With a 4070 ti and 13600k",Positive
Intel,"I mean, Path tracing is to Ray tracing, what Ray tracing is too rasterization.",Neutral
Intel,When Cyberpunk first came out the 3090 only got 20 fps in RT Psycho mode.  https://tpucdn.com/review/nvidia-geforce-rtx-4090-founders-edition/images/cyberpunk-2077-rt-3840-2160.png  Still does  Fast forward just one gem and you don't see anyone saying its demanding with many able to get RT Psycho on thier cards as new cards got faster.  Give it 2 gens and you are going to get 4k60 here.  Gpu will improve and get faster.,Neutral
Intel,"Look at that, my 6700XT is just 19fps slower than the RTX 4090 in this title.",Negative
Intel,It's a good thing nobody has to actually play it native.,Neutral
Intel,Well good thing literally nobody is doing that…,Negative
Intel,The future is not in native resolution so this is really pointless information.  Cyberpunk looks absolutelt mindblowlingy insane with all the extra graphical bells and whistles it has gotten over the years and with Nvidia’s technology it runs so damn smooth as well.,Positive
Intel,4.3fps lol. No amount of upscaling is going to fix that and make it playable. People were saying the 7900xtx had 3090ti levels of RT when it launched. A 4060 ti is 50% faster then it.,Negative
Intel,That's actually pretty amazing for any GPU to get a metric in frames per second instead of minuites per frame.,Positive
Intel,"Meh I get like 90+ fps with everything cranked with RR, FG and DLSS(Balanced) toggled on with my 4090 @ 4k.  Path tracing does introduce ghosting which is annoying buts not really noticeable most times but at the same time with RR enabled it removes shimmering on 99% of objects that is normally introduced with DLSS so I am willing to compromise.   Honestly as someone who used to have a 7900XTX I am disappointed with AMD its clear that AI tech in gaming is the way forward and they just seem so far behind Nvidia now and even Intel(going by some preview stuff). FSR is just not even comparable anymore.",Negative
Intel,Who cares when it looks worse than with dlss and ray reconstruction on top of running a lot worse? Native res 4k is pointless.,Negative
Intel,"Well the upscaling in this game is really good, DLSS with ray reconstructions AI accelerated denoiser provides better RT effects than the game at native with its native Denoiser.  Also Path tracing scales perfectly with resolution so upscaling provides massive gains. using DLSS quality doubles performance to 40fps, and dlss balanced gives 60fps on average, performance about 70-80 or 4x native 4K, that includes the 10-15% performance gain RR gives as well over the native denoiser as well.  I've been playing with about 60fps on average 50-70. with DLSS Balanced and RR and its been amazing. I don't like frame gen tho since it causes VRR flickers on my screen",Positive
Intel,"Running Ray Tracing or Path Tracing without DLSS or Ray Reconstruction is like intentionally going into a battlefield without any gear whatsoever, it's absolutely pointless and suicidal, what we can clearly see here though is top of the line 7900 XTX losing against already mediocre mid-range 4060 Ti by over 50%, which is just beyond embarrassing for AMD Radeon.  All this says to me is AMD Radeon need to get their shit together and improve their RT / PT performance, otherwise they will continue to lose more Marketshare on GPU department no matter how hard their fanboys thinks that they are pointless features, just like DLSS was back on 2020 right?  Also, with my 4070 Ti OC i can run it at average of over 60 FPS at 1440p DF optimized settings with DLSS Balanced + Ray Reconstruction without even using DLSS Frame Gen, with it on i can get over 80+ FPS",Negative
Intel,Running this way means you lose RR…why in the world would you run at native 4k?  It’s completely pointless now with RR.,Negative
Intel,Im having 80 fps  thanks to dlss 3.5  and its looking better than ever,Positive
Intel,"Why would I run native? I have amazing DLSS, ray reconstruction for huge image gains and frame gen. Nvidia offering all the goodies.",Positive
Intel,"I have a 13900KF-4090 rig and a 7800X3D-7900XTX rig. They are connected to a C2 OLED and to a Neo G9.  I've been holding on to play the game till 2.0 update. I've tried many times with different ray-tracing options and they all look good and all. But in the end I closed them all, turned back to Ultra Quality without ray-tracing and started playing the game over 120FPS.  This is a good action fps game now. I need high fps with as low latency as possible. So who cares about ray-tracing and path-tracing.  Yeah ray-tracing and path-tracing are good. But we are at least 2-3 generations away for them to become mainstream. When they are easily enabled on mid-range gpus with high-refresh rate monitors, they will be good and usable then :)",Positive
Intel,"If you have the HW, go all out. That's why we spend on these things.  I'm getting the best experience you can get in the premiere visual showcase of a good game.   It's path tracing. It isn't cheap but the fact that we have DLSS and FG with Ray reconstruction is a Godsend. It looks stunning and it's still early in development.",Positive
Intel,This doesn’t seem right…. 3080 performing worse than a 2080TI?,Negative
Intel,How tf is a 2080 ti getting more fps than a 3080?!?,Neutral
Intel,I'm not seeing the RTX A6000 on here...,Neutral
Intel,4 fps for 7900 XTX  Oof. Shows you how much of a gap their is between AMD & Nvidia with pure ray tracing.,Neutral
Intel,"Everything maxed, PT, 1440p, DLSS+RR+FG, input feels good, game looks and runs great, 100+ fps",Positive
Intel,"""Get Nvidia if you want a good ray tracing experience""  Yes Nvidia GPUs give a better ray tracing experience but is it really worth it if you are required to turn on DLSS? Imo, the more AI-upscaling you have to turn on, the worse the Nvidia purchase is.   I have a 6900XT and I will readily admit that the RT experience (ultra RT, no path tracing) at native 1080p resolution is ok, like 30-45 FPS (around 50 on medium RT settings), but if I turn RT lighting off (so RT shadows, sunlight, and reflections are still present) suddenly I get pretty consistent 60 FPS (i left my frames tied to monitor refresh rate, so 60 FPS is my max) and i can't tell the damn difference at native 1080p compared to RT medium or Ultra.   So would I spend another $400-$1000 to get an imperceptible outcome (imperceptible to me that is)? Most definitely not.",Negative
Intel,Does anyone actually play this game? Its more of a meme game imho,Neutral
Intel,4.3 fps 😂😂😂,Neutral
Intel,"RTX 4090 DESTROYS 7900XTX with over 400% increased FPS, coming in at an astounding…. 19.5FPS 😫😂",Neutral
Intel,"Overclocked RTX 4090 can (there are factory OC models that run up to 8% faster than stock). A measly 2.5% overclock would put that at 20 FPS.  Native is irrelevant though, DLSS Quality runs much faster with similar image quality.",Neutral
Intel,[85-115fps (115fps cap) in 4k dlss balanced](https://media.discordapp.net/attachments/347847286824370187/1154937439354368090/image.png) ultra settings,Neutral
Intel,Lol the 3060ti is better than a 7900xtx...crazy,Positive
Intel,Why would you use it without DLSS? Upscaling is the way of the future and AMD better improve their software to compete. Future GPU's aren't going to be able to brute force their way to high frames.,Neutral
Intel,Who cares about native though.  Cyberpunk PT at 4k DLSS Balanced + FG is graphically so far ahead of any other game ever released that it literally doesn't even matter if other games are run at native or even 8K. Cyberpunk is just in league of its own.,Neutral
Intel,ThE gAMe iS PoOrLY OpTImiSed!!!,Neutral
Intel,Who cares about 20 fps native? Use the tools and tricks provided by the game and card manufacturer and you are good. The whole native rendering argument can be thrown in the garbage bin.,Negative
Intel,"The 7900xtx losing NATIVELY to the 4060ti 16gb is embarrassing, that card is half the price and overpriced, nevermind Nvidia's far superior upscalers. AMD falls further and further behind in RT, sad.",Negative
Intel,"99.7fps  with everything maxed out at 4k balanced with frame gen.   64.3fps with everything maxed out at 4k balanced w/out frame gen.   29.4fps with everything maxed out at 4k native.   That’s a big difference from this chart what I’m getting vs what they’re getting. For record, I’m using a 4090 suprim liquid undervolted to 950mv at 2850mhz. Stock is 2830mhz for my card.",Neutral
Intel,"Even with frame gen and DLSS quality, Ultra+Path Tracing looks incredible. I get solid 90 FPS. Funny thing is I can't get into playing CP 2077, even with incredible graphics and a QD-OLED monitor. It's just not my type of game :-D",Positive
Intel,I was wondering why a thread like this with the name like this is in this sub. And the I see the name of the OP. And it all makes sense,Neutral
Intel,"""4090 is 4 times faster than 7900XTX""",Positive
Intel,"So? Native 4k is such a waste of compute hardware, only an idiot would use it.",Negative
Intel,People need to change their mind frame. AI is here to stay and will be a major part of graphics rendering going forward.,Neutral
Intel,I played 2.0 yesterday with 2k ultra and path tracing with 155fps (DLSS3.5),Neutral
Intel,"The 4060 Ti literally better than the 7900 XTX in RT, cope snowflakes!111!11! /s",Positive
Intel,"So Nvidia only needs to increase their RT performance 3x while AMD needs to do 14x. We are not seeing that within a decade, if that.",Neutral
Intel,Sounds like Ray tracing isn’t ready for prime if nothing can run it and you have to use AI smoke and mirrors to fake it playable. I’ll worry about Ray tracing when it’s a toggle on or off like shadows and there is no major performance hit. See you in the 9000-10000 series of Nvidia or 12000-13000 series for AMD.,Negative
Intel,Native resolution is a thing of the past. DLSS looks better than native anyway.,Neutral
Intel,Is this with DLSS + FG?,Neutral
Intel,"Ray tracing already wasn't worth it imo, that's why I saved money buying an AMD card. It's not worth the cut in frame rate that then has to be filled with upscaling that makes frames look blurry anyway, so what's the point of even using it?",Negative
Intel,"Meanwhile me using medium settings + performance dlss on 3080 ti to get that locked 120fps. I just can't go back to 60fps. The game still looks amazing, especially on blackstrobed oled.",Positive
Intel,What is even with with all these path traced reconstruction bullshit lately? Ray tracing itself isn't even that mainstream yet.,Negative
Intel,No one is using these settings.,Negative
Intel,"Ray tracing is fun and all, but games have really improve much.. Same animations.. Same type of quest... Same listening to notes.. There have to be better ways....",Positive
Intel,"4K rendering for games like this is pretty pointless unless you're upscaling it to a massive 8K display via DLSS Performance.  Ultimately, the final image quality is going to be more affected by how clean the Ray / Path Tracing is than whether you're rendering at Native 4K or not.",Negative
Intel,"I'm honestly kind of shocked my 4070 is that far above the 7900XTX. I'd have thought it would be no where near close, regardless of ray tracing.",Negative
Intel,cyberpunk sucks don't worry about it,Negative
Intel,3080ti?,Neutral
Intel,"Yes, that is true, but what the numbers do not tell you is that if you tweak some settings, you are able to get 90 fps on that resolution with path tracing with no problem.",Neutral
Intel,I feel like the 4k resolution is here to stay for a couple years boys. Get yourself a decent 4k monitor or OLED TV and chill.,Neutral
Intel,"This is why DLSS3 and frame generation are so important. 4x the performance at over 80FPS with a relatively minor amount of silicon dedicated to AI, with only minor visual degradation.",Neutral
Intel,"It's way too early for using ray / path tracing in games comfortably, maybe in 10 years it'll be different, but for the foreseeable future I'm more than happy with normal rasterization.",Positive
Intel,Who cares tho? Its not even the way it's meant to be ran. This tech is is a synergy. No point in trying to 'flex' native at this point. A 4090 gets very high FPS and amazing visuals when doing things right.,Negative
Intel,can wait in 2030 with the 8030RTX low profile 30 watt card run this game with full path tracing in 4k 120fps lol (in 2030 a low end card will run this like butter don't down vote me because you didn't get it lol),Positive
Intel,"Seems like this tech is only around to give Nvidia a win. Also path and ray tracing doesn’t always look better. If the one true God card can’t run it, is it reasonable tech or is it a gimmick? Especially seeing how like no one has a 4090. Like a very small percentage of PC gamers have one. Most aren’t even spending $1600 on their whole build!",Negative
Intel,What body part do you think will get me a 5090?,Neutral
Intel,"Once they introduced this dlss upscaling anti-lag all these gimmicks I don't think we've seen true 4K native and almost anything lately.  This is maybe one game that does look that good but many of the games don't look that great and have some really serious system requirements when there are games that look much better and are much better optimized.  Companies are taking shortcuts, instead of optimizing they give you suboptimal and use FSR dlss all these gimmicks.  Don't get me wrong I always thought it was great for people who wanted to reach a certain frames per second someone with a weaker system to allow them to play at least the minimal at a steady frame rate.  It just seems like this is the normal now non-optimized games.  The 4090 is a beast if you could only get about 20 frames native with Ray tracing it tells you everything you need to know about Ray tracing.  It needs to be reworked that needs to be a new system I believe one real engine has it built in in a certain way probably not as good as Ray tracing directly.  To me it seems like ray tracing is hampering many games that tried to add it, too much focus on that instead of actual making a good game.  Let's daisy chain 4 4090s ..gets 60 fps.. and then house goes on fire!  Obviously I'm being sarcastic but realistically it's actually pretty sad a 4090 20 frames per second.",Negative
Intel,"Its not really surprising, raytracing is decades away from being truly mainstream. I applaud the misguided pioneers who betatest the technology, their sacrifice will be honoured",Neutral
Intel,"bro, for who are they making such intensive/badly optimised games? for aliens with more powerful pcs? i trully dont get it.",Negative
Intel,That not even proper path tracing.,Negative
Intel,And I thought that ray tracing was a needlessly huge performance drop for a barely noticeable difference...,Negative
Intel,"Can't wait to buy a $1,600 GPU to run a game at 20fps!!!  &#x200B;  Nvidia fans: i cAnT wAiT 🤑🤑🤑",Neutral
Intel,Native is a thing of the past when dlss produces better looking image,Neutral
Intel,Lmfao ... where are all those nvidia fan boys who prefer eye-candy path trace over 60fps?,Negative
Intel,Sounds like some efficient tech. Lmao. What's the fucking point if it makes the game unplayable? RT just washes out the image and looks like shit. Kills the contrast. Can't wait for this fad to end,Negative
Intel,In my opinion it's too early for Ray / path tracing it's computationally expensive and if you are using rasterization at ultra settings not only does look nearly identical but it's easier to do the work for the graphics and that gives you more fps...,Negative
Intel,"Rather than having 2-3fps, I would go and see a video of path tracing.",Neutral
Intel,"Does this fact really matter tho? Dlss in cyberpunk is known the look on par or even better than native, frame generation will further smooth it out. I can definitely see the 7900xtx being able to do path tracing at 1440p at least once amd release fsr 3 and better once they release something to now match ray reconstruction which also give a small boost in fps making path tracing even more doable.",Positive
Intel,cool idc i wont play that shit at such shit settings,Negative
Intel,Do we need this BS? Seriously? Gpu’s will end-up costing over 2 grand soon,Negative
Intel,I don't even get the hype around ray tracing most games don't have it and when they do it either makes the game look blurry or isn't noticeable unless you're in a weirdly lit area or looking at a reflection,Negative
Intel,"Am replaying it with a 6950xt and a 7700k(defo bottleneck).  All maxed out(no RT) at 1440p. If I wanted 4k I would have to enable FSR.  Its completely playable imo.   Mostly 50-60fps.  For me RT vs off vs path tracing.   The game doesn't look more real or better, it just looks different.  But ignorance is bliss.",Positive
Intel,This chart is bull. Since when is a RTX 3060 faster than a 3080. Someone needs to seriously question the source of the chart,Negative
Intel,Just goes to prove once again that Raytracing was pushed at least 4-5 gens too early as a pure marketing gimmick by Nvidia,Negative
Intel,"The Tech Power Up article says:   >Instead of the in-game benchmark we used our own test scene that's within the Phantom Liberty expansion area, to get a proper feel for the hardware requirements of the expansion  So it's very hard to reproduce their data at the moment, but I've run 3 tests with the built in benchmark (run to run variance below 1%, so 3 runs are enough) and my config is averaging 25.7 fps at 4K native with Path Tracing, so the title's premise is not correct. Still, the game is not playable without upscaling at 4K with Path Tracing, so the basis of the sentiment remains correct. Benchmark data available [here](https://capframex.com/api/SessionCollections/279f9216-bdb8-4986-a504-91363328adbe).",Neutral
Intel,RTX 4090 1600€ 19FPS 👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼,Neutral
Intel,"Yay i will get 13,8 FPS with my 4080 👍😂 Someone want to borrow some frames? I don't need them all!",Negative
Intel,The first card on this list a normal person would consider buying is a 4070. 8.5 fps lol.   Even with upscaling that's basically unplayable.   Then you have all the UE5 and path trace type demos where a 4090 is only getting 60fps with upscaling.. at 1080p lol.   We are not remotely ready for these technologies yet. They're at least one if not two console generations away. So multiple years.,Negative
Intel,"Looks like 4K is mainly just for videos, not gaming for the time being.",Neutral
Intel,How can you bring out such technology and no hardware can process it properly.  an impudence,Negative
Intel,the question is who need path traycing? i mean that cool and beautiful but 20 fps with a 1600dollar price tag that wild.,Neutral
Intel,You could just you know... disable path tracing and get 70 fps or go below 4k,Neutral
Intel,"That what I expect when a game gets built specifically to market Nvidia, but Nvidia over estimated their technology.",Neutral
Intel,"um...im running cp 2077 on a 4090 at 70-90fps, 5k monitor with path tracing/ray reconsturction on and most settings maxed out...",Neutral
Intel,Yo guys let's make this really unoptimized game then rely on hardware manufacturers to use AI to upscale the resolution and insert extra made-up frames to make it playable on a $1600 GPU lmao.,Negative
Intel,"so it is settled, Devs have zero optimization in games.",Neutral
Intel,"Love how shit the 3080 was lol, even when it came out it was shit. But somehow everyone loved it",Negative
Intel,"Or, hear me out, developers could just do better. I've seen far better looking games without the constant frame hit.   Cyberpunk hasn't looked good enough to justify the frame rates from the get go.",Negative
Intel,Once again proving RT is useless,Neutral
Intel,Cyberpunk w/ path tracing at max settings seems even more demanding than Crysis was at launch.   20fps with a 4090 is insane.,Negative
Intel,"""only gamers get this joke""",Negative
Intel,"People have forgotten or are too young to remember when Nvidia had hardware tessellation features when that was first emerging. They had tessellation maps under the ground and water in Crysis that were doing nothing other than tanking AMD (ATi) performance. I am not saying this whole Cyberpunk thing is exactly the same, but it's essentially similar. In all honesty, turning everything up to 11 on my 3090 doesn't net much visual gain beyond some mirror like reflections and nothing I really care about when actually playing the game (compared to standard High/Ultra) and not pixel peeping, which seems to be the new game now for people buying expensive GPUs, so they can justify their purchase. Meanwhile, everyone else just gets on with enjoying video games instead of living vicariously through someone's 4090 at LOL 1080p",Negative
Intel,Starfield is peaking around the corner.,Neutral
Intel,starfield as well,Neutral
Intel,I remember when physx was so demanding people had a dedicated 2nd Nvidia graphics card just to turn the setting on in Arkham City. Now it's considered so cheap to calculate that we just do it on the CPU lmao,Negative
Intel,The new Crysis,Neutral
Intel,"I wouldn’t be so optimistic. Transistor shrinking is crazy hard now, and TSMC is asking everyone to mortgage their house to afford it.",Negative
Intel,How long until a $200 card can do that?,Neutral
Intel,Most improvement is probably going to AI software more than hardware in the next few years.,Positive
Intel,"8800GT giving advice to 4090:  “I used to be 'with it. ' But then they changed what 'it' was. Now what I'm with isn't 'it' and what's 'it' seems weird and scary to me. It'll happen to you!""",Neutral
Intel,GPU need to have its own garage by then,Neutral
Intel,Yep! Insane how fast things change.,Neutral
Intel,"Most likely there will be no gpu that supports path tracing and gives you native 4k 120fps in 5 years, maybe even in 10 years.  The technology has slowed down a bit. It’s increasingly more challenging to make more dense chips.  That’s why Intel has been struggling for many years already and every iteration of their cpus gives only minor improvements. AMD went with chiplets but this approach has its own problems.  Nvidia stands out only because of AI. Raw performance increase is still not enough to play native 4k even without ray tracing.",Negative
Intel,Yeah rtx 12060 with 9.5 gb Vram will be a monster,Neutral
Intel,I'm willing to bet hardware improvement will come to a halt before that.,Neutral
Intel,"In 10 years AI based upscaling will be so good, no one will want to natively render unless they are generating training data",Negative
Intel,Transistor density advancements have been declining for a good while now. We can't expect hardware performance gains of old to continue into the future,Negative
Intel,Like the 1080 barely doing 4k30 and now we have gpus that do 4k120 id way heavier games.  Its still weord to me to see 4k120,Negative
Intel,But then the current gen games of that era will run like this. The cycle continues,Neutral
Intel,TRUE! i think 5-10 years was the actual point in time where anybody should have paid their hard earned dollar for raytracing gpus. instead ppl dished out $1000s for the RTX2080/Ti and now are sitting on them waiting for raytracing to happen for them xD,Neutral
Intel,Who knows what new tech will be out in even 4 years lol,Neutral
Intel,"And that’s when I’ll turn on this setting in my games. Fuck having to pay $1600 for a sub 30fps experience. Path tracing is far from being a viable option, it’s more like a proof of concept at this point.",Negative
Intel,"You wish, GPU makers have stagnated like it's crazy, even the graphics have peaked. I expected to see CGI level graphics like Transformers now",Negative
Intel,"...but will cost your everlasting soul, body, firstborn and parents. If it's on sale.",Neutral
Intel,It is path trancing though. The technology used by Pixar to make Toy Story 4 (though they spent up to 1200 CPU hours for one frame) Path tracing used to take up to a day per frame for films like the original Toy Story. And they had their supercomputers working on it. It is a miracle of modern technology that it even runs real time.,Positive
Intel,"You basically need a 4090 to crack 60 fps at 1440p w/ dlss on quality without frame gen. It looks good, but not good enough to run out and buy a 4090.",Negative
Intel,"Same card, I just turned off RT at 4K. 75-120fps is better than 40 with muddy but accurate reflections",Neutral
Intel,"The 7900xtx is one of the worst gpus iv ever had lol on paper it looks so so good, In games it just play isn’t. Fsr is garbage, 4080 mops the floor with it :( plus the drivers from amd besides starfield have been awful this generation",Negative
Intel,I think that most of the progress will go together with software tricks and upscalers.,Neutral
Intel,"lol. And you missed the 2080Ti.   Every result under 10 fps is just to be ignored, it isn't representing anything outside of ""woot, the card managed to chuck a frame our way"".",Negative
Intel,That's VRAM for you,Neutral
Intel,we are counting decimals of fps its just all margin of error.,Negative
Intel,If you buy a card with low ram that card is for right now only lol,Neutral
Intel,Wake me up when a $250 GPU can run this at 1080p.,Negative
Intel,Well DLSS isn't best. DLAA is,Negative
Intel,Reviews have been saying that the game looks better with DLSS than native. Not to mention runs extremely better.,Positive
Intel,Can’t you just disable TAA? Or do you just have to live with the ghosting?,Neutral
Intel,TAA is garbage in everything. TAA and FSR can both get fucked,Negative
Intel,"Yeah. 70 to 100fps on a 4080, but with 3440x1440 and DLSS-quality-FG-RR (nvidia needs new nomenclature....)",Neutral
Intel,"same, high refreshrate at 4k with optimized settings + PT + FG. With a 4080 of course, it's insane that it can look and run this great.",Positive
Intel,"Not exactly. Path tracing is just ray tracing, but cranked up to 99% with the least amount of rasterization possible.",Neutral
Intel,"> Give it 2 gens and you are going to get 4k60 here.  Assuming the 5090 literally doubles a 4090 (unlikely), that only gets us to 4K 40hz.  Assuming a 6090 doubles that, 80. which won't be bad.  Going with more conservative 50% boosts. 5090 will give 30. 6090 will give 45.  And i feel like 50% is being very generous, as nvidia have claimed moores law is dead and they can't advance beyond a 4090 by much. I'd guess we get 30% uplift in 5090 and maybe 10-15% uplift in 6090. So we'd still be under 4K30.",Neutral
Intel,Imagine buying a 4090 and then using upscaling.,Neutral
Intel,Path tracing is so much more demanding than ray tracing due to light scattering being modelled. It is a marvel it even runs.,Positive
Intel,PC gaming is in Crysis.,Neutral
Intel,Well it's a good thing 99.999999% of PC gamers don't give a shit about ray or path tracing.,Negative
Intel,You're the first other person other than myself I've run into that had an XTX but ended up with a 4090 in the end.,Neutral
Intel,"Plus frame generation works very well in Cyberpunk in terms of image quality. In some games, you need to get closer to ~80 fps output for an acceptable image quality with FG. But the FG in CP2077 is decent with 60 fps output, ~~and I get ~65-70 fps output with quality DLSS + FG at 4k on a 4090.~~ EDIT: I misremembered what I was getting. With path tracing, DLSS quality, frame generation, and ray reconstruction, I got 80.1 fps with the benchmark!  Of course there's the matter of latency, and the latency of CP2077 with FG output of ~65-70 fps isn't great. So I'll often use DLSS balanced + FG. Thanks to ray reconstruction, this now looks very close enough native 4k (to my eyes), with acceptable latency (to me), at a high framerate output.",Positive
Intel,"Nvidia features are always useless until AMD copies them a year or two later, only then they become great features 😁",Positive
Intel,"Tbh, list the games that have ray tracing right now. Pretty few.  It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it). I would personally go amd over nvidia because those 50 or so euros more that nvidia has against the amd counter parts simply are too much for just better rt and  dlss. I could spend those for a better amd card with generally better performance.  Regarding dlss, personally I would just lower the graphics before resorting to kinda bad generated frames, fsr even worse. But that's my point of view.  I find that amd still has to fix some driver issues but other than that, they are a fine brand. (so is nvidia)",Neutral
Intel,Yeh because native 4k looks worse than dlss + RR 4k,Negative
Intel,"Yeah the game with DLSS, RR and path tracing at 1440p looks amazing and with very high fps",Positive
Intel,What's the point of having $5000 PC when you're still gonna have literally the same graphics as $1000 PC then?,Negative
Intel,"This is a tech demo. That's the whole point. It's not really playable yet, but the game really is meant to showcase what is possible in the future and how close we are getting. That's what Nvidia is doing here by funding this whole project.  Crysis who many people are comparing this to, was in itself quite revolutionary for its time. The destructible environment in Crysis to this day holds up, and that was it's killer feature really.  You're gonna have swings at the future that miss as well, and that's ok.",Positive
Intel,"Did you try DLSS?  Imagine getting down voted for asking if he was using DLSS. What a fragile community. Sometimes it's okay to use DLSS, other times you better not or people will come to your house with torches",Negative
Intel,VRAM,Neutral
Intel,"Depends on what you are after i guess, is it worth it for me? Absolutely, DLSS+FG+RR with PT is a great experience.",Positive
Intel,still better than 90% of games in 2023,Positive
Intel,Arguably better now Ray Reconstruction has been added. It's quite a big image quality upgrade.,Positive
Intel,"…no it does not, unfortunately. Have you seen the artifacting in Cyberpunk?",Negative
Intel,"Someone ate up the marketing. I often see people wonder why games are so unoptimized today, your answer is right here. \^",Negative
Intel,All graphics you see on your computer screen is fake,Negative
Intel,Path tracing is more demanding than Ray tracing,Neutral
Intel,"That's why I downscale from 1440p to 1080p, running DLSS and using Contrast Limited Sharpening. It looks better than native resolution and still performs well.",Positive
Intel,I guess between the 3090 and the 4070,Neutral
Intel,Yep hahaha,Neutral
Intel,RemindMe! 7 years,Neutral
Intel,"No, this tech is around so game developers can sell more copies of their games by making them look better. The only reason why Nvidia is winning is because their GPUs are much better at intensive ray-tracing.",Positive
Intel,"This is an absurd, fanboyish thought lmao",Negative
Intel,dont let novidia marketing see this youll get down voted into oblivion,Negative
Intel,I would still argue it's still in proof of concept stage. The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native. That will the allow the lower end GPU'S to get 4k 60 or 1440p 60 upscaled.,Neutral
Intel,I mean I have no problem playing at 30fps dlss balance at 1080p DF optimized settings on a 2080ti with pathtracing idc what anyone says RT/PT is the future it just looks so much better than Rasterization,Positive
Intel,"Possibly a little more. Assuming that the leaked 1.7x improvement is right and path tracing performance scales the same, that would be about 34 fps at 4k native. Might be possible to hit 60fps or close to it with DLSS Quality.",Neutral
Intel,It could as well do 60fps if Nvidia adds a lot of path tracing HW into the GPU.,Neutral
Intel,Fanboyism of both kinds is bad,Negative
Intel,"It won't end, but hopefully we see games that have art designed with RT lighting and reflections in mind.  Cyberpunk isn't it. We'd need a full rebuild of the game, every location, every asset and surface remade, so it looks like originally intended by the artists.",Neutral
Intel,Since it needs more than 10gb vram,Neutral
Intel,Cyberpunk is more optimized than majority of games that came out in last two years. Just because you can't run it at 4k 120 fps with path tracing doesn't mean it's not optimized.,Neutral
Intel,It’s not unoptimized at all. Game runs great for the quality of the graphics. You guys just don’t understand the insane amount of power path tracing takes. The fact you can do it at all rn is insane but once again people just throw “unoptimized” at it because they don’t understand the technical marvel happening in front of their eyes.,Negative
Intel,"No, this game is actually very well optimized. It's just extremely demanding at 4k native, which is why DLSS exists.  It would be unoptimized if it ran like this while looking like nothing special, but this is probably the game with the best graphics, period. At the very least, it's certainly the game with the best lighting.",Positive
Intel,"and people will be like ""well RT is the future""  raster was fine for over 15 years and raster games look great but we get it we must let incompetent game devs and their bosses make shit quality games because average gamer is ready to spend $100 to pre-order electronic version of horse shit and then justify it for years because they can't admit they wasted money",Negative
Intel,It's settled that you have no idea what you talking about,Neutral
Intel,Its full path tracing u cannot really optimize this much.,Negative
Intel,"It always did. Most games don't scale well, Cyberpunk does.  Look at the last two years of pc ports and you will understand how cyberpunk looks and performs like it's black magic.",Neutral
Intel,"Baked illumination requires you to choose where to add light sources and how the lighting is applied to everything, it's a creative choice of the author, the AI needs instructions to know what to do, so it would need constant handholding, on the other hand RT uses simple physics to calculate how light bounced and NN to apply the light on surfaces.",Neutral
Intel,But the destructable environment and the water graphics and other things in Direct X 9 made high end pcs kneel as well.  I remember playing on my 9800gtx/+ with my Intel Q9300 quad core (lapped and oc'ed to 3.0ghz - EDIT: checked some old evga posts got it to 3.33ghz) with 2x4gigs DDR2 @1000mhz cas 5 trying to maintain a sustained 30fps at 900p resolution on my 1680x1050 monitor. And I oc'ed the crap out of that 9800gtx 835 mhz (cant recall if it was unlinked shadder or not now) core on blower air (won the silicon lottery with that evga card).  Tweaking settings was mostly user done and guided with old school limited forum help. Ahh the good old days of having lots of time during school breaks.,Neutral
Intel,"The equivalent of a 4090 at the time, 8800 ultra (768MB) got 7.5 fps at 1920x1200 very high quality. The other cards were not able to run it. Even the lowest tier last generation card runs this, even though it's at 5% speed.",Neutral
Intel,No and no again lol  The 8800gtx could do 1152x864 high and that was the top dog. DX9 cards could do that at medium but high shaders was like a 25fps average tops and that dropped into the teens on the snow level.   It took things like the gtx480 and HD 6970 to do 1920x1080 30fps max settings. That's before getting into Sandy Bridge being the first cpu that could sustain 40+ fps if the gpu power was there.   Crysis came during the core 2 duo/k8 athlon and first wave dx10 cards era and it took *2 more generations* of *both* gpus and cpus to do it some justice.,Neutral
Intel,"My 8800ultra was getting single digit fps at 1080p ultra, so CP isn't quite as demanding.",Neutral
Intel,Amd had a tessellation unit. It went unused but it was present,Neutral
Intel,"My fun story for PhysX was Mirrors Edge. I don't remember what GPU I had at the time but the game was pretty new when I played it. Ran fine for quite some time until one scene where you get ambushed in a building by the cops and they shoot at the glass. The shattering glass with PhysX turned on absolutely TANKED my framerates, like single digit. I didn't realize that the PhysX toggle was turned on in the settings. This was at a time when PhysX required a dedicated PCIe card in your system.   Once I turned it off it ran fine. Now I can run that game at ridiculous framerates without my system getting warm.",Positive
Intel,"I remember around about 2008 when companies like Asus and Dell were selling ""Physics Processing Units"" and some claimed that these would be commonplace in gaming machines just like Graphics cards had become 10 years previously.",Neutral
Intel,"Hardware accelerated physics (as in on the gpu), is different than what's going on the CPU.",Neutral
Intel,hobbies quaint consist aromatic political hat aback boat relieved crush   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"People used to have ATI/AMD for main and a lower-end NVIDIA for PhysX.  When NVIDIA found this out they pushed out drivers that disabled PhysX on their cards if an ATI/AMD card was detected, limiting you to the intentionally piss-poor CPU implementation of PhysX.  Think about that crap.  One day everything's going fine for consumers, the next day NVIDIA decides they don't like how consumers are legitimately using their cards and gimps everyone, weaponizing a physics engine company that they bought in 2008.",Negative
Intel,"Oh damn, I forgot about those cards.  I wanted one so badly.",Negative
Intel,Remember when companies tried to sell physics cards lol,Neutral
Intel,>PhysX  It never was a dedicated Nvidia card - it was a [dedicated psysx](https://www.techpowerup.com/review/bfg-ageia-physx-card/) on which the tech later was bought by Nvidia and implemented in it's own GPU's.  But the things never became really populair.,Neutral
Intel,"This makes me wonder if we'll ever see something similar with Raytracing, where we get a 2nd GPU purely for RT, and then the main GPU just does all the other stuff. Would that even be possible?",Neutral
Intel,I remember when people had a dedicated PhysX card.,Neutral
Intel,The ray/path tracing in this case done by specialized hardware which has more room to grow faster.,Neutral
Intel,"I would, transistor shrinking isn't the only method of increasing performance, and honestly, these companies have to keep putting out better cards to make money.  There have been many breakthroughs over the last few years. I give it another 5 as both amd and nvidia are pushing ai accelerated ray tracing on their cards, nvidia is in the lead for now but amd will eventually catch up.",Positive
Intel,There is still a big leap possible since all lithography processes at the moment are hybrid euv and duv.   But the moment everything is done euv things will drastically slow down.,Neutral
Intel,"No those are just pennies in the pocket of Nvidia, but as the consequence you as the customer need to take a mortgage on a brand new GPU.",Neutral
Intel,"Yeah the next 10 years are gonna be spent getting budget cards up to 4090 level performance, rather than bringing the bleeding edge to 5-10x the 4090 performance. As long as 4K is king, there’s not much incentive to do the latter.",Negative
Intel,"In 10 years we will be begging one of several thousand test-tube created Musk Family members for $200 so we can buy a cheeseburger.  But the jokes on them, we’re just gonna spend it on space crack.",Negative
Intel,"Never. Even if performance can be pushed that far, by the time it happens there won't be such a thing as a $200 graphics card anymore.",Negative
Intel,It's not happening unless the market crashes and they start focusing on offering good price/performance cards instead of bumping up prices every generation.,Negative
Intel,25 years,Neutral
Intel,"The software needs to run on hardware, right now it eats through GPU compute and memory.",Neutral
Intel,And sadly ended up with 450 Watt TDP to achieve that performance.,Negative
Intel,This. We'd be lucky to see more than 3 generations in the upcoming decade.,Positive
Intel,"Having seen a few newer games on relatively low resolution CRT display, I can't help but think it might come down to improved display tech and embedded scaling. Like DLSS3 features in the display instead of the GPU.",Neutral
Intel,Not with that attitude,Neutral
Intel,Love how it's still gimped on memory size 😂,Positive
Intel,"They just need to render the minimum information needed , and let the ai do the rest",Neutral
Intel,"Don't get upscaling, to me it looks shite. Why spend hundreds or thousands on a GPU to not even run at native resolution.   It's something Nvidia are pushing because they've got a better version of it at the moment.  Maybe it'll improve, it's pretty much snake oil currently imo.",Negative
Intel,"Feels pretty good with 120 frames. Playing with a controller, i don‘t mind or even feel the input lag on a projector screen.",Positive
Intel,"It's mad that even with a 4090 running games at 4k, they still look less realistic than the low-res FMV sequences in The 7th Guest from 30 years ago.",Negative
Intel,"Do keep in mind that despite both being named the same, the devil's in the details. Movies use way more rays per scene and way more bounces too.   Path tracing in CP2077 shows temporal artifacts due to denoising, something that doesn't happen in movies. It is being improved with the likes of DLSS 3.5, but it is still quite off when compared to said movies.",Neutral
Intel,"Keep in mind the systems that rendered toy story 1, costing upwards of like 300k IIRC, have less power than your cell phone today and were super delicate/finicky machines. There's a dude on youtube that got a hold of like the second best one that was made at the time and the machine honestly was really impressive for when it came out, but it pales in comparison to even a steam deck really.",Neutral
Intel,"> Path tracing used to take up to a day per frame for films like the original Toy Story  Toy Story didn't use path tracing though, A Bug's Life was Pixar's first movie to use ray tracing (not path tracing) and only for a few frames in the entire movie for specific reflections, they started using ray tracing more generally for Cars and I can't find exactly when they started using path tracing but it should be around the early 2010s which is also when the other Disney animation studios started using path tracing",Neutral
Intel,No it's not a miracle. Because it's downscaled a lot and also uses lower quality math and piggybacking on AI to make up for it. It's basically running through 3 layers of cheats to produce results that aren't that much better than just traditional raster.,Negative
Intel,"That’d be some next level consumerism, paying 1500$ minimum to turn on a single setting in a single game just to play it wayyyyy slower than you would otherwise",Negative
Intel,"Hell playing path tracing with my 2080ti at 25 FPS is still looks absolutely fantastic, I would absolutely play with pathtracing on a 4090 constantly. Idc dlss looks great with RR even at 1080p. I won’t upgrade for another year or 2 (just bought a phone so I’m broke right now)",Positive
Intel,4090 getting 60fps at 4k balanced dlss with no frame gen. 100 with frame gen. I can make it drop into the 20s if I stand right next to a fire with all the smoke and lightening effects or if I go to a heavily vegetated area it’ll drop to mid 40s. But it’s stay consistently at 55-65 and even goes Into the 90s if I head out of the city.   Haven’t tried it at quality or with dlss off though. May go do that now that it says only 19fps lol. Have to try it to see for myself,Neutral
Intel,"https://www.tomshardware.com/news/nvidia-dlss-35-tested-ai-powered-graphics-leaves-competitors-behind  Well it's so close (54fps) it's more like a 3080Ti or higher from 3000 series or a 4070TI + from 4000 series it seems? The old 3000 series is punching way above it weight vs the 7900xtx, which was meant to deliver similar RT performance to the 3090.. which it doesn't.",Neutral
Intel,At some point the RT pixels are so expensive that native resolution w/o DLSS and frame gen is just not gonna work for the time being.,Negative
Intel,"On the other hand, if you set the new DLSS 3.5 to performance (which you should in 4k), and just enable frame generation, you get 90+ in 4k with basically zero issues unless you pause to check frames.",Positive
Intel,So rendering at 960p? Oof...,Neutral
Intel,"you take that back, I literally returned my 4080FE for a 4090 liquid x for more frames in cyberpunk.",Neutral
Intel,I almost see no difference.   And to see the little diffidence I need to take two screenshots and compare them.,Neutral
Intel,The game look is ruined on PT it makes everything look completely different.  Regular RT keeps the style of the game and performs good.,Negative
Intel,Dont use useless raytracing and you wont have any problems lol,Negative
Intel,"Because PC gaming blew up during a time where you could buy a mid range GPU and not need to upgrade for 5-6 years. Now those sample people buy a GPU, and 2 years later it can't run new games. At least that's my theory.",Negative
Intel,AW2 looks insane. Can't wait to play soon,Positive
Intel,Well the 3050 managed to hold with just 8GB while the 3070Yi crashed?,Neutral
Intel,"Yeah, and people insisted on defending the configurations at launch lmao. The cards just won't be able to handle heavy loads at high resolution such as this game, regardless of how fucking insane the actual processing unit is. You can't beat caching and prepping the data near the oven. Can't cook a thousand buns in an industrial oven at the same time if there's trucks with 100 coming only once an hour.",Negative
Intel,My 3080 in shambles,Neutral
Intel,"It looks better than native even with fsr quality, the Taa in this game is shit",Negative
Intel,You dont need to increase raw performance. You need to increase RT performance.,Neutral
Intel,Imagine!,Neutral
Intel,DLSS unironically looks better than native TAA even at 1080p in this game because of the shit ghosting.,Negative
Intel,Only people who don't give a shit about high quality graphics don't care about ray tracing.,Negative
Intel,I got a stupidly good deal on the 4090(buddy won an extra one from a company raffle) and it just so happened my XTX was one of the cards suffering from the mounting issue on release.  Honestly very happy that all of that happened. Sure I would have been happy with the 7900xtx if I got a replacement but the 4090 just kinda blew me away especially with all the DLSS tech behind it.,Positive
Intel,"Nvidia used cp77 as a testing grounds for their dlss3 and rt tech, it'd no surprise it looks and performs relatively good, they literally partnered with cdpr for that.  Do not expect these levels of RT on most games anytime soon, probably in a couple years (with rtx 5000 series) the tech will be more mature and not as taxing for most cards, because needing to upscale and use framegen to get 60fps on an RT PT setting is kinda absurd.",Negative
Intel,">only then they become great features  Watch this happen with ""fake frames"" FSR3 in real time",Positive
Intel,>Ray tracing as of right now is a gimmick   This line was already false years ago when control launched. Now it’s just absurd,Negative
Intel,"There are a lot of new games that features Ray Tracing on them as of now, too bad most of them doesn't look as visually as good as they could be though as most of the time their RT effects are dumbed down or reduced to the point it's pointless.  But the thing is that wasn't the point even from the beginning, RT Overdrive on Cyberpunk is literally considered as a Technological showcase  What matters here IMO for all of us is PC platform in general is showcasing here what a real high-end computers can achieve beyond what current gen console can do, and they are showcasing it really well here on Cyberpunk with much better visuals and using plenty of tricks to make them more than playable, where they shouldn't be considering how much demanding they are over standard, that is impressive enough to me and shows a good sign of technological engineering innovation.   As what Bryan Catanzaro said on his interview on DF Direct, work smarter not harder, brute force is not everything, of course it still matters a lot but doing it alone is starting to become pointless just like how it is on real life.",Negative
Intel,Much more fps :) Newer generation cards have at least 50-60% better raster performance then prev gen.,Positive
Intel,The Evangelical Church of Native,Neutral
Intel,"So just to be clear, you don't like running games at native resolution? Because the purpose of DLSS is to improve performance by specifically not running at native resolution",Negative
Intel,Nice but ive been playing video games since the 70s and its Shit end off..,Negative
Intel,The future of frame generation is going to be huge. I can definitely see 360p frame rendering upscaled to 4k with 80% maybe even 90% of the frames being generated. All with equal to or even better visual quality.   A 20 FPS game like this will become a 100-200fps game.  We may be looking at the end of beefy graphics cards being required to play games. Just a good enough mid or low tier chip (granted 5-10 years from now) with frame generation might be enough.,Positive
Intel,"After 2.0, it's around 100""fps"". Just 60""fps"" (meaning with FG enabled) is far from being playable due to latency. You need at least 90""fps"" for playable experience.   Cyberpunk PT vs RT is a huge difference in terms of graphics, while DLSS Quality vs Balanced is really slim in comparison. What you say would be a really bad trade-off.   At 1440p DLSS Quality it's easily 120+""fps"" which is perfectly comfortable.",Neutral
Intel,Cyberpunk is greatly optimized. It looks fantastic for the hardware it asks. Starfield looks like game from 5 years that asks hardware from the future.,Negative
Intel,"Yeah, sure, now go back to play starfield.",Neutral
Intel,"There literally is a /s, what else do you need to detect sarcasm?",Neutral
Intel,"Someone made a really cool comment yesterdays on Reddit that even a generated frame using path tracing is less fake than a rasterized frame. It is effectively closer to reference truth. I had never thought about it that way, but people really are clutching their pearls with this shit",Positive
Intel,"I know, which makes path tracing even worse off imo.",Negative
Intel,I will be messaging you in 7 years on [**2030-09-23 15:26:41 UTC**](http://www.wolframalpha.com/input/?i=2030-09-23%2015:26:41%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Amd/comments/16pm9l9/no_gpu_can_get_20fps_in_path_traced_cyberpunk_4k/k1v42p3/?context=3)  [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FAmd%2Fcomments%2F16pm9l9%2Fno_gpu_can_get_20fps_in_path_traced_cyberpunk_4k%2Fk1v42p3%2F%5D%0A%0ARemindMe%21%202030-09-23%2015%3A26%3A41%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%2016pm9l9)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,Neutral
Intel,haha this is gold 🥇,Neutral
Intel,"At the moment, it’s definitely what it seems like.  Especially seeing how there’s only one card on the planet that can run this. Do you have a 4090?",Neutral
Intel,">I would still argue it's still in proof of concept stage.  Not at all, in fact another AAA game with ""path tracing"" is coming out next month.  >The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native  Forget ""native"", that ship has already sailed. Nvidia, AMD and Intel are all working on machine learning techniques, and several game developers are starting to optimize their games around DLSS/FSR.",Neutral
Intel,"Nope, not even close.  Shaders 3.0 - that's improvement. Ray tracing is noticeable, but nowhere near to justify the hit.  I've looked techpowerup comparison RT vs LOW (lol). Low looked far better and cleaner, then blurry mess of RT (Overdrive).   I don't need ""realistic"" lighting, I need pleasent bisual part with good gameplay. For ghraphics I have a window in my appartment.",Neutral
Intel,"In a screenshot, sure. When actually playing the game, unless the fps is low enough to remind them, 99,99% of people will forget if they left it on or not.",Negative
Intel,Pixar movies will sometimes take the better part of a day to render a single 4k frame on a render farm. So the fact that Cyberpunk takes more than 0.05 seconds to render a 4k frame in its path-tracing mode on a single 4090 clearly means that it's unoptimized garbage! /s  EDIT: Apparently people in this thread don't understand sarcasm (or don't understand how a path-traced game can take longer than 0.05 seconds to render a 4k frame with high-poly path tracing and still be optimized).,Negative
Intel,Man you had to lap your q9300 to hit 3.0ghz? Those chips really weren't OC friendly.  My q6600 hummed along 3.2 no problem on 92mm arctic freezer and a slight voltage bump.  I was so pumped when I upgraded to a gtx 260. Crysis was a dream.,Positive
Intel,Funnily Crysis still have better destructible environments than Cyberpunk has tho,Positive
Intel,"This. People losing their shit either weren't around when Crysis launched or have forgotten just how demanding it was . PT CP kicks the shit out of a 4090, Crysis murdered my 8800ultra.",Negative
Intel,crysis did not do 1080p 60 on maximum setting and dx10 on a 8800GT.   I had a 1280*1024 monitor back then and barely had 30 fps with my 8800GT.   Even the 8800 Ultra did not do 60 fps on full hd.  https://m.youtube.com/watch?v=46j6fDkMq9I,Negative
Intel,"Man, I remember how hype the 8800gt was. Thing was a hell of a big jump compared to previous cards that came out, prolly our first REALLY powerful card.  Still remember the old xplay video where they build a PC to play crysis and Bioshock. Put 2 8800GTs in sli in there with a core 2 quad which costed one thousand dollars back then!",Positive
Intel,"My poor HD 4850 got pushed to the limit to give me 1280x1024, lol",Negative
Intel,I remember thinking $300-400 (IIRC) was so much for a GPU back around those days.  7800XT is the closest we've seen in a while and not even close to the top end for today.,Negative
Intel,This is still the case to this day because the game includes a really old DLL file for PhysX. The other day I followed the instructions on the PCGamingWiki to delete some DLL files in the game directory and only then it ran perfectly smooth on my RTX 3070.,Neutral
Intel,LOL I remember this exact scene also.,Neutral
Intel,"CPU PhysX back then was single-threaded and relied on ancient x87 instructions if I recall correctly, basically gimped on purpose. Even with a 5800X3D the shattering glass reduces the frame rate to the low teens. Sadly an Nvidia GPU is still required if you want to turn PhysX effects on for games from that era, though I hear that it's possible again to use it with an AMD GPU as primary.",Negative
Intel,"OMG I had the exact same experience, hahaha I remember it vividly, I was so confused why that room would just destroy my FPS until I figured out PhysX was enabled LOL",Negative
Intel,"I'd like to see ray tracing addon cards, seems logical to me.",Neutral
Intel,"TBH, I could probably run some of the old games I have on CPU without the GPU.",Neutral
Intel,glorious squash wild file crawl ancient crowd racial soft north   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,You could probably run Mirror's edge with physx on today's hardware without gpu fans turning on,Neutral
Intel,Last time I tried on an R7 1700X and RX580 I still couldn't turn on PhysiX without it being a stutter party 2 fps game.,Negative
Intel,What gpu you have,Neutral
Intel,Now it even runs fine on a Ryzen 2400G.,Positive
Intel,"And they were right, PhysX and systems very much like it are still used but things have advanced so much nobody even thinks about it and it no longer requires dedicated silicon.",Neutral
Intel,"Yeah, it’s been common knowledge for many years now that Nvidia are the most ruthlessly anti-consumer company in PC hardware, and it’s not particularly close.",Negative
Intel,Ah good old Ageia before nvidia bought them out https://en.wikipedia.org/wiki/Ageia,Neutral
Intel,"No, you could actually install two Nvidia cards and dedicate one of them to only PhysX.",Neutral
Intel,"It would certainly be possible, but it wouldn't really make sense. Splitting it up on multiple GPUs would have a lot of the same problems that sli/crossfire had. You would have to duplicate memory, effort, and increase latency when you composite the final image.  It may or may not make sense to maybe have a raster chiplet and a ray tracing chiplet on the same processor package on a single gpu. But, probably makes more sense to have it all on one chiplet, and just use many of the same chiplet for product segmentation purposes instead.   A separate PPU did make sense tho, I'm still annoyed that the nvidia ageia deal essentially kill the PPU in the cradle. Our gaming rigs would cost more if PPUs became a thing, but we could have had a lot better physics then we do today. There is still a revolution in physics to be had some day...",Negative
Intel,would be cool if 2000/3000 series users could get a small tensor core only PCIe card to upgrade to framegen,Neutral
Intel,"That's true, but if he's talking about *the equivalent* of a current $200-class card, I'd say about it's 10 years, what do you think?",Neutral
Intel,At least with the 4090 you can run 70% power target and still hit insane FPS while only pulling around 300w which is the same as my old 3080. The gains with that extra 150w are a couple percent at best. Not worth it to me.,Negative
Intel,Intel design will be a little bit different as far as now.  If I understood it right AMD chiplets communicate via lines on pcb but Intel wants to make something like chip-on-a-chip.,Neutral
Intel,bedroom ring library cows summer thought aspiring worm north joke   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"It's also worth noting that those early movies don't use path tracing either, Pixar switched to PT with Monster University around 2013 IIRC.",Neutral
Intel,"There is a lot of room for improvement, both in the software and future generations of hardware.  It's coming along though!  Overdrive mode looks nice, but there's just a lot more ghosting than the regular RT mode.",Positive
Intel,"They are still the same thing and should be named the same, your disclaimer is just semantics about the implementation.",Neutral
Intel,"Weird argument.  Traditional raster is cheats upon cheats upon cheats already.  In fact, all of the lighting entirely in raster is essentially cheats.",Negative
Intel,It will not work well/look good using a single frame worth of data either due to the low ray counts. Digital Foundry has shown Metro Exodus builds the lighting over ~20 frames or so and it seems like Cyberpunk is even more. Even Cyberpunk ray pathtracing doesn't look as good in motion due to that same build-up effect.  You need to cast 20-50x the rays to have enough data for a single frame but then you will be measuring the game performance by seconds per frame.,Negative
Intel,"RT looks significantly better than raster when it’s not just using simple reflections like a Far Cry 6. Raster is all cheats, the so called reflections are essentially rendering the scene or part of the scene being reflected in reverse then applying fake material effects to mimic metal, water, etc. Raster also takes HUNDREDS to thousands of person hours to get the fake reflections and lighting to look decent to good. That’s why devs love RT and want it to grow, as RT/hardware does the work for them.",Negative
Intel,It's significantly better than raster. It kills fps but the quality is great,Positive
Intel,Better graphics needing more expensive hardware is hardly a hot take.,Negative
Intel,"Yes, better graphics costs performance. SHOCKING",Positive
Intel,People do it!,Neutral
Intel,It’s not way slower. I get 110 FPS at 4k with all DLSS settings turned on and honestly it’s insane.,Negative
Intel,"I shamefully admit I bought a (inflation adjusted) $400 console in the past to play one game. That's marginally better than buying a GPU for one setting in one game, but still.",Neutral
Intel,Sounds like most nvidia fanboys,Neutral
Intel,"If you're into the eye candy, I can see a lot of people that have the money doing it.  Not everyone though. That's around a mortgage payment for me.",Neutral
Intel,And yet people will tell you how nvidia is mandatory. Like you want to overspend to play 1 game with shitty fps? I don't understand these people.,Negative
Intel,Nvidia fanboys being happy with fake frames and fake resolution will never not be funny to me.,Negative
Intel,What? I thought amd was always more tuned to raster as opposed to reflections,Neutral
Intel,Which is why nvidia is rabidly chasing AI hacks,Neutral
Intel,This and rumors are saying that the 5090 is gonna be 50-70% faster than 4090 which wont be enough for native 4k either.,Negative
Intel,Say that to the people playing upscaled games at 4k (540p) on PS5.,Neutral
Intel,"I understand some RT effects might not catch everyones eyes, but seriously, path tracing cyberpunk vs 0 RT cyberpunk IS a big difference",Neutral
Intel,Huh? [Watch this is in 4K on a big screen.](https://www.youtube.com/watch?v=O7_eHxfBsHQ&lc=UgyO2vRiSI6CRU5kYmV4AaABAg.9uyI5P8oamz9uylTN_rdmL) The differences are really apparent in each version.,Neutral
Intel,"PT looks way better in the game to me - it makes everything in the environment have a more natural look to it.  It adds, not ruins it...",Positive
Intel,"That's how I feel as well. It looks awesome don't get me wrong, but it doesn't look like cyberpunk to me.",Negative
Intel,It's not a perfect way of measure but you can clearly see how (at least on Nvidia GPUs) the 8/10GB cards are way behind the >11Gb cards. Meaning you need 11 or 12GB of VRAM for this scenario which cripples the 3080 but not the 3060.  We said from the start these configurations are shit but no-one listened. There you go.,Negative
Intel,The crypto miners did me a comical solid by preventing me from acquiring many countless times and hours I wasted (came from a gtx1070 before finally being able to upgrade). Was able to get my 6900xt eventually for around $600 with 2 games. Becoming increasing thankful for the extra Vram these days.   Once I start getting through my game backlog and into the gard hitting ray tracing ones will hopefully upgrade to something with at least 24gigs of GDDR# lol.,Positive
Intel,"I mean, Alan Wake 2 is also releasing with path tracing so my guess is we're definitely gonna see more games featuring heavier RT, although not necessarily path tracing due to the performance hit.",Neutral
Intel,"I expect path tracing support to be the exception for a while because most developers will consider the non-trivial dev time to implement it not worth it. However, I'm sure Nvidia would be willing to throw dev help at most developers who would be willing to implement path tracing with their help.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode. Presumably, a less complex world helps, but they may also have implemented opacity micro maps, which I'm not sure if Cyberpunk has done in the 2.0 update. Perhaps they're cherry-picked examples for the trailer.",Neutral
Intel,Raytracing now is a gimmick *  ( * - in AMD sponsored games where RT effects are so tiny you have to zoom on to notice them),Neutral
Intel,"Except this is a tech demo. CP77 has lots of cut corners to achieve that, NN was trained to keep the interface stable in that game and even then, ray reconstruction creates horrible artifacts at some angles and distances. And that's with Ngreedia's engineeers literally sitting at CDPR's office.",Negative
Intel,You can already have 120fps on $1000 PC,Neutral
Intel,It improves both looks and fps so it is a win win,Positive
Intel,"The issue with this is the inputs will only be registered at 20Hz. This would feel awful. The benefit of high FPS is having very low latency. This will just make it look smoother, not feel any better than 20 FPS",Negative
Intel,Considering there is no input on the generated frames that is going to feel horrible to play. Base FPS (incl DLSS) really still needs to be 50-60fps to feel playable imo.,Negative
Intel,I'm thinking will there ever come a day where 100% of frames are generated by AI. Like the data inputs directly from the card into the neural network and it no longer requires physical hardware limitation to generate frames.,Neutral
Intel,"Don't forget that all NPCs will be controled by AI to simulate a real life.  They'll wake up in the morning, shower, commute to work, work, then go home.  All this to do it over and over and over until they die...  mhhh... this is kinda sad to type out.![gif](emote|free_emotes_pack|cry)",Negative
Intel,It absolutely is. I’ve been playing at 1440p with DLSS Balanced and Ray Reconstruction and it is absolutely the worst version of DLSS I’ve ever seen. It’s basically unusable.,Negative
Intel,There are horrible artifacts with RR. And ghosting. It replaces artifacts with different ones. Needs a lot more work tbh. Or training.,Negative
Intel,"The thing about ray and path tracing is that they are demanding on their own, not because they are badly optimised",Negative
Intel,"I have a 4070. It runs the game just fine with the tools that have been set in place.  This is absolutely not a pissing match. Cyberpunk is demanding, and takes advantage of lots of tech available for upscaling and ray tracing and frame generation. AMD card’s currently do not match Nvidia’s in those categories, not even close. This us not an attempt to showcase Nvidia tech, but it does take advantage of it.",Positive
Intel,Because it's not about the flagship card. Only if the flagship can attain good performance at native then you will have headroom for lower end cards. Think about it. Of course upscaling is very important. But you also need some headroom to upscale.,Neutral
Intel,"Yeah. I wanted better cooling performance after upgrading to the artic freezer 7 as well with those weird crapy plastic tension clips you pushed in manually, fearing bending them (worse than the stock intel cooler tension twist lock). Kept having random stability crashes until after i sanded it down for better thermals...Good old sandpaper and nerves of steel fearing an uneven removal of the nickel coating.  Was trying to maximize performance as back then core 2 duo and the extreme versions were king and they had way higher single core clocks and were easy to oc. Wanted the multhreaded for Crysis, LoL (back when you had to wait 5min+ to get into a game), BF2, and was eventually playing TF, Day of Defeat and Natural Selection.  My upgrade back then was to the Evga 560ti DS, which I ended up installing a backplate and  a gelid cooler. They had wayy better thermal tape/individual heatsinks for the memory/vram chips for the heat transfer. Evga back then told me if I ever needed to RMA it that I would just need to re-assemble it as it was shipped. Remember using artic solver ceramique instead of as5 due to it potentially not degrading as fast as well.   Good times =)",Negative
Intel,"Yea same here, didn't play crysis until I got a hold of a 260 after having 2 8600gt's in sli. Played mostly wow at the time and those 2 8600gt's in sli got blown away by the gtx 260, but man that card was hot!  Remember in 2011 upgrading to a gtx 570, the first gen with tessellation and playing dragon age 2 which was one of the first games to have it. Ended up turning off tessellation cause it hit the card too hard, least until I got a second 570 in sli, which sadly died like a year later due to heat while playing Shadow Warrior.",Positive
Intel,Q6600 was the bomb. I ran mine at 3.0GHz all its life and it's still alive today. Had it paired with an 8800GT which was IMO one of the best GPU value/performance of all time.,Positive
Intel,Had 3.4ghz on my q6600 didnt want to hit 3.6ghz on modest voltage but 3.4 all day and was quite the lift in fps in gta4 compared to stock 2.4ghz. Run 60fps 1440x900 no problem those where the days when OC gave actual performance boost,Neutral
Intel,> I was so pumped when I upgraded to a gtx 260. Crysis was a dream.  In those days I was a broke teenager and I remember upgrading to a GTS250(rebadged 9800GTX+ with more vram) and crysis still hammered my Athlon x4 965(?) system.  While my richer neighborhood buddy upgraded to a 260 just to play Left 4 dead.,Positive
Intel,"After checking sone old Evga posts, (mod rigs server is done, along with old signatures), I was able to get to 3.33ghz.",Neutral
Intel,"Yeah my lapped Q6600 was my daily driver at 3.2 GHz with a Tuniq Tower 120 Extreme. It ran it pretty cool. I think when Crysis came out it and my 8800 GTX could do 20-25 frames at 1680x1050 with drops into the teens. EVGA replaced it with a GTX 460 when it died, big upgrade.",Positive
Intel,moving data between the two is the issue,Neutral
Intel,grey cagey sharp detail handle north grandiose connect sparkle hungry   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"I wonder the same thing. But I guess if Nvidia would put it all in an extra card, people would just buy more amd to get the best of both worlds.",Neutral
Intel,Game physics doesn't seem to be a focus anymore though,Neutral
Intel,I'm ashamed I even own one NVIDIA card.    Unsurprisingly it's in a laptop that I bought during the [illegal GeForce Partner Program](https://hothardware.com/news/nvidia-geforce-partner-program) before the public knew about the program.  Not making that mistake again.  Screw NVIDIA.  Every other card I've ever gotten has been ATI/AMD (and obviously the low-end integrated Intel GPUs on some office laptops)  EDIT: Keep them downvotes coming.  You've all been bullshitted by NVIDIA's marketing.  Your boos mean nothing; I've seen what makes you cheer.,Neutral
Intel,Anyone who owns an Nvidia GPU should be ashamed of themselves tbh.,Negative
Intel,"There was more nuance to that ""bugfix"" than just ""it was a bug that got fixed""   Modders were able to re-enable the AMD/NVIDIA PhysX combo basically immediately via editing DLLs.  Not driver replacements, but actual DLL tweaks.  The speed at which modders were able to fix what NVIDIA ""broke"" combined with the public outrage put them in a funny legal spot.  If they continued with PhysX disabled and claiming it was a bug then the bug can easily be fixed, as shown by modders doing it first.  So either fix it and get everyone back to normal, or leave PhysX disabled even though it can be easily enabled and open the company up to potential anti-competitive lawsuits.  Obviously not many modern games use the GPU implementation of PhysX anymore and the performance difference between current GPU and CPU implementations are negligible anyway, but their intent at the time was clear once it was shown how quickly modders were able to get everyone back to normal.",Neutral
Intel,"Yes, but not after Nvidia bought Ageia.",Neutral
Intel,"If the $200 cards of today aren't 3x better than the flagship cards of 10 years ago (they're not), then you shouldn't expect a $200 card in 2033 to be 3x faster than an RTX 4090. Average annual performance gains are more likely to decrease between now and then, rather than increase.",Negative
Intel,I like how you said: static image  because in motion upscaling is crappier,Negative
Intel,"It isn't a weird argument. Rasterization is premised on approximating reality, while RT is simulating it.  The extreme few amount of rays we trace right now, including bounces, means effects are often muted and the performance hit is enormous. To compensate, we're using temporal upscaling from super low resolutions *and* frame interpolation.  Temporal upscaling isn't without it's issues, and you know how much traditional denoisers leave to be desired. Even Nvidia's Ray Reconstruction leaves a lot to be desired; the thread on r/Nvidia shows as much, with ghosting, artifacts, etc. all over again. It's like the launch of DLSS 2.0.  All of that, for effects that are oftentimes difficult to perceive, and for utterly devastating traditional rasterization performance.",Negative
Intel,"It's a shame raster is so labor intensive, because it looks so interesting when done right.     I look at screenshots from cyberpunk, Raster / Ray Trace / Path Trace, and in most of them the raster just looks more interesting to me. Not constrained by what would be real physical lighting. The 100% RT render versions of old games like Quake gives a similar feeling.     But I imagine there will be more stylistic RT lighting over time, it saving a ton of labor is all things considered good, freeing it up for actual game design.",Positive
Intel,">That’s why devs love RT and want it to grow, as RT/hardware does the work for them.  It's the marketing departments that love RT. Devs hate it because it's just yet more work on top of the existing raster work.",Positive
Intel,"It *is* way slower, you're just compensating it by reducing render resolution a ton",Negative
Intel,If it was bloodborne i am guilty of that myself,Neutral
Intel,price zealous rinse detail yam rainstorm groovy automatic snails fact   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,4k max without dlss or fsr or frame gen I still get 30fps. Sorry you can’t even hold 10. Guess I’ll shill some more to at least not be spoon fed shit from amd,Negative
Intel,"Ray tracing and path tracing aren't just ""reflections"" , but this thread is specifically about the new path tracing modes in CP2077.",Neutral
Intel,Rasterisation is a “hack” too,Neutral
Intel,If it works it works....computer graphics has always been about approximation,Positive
Intel,"4090 is \~66% more powerful than 3090 overall, but it has 112% more fps in this test.     The RT capabilities of NVIDIA cards has grown faster than general capabilities.",Positive
Intel,"I guess on the internet, you can just lie. There’s no game on ps5 that upscale from 540p to 4k.",Neutral
Intel,"I guess I was wrong on that one, although SWJS upscales from a resolution as low as 648p, which is not much.",Negative
Intel,It's big in certain lighting situations. In others it's not as noticeable as you'd hope,Neutral
Intel,"PT vs no RT comp is silly.  U should compare PT vs RT, and there, the difference is minor and not even subjectively better. Just different.",Negative
Intel,Natural? It over saturates the light bleeding into the entire scene. Everything becomes so colorful. Even when floors or ceilings are not supposed to be reflective. Literally tiles with grainy texture suddenly becomes a mirror..  It's ridiculously overdone.,Negative
Intel,Guys I'm trying to suffer while I play the game what settings will let me suffer the most,Negative
Intel,"I mean this is native RT/TAA. Was never meant to be played this way. You need to use DLSS and Ray Reconstruction. With that setting, i get about 40-50fps at max settings with my 3080. Not too bad.  The thing about AMD is that they don't have any of this AI technology (yet). You have to rely on raw power, which won't get you far.",Neutral
Intel,"Until the next generation of consoles, I think path tracing will be added to something like a couple major games per year with Nvidia's help. If I'm right, that's wouldn't be a lot, but it's definitely be a bonus for those who have GPUs that are better at ray tracing.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode (which could be for a variety of reasons, including CP2077's larger world).",Positive
Intel,Alan Wake 2 is also an outlier since the same dev made Control which was a test bed for DLSS 2.0 and one of the earliest games to fully embrace ray tracing. Alan Wake 2 is also NVIDIA sponsored and looking to be another test bed for them.,Neutral
Intel,"Redditors trying to have reading comprehension (impossible)     >It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it).",Negative
Intel,"Every single game has cut corners, starfield has multiple cut corners. Tech demo would imply there's no great game under great visuals.",Positive
Intel,Get that fps with a 5120*1440 240Hz monitor,Neutral
Intel,"It's almost like crushing your fps with all these fancy new graphic features isn't worth it, even on the best cards. It's user preference in the end.  That said, many gamers, once they play at high hz screen and fps, find it very difficult to go back to low fps for any reason. Games feel slow by comparison, no matter how great the lighting looks.",Negative
Intel,"Both DLSS and FSR have weird image artifacts (FSR is worse though) so i guess if you don't see them then DLSS only works in your favor then.  I personally want as little noticeable image artifacts from DLSS/FSR when I play so opt for native resolution, generally.",Negative
Intel,Have you tried dlss quality. Dlss balanced sacrifices image quality and dlss quality doesn't/can enhance it sometimes,Neutral
Intel,">a lot of the comparisons just make it look like it gives too much bloom  This is due to colors being compressed to SDR. In HDR that ""bloom"" looks awesome as it's super bright like it would be in real life but still not losing any details. You lose those details only on those SDR videos.  PT just looks and feels way more natural than just RT, when only raster literally looks like it was two different games from two different hardware generations.  Once I saw Cyperpunk PT there is no way I'd imagine playing this game in any other way. All other games look absurdly ugly next to it now. It's something like when you get a high refresh screen and can't look back at 60Hz anymore.",Negative
Intel,"Trust me, I know how the technology works. That doesn't change the fact that it's not worth it imo. I'm not  paying 4090 money to have the newest fad that just makes games look over saturated and glossy.",Negative
Intel,We are talking about running the game just fine now are we? Because rasterization is just fine. I will consider ray tracing reasonable when you can do it maxed out for $1200(GPU) native. Until then it’s a gimmick. IMO,Positive
Intel,"So even you, someone who is an enthusiast of this sort of thing, i.e. a small minority of the userbase, had to spend \*minutes\* benchmarking to realize you didn't have RT turned on.   Yeah, you would lose that bet.",Negative
Intel,Those wolfdale C2D were beasts. I had a buddy with one and we took it 4.0. And it kept pace/beat my C2Q as most games didn't utilize MC as well back then.  Man XFX and EVGA all the way. Shame they both left the Nvidia scene.,Negative
Intel,"Ah, WoW. That started it all for me on an C2D Conroe with ATI x1400 on a Dell inspiron laptop freshman year of college.",Positive
Intel,8600gt in SLI man you are Savage!🔥,Negative
Intel,"I had the same setup, and i managed to get my Q6600 stable at 3.2.  Was a blazing fast machine and it still shit the bed in some parts of crysys",Negative
Intel,there there’s no way there will ever be another 8800 GT. you got so much for your money.,Negative
Intel,"Those 9800gtx+ were solid cards, it's what I upgraded from.  You make due with what you had.  My first system was a Craigslist find that I had to tear down and rebuild with spare parts from other systems.",Neutral
Intel,Seemed like a perfect use case for the sli bridge they got rid of.,Positive
Intel,Why would it need to send the data to the other card? They both feed into the same game.,Neutral
Intel,Big up the Vega gang,Neutral
Intel,"That is because destructible buildings and realistic lighting REALLY does not go hand in hand. Realistic looking games use a ton of ""pre-baked"" light/shadow, that might change when ray tracing is the norm but it has a delay so things still look weird.",Negative
Intel,"I remember playing Crysis, flipped a wheeled garbage bin into a hut, which came down on top of it. I blew the wreckage up with grenades and the bin flew out, landing on it's side. Took pot shots at the castors and the impact made them spin around in the sockets.   Here we are 15 years later and game physics have barely moved an inch from that",Neutral
Intel,Cause regular stuff is so easy to simulate or fake simulate that super realistic complex items are not really worth the trouble.  &#x200B;  water still looks like crap in most games and requires a lot of work to make it right.  and even more processing power to make it truly realistic.  Cyberpunk is perfect example.  the water physics is atrocious.,Negative
Intel,It's because it's not the big selling point now and as the comment you're responding to... Things have gone so far no one really thinks about it anymore. Ray tracing is what physx used to be or even normal map and dx9/10 features you don't consider now.,Negative
Intel,"Honestly, same here - I hate Nvidia so much for what they do so shamelessly to the average consumer that I’d struggle to recommend their products even if they *were* competitively priced.",Negative
Intel,"People are just stupid, if game companies just slap ray tracing on and don't tweak the actual look of the games lighting, gaming graphics is doomed. Rt is not the magic bullet people seem to think it is, everything looking 'realistic'=/= good art design.",Negative
Intel,"I thought RT was loved by devs, as it allowed them to avoid the shadow baking process, speeding up dev time considerably?",Neutral
Intel,We are guilty of the exact same sin.,Neutral
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"All rasterization does is approximate colors for all the pixels in the scene. Same thing does raytracing, path tracing, AI reconstruction and whatever other rendering techniques there are.   The final result is what's most important, doesn't really matter how it's achieved",Neutral
Intel,would you care to explain ? Kinda interested to here this,Neutral
Intel,jellyfish follow simplistic plucky quarrelsome unwritten dazzling subsequent mourn sable   *This post was mass deleted and anonymized with [Redact](https://redact.dev)*,Neutral
Intel,"It isn’t minor. The PT with DLSS 3,5 is quite a bit more accurate and reactive, as in dynamic lights are reacting closer to instantaneous than without. The best way is to watch on a 4K monitor or to actually play it on someone’s 4090/4080 or wait for AMDs FSR 3 to see if that helps it do path tracing.",Positive
Intel,> Literally tiles with grainy texture suddenly becomes a mirror  PT doesn't change what the materials are bro. You are just seeing what the materials are meant to look like.,Neutral
Intel,Turn on path tracing. Embrace the PowerPoint.,Neutral
Intel,Control also has some of the best implementations of RT I've ever seen. It's one of the very few games I've turned on RT and felt like it was worth the performance hit. Everything else I've played has looked very good with RT on but not good enough to be worth the performance hit.,Positive
Intel,"Of course it is not perfect but it also allows max RT, PT, RR which improves the graphics greatly...and i do not enjoy below 100 fps either",Negative
Intel,"I tried 1080p DLSS Quality and it was just as bad, but I’m not really happy with the performance of 1440p DLSS Quality (like, 30-40fps), so I didn’t want to use it.",Negative
Intel,"You're is coming off as salty that you *can't* afford a 4090. If other people wanna spend the extra cash and turn on all the bells and whistles, why does that bother you?",Negative
Intel,"Mate, you can't fucking run starfield at good fps, a game that's raster only that looks like from 5 years.  Look at how many games scale and run badly in last two years and compare that to cyberpunk",Negative
Intel,"This just absurd and frankly entitled. Not everyone can afford a PC strong enough to run the most demanding games absolutely maxed out.   I never could, couldn’t even afford a PC until this past summer.   This obsession with performance metrics is ridiculous. Like, less than 5% of PC gamers even have 4k monitors. Almost 70% still have 1080p.   CDPR made a VERY demanding game that looks GORGEOUS completely turned up. Modern hardware cannot handle it natively, it can’t. It is not a gimmick that a company has developed a superior technology that allows the game to be played at almost the same quality as native, significantly smoother and faster. You’re in denial if you think otherwise.  I’m not tied to either brand. I have an AMD CPU bc it was the best option for what I wanted, and a Nvidia GPU for the same reason. I care about RT, frame gen, all of that. The Witcher 3 still is my favorite game, and the entire reasoning for building a gaming PC over a console is graphical quality and speed while doing so. That game is absolutely gorgeous with RT on. Same with Cyberpunk, which I can enjoy thanks to DLSS.   You don’t care about those things? That’s fine, but it is solely a personal opinion.",Negative
Intel,"Yea well I was defining an arbitrary performance. It's not about having native performance. But, but if a flagship is able to get 4k 60 with path tracing at native then that allows headroom for 60 tier or 70 tier cards to perform well.  In this example the 4090 gets 19.5 fps at 4k. The 4060ti gets 6.6 fps. Let's assume a next gen flagship is able to reach 60 fps at 4k. That would theoratically allow the 60ti card to reach around 20 fps. Once you do your various upscaling and frame generation techniques you can then reach 60 fps ideally with a mid range card.",Neutral
Intel,"The quad core showed up the core 2 duos like 2 yrs after because of emerging support and I was on cloud 9. After evga left, i jumped ship with my XFX 6900xt and I think I am here to stay with team Red until value/longevity says otherwise.",Neutral
Intel,Shame you don't. They did it for a reason.,Negative
Intel,The 9800GTX was a bad ass. I'd consider it the biggest upgrade in generation until the 1080 and then the 4090.  It was far more powerful than anything near it.,Positive
Intel,"I do miss it, but it was a pain in the ass to get working with my custom cooler due to the HBM.",Neutral
Intel,They've actually gone a bit backwards. Devs dont seem to care about implementing physics anymore. It's just an afterthought. And you can forget about destruction,Negative
Intel,"I gobble up the downvotes every time I say it and I don't give a crap.  Every other corporation sees through their bullshit.  Otherwise every console would have NVIDIA cards.  No console has an NVIDIA card anymore.  But the general population is more than willing to get bullshitted into buying their cards against their own interest.  EDIT: Forgot about Nintendo Switch, as is tradition.",Negative
Intel,"Anyone can say anything.  Anyone can change their intent after the fact.  Happens all the time.   Judge them by their actions alone and ignore what they say.  Going solely by **events** :   1) People use (mostly used) NVIDIA cards for driving PhysX   2) NVIDIA disables this ability, locking everyone to the god-awful CPU implementation at the time   3) Modders edit DLLs to get functionality back   4) NVIDIA THEN says it was a bug that will be fixed    It's not foil hat because I'm coming to this opinion based solely on what's measurable and real; events and actions.   It's not conclusive, but I'm not going to rely on any company's PR statements to sway me one way or another because they will (obviously) say whatever they're able to prevent lawsuits and public image damage.  Based strictly on **events** that looks like damage control on a bad decision.  They rectified it, even more recently made PhysX open source, I give them credit for making it right, but the intent at time time was clear.",Neutral
Intel,This is why I'm excited for Alan Wake 2. It is coming out with path tracing always having been part of it. This will definitely mean they built the art style around that.,Positive
Intel,"Yeah, I can't really blame people for not recognizing great design if they are not interested in it. I don't think most people are interested in design, even though we all benefit from it without knowing it.     People might get some sort of subtle feeling when they see some great craft made by a experienced professional, but they can't really put it into words or evaluate the quality of the design without experience. They likely can't tell what cause the feeling to begin with even if they can notice it.     But everyone can instantly judge how real something looks, which is the go-to standard for ""good"". And bigger numbers are better, even if they are fake. Niche features that nobody uses also sells, as long as it's a new feature with a captivating name.     This reminded me. Live action lion king, what a travesty, stripping away all the artistry and expression for realism.",Neutral
Intel,"If you're using RT exclusively maybe, but no one in the games industry is - nor will they be any time soon.",Negative
Intel,"A lot of raster techniques reduce the complexity of lighting into very naive heuristics. Like shadow mapping renders the scene from the light's pov to create a depth map, then the objects that are at a greater depth have darker colors drawn (I'm simplifying that but roughly how it works). It's like teaching a kid how to illustrate shadows on an ball vs the how light actually works. Raster techniques have evolved a lot since then, and use raytracing in limited contexts, screen space reflections and ambient occlusion for example, but they're still operating with extremely limited data in narrow, isolated use cases, which is why they often look awful. There's just not enough information for them to look correct, just enough to trick you when you're not really paying attention.",Negative
Intel,All lights in a rasterised scene are “fake”.   Someone explains it much better here:  https://reddit.com/r/nvidia/s/eB7ScULpih,Negative
Intel,Or a bag of fake tricks.,Negative
Intel,">Every scene is lit completely differently with PT enabled.  It's really not, and it's very disingenuous of you to try and pretend it is. There are plenty of screenshot comparisons showing the difference in certain areas if you want me to link them, and I have the game myself and use a 3090.  There are differences, but saying ""every lit scene looks completely different"" with PT Vs regular RT is false.",Negative
Intel,"I'm not talking about DLSS 3.5. I'm talking about PT vs regular RT in CP2077. The visual difference is subjective, it looks different, is it better? Not in my opinion, it over saturates the scene.  DLSS 3.5 is a separate issue, it fixes some of the major flaws of PT, the slow reflection updates.",Negative
Intel,Have you tried Metro Exodus Enhanced?,Neutral
Intel,At less than 4K resolutions Quality is the only sensible option. The resolution it upscale from becomes too low otherwise and DLSS then struggles to have enough data to make a good image.,Negative
Intel,"It doesn't, I have no problem with people buying what they want. My problem is people assuming I can't afford something...",Negative
Intel,What is this? A reasonable comment in this dumpster fire of a sub?,Negative
Intel,Thinking that a $1200 graphics card should be just strong enough to run everything Max is entitled. I wear that crown.,Positive
Intel,"I've got an evga 3080ti now. I suspect my next card is either going to be an XFX 8800xt, or catch an XFX 7900xtx on clearance.",Neutral
Intel,"I don't blindly support a single company, such as simply switching to AMD because my chosen vendors left nvidia. That's how competition stalls.  That being said the card I have now is a product of what I could get my. Hands on during the crypto boom, and is EVGA.  My next card is likely to be an XFX 8800xt/7900xtx.  So not sure why you're in here attacking me.",Negative
Intel,Except it was just a rebadged 8800GTX/Ultra,Neutral
Intel,"I mean, the Nintendo Switch is using Nvidia graphics, albeit it’s a tiny 28nm iGPU from 2014… but I get your point. They’ve opened up a large technological lead over AMD thanks to the RDNA 3 debacle, and I’m *still* recommending only Radeon GPUs because Nvidia is just that hubristic.",Neutral
Intel,Pixel art go brr,Neutral
Intel,Yeah they will. Lumen and lumen style gi systems will have to exist for fallback so the industry can move on.,Neutral
Intel,"Yeah, I agree. It’s just a shame because it’s not really playable otherwise. DLSS with ray reconstruction is a little mixed.",Negative
Intel,"It just doesn’t make sense. Game developers do not set GPU prices.   The game runs well on a $1200 GPU. Perfectly well. Not completely maxed though, and it’s weird to think the game shouldn’t be able to make full use of available technology if it wants to. My GPU cost less than half that and runs the game very well with RT on thanks to DLSS.",Negative
Intel,"RR is not perfect, but I think overall Cyberpunk 2.0 looks so much better than it did previously - and it looked pretty damn good earlier too!  [Digital Foundry's video](https://www.youtube.com/watch?v=hhAtN_rRuQo) is again the yardstick for how many small things they point out where RR improves the situation, even if it has some things where it fails atm. But maybe DLSS 3.6 will solve those.  I do feel DLSS itself has also improved in the past few years in terms of ghosting and performing at lower resolutions.  On a 1440p monitor these are the starting point resolutions:  - Quality: 1707x960p - Balance: 1485x835p - Performance: 1280x720p - Ultra Perf: 853x480p  So even the highest quality option is sub-1080p. I wish Nvidia introduced a ""Ultra Quality"" preset that would be say 80% per axis resolution, something between DLAA and DLSS Quality. At 1440p this would be 2048x1152.",Positive
Intel,"If only Intel had stayed in the memory business!   They'd be enjoying Micron valuations and wild profits and performance from copackaged CPU+GPU+LPDDR of their own design and manufacture...     But no, they'd rather invest billions in buying donuts as a service, or whatever their crazy investements went into.",Negative
Intel,"damn an iGPU using 32GB of vRAM, I wonder if they're testing a Panther Lake laptop with 48GB RAM or even more (since X7 & X9 Panther Lake only accepts soldered memory)",Neutral
Intel,So there's a 20GB variant. A 28GB variant and a 32 GB variant?,Neutral
Intel,"If Intel is really about to release a B770, honestly the **only thing that could make it competitive is the price**. (FOR ME, competitive in 2026 means <400€) From a performance standpoint, it would need to undercut existing GPUs quite aggressively to make sense, especially given how crowded the mid-range already is.  That said, I’m pretty skeptical about how realistic that is. **With the recent RAM shortages and rising memory costs**, pricing a new card competitively while still keeping margins doesn’t sound easy at all. Memory is a huge part of the BOM, and we’ve already seen how shortages can push prices up across the board.  So unless Intel is willing to take a serious hit on margins (which seems unlikely), I’m not convinced the B770 will land at a price point that truly shakes up the market. Happy to be proven wrong, but for now the pricing question is the big unknown for me.",Neutral
Intel,Optane was practically **built** for the type of AI workloads that they're shoveling money at.  If Intel didn't give up literally only a matter of months before GPT released and the bubble began in earnest lol,Neutral
Intel,"If Intel stayed in memory business, it would be long dead in the 80s and killed by Japanese memory companies. CPU remains the top niche area with less competition and deeper moat. See how China has quickly come up with their GPU designs? Well it will take at least another decade for them to make 2nm CPUs",Neutral
Intel,"are people high or something? intel was losing money on optane and their SSD business became irrelevant the minute regular memory manufacturers slammed the market. don't get me wrong, they were some of the most durable on the market, but they were no where near printing money on the memory business.  optane may have survived if their nodes were on schedule, keeping CXL support on schedule, but not because it was profitable.",Negative
Intel,"Their iGPU's since at least Meteor Lake could use 50-75% of system ram, soldered or socketed.   Same issue as with Strix Halo where the iGPU is anemic enough to not be a real option.   Arc GPU's have a similar option that can use up to 50% system ram as expanded vram (with the obvious performance hit)",Neutral
Intel,"it's not like any of this AI garbage right now is profitable for anyone except nvidia and the hardware companies anyway, it's not stopping everyone from shoveling money into it",Negative
Intel,"> With up to 192GB of VRAM across eight GPUs in a single system, Battlematrix positions itself as a relatively cost-effective alternative to other professional GPU ecosystems for AI inference workloads.",Neutral
Intel,Hope they do some image and video generation benchmarking as well. Nice to see someone testing AI rigs out there.,Positive
Intel,Wish they’d give prompt processing speeds. AI coding generates very few tokens compared to input. Nvidia seem to dominate here.,Neutral
Intel,"How many concurrent users will this serve, 30 devs would be nice",Positive
Intel,:),Neutral
Intel,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Negative
Intel,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Neutral
Intel,I hope it comes to desktop CPUs,Positive
Intel,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Positive
Intel,Yeah this headline doesn't add up based on my own testing,Negative
Intel,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Negative
Intel,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Positive
Intel,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Positive
Intel,concur  some benchmarks are biased,Neutral
Intel,lateral,Neutral
Intel,If it’s just “16% faster than 890m” it’s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Neutral
Intel,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Neutral
Intel,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Positive
Intel,Answer to strix halo was the partnership with nvidia,Neutral
Intel,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Neutral
Intel,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Neutral
Intel,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Neutral
Intel,"I mean no offense, but Passmark is irrelevant.  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Neutral
Intel,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Neutral
Intel,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Neutral
Intel,"I guess we'll see more when we get actual info about the potential devices.  Right now, I haven't read about a device coming to market.",Neutral
Intel,Will be a interresting CES,Neutral
Intel,I'm half-expecting this to show up as a server-only AI-focused SKU with video outputs removed.,Negative
Intel,Merry Christmas everyone,Neutral
Intel,"4070 performance for $400, I'm calling it. Would have been great if this had come out right after the wave of negative press that the 5070 received for only being 10-15% better than the 4070 with a mediocre 12 GB of VRAM, but I feel like Intel missed the boat again if the Steam Hardware survey is anything to go by, the 5070 has really made a comeback with recent sales.",Neutral
Intel,They can't even ship B60's.,Negative
Intel,"What is taking Intel so long?      It's already been almost a year after Battlemage's initial launch. And for what? RTX 5060 performance at the same price with some extra VRAM?  I had really hoped Intel would be able to gain ground on their competitors. At this rate, we'll get the ARC C770 to compete with the RTX 6060 in another 3 years.",Negative
Intel,Aren't they always,Neutral
Intel,give it some time...,Neutral
Intel,Merry Bitmas,Neutral
Intel,"4070 performance for... used 4070 price, now with driver issues and an objectively worse upscaler!  intel greatest hits",Negative
Intel,Sure they can if you search for it   B60  https://www.idealo.at/preisvergleich/OffersOfProduct/207972918_-arc-pro-b60-sparkle.html   Or b50 https://geizhals.at/intel-arc-pro-b50-a3584363.html,Neutral
Intel,"Intel's GPU division has been operating at a loss never mind Intel as a whole and ARC series cards aren't as popular as the enthusiast circles would have you believe. Coupled with how expensive R&D is for things like GPUs, it's hard for them to pump out a competitive product while remaining just profitable enough to undercut AMD and Nvidia.",Negative
Intel,What is taking Nividia so long with the super cards?,Negative
Intel,"Battlemage gpu chips are made through TSMC and Intel is getting screwed on supply, this is why even if the B770 comes it will only be a small amount. Hopefully Intel can put together enough rare earth to pump out discrete Celestial Gpus but it takes time to ramp everything up. In addition Intel has their chiplet design, EMIB that could take off soon. They may be able to bring Apple back into the fold, but let us hope Discrete Arc lives on.  I have learned to not have expectations for anything that is outside my direct control, I do the best I can to just go with the flow. Whatever will be, will be.",Neutral
Intel,There's definitely been lame ones.,Negative
Intel,"I'm fully aware it's not a great deal, but that's my expectation when it comes to Arc.",Positive
Intel,"These are European links and will be out of stock. I found a mom and pop place back in my old stomping grounds in San Francisco, and they normally only sell B60s in prebuilt systems but a special order is possible.   Intel can't rely on TSMC for Battlemage supply, so let us pray that Discrete Celestial GPUs are made (entirely) at IFS and release in 2026 / 2027.   May your Bits Byte Hard, long live the Arc.",Neutral
Intel,"If CES 2026 comes and goes without any details for Discrete Arc GPUs then it could be awhile. The main thing being promoted is Panther Lake which should be made entirely at IFS, a step in the right direction. The TSMC monopoly is destroying the industry and it hurts companies here in the US.",Neutral
Intel,Not sure how the link being European matters. They are European shops and the B60 is in stock.,Neutral
Intel,"We kind of know where it will land. It will be a 3050M level chip, maybe a bit better, but will have improve scaling and frame gen.   8060S is 40 RDNA 3.5 units. One Xe3 unit is about 2.1 RDNA 3.5 units. That put it at about 65% of Strix Halo, though it will have a worse memory bus and no MALL cache. Somewhere in that range.   So that 60% is almost bang on the 3050M. Maybe a bit better. It won’t be like the 4050 but 3050M isn’t bad for an iGPU that fits in a normal socket",Neutral
Intel,"> and no MALL cache  PTL does have 8MB of memory side cache, fwiw.",Neutral
Intel,"I don’t know how this score compares to the 3050M. I only know that this score is about 55% of the B580. And the B580, at 2K and 4K, is about 1.7–2 times the performance of the desktop 3050",Neutral
Intel,and also an iGPU won't be stuck with 6GB of vRAM 😅,Neutral
Intel,Really don't understand why they don't go with a larger cache.  Pretty sure they still have a bunch of cache chunks spread all around the SoC.,Negative
Intel,"in configs without die-to-die memory performance in general should be worse if bandwidth limited. despite not being specifically dedicated to onboard memory like Lunar, people are still planning configs with local LPDDR5x, though peak bandwidth is limited by a 128bit bus.",Negative
Intel,I see it personally as not really being at 20W if you’re asking much from the GPU. It’ll increase the juice dynamically if it gets demanding enough. So it’ll be hard to say unless you force the power limits way down manually.,Neutral
Intel,"I actually like the idea of discrete GPU naming scheme for the new iGPU, 300 series for integrated graphics is really makes sense but they should add 'M' suffix to make it clear.",Positive
Intel,"I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  Also looks like the 3\_8 and 3\_6 versions are differentiating maximum boost clocks, though I wonder if instead those may reflect that configurable upper TDP bound. Might make sense for 65W and 80W to be differentiated if that will coincide with anything about the ""experience-based"" PL1 behavior.",Neutral
Intel,There are 2 dies.  One for professional workload which will be mass produced.  Second is for gaming. Even a 10-12 core xe3 will be barely enough for modern 1080p. Lunarlake can only run alanwake at 1080p at low settings getting only 25 frames so even if this is 50-100% better this is the minimum for a 2026 product.   I see no point of a 8 xe3 core system when all people will do is just complain.,Negative
Intel,"quite confusing, Xe3 should start from C (celestial), if using name like B390  we think this  is a battlemage Card (Xe2)",Neutral
Intel,"I wonder if rumors about Zen 6 clocking way higher than current cpus turn to be true, and the 5.1ghz max on mobile PT mean Amd might have an edge in next generation   Only time will tell",Neutral
Intel,please add M for Mobile or i for iGPU  * Arc B390M Xe3 Graphics * Arc B390i iXe3 Graphics,Neutral
Intel,"A clock speed regression vs the prior gen on N3B, with a remark that it's difficult to cool, really isn't a good look for the process side. 18A branding with more like N4 performance...",Negative
Intel,Will the 10 core Xe be better than radeon 890m or worse?,Neutral
Intel,"i mean i get this is a laptop part but man 16 threads is not much to phone home about when it comes to horsepower, isnt next gen desktop aiming for something like 48 threads?",Negative
Intel,Still weaker than x3d,Neutral
Intel,Yes indeed we need that M&M. Mobile platforms are not a priority for me and are dedicated mobile gpus really comparable to Big Boy Discrete GPUs? It is very confusing.  Lunar Lake laptops should fall in price. Has anyone used Lunar Lake and if so which models? Buying latest gen is for guinea pigs and the rich!,Negative
Intel,Doesn't the B already serve that purpose?,Neutral
Intel,">I'm surprized to see nothing between 4 and 10 Xe3 cores. Seems like there's a lot of room in there for something like an 8-core B360 iGPU or even a 7-core B350.  The 10 Xe3 core model is a binned down 12 Xe3 N3 die, and I doubt yields are so bad that they would even be able to find more dies where they have to disable more cores.   The other die is the 4 Xe3 Intel 3 die, so you can't go up from there.",Neutral
Intel,To be fair Alan Wake 2 low settings look great. This was covered by DF awhile back they said in some ways Alan Wake 2s low settings look better than some modern games high.,Positive
Intel,"I mean, but that logic, most of Intel's historical bigger iGPUs don't make sense. There are use cases other than AAA gaming. Media creation is another big one.",Negative
Intel,"Xe3 is not GPU family name but GPU core architecture, it's like Nvidia Ampere, Ada Lovelace. But Alchemist, Battlemage, Celestial is GPU family name.   Panther Lake 12Xe3 being B series GPU makes sense because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture. Intel confirmed Celestial will have Xe3P.",Neutral
Intel,"Think of it like AMD Zen X+ nodes. Ryzen 8000 is more or less a laptop only APU series on Zen 4+.   Xe3 is a half-generation, it doesn't get the letter upgrade to C, but it gets the 3, signifying a new architecture, but not a new generation.  Zen 4+ is a half-generation, it doesn't get the number upgrade to 5, but it gets to be 8000-series, signifying a new architecture, but not a new generation.",Neutral
Intel,"Mobile Zen 6 is likely to come around the same time NVL does. Both should use N2 and will presumably have similar frequencies, well above any 18A parts.",Neutral
Intel,"Considering the timing of 2nm, zen 6 would be around late 2026, and mobile zen 6 late 2027 wide availability. for whatever reason amd takes forever despite high mobile demand, but this quarter it looks like it worked out for them (maybe a big bump up from x3d sales).",Neutral
Intel,What're you doing on a laptop?,Neutral
Intel,"These are for thin and light office notebooks and light gaming. Think Lunar Lake. For CPU power, Nova Lake H will exist.",Neutral
Intel,It's for handhelds and office laptops not hyper enthusiast shit.,Negative
Intel,Nobody buys AMD laptops,Neutral
Intel,">Still weaker than x3d   Source?? Also Panther Lake is H series only, HX will be based on Nova Lake.",Neutral
Intel,Yeah so it is weaker for gaming with a dGPU than the 0.2% of laptops currently sold that have either a 7945hx3d or 9955hx3d that makes up for less than 0.1% of all laptop users. What's your point?,Negative
Intel,"While I haven't used it daily or anything, and I've only done initial setup on the Lunar Lake, the feedback we've gotten both on Arrow Lake and for Lunar Lake (e.g. 268V and 265H) Dell models is that it's a big increase in battery life and performance. The integrated graphics (e.g. 140V and 140T) are very capable compared to a **workstation** grade NVIDIA Ada 500 GPU, but they are not even comparable to a gaming GPU like the GeForce 4060 or even a 5050.  The integrated graphics do however get used for 99% of all workloads unless explicitly specified because they are vastly more battery efficient and draw less power compared to a dedicated NVIDIA chip, meaning you can have a much smaller external power supply, and your graphics performance in those basic desktop workloads with one of these chipsets will be **much** better than previous generation Intel chips. Exceptions are obviously something like gaming or AutoCAD that specify to use the high performance dedicated graphics chip.  140V/140T are barely functional for modern AAA gaming, but if you stay 5-10 years back for AAA titles you might be okay. It will smoke most Indie games. Just look at the per title benchmarks for a 140V/140T and you can see if your game benches. You could probably get away with a lot of functional mobile gaming without a dGPU, but I wouldn't expect to be able to play a recent Call of Duty or Black Myth or anything with anything like an acceptable framerate at a decent resolution. This integrated graphics chip compares very favorably to its more common Ryzen 7 equivalent, I believe the 780M, and it's a very good APU for handhelds overall due Lunar Lake's power efficiency compared to other X86 chips.  You have to understand that for these next two generations Intel seems to be making big strides in terms of both power efficiency and integrated graphics for mobile, it's a very attractive option and the first time I've seriously considered a laptop without a dGPU. I think Panther Lake is going to be a very nice kit next year for both laptops and handhelds and give AMD a run for its money.   I suspect AMD genuinely needs a new APU graphics architecture implemented next year to keep up, which I expect them to. Not a bad problem to have.",Positive
Intel,"Yeah for sure. It's a small die and should be yielding pretty high. See also the number of 4+8+4 SKUs. Looks like the larger CPU tile is also yielding decently, so not a ton to cut down.  I'm partly saying that because a larger Intel3 die was certainly possible. Even if it was 6 Xe3 cores and built as half of the larger die (just one of the two render slices)  it would fill the void a bit more.",Neutral
Intel,Which is rather silly. They should've named celestial Xe3 and the current Xe3 as Xe2P,Neutral
Intel,"> But Alchemist, Battlemage, Celestial is GPU family name.  Specifically, *discrete* GPU family name.   > because Xe3 is just enhanced version of Xe2, unlike Xe3P which is entirely new architecture  That is simply not true. Xe3 brings much bigger changes over Xe2 than Xe3p does over Xe3. That's why they were named that way.   > Intel confirmed Celestial will have Xe3P.  No, they actually haven't said anything about Celestial (again, as a dGPU) at all. They said that C-series naming (i.e. NVL iGPU) will start with Xe3p.",Neutral
Intel,Highly immersive porn on the go,Neutral
Intel,"Idk, FEM sim of a pressure boiler?",Neutral
Intel,These are H series chips. Even the U series chips don't go down to LNL min power levels.,Negative
Intel,"NVL-H is 400 series, *replacing* this, next year. Not supplementing this lineup.   Adding more cores won't do anything for gaming.",Negative
Intel,PTL extends up to the -H series too.,Neutral
Intel,"HX this year will still be Arrow Lake.   Nova Lake will be a full line up, with S, U, H, and HX, but end of 2026 / early 2027",Neutral
Intel,">Source??  You can't seriously be asking for a source for whether or not this part will be able to power dGPU gaming laptops better than X3D chips.   >Also Panther Lake is H series only, HX will be based on Nova Lake.  Not till late next year or early 2027. It's all arrow lake till then.",Negative
Intel,> Source?   Common sense suffices. It's a tick core with a clock speed regression at that.,Neutral
Intel,"Surely a cost decision. The 4Xe die, including the choice of Intel 3, is supposed to be the cheapest thing to deliver an acceptable mainstream PC experience. They need PTL to be a proper volume runner and start displacing the RPL that's still a large chunk of sales. WLC should hopefully finish the job.",Neutral
Intel,Just get a Vision Pro?,Neutral
Intel,"Tbh, more cores would just make that go faster, but 16 would already be plenty. Especially for something like that where it's probably going to be a linear analysis and ram constrained if they actually modelled the gas (which would not necessarily be required).",Neutral
Intel,Lmao this is funny we both responded to the same comments with the same things within like 2 minutes of each other.,Neutral
Intel,I don't see anything wrong with asking for actual benchmark information especially when there isn't anything official. X3D is nice but it isn't the end all be all. I would be curious to see if Intel can manage to compete.,Neutral
Intel,"Oh I totally agree, but it would've been nice you know? Jumping to 6 Xe3 is a  significant area increase for a tiny tile. I understand exactly why the 4-10 gap exists, but I can't say I don't wish there was something to fill that gap if only because it looks weird.  I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.",Neutral
Intel,"PTL's main changes are fixing MTL/ARL's terrible SoC design, which should net a few % performance. It'll see a mild IPC increase, getting a few more % performance. And it'll lose a bit of clockspeed, erasing most of those gains.  Expect PTL to be very similar performance to ARL, but with lower power consumption, a much better iGPU, and most importantly to Intel: Using their own fabs instead of TSMC.  It absolutely won't be X3D in gaming.  Edit: Actually shocked that people think this would compete with X3D.  9955HX3D is \~16% faster than a 275HX in gaming...and a 275HX itself is easily 10%+ faster than a 285H in gaming.  Not even Intel themselves are claiming this. Their own marketing refers to PTL as ""ARL performance with LNL efficiency"". Nobody realistically expects PTL-H to see a 25%+ gaming improvement over ARL-H. The fact that IPC increase is less than 10% and clockspeed is slightly lower than ARL-H should make this obvious",Neutral
Intel,CGC is a LNC tick. This is well known at this point. And we see it's even a clock speed regression.    Even entertaining the notion it will close the gap to AMD's X3D chips is just delusional.,Neutral
Intel,"Oh, yeah, I get you. Wish they could give more granularity. Just personally think some sacrifices are worthwhile if it can condense Intel's mobile lineup back down to something sane again.   > I expect the big CPU tile and small GPU tile pairing will find its way into a number of gaming laptops as the iGPU performance is pretty low on the priority list for those.  Yeah, should be a good fit. Shame they don't have anything with a bit more CPU umph, though. 4+8+4 and only up to 5.1GHz is *fine*, but not great. Especially without an HX replacement.",Neutral
Intel,"Two options seems right, either you care about it or you don't.",Neutral
Intel,"Its not ""delusional"" to want to see actual numbers instead of speculation. I have been in this game long enough to see plenty of speculation even with accurate information not give the actual numbers.",Negative
Intel,"Given how well ARL HX was received in gaming laptops, I think they may wait to have something from NVL take that top spot. 5.1ghz does seem low though. ARL-H will happily do 5.4 and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP.  I suspect these may not be totally final clocks though they do seem reasonable.",Neutral
Intel,"Honestly, surprised ARL-HX is doing as ok as it is. The deficits of the architecture in gaming are well known. If it could hit the same clocks and core counts, PTL should look a lot better still. And all that besides, ARL's cost structure is horrible. For Intel's own sake, the sooner they move on, the better.   > and 18A should clock decently, though I suppose that's a sacrifice for the 25W TDP  From the same leaker, these chips are at 65W or even 80W TDPs, so they're not merely power limited. It seems that 18A just significantly underperform some expectations, though in line with some rumors and the gist of the revisions Intel's been making to its projections over the last year or two.  > I suspect these may not be totally final clocks though  If they're defining SKUs and such, these clocks need to be finalized for all practical purposes.",Negative
Intel,"If I'm reading correctly those are max power limits, not the TDP,  though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  As for why ARL HX is doing well in gaming laptops, I think a good bit of that is also part of what made it lackluster on desktop. It doesn't really scale up that well with higher TDPs and power limits, but it does seem to scale down. The 285HX with its 55W TDP and 160W max limit doesn't perform far off the 125W TDP and 250W max of the 285K.  It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system. The 9955HX3D is very impressive, but quite a lot of laptop buyers seem to value the ability to do more laptop-like things with their gaming laptops than the extra frame rate. I'm hoping this gets shaken up as AMD adopts new packaging tech as seen in Strix Halo.",Neutral
Intel,"> If I'm reading correctly those are max power limits, not the TDP, though I get what you mean. The 285H sits at a 45W TDP and tops out at 115W. Cutting 35-50W from that has to come from somewhere.  Not guaranteed given it's just a twitter leak, but I'm assuming the leaker is using the term TDP consistently with Intel's historical usage, i.e. PL1. For ARL, 115W is PL2. I would also assume there wouldn't be the disclaimer about it being hard to cool if they cut the PL2 so much, though PowerVia does create some interesting complications there, so maybe not quite apples to apples.  Either way though, don't think it should have much impact on ST boost. You're talking a good 70%-ish of power going to compute, so even at 65W PL2, that's still 40-50W available for one core. Should be *easily* sufficient to hit whatever the silicon is capable of.  > It also offers much better battery life than AMD's offerings in that space. I credit this to the lower interconnect losses in something like Arrow Lake than with Fire Range's chiplet system.  Yes, and this is something I've very much looking forward to with NVL-HX. At this point, the biggest demerit of the -HX platform vs -H is the use of standard DDR5 vs LPDDR. That's because it's still based on the desktop silicon with the different SoC/hub tile. But with NVL using a shared SoC die, they should be able to offer an -HX platform with the core counts people expect (though probably limited to single die 8+16), but the power/battery life advantages of -U/-P/-H. In general, should help make the -HX more of a straight-up upgrade than the tradeoffs one faces today.  AMD has this situation even worse today, because there's a much bigger gap between their desktop SoC architecture and the mobile one. Though as you say, they may also bring them closer together in the future.",Neutral
Intel,Try the shunt mod,Neutral
Intel,"Cool, errr...  icy",Neutral
Intel,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Neutral
Intel,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Positive
Intel,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Neutral
Intel,did you use dry ice? how did you hit sub-ambient?,Neutral
Intel,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Neutral
Intel,Are you in the US? If so how were you able to get Maxsun?,Neutral
Intel,Oh... for sure 😁,Neutral
Intel,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Positive
Intel,Great work dude! Only 200MHz to go 😉,Positive
Intel,Car coolant in the freezer 😁,Neutral
Intel,That's the way! Let us all know the results.,Positive
Intel,I am in Australia.,Neutral
Intel,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Neutral
Intel,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Neutral
Intel,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Positive
Intel,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Neutral
Intel,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Neutral
Intel,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Negative
Intel,Oh! You put the car coolant to run through a freezer? Wow! Nice,Positive
Intel,But Core Ultra 255Hx is almost $150 more than Zen 4 8945HX on lenovo Legion. That price gap is enough to upgrade 5060m to 5070m.,Neutral
Intel,"Can I redeem the codes to my accounts on a different Intel system? I bought a B860 motherboard and an Ultra 5 245k, but I won't be building that system till Christmas. I'm currently running an 8700K on a Z370.",Neutral
Intel,Idk if I did it wrong but redeemed my cpu but not my arc card on the website. Couldn’t contact support because it kept throwing invalid captcha at me.,Negative
Intel,just purchased a laptop from micro center with a 275hx but don't know how I would redeem this specific offer as I only get the one that lets you pick 1 of 4 games. Does mine not qualify/is micro center not participating?,Negative
Intel,"nope, you need to have installed Ultra processor to get promo game, because Intel used software to check it",Neutral
Intel,I really would like to know how it will compare to AMD Ryzen AI MAX+ 395 (Strix Halo) APU?,Neutral
Intel,That naming scheme really is complete and utter dogshit,Neutral
Intel,Sounds like fooz will be getting a taste soon? Christmas or Q1 2026?,Neutral
Intel,Can we just toss random darts at tech words and numbers to assign a different naming scheme to each and every different Intel Product?,Neutral
Intel,I'm looking forward to check how those series will perform!!,Positive
Intel,"This isn't a ""halo"" product. An upgrade to Arrow Lake/Lunar Lake when it comes to iGPU, but not Strix Halo. It won't cost as much as well.",Neutral
Intel,look forward to new APUs,Positive
Intel,Wait for Nova Lake AX. It will have 48 GPU cores instead of 4/8/12.,Neutral
Intel,Likely will be worse since it has only 1/3 the power usage and significantly smaller chip size.,Negative
Intel,"It's meant to confuse you in purpose, so you ignore it and go by the 3/5/7/9 scheme. Marketing success is dependent on the company leading the customers to the way they want it. So it needs to be complex and confusing.",Negative
Intel,"also why is everything in a lake. like really that's the last place you want your chips to be.  and why is it 255k, 285k and so on instead of just 250k and 280k.  and why is it arc xe3 and not just xe3 ahh  so many weird conventions really. no consistency at all. just pure confusion.  apple and nvidia get it right too, it's really not that hard.",Negative
Intel,"'Strix Halo' is lees about being a halo product, because it’s just an AMD internal naming scheme for the set of specific APU products. APUs that will follow 'Strix Halo' will be called 'Gorgon Point'. Since 'Strix Halo' is already out and the Panther Lake in question is about to come out it might be better to compared it to the next AMD 'Gorgon Point' APU line? But I still think it can be compared to 'Strix Halo'. At least I’m looking forward to see the comparison.  Edit: changed 'Strix Point' into 'Gorgon Point' since that’s what I meant.",Neutral
Intel,You’ll be waiting till 2027 on the amd side.,Neutral
Intel,All halo iGPUs are way too overpriced. Even regular iGPUs are overpriced going into $1K laptops.   And if you want to spend that money you can do it today with Strix Halo.,Negative
Intel,"Lesser power usage doesn’t always equals to performance losses.  See the Apple Axx SOCs and the Qualcomm snapdragon SOCs.  Even with the x86-CPUs over time they had more performance gains while maintaining almost the same power consumption.  Not anymore, but they could catch up with better chip design?",Neutral
Intel,>also why is everything in a lake. like really that's the last place you want your chips to be.  Easier to cool them,Negative
Intel,"Stop trying to find meaning in the naming scheme. Patience grasshopper, all will be revealed in time.",Negative
Intel,Like how is every chip company so bad in naming stuff?  Intel names things from geographical stuff to avoid politics etc. So I get the lakes  The numbers I dont.,Negative
Intel,"> APUs that will follow 'Strix Halo' will be called 'Strix Point'.  What? No, that's not what any of these terms mean. The Strix lineup is already out. ""Strix Halo"" is literally their halo, big compute/iGPU product.  PTL will compete with normal Strix Point and its refresh next year.",Neutral
Intel,Really? This should be good for Intel in 2026.,Positive
Intel,These Halo iGPUs are meant for LLM first and foremost. If you just want to game just get a normal RTX laptop.,Neutral
Intel,They are like 20% more efficient.  You are talking about 200% more efficient if it hits the same performance.    They might as well make it a mobile chip if that is how good it is. Would be great on phones.  Would be better than most pcs right now on 5W envolope.,Positive
Intel,"It's intentional, and non specific. So most go by the 3/5/7/9 naming. In order for them to have max profit, they need to lead you to the chips they want you to buy.",Neutral
Intel,'Strix Halo' is a product line as 'Gorgon Point' is also/another product line. Both have different variants.  In the case of 'Strix Halo'  [Check](https://hothardware.com/news/amd-strix-halo-cpu-rumors)  Find the official lineup here  [AMD](https://www.amd.com/content/dam/amd/en/documents/partner-hub/ryzen/ryzen-consumer-master-quick-reference-competitive.pdf)  Edit: changed 'Strix Point' to 'Gorgon Point' since that what I meant.,Neutral
Intel,👍,Neutral
Intel,"Yes, Strix Point lives *alongside* Strix Halo. Strix Point is already out, and came out *before* Strix Halo at that. It is not the followup to Strix Halo.",Neutral
Intel,You are right. I meant 'Gorgon Point'.,Neutral
Intel,"Gorgon point is just a refresh of strix point. These terms mean less in the next gen anyway as the follow on products have quite different names. Medusa point isn't really the successor to gorgon point, for example.",Neutral
Intel,My brain hurts and I’m still confused,Neutral
Intel,That's why you have guys that are completely ChatGPT-levels of confident when they are completely out of whack with the info lol. No worries.,Negative
Intel,"I think him saying ""unreleased products"" could mean it's still coming.",Neutral
Intel,I think B770 is locked in and to release shortly isn't it? sure I saw a leak of packaging details etc.,Neutral
Intel,"We finally got reviews for the B50 and it is a SFF gem! That said… I wish Intel would get better at promoting upcoming products, they seem to be slowed down by the restructuring.  I’m going with the flow, whatever will be will be. I’m waiting for Dividends to kick back in so I can retire with Intel.",Positive
Intel,"Boy, I am glad they picked an easy naming convention.   /s  After Xe, Xe1, Xe2 no doubt next would be Xe3, then Xe3p  After A-series, B-series, no doubt next would be C-series.    Too bad those generations do not line up",Positive
Intel,Intel is not serious with dGPUs,Neutral
Intel,"If it was coming, you expect them to talk about the future even a little bit. Instead, nothing.    They cancelled Celestial over a year ago now. Sounds like things haven't improved since.",Negative
Intel,"I think him saying he doesn't talk about unreleased products is 100% bullshit. He talks about upcoming tech constantly. This is not just unreleased, it's unannounced, unclaimed and nonexistent outside of pure speculation.",Negative
Intel,B50 is interesting once the software is there (planned Q4).,Positive
Intel,"Yes, they are literally just playing. It's all a game lol",Neutral
Intel,"They have released about the same number recently as AMD, I'd say they're pretty serious. The B50 is a pretty compelling product too, big features for the price.",Neutral
Intel,They are as serious as AMD.,Neutral
Intel,This interview happened during the quiet period so I don't think he could talk about the future,Negative
Intel,"They cancelled Celestial over a year ago now. Sounds like things haven't improved since.   MLID people showing themselves.  Tom Peterson previously said Celestial discrete hardware was already done and they were working on Druid. So if they cancelled it, it must have really sucked.  I am sure after they release Celestial 70 series beater, MLID will come and say they cancelled the 90 series beater and it's for sure dead from now on!",Negative
Intel,Where did you hear that it was cancelled?,Neutral
Intel,"We are supposed to get B60s also but they will likely be very limited and part of Battlematrix. Intel is moving very slow with Pro and Consumer GPUs and they can’t rely on TSMC for supply and obviously are not ready to manufacture through IFS? We are getting left in the dark, all we can do is wait.",Negative
Intel,There has been no update regarding celestial dGPUs internally.,Negative
Intel,AMD is skipping a generation to focus on the next. Intel has lost its focus on GPUs. These are not the same things.,Negative
Intel,Lol no. ARC was 0 margin product. Now it's fate depends on the whims of VPs not engineers.,Negative
Intel,"You could likewise point not that there was no word about dGPUs in the PTL presentation either. I think people need to accept that it's just not happening, at least for the foreseeable future.",Negative
Intel,"> MLID people showing themselves.  I'm not getting this from MLID.  > Tom Peterson previously said Celestial discrete hardware was already done  No, that's absolutely false. Actually watch the interview instead of reading reddit comments. He said Xe3 (specifically in PTL), not Celestial, was done. And this was after the PTL tapeout was announced, so that didn't even tell us anything new.   And as we now know, they don't consider that even in the same family as what would be Celestial.",Negative
Intel,Ex-Intel coworkers/acquaintances.,Neutral
Intel,>There has been no update regarding celestial dGPUs internally.  Do you have internal information?,Negative
Intel,Why would they? Battlemage is not even finished. Battlemage is not even 1 year old yet. They will still release B7XX gpus and probably B3XX.  I expect them to tallk about celestial by next year.,Negative
Intel,> Intel has lost its focus on GPUs  So despite them repeatedly telling you they have not... they have?,Neutral
Intel,"Battle mage is not a 0 margin product...  I know how much silicon cost etc due to my job. Believe me there is at least %30 gross margin in Battlemage and that is assuming somehow Intel got a worse price compared to my small ass company.     It's not profitable due to amount of R&D it takes to develop it, Intel earns a significant chunk for each Battlemage sold. They are simply not as greedy as Nvidia and AMD to earn market share.",Negative
Intel,Dude XE3 even has some test shipment reports etc. It's too late to cancel.   Sure if it's not good maybe we will only see B580 replacement.    But it's literally impossible and stupid to cancel it right now. Especially given how much gross profit they made from B580,Negative
Intel,"So, no news story has come out stating that?",Neutral
Intel,Xe3P-HPM suggests otherwise,Neutral
Intel,"Yes, through my ex colleagues",Neutral
Intel,"No, I don't listen to them, they have a nasty habit of downplaying bad situations. I'm going by their actions.",Negative
Intel,"> Dude XE3 even has some test shipment reports etc  Celestial wasn't base Xe3, and didn't tape out before cancellation. What test shipments are you referring to? PTL?  Btw, they still aren't saying anything about BMG G31, and that was much further along than Celestial was.   > But it's literally impossible and stupid to cancel it right now.  You can cancel a product at any point before it's released. Anything else would be sunk cost fallacy. Surely you're aware of the massive budget cuts and layoffs they've announced. Not everything can survive.   > Especially given how much gross profit they made from B580  By all reports, BMG still wasn't profitable for them. Hell, even if it *was* profitable, doesn't mean profitable *enough* for Intel to keep funding it in this environment. They're prioritizing spending reduction, not profit maximization.",Negative
Intel,Why would Intel tell you? It would just stall selling all current ARC cards.,Negative
Intel,No. Or at least not from any reliable source. Obviously discounting MLID and his ilk.,Negative
Intel,What about it? That some reference exists in drivers?,Neutral
Intel,"You don't even know that?   Lip Bu has been hiring gpu designers not firing them.   Most of the Cuts are from foundry side and slightly from gaudi side.    Intel if anything is focusing on gpus to create AI inference gpus.    Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.      You should check your sources, Lip Bu would cut 14A before he cuts Inference gpu side.",Neutral
Intel,BMG was 0 margin product,Neutral
Intel,"Of which are recently implemented, as in 'in the past week' which would be exceptionally stupid to do for a cancelled project",Negative
Intel,"> You don't even know that?  What do you claim I do not know?  > Lip Bu has been hiring gpu designers not firing them.  Celestial was cancelled under Gelsinger, as well as several rounds of client GPU layoffs. If Lip Bu is hiring anyone, it's not to build the team back up again.   > Only potential explanation would be only cutting consumer graphics but that doesn't make much sense as they are very familiar.   I am specifically talking about client graphics, yes. There's shared work, and arguably should have been more, but they were quite different. Client iGPU, client dGPU, and server dGPU were basically all separate SoC designs.",Negative
Intel,They're also using Xe3p for NVL-P and that Island AI product.,Neutral
Intel,"It wasn't though?  As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  I don't believe there is nothing Intel can do convince you. I heard these news every single time.     Also just look at job listings, there is many for gpu development.",Negative
Intel,"Which means it is being used, produced, and cannot be disqualified yet, nor does anything, not one trustworthy source, show it is cancelled",Negative
Intel,"> As far as you guys go Arc was cancelled, battlemage was cancelled now it's Celestial?  Go ahead and point out where *I* said ACM or BMG were cancelled. I have no idea who ""you guys"" are, nor do I care what others are or are not saying.  > I don't believe there is nothing Intel can do convince you.  Well yes, if they cancel a project, and I know they have, I'll say they cancelled it. The way Intel can convince me to say otherwise is by... not cancelling projects. And you *do* realize they haven't talked about client dGPUs past BMG in many, many months, right? It's not like Intel's really denying anything.  I'm not sure what you're looking for here. Should I lie and pretend not to know what I do? To what end? It's not like Intel's harmed by me saying they cancelled such a project. That's not something you can keep a secret indefinitely, and anyone who *really* cares already knows.  > Also just look at job listings, there is many for gpu development.  If you lay off 10 people, 5 more leave on their own, and then you backfill 4, that's still a net loss. They still need some people, but not as many as they had, and not for client dGPU.",Negative
Intel,"Intel have not claimed there to be upcoming Celestial, or more Battlemage or anything like it. How did their silence convince you of anything?",Negative
Intel,Actually after the Nvidia deal there's definite reason for Intel to cancel future ARC development.  They have a partner that makes better GPU than them. Why would they continue? If the Nvidia deal is successful I expect even their iGPU will disappear.  In no sense it makes sense to develop a line that's redundant with what your partner is doing.,Negative
Intel,"> Which means it is being used, produced, and cannot be disqualified yet   Again, Xe3p lives for various things. Celestial, the client dGPU planned to be based on that IP, is dead. I'm not sure what contradiction you think exists here.    > nor does anything, not one trustworthy source, show it is cancelled   Intel's refusal to talk about future gen client dGPUs and the mass layoffs in the hardware team don't tell you anything? Or what about Gelsinger's remarks about less focus on dGPU?    This is how Intel usually handles cancellations btw. They simply pretend it never existed, unless investors really demand to know. Even then they like to delay the acknowledgement.",Negative
Intel,Think pretty easy naming. Any X infront just automatically means better iGPU,Positive
Intel,"Looks pretty snappy at \~75% faster than the top-end LNL chipset and this is with pre-release drivers.  Granted TDP is probably higher.  If they get the drivers cleaned up then it might release a bit higher which makes it a viable (though still materially weaker) thin and light XX50 dGPU model alternative some some of the market.  Should do well addressing the 'I want a thin and light laptop, but I want it to have an ok GPU' crowd.",Neutral
Intel,"Panther Lake 12 Xe3 performance looks great to match RTX 3050 laptop performance because the entire chip only draw half power of RTX 3050. Seems like using 18A BPD really paid of to reduce CPU consumption by a lot with the helps of Tsmc N3E for iGPU.    Also it's so weird to see Asus Zephyrus G14 with Intel chip, usually it always has Amd CPU paired with Nvidia GPU. I heard G14 is pretty popular gaming laptop but this laptop during full load can use 120w+ power.    Using Panther Lake 12 Xe3 will makes this laptop looks even more appealing because it reduces power requirements from 120w+ to 45w but still giving about the same GPU performance which is insane. This is a massive game changer for people who use laptop as portable gaming machine and to those who travel a lot. I can totally understand why Asus this time use Panther Lake for G14.",Positive
Intel,"Nice to see the G14 with an Intel CPU. Thought the lineup was AMD only tbh, while the larger G16 laptops get Intel.",Positive
Intel,Naming for these chips are terrible,Negative
Intel,"Can Asus send me this laptop for review? I have 14 followers on snoozetube and 60% are probably bots, but bots are human too?",Neutral
Intel,Can't wait for 14inch laptops with actually good battery life and convenience than the cheap gaming laptops it's going to kill,Positive
Intel,"Nah, should've kept that info at the end like every other Intel and AMD CPU ever made.  But otherwise this branding really feels like AMDs APU line, where they had to emphasize their iGPU was better than average.",Neutral
Intel,"The possibility of being nearly 100% faster than Lunar Lake in some tasks, and minimum possibly 50% faster while being able to fit it into a sub 3lb/1.5kg design with a 80+wH battery is going to really nice. If the 4 LP-e cores scheduling work well and maybe a more efficient OLED panel you could easily get true 24 hrs use on x86",Positive
Intel,"If the game could be 50–60% stronger, that would be That would be a killer",Neutral
Intel,Its GPU part isn’t 18A at all — it’s actually N3E and 4Xe3 integrated graphics use Intel 3.,Neutral
Intel,"That’s been true for the past generations, but it looks like it will change this generation",Neutral
Intel,Still better than Ryzen 365 AI pro MAX+,Positive
Intel,"I disagree, GPU focused = X (like Xe3). Just takes getting used to , but otherwise it follows the same 3 7 9 scheme that probably didn't make much sense at first either :)",Neutral
Intel,https://browser.geekbench.com/v6/compute/compare/5050048?baseline=4771132,Neutral
Intel,The typical consumer doesn't know anything about the last letter. Having it in front will be much more successful to communicate to consumers the difference.,Neutral
Intel,Yea putting ai the model name is disgusting 😂,Neutral
Intel,I can't wait for the Ryzen 688S AI Pro MAX+++,Neutral
Intel,"I agree with this and now snoozetube creators are doing 128gb reviews for the 365 AI Pro Max+ and glossing over the fact that it costs decent money but lacks any kind of power when compared to discrete GPUs.  Amd continues to pump out expensive APUs that are mediocre, while doing everything related to Radeon half heartedly.  Why is that?",Negative
Intel,"Nerds argue over names for tech products but will eventually figure out some kind of logic in why they named it that way. Entire generations need to be released and compared.   As for average users they will always be perpetually clueless and unfortunately will become influenced by an influencer with no integrity and or a store associate who has been trained on scripts that make the most money for the store.  God help us all, I pray for Jesus - just like Pat Gelsinger, who will get no credit for the Intel turn around.",Negative
Intel,Ryzen metaverse Ai max++ 3D Hypercache macroboost,Neutral
Intel,I think it's rather on point. the 395+ is a beast for running large MoE AI models. It's value for money in that respect is almost unbeatable.,Positive
Intel,you forgot the x3dx2    when both cpu tiles are stacked on 3d cache tiles.,Neutral
Intel,Ultra TypeR S-line AMG M Bi-Turbo CCXR LM Harley Davidson Edition,Neutral
Intel,"Very very few know anything more than that, usually completely unaware that there's a whole SKU number after that.  How many times do you hear stories about some user proudly boasting about having an i7, only to find out that it's like a 6th gen, and they don't even realize / believe that something like a i3-12100 is actually a better CPU.     The average user understands the difference between, say, a Core Ultra 5 and 7, because the ideal of 3, 5, 7, 9 being product tiers exist in plenty of industries, like BMW's product line. Bigger number = more performance. How? By how much? No clue to them.  So since the average user is going based off just the name 5, 7, or 9, having that X visible in a location they'll see is certainly very important. They'll notice the X.",Neutral
Intel,"It would be like a Chromebook named ""Chromebook CloudCompute+"" just because that's what those are built for",Neutral
Intel,I personally prefer them Name it Ryzen 3 / 5 / 7 / 9. It’s easy to understand and easy to compare to intels naming but sadly both companies have ruined it now.,Negative
Intel,That's honestly sounds even more cringe. Can you imagine Amd Ryzen 9 395X3DX2 AI Pro Max+? That's ridiculously bad LMAO,Negative
Intel,BMW Individual M760i xDrive Model V12 Excellence THE NEXT 100 YEARS,Neutral
Intel,I actually did my research and found out that core ultra 5 125u is not much different from core ultra 7 155u... Ended up buying ProBook with core ultra 5 125u and saved money for upgrading the ram and SSD,Neutral
Intel,"but i think its gonna happen aye,  i wonder if the RAM bandwidth needs for AI benefit from cache like games do, or are they better slapping more ram channels on it...",Neutral
Intel,"Yeah, all of the U chips within a generation are the same physical chip, just different bins (usually tiny clockspeed differences). I don't think they even have core count differences any more for the most part.",Neutral
Intel,Same core counts too,Neutral
Intel,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Neutral
Intel,"To summarize from the article for some folks:  6300 puts this GPU as 33.4% faster than the 140V, 71.3% faster than the 890M, and 54.3% of the 8060S. Just over half of Strix Halo's top config.  Bear in mind though, that this benchmark favors ARC GPUs compared to gaming results. The 140V and 890M are roughly equal and this benchmark puts the 140V as 28.4% faster.",Neutral
Intel,"Ugh, who is naming these products?!?!? Between the internal code names (which are now publicly used) and the actual product names, it's a mess. As bad as monitor naming.",Negative
Intel,"Sick, Panther actually sounds good, and Lunar/Arrow on mobile already sounded good to me. Please keep pumping those iGPU numbers Intel Arc engineer bros.  AutoCAD on integrated Intel graphics WHOOOOOO  AMD step it up next gen please, Intel is no longer a static target in graphics, kthx.",Positive
Intel,This sound more realistic and not as good as the 50% better Intel announced.   For Intels sake I am hoping the 50% is real,Negative
Intel,... with early drivers and probably no decent support yet. Its also probably an engineering sample? 😉,Negative
Intel,Wow pantherlake is looking enticing. Should I wait for novalake? Will novalake have same battery life like lunar lake or pantherlake?,Positive
Intel,"It's going to cost like $10,000, right?",Neutral
Intel,"The top lunar lake has 8Xe2 cores, and this has 12Xe3 cores. 30% faster seems… bad? Like why not 50%+ given 50% more cores and architectural improvements?",Negative
Intel,"We have seen how Lunar Lake on MSI Claw 8 AI+ performs on early firmware and drivers, Intel even managed to improve Lunar Lake performance up to 30% after that. I expect 50% performance improvement on Panther Lake is very possible.",Negative
Intel,seee ceerto seeeh,Neutral
Intel,"It really is a mess, someone should've been fired long ago.",Negative
Intel,"The only thing they want you to care about is 3/5/7/9.  If the naming is too clear, then they can't steer you to the higher margin/profit variants. At some level it needs to be unclear. Not defending them, but how things are.",Negative
Intel,Intel wasn't a static target since the 10th gen. They've been pushing (at least in terms of performance) since then. It's mostly the efficiency which stepped back. And they messed up their current lineup in terms of performance and pricing but they've improved for sure.  Next gen seems good for them though. Hope they price it well. AMD isn't zen 1/2 either. They're bumping core numbers too and introducing some new cores as well.,Neutral
Intel,"I mean, why?  Is AMD going to be coming out with anything more powerful than the HX 370 in that power range?",Neutral
Intel,because 50% more cores need 50% more power for 50% more performance.,Neutral
Intel,"There are so many variables here that we have no way of accurately saying that.  It's possible that Xe3 is simply treated more like RDNA in this benchmark, and that the Xe2 140V is unfairly biased towards in that way. If that is the case, the 890M and 8060S may be better metrics to base off, and we do see over 50% gains for a 50% wider GPU than the 890M.  It's also possible this is a power-limited scenario. Like for like on TDP, this would be a solid improvement given the GPU both has to be moved off-tile compared to Lunar Lake, and has twice as many CPU cores to fight for power.  This could be pre-release drivers not getting the true 100% out of the hardware or silicon with non-final clocks still being tuned. The point is there's no way to know for sure.",Neutral
Intel,I don’t understand why you people continue to use that MSI claw to demonstrate “intel improved lunar lake performance post launch” when there are millions of lunar lake laptops that never had performance issue of that one handheld.,Negative
Intel,"they're keeping their jobs because this is technically better than the past. back in the day you'd have model numbers exclusive to a retail store, much less specific to an OEM, because businesses wanted to feel like they got a bespoke deal.   These days the SKU naming is mostly for accounting purposes, while the ""real"" naming decisions are made by the OEMs. basically most people are buying the Thinkpad/Yoga/ROG ""brand"" rather than the specific processor model, which only a much smaller crowd bothers to comprehend. It's like how people buy the Steamdeck rather than whatever APU is in there, which is fairly old at this point.",Neutral
Intel,"Energy efficiency is huge in mobile but I was explicit in saying they were no longer a static target in graphics, where they were not making significant or impressive gains in iGPUs for many years, it seemed. Now they are making one of the best iGPUs on the market.",Positive
Intel,"Possibly yes.   Current AMD handheld chips are better than Lunar Lake performance. Assuming normal cadence. Next generation would be similar performance to Panther Lake.     I was hoping more of a leap frog, rather than similar performance. %50 would be a very clear edge.",Positive
Intel,Is the TDP fixed to be the same between LL and PTL in this test?  I genuinely don't know.,Neutral
Intel,But LNL's TDP is too low compared to H45 cpu,Neutral
Intel,"> because 50% more cores need 50% more power for 50% more performance.  insightful, and easy to forget given feature bragging    :)",Neutral
Intel,FTFY: because 50% more cores need 50% more power for 35-40% more performance.  Cause it don't scale linearly.,Neutral
Intel,"Mostly because people with LNL laptops are less likely to games since gaming are not the point of those laptops -> less testing, whereas the Claw 8 AI+ is a gaming PC handheld, so it is mostly use for gaming purpose and thus have more people testing for its performance.",Negative
Intel,"As weird as it sounds actually Lunar Lake improvement mainly comes from MSI Claw not laptop, that because majority people who use MSI Claw give feedback the most which is why Intel focusing on the handheld first then laptop.    Intel even use Claw as benchmark for Lunar Lake compared to laptop. You can read from this article :  https://arstechnica.com/gadgets/2025/04/intel-says-its-rolling-out-laptop-gpu-drivers-with-10-to-25-better-performance/   Also Claw got BIOS update way faster than any laptop with the same chip so it helps Intel to mitigate power and boost behavior to maximize the performance. There is so many bug reports on Arc forum, most of them are Claw users, that's why laptop got benefits too.",Positive
Intel,AMD isn’t coming out with a better next gen iGPU for mobile (Gorgon Point) since it’s a simple refresh. Same arch with same CU count based on rumors.  The generation after will be competing with Nova Lake,Neutral
Intel,"Amd handheld with Z2E isn't better than Intel Lunar Lake, you can see the comparison on MSI Claw sub or even on youtube. Z2E in most game is 10% slower than Core Ultra 7 258V, it only won in the game where Intel GPU performs bad.    Not to mention at 17w Z2E losing badly to 258V, Intel is on their own league, it's not even competition for Amd.",Negative
Intel,Intel is gonna have better integrated graphics than AMD,Positive
Intel,I have no idea.,Neutral
Intel,"We don't know what TDP was run here, and both MTL amd ARL H have been 28W outside of the Ultra 9 SKUs. It's entirely possible this was run at 28W, which is also in reach of Lunar Lake's boost envelope.",Neutral
Intel,The lunar lake laptops are tested by reviewers all the same as the strix point ones. Regardless it’s misleading because it’s not “lunar lake” but rather the performance profile/boost behaviour of that specific MSI CLAW that was changed and people act like it’s lunar lake’s drivers doing “30% magic”.   It’s not.,Negative
Intel,"And is there documented so-called “large” performance improvements on LNL systems that already performed as expected on day one (it tied 890m on high power and was always better at low power, talking about real games not 3dmark)? Or was it only bringing the Claw back to where LNL should always perform?",Neutral
Intel,"True. I don't see how the next igpu from Amd going to use rdna 4, it won't even support fsr 4. Meanwhile Intel going to push their igpu tech even further with XeSS XMX 3 with Xe3 and Xe3P, they will be way ahead of Amd in igpu market especially when Intel Lunar Lake already beating Amd Strix Point and Z2E.   Intel also dominating mobile market. Honestly it's not looking good for Amd.",Negative
Intel,Yeah people act like strix point is in that segment..... It's not.,Negative
Intel,🫨,Neutral
Intel,Wouldn't be the first time.,Neutral
Intel,"Oh, I guess it be like that.",Neutral
Intel,"I loved how Tom Peterson did the circuit of tech blogs, tubers, and related last Fall to announce and advocate for ARC Battlemage.  Looking forward to seeing alot of him over the next few months for Xe3 and ARC C-series",Positive
Intel,"""we are tending to prefer e cores now when gaming""   That's very surprising",Neutral
Intel,If only they release a high level C-card. Battlemage kinda never scratched the mark.,Negative
Intel,"The point of Xe3 being actually battlemage instead of Celestial, this is so horribly confusing. I can't understand what's going on, nor why they would do that.",Negative
Intel,I’m guessing Xe3P will be on Intel 3-PT.,Neutral
Intel,"Tom is a funny guy, love it when he gets camera time",Positive
Intel,Cool hope they will manage to release it soon not in 12 months when everyone will be talking about RTX 6000 and RDNA5/UDNA or leaks about them.,Positive
Intel,"When is the panther lake reveal going to be, CES?",Neutral
Intel,"e cores are surprisingly powerful now, and games are getting more multithreaded. It might be better to spread them out over 8 less power hungry cores than 4 P cores and then spill over to the slower cores. Especially in a laptop environment where efficiency matters and the GPU is usually holding back gaming performance so much that CPU performance is really not that important.  In fact, I bet it's mostly down to not spilling over to e cores from P cores. That's what causes slowdowns and stutters usually attributed to e cores. The engine has to wait a little longer for workloads assigned to e cores than to P cores. The if the main thread is assigned to a P core that's all well and good, but if sub tasks are distributed among the P cores and then something important is assigned to a slower e core it holds up the sub tasks and in turn the main thread. But most importantly, it doesn't do so evenly. Maybe you don't need something assigned to e cores, or the e core task is light and doesn't hold up the other threads. Then things will run at the pace of the P cores, but every now and then you'll have a slowdown.",Neutral
Intel,Wonder if there's some energy star requirements.,Neutral
Intel,"Shouldn't be. That's what the math has always said.  As soon as software can scale to ""many"" cores, the tradeoffs that go into single powerful P style cores are a bad deal. Both frequency and sequential optimizations (like multi-level branch prediction) scale poorly.  Gaming benchmarks tend to have a really strong feedback loop that favors last gen hardware design though. So seeing the benefits to E cores for gaming requires 1.) e-cores exist for a while 2.) some games optimize for them.  Long term, an optimal Intel-style processor design will look something like 4 ""legacy"" P cores + dozens of E cores.",Negative
Intel,It's either N3P or 18AP.,Neutral
Intel,That’s why rumors want Intel to throw away P-cores (Israel team) and only keep E-cores (Austin) to make Titan lake a unified core with only E-cores remaining.,Neutral
Intel,>and games are getting more multithreaded.  Can you give me a few AAA examples of modern games which get a noticeable improvement in AVG. FPS or 0.1/1% lows when using more than 8cores/16 thread's ? I'm genuinely curious which games you are talking about.,Neutral
Intel,"Really should focus on the Main/Sub thread part. Most games will usually only load up 1-3 cores, with the rest of the cores only used for incidental workloads with lower priority and sync requirements (Multithreading is hard yo, Multithreading with latency requirements is mindnumbing).  This makes plenty of games very suitable for the P/E architecture, as long as you have enough P cores for the Main threads, the E cores will be perfectly sufficient.",Neutral
Intel,"Energy Star probably won't be around for too much longer, at least in it's current form.  https://www.npr.org/2025/08/13/nx-s1-5432617/energy-star-trump-cost-climate-change",Neutral
Intel,That makes me imagine in a different reality if Xbox had remained with Intel and NVIDIA and their next generation would use an Intel E-core based SoC with NVIDIA chiplet.,Neutral
Intel,It has already happened. Stephen Robinson - heading the Austin team - is now the lead x86core architect.,Neutral
Intel,"Off the top of my head, Starfield, Bannerlord, BeamNG, UE5 games due to how the rendering pipeline works, etc.",Neutral
Intel,"Most games of the past will only load up a few cores, but that's beginning to change. Cyberpunk 2.0 loads up 16 threads/cores easily, and some others like Battlefield 6 also scale pretty well. If you have less than 16 threads on fast cores like Panther Lake and Arrow Lake, then you can run into issues. Or if your 16 threads are on one die and any spillover has to go across the SoC die like with AMD and probably Nova Lake.",Neutral
Intel,"Seeing how Nvidia working together with Intel to make integrated high end GPU i can see the possibility of Xbox using that chip, maybe in the future.",Neutral
Intel,I'm tired of AMD slop consoles and handhelds,Neutral
Intel,">Starfield  [Not really](https://youtu.be/BcYixjMMHFk?t=1015), game doesn't benefit from more cores/threads.  >Bannerlord  Sadly, there's no benchmarks of different CPUs that I could find for this specific game, but I do know that this game heavily relies on a good CPU, but without decent data(review), which shows multiple CPUs tested, it's hard to understand the benefits of more cores/threads,  >UE5 games due to how the rendering pipeline works  It's true that UE5 can utilize 8 cores / 16 threads, but more than that? I'm not sure, if possible, provide a review/video which shows that UE5 scale with more cores/threads, so far, it seems that it is limited at 8c/16t - big channels rarely add UE5 games to their CPU benchmarks, but I found Remnant 2,[ and it doesn't show any benefits](https://youtu.be/3n537Z7pJug?t=1056) of more than 8 cores.  I heard that ""games are getting more multithreaded"" like 4-5 years ago, and in most cases, it wasn't true, with more than 8c/16t almost no games scale on CPU-side, and even when they do, in most cases it's a minor improvement over 8c/16t configuration, like 1-3%.",Negative
Intel,"UE5 by default does not support multithreading well. actually I don't think async shader is even considered a default feature yet despite being added two years ago. Only the editor compilation step uses all threads, but it doesn't need to react to user input so it would be more surprising if it didn't use all threads.  If you're seeing good thread use in a UE5 game is thanks to the developer breaking up work with their own engine changes.",Negative
Intel,"Considering AMD’s lag in gaming performance especially in ray tracing, I would be totally onboard with that.",Positive
Intel,Huh????,Neutral
Intel,"12900K being that high in the Starfield benchmark, like actually within error margins next to 7800X3D despite being an older platform with half the L2 of raptor lake and lower clock speed than even 7700X, shows that the game benefits from more cores and threads very much. Yeah 9950X should outperform it technically I guess but the split L3 between two CCDs probably holds back the advantage of having more cores.  Yeah sadly there's no benchmark for Bannerlord. But supposedly it does disperse its tasks to plenty of threads.   Here's some benchmarks showing core and thread usage in a few games. While the benchmark tool doesn't get into the details of how those software behave obviously, but at least according to the graphs some games like Tarkov really suck at distributing its tasks to several cores but some others like Starfield, Cyberpunk, and Space Marine 2 are actually quite good at it.  [Will This Do? — Intel Core i5-14600KF vs. i5-13600KF vs. R7 7700X vs. i7-14700KF Benchmark - YouTube](https://www.youtube.com/watch?v=HY_weucfLPQ)  [Isn't it a blast now? — Core Ultra 9 285K benchmark. Comparison with R9 9950X, R7 9800X3D, and i9...](https://www.youtube.com/watch?v=q1zAX1VNdf0)  I haven't seen a core thread benchmark for UE5 games yet, but that's just how the engine is supposed to work though.",Neutral
Intel,"To be fair, Sony seems to be fed up with AMD lagging behind and are pushing them to do better. Consoles will likely continue to stick with AMD, at least for the next generation. Intel graphics aren't quite ready yet, and Nvidia is going to be way to expensive unless they take an old SoC and repurpose it like Nintendo.",Neutral
Intel,">12900K being that high in the Starfield benchmark  It's because Creation Engine 2 worked better with Intel hardware until AM5 X3D chips, and 12900K great result in this test just shows that it's still a good CPU with 8 Performance cores, not that it's core count matters in any significant way - for example, 7700X delivers identical performance with lower core count/threads.  >lower clock speed than even 7700X  Clock speeds can't be compared between different architectures, different architecture = different efficiency, what matters is IPC.  >shows that the game benefits from more cores and threads very much  [here's a better example where more intel CPUs are present in Starfield test](https://youtu.be/XXLY8kEdR1c?t=29m5s), as you can see, increased core count/threads past 8 performance cores and 16 thread's is meaningless and won't provide any noticeable performance improvements, 6 faster P-cores on Intel 14600K provided better result than slower 8 P-cores on 12900K.   Going from 7950X to to 7700X results in 5 FPS loss, which is like 2-3% less FPS, and it's mostly because of lower clock speed(5.7Ghz Vs 5.4Ghz boost).   >but some others like Starfield, Cyberpunk, and Space Marine 2 are actually quite good at it.  That's the point, I agree that modern games can and will utilize 8c/16t configuration, but so far I don't see a trend of games becoming more ""multi-threaded"" past that point, as I replied to that person, I'm curious what games now become more multi-threaded than few years ago, I find that observation speculative and without evidence - more cores and threads is a great approach for workloads, but games generally don't care about it past a certain point, in this case it's 8c/16t.  It could change with next-gen consoles, if they will use AMD 12 core CCDs and games will be optimized to utilize more cores&threads.  What's important now is good cores or good cores+X3D cache, not core count - even 7600X3D with 6 cores and 12 threads is better in gaming than most Intel/AMD non-X3D CPUs with way more cores.  Edit: typo",Positive
Intel,"Well you said that games are optimized for 8/16 and yet gn bench shows 7600X only gets 3-4 less avg fps than 7700X in baldurs gate 3, a highly multithreaded game, it has better 0.1% lows even, though I'm sure that one is a contained incident since BG3 is very dynamic and hence results are not always repeatable. Same with Starfield.   That's because in video games, core count matters less than how fast the CPU can access or manipulate very dynamic types of data in a random memory address.    X3Ds are not just ""whats important now"". They're going to win in old or new games, and they're winning in games because they have a large buffer of low latency data. Not winning in productivity benchmarks because productivity is about processing matters more than accessing data, and its more easily multithreaded too in many cases.    But having more cores is still more advantageous in software and games that can occupy them.",Neutral
Intel,">Well you said that games are optimized for 8/16 and yet gn bench shows 7600X only gets 3-4 less avg fps than 7700X  I said ""can and will"" not that every game is going to benefit from it.  Point of discussion was to prove that games are using more than 16 threads or they're not - it seems like you overestimated Starfield, UE5 reliance on core count/threads and most games care about 16 threads at most.   >core count matters less  Yes, that's why I replied to that person and asked him to provide me with modern AAA games which are more ""multi-threaded"" now than AAA games a few years ago, I feel like what he ""observes"" is what he wants to believe, and not something that actually happened with optimization in games.  >X3Ds are not just ""whats important now"".  AMD sells 9950X3D, best productivity and gaming CPU, if you really need both(workloads/gaming) it's the optimal way, for an average user, only benefit of E-cores is lower power draw when idle - but they do matter more if you need those workloads, I agree, but we discussed gaming performance and core reliance.",Neutral
Intel,If Panther Lake is going to be part of Xe3 then what does that mean for a possible B770?,Neutral
Intel,"What even is Intel naming at this point, before it was the one thing about their products that was at least *kinda* consistent.",Neutral
Intel,"B770 is probably likely coming out at the end of this year, I don't see any reason why they would not sell it.",Neutral
AMD,Why wouldn't it cost more it has 50 more points,Neutral
AMD,8 core | 120W TDP | 96MB L3 Cache | 5.6GHz boost,Neutral
AMD,"I’m curious how well they’ll end up selling. The marked for ram is so crazy now that significantly less is willing to upgrade. So it’s mostly 7000 series people, maybe? That’s willing to shell out.",Neutral
AMD,All of these hardware launches mean nothing because the components to accompany them are too expensive unfortunately,Negative
AMD,If these are the higher binned 9800's I wonder how much head room is going to be left for overclocking given it has the same power consumption and how that will compare to a standard +200 on the 9800x3d.,Neutral
AMD,"I'm curious if the admittedly stupid rumors that this has a different or improved memory controller over the other AM5 chips are true.   Not that it would matter to most people, but if this did get that and enabled hitting 8400 MT/s, 8600, 8800, etc more reliably, I'd be in.",Neutral
AMD,I’ve clung onto my i9-7940xe for so many years now. I’m thinking about upgrading and it’s amazing how many cores regular consumer chips have now.  If only ram were cheaper :(,Positive
AMD,Waiting for the 9950x3d2 😢,Neutral
AMD,Still rocking a 7700x but no need to upgrade until Zen 6 CPUs comes out late 2026.,Positive
AMD,"Hello No-Explanation-46! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,I’m actually one of the few who will upgrade from 9800x3d to this haha,Neutral
AMD,I think most people understand this isn’t for existing 9800x3d owners and that it’s not an “upgrade” in those cases. However it does feel like a bit of a slap in the face for people who are buying them now even though I understand the reasoning. Let’s be real here Intels Arrow lake refresh won’t surpass any of the 9kx3d chips and likely not even the 7800x3d.,Negative
AMD,It’s a whole 50X3D better!,Positive
AMD,Only 0.51% more points. So should be a 0.51% price increase!,Neutral
AMD,I imagine I’ll be clinging onto my 5800X3D for quite a while.,Neutral
AMD,"I got my ram for ""free"" when I built my system in one of those Microcenter deals.  If I wait long enough maybe Microcenter will have a deal where they give away a free CPU when purchasing ram.",Neutral
AMD,Could also be Intel 12th gen owners that have DDR5 RAM already.,Neutral
AMD,YES! R7 7700X user here I’m dying to upgrade I hope no one buys it and the price drop like hot cakes,Negative
AMD,"7800X3D owner here, I’ll check out the reviews but my current 4090 + 7800X3D system is so well balanced, I don’t need it.",Negative
AMD,5.8ghz 9850x3d lol. That might be actually insane for esport games. 720hz monitors when? lol,Positive
AMD,"Better binned IO die at best, probably.",Positive
AMD,"You could also consider getting a used 10980XE and max out the platform considering the current market conditions. They usually can OC quite a bit higher than first gen SKLX, especially if you turn off HT.  And they have in silicon mitigation for some of the SKLX security vulnerabilities. Which means they perform better if you run with the fixes turned on (they still lose some performance).",Neutral
AMD,"Now that they've seemingly fixed the degradation issue, you might consider a DDR4 LGA1700 board and a 14900K. That would get you about double the performance while keeping your RAM.",Positive
AMD,Why?😂,Neutral
AMD,It doesn't have more cores lol,Neutral
AMD,Goofin,Neutral
AMD,You legit just insulted yourself hahaha,Neutral
AMD,Not enough 9s. I need AMD to make a 9999X3D stat.,Neutral
AMD,"9850X3D  minus  9800X3D  equals  50, not 50X3D",Neutral
AMD,Same 5800x3d is going to last me until probably AM6 at this point or until the AI bubble bursts.,Neutral
AMD,5800x3d really is the GOAT,Neutral
AMD,"I have a 5950X that's all tuned up (curve optimized, PBO jacked up, etc) and I'm kind of dreading going to a 9950X3D because then I'll have to do it all over again + upgrade every single other thing just to use it. Definitely a first world problem though.",Negative
AMD,"Same but with an 5700X3D here. But I do hope websites don't continue to get more bloated, otherwise even though I upgraded to 32GB it'll run out.",Negative
AMD,I'm going back and forth on keeping my 12700 or grabbing a 7/9800x3D combo at Microcenter... I don't know an upgrade now but who knows when it'll be a good time again. And I'm still on DDR4,Neutral
AMD,That CPU fucked really hard. What a great buy.,Negative
AMD,"As Intel 12th gen owner with DDR4, I guess I'll go straight to DDR6 lol. I mean, don't even need anything faster for now. But in few years and new tech things might change!  This entire tech/memory market situation is wild.",Neutral
AMD,More likely they'll limit supply if sales are bad. Then they'll purchase less production for the next CPU and increase the price to compensate. PC market is going to be carnage for this coming cycle.,Negative
AMD,what games do you play? I'm on 7700 non x and struggle to see any reason to upgrade,Neutral
AMD,Especially if you’re playing at 1440p or 4k it won’t really matter lol. The 9800x3d already isn’t that huge of an upgrade over the 7800x3d and the 9850x3d is likely a massively smaller of one to the 9800x3d.,Neutral
AMD,> 720hz monitors when  Soon: https://www.tomshardware.com/monitors/gaming-monitors/lg-demonstrates-ultra-fast-ultra-bright-oled-display-prototype-new-panel-can-refresh-at-540-hz-at-1440p-or-720-hz-at-720p-hits-1500-nits-peak-brightness,Neutral
AMD,"9800X3D already comes close to 5.8ghz with ECLK. Stock 9850 might boost ((375?)) higher than stock 9800, but for overclockers I expect the top-end difference between them will be a lot closer.",Neutral
AMD,"They’ve been pretty strict about not performance binning the IO die.       I don’t think they would go this route as the added complexity it would add to the mfg line would be as much or more than just adding a new product.    If they do it, I think it will be a new revision or design.",Neutral
AMD,"It gonna lie, pretty sure I’m going AMD on my next build. Thanks for the suggestion though!",Positive
AMD,"nah, people's 14900k's are still dying. 13 and 14th gen are no go still.  12th gen is best if you want best single core performance in esport games without caring about power consumption and are willing to tune ram. 5950x if you are doing work and need multi threading power. 5800x3d/5700x3d are extinct and overpriced on the used market.",Negative
AMD,"I'd avoid the 13th and 14th Intel gen chips like the plague. I had to have my 14700K CPU + Z790I MBU replaced with the 9800X3D + X870I MBU last week due to severe thermal issues. From a gaming standpoint, that AMD chip runs circles around that Intel chip.",Neutral
AMD,"> seemingly  Now that the degredation coverup is fully exposed, why would you trust any 13th or 14th gen part at all?",Neutral
AMD,Coz hes rich lol,Neutral
AMD,I'm a PC slut,Neutral
AMD,They're Belief Cores. You believe they're there and suddenly it feels like they're there.,Neutral
AMD,"Imagine thinking in only 3 dimensions, couldn’t be me! I rock a 9999X9D",Positive
AMD,They need to make a 9970X3D first,Neutral
AMD,"5x - 3x is 2x, not 2",Neutral
AMD,Units only cancel out with division. So 50X3D is the correct result. Otherwise you'd have to be really careful about taking money from your wallet - you don't want your hard earned cash to turn into a dimensionless number.,Negative
AMD,I think my 7800x3d will last me a few years,Positive
AMD,Seems like the new 2600K,Neutral
AMD,"Well, you don't *have* to tune up the CPU. Seems there's less to gain from that with each new generation. I only tweak RAM now, and I'm not putting a lot of effort into it.",Negative
AMD,"Same here... I feel so outdated and I fear I'll regret the future even more if I don't  This is genuine fear because I can't afford $3,000 CPU if some CPU bubble trend were to take off...",Negative
AMD,"The next 18 months at minimum, are going to be ridiculous.",Negative
AMD,Star citizen lol that game is a mess but a fun one,Negative
AMD,"No one is gonna play at 720p lol.  I mean at least 1080p, hopefully 1440p.",Negative
AMD,"If you need more single thread perf, then pick up a 5700X3D. They're still around.       Of course it seems like a downgrade from 14 cores, but going from Skylake-X to Zen 3 is pretty massive.  You'll be able to use at least half of your current RAM too.",Positive
AMD,Do it and don't look back.,Neutral
AMD,"the AM4 x3D CPU's are bananas.  I got a 5800x3d for like $200 just ""on amazon"" new a month or two before they were discontinued and now it seems they are selling for $400-$500 used on ebay.  You could have got them EVEN CHEAPER from aliexpress too.  Incredible.   Really is the 4790k of that generation.  The 1080ti of CPU's (for gaming anyway).",Neutral
AMD,"Because due to Intel's deserved reputational damage, AM4 CPUs are significantly overvalued and 13th-current gen Intel undervalued. Given that Intel can't afford to replace every last 13500 and up, and that they are replacing damaged CPUs, it seems reasonable to assume there is no longer a widespread issue being covered up.  AM5 systems would be the sweet spot for gaming except that DDR5 prices have made them untenable",Negative
AMD,Just in time for Christmas!!,Neutral
AMD,Much like my sanity.,Neutral
AMD,I mean you can upgrade if you want since you already have the platform and the ram.  CPU prices have stayed pretty even so the next Gen of AM5 CPU would be a perfect plug in option.,Positive
AMD,I had my 3770k from 2016 until Elden Ring came out in 2022. I actually went i3-3220 -> i5-3570k -> i7-3770k over the course of my first 10 years of having a desktop,Neutral
AMD,I OC’d a 2600k to 5.2ghz for 3 years. Had to reduce it to 4.8ghz for the rest of the time I had it until I got a ryzen 2nd gen.,Neutral
AMD,I went from a 2500k > 8700k > 9800x3d.  Haven't made a lot of great financial decisions in my life but man that feels like a win.,Positive
AMD,Nah it's the new 4790k for DDR4.,Neutral
AMD,The base AM5 7600 is better in most games than the top tier AM4 5800x3D.,Positive
AMD,Say that to cs players playing at 1024x768/1280x960 res lol,Neutral
AMD,"Assuming the sixth time they pretended to fix the problem means they actually fixed it is in fact the unreasonable position in regard to 13th and 14th gen. Go ahead and gamble with your money, but I don't trust them even a little.",Negative
AMD,I ran my 4790K for yeeeeears. 3770K and 4790K were solid chips,Positive
AMD,That’s super fair. Loved my 4790K,Neutral
AMD,"Seems I phrased it badly, I meant that tweaking CPU settings and OC'ing and testing those settings seems less worth it with each new generation as they're running more optimally from the get go. Edited the post.",Negative
AMD,"Not impossible, but very unlikely. AMD is currently focused on upcoming Zen 6, and I guess their engineers are working on next gen CPU / chipset / AM port. Plus there are probably constraints with TSMC schedule, I think you have to ""reserve"" quite some time in advance for any order.",Neutral
AMD,"Probably the simplest would be to manufacture 5800x3d again.   (Newer cpus have ddr5 controller, they would have to redesign architecture.)",Neutral
AMD,"Maybe, but I'm starting to think they want people to sell off their old stuff. Just a theory of course.",Neutral
AMD,No.  Considering the tech news from the last few months apparently there isn't much money in the consumer market for something like this.,Negative
AMD,"Last AM4 cpu that got released is the Ryzen 5 5600F, which happened september. So only 3 months  I wouldnt be surprised if AMD comes up with something new or reintroduce 5800x3d and 5700x3d (these cpus only existed for a year or year n half. Which is very short)  Its been 9 YEARS since AM4 got released. AMD still releasing cpu's for it.. crazy",Neutral
AMD,"the thing is you can still get 32gb of ddr5 for two or three hundred bucks. it won't be a great bin, and definitely won't have the overclocking potential of high end hynix a-die but frankly the vast majority of people can/will just run JEDEC and not be able to see the difference. That may be easy for me to say sitting on 64gb of a-die running tight timings, but you/I seriously don't need high end ram to build a pc and run games flat maxed out. Prices have gone up but generally if you could afford to build a PC before the rampocalypse you can still do so now, you're just going to have JEDEC speeds. Which are actually, totally fine if you're using your pc and not a hardware snob (which I am admittedly). ​",Negative
AMD,5950X3D? Or even 5950X3D2?,Negative
AMD,No  Not enough volume for amd to restart production,Negative
AMD,Before the AI bubble I would have said no way. With the AI bubble making new unaffordable for the next 12-18 months at a minimum I'd say maybe.  People on AM4 aren't going to buy AM5 if they have to spend 200% of their budget to get it. DDR5 RAM prices are really going to cripple the PCbuild industry for the next year.,Negative
AMD,"If I were a top AMD executive I'd be focusing on these things  1. Getting Radeon THERE for AI purposes   2. Making the EPYC line amazing for data centers   3. Finding ways to optimize costs and cut risks     Targeting budget customers is fairly low on any list I'd have.    There's probably some value in keeping Zen 3 dies in production but they'd get minimal priority for anything new or cutting edge. Minimal development efforts. Zen 4 is already getting ""old"" by industry standards. There's not much point to getting anything newer to work with AM4 IODs either.",Neutral
AMD,"The production of the X3D variant of the core chiplets has been discontinued. Presumable it was done so that they can use those lines to produce Zen 5 X3D variant. So unless you want them to discontinue the current gen to put old already somewhat obsolete generation back into production, it's not viable idea.",Negative
AMD,"Could they *release* a new AM4 CPU? No. They'd have a very hard time convincing board partners to update all their BIOSes. Could they *manufacture* new stock of *old* AM4 CPUs? Possibly. But odds are all the manufacturing capacity they would need has already been bought out, so it would take years to do; it's not like the old fabs are just... sitting around, empty. They're either converted to new, smaller nodes or repurposed for other chips and projects, or scrapped entirely because they're worn out.",Negative
AMD,"First of all, never trust inflated prices on eBay. It's well known that scam artists who have hordes of collectibles or scalped/limited items who will orchestrate sales of items at inflated prices to drive up the market. Yes, they have to eat the cut that eBay takes, but if all of a sudden they have 10-50 items that they can sell for 20-100% more, it's worth it.   Like what people have been doing with VHS tapes on eBay for the last 6 years.   Secondly, AMD hasn't stopped releasing CPUs for AM4. They recently released the 5500X3D for the Brazilian market in June, and the 5600F in others.   The biggest issue is there's only a handful of AM4 boards still for sales and DDR4 RAM in the retail channels is almost entirely gone. Sure you can find used RAM, but minting new products that requires parts that are no longer in production is suicide.   Micron recently said they'd keep making DDR4, but I ain't seeing much available in the US. I assume much of that is still being [shuffled to the enterprise market.](https://prerackit.com/memory-markets-in-turmoil-how-chinas-exit-from-ddr4-manufacturing-triggered-a-server-ram-pricing-crisis-in-2025/)",Negative
AMD,"AM4 is just too good for me to ditch. My 6800XT holds it back in 4k gaming, 64GB 3600Mhz CL16 was dirt cheap and both single threaded and multithreaded performance with my 5950X is more than I really need.",Positive
AMD,Why would they do that when they can just sell a new x3d chip at inflated prices and have every single one of them sold before they've even been shipped?,Negative
AMD,"Considering they launched the Ryzen 5 5500X3D only 6 months ago, i would say they can, the problem is if they will. Unfortunately it can take a long time to design or adapt existing designs, test and then manufacture them in sufficient numbers for release, if they started now it would probably take a year or more to see them on the market. A 5950X3D does sound cool as hell, maybe i would upgrade from my current 5700X3D, though this CPU is perfect for my needs and the gaming i do.",Neutral
AMD,"Yes, they have actively done so in the last year and are being pushed to re-release the 5800x3d by retailers.",Neutral
AMD,"DDR6 is coming out soonish in 2027, so when that happens the cutting edge ai shit will all switch over to that. So ddr5 availability should go up permanently after that. It's gonna be a dry ass desert until then but it won't be forever. DDR5 came out in 2020 so it's already fairly old, making a switch back to 4 even temporarily quite unlikely.",Neutral
AMD,"The only one that would *maybe* make sense, and I stress it's a biiig maybe at this point, would be a 5950X3D. After that, no more AM4 anything. Otherwise, probably best to keep on cranking 5800X3Ds.",Neutral
AMD,"Very unlikely and most probably something on the lower end I’d think.   But a 5950X3D would be incredibly cool though, and a nice upgrade for my 5700X3D. 😄  On the other hand, there are a lot of people out there, me included, on AM4 that aren’t planning to upgrade to AM5 any time soon. So that would be a way for AMD to keep selling CPUs to existing AM4 users of whom they otherwise wouldn’t make money from.",Positive
AMD,they just released the 5600F in September.,Neutral
AMD,"There was some rumors that Zen4 was originally going to come to AM4. They could still do it. Maybe a lot of that work is already done. I'm skeptical there is still much DDR4 in production, and it'll soon all be gone. What we got on desktop was mostly left over stuff that servers didn't want. I don't think they would make these CPUs just for consumer when there is so much more profit in putting that silicon towards servers. If Mircon abandoned Crucial memory for consumers, that to me says a lot about where the real focus is for hardware makers. AMD right now just doesn't care much about desktop anything.",Neutral
AMD,"Restart? They never stopped releasing them, with the latest being the  5500X3D released in June this year.",Neutral
AMD,Imo the 5950X3d is the most likely option as the very last cpu on am4. dual 3XD ofc and with people starting to switch to more expensive and advanced packaging then Cowos there will be enough cowos allotment that amd can buy for older designs,Neutral
AMD,"You can get a 5700x3D, which is a lot cheaper.",Neutral
AMD,"They could mix tiles, but would be a hard sell for motherboard partners. And why buy a am5 x3d when am4  x3d gives almost the same gaming performance?",Neutral
AMD,"They can restart Zen 3 X3D production, with a dual CCD V-Cache 5950X variant and should do so IMV.",Neutral
AMD,AM4 Zen 6 Fan Edition backport manufactured on an Intel node,Neutral
AMD,"Dont think they're advocating for proper new designed chips, just respinning up old ones.    As for TSMC, I'm sure if AMD really cared to do this, they could find some way to give up some more leading edge capacity for older node capacity.  Perhaps via agreement with some other company who would love to bump themselves up the waiting list.    I think bottom line is that AMD isn't gonna be overly concerned with things.  They're still gonna be making lots of money on all this AI stuff themselves selling CPU's and GPU's, they can take hit on the consumer side for a bit.    I also think paying high prices for AM4 processors is very stupid.  While DDR5 has certainly ballooned building on AM5 platform, the reality is that total system costs are still only gonna be like 15-20% higher.  Small enough difference where it will probably still be worth it to go with AM5 in the big picture.",Neutral
AMD,"Two years in advance, I believe...",Neutral
AMD,"There are actually older Zen3 parts with the DDR5 controller, too. As I understand it, Zen cores are largely decoupled from the wider system with their IO die interfacing with the various external components, so that may be the only bit which really needs changed to support one platform it another.",Neutral
AMD,Or variants of it we never got (5900X3D or a 5950X3D).   It's not like TSMC 7 and 6 have companies fighting each other for their wafers at this point.,Negative
AMD,"Part of me wants to believe it would be possible for AMD to use the old DDR4 IO die and pair it with newer compute dies with Zen 4 or 5, but even if that was true they have no financial incentive to do it.",Neutral
AMD,Im down,Neutral
AMD,They might want it but the rest of us want DDR5.   Nobody's getting what they want in 2026 except the billionaires.,Neutral
AMD,"They want money, and they will do whatever it gives them. Old stuff is sold, makes no profit, so yeah they want more sales. Anyways, stick to ddr4 and if ddr5 demand falls then they will have to adjust.  W11 EOL+ Crypto+ AI ... That is tiny compared to 9.000.000.000 humans that use a pc.",Neutral
AMD,It was pretty clear they were dumping and clearing out stock when the 5500X3D launched during the summer.   EPYC with Zen3 and X3D is probably EOL now. Which is the main reason why AMD kept the desktop parts around as well.,Neutral
AMD,"5100x3d in 2 years, trust",Neutral
AMD,And X3D doesn’t have too much to gain from “good” RAM.,Neutral
AMD,"What about people who just want an upgrade? I would pay a few hundo to get best in-slot CPU on my current platform for a quick bump to give me another year or two.  Right now that's not possible, 5800x3d is no longer manufactured and 2nd hand is prohibitively expensive...  Why not fill that gap with brand new silicon?  The question is - is there fab capacity to make it?",Negative
AMD,5500X3D is just a stockpile of chips that couldn't be sold as better chips.   AMD likely was building that stockpile ever since launch.,Negative
AMD,Even these have gotten insane. Used 5700x3Ds used to go for 150-175 eur. Now they're 300..,Negative
AMD,"I think you're right. AM4 was a great, long-lived platform--I'm still on it myself--but I don't think anyone building now should really be looking at AM4.  Maybe build with 16 GB RAM if you're really on a tight budget, and start with an R5 7600, and upgrade to Zen 6 and more RAM when prices come back down out of the stratosphere.",Neutral
AMD,">As for TSMC, I'm sure if AMD really cared to do this, they could find some way to give up some more leading edge capacity for older node capacity  Is the 7nm node fully booked?  Would AMD have to give up anything to get more of it?",Neutral
AMD,"the mobile chips have a bunch of zen versions (2,3,3.5,4,5) running ddr5/lpddr5",Neutral
AMD,"> There are actually older Zen3 parts with the DDR5 controller, too.  Ye and Zen 4 with DDR4 controller from Zen 3 as well. They both tested the new IO die with the old architecture and Zen 4 with the old IO die during development.",Neutral
AMD,"For the silicon itself no, but if you want X3D parts packaging is the bottleneck.",Neutral
AMD,"Not really, it's closer to less than 2billion people that have a pc and almost all of that is gonna be a basic pc that isn't upgraded. Like with everything a company makes way more profit on a overpriced ""pro"" ai chip than a cheap user version and when you have a trillion dollar order for ai gpus you will have that 50k dollar ai gpu be prioritized over a 1k consumer gpu.  The real problem is that these orders are for speculative demand and so they are just selling all of their future product without regard to how much will actually be needed.",Negative
AMD,It was pretty clear they were making brand new silicon when the 5800XT and 5900XT dropped last year. They might have stopped this year despite several new AM4 releases but availability makes me think they never stopped at all.,Neutral
AMD,they definitely help a lot.,Neutral
AMD,"where were you last year when they were practically giving away 5700x3d on ali express? it was very clear at the time it was a very limited time deal as the chips were out of production and AMD was just using up all the silicon that didn't bin well enough to be a 5800x3d. if you were ok with your current performance and it wasn't worth the time when fantastic AM4's were only $150 shipped, why FOMO and panic now? In any case anyone building \*now\* can still get a 7500F which will out perform any AM4 CPU for $150, paired with whatever mobo is cheapest and JEDEC tier ram, then upgrade the CPU and ram in the future and be sitting pretty. Fab space is booked up for years solid, scheduling a brand new run specifically for people who missed the boat on a cheap drop in AM4 life extension isn't going to happen and wouldn't be economical for anyone if it did.  Also, are you aware you can currently just get a 5900x from ali for $250 all in? That's your one step AM4 life extension solution right there, they're available, and quite good chips. 12 core, 4.8Ghz boost, they're more than enough to get you a few more years.",Negative
AMD,"> Right now that's not possible, 5800x3d is no longer manufactured and 2nd hand is prohibitively expensive... >  >  >  > Why not fill that gap with brand new silicon? >  >  >  > The question is - is there fab capacity to make it?  And would it be profitable for AMD to release it at prices that you and others wouldn't consider prohibitively expensive?  I took a quick look at ebay, and it looks like the 5800x3d is going for just under $500, and the 5700x3d just under $350.  What price would those CPUs have to sell for new for you to consider them a good buy?",Neutral
AMD,"Hey Admirable_Bid2917, your comment has been removed because it is not a trustworthy benchmark website. Consider using another website instead.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Negative
AMD,"The exception is if you already have solid DDR4. In that case, it can be a pretty good idea to make a new AM4 build.",Positive
AMD,"> I don't think anyone building now should really be looking at AM4.  You're joking, right? As if anyone aside from Brad Pitt's, Elon Musk' kids or some wealthy politician could even afford actually *newly* released stuff these days, given all the price-increases …",Negative
AMD,"I mean....that's pretty much true for everything now though, isn't it?  Given how Sammy's more or less fucked the entire market, not just DIY, and some hyperscalers are finding it pretty damn hard to jump to newer EPYC platforms, I can see there being enough demand to justify firing the Zen 3 X3D line up for a little bit.",Negative
AMD,X3D is a separate production line with its' own constraints. There is limited packaging available for X3D that is not shared with the normal SKUs.  X3D being EOL is not tied to the normal Zen 3 SKUs. The normal SKUs they can shurn out as long as there is demand from both server and desktop.  X3D however might very well be supply constrained. And making lower margin Zen 3 variants might cut into scaling Zen5 X3D SKUs.,Neutral
AMD,[TechSpot/HUB tested with 6 different RAM kits](https://www.techspot.com/review/2915-amd-ryzen-7-9800x3d/)  There was less than 2% variance between them.,Neutral
AMD,"5900x sounds perfect for my needs, thank you :)  Why are 5800x3d going for silly prices then? Just random market insanity?",Positive
AMD,People are gonna cry and want $200 5800X3D forgetting it had an MSRP of $450...,Negative
AMD,"There's a whole 30%ish of the American population (loosely college educated several years into their career and moderately successful small business owners) that are doing better than ever.   The median person (not highly educated, minimal to moderate career development) might be feeling financial pressure, but there's enough of the top third (100 million people) RIFE with cash to prop up entire industries.      There's a lot of people for whom a new computer every few years is something like 1-3% of their disposable income.",Negative
AMD,"> I mean....that's pretty much true for everything now though, isn't it?  Not for stuff made with traditional packaging tech",Neutral
AMD,"you can definitely squeeze an extra 2-3% by fully tuning hynix a-die, EXPO profiles are \*really\* loose in all the subtimings. But most people won't bother. As someone who spent a lot of time tuning ram the juice definitely isn't worth the squeeze for normal people.",Negative
AMD,"Just supply and demand. X3D's are hype (with good reason), the AM4 ones  are out of production, and people who have them aren't upgrading/selling because they are still very capable. The price isn't proportional to performance, the 5950x or 5900x for example are within 5% in performance but half the price since the x3d hype is so powerfull. Not that they don't deserve that hype, but other excellent options are overlooked as a result of how they dominate the discussion around cpus for gaming.",Neutral
AMD,"I bought a 5700x3d last fall for $135 including shipping and tax, and at that price it was a no-brainer to upgrade.  But I had already decided that $450 for a 5800x3d was too much when I already had a 5600x.",Neutral
AMD,No one is buying this for 800  Stop the sensational headlines  You can get a 9800x3d and 32gb ddr5 cl30 kit at MARKET PRICE for under 800,Negative
AMD,"Another clickbait article, yeah no shit if you order by highest price sales on ebay it'll look like this. If you instead look at recent sales this month they're all around $390-530.",Negative
AMD,How many people on am4 actually made the leap to 5800x3d than just the normal 3600 > 5600x/5700x and thats it or 1600x to 3600x to just 5600x?,Neutral
AMD,Glad I bought my 64GB of RAM months ago. Cost less that $100,Positive
AMD,"Hello I_Love_Cape_Horn! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,Honestly I want a 5600x3d with the lower power draw and TDP. My mATX system is somewhat small and I’m not playing the most demanding games on it but I’d like to get a few more years out of AM4,Neutral
AMD,There was a brief period these chips were an excellent buy and very cheap but that ended a while ago. Last time I checked at the middle of this year they were £350+ and now I see they are £450+.  I get that it allows you to max out an AM4 system and if you have a bunch of DDR4 that may be a wise investment still but the days of it being the undisputed price:performance king are long over.,Negative
AMD,"What? I mean if you'd actually wanted to spend 800$ on just the CPU, why not just get a cheap AM5/1851 one and then get yourself some overpriced RAM instead?  This makes no sense at all.",Negative
AMD,No shit.  This happens with all older hardware as companies want you to buy new stuff.,Negative
AMD,I was thinking to sell my old DDR4 memory....,Neutral
AMD,This crap of supply and demand can stop anytime now.,Negative
AMD,The RAM I own will appreciate faster than gold in the next year or two woohoo,Positive
AMD,What enthusiasts are on 3 generation old hardware? I consider myself a pretty big enthusiast and never skip more than 1 generation.,Neutral
AMD,"oh ye, i am selling my one for $69420...now go write another article",Neutral
AMD,Trash fake news. Delete this garbage,Negative
AMD,Jumping from am4 to am5 is impossible though because of insane prices too,Negative
AMD,"I upgraded my AM4 AB350 system from first gen Ryzen 1600 to 5700X3D and the gaming results lined up with the reviews. But these chips falter with high RT usage games that have even higher demands on the CPU and RAM.    For example, in Stalker2 CPU-limited scenario, 9800X3D was almost 2.5X of its performance. I doubt that non X3D would be more than 20-30% slower, so it'd be better to get AM5 instead.",Negative
AMD,Why do we still allow Tom's Hardware articles in this sub at all?,Negative
AMD,I just bout a Ryzen 7 7000 series drum 220 brand new.,Neutral
AMD,Someone wanna give me $1000 for my... DDR3!,Neutral
AMD,Am4 is dead.   LGA 1700 is where it’s at rn. Having compatibility for either ddr4 or ddr5 ram is much better than am4.   And the lga 1700 CPUs especially the 13th and even more the 14th gen intel CPUs like the 14700k are way faster than any am4 cpu as they compete with am5 CPUs anyways.,Positive
AMD,"Maybe I should sell mine, I'm using it in an HTPC that I rarely game on anyway.",Neutral
AMD,I just sold mine for 375$ yesterday.,Neutral
AMD,"Pleased my 5950x, 3090ti and 64gb of ddr4 are still keeping up just fine….",Positive
AMD,This is more sad than anything,Negative
AMD,something something... turntables.,Neutral
AMD,"A 7600X is faster than a 5800X3D in most games, if you are buying a 5800X3D for $800 you are an idiot.  A 5600x will still be GPU limited at the resolutions and settings people actually play games at.",Negative
AMD,"There are still places that do sales on AM5 stuff. You can buy a 7800X3d , 32GB ram and motherboard for $580. There is literally no need to spend $800 for a 5800X3D",Neutral
AMD,Gamers would rather pay 800 for this than get Intel. The gamer brain rot is real.,Negative
AMD,Computers are quickly becoming a luxury like in the 90s.,Neutral
AMD,"they are trying their best to milk the market. Push prices up...   Fucking Tech websites, all in the pocket of other big corpo's making artificial scarcity.",Negative
AMD,"A couple of sales, (literally just a few) on ebay is no big deal...there's wacky people doing wacky things out there all the time.  The two or three people are pointing out on ebay are dwarfed by the normal sales flow from normal websites...     ""a thing happened ***once***!  let's all discuss it like it's a regular occurrence and changing the status quo! ""    it's all so tiresome.",Negative
AMD,"Check eBay.  [https://www.ebay.com/sch/i.html?\_nkw=5800x3d+cpu&\_sacat=0&\_from=R40&rt=nc&LH\_Sold=1](https://www.ebay.com/sch/i.html?_nkw=5800x3d+cpu&_sacat=0&_from=R40&rt=nc&LH_Sold=1)  $800, no not quite.  Over MSRP?  Yes.",Neutral
AMD,"Yep.   https://www.microcenter.com/product/5007092/amd-ryzen-7-9800x3d,-asus-b650e-e-tuf-gaming-wifi-am5,-gskill-flare-x5-series-32gb-ddr5-6000-kit,-computer-build-bundle  CPU, memory and motherboard $679.00",Neutral
AMD,> at [MARKET PRICE](https://youtu.be/5KXrQYWbbIs?t=17),Neutral
AMD,What motherbard ?,Neutral
AMD,"Yeah this headline is stupid. I see *new in box* 5800X3D selling for $500-600 on ebay, and plenty of used sales in the last few days between $380-500. The price has risen a bit, but not that much. It's been expensive ever since production of the 5700X3D ended.",Negative
AMD,"And even then, sales VOLUME is what matters. There will always be people who can't do basic arithmetic, who will then buy at these prices, instead of selling their old system and getting something brand new. That doesn't mean that $400 for a 5800X3D is sound market pricing, or that it is worth this much.",Negative
AMD,"I went from 2700X to 5800X3D and upgraded ram from 16GB to 32GB, so I'm good until Zen 6 at least. I got it for 340€ like 5 months after release in germany. If I sold it now I would make 50€ profit, its going for around 390-400€ used on ebay. Not bad.",Positive
AMD,"A 5600x will play games just fine, at the resolutions and settings people play games at you will still be GPU limited on a RTX 5070.",Positive
AMD,I went from the 5800x to the 5800x3d. Stuck the 5800x in my daughters PC instead of buying her a 5600x like I did for my son's PC.,Neutral
AMD,I built my PC a few years ago and since the cost difference was negligible in the overall build I went with 64gb of ram to fill up the 4 slots on the motherboard. When I was asking build opinion on a PC sub many people called me an idiot for wasting my money on all this ram.   Just kind of goes to show there’s no sense of asking Reddit for opinions.,Negative
AMD,"I bought a pair of 16 gig sticks in an attempt to beat the first round of tariffs. At the time I thought they were DDR4 and I wasn't paying much attention. They came in, I see they are DDR5 and think ""ok, future rig then."" They came in around 120 bucks. Fast forward to today, I check the price of the same pair of sticks, it's over 400. Genuinely considering selling the sticks for a better AM4 CPU.",Neutral
AMD,You can limit the power usage on the 5800x3d or 5700x3d as well if you find a deal on one of those.  I have the 5800x3d and the low power usage is great.  In a lot of games the chip is only drawing ~50W.,Positive
AMD,"It really only makes sense if you already have an AM4 motherboard. Otherwise, if you're doing a DDR4 build Intel 14th gen outperforms it.",Neutral
AMD,Because no one sane would do that... they simply took the most expensive offer on eBay and made a clickbait title around it... I may list my old PC parts for 10k; maybe they shall made another such article...,Negative
AMD,"Or it's discontinued and a sketchy seller is hoping to con someone who doesn't know any better. I've seen Ryzen 5 2600's going for 200+ on Amazon from 3rd parties while official channels were selling Ryzen 5 5600's for 130 and 3600's for 90.  Honestly, whenever I see a price on Amazon that's not remotely close to a whole number or 25 cent increment (eg. 137.53), I pause and see if something's fishy.",Negative
AMD,Could also be used by sellers to fulfill insurance replacements.,Neutral
AMD,"Seconding this, a lot of flagship CPUs released in the past 20 years are still absurdly expensive. QX9770, FX-60, P4 EE, 6950X, 9900K, etc. some of them more expensive than others, but the top CPU for a dead socket is still a pretty penny",Negative
AMD,I thought we all loved Capitalism?,Neutral
AMD,"You can still buy laptops for $200 on Amazon, they good enough to do most computing tasks.",Positive
AMD,I had a PC in the 90s didn't know it was a luxury back then guess I was a lucky kid,Positive
AMD,"Compute has never been cheaper and fairer. You can get a Mac Mini right now for $450, which will do all you need, including light productivity, sans gaming and heavy 3D rendering etc, for the better part of the next decade.   All the harder compute that you need you can rent and get the best value, instead of shelling out for hardware that becomes obsolete within 2 years:  * for inference either get the subscription or pay as you go on Replicate, OpenRouter etc.   * for gaming get GeforceNow or similar service for $10/$20 a month. On MacOS it runs natively in AV1 with Cloud GSync, there's virtually no lag and even in very dark scenes you can barely tell it's a stream. Hardware in the back gets upgraded every 2 years, and 5 years of the service cost less than the GPU it's running on right now.",Positive
AMD,"I get your whole point, but I just want to pinpoint the wacky part, maybe they have a reason for it. Like, one quick example, they have the whole built already working, and the CPU dies or gets fried, or god knows what, and they want the same CPU, idk. Which goes hand in hand with your other point of it being 2-3 sales. The CPU is not being made anymore, so Ebay is the only alternative. Maybe even a collector. There are wacky people out there thought :P",Neutral
AMD,">$800, no not quite. Over MSRP? Yes.  MSRP was $450 and in your link there are a bunch listed at $399  What am I missing?",Neutral
AMD,Still too expensive   Still the price of a non x3d zen4/5 or raptor lake + ram kit which outperforms this,Negative
AMD,Yea I sold mine for $425 two weeks ago after upgrading to 9800x3D for $440,Neutral
AMD,"People are insane, in what world does that purchase make any sense.",Negative
AMD,"A fair number of these purchases are probably going to end up with fraudulent charges (item not as described, etc) because eBay tends to side with the buyer.",Negative
AMD,It's worth whatever it consistently sells at. It seems to consistently sell for >$400 used.,Neutral
AMD,"It's a lot like the ""car is worth $2000 but needs $2000 in repair"" work making it worthless but they're low credit high APR folks and have cash and $2000 in repairs makes sense to get it going another 2 years",Negative
AMD,"Unless you play heavily CPU bound games, which many BRs, and simulation/factory games fall into.",Neutral
AMD,"""Games"" are not a uniform performance load. Tons of games will be CPU bound with *any* CPU, including the 9800X3D, even at 1440p.  As always, know your workloads and purchase accordingly.",Neutral
AMD,"For productivity yes, but not for gaming.",Neutral
AMD,"If you filter on Sold listings, people *are* paying $4-500+ for these.  It’s still insanity even if the headline is sensationalist.",Negative
AMD,Oh? I have a 6950X sitting in a closet…,Neutral
AMD,I had a 9900K in my workstation at the office back in the day.  Solid performer for the money.,Positive
AMD,Adjusted for inflation it’s around $7000. Yeah they were luxury,Neutral
AMD,Stop being logical   We need 64gb and a 5090 to browse reddit and watch youtube,Neutral
AMD,"Mac's can run games  (native ports) decently too, considering their loss power draw.",Neutral
AMD,"Okay Sam, no need to advertise your shitty services.",Neutral
AMD,"One just sold for $600  Sold Dec 19, 2025    Brand New  [51 product ratings- AMD Ryzen 7 5800X3D Processor - NEW in Sealed Box](https://www.ebay.com/p/4053561416?iid=168015596458#UserReviews)  **$600.00**  or Best Offer  \+$7.50 delivery  Located in United States  [View similar active items](https://www.ebay.com/sch/i.html?_nkw=AMD+Ryzen+7+5800X3D+Processor+-+NEW+in+Sealed+Box&_id=168015596458&_sis=1)  [Sell one like this](https://www.ebay.com/sl/list?mode=SellLikeItem&itemId=168015596458&ssPageName=STRK%3AMEWN%3ALILTX)  gangster1234484 100% positive (522)     Being kind and logical, that's what you're missing :) <3",Neutral
AMD,"The RAM kit alone is more than the 5800X3D.  People aren't buying the 5800X3D over a new AM5 processor. They're buying it over a whole new platform, which is ~$1000 including board and RAM.",Neutral
AMD,What?  I'm a Top Rated Seller and very happy,Positive
AMD,5800X3D is [13% slower than a 14700K with DDR5-6000 in gaming](https://www.techpowerup.com/review/intel-core-i7-14700k/18.html). Unfortunately I can't find a direct comparison with the 14700K using DDR4.,Negative
AMD,14700k slams a 5800x3d both in gaming and especially on productivity tasks.,Neutral
AMD,"For some upgrading to AM5 would indeed cost a lot right now, but I also feel like a lot of people don't realize that the slowest AM5 CPU (7600X) is just as fast as the 5800X3D. Including for gaming.",Neutral
AMD,Nice! Those go for around $125+ on eBay regularly. People want to max out their old rigs,Positive
AMD,">Being kind and logical, that's what you're missing :) <3  Great, but that still doesn't explain the dozen other listings for new in-box chips at under MSRP prices",Neutral
AMD,"Pretty sure it's just not showing the ""Best Offer"" price it *actually* sold at.",Negative
AMD,What's so rude about what he said?,Neutral
AMD,No its not  Newegg has combo deals ram and good board for 400ish dollars  A crappy 7600 beats the 5800x3d,Negative
AMD,"There are edge cases where no CPU can touch the X3D chips (e.g. Factorio, Baldur's Gate 3).",Neutral
AMD,"Hardware Unboxed's 5800x3d vs 12700kf (with ddr4) comparison was pretty much a wash, particularly if you are >1080p. Nominal win for the 5800x3d for gaming if you are being charitable. (I have both, but haven't done any real comparison testing, particularly because I run 4k.) So add on top some clock speed gains and there you go.",Neutral
AMD,Yeah I don't really get it. When they were sub 200 or even sub 150 for a 5700x3d it was an awesome buy but now it just feels like people panic buying.   Maybe they were waiting for zen 6 and just are afraid that ddr5 will be screwed for many years but I don't see the point in paying that much when you can still buy a faster 7700x microcenter bundle  even with 32 gb of ram for 500.  Even with ram like this I don't see why you would pay over like 250 tops for a 5800x3d.,Negative
AMD,"Even in factorio once you go to [high spm comparisons](https://factoriobox.1au.us/results/cpus?map=9927606ff6aae3bb0943105e5738a05382d79f36f221ca8ef1c45ba72be8620b&vl=1.0.0&vh=) rather than the the low SPM ones, raptor lake/non X3D zen4/5 does fine.",Neutral
AMD,"7ghz is a pipe dream, leakers saying that should be instantly discredited, OC3D really shouldn’t be mixing dubious leaks (7ghz nonsense) with legit sources (GCC code base changes) in the same article",Negative
AMD,">AMD plans to release Zen 6 Ryzen CPUs in late 2026. Based on prior “Medusa” leaks, these CPUs will feature up to 24 CPU cores with two 12-core CCX/CCD chiplets. This increases the maximum core count per chiplet from 8 to 12. Furthermore, it increases the L3 cache per CCX/CCD from 32 MB to 48 MB.  That's good news, I suppose.  AMD could've gone the cheapskate route (that Intel took with Skylake) and just stuffed more CCDs per wafer by sticking with eight cores.  Profits!  After all, the i7-6700 (\~120 sq-mm) was almost half the size of i7-2600 (\~220 sq-mm). Even the hexa-core i7-8700 was just around \~150 sq-mm.  Of course, it remains to be seen whether the core count will actually get bumped across the SKUs. I'm cautiously optimistic since N2 reportedly has excellent yields so it doesn't look like AMD is trying to compensate for poor yields with the 'extra' four cores.",Positive
AMD,">AMD has shared new information on its Zen 6 CPU architecture, confirming several new features for the next-generation processors. This information comes through two sources: a Zen 6 compiler update for GCC 16 (via Phoronix) and a new Zen 6 document from AMD.  >With AMD’s new GCC compiler update, the company confirmed several new ISA capabilities for Zen 6. This includes AVX512_BMM, AVX_NE_CONVERT, AVX_IFMA, AVX_VNNI_INT8, and AVX512_FP16. This aligns well with AMD’s comments about “new AI data type support” and “more AI pipelines” when discussing Zen 6 at their 2025 financial analysts day.  >In AMD’s Zen 6 performance document, FP16 support has been confirmed, as have changes to the memory profiler and to Zen 6’s integer schedulers. If this document is accurate, AMD is moving away from one unified scheduler with Zen 6 to six separate schedulers with Zen 6. It is currently unknown why AMD is making this architectural change. Regardless, it appears some significant changes are being made to Zen 6’s design. Has AMD moved to move, smaller integer schedulers in an effort to boost Zen 6’s clock speeds or efficiency?  >With FP16 support, AMD’s Zen 6 CPUs will be much more capable of calculating certain vector and mathematical calculations at an accelerated rate. This could be part of AMD’s AI push with Zen 6, though it will be useful for other tasks.  >AMD plans to release Zen 6 Ryzen CPUs in late 2026. Based on prior “Medusa” leaks, these CPUs will feature up to 24 CPU cores with two 12-core CCX/CCD chiplets. This increases the maximum core count per chiplet from 8 to 12. Furthermore, it increases the L3 cache per CCX/CCD from 32 MB to 48 MB.",Positive
AMD,Any words on what integrated USB standard desktop Medusa will have? And if it uses advanced packaging of strix halo to reduce uncore power drain? And if it will have XDNA 3 NPU onboard?  None of these matters to the diy build PCMR crowd that obviously dominates this sub but are extremely important to prebuilt desktop adoption and later the HX-class workstation/gaming laptops which use the same dies.,Neutral
AMD,As soon as I read 7ghz clockspeeds being targeted I didn't need to read the rest of the clickbaiting hopeium,Neutral
AMD,"Do anyone want FP16 inferencing anymore?  I thought everyone was switching to MXFP4 or similar.  For geometry, FP32 or FP64 is more interesting.",Neutral
AMD,"Unfortunately, nobody will be buying it unless DDR5 prices come down.",Negative
AMD,Looks like this will be a good upgrade from the 7950X3D so I can throw it into the backup system.  Shame that RAM is so expensive tho. I really should have bought 128GB at least earlier this year.,Positive
AMD,"man, should I just hang on to my broken hinge laptop until 2027 instead, so I can get Zen 6/Nova Lake/RTX 60 series laptops instead?  all three of them seem like a proper generational leap compared to what we're getting in 2026, would feel really bad to get a laptop only for a much better one to show up the year after",Negative
AMD,will these be backward compatible with current motherboards?,Neutral
AMD,good to buy a 9800x3d now then?,Neutral
AMD,"sounds like a bigger uplift then zen 4 to zen 5, which was expected. i might upgrade again from my 9800X3D anyway. we'll see how stuff plays out.",Positive
AMD,Hope they do another am4 round lol,Neutral
AMD,It’ll be 25% faster in multi and 10-15 in single.  That’s my best guess with no evidence lol,Neutral
AMD,">Another leak claims that AMD is targeting 7 GHz clock speeds with its Zen **7** CPUs. If this is true, AMD should deliver significant gains in single-threaded CPU performance.... If this leak is true, AMD’s Zen 6 CPUs will benefit from... higher clock speeds.  Is this a typo or really an alleged leak about Zen 7, before returning to discussing Zen 6 again? Confusing.",Neutral
AMD,"Unless it uses / supports ddr 5 so I can use my existing memory, no way. Too expensive to upgrade.",Negative
AMD,"As you add more cores you have less memory bandwidth available per core.  For a game or application that can use all of the cores performance starts to scale poorly.  If you have only one stick of RAM for instance games are something like 10-15% slower.  That would be more than offset by extra cores so performance obviously doesn't drop, but it's also not going to be 50% faster just because you have 50% more cores.  Hopefully we get 4 channels of RAM on consumer platforms one day.",Negative
AMD,"Eh, CPUs have gotten pretty boring over the past few years.  They offer more performance than most people need. Their impact on gaming is neglible after a certain point. Improvements seem to settle around the 10-15%. every two years.  It's incredible what they manage to squeeze out of X86, but it's no longer the future.",Negative
AMD,"Given the lack of memory, AMD would be more successful returning to AM4 and supporting the existing customers rather than the few that made it to AM5 before AI destroyed its profitability.",Neutral
AMD,they just announced them so they can say its out of stock in 2 months,Neutral
AMD,"ChatGPT got this from it. Looks like most stuff happened in the front-end. But don’t take away too much from it.  > AMD Zen 6 (Family 1Ah, Models 50h–57h) can be identified through AMD’s official performance monitoring documentation, even though the marketing name “Zen 6” is not used directly. The PMC manual confirms that Family 1Ah corresponds to a new core generation with significantly expanded observability and capability, implying a major microarchitectural step beyond Zen 4/5. The document is dated December 2025 and targets production silicon, not pre-silicon speculation￼. >  > From a core and frontend perspective, Zen 6 supports dispatch of up to 8 macro-ops per cycle, indicating a very wide frontend and backend. The architecture clearly relies on an Op Cache, with explicit counters distinguishing ops sourced from the Op Cache versus the legacy x86 decoders, and dedicated Op Cache hit/miss metrics. SMT behavior is deeply integrated into the design, with counters explicitly attributing lost dispatch bandwidth to sibling-thread contention, suggesting more aggressive SMT scheduling and arbitration than earlier Zen cores￼. > > In the execution and memory domains, Zen 6 exposes full 512-bit (ZMM) vector execution with first-class accounting for FP16, BF16, FP32, FP64, and VNNI operations, confirming AVX-512–class capabilities. The memory hierarchy remains CCX-based but is now fully NUMA- and CXL-aware, with performance events distinguishing local vs remote CCX, local vs remote DRAM, and near vs far extension memory (CXL). The L3 cache supports sampled latency measurement per CCX, enabling precise observation of memory behavior across sockets and memory tiers￼.",Neutral
AMD,"It's the usual with AMD rumors. Nothing makes sense.  If you ask me, Zen 6 will have a 512-bit memory bus with 10GB of L1 cache.   And tech journalists are really quite bad these days. The good ones all left to go make more money.",Negative
AMD,Its the same guys saying Zen 5 will have 40% IPC in spec.  Their creddit should have long stayed in the toilet,Neutral
AMD,"Anything MLID reports about a product more than like 6-12 months in advance is absolute guaranteed garbage(and even past that, still usually garbage, but rarely he does get credible info and will usually show proof of it).  Especially when it comes to AMD stuff which he constantly overhypes to high heavens.  It's insane the things he has reported about Zen 3, 4, and 5 well ahead of time.  To say he was simply wrong about things would be a gross understatement.  He's a clown car sitting upside down in a flaming dumpster level of wrong.",Negative
AMD,"AMDs 9950X is rated at 5.7 GHz currently on TSMC N4P.  So while 7 GHz is a bit high, the question is if AMD is goig for clocks more aggressively than redesigning the entire core to make it wider, how much do we expect from going from N4P to N2P/X?  Apple has increased clocks between the M3 on N3B to M5 on N3E from 4.05 to 4.6 GHz. And Apple has a very high IPC architecture, so its not super easy to compare, but I dont think that something closer to 7GHz than to 6GHz is unrealistic for a more clock-oriented uArch.",Neutral
AMD,"We're at 5.6+, right? Article states 7 GHz for Zen 7. Zen 6 is late 2026. Zen 7 will be 2028-2029. Why not 7 GHz? It'll be < 2 nm.",Neutral
AMD,Will they still be compatible with AM5 and/or use DDR5 ?,Neutral
AMD,I think it's possible because it's 5nm (we've been stuck at 5nm forever now) to finally 2nm.,Positive
AMD,"the biggest problem with Ryzen chiplet design is their IO die idle power. It is still their biggest weakness compard to Intel's.   They should have add some CPU cores on IO die, so when on low load, the chiplets die can power down.",Negative
AMD,"Going above 8c per CCD is absolutely a BFD, it makes so many things easier software wise, especially for gaming. I hope the new consoles that both are supposed to be based on Z6, also increase the core count. We seem to be stagnating a bit core wise for gaming at least, I remember with Z2 and Z3 lots of people were assuming that gaming would benefit from 12c and 16c very soon, that turned out not to be the case as new CPU's with drastically better IPC came out and 6c is bearable and 8c fine as of 2025. The reason I want the baseline increased for consoles and 1 CCD SKU's is headroom for background stuff. And the next gen consoles will probably last ~10 years into the future from today if they launch in 27/28, so staying at 8c frightens me even though its fine *now*.  I would not be too optimistic about the lower end, we'd be very lucky of the new low end was 8c, but I fear it will be 6c. History tells us that the cheapest SKU is always woefully insufficient and that's probably rational from the perspective of a sales department.",Positive
AMD,"I suspect the main driver for increasing chiplet core count is datacenter. This way, they can increase core count without increasing the complexity of infinity fabric even further.",Neutral
AMD,I wonder how the threadrippers will look,Neutral
AMD,My guess it is USB4 2.0 with 80 Gbit/s symmetric connections or asymmetric connections at 120 Gbit/s in one direction and 40 Gbit/s in the other.,Neutral
AMD,I wish Zen 6 Desktop had an NPU and actually good idle power drain.,Positive
AMD,"Did you actually read it?  ""AMD is targeting 7 GHz clock speeds with its Zen 7 CPUs"".  Not Zen 6 next year.",Neutral
AMD,"Most 16bit LLMs use bf16 over fp16. The `AVX512_FP16` instructions should be more valuable since it conforms to IEEE 754 and not one of the formats cooked up by Google, Nvidia, et al strictly for AI/ML.  Half-precision is relevant in graphics- color operations (e.g. post process effects, convolutions, blending, etc). Despite what you claim, even geometry (e.g.- normalized vectors, simple affine rotations, some physics solvers, etc) don't always need fp32. Yes, while GPUs have supported this, native support in CPUs is welcome.  Native fp16 should also help emulation, notably emulating ARM on x86.",Neutral
AMD,"Yes.  FP16 is the default/standard for the KV cache -- the dynamic context that holds the contents of the ""discussion"" and is updated constantly during inference so retaining precision is important.  The large weights (static data) is what gets compressed to MXFP4, INT8 or whatever. It's possible to compress both, but quality takes a dive.  Aside memory speed, long prompt processing is the weak spot of CPU inference... and that's all involving the FP16 context stuff. Smart AI move.",Neutral
AMD,"The switch to very low precision formats is pretty much only useful for LLMs, everything else uses bf16 or even fp32. Anything you'd want to run on a CPU at least.",Negative
AMD,Exactly lol I wanted to upgrade my 5800x3d but I think I'm good for a while lol,Positive
AMD,A lot of people upgraded to am5 already including myself and I would be willing to sell my 7800x3d if its really that much better,Positive
AMD,Loads of people already have DDR5 though.,Neutral
AMD,Why? Anyone on AM5 can just slot this in. No need to buy anything else. Those extra cores are going to help a lot with productivity workloads.,Negative
AMD,People pay 3k for GPUs  There's always people with more money than sense,Neutral
AMD,"I mean if Zen 6 ends up being 12 core per CCD with more L3 cache, I'd totally sell my 9950X3D on used market and grab its replacement, but that's just me.",Neutral
AMD,That and people that already have DDR5,Neutral
AMD,Individual consumers may not be the target market,Neutral
AMD,Bros backup system is 10x better than my main PC,Positive
AMD,What are you doing that requires an upgrade from 7950x3d? I still haven't even pushed mine to its limits yet.,Negative
AMD,So you want to upgrade to a 10950X3D?,Neutral
AMD,"when people ask ""why do you need so much memory"" this RAM shortage will be my answer",Negative
AMD,"Based on the improved node it looks really good and maybe a better than average generational leap. But based on the screwed up market conditions and global instability, maybe we won't see current prices again for a long time making it risky to wait...",Positive
AMD,> Hopefully we get 4 channels of RAM on consumer platforms one day.  Won't happen for a very long time.  DDR6 will introduce 4 24-bit subchannels per DIMM so consumer support will go from 128 bit to 196 bit. So it will feel like triple channel memory in some regards.,Neutral
AMD,">As you add more cores  Those come with a larger unified l3, so lower cache miss rate and less memory read. L3 per core is also dynamic  >For a game or application that can use all of the cores performance starts to scale poorly  Hardly any game fully uses 48threads  >If you have only one stick of RAM for instance games are something like 10-15% slower  No one's cutting their bandwidth by 50%  >but it's also not going to be 50% faster just because you have 50% more cores  No one's sayin that?",Neutral
AMD,"You clearly don’t know what you’re talking about, sorry.   Very few games are DRAM bandwidth bound. RAM latency, yes, but not DRAM bandwidth. You really have to construct extreme and unrealistic scenarios to be even memory bound on two sticks of DDR4-3000MHz!   Latency does not (usually) increase with more cores.",Negative
AMD,"Lol because these new architectures can just work on AM4... not how it works.  Besides AM5 is popular, I'm sure as long it is priced well AMD can get a good lot of Zen 4 owners and Intel 12-14th gen (which many used DDR5 for) to swap over to Zen 6, maybe even more mid end Zen 5 to X3D Zen 6.",Neutral
AMD,Only 512?,Neutral
AMD,They probably have an AI that read your comment and is writing another article now.,Neutral
AMD,Some zen 6 skus are confirmed to actually have a 2048-bit bus.,Neutral
AMD,"512 bit would be eight channels. SP5 Epycs (Zen 4/5) go up to 12 channels, so that's 768 bit already",Neutral
AMD,"They will have 32GB inbuilt  L3 cache, it will totally replace the need for RAM 🫡🤣",Positive
AMD,"Zen 3 with 5 GHz and SMT4!!!  Leakers were crazy about Zen 3, and some people actually believed it lmao",Neutral
AMD,Well the Zen 6 info in the article is pretty reasonable. It's the Zen 7 7Ghz clock (separate) leak that's silly.,Neutral
AMD,"He's increasingly become mainstream and his past behavior whitewashed. I've been in the hw community for very long now and there was always ridiculous speculation and lots of unhinged posters, but its annoying that you know have people like these being 'authorities' on top of that. I know people can change, but AFAIK he hasn't even though I've blocked him for the last 1-2 years, nor has he admitted to the outrageous lying it took to get him to where he is now, which at least would help somewhat in repairing his non-existent credibility.",Negative
AMD,"He’s correct more often than not, for example he called it on Zen 6 being 2nm before anyone else and ended up being correct. This article says nothing about clock speed so that’s still up in the air. The very nature of leaking projects in development is that things can change mid development before launch too, so while the plan that gets reported on is true the end result might be different. Going to save this thread for the eventual announcement and see how it ends up lol",Neutral
AMD,Didn't he hit the nail on the head with the last generation of radeon graphics cards well before anybody?,Neutral
AMD,"Apple is significantly ahead on nodes which is part of the reason why clocks are so high.  |Arch|St clock, GHz|Node|TSMC performance claim (over previous)| |:-|:-|:-|:-| |Zen 2 (3950X)|4.7|N7|\-| |Zen 3 (5950X)|4.9|N7+|\+10%| |Zen 4 (7950X)|5.75|N5|\+5% (+15% over N7)| |Zen 5 (9950X)|5.7|N4P|\+11%| |Zen 6|?|N3P/N3X (?)|\+4-9% (+5% over N3E, N3E+10-15% over N5)|  The clocks don't really match performance claims, but parts of it are due to architectural changes. It unsurprising that Zen 3 or 5 don't have improved clocks since the transistor count increased without much density increase and you have to power those somehow.  They seem to be making the architecture wider if there are more schedulers, but one reason to separate them is because it generally keeps them smaller and allows slightly higher clocks. Given the previous clock development expecting +1.3GHz seems crazy though since it's a larger increase than what's happened over three generations.",Positive
AMD,"There's a power/thermal wall somewhere in the high 5ghz range.  6.5ghz is not 15% more power than 5.5ghz, it can be double or triple the power and thermals.  That's the main reason max clock speeds stayed around the 5ghz for so long.",Neutral
AMD,"My 10850H from 5 years ago was 5.1 GHz (and 10900K from same year was 5.3 GHz), so being at 5.7 GHz right now does not sound like 7 GHz in a couple of years is realistic",Neutral
AMD,For like a decade now shrinking process nodes has only resulted in marginal increases to how fast the transistors themselves are. It's likely that that'll still be the case even shrinking to <2nm so I don't think clock speeds will go up that much. Unless gate all around somehow results in a big improvement but I doubt it,Neutral
AMD,"Zen6, yes. It might not be 100% confirmed by AMD, but they've said enough to make it extremely likely. Also, DDR5 will last longer than initially assumed, everything points to it.",Neutral
AMD,"Shrinking the process node does not automatically lead to higher clocks, or we’d be at 10ghz by now :) the first commercial 5ghz cpu was done at 32nm",Neutral
AMD,"You either clock higher, or use the density for IPC gains. Larger structures clock worse, so it's never a case where major IPC changes, and 50% more cores and major clock boosts increase. Especially when nodes nowadays have pathetic PPA gains",Negative
AMD,"The single CCD chips are also bandwidth limited by the link between the CCD and IO die, which is another issue with the chiplet scheme.",Neutral
AMD,"The difference is negligible, at best:   [https://www.youtube.com/watch?v=YW5VagBJ4tc](https://www.youtube.com/watch?v=YW5VagBJ4tc)  TL;DW:  R5-7600X: 40W idle, 141W peak.  i5-12600K: 37W idle, 189W peak.    R9-7900: 45W idle, 142W peak.  Not sure why some Redditors lose their sleep over idle power consumption!",Neutral
AMD,"PS6 and Xbox Next will use Zen 6 and they will have more cores than current gen's 8 cores but this is also the first time consoles have access to AMDs C and LP cores (current gen is Zen 2) so the next consoles will have a mix of coresd in the same soc. The maximum core count is 11 cores on the Xbox Next which is supposed to be highest end of all the next gen consoles. 3 Zen 6 core and 8 Zen C cores are rumored for the Xbox.    PS6 is rumored to sport up to 10 cores, thats 7-8 Zen 6C and 2 Zen 6 LP cores. PS6 is targeting reduced specs from the Xbox in order to hit a more affordable price and be more compatible with the PS6 handheld that Sony is also developing.",Neutral
AMD,So thunderbolt 5 featureset parity. Hopefully.   They need to provide the full package and make it easy for mainstream vendors to crack the vastly higher volume and stable oem space.,Positive
AMD,?  It hasn’t come out. How do you know it won’t and why are you using past tense,Negative
AMD,Which is still about as likely as my arm turning to chocolate.,Neutral
AMD,"That.. does not make sense. Believe or don’t believe the 7GHz Zen 6 rumors, but it’s feasibility has nothing to do with the speed of light.",Negative
AMD,"Good info, thanks.",Positive
AMD,Isn't that just more work for the CPU? Feels like the CPU is already straddled enough for gaming.,Neutral
AMD,"Same, I'm so glad I got one when I did. I only use my Windows machine for gaming and my trusty X3D is gonna have to hold strong until at least 2027 it looks like. No way I'm paying GPU money for a RAM kit, I'll quit gaming before doing that.",Positive
AMD,I’m holding onto my 5800x3D until AM6,Neutral
AMD,"Including by default anyone who already has an AM5 setup. As a 9600X owner my plan is to upgrade to what should be a 12c/24t X3D part in a couple of years, which will just be a drop-in replacement for my existing system to give it a new lease of life. RAM prices won't be able to ruin my day until DDR6.",Neutral
AMD,"This is the second time around, too. People paid that much for 3090 during the mining craze",Neutral
AMD,Sadly timing was crap this year so I have some excess AM5 parts due to needing to swap Motherboard to get proper 8-8 bifurcation for my GPUs.  And then GPU timing was bad this year so I had to get 7900XTXes instead of R9700s which made the situation worse so I have half of an AM5 system laying around collecting dust.  At least AMD is better than Intel who is a year late on delivering on their AI promises.,Negative
AMD,"I’m on a 7900x and absolutely will be upgrading to something zen 6 towards the end of zen 6, assuming zen 7 is a new socket. I don’t push the processor hard too often (less than 5% of the time) but I want it running flat out for those 5%.",Neutral
AMD,Software Development & AI.,Neutral
AMD,I mean RAM getting expensive isn't a reason to buy more than you need. You aren't saving money if you never use it. You are just spending more money. Obviously if you have a use for it it's a good deal but that use should probably be your reason.,Negative
AMD,shame that laptops dont have the same kind of trade-in deals phones do.....,Negative
AMD,"For real? That's amazing to hear. I had no idea until now!  I was planning to upgrade to the AM5 platform til this RAM bubble hit, but it seems the silver lining will be later on that DDR6 will rock hahaha.",Positive
AMD,I kinda wish one day we getting rid of running DIMM in pairs. There was a time RAM only need to run in 1 stick to get maximum performance.,Neutral
AMD,The Ryzen AI Max is already 4 channels (technically 8 but at 32bit).  But it's super expensive.,Negative
AMD,"\> Those come with a larger unified l3, so lower cache miss rate and less memory read. L3 per core is also dynamic  They have 50% more cores and 50% more cache, so same cache per core.  But yeah, if your game only uses 8 cores, and the other 4 cores understand not to use any L3, it could help.  Would be interesting to know where we would see diminishing returns from more L3.  Check out the Epyc 9175F, it's an odd duck in this regard.  \>Hardly any game fully uses 48threads  Agree, this is why 50% more cores doesn't make much sense for gaming.  I wonder if they'll release a gamer friendly version with 8 cores but also a larger L3 cache.  I'm a weirdo so in mutlithreaded DCS with VR running it'll help for sure, but I'm in the minority.  \> No one's cutting their bandwidth by 50%  That was a thought experiment.  A single channel represents less bandwidth per core, and it has performance implications.  \> No one's sayin that?  It will be 50% faster in multithreading, but only because the IPC and frequency gains will offset any memory starvation.",Neutral
AMD,"Here is my source.  You should tell these guys that their test results are completely wrong!   Also, would love to see your data.  [https://www.youtube.com/watch?v=\_nMu1KFkOC4&t=543s](https://www.youtube.com/watch?v=_nMu1KFkOC4&t=543s)",Negative
AMD,Byte,Neutral
AMD,Maybe Mi400A with HBM?,Neutral
AMD,is this satire or are you for real?,Negative
AMD,Only 10 GHz ones,Neutral
AMD,"You cant be serious.  I could write a short book about the outlandish and wrong claims he's made about AMD products.  And no, he was NOT the first to talk about Zen being 2nm.  That is, as usual, one of the times he was just piggybacking off some other rumor already out there by more credible leakers.  >Going to save this thread for the eventual announcement and see how it ends up lol  And will slink away cowardly when his absurd claims dont come true.  Just like MLID. smh  Hate these confident claims only to never amount to anything and then you pretend you never said it or ever admit you were wrong. I deal with this sort of thing on an almost weekly basis following memestock cultists. lol",Negative
AMD,">He’s correct more often than not  Barely, he is basically a coinflip in accuracy.",Negative
AMD,"Nah, the only thing he got 100% correct was PS5 Pro.",Neutral
AMD,Absolutely not. lol   His track record with Radeon is almost as bad as it is with Ryzen.   You should have seen the INSANITY he was posting about with regards to RDNA3.,Negative
AMD,"Zen 6 desktop is supposed to be N2P/N2X, not N3.  Client/Mobile is N3, Server and Desktop is N2.  I dont think its reasonable for AMD to go for 7GHz, but if that is the strategy and they push it, together with that huge node jump, its not as impossible as claimed.  And also the ""7GHz"" leak is a ""they are trying to reach 7GHz"". So they might just not hit it, but I dont think it sounds as crazy as presented.  TSMC claims Speed@isoPower goes 1.8 from N7 to A14 (and ~1.58 from N7 to N2P) so while these numbers tend to be best case, so very unlikely to just apply to a device you port from N7 to A14 and boom its 80% more clocks, because thats simply not how frequency scaling works, but if AMD is actively shrinking and optimizing internal structures (like the schedulers, which together with the registers+renaming/allocation tend to be the crucial structures) they can pretty easily hit significantly above 6GHz, the question is just how far.  (Additionally the 7GHz are the target for Zen 7, which will likely be on A16, so its even further out)",Neutral
AMD,"N2 to N2P is 18% and N2X is another 10% on top of that. Add that on top of the gains from N4 to N3, it sounds possible.  https://www.tsmc.com/english/dedicatedFoundry/technology/platform_HPC_tech_advancedTech#:~:text=N2P%20delivers%20an%2018%25%20speed,each)%2010%25%20speed%20boost.",Positive
AMD,"> There's a power/thermal wall somewhere in the high 5ghz range.  What wall? This exact same excuse was used for basically every GHz milestone the industry has seen. You'd see people saying that about 4GHz in the Haswell era and 5GHz for Skylake and 6GHz for ADL etc.   > 6.5ghz is not 15% more power than 5.5ghz, it can be double or triple the power and thermals.  There's nothing inherent about higher speeds that would double or triple power consumption. That's only if you try to achieve it by pushing higher voltages through the same node. If the node itself gets faster, you can see the benefits in the same power envelope. Zen 7 should have a ~2 node shrink, maybe better. As a reminder, 2 full nodes for Intel (22nm -> 10nm/Intel 7) ended up with a 2GHz increase (over a much lower baseline) over their lifespan, granted over more time and with extra voltage for the top end.   I'm not sure about 7GHz, but mid-6GHz range should be plenty achievable on an N2P-class node.",Neutral
AMD,"A 10900K was on 14nm. The 6GHz 14900k was on 10nm/Intel 7, or one full node shrink. Going from N4P->N2P alone is a 2 node shrink.",Neutral
AMD,> 5nm is already 20-30 atoms across so it's also hard to manufacture perfectly.  Node designations have been marketing numbers for more than a decade and should not be taken as real feature sizes.,Negative
AMD,I thought the node name in nm was just a marketing thing for over a decade now and that nothing in the chip is actually that many nm in size.,Neutral
AMD,You only need 20-25%.,Neutral
AMD,"RAM is a mess now, i would not be surprised if some Chinese brands don't come up with DDR4 AM5 boards in the next few months. Thanks !",Negative
AMD,And you can absolutely get much higher clocks if you lengthen (mistyped and edited) the pipeline even if it decreases the per clock perf. So why are ya acting like really high clocks are physically impossible?  Clocks mean nothin without looking at the context,Negative
AMD,And you can absolutely get much higher clocks if you lengthen the pipeline even if it decreases the per clock perf. So why are ya acting like really high clocks are physically impossible?  Clocks mean nothin without looking at the context,Negative
AMD,"They could go over 10ghz relativity easily if they designed the die to actually do that, but there really isn't much of a point when it would come at the cost of multicore performance. Some nerds managed to get a 14900kf to over 9ghz, and that's a 3.3-6ghz processor.",Neutral
AMD,"Chips often reduce logic density(aka make structures larger) to *increase* clock performance.  Especially for PC parts.  Because the more tightly packed the high activity parts of the chip are, the more heat it will locally produce, and thus limit performance potential for pure thermal reasons.",Neutral
AMD,"Techpowerup [finds larger differences](https://www.techpowerup.com/review/intel-core-ultra-7-265k/24.html), with AM5 chiplet Ryzens idling about 20-25W higher than Intel CPUs.",Neutral
AMD,And an Intel laptop using just E-cores is not particularly fun to use so I'm not sure what the use case is.,Negative
AMD,"So I guess effectively still 8 cores for the actual games then. Seems like quite a gamble to me for such a long time horizon, even with the stagnation in utilization lately. But I suppose they know (or forecast) best how to prioritize their transistor usage.",Neutral
AMD,Likely not an English first language speaker. These are things he *wishes for* a future product.,Neutral
AMD,"> Isn't that just more work for the CPU?  Short answer: not really, though it's complicated. If anything, native fp16 support could help *reduce* some work being done on the CPU.  Long answer: Don't dismiss the importance of your CPU's FPU. Hardware T&L has been available since the late 90s, yet game engines still do a lot of matrix operations on CPU; there's inherent GPU limitations that are difficult to overcome: high readback latency, lack of fp64 support (on consumer GPUs), game state logic being heavily reliant on sequential logic and deep branching, and numerous other reasons.  Regardless, many of the software bottlenecks we experience are often due to memory stalls, threading- scheduling, synchronization overhead, resource conflicts- or other false work- not because the CPU's FPU is overtaxed.",Neutral
AMD,I think most of us 5800x3d users are going to end up upgrading around next-gen or when games like Witcher 4 come out.,Neutral
AMD,"That’s going to be a long wait, likely 2028-2029",Neutral
AMD,Yep. I've got 2 systems with DRR5 in. One with 32GB and another with 128GB. I won't need to be upgrading the RAM in them any time soon.,Neutral
AMD,"You are off by a factor of 10. In the recent steam hardware survey, 4090's + 5090's make up over one percent of the userbase. Kinda insane.",Negative
AMD,What do you mean gpu timing was bad that made you get a 7900xtx over a 9070xt?,Negative
AMD,"Unless multiple (usually semi-reliable) leakers have  been completely bamboozled, zen 7 should still be on AM5. And it makes sense since DDR6 seems to be late, like 2029 late.",Neutral
AMD,"Now this is more reasonable upgrade. If someone has specific task that truly requires the move, it's understandable. Otherwise, it's literally consumerism.",Neutral
AMD,I use 40/60GB right now and it is currently a limiting factor for me moving my work VMs off of an older Skylake-era Xeon system.  People use computers for more than video games.,Neutral
AMD,"\-I'm saving money if I sell it for profit, RAM acts like gold/silver, if you buy gold is rare to lose money because gold price rarely goes down.   And this is less logical and more emotional, Why not max out my RAM if I can? we spent a lot of time of our lives working and making money for others, gifting ourselves RAM isn't wrong",Neutral
AMD,"I mean, yes, but you also have to look at the global political situation, and various risks with regards to pricing and supply in the lifetime of the system. If things could get hairy and stay that way for a couple more years than your typical system lifetime, then its not irrational to aim for more capacity. Its expensive as hell now obviously, but if you bought more than you needed 6 months ago, then you can feel a lot better about it now.",Neutral
AMD,"> Agree, this is why 50% more cores doesn't make much sense for gaming.  Most games are embarrassing non parallel, 8cores is gonna remain relevant for a long time if the generational improvements are kept up (15-20% core uplift)  The problem is that gaming has never been a main target of core uarchs. Dc is, and this is why 50% increase in cores makes sense, it's a design handed down by the dc market. Theoretically 8 super larger cores are what ya want in gaming but no one's gonna work on a gamer uarch  >I wonder if they'll release a gamer friendly version with 8 cores but also a larger L3 cache  You already have that, they are called past generations of 6core skus. But you already know what's gonna happen, the full 12core skus are gonna have higher clocks and they are still gonna be faster than any cut down skus (6-8cores) in gaming, so the more l3 per core argument don't work.",Negative
AMD,⚡⚡⚡ Breaking news! Integrated HMB memory on the Zen-6 chiplets confirmed! ⚡⚡⚡,Neutral
AMD,MI400 is 24576-bit bus,Neutral
AMD,Why would that be surprising? Dual socket zen 5 can already go up to 24 memory channels. 2048 bit could be achieved with 2x16 channels.,Neutral
AMD,What do you think Venice is? 16ch of DDR5.,Neutral
AMD,Can you show me who leaked 2nm before him? I tried to find anything referencing it before his leak this March and can’t find anything.,Negative
AMD,"Yeah that's fair, I'm not up to date on the process rumours. Going with just speed @ isopower is still quite optimistic like I said, since when the architecture gets wider and you add more cores you still need to power the extra transistors.",Neutral
AMD,"Realize that these performance figures are chosen at a specific point on the V/F curve.  They are essentially never taken at the top end of the curve. You can have nodes that regress at the high end that still post half decent ""performance"" figures (like Intel 10nm initially).",Neutral
AMD,"It says there than N2P is 5% faster than N2 or 18% faster than N3E.  That's 18% from N3E to N2, while N3E is 10-15% faster than N5. Since N4P is 11% faster than N5, that would make N2P 17-22% faster and N2X 27-32% faster than N4P. Ideally, that is.  [https://web.archive.org/web/20220617115333/https://www.anandtech.com/print/17452/tsmc-readies-five-3nm-process-technologies-with-finflex](https://web.archive.org/web/20220617115333/https://www.anandtech.com/print/17452/tsmc-readies-five-3nm-process-technologies-with-finflex)",Positive
AMD,also 6-7 is 50% less proportional increase than 3-4 ghz,Neutral
AMD,"FWIW Node designations with units in their name most definitively refer to the discrete smallest feature size for the resolution of the litho. That feature is no longer the smallest gate length possible for the theoretically smallest planar transistor for the process libraries, though.  Node process information should have never been used as part of marketing IMO. Most consumers don't have a clue regardless.",Neutral
AMD,"There is stuff in the chip that can have those dimensions in its geometry (eg the width of metal signal lines, or knees for vias). Just not the gate for a planar transistor. ;-)",Neutral
AMD,That's a pretty big increase and not something you see from each new node nowadays,Neutral
AMD,"That's not going to happen, since AM5 CPUs don't have a DDR4 compatible memory controller built in",Negative
AMD,"Shorter pipelines (fewer stages) generally means decreased clockspeed. Intel's cancelled Prescott successor Jayhawk had 40-50 pipeline stages in an attempt to ramp frequencies even higher (until it got killed off by the end of Denard scaling and the sheer insanity of the project).  Additionally, there are physical limits to how fast you can switch doped silicon. I've read that 10GHz is around the theoretical limit which means real-world limits would be even less.",Negative
AMD,"> Some nerds managed to get a 14900kf to over 9ghz  On a single core, with 2/3 of total cores and HyperThreading disabled, using a hand-selected golden sample, at 230 degrees C below zero. And it's barely stable enough to capture the CPU-Z validation.  You are not reaching anywhere close to 10GHz even with extreme difficulty for a stable consumer retail product even with exotic water cooling anytime soon - maybe ever, without an industry-changing materials breakthrough or paradigm-shifting rearchitecting of how we do computing.",Negative
AMD,"Not necessarily because of the chiplet design.  Here, the 5800X3D is actually idling lower than Intel CPUs, 1W lower than even the i3:  [https://www.techpowerup.com/review/amd-ryzen-7-5800x3d/20.html](https://www.techpowerup.com/review/amd-ryzen-7-5800x3d/20.html)  I've no idea why Zen 4 and 5 CPUs are showing such abnormally high idle power consumption!",Negative
AMD,Either that or it’s a typo and they meant zen 5,Neutral
AMD,"Yea at least with regards to gaming I tend to consider the current console generation, i.e. my system is well beyond a PS5 Pro. So I know that I'll be able to at least *run* major releases as long as they're releasing on PS5 which will be a long time, even after PS6 releases. There are *still* cross-gen titles releasing on PS4 and the cross-gen will be even longer for PS5/6. So I don't expect feeling real pressure to upgrade for a long time, hardware failures nowithstanding.   &nbsp;  ^^(*knockonwood*)",Positive
AMD,I’m good with it. Getting in on the end of AM5 isn’t anywhere as appealing as getting in on the start of AM6. And I’m not needing an upgrade today.,Neutral
AMD,4090s were under 2k for a while to be fair.,Neutral
AMD,He meant the pro version with 32gigs,Neutral
AMD,I’m just going off what AMD had said early on about AM5. If zen 6 and 7 are going to be AM5 I’ll be riding the 7900x much longer than I originally intended. I don’t necessarily follow leaks a ton anymore so they may have pulled back on that and decided to give us an extra processor generation and push closer to matching the longevity of AM4.,Neutral
AMD,"I mean for anyone who makes money from their PC, e.g. video production and editing, it’s not uncommon to upgrade to top of the line every 2-3 years. Performance is more earnings for you.   I went from a 5950X to a 7950X and I’ll certainly get a Zen6 if there’s more cores.",Positive
AMD,I got my 7900x from MC for right around $200 when 9000 series dropped with the plan being going to the top of the top end when we have the next socket swap.,Neutral
AMD,"It's equally justifiable to buy something because you just want it. Do I need to use 64GB of ram on a regular basis? Not at all, but I bought it because I felt like it.",Neutral
AMD,"It's a small price to pay to stay on the bleeding edge of hardware. I bought a 9800x3d to replace my 7800x3d, and I'm going to buy the overclocked 9850x3d when that comes out. I sell the old CPU on Facebook, and treat my gaming PC hardware like a subscription.   It's not really any different than replacing components every 2-5 years, as is typical, but this way I always have excellent performance and the best possible experience gaming.  When something faster than the 5090 comes out (excluding the $10k workstation card), I'll buy it and sell my 5090.  Higher end hardware makes gaming more enjoyable, and can enhance the artwork therein",Positive
AMD,The guy covers what you said quite well at the end of his line there.,Neutral
AMD,Yep. Video editor here who work with 8K RAW footage every once in a while.   I literally need a MINIMUM of 128gb to do my work and the most basic effects and transforms. And 24GB VRAM is survivable but 32GB is much better.,Neutral
AMD,What the fuck  Edit: wait is this the real Kepler?,Negative
AMD,ok it's a server cpu then,Neutral
AMD,"Not everything gets mass reported on.  MLID takes advantage of stuff like that.  But no, I cant find any specific source for this looking back, to be fair.  I know it wasn't the first it was talked about, though.    MLID in same 'leaks' also said that Zen 6 would release in mid 2025.  Along with claims of reaching 7Ghz, do we really need to play this game of pretending he's reliable?",Neutral
AMD,Thanks for correcting. So about 6.75 ghz boost assuming all holds.,Positive
AMD,What feature in N3 measures 3nm in size,Neutral
AMD,"Not from one node, but just to take TSMC's numbers at face value, N3E delivers 4-9% perf vs N5, and N2P delivers 18% vs N3E. So cumulatively, that would be enough.  Plenty of other factors here, and I personally think 7GHz sounds a bit too aggressive, but I think some people will be pleasantly surprised. Mid-6GHz as soon as Zen 6 should be very achievable.",Neutral
AMD,"What AMD themselves could do however, is launch new AM4 CPUs.  They already said they did test Zen 4 with the Zen 3 IO die back in the day, but chose against releasing it on both platforms. Probably because performance was lackluster without more bandwidth from D5.  It's extremely unlikely to happen, but they could do it.",Neutral
AMD,well overclockers have already achieved 7ghz so it must be a thermal or power limit with current silicon,Neutral
AMD,">Shorter pipelines (fewer stages) generally means decreased clockspeed  Yea i fked up, meant to write lengthen.  There's 1 reason higher clocks would be preferred over lower clocks at same st perf, low latency workloads annnndd wait for it, ai cpus. Guess what? Amd projected that 80% of the dc cpu growth over the next 4years are gonna be from ai gpu clusters. They are absolutely cranking up the clocks",Neutral
AMD,"It's a bit of mystery. cause it's not like zen 4 cpu uses more power than zen 3 when you just look at software reading, like maybe few watts more from quick googling. But also the same way 5800X3D being almost on par with intel makes no sense either cause intel cpu:s can idle real low, so I'd expect almost a ~10W gap even between single ccd zen 3 and lower core lga 1700/1200.  My guess would be the motherboards affecting stuff somehow, but even then the x570 has dual chipset like the x670E does so it's a bit weird and i doubt chipset(s) are gonna draw much power anyways, maybe some other voltage stuff going on or some random power states of pcie things that all add up.",Negative
AMD,I mean it costs 30 grand atleast,Neutral
AMD,2048 bits per HBM stack x 12 stacks.,Neutral
AMD,"No, because of what u/ComplexEntertainer13 commented.   Zen 6 may hit that clock speed, who knows. But assuming that's a given based on TSMC's cited perf/watt uplift numbers would be inaccurate.",Neutral
AMD,"The people who can give you the right answer can't do so without breaking NDAs :p. But based on what I know what the person you're replying to said is pretty true. The number the foundry labels (like 3nm) for the node doesn't necessarily mean there is something exactly 3nm wide in the design but usually it's a pretty good indicator for around what the minimum feature size is (so in N3, there is probably something on the chip that can be drawn to a couple of nm in resolution). What that feature is though is uncertain (gate length, metal width, metal pitch, etc.)",Neutral
AMD,"when they say performance increase, it's usually under the label of performance increase at the same power consumption. However, that doesn't necessarily directly correlate to the speed of the transistor itself, i.e. how fast the transistors themselves can switch (which as far as I'm aware has not increased by much in the last decade). Otherwise if you took those generational 4-9% jumps across the last 10 years of nodes we should be definitely be well above 7 GHz by now, but we're not.",Neutral
AMD,Interesting! But I agree that it's unlikely. Makes more sense to just keep existing AM4 products in stock for budget friendly computers,Positive
AMD,"14900 was overclocked to 9.1ghz with liquid helium https://www.tomshardware.com/news/core-i9-14900kf-breaks-world-record-almost-achieves-91ghz . Interestingly though, before that the fx8350 had the overclock record at 8.7ghz for 8 years, so max theoretical clocks is very dependent on the architecture as well as the node. (intel 7 was refined to be lower density to achieve higher clock iirc).",Neutral
AMD,"Definitely something to do with their motherboard or testing methodology.   Regardless, I've a 5700X3D and the idle power consumption has never been an issue for me.   I run it at just 45W PPT at peak 4.1 GHz, since the multiplier is locked, and it idles considerably lower than peak.  Idle power draw seems on par with my i7-4770, my previous CPU, as a matter of fact.   Still, it would be nice if the likes of GN or HUB do some proper lab testing and settle this (often meaningless) argument once and for all.",Neutral
AMD,"Considering MI350X costs about $25,000, I'd say that's a good bet.",Positive
AMD,"So N4 has a 4nm sized feature, despite not having any feature shrink from 5nm?",Neutral
AMD,"> However, that doesn't necessarily directly correlate to the speed of the transistor itself, i.e. how fast the transistors themselves can switch (which as far as I'm aware has not increased by much in the last decade).  What do you mean? There's no other metric for speed.   > Otherwise if you took those generational 4-9% jumps across the last 10 years of nodes we should be definitely be well above 7 GHz by now, but we're not.  For the same design, and assuming honest marketing, that might well be true. But parts of the speed gains from nodes are absorbed by other things. No one's trying to build NetBurst on N3.",Neutral
AMD,No what I said is that number has a rough correlation to minimum feature size. So N4/N5 probably have something which has like a 4-8nm resolution. It doesn't literally mean there's something that can be drawn to 4nm precision but there is something in that ballpark.,Neutral
AMD,"You can consider speed of the transistors themselves as basically how fast the transistor can flip a bit from 0 to 1 (or vice versa). This in some sense determines how difficult it is to make a design run at certain frequencies. On the other hand power consumption is (partially) determined by how much power you need to flip a bit from 0 to 1 (or vice versa). When you go to smaller nodes your transistors are smaller and so you have less capacitance at each node and by extension you need less energy to flip each bit in your cpu. So for the same performance you use less power, and IF you had a cpu that is capable of running at a higher frequency you could achieve higher performance at the same power by increasing the clock speed. But shrinking the node (these days) doesn't mean your transistors actually became any faster at flipping bits. So if your cpu wasn't capable of running at say 6 GHz before, if you move to a new node you won't magically be able to now run it at 6 GHz.",Neutral
AMD,"What on earth are you talking about? When they say performance at iso-power, that explicitly means it's not taking less power, and instead going faster. Are you seriously denying that new nodes improve performance?",Negative
AMD,"Radeon subreddit is in shambles because it doesn’t support RDNA3, they’ve got pitchforks out and are claiming they’ll go nvidia next gen, lmao",Negative
AMD,Well that’s not cured my impotency.  Damn.,Negative
AMD,"I wonder if GN or HWUnboxed will roast AMD for their [misleading ""performance"" charts](https://cdn.videocardz.com/1/2025/12/AMD-FSR-REDSTONE-VC-5_videocardz.jpg) like they did for NVIDIA and MFG. I have no problem with Upscaling performance, but once you start introducing Frame Generation [like AMD has here](https://cdn.videocardz.com/1/2025/12/AMD-FSR-REDSTONE-VC-4.jpeg), you're muddying the waters of what is ""performance"".",Negative
AMD,Launched without 7000 or earlier support despite the leaks.  lol?,Neutral
AMD,AMD never misses an opportunity to miss an opportunity.,Negative
AMD,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,"Marxist-Leninist-based Upscaling. No wonder they're called Team Red. /j  This is an improvement, but I'm more excited for the hardware of UDNA.",Neutral
AMD,The Ray Regeneration thumbnail is legit just a contrast filter lmao,Neutral
AMD,Would they be using their ML/AI cores of e.g Strix point/Krackan point for this too?,Neutral
AMD,Hopefully this leads to them FINALLY being competitive with nvidia in the high end again sooner than later,Positive
AMD,"It's really obnoxious that they released this but didn't roll it out to Adrenalin yet. I had to DDU and reinstall it twice, since Windows overwrote it immediately with 25.10.30 the first time.  AMD owes me 10 minutes of my life back, is what I'm saying. /firstworldproblems",Negative
AMD,does this work on RDNA 3.5? (890m specifically?),Neutral
AMD,"This wasn't the road I wanted AMD to pursue, the ""fake frames"" like Nvidia currently getting a lot of heat from.  But it just shows that AMD has no guidance except copying everything Nvidia does. Grow a pair and just make your technology better because it **IS** good right now, just not in the test metric Nvidia wants to push on consumers which is basically a big fat lie in promises and practical performance.",Negative
AMD,"It'll probably at least partially support rdna3 eventually, but it's pretty obvious that AMD just needed to get this out into the wild with at least rdna4 support asap.",Neutral
AMD,"I mean yeah, RDNA3 doesn't have the physical hardware for this ML-based stuff. If they bring Redstone features to RDNA3, it'll be entirely different.",Neutral
AMD,"I was on AMD for my last 3 GPU upgrades.  I had a Radeon 6800, then i saw AMD announcing that they'll put that card series into legacy support, which slightly pissed me off. They still make and sell 6000 series cards.  Then there's the fact that partial FSR4 support is possible on older cards, but not released or enabled by AMD.  I don't really care about the upscaling part of it, the 6800 chewed up any game i threw at it at 1440p without RT. I wanted it because TAA or FSR3 are horrid when it comes to image quality when you use them as AA solutions.   Playing something like Final Fantasy 7 Rebirth was a travesty if using TAA or FSR 3. Such a beautiful game that looks like a smudged mess with those solutions.  So i got an Nvidia 5080 for black friday sales. I basically just don't trust AMD's GPU division to not abandon even their 9000 series once they release a new series. And i'm done giving money to a  company division that is content in merely keeping their cards in ""Nvidia -50$"" price range for much lower feature sets.  I HATE AND LOATHE Nvidia as a corporation and hate that i gave them money, but i ultimately just picked the better product for my needs.  Their CPU division is banging and my 5800x3D looks like it'll keep chew anything i throw at it for a good while still, but AMD's GPU division can go fuck itself for now.",Negative
AMD,Anyone with a brain could look at RDNA3 and realize it wasn’t a major architectural shift over RDNA2. People read about a couple of low-precision math instructions and assumed RDNA3 had closed the gap with Turing. Honestly I don’t expect AMD to truly lock their feature set in until UDNA launches.,Neutral
AMD,Not so Fine wine now eh? Lol.,Neutral
AMD,"My problem whit the 9070XT, which isn't a cheap card by any standards, is the issues I came up against given the supposed 2.1a ports. The ports are not full bandwidth, I have multiple 4K monitors connected and get stuttering, freezing, and timeout issues constantly. I have to manage the displays as if I purchased a cheap card, lowering Hz here, color range there, etc. I can't run all my monitors at full specs at the same time! RIP I should have purchased a 5070ti...  Having to run my LG C5 and Alienware AW2725Q at 60Hz is crazy. Switching settings every time I want to play is such a pain.",Negative
AMD,I just build a amd pc after many years with just laptop and I saw Radeon subreddit. Is AMD really deserve that hate or Radeon subreddit is just that toxic :D?,Negative
AMD,"They shouldn't worry too much, as even RDNA4 doesn't support it in almost any use cases except for very specific games.",Negative
AMD,I went from 7900XTX to 5080 exactly because of this. And VR,Negative
AMD,Some of us refuse to use FSR lol,Neutral
AMD,Still balding here....,Neutral
AMD,HUB is saying it sucks.   https://www.youtube.com/watch?v=LpAZF_-qsI8,Negative
AMD,"What's misleading about this? Nvidia specifically marketed their GPUs as ""4x"" performance or whatever and compared GPUs with FG/DLSS off with DLSS and FG on. These charts from AMD are specifically performance charts for FSR which means the point is FG performance.",Neutral
AMD,Gn have published a video but I have yet to watch it,Neutral
AMD,AMD's GPU market share has gone so down that most people actually don't care.,Negative
AMD,"I don't think they will do a ""AMD IS LYING!!!!"" with a stupid thumbnail like they did to Nvidia.",Negative
AMD,oh NOW frame generation is bad when AMD gets their hands on it  give me a break,Negative
AMD,"amd repeatedly said before this launch that it was exclusive to rdna4, people's own fault if they decided to assume they were lying",Neutral
AMD,"I actually thought it wasn't leaks, but AMD's own statements that claimed something like them wanted to make this adoptable for multiple older architectures, and things out there. Maybe I misunderstood that. Either way, I'm glad I went with Nvidia last generation.",Neutral
AMD,"It looks dissapointing, so you didnt lose much",Neutral
AMD,"Leaks from where? If it was some of the typical suspects, I'm *shocked* that they would make shit up.",Negative
AMD,They are RDNA 3.5 and ML Redstone is only for RDNA 4,Neutral
AMD,How can they be competitive with nvidia in the high end if they don't have any high end RDNA4 graphics cards and Redstone is exclusive to RDNA4?,Neutral
AMD,Lol. It’s not even 4x frame gen. How would they compete with a mid level card like the 9070xt?,Negative
AMD,Actually that Windows owes you time...,Neutral
AMD,"Nope, RDNA4+/UDNA (we presume will also support this) exclusive.",Neutral
AMD,This is what AMD has always done.  It’s part of their origin story.  AMD literally copied Intel’s silicon to break into the CPU market back in the 70s,Neutral
AMD,"FSR4 INT8 already runs on RDNA1-2 and 3. Even some RDNA1 and Radeon 7 on GCN5.  XeFG also runs on DP4a or SM 6.2 path on the same GPUs.  If Intel can, certainly AMD can. They just don't want to.",Neutral
AMD,"Yeah. I think many people have unreasonable expectations. Still, we know there's an int8 version of fsr4 upscaling out there which works pretty well. If AMD just officially published that I suspect a lot of RDNA 2&3 owners would be pretty happy. (Maybe with the added promise of trying to achieve something similar with FG and ray reconstruction)",Positive
AMD,Yes they do. Shader cores are more versatile than matrix cores and can do everything they can at a lower efficiency/performance. AMD gating FSR's newer versions and NVIDIA gating DLSS behind never hardware with that excuse is bullshit.,Neutral
AMD,Yeah but AMD claimed “Architectured to exceed 3.0Ghz”. It was a major architectural shift since even RDNA4’s boost clock did not exceed 3.0Ghz,Neutral
AMD,AMD Vinegar™️,Neutral
AMD,This myth is so heavily reliant on the R9 290X surpassing the GTX 780Ti its not even funny.   Probably users younger than those cards in here lmao,Negative
AMD,But muh drivers,Neutral
AMD,"not sure how old your card is, but is just got it recently and im running 120hz on lg c4 and lg ultrawide monitor(not sure exact model) on 240hz no issues.",Neutral
AMD,...at 60Hz? That don't sound right. You sure your cables are up to spec?    Even RX 6000 could already do 4K120 10b HDR.,Negative
AMD,Holy shit my teeth are straightened and I think one grew back,Negative
AMD,"Yes, it was a good video, discovered what other outlets did not and [Tim also did mention that Frame Generation does **not** increase performance, just perceived smoothness.](https://youtu.be/LpAZF_-qsI8?t=793) Kudos to that video, they (or Tim) did great work.",Positive
AMD,It's still presenting the FPS increases without any context for how compromised the experience is compared to regular frames. But I agree that using it as a tool to lie about performance uplift of a new product compared to the old one is considerably more dishonest.,Negative
AMD,"> What's misleading about this? Nvidia specifically marketed their GPUs as ""4x"" performance or whatever and compared GPUs with FG/DLSS off with DLSS and FG on. These charts from AMD are specifically performance charts for FSR which means the point is FG performance.  What's misleading is they're making out that the FPS you're getting is more performance and making the game ""faster"" (says it in the top right of the chart, their words not mine), yet the latency is increased and delays inputs, that's anything but faster or more responsive gameplay. If this was just upscaling I would have no problem, but as I said, once you start adding Frame Gen as ""performance"" and making the game a less responsive experience, it's not faster, you're delaying inputs for perceived smoothness in the image. Both practices are dumb and misleading and I do not advocate for either what NVIDIA or Radeon have done with marketing their products. What NVIDIA has done is worse, but go to the root of both marketing strategies and both AMD and NVIDIA are pretending like Frame Generation = more responsive and more/faster performance.",Negative
AMD,"I saw it, he doesn't even mention the graphs being misleading from the slidedeck (it's clear from the video he has access to the slides), but GN does look at latency results. In the end, I wish he was harsh like he was with NVIDIA because honestly, AMD is just copying NVIDIA's homework and using the same BS playbook, pretending Frame Gen is increasing frame rate and [making the game's performance ""faster""](https://cdn.videocardz.com/1/2025/12/AMD-FSR-REDSTONE-VC-5_videocardz.jpg) (their words not mine).",Neutral
AMD,"DW bro [we got you caught in 4K calling them ""fake frames"" just earlier this year.](https://www.reddit.com/r/hardware/comments/1n0qtts/nvidia_geforce_rtx_5090_and_the_age_of_neural/navh6bm/) Before you say ""that's just one post or is a joke, [here you are doing it again in a separate thread](https://www.reddit.com/r/hardware/comments/1n67v2u/steam_hardware_software_survey_august_2025/nc3jscw/). Enjoy!",Negative
AMD,"Bro lmao, every time I see you defending AMD like their white knight. Redstone is clearly DLSS 1.0 and AMD rushed to release it because they're getting stomped in software features.  Though in AMD fashion, they manage to incredibly disappoint as always and reviewers are just letting you know.",Negative
AMD,"There is a leaked INT8 path for FSR4 that works on older hardware and has been implemented by others (i.e., on Linux in Proton-GE).  It works pretty good already, which makes AMD's reticence to put it out officially *baffling* -- especially since they aren't putting RDNA4 into APUs for a while yet, and want to sell a bunch of those in gaming-focused handhelds and Steam Machines.  AMD has a good software product here for once and they need a broad installed base to drive developer adoption, but they don't seem to care and it's infuriating.",Positive
AMD,"No one cared about the new framegen (on older cards), what people wanted is FSR4 upscaling on RDNA3/2, which already has been proven to work well on Linux",Neutral
AMD,"They aren’t even RDNA 3.5, they are XDNA 2. That’s why I asked whether they will leverage XDNA 2.",Neutral
AMD,"4x frame gen is really niche, there are so few cases where it actually makes sense and doesn't cause an unacceptable amount of artifacting and/or latency. You pretty much need a 240hz+ monitor, for one thing. I don't imagine that being a major factor in almost anyone's purchase decision.",Negative
AMD,"4x FG is a gimmick at this point. Maybe FG progresses to the point that it's viable in the future, but it really isn't right now.",Neutral
AMD,Yeah that's fair. Windows loves to replace new drivers with shitty old ones against your will.,Negative
AMD,"The fact that the INT8 version got leaked the way it did says...something.  It is pure speculation at this point by anyone except AMDs management as to why they have not released drivers that include INT8 for older RDNA versions but I think based on the independent testing from that leaked version that not officially releasing it is bad.  It is one thing to want to sell new cards but it is quite another to have something like the INT8 be out in the wild and then try to ignore its existence for the owners of cards not really that old.  And given how they recently they tried to put cards that they still sell into ""maintenance mode"" it really does seem like some parts of AMDs management is not making good decisions for their customers.  Now maybe their data shows that such decisions are better for their quarter to quarter bottom line but I really do question if that is the case.  I'd have to see some *hard* data to prove to me that whatever additional profits they are making but implementing these decisions are adding value to the company/brand over these anti-customer moves.",Negative
AMD,"It's pure market segmentation. Even if it doesn't work the same, they could still allow older cards to get it.",Negative
AMD,People use fsr3 to play at higher frame rates. So why would they release something that almost entirely fails to do that on the most sold gpus which are the low end 6600s and 7600s?,Negative
AMD,"It runs, but performance impact is considerable and inconsistent and quality isn't on par with FP8 either, no?   Could be they decided against releasing it in this state because of it.",Negative
AMD,The unreasonable expectation that cards that are still being sold get feature support,Neutral
AMD,My 280X survived until Overwatch 2.,Neutral
AMD,"I have had enough discussions with them, and the thing they call driver is adrenaline software which is driver control panel, clearly not understanding what a driver is.",Negative
AMD,The only benefit I can think of is maybe better motion clarity by pushing the FPS to 200+ or something if you have a really good monitor (i.e. an OLED). But even then the hit on latency means it would only work well for games where latency is not hugely important.,Neutral
AMD,"If I recall, the main harshness on Nvidia was the way they were trying to push reviewers to only review performance with frame generation enabled, an inherently dishonest take.  To be fair, I also haven't seen the video on this yet either, but I think ""frame gen generally sucks in these ways"" is pretty well established and those factors are unlikely to change drastically",Negative
AMD,"I saw first 3 minutes, he did said he didn't had much time for this video  but still doesn't excuse the ""less harsh"" opinion on amd.  Will update this after watching whole video",Neutral
AMD,"Both are bad, at least this is being used to show \*gains* from having the feature on or off, rather than presenting it as \*gains* over the previous generation of cards for the purpose of representing a new product's performance as higher than it really is",Negative
AMD,Proton-GE doesn't implement the INT8 model it's the FP8 model running through the cooperative matrix extensions added to Mesa.,Neutral
AMD,"I mean, why did you expect this? They never said they will do it.",Neutral
AMD,Another reason to dump Windows,Neutral
AMD,"I wish they would.  Isn't the NPU in the 90x0 an XDNA also?  That's what is doing this upscaling.  But, who knows if Windows is locking it down for CoPilot, or if it is accessible in the same way as the one in the GPU (since it is technically part of the CPU).  I bet they will figure it out though.  With Intel about to release their own competitor to it (maybe... no GPU benchmarks yet), they will want to use all they have to combat it.",Neutral
AMD,Yes everything is a gimmick until AMD releases a shittier version of it and then gets praised sky high.,Negative
AMD,With a 4k240hz oled (or any other ultra high refresh rate oled) mfg 4x is literally transformative despite the 2 Steves telling you otherwise.,Neutral
AMD,"Seconding what the other comment says, it's great.",Positive
AMD,The market segmentation is stupid.  Why NOT want to sell more RDNA3 GPUs besides RDNA4? WHY make consumers reluctant to buy more AMD products? It's so stupid.,Negative
AMD,"Path Tracing, DLSS Transformer and Ray Reconstruction Transformer can run on laptop 2050. Or MX 570.  What's your point exactly?",Neutral
AMD,Performant impact isn't that considerable.  It's consistent.  Quality is almost identical.,Neutral
AMD,"Because it was already developed, exists, and all they had to do to get the easy win was launch it officially.   But they chose not to do so while still making every APU including $1500+ Strix Halo products that could use it their only option.",Neutral
AMD,"It’s a bit more complicated. Krackan point/Strix Point are APUs, is everything on the same chip so… why not?  Also I’m on Linux, firmware/drivers are there but there is no program/frameworks using them… So it’s purely a drivers issue from amdgpu to actually schedule compute on the NPU.",Neutral
AMD,You guys say this but people still consider FG a gimmick despite FSR3 supporting FG for years now (and doing a decent job with it too tbh unlike the upscaler).,Neutral
AMD,Not every negative Nvidia comment is a pro-AMD comment. Stop promoting tribalism for 2 giga-corporations that don't give a shit about you.  I genuinely don't think 4x FG is a valuable feature at this time. The latency hit and the image degradation are not worth the smoothness.,Negative
AMD,"I really expected them to announce an MFG, you know damn well that they must have been working on it since the moment they caught wind of Nvidia having it.  It must be a lot harder to do as well as it's already being done by Nvidia than people expect - everyone seems to think everything these days is practically just checking a box off.",Negative
AMD,"You think shareholders are going to like that old product is cannibalizing new? This is exactly what happened with the 1080 Ti.  Sure, it's irrational from a consumer and engineer perspective, but nobody cares about them. They only care about the shareholders.",Negative
AMD,Doe dlss transformer only give 5 extra fps on a 2050?  And path tracing is designed to look good not help games run better. Your example would make sense if there was somehow less light bounce than in rasterized modes.  Fsr4 on rdna3 fundamentally fails at the thing its most used for.,Positive
AMD,"The xdna npu is a completely different architecture (based on xilinx IP) than the rdna4 ml extensions, which are a set of new shader instructions.  They don't really have anything in common in terms of architecture, I'd be surprised if the amdgpu driver ever ""supports"" both, as it'll be effectively adding an entire new driver stack beneath that interface for the npu, and much of that interface would simply be not relevant to the npu (and likely the npu will need new interfaces that aren't relevant to the GPU side of things either). It'll just be functionally 2 different drivers sharing a name.",Neutral
AMD,"Neither of the things you mentioned are even remotely true. Latency hit is negligible if you're close to base FPS of 60 or higher, Nvidia Reflex is far far better than AMD's Anti lag. And Reflex 2 will kill the latency debate with FG once and for all.  And personally I haven't noticed any image quality issues either. The FG model they trained is very good. It's an amazing technology for me, I have a 360 Hz OLED monitor and it's sublime to game in the  200-360 FPS range.   Also let's not pretend to not know fans of which corporate, Intel, Nvidia or AMD represents a literal cult.",Neutral
AMD,"Lol, the 'shareholders' do not care about Radeon consumer products. Most AMD shareholders probably aren't even fully aware of these DIY discrete GPUs, they are a rounding error in the business.  The amount of sales lost to people buying RDNA3 over RDNA4 due to FSR4 is essentially $0 in the grand scheme.",Negative
AMD,It would be hilarious if a Samsung chip has newer RDNA IP compared with AMD’s own APUs.,Neutral
AMD,Is Juno expected to be a mobile version of RDNA4?,Neutral
AMD,">PhoneArt reckons that the prime core will reach a maximum clock frequency of 3.9 GHz. A commenter, Erencan Yılmaz, reckons that this figure should be reduced to 3.8 GHz, due to power consumption considerations when looking at a 2 nm GAA-based design.  What does this even mean? Are they implying that GAAFETs have poor power frequency scaling at the highest end of the V/F curve or something?   Anyway, interesting to see the % Fmax gap between TSMC and Samsung based designs for the P-cores remain around the same.",Neutral
AMD,Very interesting. I would like to know more,Positive
AMD,It's actually quite probable since it actually features AI upscaling and frame generation and rdna 3.5 doesn't support it,Neutral
AMD,"> What does this even mean? Are they implying that GAAFETs have poor power frequency scaling at the highest end of the V/F curve or something?  If you look at the actual comments, they supposedly have their own leaks of the specs showing 3.8. The power consumption comment was a reply to some rando comparing clock speeds to Snapdragon.  Just lazy reporting.",Negative
AMD,"Money  Making GPU tech from scratch is stupidly hard. And it’s going to be hot garbage for a few generations until you polish it.  Look at Intel, despite making iGPUs for decades, still had tons of trouble making decent architecture for performance.   Qualcomm bought their GPU tech from AMD (was ATI back then).  Apple managed to do it, but I bet they were working internally for a few iterations before it was good enough to ship.   Just licence the GPU and not reinvent the wheel if you don’t need too.",Negative
AMD,"Apple didn't make their own gpu from scratch tho. It's more like a custom power vr gpu in it's first iterations. The first ""custom"" apple gpu is a power vr gpu with ""image block"". Which is actually a feature that no other tile based gpu had, only the adreno 840 released this year has this feature, imagination and Mali don't have it yet.",Neutral
AMD,"I think people forget a fully featured GPU arch is not just a collection of ""dumb"" SIMT compute elements.  There is a lot of arcane knowledge. Modern GPUs have so many specialized parts that are domains onto themselves - display drivers, video encoders/decoders, TMUs, RT engines, schedulers, memory & cache management....",Neutral
AMD,Yea I wasn’t even confident they did it either. Just further showcases how difficult it is.,Negative
AMD,"If you saw the similar Hardware Unboxed video from a few days ago, this one agrees with it and presents the info in a different way.  AMD urgently needs to fix this.",Negative
AMD,Very clear Redstone needed more time in the oven. Also it's going to struggle to gain traction unless they add support for older cards.,Neutral
AMD,The frametime issues are truly catastrophic. Looks worse than when I would force Crossfire on games that didn't support it even. I don't understand why they thought it was a high-quality release to represent the brand. WTF man.   At least it looks good and they can theoretically fix the frame pacing. They never did on FSR 3.,Negative
AMD,Maybe Nvidia was up to something with their Flip Metering stuff. The frame pacing of DLSS FG/MFG is flawless.,Neutral
AMD,FSR 3.0 all over again. Their framegen was also unusable on launch. They really never miss a chance to miss a chance,Negative
AMD,"V-Sync on driver level, cap frame rate -3 of your refresh rate   It mitigates the frame pacing in CP2077 and completely solved it on different games  I know it's a crutch but at least it's something until they solve it.",Neutral
AMD,"It seems fixable by software (driver), so I do not despair as much as many others.  They needed something out to show their progress.  I'm hopeful it will get much better in the coming months.",Positive
AMD,"What confuses me about this is that the framerate and frametime graphs displayed by MSI Afterburner in many games tend to not be flat with DLSS-FG on my 4090. In fact, FSR-FG often appears flatter. However, the DLSS-FG tends to subjectively feel smooth to me (so long as it's not inheriting stutter from the rendered frames).  Does anyone have any explanations for this in light of the HUB and DF videos? Could the flip metering hardware of the 50 series be playing a significant role here (I think both HUB and DF used 50 series cards to compare FSR Redstone to)? Is there an issue with using MSI Afterburner's framerate and frametime graphs for this purpose (I can't seem to post a screenshot unfortunately)?",Neutral
AMD,"With nvidia bowing out of consumer GPUs next year, AMD is lining up the fine wine perfectly.   When they fix it next year, I hope the media covers it equally as positively as they were critical.",Positive
AMD,"Yeah, I think in Digital Foundry's podcast they called out the Hardware Unboxed content as excellent and basically said ""I am not sure we even need to make a video now, but we will"".  And I think it's good that they did, more attention on this can only be a good thing.",Positive
AMD,"They will fix it in RDNA5, just like how adding frame generation was a “fix” for FSR3 and it being limited to RDNA4",Neutral
AMD,It might be an unfixable hardware flaw.,Neutral
AMD,Why urgently? It's an optional feature.,Neutral
AMD,"The ""time in the oven"" is releasing the feature in RDNA5, not in RDNA4. Just like how frame generation was a demo feature for RDNA3, Redstone is a demo feature for RDNA4 with the real version in RDNA5.",Neutral
AMD,"Yeah seems so. However, if they have to spend more engineering time/power on improving advanced features I would expect porting them to the older gens is pushed further down the timeline.",Neutral
AMD,Nvidia did it by adding ML hardware support to cards well before they were needed with the RTX 2xxx cards. It's surprising AMD waited so long to do it. They must have thought traditional algorithms would work just fine.,Neutral
AMD,What did you expect though? It’s AMD. Their software is always a couple years behind.,Neutral
AMD,Don’t they have flip metering on RDNA 4?,Neutral
AMD,"Has anyone done some god quality testing on the frame pacing of no flip metering vs flip metering? The only such coverage I recall finding is this is [this Gamers Nexus clip](https://youtu.be/Nh1FHR9fkJk?t=1922), but they only tested this on two games, and only one of the two showed an obvious framepacing improvement from the 4090 to the 5090.",Neutral
AMD,"it's not flawless, unfortunately. for some games though. yes, it's miles better than FS FG, but there is still room to improvements  take Indiana Jones with path tracing for max GPU load, take RTX 5090, run it with 4xFG without frame cap and check msbetweendisplaychange with capframex. it will have the same sawtooth graph with some short lived frames, but to a lesser degree ofc. it will be very noticeable to the naked eye on OLED monitors because they will flicker due to those variations  reflex by itself (and reflex is forced on when FG is used) also adds not so perfect frametimes that can be seen with msbetweendisplaychange in some heavy games (Cyberpunk 2077 would be another example)",Neutral
AMD,"Someone should remind them of that old adage: ""Better to remain silent and be thought a fool than to open your mouth and remove all doubt.""",Neutral
AMD,"There are different statistics that you can use to populate your frame time graph, each of which are valid depending on what you’re trying to show.   Pure frame time measurement, as in “this is how long it takes to process each frame” is valid. But so is ‘ms between presents’ and ‘ms between display change’, the first being the timing of the frames being presented to the render queue, and the latter being the rate that the actual display updates and shows the new frame. Both of these measurements are captured by presentmon and frameview. I’m not sure exactly what measurement afterburner uses, but I suspect they are measuring pure frame time. But if you looked at time between display change it would probably show the issues that hardware unboxed and digital foundry showed.",Neutral
AMD,Its rumored they will reduce production however nothing is confirmed. Nvidia still makes A LOT of money from gaming and wil lnot be giving it up. They were about $100 million short of a new record in gaming revenue last quarter.,Neutral
AMD,People said they will bow out of GPUs since 40 series just launched,Neutral
AMD,"With nvidia bowing out of consumer GPUs next year   People has been saying this for YEARS now and their gaming market share/revenue has only gone UP and UP. I don't know why anyone with enough sanity would believe this stupid narative   And let me ask you, if shortage hits Nvidia and forces them to reduce gaming GPU production, why would you think AMD will be safe from it?",Negative
AMD,Nvidia aren’t bowing out. They’re just reducing production on the 5000 series. They’re also going to be launching the 6000 series.,Neutral
AMD,lol,Neutral
AMD,"[amd isn't nvidia, they don't release slop. they want to make sure they realease quality products for gamers](https://www.reddit.com/r/hardware/comments/1nw18md/comment/nhdqwi7/)     \- you",Neutral
AMD,"> With nvidia bowing out of consumer GPUs next year, AMD is lining up the fine wine perfectly.  If they don't fix this ""optional feature"" (and future ""optional features"") AMD will be lining up a miraculous market share loss against no competition.",Neutral
AMD,If they leave things broken developers will ignore Redstone. Redstone is supposed to be AMD's answer to Nvidia's suite of DLSS features.,Neutral
AMD,"because all of this is hurting their brands even further, as if gating FSR4 (ML) behind RDNA4 didn't piss people off enough.",Negative
AMD,"There doesn't seem to be anything preventing RDNA4 from running this correctly, it has support for hardware flip-metering. AMD engineers just fucked this implementation up and need to fix the software.",Negative
AMD,"If I had to choose between them supporting my card fully and them fixing up and keeping Redstone competitive, I'd take the latter.  I bought my card for the features it had at the time of purchase. I didn't expect future new stuff beyond maybe FSR4. Making an official WMMA / INT8 version for games to fall back on would be more than enough, but I don't expect that to come.",Neutral
AMD,remind me who had gpu driver issues this gen again?,Negative
AMD,"Hadn't heard about it but they support ""Hardware Flip Queue Support"", which I think is the same thing?  But they [advertise it](https://www.notebookcheck.net/fileadmin/_processed_/2/e/csm_RDNA_4_Architecture_Press_Deck_page-0005_768d67dd27.jpg) with the following benefits:  1. Offloads video frame scheduling to the GPU 2. Saves CPU power for video playback  I don't think it has a role in frame generation, or even gaming, I think it mostly has to do with video playback.  Maybe it is the same thing and Redstone is just bad at frame pacing anyways?",Neutral
AMD,"As someone who came from a 40 to 50 series GPU, I can tell you it's amazing. It literally fixed VRR flickers for me and the pacing is flawless. The effect is exacerbated if you have an OLED display as it has near instant pixel response time.   The end result is a flicker-free smooth gameplay. It's hard to explain but it feels like I'm playing games on a thin fabric, it's that good.   So if anyone's on an OLED and hates bad frame pacing with VRR flickers, upgrading to a Blackwell GPU is the way to go, thanks to its HW flip metering logic.",Positive
AMD,I can only talk from personal experience but I have a 40 and 50 series gpu   I find Frame gen literally unusable on the 40 series card whereas I can literally not tell it is on with the 50 series   It felt like fucking magic to me   The 50 series is also a lot faster overall but I went up to 4K at the same time and am getting less frames so it’s not just more performance,Negative
AMD,You should apply this to yourself instead of typing it out bro,Negative
AMD,"> But so is ‘ms between presents’ and ‘ms between display change’, the first being the timing of the frames being presented to the render queue, and the latter being the rate that the actual display updates and shows the new frame.  So I take it that the former is the time between frames entering a queue of frames to be sent to the monitor, while the latter is the time between those frames actually being sent to the monitor?  Anyways, I installed PresentMon, and using various metrics:  - FrameTime-Display - FrameTime-Presents - FrameTime-App - Ms Between Display Change  I couldn't notice any difference in these graphs between DLSS-FG and FSR-FG in Cyberpunk and Avatar (This is on 40 series, so no flip metering). With MSI Afterburner, the lines for both framerate and frametime appeared much flatter for FSR-FG in both games (even though it didn't subjectively feel smoother than DLSS-FG to me).  At times, FSR-FG has felt noticeably _less_ smooth than DLSS-FG in Avatar, but they both felt about the same on this particular occasion.  Others are reporting that DLSS-FG felt much smoother to them after upgrading from 40-series to 50-series, so I wonder if DLSS-FG isn't much smoother than FSR-FG on the 40 series. Also, I wonder if some of the FSR-FG framepacing issues are inconsistent, getting okay-ish frametimes on some occasions, but othertimes getting awful frametimes in the same game.",Neutral
AMD,so what does that mean for consumers if nvidia is going to reduce production but still wants similar gaming revenue?,Neutral
AMD,Wym they're clearly bowing out right now. They only shipped 11 million GPUs this quarter compared to amds 900k.,Negative
AMD,> They’re also going to be launching the 6000 series  That's a mid 2027 launch at best,Neutral
AMD,"40% is significant, and you know they're going to be top-heavy too, they're not going to waste hardware on budget 6060s. $3k GPUs is inaccessible for 99% of gamers that it's basically bowing out of the segment.",Neutral
AMD,Perfection,Neutral
AMD,"To be fair, if it's true Nvidia is cutting GPU supply by 40% soon, AMD will probably have no problem clearing their inventory if it's the only thing available at a reasonable price.",Neutral
AMD,"You realize most people pick up Radeon GPUs because they're incredible value for money. It's the raster and VRAM that is the attraction, Redstone is just a cherry on top.  Seriously, this is textbook example of loss aversion. Had there been no Redstone everyone would have been fine, now we get something as a bonus and although it's not quite ready yet it somehow diminishes the original value of the product?",Positive
AMD,"Don’t worry, I don’t think its an either/or. Its just an order of priority. Ignore the doomsayers, Radeon’s given every indication they intend to bring FSR4 to RDNA3. They aren’t even putting RDNA4 in their APUs in 2026, so supporting it going forward is pretty much a necessity for those lower power devices.",Negative
AMD,"Literally right now, as we speak, NVidia drivers are mostly great while AMD struggles with whole host of problems introduces by fresh branch.",Negative
AMD,Dude the current 25.10 drivers are the worst they've been in years and some people are even using 2024 May drivers because for some reason later drivers cause hard pc shut downs in the Spiderman trilogy.,Negative
AMD,"Nvidia, AMD, Intel. You just hear about Nvidia's more because they sell like 95% of all GPUs",Neutral
AMD,Adrenalin has been crashing so much on my windows clean install that I just said fuck it and installed a Linux distro to see if the problem is software or hardware lmfao  What a horrible purchase I made with my 9070 XT. I regret it SO MUCH. By the way I bought it because of FSR4 support specifically.,Negative
AMD,Oh right i forgot AMD has the best software. Their upscaling tech is years ahead of everyone else.   I heard they give out free handjobs with each GPU purchase. They’re just that good! That’s why everyone owns one right?,Positive
AMD,"I can't remember if it was Digital Foundry or Hardware Unboxed, but someone mentioned it was the same thing. The video frame scheduling on GPU.",Neutral
AMD,On a 4070 Super I can't tell when frame gen is on. I used it in Cyberpunk to get above 60 FPS. The base framerate was in the 50's with all the cool path tracing stuff and I couldn't tell it was starting in the 50's.,Neutral
AMD,">This is on 40 series, so no flip metering  I believe both Hardware Unboxed and Digital Foundry showed the issues specifically on Radeon GPUs using FSR FG, No?  There are going to be differences between how AMD and Nvidia handle frame pacing, even outside of the FSR/DLSS conversation. There could be something about how Nvidia handles frame pacing that is better for FG, but is not a part of DLSS FG specifically.",Neutral
AMD,Nvidia will increase prices.,Neutral
AMD,So then you agree they’re still making GPUs then?,Neutral
AMD,Everything after your first sentence is pure speculation. Nvidia has like 95% of the GPU market. They aren’t just going to sell 6090s lol,Neutral
AMD,"Friend - they'd reduce production on top tier as  * AMD doesn't compete on that level * their OEM numbers are probably 60-65% of their volume  If anything, they will pump out 6060s to keep AMD out of Steam Surveys since the die will be miniscule thus high yield per wafer, in consumer eyes, and most important - in affordable products thus increasing their user base and indirectly influence.  They can afford for 6090 tier buyers to eat the cost - as they've done willingly for halo GPUs since reviews existed.",Neutral
AMD,Chip yields increase exponentially as area is reduced linearly. They will try to sell traditionally 50-tier sized chips in 70-tier cards and be not much less profitable than the huge AI chips while hedging against the bubble popping.  They could also do another generation of dual fabs where Samsung or even Intel produces consumer chips while TSMC fabs for their data centre designs.,Neutral
AMD,In 2021 Nvidia GPUs were all sold out and going for 2-3x MSRP (when you could actually find one) because of crypto and AMD still couldn't take advantage of the situation.,Negative
AMD,"And they are 'incredible' value for their money because they offer similar features. No one is going to buy an AMD card without FSR, FG, etc... in todays market.   Raster time is over",Neutral
AMD,"Hmm no, If I wanted raster I would buy last gen used or on sale as usual, this gen was different because it's supposed to be the one that gets upscaling, ray tracing and frame gen right so NVIDIA tax becomes unjustified.  I'd get a new GPU to step up to 144fps and that requires both upscaling and frame gen, actually I could do without ray tracing but the reason to have a >60fps display is that frame gen is supposed to be ok at higher frame rates. Redstone is not.",Neutral
AMD,"I always love that response in a literal thread about AMD's driver/software issues/bugs.  I'm surprised bro didn't just say  ""I have a 6600 XT and have no issues.""",Positive
AMD,Sounds like a console fits better for you,Positive
AMD,This is no surprise. The 5070 Super 16GB will be the new 6080,Neutral
AMD,"you realize the dram shortage plays in AMD's favor right?  game devs aren't going to optimize their games, your best hope is nvidia figures out their fake vram neural texture compression just so you could have the privilege of paying $800+ for a xx70 gpu with MAYBE 6gb of VRAM in 2026/2027  and by then AMD will have ironed out Redstone, maybe even be on UDNA and have it backported to GPUs with 16gb+ of VRAM  raster will prevail",Negative
AMD,8GB at any rate with how difficult it will be to acquire VRAM,Neutral
AMD,"Most NVIDIA cards except for the 5070 have the exact same VRAM as the AMD counterpart, so I don't know how this plays into AMD's favour 😂  Also Redstone wasn't important according to you, now it's suddenly important",Neutral
AMD,"talking mid to long term, when nvidia cuts 40% of gpu production, they will recoup costs by selling $1000 midrange gpus while AMD will continue to provide GPUs with more vram at better value and feature parity  redstone will be fixed, that's the point.",Neutral
AMD,"Ah yes, because AMD is obviously not affected by a GLOBAL shortage and definitely doesn't need to cut production and raise the price  The fact that Samsung just reported that they have no stock at all definitely won't affect AMD but only NVIDIA",Neutral
AMD,"Last quarter shipping numbers were 94% to 7%  NV cutting it be 40% is only ~38% drop, still flooding the market >6:1 over AMD.  These people are over dosing on the kool aid.",Negative
AMD,But have you considered that AMD is our lord and savior?,Neutral
AMD,Venice is the next release. This would have leaked ages ago unless they are going for dual supplier.,Neutral
AMD,"AMD already announced they taped out Venice on TSMC 2nm back in April.  [https://www.amd.com/en/newsroom/press-releases/2025-4-14-amd-achieves-first-tsmc-n2-product-silicon-milesto.html](https://www.amd.com/en/newsroom/press-releases/2025-4-14-amd-achieves-first-tsmc-n2-product-silicon-milesto.html)  If they use Samsung at all, It is probably going to be something else.  That said, there are going to be Zen 6 EPYC SKUs that use different chiplets (there are at least two chiplet variants).  So it is possible that ""Venice"" is referring to a family of CPUs and there could be some Samsung based SKU in there.  But I think it is more likely that this headline is wrong, and AMD is contemplating Samsung for a different product.",Neutral
AMD,If i had to guess AMD is probably looking to offload lower end and mobile SKU's to Samsung to save on costs and get more TSMC production dedicated to more important datacenter/mainstream silicon. so Samsung might take over APU silicon for desktop and mobile along with motherboard chipsets and maybe some entry level desktop GPU's and most laptop GPU's.,Neutral
AMD,"AMD and TSMC have been pretty intimately close ever since AMD ditched global foundries. Also TSMC isn't too kind on companies that are unreliable partners, AMD just hopping to Samsung would be pretty shocking",Neutral
AMD,Moving away from TSMC is always a good thing,Positive
AMD,"Ram shortage.. You need a deal for quota package memory and GPU, if not.. Who's gonna buy a new CPU.",Negative
AMD,TSMC ?: [https://www.tomshardware.com/pc-components/cpus/amds-first-2nm-chip-is-out-of-the-fab-epyc-venice-fabbed-on-tsmc-n2-node](https://www.tomshardware.com/pc-components/cpus/amds-first-2nm-chip-is-out-of-the-fab-epyc-venice-fabbed-on-tsmc-n2-node),Neutral
AMD,Why would AMD hamstring their very populair and performant server line by switching to a subpar node? Delays or price hikes at TSMC?,Neutral
AMD,So they were unable to secure capacity at TSMC?,Neutral
AMD,I thought Venice was old news a long time ago.      [https://www.techpowerup.com/review/amd-3800-plus-venice/](https://www.techpowerup.com/review/amd-3800-plus-venice/),Neutral
AMD,"Probably not for launch, but filling up capacity later on.",Neutral
AMD,Possibly Samsung will be building the IMC,Neutral
AMD,Maybe lower end skus?,Neutral
AMD,Maybe iodie? Gpu?,Neutral
AMD,"Moving from TSMC's 2nm process to Samsung's 2nm would likely be a big downgrade. I believe AMD would have to compromise on frequency, power efficiency, or yield and even in the worst case it has to tweak the architecture.",Negative
AMD,The power efficiency throws itself out of the window,Neutral
AMD,"It goes both ways. if TSMC provides capacity to their latest nodes to every company that asks for it, giving AMD less allocation than they want, then AMD has to look elsewhere to fill the gaps. Can't rely on any one company for everything.",Neutral
AMD,They are business partners not spouses in a relationship,Neutral
AMD,"Unless you want to compete at selling the best high performance accelerators, then it makes sense why it is AMD and not Nvidia",Neutral
AMD,"Yea, at best it's dual sourced, or maybe it's going to be used for the IODs, if Venice actually uses Samsung (which I doubt).   FWIW, another prominent (though I'm very dubious about how accurate he is) twitter leaker, jukon, thinks it is for the PS6.   IMO, this rumor ends up going nowhere. We went though similar things with Samsung 4 and 3nm.",Neutral
AMD,"Because AMD has significantly more understanding and insight regarding the processes involved, than random gamers?",Neutral
AMD,"Gives them more leverage in negotiating prices with TSMC, also could be looking at Samsung as a second source; AMD is probably doing enough volume now in the datacenter market that they can justify the costs of adapting their design to Samsung's fabs, and then use the Samsung chips for lower end datacenter products.",Neutral
AMD,GPUs more important?,Neutral
AMD,Vertical integration in CPU/GPU/memory is now the hottest thing. Amd wants one contractor to provide all the comments and and can have a streamline input in the entire stack without delays in communication,Neutral
AMD,Because TSMC has too much demand and can’t keep up? They have also raised their pricing a absurd amount as they had no real competition,Negative
AMD,"Why would Nvidia hamstring their own very popular GPU line by switching to a subpar node with their Ampere generation?   Not saying I fully believe the rumor, just stating that it's not a totally implausible suggestion.",Negative
AMD,"Writing is on the wall with China and Taiwan, AMD won't be the first to move either.",Neutral
AMD,Capacity for Venice was secured eons ago. They're already sending samples to hyperscalers...,Neutral
AMD,wow TPU is way older than I thought lol,Positive
AMD,"I remember that specific article! What a trip down the memory-lane … Thanks for this!  > Advanced Micro Systems (AMD) has released a new revision of their Athlon64 S939 […]  *Oh dear, the glorious socket 939 and its Athlon&nbsp;64* — Makes me a bit melancholic already.   *That was the time to be alive*. That was true journalism at heart from enthusiasts!   Explaining every short and abbreviation with the respective written-out long version and bringing out pieces for actual hardware-hits (instead of today's hit-pieces over the next refresh-cycle), where you could readily feel the joy of the editors themselves writing the article, describing new hardware between the lines.  Not the nonsense clickbait sh!t we have to day, which fabricate news around a single ~~Twitter~~ *~~X~~* *Twix*-post from some leaker no-one knows anyway …  Back then, you could go days, not seldom even a week without a single hardware-news, and no-one bat an eye, as it was only fueling anticipation and building up anticipatory excitement.",Neutral
AMD,Athlon 64 3000+ was my first CPU that I got brand new. I upgraded from a very ageing Pentium 133MHz. 8 years of new releases brought 13.5x of clock speed increase as well as a lot more instruction sets. Now my current PC is on an over 9 years old platform...,Neutral
AMD,"Doesn't really make much sense given it'll go against their super successful scaling strategy with their CPU products.  They cant just swap TSMC for Samsung chiplets and have everything all work out the same because they're designing these Epyc packages based on a very specific die size for the CCD's.  This makes scaling super easy for them in so many ways.  Adding in some new CCD based on a wholly different process tech seems like it would throw everything out of whack, no?  They've never done anything like this before.  They've always had the same process tech for all CCD's of the same type, for all ranges of CPU's of that architecture.  Dense/C versions are a bit different, but those are also a strategically produced different line.  You wouldn't do that just for a different process technology alone.",Neutral
AMD,"I/O die only makes sense if they are getting wafers cheap as they don't benifit from the leading node density as much, but they may like it for low power.  Midrange GPU would make sense.  One of the mid/low end APUs would make sense too.  PS6 handheld or even the main console APU could potentially work as well, but that would require consistent parametric yields since they don't have opportunities for performance binning.",Neutral
AMD,Versus current gen 5/4nm?,Neutral
AMD,"Venice isn't their AI GPU lineup, and AMD already confirmed they will use TSMC N2 for their MI400 series IIRC.",Neutral
AMD,"This is about CPU's, where AMD is currently the top dog.  Much like how Nvidia used Samsung for Ampere GPU's while still being competitive, it's possible AMD could utilize Samsung for Zen 6 and still be very competitive.  Especially because AMD's whole chiplet scaling strategy is still a lot more cost effective than Intel's messy bullshit.  Epyc CPU's are also not typically being pushed to higher limits, so efficiency sweetspots matter a lot more, and that's not always gonna be such a huge difference in terms of performance per watt.  It might hurt them a bit more in consumer, but overall they're probably pretty confident in their architecture teams to maintain improvements without relying entirely on process node gains.  Intel's P-core team definitely has plenty of question marks surrounding it since Alder Lake.",Positive
AMD,"> Because AMD has significantly more understanding and insight regarding the processes involved, than random gamers?  Dude, don't you think such a assessment is a 'lil bit too much to drop casually?! o.*0*  *This is Reddit!* 90% of users here are armchair-generals or virtual CEOs, who actually know their sh!t …",Neutral
AMD,That and Nvidia and Apple bought up all of TSMCs 2nm production time.,Neutral
AMD,"This is just a meaningless appeal to authority with no logical backing. Moreover, any decision to make chips in Samsung, whether they turn out true or not, will also involve non-technical factors like cost. They could very well choose to go with a subpar process if it is way cheaper. Beancounters often hold significant decision-making power.     And historically Samsung has had a number quality issues versus the having the same architecture made at TSMC, so these concerns are pretty fair.",Neutral
AMD,Perhaps this would make more sense if this wasn't:   * just a rumor  * no mention on whether this was for CCDs or IODs  * being fabbed at Samsung which has a horrendous track record   But sure.,Neutral
AMD,"that's not a reason to choose Samsung over TSMC, try again",Neutral
AMD,"FWIW that is not necessarily how prices are negotiated between designers and foundries.   The roadmap has more effect, and TSMC, Samsung, or Intel being in the picture is rarely used to drive prices lower.   Sure competition among fabs and packagers may set the ballpark of costs. But designers rarely use the threat of going with a different process to get a lower/better price from TSMC, Samsung, etc.   If AMD is going w Samsung it is likely because Samsung's roadmap may align w AMD requirements for a specific design.   The reason why this news may be unlikely is that AMD does not have an stablished silicon team with Samsung (that I am aware of). And that usually is how you can tell any type of significant volume from a large design team is going to be on a given foundry.   However Samsung does have a very nice roadmap for the type of large dies AMD has in their pipeline for their monolithic SoCs (mainly their APUs and GPUs). So there could be some alignment there.   For the CCDs and IO chiplets + 3D cache, it seems AMD is very aligned with TSMC and the packaging flow there.",Neutral
AMD,"I understand dual sourcing for lower end SKUs, but the article makes it sound like the entire new EPYC line is moving to SF2, a very likely worse node than N2. This would impact the performance and potentially allow Intel to close the gap further, so I’m struggling to understand why. If this article is true, there could be something off about N2.",Negative
AMD,"Most analysts have AMD as a top 5 TSMC customer, and Venice is a flagship product from AMD.   It's hard to believe Mediatek can tap TSMC N2P for their next smartphone chips, but AMD couldn't for their high margin server CPUs?",Neutral
AMD,"Every year there's new rumors about AMD using Samsung for their 4nm and 3nm node, and then people like you always say the same comments like this, and then it never ends up happening. But then a new Samsung node gets announced, and the cycle repeats.   *Sigh.*",Neutral
AMD,"Yeah, the Chinese overtaking Taiwan we're told to happen the next Monday morning since the 1970s …  Even announcing another round of *»This is the year of the Linux-Desktop going mainstream«* has more credibility to it, and actually is becoming reality rather sooner than later, thanks to Valve's Steam now, also their SteamDeck.  ---- Intel just dug up that age-old Taiwain-spectre, to scare investors and governments into submission for subsidies, and y'all damn fools all bought readily into that nonsense and manifested a threat, China really couldn't care about less.  *China has a host of other problems of greater importance* — Taking out Taiwan, will inevitably result in vaporizing it for EVERYONE, which wouldn't help Peking/Beijing one bit anyway to begin with. It's a futile undertaking and they know it.",Neutral
AMD,I still remember being excited about getting a Denmark (Opteron 165 - basically dual core Venice) and OCing it like crazy.  Those were the days... when dual core felt like overkill and a 50% OC - clocking higher than the fastest available SKU - could be reached with the stock cooler on an entry level part.  today's CPUs a way better and way more boring. Almost zero point to OCing.,Positive
AMD,"> They cant just swap TSMC for Samsung chiplets and have everything all work out the same because they're designing these Epyc packages based on a very specific die size for the CCD's.  Hasn't they've done so before already? AFAIK a bunch of CCXs were dual-sourced (TSMC, Samsung) and AMD even opted for *a three-pronged strategy* on sourcing (TSMC, Samsung, GlobalFoundries) for a single design of their CCXs (Ryzen, Threadripper, Epyc), no?  > They've always had the same process tech for all CCD's of the same type, for all ranges of CPU's of that architecture.  You think that Samsung's and GlobalFoundries' 14nm back then were identical down to the last bits?  Yes, they were the same process, but GloFo surely made quite a bit of custom tweaks on their own, don't you think?",Neutral
AMD,"> as they don't benifit from the leading node density as much, but they may like it for low power.   AMD has been wanting to jump up to DDR5-9000 for a while if you've been reading the tea leaves/what comes out of their PHY guy via AGESA update, but the I/O die simply isn't up to the task as currently built. Jumping up to 2nm for a faster switching frequency and just adding a <profanity> load of transistors to make it less fragile solves that problem adequately. It's not elegant, but what's the point of more advanced materials (or transistors, as it were) if you never use them?",Neutral
AMD,"> Especially because AMD's whole chiplet scaling strategy is still a lot more cost effective than Intel's messy bullshit.  Still baffles me how Intel can't let go of their big-die philosophy, tanking their margins and destroying yields that way since years while even crippling their overall volume — Still times higher manufacturing-cost than AMD.",Negative
AMD,LOL. They did not. Both AMD and Intel are also doing bring up of high volume SKUs on N2.,Neutral
AMD,Nvidia has no products on 2nm. And AMD was first to tape out on 2nm with this CPU. Meaning they get all the early production to themselves. Think.,Neutral
AMD,"Interesting. Yet, I am quite certain the people at AMD and Samsung (or TSMC for that matter) still have significantly more understanding and insight regarding the processes involved than random terminally online gamers. Just a hunch.",Positive
AMD,">I understand dual sourcing for lower end SKUs, but the article makes it sound like the entire new EPYC line is moving   Which makes no sense because AMD already confirmed that Venice will be fabbed, at least for some part of it, on TSMC 2nm.   >SF2, a very likely worse node than N2  You are probably right, but the article also mentions SF2P, which very well could be Samsung's next real node jump after SF2 having very minimal PPA benefits over SF3 GAP.   So maybe they could catch up to TSMC 3nm rather than being decently worse than it, as they currently are.   >This would impact the performance and potentially allow Intel to close the gap further, so I’m struggling to understand why.  TBF I doubt Intel and Samsung are going to be in much different places if Venice is fabbed on SF2P and DMR on 18A, and AMD's design side also just gaps Intel's, so they could still win at what could be considered node parity.",Negative
AMD,It definitely feels like an ego thing that they didn't just copy Ryzen's super successful and ultimately quite straightforward chiplet scaling strategy.,Neutral
AMD,Who goes first? :),Neutral
AMD,Late 2026,Neutral
AMD,Your own recent posts are out of touch even if you claim authority because you're a director.  These Samsung foundry rumors are like Intel foundry rumors. I'll believe it when I see it.,Negative
AMD,">Interesting. Yet, I am quite certain the people at AMD and Samsung (or TSMC for that matter) still have significantly more understanding and insight regarding the processes involved than random terminally online gamers. Just a hunch.  Maybe. However, that may be why u/heylistenman is asking why AMD is rumored to be switching foundries, to get insights from people more involved in the industry than random gamers.",Neutral
AMD,We’re not allowed to discuss this rumor as mere enthousiasts frequenting a forum about hardware? Why do you think AMD would move EPYC to SF2 instead of N2?,Negative
AMD,"> It definitely feels like an ego thing that they didn't just copy Ryzen's super successful and ultimately quite straightforward chiplet scaling strategy.  Well, I think Intel at least *tried* to copy AMD's chiplet-paradigm helplessly for the last couple of years …  It seems that Intel was taken totally by surprise on anything Chiplets and got caught with their pants down (again), when AMD brought it this quick to market, when having worked on them for a decade plus.  Though IF Intel would already worked on anything *disintegrated silicon* for said 10 years by then (like Intel claimed when announcing their heterogenous '*Mix and Match*'-approach around 2018), *Intel would* ***not*** *have needed +6 years for finally reaching a fairly comparable design-approach* only half a decade later in 2023 with Meteor&nbsp;Lake (also Arrow Lake, Sapphire Rapids, Ponte Vecchio).  Since sure enough, all these disintegrated designs \*somehow\* brought Intel truly massive troubles engineering-wise and they had tremendous difficulties to overcome those for years — The years-long horror-stories over validation and dead-on silicon past tape-outs on *Sapphire Rapids* and *Ponte Vecchio* for example are testament to that … *Meteor Lake* was also everything but a performer.  ---- The issue at hand for Intel was quite many-layered …  * Intel hardly knew how to do it (despite claiming otherwise for years). *Shocker!*  * Intel had no real equivalent to AMD's SuperGlue aka *Infinity&nbsp;Fabric*™   That's for sure the most striking one, obviously — The reason they had to wait for PCi-E 5.0 to become a thing, for ""Intel's"" *CXL* to finalize upon it (which in itself is basically just AMD's former *CCIX* in disguise as Copy-pasta anyway done out of spite).  * Intel haven't remotely had adopted a design-strategy by then, which would've offered so to speak ""intelligent"" chiplets/tiles, which could be freely thrown together randomly at will on a (PCi-E) bus (which AMD actually evidently can with their CCX and I/O-dies since ages).  So Intel's claims before the press in 2018, of already working on disintegrated silicon since years already (and that AMD *weren't* actually as spearheading as they became), was pure virtue-signaling, a blatant lie.  As a result, Intel's IP-blocks still remained virtually *dumb* (in the sense of *head*|*less*) for years on out afterwards and most of it had to be *re-engineered from scratch* in all this other trouble like lay-offs, down-sizing or Intel's road-maps being constantly thrown out again (only to start over once more).  The reason why AMD had a years-long edge on chiplets from the onset, was since Intel just couldn't place or handle the stuff as ***independent*** *IP-blocks* — Everything was grown intercoupled and -linked together.  AMD on the other hand already had a decade-long headstart, due to their already well-tuned modular concept of IP-blocks (building-block principle with IP-blocks to freely chose from) attuned for the console-era from back then for the console-contracts (those, Intel always made fun of since).  So Intel allegedly been working on chiplet-esque designs for years already, was utter bullsh!t, and that's actually the sole reason WHY Intel struggled so hard for years with anything Tiles — They started at 0.",Neutral
AMD,"> It definitely feels like an ego thing …  Well, yeah. It definitely IS a ego-thing for Intel — They called them *Tiles* purely out of spite.  Since you can't just call basically the very same what your competitor has, the same name, can you?   That's not how Intel rolls nor would have ever done anyway, even if doing so, would've saves them a lot of engineering-pain in the arse, even more teething-problems and cost them years of falling back behind their only lone competitor …  Truth be told, Intel's *Tiles* are basically in essence just Copy-Pasta: *A botched Copy'nPaste-job from AMD's chiplets*, yet relabeled as 'Tiles', just for Intel trying to pretend to have their ""own"" chiplet-esque implementation, even if it's basically the very same …  Wanna hear a joke? When Intel back then around 2018 out of the blue announced their heterogenous *Mix and Match*-stuff (pretending, they'd already worked a decade on this by then), [the actual effing PowerPoint-slides actually didn't even incorporated anything called 'Tiles']( http://web.archive.org/web/20210923105815/https://newsroom.intel.com/wp-content/uploads/sites/11/2018/08/monolithic-vs-heterogeneous-infographic.pdf) — *Called them* ***chiplets****!*  Yet some big weak weasel's ego was hurt in Santa Clara, and they renamed it *Tiles* shortly afterwards.  > … just copy Ryzen's super successful and ultimately quite straightforward chiplet-scaling strategy.  They joke is, due to both of them share a cross-patent agreement, Intel even would've had access to AMD's patented stuff over everything chiplets — Chances are Santa Clara chose not to over license-fees, or spite …  *That's how you're f–cked over by your own ego* — Rather throw years of potential lead into the gutter, instead of even *thinking* about having to share some meager percentage of profits of yours with others.  That's Intel for you. A bunch of braggarts and loud-mouths, wo always think they can do and know better, yet fail nigh every single time and still can't bring themselves to be humble for once …",Negative
AMD,AMD is actually rumored to have the first TSMC N2 products out.,Neutral
AMD,"I believe  For N2; APPL A20/M6, NVDA Feynman, INTL Nova Lake   For N2P; APPL A21, MTEK 9600, QCOM SDE8G6, AMD Venice  AVGO, MRVL, AMZN, MSFT, GOOG have also volume contracts for n2/n2p production with several SKUs in bring up already.",Neutral
AMD,Late 2026 is Rubin and it's on 3nm. 2027 is Rubin Ultra also on 3nm.,Neutral
AMD,"You're absolutely allowed to discuss the rumor. I am also allowed to point these discussions are way beyond the pay grade of a lot of people in this sub.  FWIW Designs for 2nm generation are already in production, so that ship has sailed. If AMD was to be planning on doing EPYC on SS 2nm, they would be already be at the bring up stage or close to it. Which does not seem to be the case.  Edit: However Samsung's foundry roadmap aligns with some of AMD's monolithic die roadmap, so it would make sense for AMD to at the very least explore their foundry options there. But for 2nm nodes, those negotiations should already have happened a while back.",Neutral
AMD,"AMD only ever claimed N2, which is interesting because Mediatek had no problem specifying N2P, though IIRC their original press release *also* only had ""N2"".   Given that AMD seems like they will launch their N2 products the earliest, I think they might be on N2 rather than N2P, but who knows. The differences between the two nodes seem very minor anyway.",Positive
AMD,"Is it strange that i feel like 3nm is just old tech now ;)  Google AI says  ""Nvidia is heavily invested in 2nm chip technology, planning its next-generation ""Feynman"" AI architecture on this advanced process node via TSMC, targeting mass production around late 2026 or 2027, following its current ""Rubin"" (3nm) chips, with its cuLitho tech aiding the shift for improved efficiency and speed, aiming to maintain leadership in AI hardware despite rivals also targeting 2nm""  Google AI wouldn't lie would it? ;)  Watch Nvidia 2nm get delayed until 2029 because no ram available ;)",Neutral
AMD,">You're absolutely allowed to discuss the rumor. I am also allowed to point these discussions are way beyond the pay grade of a lot of people in this sub.  I'm just baffled why you brought it up if you seem like you *agree* with the premise of the original comment anyway, other to just dunk on gamers? Lol.   >FWIW Designs for 2nm generation are already in production,  Only 18A and Samsung 2nm, both of which are likely not on par with TSMC's 2nm node. Meaning calling them part of the ""2nm generation"" is a stretch.",Neutral
AMD,Google AI is telling me the correct info: https://i.imgur.com/HPg7fhN.png,Neutral
AMD,"> I'm just baffled why you brought it up if you seem like you agree with the premise of the original comment anyway, other to just dunk on gamers? Lol.  It's their pastime.",Negative
AMD,Another AI miracle.,Neutral
AMD,"AI has fuzzy memory like humans. The way attention works in AI is based on context. Certain keywords increase or decrease certain likelihoods.   Vague prompts increase the likelihood of a hallucinated response.. In my screenshot you can tell it also provided the picture of Nvidia's official roadmap. This is called grounding.   Can't trust AI 100% time just how you can't trust humans 100% of time. But there are ways to increase accuracy via grounding, or prompts. We're still figuring out how to extract the best out of these models. They aren't perfect.",Neutral
AMD,"The rumor that Samsung will manufacture the PS6 APU doesn't hold up when you look at standard semiconductor timelines. If we assume a late 2027 launch, mass production needs to start by early 2027. In the semiconductor world, you don't select a foundry 12 months before production; that decision is locked in 2–3 years in advance during the design phase because the physical design is tied to a specific process node. At this stage, AMD should already be in the 'bring-up' phase with early working silicon in the labs. Switching to Samsung now would require a massive redesign that fits neither the timeline nor the logic of a major console launch.",Neutral
AMD,"*— Article itself is in Korean —*  The article states that according to South-Korean media reports, AMD is in talks with Samsung's Foundry-division about chip-manufacturing …  While AMD will expressively **not** leave TSMC behind, especially not in the imminent advent of their N2-ramping (the majority of AMD's setup is scheduled to move to N2 later on), AMD may adopt a two-pronged strategy for possibly having Samsung's foundry manufacture the chips for the upcoming PS6.  So AMD could eventually use Samsung's SF2-process for PlayStation&nbsp;6-chips, for the console which is scheduled for end of 2027, to split and shoulder the load of manufacturing, for ease of availability.",Neutral
AMD,"Samsung: ""Get an ambulance... but not for me!""",Neutral
AMD,"RAM might be three times as expensive and rising, but at least we get to reap the benefits; for instance we all get to marvel at whatever the fuck this AI generated thumbnail is trying to convey",Negative
AMD,"Has Samsung ever made x86 chips? Is there any reason they couldn't, in partnership with AMD? Or is this also suggesting that Sony could be moving to ARM?",Neutral
AMD,From the article it seems they’re not actively replacing TSMC.   AMD is choosing to probably dual-source chip manufacturing to both Samsung and TSMC and treat Samsung as a secondary supplier.   Most likely it is because TSMC allocation is too full for them to make the amount of chips they need.,Neutral
AMD,You're assuming they will use Samsung at launch.,Neutral
AMD,"You don't have to lock in your foundry choice 3 years in advance, but 2 years is pretty close.  Three years in advance, you are kicking the tires on the PDK and making sure the node will achieve your goals for your design.  That is not so expensive you cannot do it for multiple foundries.  The next step, is finalizing the layout for masks, which culminates in the tape-out.  You could do that for multiple processes, but it does not make much sense, as you should have known which process you prefer before that step.  Masks are very expensive so you don't want to make them for two different processes.  So tape out is the last chance you have to make a foundry decision, but you should make it months before that.  As an example, AMD announced back in April that Venice chiplets taped out on TSMC 2nm.  Venice is going to be included in Helios racks that are being delivered in Q3 next year.  So maybe a little less than 1.5 years from tapeout to product.  Add in the time for doing the physical layout, and you get pretty close to 2 years.  So I'd expect that right about now is when the final decision on where to manufacture PS6 is being made.",Neutral
AMD,"> In the semiconductor world, you don't select a foundry 12 months before production; that decision is locked in 2–3 years in advance during the design phase because the physical design is tied to a specific process node.  I understand where you're coming from and the reasoning of year-long lead times for a given process is pretty much self-explanatory, but it's not *that* much like 2–3 years. It's 1–2 years in advance, if it takes that long.  Also, it's not that AMD (just like others like nVidia, Broadcom, Qualcomm et al) aren't used to foundry-hopping since decades due to the short cadences of GPU- and CPU-release cycles — *They all have extremely well-attuned process-tailored teams for such foundry-stuff, speeding up things massively*.  Taking GPUs as an example, these are AMD- and Nvidia-teams of hundreds to thousands of process-engineers, VLSI-magicians and all kind of specialists for EDA-tooling and whatnot else is needed, working closely on-site directly at foundries like TSMC, Samsung or GlobalFoundries and interoperate with the foundry's respective specialists for stuff like that …  So given the current time-line at end of 2025, it would be *virtually two full years* up to it, until a launch scheduled for end of 2027, which ought to be pretty sufficient and timely fitting — Pretty realistic if you ask me.",Neutral
AMD,"> At this stage, AMD should already be in the 'bring-up' phase with early working silicon in the labs. Switching to Samsung now would require a massive redesign that fits neither the timeline nor the logic of a major console launch.  Nah.. Remember the launch of Zen&nbsp;5 (Ryzen 9000), which AMD suddenly postponed, due to unspecified 'quality-control' issues at early production-runs? *They recalled and canned ALL already shipped batches prior to launch*.  Who knows what it really was, but it's possible, that it was comparable to Intel's oxidation-issues or comparable process- or design-flaws — It still didn't took them years to correct that, but just a couple of *months* …  I'd say, over the years no other companies became as time-efficient and extremely attuned to toss a design, fix it and redesign it ASAP, only to bulldoze through the whole initial pre-manufacturing process once again in a speed-run-like fashion all over again, as much as as AMD and nVidia are by now — I'd bet, that a complete design-change might take them barely more than 3–6 months total.",Neutral
AMD,"You're joking, but considering how much contracts and design-wins Samsung's foundry-division got over the months from others, I'd consider them the *»Foundry-winner of the Year«* for 2025.",Neutral
AMD,I don't see why they could not make X86 chips. But this is suggesting that the PS6 chip could be manufactured both at TSMC and at Samsung to ensure that they don't have availability problems.,Neutral
AMD,The process is architecture agnostic.   They're probably doing the switch to free up volume for the best TSMC processes for AI and CPUs.   Samsung might also be offering a price competitive option.,Neutral
AMD,"Samsung can fabricate whatever their customers bring them. So yes, if AMD pays them to fabricate x86 chips, there is no reason they can't do it.",Neutral
AMD,Global Foundries 14nm node was licensed from Samsung. Their 12nm and 12nm+ was just refinements of that node. So their nodes have been used for x86 at least.,Neutral
AMD,"> Has Samsung ever made x86 chips?  Yes, they manufactured a bunch of Intel-designs over the years.  In any case, *processes don't care about architectures*. These are architecture-agnostic, as others pointed out.  > Is there any reason they couldn't, in partnership with AMD?  Nope. A foundry manufactures what the client asks them for, or don't, depending on contractual incentives.",Neutral
AMD,More like everyone wants tsmc chips so much they became too expensive and Samsung made a half decent chip at a lower price,Negative
AMD,"And even relatively inexperienced teams can pull off faster timelines than 2 years. I was following the crypto space early on when ASICs started being taped out.   And you had relatively small teams that manged to design, raise funding and get shit out the door within not much more than a year. Granted a Bitcoin miner is some of the easiest chips out there to design, so that part was not very time consuming. But you had some companies like KNC that launched on at the time leading edge nodes like 20nm planar back in mid 2014, so not like they launched on some ancient/quick node. And they were formed and started taking pre-orders in early 2013.",Neutral
AMD,Are you really going to write every comment using AI?,Neutral
AMD,"> But this is suggesting that the PS6 chip could be manufactured both at TSMC and at Samsung to ensure that they don't have availability problems.  I'd just see it as a repeat/extension of the first Ryzen-launch — AMD not just adopted a two-pronged approach back then with the original-Ryzen in 2017 (TSMC, Samsung), they even opted for *a three-pronged strategy* (TSMC, Samsung, GlobalFoundries) for a single design of their CCXs (Ryzen, Threadripper, Epyc) …  Still wasn't enough to satisfy demand anyway — Limited availability, as markets just gobbled up ALL of it in months.",Neutral
AMD,"That's not how it works.  You don't just bring a design to a silicon foundry the way you bring a blueprint to a factory.  You have to create the design using that foundry's software tools.  If AMD uses Samsung as a second source, that means they re-created the low-level design for Samsung from scratch.  None of the people who created the TSMC design would be legally allowed to take part.  That leaves Samsung-specific employees and the broader design team that creates the higher-level design.  It is true, of course, that there's nothing special about an x86 chip versus some other chip.  The point is, Samsung just runs the process.  AMD has to design the chip on that process, with assistance from Samsung being limited to issues well outside the scope of the high-level chip design.",Neutral
AMD,Pre 10nm Intel was generation ahead of Industry by a full node shrink they introduced FinFet before everyone was on HKMG,Neutral
AMD,"I'm reasonably sure Samsung has never manufactured any x86 chips.  Intel did use them for chipsets in the past, but not processors.    Actual Intel x86 processors have always been made by Intel in-house, with the exception of their recent use of TSMC.    AMD x86 processors have been made in-house, by Global Foundries (which was spun off from AMD), and by TSMC.",Neutral
AMD,Taiwan says China will invade in 2027.  That's why 2026 will see a massive shakeup in TSMC orders.,Neutral
AMD,"Yup, I think 2 full years is actually a ample amount of time for a experienced and well-attuned team, and being tightly integrated at given foundries on top of that, tremendously helps to cut months of time.  AMD, Nvidia and others are used to short, highly stressful stints like that, since it's virtually their daily work.",Positive
AMD,"is it really AI? using "".."" and space after ""—"" makes me think it wasn't AI generated.",Neutral
AMD,"Good Lord, what have you people with your accusations of using AI for comments every time?!  You really become a pain in the butt with this sh!t over AI-stuff — *Just because I use hyphens?!*  You can browse my comment-history, and you'd see, that I'd quite likely already used hyphens on my first comments ever made here on Reddit about 8 years ago (a time, when AI wasn't even existing). It's a style of writing for me ever since I use since like two or three decades, as I got used to it, as many are from the scientific/technical-driven realms.  ---- And talking about y'all imbecile reasoning over em-dashes (or en-dashes for that matter); Did it ever occur to you, that using em-/en-dashes is actually quite common, actually often mandatory in the scientific field of applied since?!  Ever opened a effing book? Or read a news-paper the last decade? Em-dashes (—) are *usually used to bind two sentences of the same scope*, which belonging together analogously. Meanwhile a en-dash (–) is used for date-formats in everyday life and money-values et al and so forth.  In any case, in print media (newspaper, books, scientific works), dashes are heavily used, in the field of typography even as a stylistic element … and if you would've cared to look at the way I wrote my post;  I usually try to write while trying to maintain a pleasing appearance for *ease of reasoning*, and dashes tremendously help with that, that's also why I use horizontal rulers within posts (differentiate separate issues).  You can see the source of my posts using RES (Reddit-enhancement suite), and discover, that I often also use proper non-braking white-spaces (`&nbsp;`) for when it's needed, given proper HTML-entities like &times; (`&nbsp;`) for formulas or given specific brand-nams and whatnot (Intel**_**Core, Intel**_**ARC; AMD**_**Ryzen, AMD**_**Radeon; nVidia**_**GTX/RTX).  In short; *Shut up and learn about [Quad](https://en.wikipedia.org/wiki/Quad_\(typography\)) and that sort of stuff!* -.-",Neutral
AMD,> None of the people who created the TSMC design would be legally allowed to take part  Really? I would be shocked to learn that people who worked on the physical design of a chip fabbed at TSMC couldn't also work on a design for a chip fabbed at Samsung.,Neutral
AMD,"> I'm reasonably sure Samsung has never manufactured any x86 chips. Intel did use them for chipsets in the past, but not processors.  That's not true. Just because Intel never \*admitted\* it publicly (so save face and protect their golden cow, their stock), doesn't mean, Intel never outsourced before — They did in fact, and they did in fact *en masse* for years …  Since back then during the self-inflicted 14nm-shortages (Meltdown, Spectre, Foreshadow; also 10nm-issues), Intel's CEO Bob Swan reshuffled much capacity and brought over almost all chipsets to Samsung being fabbed there.  Their low-end CPUs like i3s and Pentium were later manufactured at Samsung as well.  TechPowerUp.com – [Intel Turns to Samsung in Order to Resolve CPU Shortage on the 14 nm Process](https://www.techpowerup.com/256613/intel-turns-to-samsung-in-order-to-resolve-cpu-shortage-on-the-14-nm-process)  (2019)   TechPowerUp.com – [Samsung Scores PC CPU Manufacturing Order from Intel](https://www.techpowerup.com/261641/samsung-scores-pc-cpu-manufacturing-order-from-intel) (2019)  > Actual Intel x86 processors have always been made by Intel in-house, with the exception of their recent use of TSMC.  Just goes to shows that you ain't aware of the actually situation and not really informed. No offense though!  No, with all due respect, but that's pure nonsense — All throughout the time of Intel pumping the mobile market with Atoms (2007-2013, fighting the myriad of ARM-licensees like Samsung, Qualcomm, MediaTek), these very Atoms (especially each and all for the mobile market), have been made *virtually exclusively by TSMC* for several years in a row (and NOT Intel itself) since at least 2009.  True be told, I'm not even aware if Intel ever manufactured their own Atoms ever again past first Gen Atoms (Silverthorne) in 2008, when they basically showed over their whole Atom-business to TSMC for several years.  ArsTechnica.com – *Atom can’t feed fab monster;* [Intel outsources chips to TSMC](https://arstechnica.com/gadgets/2009/03/atom-cant-feed-rd-monster-intel-outsources-chips-to-tsmc/) (2009)  Remember, up until recently with TSMC after them announcing ""outsourcing-possibilities"" (to keep pace) in 2021, Intel **never** had ever admitted publicly upon any outsourcing, except back then as mentioned above.  This was done fully *strategically*, as it would've signalled an actual *admission of struggling and falling behind* (and necessarily having to contract outsiders), which Intel has been desperately trying to avoid for years on end since 2012 or so and through-out the whole fiasco of their 10nm™ …  Ever before, these were all 'rumors' Intel either vehemently disputed publicly (despite doing *the exact contrary*), or just let pass without comment, despite everyone involved knew from day one, that such rumors were 100% true.",Neutral
AMD,Is there a source for this claim?,Neutral
AMD,"It's laughable, as em-dashes (or en-dashes for that matter), have been used especially in typography for at least *the whole of the 20th century*. Actually, around 1850–1990 was the height of such usage in anything print.  Look at any newspaper-archive; Virtually every other older printed text (and even handwritten ones) the last hundred years, uses dashes for *separating the principal clause from the subordinate clause* (En-dash; –, HTML:`&ndash;`) like a counter-argument the writer dropped mid-sentence (for not trying to disrupt the readers' current train of thought), while embedded sentences either \*within\* a principal clause or between principal and the subordinate clause, is ought to be signified with a Em-dash before at the beginning and the end afterwards (Em-dash; —, HTML:`&mdash;`).  **Edit:** Specifically full dashes are also used in writing (directly at the point of breakage), to signify to the reader a sentence or written (direct) speech, which ended abruptly mid-sentence. The same goes for suspension marks (ellipses; …; `&hellip;`), if the recipient is supposed to fill in the blanks by himself.  I even often type them directly using the num-block (Alt+0150 for – and Alt+—). … but I'm using AI! -.-  I just despise wrong spelling or the complete absence of punctuation marks, that's all.",Neutral
AMD,"The use of ""—"" strongly hints at AI, since virtually nobody types that character.  99%+ of people type "" - "" instead, because ""—"" is a special character that can't be simply typed on any normal keyboard.",Neutral
AMD,"Bro, you wrote a novel in this post with pixel perfect formatting and bunch of formatting flourishes. Nobody does that. You are using AI. It's clear as daylight.",Negative
AMD,"There are a lot.  https://news.sky.com/story/taiwan-to-prepare-for-combat-by-2027-president-says-as-he-warns-china-is-preparing-to-take-the-country-by-force-13475504  Also, I'm seeing that China may surround Taiwan and quarantine it.  No matter how you look at it, this will damage TSMC.  TSMCs chip future is likely to come to an end around this time.  Samsung and Intel need to start ramping.  They have about a year.",Neutral
AMD,"Option+Shift "" - "" is pretty easy to type on a mac—although people usually don't bother.",Neutral
AMD,"For many linux users AltGr + Shift + - results in ""—"".",Neutral
AMD,Long press the - on an iPhone and it pops up as an option.,Neutral
AMD,"> The use of ""—"" strongly hints at AI, since virtually nobody types that character.  No, it does actually **not**. If anything, it just shows that LL-models (for training AI in the first place) were crawled (illegally) upon huge libraries of digitized text-corpuses to begin with …  I mean … *Where do y'all fools think, these AI-bots got those dashes from in the first place?!* Exactly.  In any case, if you happened to put open any decade-old newspaper, you'd know, since newspaper and articles (at least the ones being written by actual humans) to this day are full of hyphen and dashes.  Anyway, at least I do since decades (and many I know too), and so do many in the scientific field in general.",Negative
AMD,">Nobody does that.  At least one person does that, and did that years ago already. Seriously, look at the comment history",Neutral
AMD,"Well, no kidding. The U.S. constantly try to push it, no? I mean, aircraft-carriers in the Taiwan straight?",Neutral
AMD,"Or *Option*(+Shift)+*Minus* for different dashes (`⌥`+`-`), while *Option*+*Period* for ellipses is another (`⌥`+`.`), and (`⌥`+`+`) for the plus-minus-sign (±) — I picked those up on a Macintosh IIfx back then in the Eighties at the university and have been using those ever since …  Though it's not just Mac&nbsp;OS&nbsp;X which has been incorporating special typographic characters into keyboard-layouts since decades even under Mac&nbsp;OS&nbsp;9 and below (e.g. proper typographic quotation-marks [“…”] or Guillemets [»…«]), most Linux-distributions' default keymap are also filled to capacity with loads of special characters, of which most you can't even reach under Windows without Alt-Codes via Unicode through the num-pad — Just see Ubuntu's default keyboard-layout …  That's why the likelihood of layouters, writers, editors and content-creators (in the *classical* sense at least, journalistically speaking), often use Mac OS out of principle — The chaos and annoyance to even reach special typographic characters under Windows, is a complete mess and no-go for many.",Neutral
AMD,"Because sure, why not.",Neutral
AMD,Yeah because one cannot pay more for ram than cpu.,Neutral
AMD,"As someone who doesn't have a single piece of Intel silicon in my build, I've never understood people cheering on their downfall. We need competition, people, or shit like this happens.",Negative
AMD,Bought 64GB memory and 4TB storage in June. Have bought a Ryzen 7 9700X and RX 9070 this Friday.  I feel quite lucky.,Positive
AMD,"I don't think this has anything to do with the memory shortage anymore, it's just pure greed.",Negative
AMD,"Of course they do. Black Friday prices aren't supposed to be the new normal prices. I'm sure Intel and Nvidia are doing it too, this is not newsworthy.  In the Videocardz article about it they even admit this is not raising the normal prices, it's just prices returning to normal after Black Friday:  https://videocardz.com/newz/amd-rumored-to-raise-ryzen-9000-and-older-cpu-prices-tonight",Negative
AMD,Won't someone think of the (checks notes) 355 BILLION dollar company!?!?!?!,Neutral
AMD,At this point I can see me running my 4670k GTX 970 build from 11 years ago until 2040 and beyond. Got my duct tape ready.,Neutral
AMD,A decent PC about to cost the same as a new car,Neutral
AMD,"“Hey people are still buying ram at these prices, let’s raise our prices too!”",Negative
AMD,"They did not get the memo, that people aren’t building PC due to memory prices?  Good luck AMD, I managed to get 9800x3d for 399€ brand new and I was still on the fence about it. Like really on the fence, it was a stretch. Cause they blow up!  Not to forget I am an enthusiast. I upgrade GPU every gen and always get the latest platform.  If this was a strech for me, then 90% won’t even look at those if you increase the price, especially now.",Neutral
AMD,they’re ryzen the prices  i’ll go now,Neutral
AMD,"AMD: ""What are you going to do? Buy Intel?""",Neutral
AMD,"Nice, now it's gonna be rising prices for RAM, GPU, and CPU!",Positive
AMD,"RAM makers are not going to like this, they rather prefer CPU prices being low because now when they ship DDR5 memory to margin highs  they want consumers to not second ask themselves regarding PC upgrades like they already do.",Negative
AMD,people who built a pc at summer/spring must be happy af,Positive
AMD,"This is what happens when there is no serious competition around the horizon, it's not the first time that AMD has done this btw.",Neutral
AMD,I think AMD should reduce prices instead of increasing.,Negative
AMD,"Intel is back, AMD should lower their prices. It would be better for their future.",Positive
AMD,"Just bought mine, sorry everybody. Hopefully RAM and motherboard prices will crash for everyone else to make up for it, now that I ovepayed.",Negative
AMD,man i was JUST thinking of upgrading from my 3900x....,Negative
AMD,I guarantee you they wont raise prices of their datacenter CPUs because you bet your ass their customers would switch to arm,Negative
AMD,"Proof that not having competition is bad.  Fast RISC-V chips soon, hopefully.",Negative
AMD,"If AMD holds the commanding lead in the retail market share with their Ryzen CPUs, there board and investors will probably be demanding more profit be taken.",Neutral
AMD,"Damn, didn't know CPU chips had tiny ram slots in them",Negative
AMD,Good thing I got my 9800X3D before this happened.,Neutral
AMD,And they will not drop them until Intel gets their shit together,Negative
AMD,"Hehe, glad I got my Ryzen 9 a few days ago.",Positive
AMD,I'm so glad I didn't wait longer to build my PC,Positive
AMD,"when intel was the top dog, everyone excused it by saying they had the best performance and stability, amd bulldozer was bad blah blah blah don't buy AMD  now that AMD is the industry leader all of a sudden competition is important  so obvious.",Negative
AMD,I mean sure why not . Their gpus finally hit MSRP 8 months after launch so something gotta give,Neutral
AMD,Glad I bought a 9800x3d last week I guess,Positive
AMD,"Instead of reporting on your lack of information, be a farking journalist and actually research the answers before writing about it. Even AI can match this level of journalism.",Negative
AMD,"Depends how much but I like to think this is my final amd product at this point. They have not been making the best decisions the past few years...   Like if intel is cheaper and more powerful at this point, might as well get that then.",Neutral
AMD,Because AMD is a damn corporation aiming to keep its margins while learning from the giants about consumer exploitations.,Negative
AMD,"The setup looks clean, but the price news definitely stings.",Positive
AMD,"No, they're not:  https://www.tomshardware.com/pc-components/cpus/amd-isnt-increasing-prices-on-cpus-at-least-for-now-ryzen-appears-to-be-safe-from-the-ai-hysteria",Neutral
AMD,I think it’s way more likely rx 9000 has 0% than it does like 0.1% I mean it just makes sense 0 people bought it,Neutral
AMD,Cache memory is going up in costs so AMD has to make up for it. /s,Neutral
AMD,"You've been clickbaited. Yes, prices go up again after a sale. This is a non-story.",Negative
AMD,"This is not AMD raising prices. This is prices going back to normal after Black Friday is over.   The Videocardz article on this ""news"" says this is a return to normal, not the normal prices being raised:  https://videocardz.com/newz/amd-rumored-to-raise-ryzen-9000-and-older-cpu-prices-tonight",Negative
AMD,"""Eeh, might as well"" -Lisa",Neutral
AMD,Very good example being nvidia vs amd. Nvidia can keep their prices outrageous just because there’s no meaningful competition.,Neutral
AMD,Intel pricing is pretty competitive these days. As soon as gaming isn't your top priority they aren't a bad choice at all.,Negative
AMD,What model wifi card or Ethernet controller do you use? I've had really good luck with the Intel ones.,Positive
AMD,"But you see, good guy AMD would never do such a thing!",Neutral
AMD,People cheer on the downfall of Intel for the same reason they'll cheer on the downfall of a rival sports team.,Neutral
AMD,"last time after amd 64 intel got back swinging, This time it looks really sad",Negative
AMD,"The people that cheered for Intel's downfall were probably computer users from the time when it was Intel or nothing, that wasn't a great time because competition was low.",Negative
AMD,intel was a bad actor and actively tried to sabotage AMD and shun them from system integrators. sorry can't sympathize with intel.,Negative
AMD,"I bought about 80 in the last ten years, I'm doing my part.",Neutral
AMD,"I hate to break it to you, but the existence of Intel does not prevent sales being ended and prices returning to what they were previously.",Negative
AMD,"> I've never understood people cheering on their downfall  They were almost a monopoly for a long time, excepting the AMD K8 era.  Intel's business practices and strategic decisions have been extreme short-term profit motivated since.. even before 2010? When they were dominating, they would just hold back from bringing tech to market to maximize profits. Deliberate decisions to rest on their laurels, to not invest into real R&D. It's been a slow motion train wreck, competitors with much more growth R&D momentum catapulting past them, leaving them in the dust.  It's kind of like a desire for justice? Cheering on an org reaping what they sow? Of course, in America companies like Intel, or certain industries, like in 2008, get bailed out anyway. No justice; moral hazards aplenty.",Negative
AMD,"Well just read a article about intel making their own version of x3d cache for their next cpu linup, which will have lots of cache memory. If they actually perform for once and not take 300 watts to do it, could be enough competition to push AMD to lower prices again.  A big will see though.",Neutral
AMD,"Jealous. I bought 5x20TBs two months ago from Amazon, and they finally acknowledge it was lost somewhere and gave me a refund. I can only buy 3x20TBs now.",Negative
AMD,I bought 64gb of ram 2 years ago for $130. 6000mhz,Neutral
AMD,"I bought my 7600X for $150 and DDR5 32GB kit for $80, and additional 1TB SSD for just $40. Now I see both the SSD and the RAM being 2 - 4x the price of what I paid for nowadays when I am browsing our local online marketplace. I can say that I feel the same.",Neutral
AMD,Damn.. I bought 64GB memory last week and a 9800X3D. Paid $380 for the memory.. at least I got the CPU before it increased.,Negative
AMD,"Yeah, I jumped a little earlier and I'm still sitting on AM4, but when it looked like tariffs were gonna blow up the PC component market I made sure that I had a 5950X, 64GB of Samsung B-die, an 8TB flash drive, and a 9070XT.  Planning to coast on this for the next few years and hope there's still a hobby on the other side.",Neutral
AMD,"I bought a 9950x3d and 192gb like 2 months ago, i am happy i did so ahahah",Positive
AMD,The memory shortage is just one symptom of AI chewing through supply and increasing prices.   The DRAM still needs a CPU…,Negative
AMD,"""Black Friday sales have ended"" is not in fact a sign that we have been overtaken by greed.",Negative
AMD,"its... capitalism, from a publicly traded company. When was it ever not about greed??",Negative
AMD,"Nope, there isn't. It's simply a power move by AMD Ryzen because they know exactly the consumers will still choose them anyway over Intel that is already dragged on the mud by the reviewers and tech enthusiast community in general.",Neutral
AMD,"Please boost this, the entire thread is being ragebaited by an absolutely garbage article writing about a complete nothing burger. In the article it reads  ""The timing follows Black Friday and Cyber Monday discounts.....return to standard pricing.""  The article puts in complete rumor gibberish, and baits with RAM drama to confuse the reader because its literally just saying ""AMD cpu's went on a discount for Black Friday, they are now losing the discount."" Except the college student who wrote this had to pad the word count to 1000 on a 50 word article.   Completely nothing burger written for interaction bait.",Negative
AMD,"I mean, their market cap is only a little higher than the GDP of Portugal.  They're only slightly too big to fail.  I'm pretty sure the other tech giants throw fries at them at the lunch table.  It's actually kind of sad, really.  NVIDIA could find the cash to acquire them between its couch cushions at this point.",Negative
AMD,Nah don’t worry new car prices are through the roof as well,Negative
AMD,"Yep, Ive been planning on building my dream PC since I can finally afford to.   Not going to happen if the whole industry decides to fuck us. I'll watch and laugh at them when AI pops and the market is flooded with their existing and planned hardware.",Negative
AMD,"It will be sooner than later I can feel. These companies will go extinct, they are too greedy to exist.",Negative
AMD,"Intel isn't back right now... Arrow Lake isn't getting much traction,  the refresh isn't getting much traction (though it's honestly pretty good), so desktop DIY seems to be AMD's market.  They hence have pricing power.  For mobile AMD isn't really a big player but you don't buy DIY laptop chips so it isn't relevant.  Lunar Lake is pretty good (it's efficient, though ARM solutions still beat it in performance per watt) and Panther Lake is sounding better (ARM still wins, but it's a good gap over AMD and most people ex-Apple want x86).  That's a different market than desktop though.  When Nova Lake comes out it'll be a much better chipset than ARL, we'll see if they have giant cache chips or not which is what a lot of DIY people want.  If that happens then maybe Intel can compete better and exert more competitive pressure on AMD.",Neutral
AMD,Intel is back in what? Panther Lake is only M2 level,Neutral
AMD,"They're not raising them. In fact, the 9800X3D is cheaper now than ever.",Neutral
AMD,This article is complete clickbait about Black Friday sales ending. It applies to all of their competition as well.,Negative
AMD,There are people that prefer their cabling doesn't spontaneously combust.,Negative
AMD,not to mention 3D in the title of the CPU...,Neutral
AMD,considering how many reviewers pick a sales price and base their performance/dollar graphs based on that you may as well see this as raise compared to the misinformation you are fed.,Negative
AMD,I guess we will know in less than 24 hours,Neutral
AMD,"prices going up above msrp 18 months after release isn't ""normal""",Neutral
AMD,"This is just factually wrong. Look at GPU price history from MSRP to market price for the last year. GPU launches were incredibly inflated until these last 2 months, so what you think is ""black friday deals"" and ""cheap"" is what was supposed to be its original MSRP lol. Now this price hike is just to artificially raise the price to maintain the same profit margins they've had from the past year out of pure greed. No reason for AMD to raise prices on existing products on shelfs except for pure greed. No reason for RAM manufacturers to not offer long term contracts to large customers and OEMs when they are purposely causing a shortage, except for pure greed. No reason for Nvidia (who does not produce a large quantity of their FE GPUs themselves and relies on AIBs) to not bundle Vram with their dyes to the AIBs that have a smaller profit margin than Nvidia, except for pure greed.  20% of this is speculation of a AI bubble pop, the other 80% is companies seeing blood in the water and hopping on the price gouging train to artificially raise the prices for the whole industry. When all the businesses collude and work together to raise the price of computing power, which is now an essential commodity now that it powers the world, who stops them? I can't see it being government as they are apart of the majority of the demand for these AI datacenters.  Source you can check out: GPU Prices Crater Before Inevitable Opportunity to Screw Consumers - Gamer Nexus",Negative
AMD,"Counterpoint: Competition only works if there’s checks and balances to prevent price collusion. The stupid SSD mafia colluding and keeping prices high (DAE remember the great fire sale of SSDs in late 2023?). There’s no reason why a 2TB 990 Pro should be that close in price to a 9100 Pro. Were they losing money then? I am hard pressed to believe they were.  The skeptic in me however is willing to bet the RAM prices are never going to go down, and this will become the new normal, and they’ll just pocket the difference (unless there’s something major that happens like upstart Chinese suppliers flooding the DRAM market forcing them to).",Negative
AMD,"and the solution to that isnt shitting on Nvidia, its for AMD to make better cards.",Neutral
AMD,"I mean, there is no competition for the high end. When it comes to medium-high performance, the competition is absolutely there. It's really just the 5090.",Neutral
AMD,"this is partially incorrect.  as there were lawsuits and settlements about amd/ati and nvidia price fixing.  if price fixing is happening, there is no competition.  it is a fake competition, just like the memory industry, where a memory cartel sets their prices through price fixing and unified supply control (let's all massively reduce production and increase prices for example)  BUT it can look to the average consumer to still be ""competition"" then.  is amd and nvidia rightnow price fixing?  well there sure as shit won't be an investigation into it rightnow. hell nvidia can triple down on fire hazards without a recall. and the pricing between nvidia and amd are surprisingly almost always very aligned.  what a coincidence.  amd is also not interested to sell anything aggressively, despite wrongfully claiming they would.  so there is no meaningful competition going on at all here anymore.  and it is reasonable to expect, that price fixing is going on  as well of course.",Negative
AMD,"Yeah, for productivity Arrow Lake currently offers better performance than similarly priced AMD competition. They also don't seem to be cooking themselves (so far) and don't suck back stupid amounts of power the way Raptor Lake did.   Intel's lack of platform longevity is still a pain point, but if that doesn't matter to you and you just care about getting the best bang for your buck right now I wouldn't fault anyone for going Intel.",Positive
AMD,"Even with gaming, I just picked up a 225f for $155 on Amazon.  While not exactly on par with a 9600, it is close enough to save $40 on the CPU and saved $50 on the same brand's Intel vs AM5 ITX board.",Neutral
AMD,just purchased an i5 14600KF after selling my Ryzen 5 5600 just because I had DDR4 ram to utilize it with.  I believe there is a lot of misinformation online on how the 14th gen is still messed up to this day but I'm having zero problems (updated BIOS to be sure). Ran BF6 earlier (CPU demanding game) and CPU was running 70% - 80% at 58 to 60 degrees at stock lol,Neutral
AMD,"I've been out of the loop with pc components for the last 3/4 odd years. My build is primarily music production-focused but I do play games on it quite a bit (primarily PvE, I don't really need anything beyond 4K @60fps): AMD Ryzen 3600, Nvidia GTX 1660Ti, 32 GBs of DDR4 RAM, Asus TUF B450M pro-II. What should I be looking at if I wanted to upgrade without dumping half of my wage for DDR5 sticks?",Neutral
AMD,"For homelabbing, Intel seems almost purpose-built for this. Lots of cores for cheap (never thought I’d say this about Intel), decent enough single-thread, and that excellent Quicksync.",Positive
AMD,Isn't Intel in the middle of divesting themselves from their networking business?,Neutral
AMD,"I dunno, I just use the sharkfin antenna that came with my motherboard. Works fine.",Positive
AMD,"The article is clickbait. Yes, black friday sales are over, so prices will rise again. This also applies to their competition.",Negative
AMD,"Amd was better than intel around 2000s, then intel bounced back, now amd is top again, intels gonna bounce back again",Positive
AMD,"You forgot the (brief) period where AMD K7 was dominant.  Also, INTC did not purposely hold back development for years. It literally fell apart when 10nm was delayed. The chip and fab business was so tightly bound that any delays in the fab (10nm) caused the chip design business to stall and make silly workarounds (e.g., Coffee Lake, Rocket Lake, Ice Lake, etc).  You can point to the massive $100B in stock buy backs, but INTC during that same time also spent more on R&D than AMD and TSMC combined.",Neutral
AMD,> not invest into real R&D  [$8-14B/yr](https://www.macrotrends.net/stocks/charts/INTC/intel/research-development-expenses) on what?,Neutral
AMD,"In a similar situation just on a smaller scale. Bought a 20TB disk for 300€, the store sent it in just a cardboard box and it was obviously DOA.  The return department dragged their feet with the replacement for 2 fucking months until one day they just randomly closed the case with a refund. The price of the same drive is now 450€.  The store is Senetic btw, any European shoppers avoid it. Shit packaging, completely unresponsive for any support apart from the initial (legally mandated) return ticket and fucked me over in the end when it was in their financial interest to do so.",Negative
AMD,"Unless you really need them new, I'd check ebay refurbs for 20tbs. I got my 14tbs for like 180 a couple months ago. you might get lucky, however they were WD white enterprises.   i can send a link if you'd like",Neutral
AMD,"I bought 48gb 6000 cl30, Trident Neo Royal Z (the silver with sprinkles) and felt outrageously lavish for paying 320€ this September.    Now they're more than 500€. Jesus Christ. At least the 9800X3D just hit record low with 440€ where I live.",Negative
AMD,"I'm glad I grabbed a 2x32GB DDR4-3200CL20 kit for my laptop earlier this year, paid $90 brand new",Positive
AMD,for what do you need 64gb?,Neutral
AMD,Did you read the article or just comment capitalism bad immediately when you saw the headline,Negative
AMD,They can control the diy market all they want. They still don’t have the real important markets of pre built computers and laptops.,Negative
AMD,"Genuinely, who is buying this shit? Average new car price is ~$50k now.  I make 6 figures and the most I've ever spent on a car was $22k, and even then I kinda regretted it (until I was able to sell it at a profit during the pandemic...)  Apparently several manufacturers are straight-up discontinuing base trims next year in an effort to boost that average sale price even higher.  This can only end in tears.",Negative
AMD,What do you mean intel isn't back? Intel has a 75% market share on CPU's.,Neutral
AMD,Thats a stretch but yes they aren't back yet,Neutral
AMD,Unfortunately I couldn't open the article. Thanks for clarifying!,Negative
AMD,Extra 3 dollars at least just for that.,Neutral
AMD,Source? There is nothing to suggest that's what's happening.   The Ryzen 9950X MSRP is $649.99 at launch (August 2024). I bought it in November 2024 for $599.99. It's currently $539.99 on Amazon. It would take a $110 increase to hit MSRP. This would be a MASSIVE increase and doesn't seem very likely.   My prediction: It'll hit $599 again at most,Neutral
AMD,"What's the MSRP? What's the current price?  That's all that matters.  Not whatever you imagine is ""supposed to be its original MSRP lol"".",Neutral
AMD,"Look, AMD is way behind Nvidia in market cap. By paying these increased prices, we're helping AMD stay competitive in the one arena that really matters. I think I speak for all gamers when I say that increased competition benefits us all.   /s",Neutral
AMD,"RAM prices have to come down. The entire client market, especially OEMs, will be at risk of collapse otherwise. There's no reason to think a rapid 500%+ increase in memory prices due to a shortage (and the resulting panic buying) is permanent.   I dont believe we're about to witness the collapse of the client PC, smartphone, and tablet markets.",Negative
AMD,"The RAM situation is going to kill the entire PC market entirely if it stays this way.  That might be somewhat acceptable if AI demand simply never goes down whatsoever, but this really cant last.  Literally everything like PC's, laptops, smartphones, consoles, etc will all have to go up in price quite a bit.  It's not sustainable.   Also, SSD prices have been very reasonable overall for a while now.  And yes, the latest SSD's will cost more, but come down in price fairly quickly all told.  This is really not an issue.",Negative
AMD,I fully support China eating the revenue of these companies,Negative
AMD,"...so you're describing the lack of competition, aka anti-competitive price collusion. 😅",Negative
AMD,"Lay persons are going to be priced out of the market, we're going to all be on thin clients eventually as our hardware dies, paying monthly for a resolution/framerate package that doesn't meet it's advertised performance.",Negative
AMD,"> Counterpoint: *Competition only works if there’s checks and balances to prevent price collusion*.  That's what I'm saying since years now; Gamers blindly buying nVidia for the sake of it, ruined the GPU-market.",Negative
AMD,"Even in medium-high segment, there is no competition, it's an absolute dominance of the 5070 and 5070ti against the 9070 and 9070XT. It's not even close.",Positive
AMD,"Brother, that's a whole lot of stupid.",Negative
AMD,"> Nvidia doesn't keep their prices high because of ""no competition""... they do it because they are greedy as fuck and have always been that way.  No, Nvidia keeps the prices high, because they CAN — People buy their cards, without thinking, no matter what.  > Even back in the day when AMD/ATI was crushing their shit GPUs by huge margin they still charged more because they handed out bribes to game developers for years and always got their BS software gimmicks like Phys-X in new games. They also spent way more on marketing because AMD had to spend huge amounts on CPU.  There you have it, the answer on why people buy Nvidia-cards. Not because those would be (always) necessarily better, but mostly out of the market's *brand-perception* — nVidia spent billions to fabricate their leader-image.  Same reason forwhy so many people still stick to Intel and answer questions about their CPUs with *""It's a i7!!""*.",Negative
AMD,I got another 3+ years out of my old PC simply by upgrading from 1700x Ryzen to 5600 Ryzen. That system is still viable I just wanted to upgrade to 9800x3d. Platform longevity should not be underestimated.,Positive
AMD,"Mmm I’m a bit confused about this, I thought due to AMD being on a smaller node they were more power efficient…",Neutral
AMD,"> Intel's lack of platform longevity is still a pain point […]  Kind of blows one mind, how Intel still sticks to their idiotic 2-CPUs-per-socket mantra no matter what …  As if it didn't cost them already a good amount of users switching sides, due to AMD have the better cards here.",Negative
AMD,And with the price of RAM right now those sorts of savings do mean a lot. 32GB+ of fast RAM has become as expensive as the GPU for a new computer.,Negative
AMD,If Intel comes out with something that is like the x3d chips then I'd consider them. That extra cache is too awesome.,Positive
AMD,"A big upgrade would be something like a Radeon 9060 XT 16GB, which is about 2x as fast as your current GPU. You can also get a used CPU, something like 5600X or 5700X should be available for not much money and will work, just update your BIOS first.",Positive
AMD,Ryzen 5000x3d is a drop in upgrade.,Neutral
AMD,Not sure. But I still use and have seen many Intel networking chips in the wild.,Neutral
AMD,Well Intel may be the one who made the chips that handle your wifi. They make a huge portion of networking chips.  So you very well might have a partial Intel system without knowing it!,Positive
AMD,"the situation looks dire for intel, they just sold everything off, a lot of engineers are leaving, the money cushion is gone",Negative
AMD,"I run multiple VMs, some docker containers, gaming, software development, etc",Neutral
AMD,"I was playing some Clair Obscur last night, had a few chrome tabs open (youtube etc) to look up certain builds or whatever, discord, etc. I don't even rice my desktop experience with widgets. But after a ~4hr gaming session my RAM was at 27GB. I have 64GB.  One thing I noticed in the past from my years of PC experience is that your system will generally use less RAM if its going to be approaching the limit. This suggests that theres algorithms that will use disk space instead and/or do memory reallocations. There is a performance cost to this.  That said I think 32GB is a good amount of RAM, I rarely approach it.  However you never know what apps or games are leaking memory so having a lot is nice.",Neutral
AMD,Consumerism just never stops.  It boggles my mind how completely thoughtless most consumers are with how they spend money.,Negative
AMD,> Average new car price is ~$50k now  Because of the high end dragging the average up. Entry level cars have been pretty consistently priced after accounting due inflation for quite a while now,Neutral
AMD,"A lot of people think they deserve a treat, the treat being a $75k new car. To the point that “paying off the car” is a common financial milestone.",Neutral
AMD,the carbrain insanity is very sad to see. People buying cars multiple times their yearly income in costs then wondering why they are getting fleeced. An average person spends close to half of lifetime income on a car.,Negative
AMD,"You say this as if their market share hasn't been steadily decreasing over the past couple of years.   What's even worse is that Intel's market share shrink has largely been slowed down by them pushing a bunch of high volume, low margin chips. If you look at revenue share in desktop, AMD is already at 40% share.   Intel's competitive position in desktop has only been deteriorating since X3D came out. Something which Intel's own executives have acknowledged, multiple times, in different conferences and earnings calls.   It's a lil insane people are still denying what Intel's leadership themselves have admitted is a problem.",Negative
AMD,I think they mean amongst youtubers.,Neutral
AMD,"They did something AMD can't yet, ARM level idle but in PPA,PPW and raw performance both AMD and Intel need to work on it.",Neutral
AMD,You get an extra fiddy from the 9850X3D,Neutral
AMD,">The RAM situation is going to kill the entire PC market entirely if it stays this wa  ""Hi, I'm Michael Dell""  ""and I'm Tim Apple!""  ""And together we're excited to announce our new industry-wide pricing initiative!""  ""To help alleviate the burden on consumer PC pricing, we're pioneering the launch of a new initiative that has been in the works for some time: 30-year PC Mortgages with fixed-rate APR!""",Positive
AMD,"> The RAM situation is going to kill the entire PC market entirely if it stays this way. That might be somewhat acceptable if AI demand simply never goes down whatsoever, but this really cant last.  We're lucky that internet infrastructure is so terrible in North America, otherwise Big Tech might succeed by sheer force in a pivot to replace local PCs (and ownership) with pure streaming clients.",Negative
AMD,"Yeah effectively. Competitors != competition. When the industry has super high R&D and setup costs (like CPU/GPU design and manufacturing) it's very difficult for competitors to enter and disrupt the market. Just look at Intel ARC, they are a billion dollar company and yet they are still having difficulty in GPU development.",Neutral
AMD,Its literally close according to every single benchmark but whatever.,Neutral
AMD,"Platform longevity has to have some kind of financial value placed on it.   The maximum value of long platform longevity is the price of a motherboard. And you maximize this value if you buy into the very first generation on that platform. So every subsequent generation you enter in, the value of platform longevity goes down.   You also have to consider what impact generational improvements have on platform longevity. Zen 1 -> Zen 3 was a huge jump. Assuming Zen 7 on AM5, I don't think we're gonna see that level of improvement with Zen 4 -> Zen 7.  Let's say someone upgrades from AM4 to AM5 with the launch of Zen 6. They receive less value from platform longevity than someone who entered at Zen 4. And someone who bought in at Zen 4 will likely receive less value than someone who bought in at Zen 1.  So yes, platform longevity does have value. It's better to have platform longevity than to not have it. But best case scenario, that value is the price of a motherboard, and we could argue on the whole that you subtract from that value based on the circumstances.",Neutral
AMD,"99% of users do not even know what a CPU is, let alone know how to upgrade one.",Negative
AMD,"1700x was already obsolete when it was released, my 4 cores non ht 4670k was superior for gaming with a basic overclock and it was released almost 5 years before the first gen ryzens. I see Soo many people braging about upgradability but all of them could've bought a 8700k a few months after the release of Ryzen and had a system stronger than 1000, 2000, 3000 series and slightly weaker than the 5000.",Neutral
AMD,"They were comparing to Raptor Lake. Also, ARL is on a smaller node than Zen 5.",Neutral
AMD,"AMD isn't on a better node. And power efficient is a difficult metric to measure because it varies wildly on context: ISO-Performance, ISO-Power, ISO-task that isn't time sensitive (web browsing). It depends on power profile (high-performance will sacrifice efficiency for performance). It depends on where in the efficiency curve you are. It depends on how high in the product stack you are (a 7500F is more efficient in gaming than a 9950X, but may be less efficient in highly threaded productivity apps, etc.)  edit: Downvoted for being factually correct. You don't just run CB24 at full power and divide scores to determine efficiency. That's way too simplistic.",Neutral
AMD,"sure, the x3d makes a huge difference, but for my needs I didn't need the extra power or cost associated with an x3d chip for this more budget build.",Neutral
AMD,"Forgot to mention I edit video in DaVinci Resolve so CUDA cores are kind of a must for me, but yeah, the GPU and CPU are definitely getting swapped before anything else",Neutral
AMD,"Oh absolutely! The latest generation 10 gig chips from them are really good, very power efficient compared to prior offerings. But Intel networking stuff has (mostly) always been solid.",Positive
AMD,"Ah, I didn't know that. Not sure why I got downvoted for answering your question, though.",Neutral
AMD,"Wouldnt say that too early, amd suffered same fate like current intel and bounced back",Neutral
AMD,Yeah but everything seems to be more or less tracking with inflation except wages for some odd reason.,Neutral
AMD,"Losing market share isn't great.  But they still have ~75%.  Saying a company with a 75% market share ""isn't back"" is dumb.",Negative
AMD,AMD's PPA is the best on the market. Just not for light workloads no one cares about. Run some database benchmarks.  Nobody cares what Geekbench gamers think.,Positive
AMD,Ninety-eight fiddy ex three dee,Neutral
AMD,"more like  ""Introducing out new lineup running Windows/MacOS SE, designed for just 4GB of DDR3! And it still starts at only $999!""",Neutral
AMD,"you could buy computer parts with a fixed rate loan for a very long time. You shouldnt though. If you cant afford it without a loan, you cannot afford it period.",Negative
AMD,you need to solve communication at faster than light speeds (good luck) to make everthing a streaming client. Anything that isnt local server streaming is absolute ass.,Negative
AMD,"Only if you ignore all the features. You know, the features buyers DONT ignore.",Neutral
AMD,I was talking about sales though?,Neutral
AMD,"Time is also a factor, the amount of time I spend swapping a motherboard is far more than the time I spend swapping a CPU.       Also potentially getting a bad motherboard if I was also swapping a motherboard (has happened twice).       That means loss of more time and effort to reinsert the old motherboard, RMA/return the new board, and then swap the board again again if I manage to get a replacement. Getting an RMA repair/replacement again isn't a 100% guarantee in this day and age with how stingy companies are. So I mean depending on how far you want to take this. By the end of this I could be out an extra $100-$200 in time.",Negative
AMD,"IDK about anyone else, but I had some bad experience with intel and wanted to support AMD. Helps I got the x370 Taichi motherboard as a gift. I still have 2x 32GB Kits of DDR4 that I upgraded to 64GB on two machines. I wonder if I should sell those. I think I should sell my 64GB Kit of DDR4 as well.",Neutral
AMD,Sorry I didn't mean to say it is a one size fits all solution. I'm just hoping that Intel figures something similar out to really push AMD down in price or get them to innovate further,Neutral
AMD,"In that case a 5060 Ti 16GB or something used, e.g. a 4070. I also use Davinci for editing on a 5060 Ti, for Fusion I'd welcome even more power tbh, but everything else works great.",Positive
AMD,"I guess it's the smugness of ""not a single piece of silicon from Intel"" when most likely you and most people do in fact probably have something from intel.   It's r/hardware anyway, most people here are teens that worship youtubers, dont sweat it too much.",Neutral
AMD,"Bulldozer era AMD was truly awful, their GPUs weren’t good, their CPUs weren’t good, and their server CPUs weren’t good either. Their turnaround is crazy.",Negative
AMD,[Inflation adjusted incomes have been trending higher for decades and are up about 5% in the last 5 years](https://fred.stlouisfed.org/series/MEPAINUSA672N),Neutral
AMD,"How can a company ""be back"" if you are literally losing share?   At best one can claim a company is ""back"" if they *stopped* the bleed, but even that hasn't occurred.",Negative
AMD,Geekbench is the opposite of gaming benchmark   And AMD wins ofc in benchmarks that rely on fat caches,Neutral
AMD,and your specific device will be abandoned by the OS in 3-5 years,Neutral
AMD,"Good. I wasn't. You can clearly read ""performance' in my message.",Positive
AMD,I can't image the failure rate for a brand new motherboard that needs to be RMA'd immediately is higher than the failure rate of a motherboard that's by this point already several years old. Although I don't really have the data.  What's the actual time difference of install of just a CPU vs Mobo + CPU? 10 minutes?,Negative
AMD,I went from a Ryzen 1700 to now a 5800X3D on a x470. The value of platform longevity can not be understated!,Neutral
AMD,"Yeah, I'm thinking that 5800XT + 5060Ti 16GB might be the best combo I can aim for until DDR5 is back to normal prices again.  How about ""higher tier"" cards compared to the 5060Ti? 5070s, 5080s etc.?",Positive
AMD,"No worries, it's just internet points at the end of the day, I was just a little confused when I went to respond to his comment and saw the vote count.  I wasn't even trying to be smug, it was more of a ""I don't own anything Intel related (so I have no reason to be going to bat for them), and I still think them failing to compete with AMD sucks for everyone"".",Negative
AMD,7970Ghz Edition was pretty beastly,Neutral
AMD,"game cpu king, productivity king, server? performance king but the industry is shifting to computer per watt",Neutral
AMD,"Intel isn't ""back"" because it never ""left"". They have always retained an extraordinarily high market share.",Negative
AMD,"I didn't say geekbench was a gaming benchmark. But that people treat geekbench like a gaming benchmark and also that geekbench scores are easily gamed (benchmark runs so quickly that the CPUs can easily go beyond their thermal throttling point before the benchmark completes)..  Also plenty of workloads Zen excels at that don't get the benefit from cache. Like I mentioned database benchmarks, where SMT can give you 50% more IPC. Meaning better perf. and better perf/watt.  You know workloads that pay the bills. Not Geekbench. Of course Geekbench also pays the bills by misleading consumers.",Neutral
AMD,Weird. Considering we're in a price related thread. Whatever I guess.,Neutral
AMD,> What's the actual time difference of install of just a CPU vs Mobo + CPU? 10 minutes?  With all the stuff you have to disconnect/rehome/recable? I think for the average home builder closer to an hour.,Neutral
AMD,"Maybe for an experienced builder, but to me (someone who's only built 1 PC) the prospect of just changing the CPU seems like a far less daunting task.  If you just want to replace the CPU, you unscrew 4 screws to remove your cooler/AIO pump head, pop open the socket, put your new CPU in, repaste and remount the cooler, plug some fans back in if necessary, and you're good. Maybe remove the GPU if you're really pressed for space.  When changing an entire motherboard, you have to unmount the cooler, remove the GPU, unplug every cable plugged into the motherboard, possibly remove some AIO fans/the radiator (or just fans in general) from the case if they're getting in the way of the motherboard screws, remove the motherboard, and remove any SSDs you want to reuse.   Then you have to put everything (CPU, SSDs, RAM) on the new motherboard, put the new motherboard in, repaste and remount the cooler, remount any fans you had to take off and plug them back into the motherboard, plug all your PSU cables again, then put your GPU back into place.  You basically have to redo a big chunk of the entire build process. Not to invalidate what you said, maybe you are proficient enough that all that would only take you 10 additional minutes. But I would not bank on the average DIY PC owner being able to do all that so quickly.",Neutral
AMD,"> What's the actual time difference of install of just a CPU vs Mobo + CPU? 10 minutes?  Maybe if your build has basically nothing in it, mine has an AIO I would need to remove, PCI-e cards, GPU, 2 SSDs that would need swapping. I think it's more like an hour.  CPU, you just pop off the AIO, swap it out and put it back on.",Neutral
AMD,>I can't image the failure rate for a brand new motherboard that needs to be RMA'd immediately is higher than the failure rate of a motherboard that's by this point already several years old.  You might be surprised. Reliability of these things can very much follow a bathtub curve,Negative
AMD,>I can't image the failure rate for a brand new motherboard that needs to be RMA'd immediately is higher than the failure rate of a motherboard that's by this point already several years old.  The failure rate for a brand new board is thousands of times higher than the failure rate of a three-year-old board.,Negative
AMD,"QC for PC parts seems to have been rather lax lately, I received a board with multiple damaged pins, and another one with a bad bios, I also got a bad CPU. This was all on the build I did around this time last year. Because of these issues it wasn't up and running till February. I also remember when I was a young teenager I got a bad board from Frys in the late 90s. My friend also received an Asrock board a few months into 2025 that basically melted with stock settings.  For some on compact builds multiple hours. Multiple factors go into this. Swapping a motherboard can be a major PITA. Not to mention when dealing with multiple thousands of dollars of components I have to take breaks to deal with the stress.",Negative
AMD,"5070 has a reasonable price, but it's somewhat limited by 12GB of VRAM. 5070Ti is a great card at a still somewhat reasonable cost and 5080 is probably the worst offender, it's way too expensive for what it is.",Negative
AMD,"Productivity king is pushing it, intel still has the lead in that",Neutral
AMD,"productivity is clearly in Intels court right now. Unless you need AVX512, but the vast majority never will.",Neutral
AMD,"I agree, currently they’re the best in the very same categories they were awful at a decade ago. Productivity can be argued that Intel is still better for some things. Still, in a good number of use-cases, AMD is still fast than intel per watt. Most of the Core Ultras are using a shit ton more power to match AMD’s performance.",Positive
AMD,And are going bankrupt while doing so,Neutral
AMD,The term ‘Is Back’ refers to a positive directional change and not a current status. Their current status is collapsing market share and margins.,Neutral
AMD,"With a bunch of lower end, cheaper chips. They simply aren't competitive.   Here's a quote from the CFO of Intel:   >""As you know, we kind of fumbled the football on the desktop side, particularly the high-performance desktop side. So we're -- as you kind of look at share on a dollar basis versus a unit basis, we don't perform as well, and it's mostly because of this high-end desktop business that we didn't have a good offering this year,""",Negative
AMD,"10 more minutes vs a CPU swap, so not counting that time or cooler installer.   Move RAM over, move GPU over, move storage over. Cables are already routed and sitting right there.    Maybe 10 minutes is optimistic, but I think if its not your first build, an additional hour to swap the Mobo vs just the CPU seems pretty long.",Neutral
AMD,"I wouldn't want to do it that quickly, I like my cables nice and neat and shit.",Negative
AMD,"but ARM is winning in compute per watt, AMD is only a leader in x86 in that category",Neutral
AMD,"I was thinking swap as in same case.  Cable locations vary by mobo, although generally not by a lot.",Neutral
AMD,"ARM isnt winning in compute per watt, Apple design team is winning in compute per watt. Other ARM offerings arent any better than say arrow lake.",Neutral
AMD,Laptop CPU naming scheme is a real mess,Negative
AMD,I LOVE LAPTOP CPU NAMING SCHEMES THAT MAKE 2 GEN OLD CHIPS SOUND MODERN!!!!,Positive
AMD,4 core/4 threads vs 8 cores/12 threads.   AMD really went light on this CPU. Only one full Zen core.,Neutral
AMD,"I don’t understand why they pit them together: sure 10-30% more performance, _at 50% more power consumption_.  Why not comparing it to an AMD with the same power consumption? It may give better performance…  Edit: I also like that they say “Intel consumption is _slightly higher_”, and on the next sentence “…would draw 68W or _almost 50% more_”. Article is clearly written by Intel PR…",Neutral
AMD,Yeah sure not in a similiar configuration.,Neutral
AMD,Don't see any reason to buy anything on intel 7 unless at a large discount.   The intel stuff on tsmc 3nm is much better,Neutral
AMD,So there are two laptops at same price just one has the Ryzen AI 5 330 the other has the Core 5 210H which one is better? I just saw this review of a laptop with the intel [https://www.youtube.com/watch?v=q5OFCLvxA6U](https://www.youtube.com/watch?v=q5OFCLvxA6U)  and it looks very very bad for battery life not even very light gaming.,Negative
AMD,"I would like laptop without AI please, especially CoPilot, that product is hot trash",Negative
AMD,"What a silly comparison, they aren't even close in price... The AMD only has 4 cores, of course it's going to take a huge hit in multithreaded...   Also, I just realised this is a repost from 9 months ago, maybe prices were different then? Why post something so old now though that's not even relevant anymore?",Negative
AMD,The real budget performance winner is an m1 macbook air.,Positive
AMD,"Hello Balance-! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,Yup. Now it seems like Apple is the only company with good CPU names.,Positive
AMD,"Was gonna say price, but looking at Lenovo's website and I couldn't even find the Ryzen AI 5 330 in my country, but the option with Ryzen AI 7 350 is still cheaper than the option with the Intel Core 5 210H.   So this is a really odd choice for comparison.",Negative
AMD,50% higher power consumption for *50% higher performance (in nT benchmarks),Neutral
AMD,"68W is only the peak at the start, otherwise the chip stays at around 50-58W during gaming. Even the AI 5 330 can peak at 58W and 43W in games too according to their review so it isn't that much of a difference either way.",Neutral
AMD,The better comparison should be 210H vs AI240  13420h reskin vs 7640HS reskin  But that would be boring,Neutral
AMD,"And probably on par in consumption, since the node is smaller.  This comparison right here is like comparing an i3 to an i5 and saying the i5 has better performance…",Neutral
AMD,That's Alder Lake. Can't remember anything later on the Intel 7. There used to be some decent 12-14th gen CPU for it.,Neutral
AMD,"Eh, I'm not a fan of the Pro Max Ultra shenanigans.",Neutral
AMD,Well you can configure the cpu and GPU cores to be different and still have the same name so no they're not better,Negative
AMD,"My gripe with Apple is it’s TOO simple. M4, pro, max, ultra.   Doesn’t really say how may CPU or GPU cores, as it varies by device SKU (some got the binned cpu like the air got 1 less gpu core)",Negative
AMD,I see a $100 price difference in the US with the ryzen at $500 and Intel at $600 through Lenovo. However the thing is the core 7 240H model is only $1 more than the core 5 and the ryzen 7 with 860M graphics is only $525. So why bother with either model at that point?,Neutral
AMD,"Where I'm from, 330 is very sparse and for some WILD reason more expensive than similar 350 options.  At the same time, 350 is more expensive than 225h... (Edit: re-checked, 225h isn't cheaper anymore it seems)",Negative
AMD,"Yeah. That’s like comparing an i3 with an i7. An i7 will consume more, and will have better performance. That’s why the comparison is odd to me: why comparing a Core 5 with an AMD that has to be compared with a Core 3?",Neutral
AMD,yeah i've never even heard of the ai 330. Crazy they gave it the 5 specification when it has 4 cores and three of them are C cores with only one full zen 5 core.  [https://imgur.com/a/lMajlf6](https://imgur.com/a/lMajlf6)  generally speaking amd is better efficency wise although intel does make up for it with the lower idle draw and lower draw in low-medium loads. The higher binned hx stuff have better efficency but you have to power limit them bc out of the box they go like 100w+     edit: from what im seeing both intel and amd have given up on the i3/3 naming in the laptop space but I still wouldn't say it is a fair comparision bc u should compare it to the 226v which has the NPU like the ai 330,Neutral
AMD,Go ask why amd names it a ryzen 5 and not a ryzen 3? Hmm?,Neutral
AMD,No the 210h is on intel 7 same with the 240h,Neutral
AMD,"The 210H is ""Raptor"" Lake(Alder, since the P Cores have 1.25MB L2 instead of 2MB like actual Raptor Lake)   12450H -> 13420H -> 210H are the same CPU, with clock speed bumped up with each iteration",Neutral
AMD,Well they always list the core count directly in the full product title on their site,Neutral
AMD,"It's not.   The Core 210 is Intel's budget chip.   Both chips are priced similar. It's a Core **5** vs a Ryzen AI **5**  Both are positioned as ""5"".  Intel adding E cores to boost the nT performance on their products has been giving them the nT lead in the lower end segments for a few generations now",Neutral
AMD,Whatever that has to do with technical specifications…,Neutral
AMD,"That and this comparison also saw the Intel version having better gaming percent, but it is only like a 5-10% difference across the board, with more games favoring Intel and some other favoring AMD",Neutral
AMD,Why did YOU bring up i3 vs i5 then? Huh?,Neutral
AMD,"Because they are both from Intel, I was trying to convey the fact that they are not on the same category. I need to refer to the same family somehow…  Now, saying Ryzen 5, 7, 9, etc. makes only sense within AMD, same for Intel within Intel. But different manufacturers have different nomenclature, and even different ways to measure TDP.  TDP on Intel is true TDP at minimum power, for AMD you can go lower than the TDP they publish for PL1.  So, at same power, AMD is also competitive, just that they have different nomenclature.",Neutral
AMD,"They *are* the same category. The two companies just approach that category differently. Intel decided to have 4 extra E cores to boost nT performance, but at the expense of higher power consumption if you load those cores.  This is the lowest end """"""current"""""" gen CPU H series CPU from Intel.",Negative
AMD,Bought an XT last week cause stock has largely been sorted in the last few months and quite a few models have been under MSRP and that situation could change in the coming months because of the DRAM situation. I also wanted to go AMD since I moved to Linux so I've been pretty pleased. For as much as we complain this isn't really a bad time to buy a gpu.,Positive
AMD,TLDW:     **7 Game Average (Low/Medium):**         1080P:       RX 9070 16GB is:     ~28% faster than the RX 7900 GRE 16GB    ~24% faster than the RX 7800 XT 16GB    ~33% faster than the RX 7700 XT 12GB    ~80% faster than the RX 6700 XT 12GB       ~159% faster than the RX 5700 XT 8GB     1440P:     RX 9070 16GB is ~96% faster than the RX 6700 XT 12GB       4K:    RX 9070 16GB is ~113% faster than the RX 6700 XT 12GB       **7 Game Average (High/Ultra):**         1080P:       RX 9070 16GB is:     ~25% faster than the RX 7900 GRE 16GB    ~30% faster than the RX 7800 XT 16GB     ~60% faster than the RX 7700 XT 12GB       ~100% faster than the RX 6700 XT 12GB       ~160% faster than the RX 5700 XT 8GB,Neutral
AMD,I like these types of videos that compare against previous gens.  It's very useful for potential buyers to see generational gains and determine if it's a justified upgrade,Positive
AMD,I'm happy to see 9070XT is doing well. Having said that my 7900XT Nitro+ ain't going anywhere any time soon.,Positive
AMD,"Got a 9070XT on Black Friday, now that they're so cheap I couldn't resist. Really impressed with it, performance is great, runs pretty cool and Adrenaline is really nice to play around with undervolting and overclocking. Got a real nice undervolt on it.",Positive
AMD,"Adjusting for inflation doesn't really help, a lot of the older AMD cards dropped below MSRP pretty shortly after release (excluding 5700XT). I ended up getting my 6700XT for ~$290 USD.  Also in Australia it's currently possible to get a 9070 for a theoretical $425 USD excluding tax if you are able to use the $60 AUD store credit, $50 AUD gift card (easy since it can be Amazon), $20 USD Steam credit and ~$10 AUD cashback.",Neutral
AMD,It's the best value for money gpu,Positive
AMD,"As a day 1 RX9070 owner, I can say I'm quite happy with it. I had some stability issues in windows early on, though those went away with driver updates and probably would've been completely avoided if I did a clean install, and it does suck when games only support the Nvidia features even though AMD has an alternative. Aside from that, performance has really solid. I play at 1440p, so regardless of the game I'm playing I know I can crank basically all the settings to the max, not bother with any of the upscaling, and still get FPS numbers well above 100.  Also, it does hurt knowing the ""best value GPU"" in 2025 is more expensive than *my entire rig* was back when I started. Not sure how anyone can afford to get into PC building nowadays if they're starting from zero.",Positive
AMD,Best graphics card for linux.,Positive
AMD,"Just bought the XT version of this card for 4K gaming on my TV and it’s an absolute beast.   Loving it, especially with a slight Undervolt to lower to power consumption.",Positive
AMD,"After going from AMD to Nvidia, raw perf doesn’t really mean anything to me anymore. Transformer DLSS is so good it beats AMD at native res.",Positive
AMD,plenty of cards in stock at your local Micro Center...,Neutral
AMD,Bought a 6800 reference card back in December 2020 and just got the 9070 today to replace it. For me the biggest draw was power to performance 220W vs 304W of the XT,Neutral
AMD,"I've been rocking my 6700XT for years, it might be time for an upgrade, holy cow!",Positive
AMD,"Do they talk about the value with ""maintenance mode"" or ""extended support"" at 3.5 years, and without it?",Neutral
AMD,While the raw performance is good I am still concerned about the adoption rate of FSR4 and Redstone.  I'm waiting another gen to see if it catches momentum.,Negative
AMD,Buying the 7900 XTX for $889 a couple years ago was the best decision I made. Card kicks ass,Positive
AMD,Been having driver problem lately with my Rx 9070. :/,Negative
AMD,"For people that havent been following since 2019:  5700XT 2019 launched at $400 (adjusted from $500), rivaled the 2070 super but had bad drivers for at least a year. Overall, underwhelming  6700XT launched at ??? But once available in 2022 it was generally under $430. Good drivers, and value. Overall, great GPU crushed under pressures of pandemic.  7700XT, trash. Dont even wanna talk about it.  7800XT launched $500 late 2023, delivering around 25% more perf than the 6700XT. Overall good launch/drivers.  7900GRE launched at $550, 2024. This GPU was an absolute mess. It launched super late, had weird underclocked memory issue at launch (despite it being around for a year in China already) and was priced too closely to the superior 7900XT.  9070 launched at $700 in 2025. Good GPU but completely crushed by the 5070 by value/availablility. FSR4 launched with limited implementation and heavy RT perf was only about a rtx 4070. Path tracing not really possible.",Negative
AMD,guessing it not worth upgrading from the 7900xt looking at those GRE numbers,Negative
AMD,"Any explanation for why the 7800XT beats the 7900 GRE at 1080P low/medium?  Is it that the greater amount of MCD:s in the 7900 GRE vs 7800 XT puts more pressure on the CPU, which results in a bottleneck on the 7900 GRE? Or is it just more MCDs = more latency = worse performance with high FPS due to latency?",Neutral
AMD,Cannot do sound now but at no point in graphs i saw mention what RT settings they used.,Negative
AMD,"That's basically the same difference in their memory bandwidth, could have saved themselves a lot of testing and just used those stats.",Neutral
AMD,"And with how idiotic MS is liable to get with Windows, with the AI bubble visibly approaching explosion, the better Linux driver sit only makes a good deal better.",Positive
AMD,"Since you've not had it long, let me pass on some unsolicited advice:  Leave the undervolt off when not gaming. You won't eat up any extra wattage or anything, but it will ensure that you don't run into any issues with stability at low power states. Not that you're guaranteed to have issues, just that it sure makes things easier to diagnose when you KNOW it's not your ""OC."" Adrenaline makes it so easy that you might as well.   Additionally, when starting a new game and facing crashing, the absolute first thing you should do is cut your undervolt in half. It's astounding how much variation in stability you'll face with undervolts and different games.   For example, my 9070 XT is stable at -70mv in any synthetic benchmarks I can throw at it. But it wasn't 100% stable in Horizon Forbidden West until I dropped it down to -65mv. Then in Tiny Tina's Wonderlands I had to drop it down to -45mv to avoid the occasional crash.   But if I load up Horizon again, -65mv for 10 hours straight no problem.   Play around and figure it out. It's fun.",Neutral
AMD,The 6700xt was around $350-$400 when the 7800xt released for $600 in my country.,Neutral
AMD,"Yup. Bought my 5700 non-XT well under $300 (MSRP 349). The 6700XT did eventually drop below MSRP, and stayed there for a while. But do remember that it took a long while for that to happen.",Neutral
AMD,6700 XT was going for 900 during 2021. It didnt start getting cheap until LONG after it's release.,Neutral
AMD,"Which is amusing, because at launch it was the worst - clearly positioned to be an upselling device to make the 9070XT look better.",Negative
AMD,"I mean, technically true but boy what a lame way to win. Only 20-25% faster than the 7900gre it replaced. I guess Moore’s Law really is dead",Neutral
AMD,Except for when it isn't: [https://www.youtube.com/watch?v=mPQWoSEYMLs](https://www.youtube.com/watch?v=mPQWoSEYMLs),Neutral
AMD,"Really? The last time I looked, the 5070ti, 9070XT and 5070 are in the top sellers list. not the 9070.",Neutral
AMD,Okay but like how about just the best GPU?,Neutral
AMD,>Not sure how anyone can afford to get into PC building nowadays if they're starting from zero.   Have 9th or 10th gen Intel office PCs flooded the market yet? Those types of desktops used to be good for an low budget gaming / workstation PC.,Negative
AMD,"> Not sure how anyone can afford to get into PC building nowadays if they're starting from zero.  Before the RAM/Storage prices went insane last month, people just accepted that you either build a budget rig (still stronger than a console) or expect to spend >1000 on it. Now though i would suggest agaisnt buying a new build until memory prices get better.",Negative
AMD,When consoles are $600+ what people consider good value changes. Welcome to modern electronics. Hell welcome to modern everything. A carton of eggs is over $5 now,Negative
AMD,"Which is more important then you'd think with the total estrangement from reality down in Redmond nowadays. Even if you're not immediately planning to switch to linux or go dualboot (like me), with how loony toons shit is getting in that operation, being able to as painlessly GTFO of the MS ecosystem as possible on short notice is a capability worth planning for in a build for if and when MS finally does a dealbreaker for you.",Neutral
AMD,"been extremely pleased with my 9600x/9070 system on Linux for at least 8 months or so now.    Only somewhat miss Adrenaline, and setting up fsr4 being slightly less annoying on Windows",Negative
AMD,I had the oppotunity to test out DLSS vs FSR native implementation by developers in same game a few times and DLSS is just so much better. I always use DLSS quality if its an option now. It has better AA than native.,Positive
AMD,It doesn't help that it is now a felony to play games that don't have PT or DLSS.,Negative
AMD,"> 7700XT, trash. Dont even wanna talk about it.  Agree in general but it was on sale multiple times for ~$360 with 2 bundled games, if you wanted or sold those it was a solid buy, arguably the best value card on the market at those discounts. 9060XT 16GB is marginally slower for the same money, albeit with better RT, upscaling and more VRAM.",Negative
AMD,Man what is it with you people and needing to upgrade every damn generation?,Negative
AMD,"Both actually have the same number of active MCDs. On paper, the only spec that's worse on the 7900 is the memory bandwidth due to running the VRAM at lower clocks.",Negative
AMD,"> It's astounding how much variation in stability you'll face with undervolts and different games.    I've  benchmarked with as low as -125 on my 9070    -77 seems to be the limit for Helldivers, and seems to be rock solid for everything else as well so that's where I stay",Positive
AMD,"I bought 12/07/2023 so before the 7800XT release, the 6700XT was quite a bit cheaper after that release, they went down to ~$263 USD before tax but I pulled the trigger early because it came with Starfield which was... let's just say I haven't even bothered trying to play it because the stories and gameplay I saw didn't seem that interesting...",Negative
AMD,"Oh right, I think maybe covid was still affecting things back then, not sure.",Neutral
AMD,"Yep because it's using same die as 9070 xt, it's benefiting from driver performance boost",Positive
AMD,"Ehhh, yes and no. Worst of Radeon, still less of a turkey then that 12 gig 5070.",Negative
AMD,"Getting 30% gen-on-gen in pure raster, with an even bigger uplift in RT and ML workloads on a smaller die is pretty good. To do it at a 40W lower TDP is really impressive. To get those uplifts purely from architectural improvements, with no real node shrink to speak of is completely absurd.  Also there are still very big gains to come with RDNA 5/UDNA.  Next gen will almost certainly be produced on N2P, skipping N3 entirely. It should yield performance uplifts almost as great as Maxwell -> Pascal, assuming AMD/Nvidia don't decide to shrink the dies massively.",Positive
AMD,"Well if we think of it in terms of release in the US it comes across great. 6 months later, same price, less power, +28% is decent",Positive
AMD,Thats the sad reality train we are on now. Pray that we actually even get these when quantum tunnelling gets worse.,Negative
AMD,"Probably more like 40%+ in ray tracing. It is still dead, but  also this architecture is more forward thinking, and spending its transistors on next generation stuff.   I'd imagine RDNA5 will be another big leap, and this will age more poorly in some regardless, compared to the RTX 5000 series from Nvidia. If Nvidia ages worse, it's because of artificial implanted software limits. Like they blocked frame generation on the RTX 3000 series.",Neutral
AMD,"If we knew the actual margins AMD and NV were taking, we'd have a much better idea of the state of Moore's Law. That and the cost of designing more and more complex ICs creates it's own growing cost outside of ""cost of number of transistors per sq mm."" And the more and more complex software environments.",Neutral
AMD,Top seller and value for money are two entirely different and unrelated concepts,Neutral
AMD,"Do you not understand the difference between ""best selling"" and ""best value""?  The 9070 gives you the most fps/$",Neutral
AMD,"Really, where do you see top seller list for all GPU SKUs combined, comparison?",Neutral
AMD,Because of the recent performance boost 9070 is a lot closer to 9070 xt there's only 8-10% performance difference,Neutral
AMD,"That's the 5090, and it's not even close. No one is cross shopping a 5090 and a 9070 though.",Negative
AMD,"Maybe, I haven't checked.  Talking about office PCs, post covid IT spend at companies shifted massively towards laptops, now that work from home / hybrid work is a common thing. Buying a cheap, old office PC will be far harder in a couple years time, simply because supply is lower than it used to be. I doubt we'll see floods of 12th+ gen office desktops anywhere near the scale we saw with 5th/6th gen.",Neutral
AMD,The PS3 (60GB) launched at $599 in 2006.,Neutral
AMD,Eggs is a very specific US issue due to chicken pandemic. A carton of Eggs (10 units) here in eastern europe is 2.15€.,Neutral
AMD,"LOL, its not a crime but if you are looking for the best, its objectively better.",Positive
AMD,"The problem with the 7700XT wasn't necessarily that it was a bad card.  The problem was that the 6800 non-XT offered basically the same performance, basically the same efficiency, and was very often cheaper throughout most of that card's effective life.  I know someone who snagged a 6800 brand new for $330 more than a year into the RDNA3 era. Also, supplies didn't dry up for a **long** time in many/most markets. It was basically a no-brainer. It was an equivalently-priced (or cheaper) 7700XT with more VRAM. There was zero reason to even consider a 7700XT throughout most of its run.",Negative
AMD,"Retroactively, the 6800 has been $340-$380 as well. The 6800XT was $400-450.",Neutral
AMD,It will depend on your wealth level. If GPU is not a meaningful expense for you then performance will be the only question.,Neutral
AMD,"So I guess my theory that it's latency limited would be correct, then? Memory bandwidth likely isn't an issue at 1080P low/medium, but high latency very much could be, especially with the RDNA 3 MCD designs that I assume come with a latency penalty due to the memory bus and compute die not being on the same die..",Neutral
AMD,"-75, you mean? Because AFAIK, it still only works in .05mv increments.  I mean, I've benchmarked as low as -95mv on Steel Nomad. Didn't mean it would pass 2 hours of benchmark loop. And to me, stable requires hours of testing minimum.    If you can get -75mv stable in a handful of games, you're over the 50 percentile line in the silicon lottery.   But I maintain that you'll find a game that doesn't like -75mv at some point. Or maybe not, and other games you could get even more than -75mv. Shrug.",Neutral
AMD,"You bought like 1.5 years after release, that's not exactly ""shortly after release.""",Neutral
AMD,Stop basing your opinions off other people's.   Form your own. Starfield is a great game and offers a fantastic experience.   I got my copy with the 7800xt when it released and I remember it still being quite close in price. Much lower than what it was when I got my 6700xt for $900 because my laptop decided it was time to go to Laptop Heaven,Positive
AMD,"Cyptomining 2: Crypto-bro boogaloo.  Late 2020 through 22 was prime crypto-bro and scalper hell. Even 6700XTs were going for $800 for a while there because you couldn't get anything above a 6700XT at all. They sold out to bots immediately, and some humans. Within minutes of going into stock.   6700XT's were less susceptible because the 192bit mem bus made it less profitable for eth mining assho... folks. But someone, whether it as AIBs, retailers, someone, was still jacking prices sky high on them. Not talking about scalpers, but cards you could buy directly from Newegg.",Negative
AMD,yep the 9070 is just defective 9070XT dies being put to use.,Neutral
AMD,Wafer pricing is going from $16-18K 5nm to 30K 2nm so don't expect the same die sizes.,Negative
AMD,"RDNA 3 was a undercooked bad gen. RDNA 4 fixing all the issues, chopping LLC and Mem phys by a third and going monolithic makes perfect sense. N5 -> N4C helps a little as well.      N2 is highly unlikely while N3P is pretty much confirmed by Kepler\_L2 for GFX13 across all products using it.  Maxwell -> Pascal level perf gains is overly optimistic. Node progress doesn't permit another 30 -> 40 series perf uplift.  But insane gains can be expected in ML and PT over RDNA 4 fs.",Neutral
AMD,"And that's before whatever perf is shaken out of RDNA 4 by Redstone and other software upgrades to come, which may be significant.",Neutral
AMD,"ML is irrelevant. You aren’t getting this card if you want to run LLMs. RT is borderline irrelevant. If you want to run RT in the latest games you will need something much more powerful than a mid range card like this. And lastly it isn’t 30% raster, its 20% in 1440p and 25% in 4k which is pretty disappointing.  The only thing that is pretty good about this card is that everything else is worse. It wins by virtue of being just disappointing in a market flooded with absolute garbage",Negative
AMD,"Fs and I can't wait to hear more about RDNA 5, but unfortunately it seems like it'll be radio silence until well into 2027. Look at how AMD handled RDNA 4 as well, nothing until one week prior to launch. Maybe a Cerny teaser for µarch at GDC 2027 with Road to PS6?  Yeah and FG limiting egregious especially after the new model got rid of OFA entirely. Fingers crossed they unleash it when 60 series launches as a good gesture similar to DLSS TF across all RTX cards.",Negative
AMD,"Are you talking about the 26.6.3 drivers performance boost or is there a newer one recently? If it's really within 10% of the XT, then it makes sense to buy the 9070 yes.",Neutral
AMD,Sure but people were cross-shopping a 4080/90 and a 6900/7900 tier card,Neutral
AMD,The only desktops we have left at my job are incredibly cheap mini pcs or insanely expensive dual quadro towers.  Nothing really in the middle that would be viable for upgrading for home use in the future anymore. :(,Negative
AMD,And the PS4 launched at $399,Neutral
AMD,my best friend just went to leavenworth for playing at native  how dare you,Negative
AMD,exactly right.,Neutral
AMD,"Even if you're rich, you don't need to upgrade every generation. That's just mindless consumerism. And judging from the fact that this guy is even asking the question, then he's not loaded.",Negative
AMD,"> -75, you mean? Because AFAIK, it still only works in .05mv increments.    LACT in linux allows .01, on windows I did have it at .75     Never had it crash on anything at that, did pretty good in the lottery it seems.",Neutral
AMD,"Yeah, I remembered wrong with the 6000 series, I thought they dropped faster (way before I bought but not to this extent), 7000 series dropped pretty fast though.",Negative
AMD,"Dies are not, and have really never been the main cost driver in GPUs.   Dies were absolutely a bit cheaper back in like 2017, but it's not by the huge amount that people believe. These days the die is the most expensive individual piece of a GPU, but not anywhere near the majority of the cost.  The GB203 in a 5070Ti or 5080 costs roughly $100 assuming 95% yields and 18k per wafer (95% viable for 5080 or 5070Ti). A 1080Ti die (GP102) would've cost about $50 adjusted for inflation.   So yes, the dies are more expensive, but they have gone from like 7% of the total price the end user pays for a mid tier to high end GPU, to about 10-13%.   The main cost driver for GPUs is not the dies, but the fact that Nvidia can make so much more money elsewhere, and need to jack up prices in order to justify the continued existence of their consumer GPU business. A wafer used to produce GB203s is a wafer out of Nvidias wafer allocation that can't be used to produce much higher margin GB200s. Every wafer of Nvidia's allocation not used to produce GB200s basically costs Nvidia to lose tens, if not hundreds of thousands of dollars they would have made in additional profits. By raising consumer prices they can somewhat soften that blow, hence why they have continued to go up.  N2 is probably where the wafer costs will actually start being a significant cost driver if Nvidia/AMD want to maintain margins, but we should still only be talking consumer price increases of perhaps 10-15% to maintain those margins. Obviously Nvidia and AMD will shrink the dies and jack up their prices much higher anyway, then shift the blame onto TSMC and their wafer prices. But there is really no reason they need those huge price increases for the move to N2 to make sense financially.  Shrinking the die on lower tier cards may be more justified though, as die costs do actually make up a large component of the manufacturing cost there.",Neutral
AMD,"Yeah, great proof of concept of GPU semiMCM and heralds great things for shit after RDNA 4, but the silicon itself was borked.",Neutral
AMD,"ML is very relevant, and is only becoming more so every day.  You want enough horsepower to  be able to do neural radiance caching, upscaling, frame gen, RR and potentially neural texture compression, all on the fly at the same time. A modern ML feature set with a lot of horsepower becomes very relevant then, as you want to avoid a huge latency penalty for doing all of these things.  Also they do rt well enough as long as you're fine iwth upscaling.",Positive
AMD,"It did Control at 1440p perfectly acceptably. RT isn't the prime determinant yet, but RDNA 4 is pretty competent at it.",Positive
AMD,https://youtu.be/YWUqsqcM4Hs  Watch this,Neutral
AMD,You mean a 4070TI and 7900 cards? These perform about equally.,Neutral
AMD,"The [Tiny / Mini / Micro](https://www.servethehome.com/introducing-project-tinyminimicro-home-lab-revolution/) PCs have their fans because you can cluster them together and use them as compute nodes for a home server setup, but they're in no way good for gaming.  Just to make the situation even more dire, low profile GPUs are mostly dead now. The best half height, slot power card you can buy is an RTX 3050. If you have an 8 pin available, your only modern option is a Gigabyte 5060. Even if you managed to find a middle of the road 12th+ gen office desktop, you're going to struggle buying a GPU to go with it.",Negative
AMD,"You don’t need a gpu if we’re going to go down that road. Worrying about how other people spend money, now that’s a waste.",Negative
AMD,if you're rich youll upgrade for best performance irrelevant of price.  You know what is mindless consumerism? playing videogames.,Neutral
AMD,"Damn, thwarted by the limitations of windows bullshit. Didn't know you could adjust in smaller increments under linux.   Yeah, certainly sounds like you've landed a winner.",Negative
AMD,Control is also a 6 year old game,Neutral
AMD,"Playing video games is not mindless consumerism. Now if you're constantly buying games and not playing them, then yeah.",Neutral
AMD,Just wait  2̶ ̶m̶o̶r̶e̶ ̶w̶e̶e̶k̶s̶ ̶ 2 more years and prices will drop again 🥳,Neutral
AMD,"I bought a 9070 XT just after the RAM price increase was announced. I already missed a good time to buy GPUs in the past, because I didn't react to industry news. But not this time.",Negative
AMD,"Oh boy, I love when things become more expensive!",Positive
AMD,"So, has anyone considered switching to hiking? Definitely  more affordable than gaming these days.",Positive
AMD,"I'm honestly surprised it's so little, although I suppose this reminds us just how little 8 or even 16GB of GDDR6 was costing AMD and Nvidia until very recently.",Negative
AMD,"If this turns out to be true, I would be shocked that AMD raises prices before Nvidia.  Nvidia is addicted to money, and I feel like they will exploit any excuse they could to make more money.",Negative
AMD,Not really anything AMD can do about RAM prices. They were probably priced with low margin to begin with,Negative
AMD,Well the 8GB cards were not good value and arguably AMD and Nvidia can stop making them to use the same VRAM chips for higher margin SKUs,Negative
AMD,I grabbed a 9070 2 weeks ago in off the price hike hype bubbling up,Neutral
AMD,"Hello JohnSteveRom2077! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,And I thought I was an idiot for building my PC 3 months ago and not waiting a few more months for black Friday deals.,Negative
AMD,"yeah snagged prebuilt 9800x3d w/9070 XT and some newegg bundles to build a second, tried for the 499.99 9070xt today that failed lol, bot scalped in less than second lol.   So got the 599.99 asrock challenger.  My 2600x and 1080ti are still playing stuff surprising well at least on lower setting at 1440p, but time for an upgrade and teenager needed something he's been using an old 1660ti but playing mostly older stuff.  ram and vram are not going to get cheaper anytime soon and chip producers have little incentive to increase capacity when they can still rake in profit at higher per unit profit vs. more volume and without the risk of building out capacity.",Neutral
AMD,"Well se them next time, I'll buy a new TV or a 3d printer, no need to give them money.",Neutral
AMD,"I've been watching the prices over the last couple weeks since I was in the market for a 9070 XT. Prices went down over Black Friday/Cyber Monday, but they're now back to pre-BF/CM level.  Maybe, just maybe, there's a hint about the reliability of this source that he claims there'll be no new graphics cards from AMD until 2028?",Neutral
AMD,steam deck is looking better and better every day,Positive
AMD,20-40 dollars right now\*,Neutral
AMD,"20-40 isn’t a lot compared to ram prices. does anyone know if cpu price will increase, am like 98% done with pc and I plan on getting r7 9800x3d with rx 9070 xt gpu i will get in two weeks.",Neutral
AMD,AMD making sure it beats Intel's 1% market.,Positive
AMD,"I love this Nvidia does nothing... AMD we are starting to come into line with peoples price expectations. We are starting to slowly claw back sales from the grips of Nvidia.  AMD Video team ""Can't have this we need to full stupid into the wall. Consumers can't forget we are the under dog and if we eat 20-40 dollar increase that just isn't happening""",Negative
AMD,"Another proof that AMD Radeon also doesn't give a damn about their consumers and GPU market share and will happily just pass the price increase of the components to the consumers instead of taking this as an ""Opportunity"" to gain marketshare because Nvidia is also expected to have price increase on their GPU soon.  Speaking of Nvidia I wonder why they still haven't increased their GPU prices yet? Could it be that they are less affected by this dram shortages? I have heard that GDDR6 had a higher price increase over GDDR7 for this very reason, because there are a lot more products out there that is using GDDR6, compared to the GDDR7, which is what the RTX 50 is using.",Neutral
AMD,Buying the recent dip in prices to below RRP might have proved to be the right call.,Neutral
AMD,"AMD just got their products to MSRP, and now if we go back into rebate part trois - woof!  AMD just can't catch a break - well shouldn't hurt much they only shipping like 15 GPUs per week (hyperbole).",Negative
AMD,"This makes no sense.  AMD supplies the GPU die not the VRAM.  ""Hey AIBs, you are paying more for your gddr, so I better raise the prices of the GPU for reasons"".",Negative
AMD,"Woah.. at this pace, a RTX 5090 may actually be viewed as the card for future-proofing and investment into future price hikes..  Yikes!!!",Neutral
AMD,"I always hated the conversation “should I wait for this?” “Should I wait for the next gen?” No one knows what the future holds, even the companies releasing these products!  I always say just buy what you need when you need it, do your research, and enjoy!  Edit: not throwing any shade to you, just mentioning a related topic that I come across a lot!",Negative
AMD,Yea like finally buying an affordable 6600XT in the year of our Lord 2023 when RDNA3 and Ada are already out.,Positive
AMD,"Yup same, got a 9070 XT the other day for around €500ish. Had to get it for that price, and now I’m not going to be worried about “missing out” just like when the Crypto phase popped in and fucked us over.",Neutral
AMD,Because it is better right?,Positive
AMD,"As the old joke goes, the graphics are amazing but the gameplay sucks.",Negative
AMD,"Went outside, wasnt impressed with the graphics. Lack of DLSS support in 2025 is ridiculous.",Negative
AMD,"I'll just stick to 5+ y/o games, or non first person shooters",Neutral
AMD,"Jogging around local park, assuming your local park is safe can be decent alternative? Maybe riding some cheap bikes also work.",Neutral
AMD,"as far as hobbies go, PC gaming still one of the cheapest. hiking is certainly up there in cost comparisons though.",Positive
AMD,"NVIDIA's rumored (heavy pinch of salt) to be decoupling VRAM from their GPU trays. If that's true, you're going to see a steep price increase as well because the AIBs are going to be fighting with one another to get their memory modules from vendors who aren't keen on expanding GDDR7 production beyond their initial roadmaps.",Negative
AMD,Nvidia has much better margins on gaming GPUs so they can absolutely absorb the costs if they desire.,Positive
AMD,Pretty sure Nvidia doesn't ship memory out to board partners.,Neutral
AMD,Every corporation is addicted to money and stock value. AMD isn't our friend.,Neutral
AMD,AMd didnt have a load of GDDR6 piled up.,Neutral
AMD,nvidia isnt providing dram anymore. its up to the partners to raise prices.,Neutral
AMD,"Nvidia has a FAT margin on GPUs,AMD not as they try to reclaim parts of the market",Neutral
AMD,8GB is likely higher margin than 16GB.,Neutral
AMD,Five days ago someone see $560 9070 XT on microcenter.,Neutral
AMD,"They currently have the #1 spot on Amazons best selling GPU's list. Both in the US/global and in Europe (Amazon.de).  The 9070 XT in particular is very popular with DIY'ers and enthusiasts. Nvidia is doing well in the OEM and laptop markets, but for us DIY'ers and enthusiasts, AMD is offering very competitive products. Still behind Nvidia for sure, but doing better than it used to.  And for people rooting for Intel, I know this is an Intel sub mainly, but if you wish more competition, we shouldn't look at Intel. Intel is only eating at AMD's market share, not Nvidias. AMD is the only credible competition for Nvidia, but buying Intel only weakens the only credible competition, and makes Nvidias dominant position stronger.",Positive
AMD,"Do you seriously think NVidia is not going to raise prices at all?  At this point it is a matter of when, not if.  (unless this all comes crashing down shortly, which is unlikely).  We already know they have postponed the 5000 series ""super"" release due to VRAM availability and cost.  (Their 6000 series workstation/server Blackwell GPUs use 96GB of DDR7 each).  They make a lot more money per RTX 6000 Pro Blackwell than a 5090, so they'll prioritize those and hike consumer prices in order to reduce demand there.",Negative
AMD,"AMD (and Nvidia) care about their data center consumers first because that’s where the money and the high margins are. Every wafer allocation and memory module that’s allocated to a gaming GPU is one less that could have gone into a data center GPU. When the AI bubble pops and data center demand plummets, prices will go down across the board.",Neutral
AMD,Nvidia is just [passing the cost](https://www.tomshardware.com/pc-components/gpus/nvidia-reportedly-no-longer-supplying-vram-to-its-gpu-board-partners-in-response-to-memory-crunch-rumor-claims-vendors-will-only-get-the-die-forced-to-source-memory-on-their-own) to board partners instead of being forthright about it.,Neutral
AMD,Not just Radeon look at their CPUs they dont give a two things about us. Their rather deprecated and sell us expensive piles of scrap while making Apple now seem like a value proposition. Truly preposterous times for us with no proper competition and a lack of market interest due to high prices of everything.,Negative
AMD,They supply both.,Neutral
AMD,"Has pretty much always been the case. What was the longest lasting 1000 series card? The 1080ti. The longest lasting 2000 series card? The 2080ti. The longest lasting 3000 series card? The 3090...    Etc. etc.    They're the fastest card in the stack and also tend to have more vram. They've always been the most future proof, but they also cost and arm and a leg.",Neutral
AMD,[I certainly regret not buying ram a couple months ago like I was considering](https://imgur.com/a/iVsxlWV),Neutral
AMD,There are definitely times where that question can be easily and usefully answered,Neutral
AMD,"In general hardware has aged like open yoghurt out of the fridge.  Sometimes anomalies happen, it's not the first time.  I remember major price hikes and poor performance increases after the 9800pro ( x800 and x1800 sucked, hd series was little better until the hd4000) nvidia sucked after geforce 4 until the 8000 series.   People who bought gpus during the crypto boom at insane prices got fleeced. People who bought a 3060 ti or 3080 right before got lucky, people  who waited for a 4070super also got a better deal.  By the time the prices have started rising its too late.  If a gpu is available at msrp at launch you can rarely find better value later in the gen as prices dont drop anymore until the next gen. So yeah either buy at launch and msrp or wait for the next cycle   Imo if you are on a complete potato gpu now and need a cheap upgrade then second hand 3060ti are very cheap (190 euros) , else a 5070ti is the only thing id consider to have any performance/price right now. Everything else is either way more expensive for little gain, or has too little vram for the price. Or is too small an upgrade for the money over a cheap second hand 3060ti, or has poor second hand pricing.   5060 would be more interesting if the base model came with 12 GB and was priced at 250",Negative
AMD,"I remember 2 to 4 weeks before the 9070xt launch people were telling others to ""just buy a 7900xt"" for $700. I guess they got 4gb of extra VRAM out of it, but 95% will never use it. But I think there is a time to actually just wait, instead of buying $700 product that will be worth $500 in less than a month.",Negative
AMD,"500€? New? God damn thats a steal.  There used to be a general idea for buying hardware, *Just buy it when you need it, there is always the next big thing coming*, which still is in part true, but man with the volatility of the market since Corona Im 100% certain you can time your buys.",Negative
AMD,so you got it for two thirds the price it actually costs?,Neutral
AMD,Gotta redo your skill tree but it can get boring pretty easy without the right setup,Neutral
AMD,r/outside is leaking,Neutral
AMD,"Could be a pretty massive difference. Each AIB fighting to source memory on individual smaller contracts instead of NVIDIA just making one big deal for the entire production run, taking advantage of the scale to get it cheaper.",Neutral
AMD,Sure they are expanding GDDR7.  But for servers mostly.  RTX 6000 Pro Blackwell (96GB GDDR7) is a hot server GPU for inference use cases that aren't just the giant LLMs.  It is much more cost efficient than the B200 HBM offerings for all sorts of use cases that don't need to access too much RAM concurrently.  Several NVidia competitor hopefuls have also gone the GDDR route for inference focused AI accelerators.,Neutral
AMD,They're already sacrificing margins on making consumer GPUs instead of AI GPUs,Neutral
AMD,"There are rumors that Nvidia will no longer be bundling VRAM with their GPUs in the face of rising memory prices, and AIBs will have to source it on their own.  While Nvidia may be ""absorbing costs"" on their FE models which have very limited production, for the vast majority of consumer graphics cards they're going to be passing on costs to the AIBs and thus consumers.   They have absolutely no incentive to absorb costs, anyway. Their entire consumer gaming graphics division is already being run at a massive opportunity cost to their real money maker, datacenter GPUs. It's frankly a miracle that they haven't decided to entirely scrap it yet when the same dies they're selling to consumers for a few grand at most can be sold to AI datacenters for tens of thousands.",Neutral
AMD,"\> absorb the costs if they desire  What makes you think they desire? They have a massive fraction of the market, and from a profit/investor POV raising prices is win/win here unless it somehow risks significant market share loss.",Neutral
AMD,"Agreed.  I’m just sort of surprised they would absorb the cost at all, tbh.",Neutral
AMD,>Nvidia has much better margins on gaming GPUs so they can absolutely absorb the costs if they desire.  Which won't happen. Nvidia doesn't want to be a charity.,Neutral
AMD,"True, and I recommend people get cards based on their needs/desires, never blanket recommend 1 company over the other.  I’m just surprised it’s AMD who might raise prices before Nvidia, even if they can absorb some of the cost.",Negative
AMD,That's only a rumor and has not been confirmed.,Neutral
AMD,"Why are AMD's margins slimmer?  They're manufacturing at the same place using very similar technology. Their equivalent product is actually a little smaller than Nvidia's in terms of die size... they're a big TSMC customer, while obviously not as big as Nvidia they're not some small timer who can't negotiate a favourable deal either.  On this basis I would assume AMD and Nvidia have very similar margins on GPU sales to board partners.",Neutral
AMD,then why are AMD cards more expensive?,Neutral
AMD,Still seeing $579.,Neutral
AMD,"When you've been there for 20 years and kept losing market share in the last decade continuously, you are in deep problem. But for AMD, right now they prefer being the SOC for gaming and have little interest in putting effort in dGPUs.",Negative
AMD,"I doubt Nvidia will raise prices they will most likely just stop producing FE""s. Which isn't the same as eating the cost or raising it.   Then AIB's will most likely raise pricing. That isn't due to Nvidia thats AIB's doing what they gotta do.   As far as priority gaming has never been the priority for Nvidia this generation. There priority is AI chips. Everything else is secondary to that.   AMD CPU division is basically printing money now. They want there GPU division to go anywhere. There going to need to be more competitive, right now there not.   They don't offer a better software stack, Ray tracing or anything to match the 5080 in performance. They also have a reputation of dropping support a LOT sooner then Nvidia  That means they need to become a value leader and why in the world would you pay the same or more for a 9700xt over a 5070 Ti?   Specially if the two are with in 50 dollars of each other or even 100 dollars of each other.",Neutral
AMD,"Whelp if that rumors become the truth, then it is definitely right to expect the Nvidia GPUs will also get a price increase, i just wonder when it will happen and why AMD did it first with their RDNA 4...",Neutral
AMD,"Usually it's best not to buy during the post-launch rush when prices are way inflated, unless one can get one of the few actual MSRP offers.  Other than that, I totally agree that trying to predict the future is largely futile. Especially now that generational improvements have become so slow and low. Hardware holds its value for longer than ever before and the risk of buying a product only to see it depreciate it soon after is therefore also low.  People who bought any decently reviewed GPU at MSRP since the RTX 20-series have gotten pretty good value by now.",Negative
AMD,"9070 xt shouldn't be too bad either, considering it can be had for less than $/€600. Unless you're a big fan of AI upscaling.  Waiting for delivery now, I guess the shop has a bit of backlog after BF/CM.",Positive
AMD,Either way the days of the affordable 200-300$ value king are long gone.  Anomalies like the Nvidia 8800GT or the 1080 Ti will never happen ever again.,Negative
AMD,>95% will never use it  And the other 5% could use 4 or 100? Or is the XTX too cheap for that?,Neutral
AMD,"Sure, but how often can you predict something will drop that much? When the 5090 was announced didn't people sell their 4090s for as much as they bought them or slightly more? Also, if I'm going to use a GPU for 3 months, I'm not going to wait for it to drop in price.",Neutral
AMD,"I like to buy every other year now, while my current GPU still holds some value.   GTX 970 -> 980 Ti -> 1080 -> RTX 2070 Super -> 3080 -> RX 9070 XT",Positive
AMD,Brand new in the EU I’ve seen them go brand new for as little as €609 actually. He had it listed for €540 but I managed to haggle it down to €500. So roughly €100-€110 off MSRP?,Neutral
AMD,and proper setups cost quite a bit too.,Neutral
AMD,"Also means there's probably going to be a lot more variability in performance from vendor to vendor than we're used to, due to the sourcing of VRAM potentially being from different places.",Negative
AMD,They have 92% of the gaming sector. It's easy money and a failsafe if the AI bubble goes bust.,Neutral
AMD,Nvidia is also likely in a position where can get better prices on memory than AMD. So they might not be facing reduced margins yet.,Neutral
AMD,Im surprised that people even consider AMD its not priced well against Nvidia and Intel is coming hot on its heels. Their attitude towards consumers is frankly appalling considering that they are just fighting for scraps in this space,Negative
AMD,"Why is it ok to talk about AMD rumors but not NVidia ones?  The seemingly official quote in this article is clearly at least partially wrong:  It states no new products through 2027.  Do you really believe AMD won't be producing any new GPU SKUs for over 2 years?  I think in both cases these rumors are likely true, but may be a bit off given that both are sloppy third hand leaks without any confirmation from the source or quality multiple sources.",Negative
AMD,A 9070 XT's die is almost as large as a 5080 but are selling it for far less. Also nvidia is doubt able to just get a better price on components.,Neutral
AMD,"They have far lower market share, they most likely price their stuff with a lower margin. Nvidia can do whatever they want.",Neutral
AMD,"Nvidia has a 378 mm2 GPU that has a MSRP of $1000 and $750, while AMD for the same die size has $550 and $600 MSRP. In this case, nvidia makes between 50-80% more $$$ per die.  Nvidia has a 260 mm2 GPU has that a MSRP of $550, while AMD has a 200 mm2 for $300 and $350.  And so on....  A wafer of chips produced at TSMC generates nvidia much more money that AMD's one, even though the prices per wafer must be similar.",Neutral
AMD,"The 9070 XT is priced closer to an RTX 5070 right now, performs like a 5070 Ti but is much closer in terms of hardware to an RTX 5080. So while AMD does use similar manufacturing, they have to price lower than Nvidia for roughly the same hardware (similar GPU die size, 256-bit VRAM bus with 16GB of VRAM).",Neutral
AMD,RDN4 is on a more expensive and smaller node.,Neutral
AMD,"Their market share just went up, not down. But it's not where it should be, I'm just happy they have success in the DIY/enthusiast market, since that's where I am. I don't care much about the OEM or laptop markets.",Negative
AMD,Nvidia has about $50 billion more cash on hand to stretch it out and not be a first mover if they wish.,Neutral
AMD,"Apple has insanely good CPU hardware. But they have zero value, because it comes tied to a mac, so thats an automatic nonstarter for most.",Negative
AMD,"Macbook Air M4 is $750 most of the time now and there's no Windows laptop at the price that can compete on build quality, battery and experience, if you don't plan on doing any laptop gaming.",Negative
AMD,Unless the AI bubble pops and chipmakers find themselves sitting on a huge pile of chips that they can't sell.,Neutral
AMD,"The usual trick is to list it as a business expense and claim the VAT back, but it wouldn't knock more than 5% off the price, else there'd be no profit for the seller.  It was either used or fallen off a truck.  Legit brand new was €589 in Germany around BF and CM.",Neutral
AMD,>It's easy money and a failsafe ~~if~~ when the AI bubble goes bust.  FTFY,Neutral
AMD,Google’s TPUs are already showing a lot of promise and its been used to train the best AI model currently,Positive
AMD,I switched to AMD mainly because of Nvidias shit drivers on Linux. Which is funny because I had previously switched to Nvidia because of AMDs shit drivers on Windows.,Negative
AMD,"Not everyone lives in the USA.  In my country it's priced competitively with a 150€ price difference to the 5070 Ti.  If you don't care about the RT difference and CUDA, going with AMD is a no brainer",Neutral
AMD,"The 5080 die is the same one as the 5070 ti, which is the direct competitor of the 9070xt and sells more than the 5080. Average margins will be slightly higher for Nvidia, but not by much. Especially considering AMD uses GDDR6.",Neutral
AMD,GDDR6 and GDDR7 do not have the same cost.,Neutral
AMD,"Ah, I was comparing 9070XT to 5070Ti.  They're all remarkably small dies but I guess that's progress.",Neutral
AMD,"Certainly NVidia has notably higher margins here, but you're likely over estimating it somewhat.  GDDR6 and GDDR7 do not have the same cost.   The overall board cost is also influenced by cooling and complexity of the board traces / power circuitry (though, not by that much).    Additionally, Its not clear how much of the actual sale (vs MSRP) NVIdia earns vs AMD -- there are various people making profit between them and the consumer.  We also know that NVidia sells the vast majority of its gaming GPUs not directly to consumers but to OEMs for pre-builds.  We don't know if NVidia sells GPUs to those OEMs at the same price they sell to D2C retail brands.  Do the big OEM system builders (NVidia's largest customers) get bulk discounts others do not?",Neutral
AMD,Oh they're about £100 or more different in price where I am.,Neutral
AMD,Yearly it went down.,Neutral
AMD,"I wouldn’t put it past people doing that but there are plenty of used 9070 XT’s in Europe selling for under €550 on some sites, just seen one sell for €530 a few hours ago and another for €550. Not uncommon.",Neutral
AMD,outside of US AMD is way too expensive to even consider for a GPU.,Negative
AMD,I dont also live in the USA. 150 pricce difference with no guarantee when they will cut first day game driver updates. Please id rather spend that extra 150 and get better support lol.,Negative
AMD,">GDDR6 and GDDR7 do not have the same cost. The overall board cost is also influenced by cooling and complexity of the board traces / power circuitry (though, not by that much).  Hope you're not trying to say that GDDR7 is responsible for the increase of $400 of board costs. Come on! A little research showed that there's just a difference of $2-3 per GB, which comes around **$30-50 for 16GB**. Please don't try to say the PCB circuitry is $300 because PCB's is one of the cheapest components of a GPU.  LE: You say the costs are influenced by cooling. FYI, the TDP increase for 5080 vs 9070 is just 45W (350W vs 304W). That's $10 **max** in cooler costs if you've been closely following the BOM cots for the industry.",Neutral
AMD,"That doesn't mean AMD isn't selling well, it can mean that Nvidia happens to sell even better. But mostly in the OEM and laptop market, not the DIY and enthusiast market that interest us.  The 9070 XT is the best selling video card on Amazon in the US and in Europe. And the best selling card on Mindfactory and various price aggregators. So it's actually looking pretty good for AMD, the 9000 series is a pretty big success.",Positive
AMD,I literally listed an European country where it is cheaper and the other guy did too while being way cheaper,Neutral
AMD,Cheapest RX 9070 XT in my Amazon (Ireland) is 702eur  Cheapest RTX 5070 Ti is 963eur (+261eur),Neutral
AMD,Not sure how that can be considering neither AMD nor Nvidia has publicized their contracts and we know they arent buying GDDR off the market,Neutral
AMD,Either your country is an anomaly in pricing or you got things wrong.,Negative
AMD,"Tip: some German shops ship to Ireland.  [https://www.alternate.de/listing.xhtml?q=\*&filterCategoryPath=PC-Komponenten%2FGrafikkarten%2FAMD+Grafikkarten%2FRX+9070+XT&s=price\_asc](https://www.alternate.de/listing.xhtml?q=*&filterCategoryPath=PC-Komponenten%2FGrafikkarten%2FAMD+Grafikkarten%2FRX+9070+XT&s=price_asc)  Add €18.90 for shipping and maybe €20 for VAT adjustment, it's still €628. Quick, it's almost sold out.",Neutral
AMD,"No, my country is normal like a lot of others",Neutral
AMD,"That listing is gone, so now the cheapest from there is    €634 ->**€655.31 + p&p**",Neutral
AMD,Check other retailers. There are multiple storss in Germany selling 9070XT's for 600€,Neutral
AMD,"Bad luck, it was there an hour ago.  Here's this one, but they'd deliver in January  [https://www.amazon.de/dp/B0DTT7CPWV?linkCode=xm2&camp=2025&creative=165953&smid=A3JWKAKR8XB7XF&creativeASIN=B0DTT7CPWV&tag=geizhalspre03-21&language=en\_US](https://www.amazon.de/dp/B0DTT7CPWV?linkCode=xm2&camp=2025&creative=165953&smid=A3JWKAKR8XB7XF&creativeASIN=B0DTT7CPWV&tag=geizhalspre03-21&language=en_US)",Neutral
AMD,"I like that they used the term ""Machine Learning"" instead the infamous two letter acronym, starting with ""A""",Neutral
AMD,"Now that you mention it, I also observe that in their RDNA slides they barely mentioned AI at all, just 1 or 2 slides and that's it: https://www.techpowerup.com/review/amd-radeon-rx-9070-series-technical-deep-dive/  But their Ryzen presentation is complete other way around, just AI AI AI all the way.",Neutral
AMD,A.R. may be a little gimicky but I wouldn't call it infamous.,Neutral
AMD,No reason to buy a new cpu when you have to sell a kidney for ddr5 ram,Negative
AMD,I've been holding onto my used ddr4 for no reason for the last 2 years. Turns out maybe being lazy and waiting actually benefited me.,Negative
AMD,If the demand for these in data centers was actually there wouldn't the prices have raised already?,Neutral
AMD,I don't believe they'll keep that promise because their sales will drop when the memory situation impacts all their sales channels.,Negative
AMD,Can we just use banks of CPUs as RAM?   It would only take 333 copies of a 7800x3d to make its 3D cache add up to 32GB!,Neutral
AMD,Honestly just relieved AMD didn’t bump the prices. I can finally upgrade without crying.,Positive
AMD,"None of this is helping anyone. All it's proving is that customers must buy on initial release date, as the future could skew product pricing at a later time. Which doesn't help anyone and encourages scalping even more.",Negative
AMD,"They never were going to. I told you so. The source that said they would, also said AMD is not going to release a new GPU until 2028.",Negative
AMD,"Hello kikimaru024! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,"Good to see AMD hasn't forgotten its roots. Also, Zen6 is probably going to come in higher",Positive
AMD,they better not. if they get greedy and start raising prices just because they can then I'm done with them.,Negative
AMD,I was born with two for a reason.,Neutral
AMD,\+300 euros for a 32 GB Ram kit.  Absolutely insane,Negative
AMD,I'm so glad I bought 64gb of ddr5 for £170 right the price hike.,Positive
AMD,Unless you have socket compatibility... Intel better get with the program here.,Neutral
AMD,"Sure DDR5 went up, but is a couple hundred more really gonna make or break say a $1500 computer that now costs $1700?  I think people are way overthinking this or worrying way too much about it.",Negative
AMD,For the past 1.5y I couldnt get rid of my two Aliexpress Corsair 16gb sticks that i bought for $30 a piece but were not a kit.   Nobody wanted single 16gb sticks - they all wanted 32gb kits and who can blame them when they were as low as $79.  Now I got $230 in total for them.  Crazy world.,Negative
AMD,I built a media server around my old ddr4 and boy am I fucking glad I did,Positive
AMD,sir this is a casino,Neutral
AMD,"yes, but it's also about manufacturing capacity. Data centers aren't using consumer grade ram, but they have booked nearly all the capacity at the fabs that manufacture consumer grade ram.",Neutral
AMD,"The DRAM cartel sees the bubble coming and wants to make huge profits by making the bubble bigger. They are going to scam these companies (other than OpenAI) by refusing to increase production and charging insane prices even though they could easily (and were originally planning) to raise production and keep prices reasonable. That was when these scumbag ""AI"" companies weren't trying to corner the market to screw over their competitors  OpenAI's Sam Altman is the one who started all of this. If he can buy up 40% of the DRAM supply he thinks he can stop his competition from being able to source ram at reasonable prices. The fact that everyone else on earth gets fucked over doesn't matter to him at all. The DRAM manufacturers are playing along because they'll get insane profits. When the bubble finally bursts they will go back to normal operations and prices will crash but they think they'll walk away rich first",Negative
AMD,"There's already separate CPUs specifically for HPC / HBM / whatever feature is required for server style ai computing, so idk why there's worry for regular CPUs, especially when they can be very inefficient for server computations.",Negative
AMD,That is the good news about chiplets. Server CPUs will just absorb the slack.,Positive
AMD,Even less if you use 9950X3D2's. Probably cheaper too.,Neutral
AMD,"I bought a 7800X3D (tray version), 2 years after launch for 50% of MSRP so not sure what your argument is?",Neutral
AMD,People have needed to buy on release date for 7+ years now. People got ass mad when I was saying those about the 2080 Ti because the premium models always have more limited availability.   The best time to buy is almost always at launch now.,Neutral
AMD,Buy on initial release date? When companies release said like raptor lake? No thanks.,Negative
AMD,"I think right now you buy a ""justifiable"" MSRP day 1 or wait for the end of the line platform to build and buy around.",Neutral
AMD,"Indeed, false reporting of price increases that don't exist do not help anyone.",Negative
AMD,"Roots? You people have the memory of a goldfish.  AMD does the same shit as everyone else, charge the most the market will bear.",Negative
AMD,"It's not about roots. The PC market makes up a huge portion of both Intel and AMD's sales in a way that it doesn't for Nvidia, Micron, or Samsung.",Neutral
AMD,"They happily would if the demand were there. We've seen that across their GPUs.. they're not being benevolent, they're just spinning.",Negative
AMD,"I seriously hate this argument.  If you had a chance to get a 200%+ higher paying job, would you? Hypocrites.",Negative
AMD,Roots of charging 300+ dollars for a ryzen 5 with 6 cores since zen 3?,Neutral
AMD,"So you were done with them since 2004, right?",Neutral
AMD,How many ram sticks you would like to buy? /s,Neutral
AMD,"Intel doesn’t change RAM with every socket either, so socket compatibility isn’t really required to keep your RAM",Neutral
AMD,A couple hundred? The exact kit I'm using now cost a little under $300. Now it's $1100.  Don't downplay this shit.,Negative
AMD,"I had to settle for 32GB DDR5 at the start of November at $259 AUD, that same kit today is $739 AUD.  At todays prices, RAM alone would have cost more than the rest of my build and I wouldn't have been able to justify it.",Neutral
AMD,Not everbody got rich parents bro,Neutral
AMD,you're talking to the same people who contemplate whether it's a good idea to save $50 by not buying an nvidia gpu,Negative
AMD,Same here for my homelab.. 64gb for ~£70. Wild to think how much money I have in ram just in my room alone,Neutral
AMD,"Isn't the chip is the same ?   They have one more for ECC (on DIMM, not the on chip bullshit) and a controller (maybe for RDIMM ? )",Neutral
AMD,"yea they are scooping up all the LPDDR too, we are so fucked lol",Negative
AMD,"Problem as with the Crypto it takes longer to prices to go down than prices to shoot up. I mean it took ages for GPU pricing to get back to ""normal"" and arguably never did. RTX 40 had a huge markup with the 4080 being a 3070 successor with a massive markup.",Negative
AMD,"I mean, can you say that isn't the right call.  if everyone knows its a bubble, why would you start making new capacity that in a few years will at best ride the end of the bubble, if not completely burst by then.  so yeah, doubly smart",Neutral
AMD,"Fabs take 3 to 5 years to make     They arent increaseing production becose they cant without the wait, they bet that the buble would pop before they could payoff their new fabs",Negative
AMD,"There's an insane doomerism bandwagon going on over hw spaces on  reddit, for a while now. From bad reporting to bad reading skills, people are doubling down on the worst possible interpretation and outcomes as guaranteed truth.",Negative
AMD,* RTX 50 series are cheaper now than at launch * DDR5 was much cheaper around mid-2023 to mid-2025 than it was at launch in 2021,Neutral
AMD,"I genuinely can’t believe the conclusions (more like delusions) people come to on this subreddit, like AMD has been the good guy any time in the past several years.",Negative
AMD,Their price increases are due to the memory increases.,Neutral
AMD,"> If you had a chance to get a 200%+ higher paying job, would you?  depends on a job. I could do that right now, but i know too many people in that job that end up with permanent disability before 40 from job requirements.",Neutral
AMD,"The argument I'm making is that instead of raising prices, AMD is not cutting them and Zen 6 will come in higher.   Not sure what you're rambling about",Neutral
AMD,"More like [$195](https://pcpartpicker.com/product/4r4Zxr/amd-ryzen-5-9600x-39-ghz-6-core-processor-100-100001405wof), the launch prices are just inflated.",Neutral
AMD,1 kidney for 1 ram is only fair,Neutral
AMD,I'd settle for a chip.,Neutral
AMD,Rich kids got not sense for prices under a couple grands; therefore they can't understand the problem since their parents will buy them everything to make them shut up,Negative
AMD,the exact kit you are using is irrelevant. A typical 32GB of DDR5 can be got for 300 dollars now. Only a few extremists need the best frequencies and timings.,Positive
AMD,You can definitely find a kit under 500,Neutral
AMD,"You don't need to look at the exact same kit...  Look for a current kit with similar specs...  Most people don't need more than 32GB (and even that's more than most really need), and that was $100 and now it's about $300 for a similar kit.",Neutral
AMD,a 300 dollar kit im assuming is 96gb  Most people dont need that much   Motherboard prices are coming down a lot to offset some of the increase in ram price,Neutral
AMD,Are these the same people who spend a bunch extra on RGB lighting and noctua or maglev fans then too? lol,Negative
AMD,I suddenly feel rich with my 7900XT.,Neutral
AMD,"For server DDR5 yes, for HBM no.",Neutral
AMD,"I admit that I don't know the difference between ECC and non ECC DDR5 memory chips.    I assumed that it was the case since that's how it goes over at TSMC for logic chips. I don't have time to look into it now. If that assumption was wrong, then I look dumb and I'll ""take the L"".",Negative
AMD,Part of that markup is from the die area getting more expensive. The other part is Nvidia further increasing margins and implementing more expensive memory for what is arguably a net negative. The extra bandwidth or some ten watts of power saving does basically nothing on most of the stack and instead you get gimped amounts of memory at higher cost.,Negative
AMD,"OTOH, SSD prices did take a sharp dive for a glorious moment.",Neutral
AMD,new nodes are more expensive than old nodes nowadays. So prices for GPUs will keep increasing just from basic manufacturing costs.,Negative
AMD,"Its not just HW spaces, reddit is very doomer outlook in general.",Negative
AMD,"""Forgotten its roots.""",Neutral
AMD,Dual channel and dialysis it is then.,Neutral
AMD,Under $500 is still absurd for 64 GB of DDR5.,Neutral
AMD,Even that's a three fold increase. You get low end 32 GB kits for $300 whereas for the same price I got a Corsair Dominator Titanium 64 GB kit.  The market is fucked and it's entirely the fault of the corporate AI fad.,Negative
AMD,Nope it's a 64 GB Corsair Dominator Titanium kit.,Neutral
AMD,My 64GB kit went from $135 at the start of this year to $780 now.  https://old.reddit.com/r/buildapcsales/comments/1i21ta2/ram_patriot_memory_viper_venom_64gb_2_x_32gb/  https://www.newegg.com/patriot-memory-viper-venom-64gb-ddr5-6400-cas-latency-cl32-desktop-memory-matte-black/p/N82E16820225335?Item=N82E16820225335,Neutral
AMD,Fun fact: RGB fans are so popular now that they are usually cheaper than non-RGB ones. So you can buy the cheaper RGB variant and just turn the lights off.,Positive
AMD,"No ""L"" to be taken, it's greed from manufacturer any way \^\^",Negative
AMD,"True but AFAIK Nvidia could've sold a full AD103 die with a cheaper board and cooler at 300W priced at 800 USD. But Nvidia slipped a pricey cooler, board, pushed wattage and charged 400 more for that.  Crypto IMV really did a number because they thought they could get away with it. Ofc the OG 4080 sold poorly and never sold out.",Negative
AMD,"Yeah, 3x increase for 1 part, one of the cheaper parts, making a $1500 PC using a typical 32GB kit be about 13% more expensive now than before. Hardly breaking the bank IMO.  I am curious how much difference in spec this kit for example is vs the one you got for $300 for 64GB before.  https://www.amazon.com/gp/product/B0BPHSVVS5/ref=ox_sc_act_title_1?smid=AWD7GDDT7Q2ZT&th=1  You are claiming 64GB is $1100, but this is less than half that price for 64GB 6400 CL34 which seems decent enough to me. I wonder what are the specs of your $1100 64GB ram then that it's that much better.",Neutral
AMD,No one is paying 800 dollars for that   The ask price is not the true market price  There are klevv sticks on amazon that is more reasonable,Negative
AMD,im seeing equivalent specs for under 600 now.,Neutral
AMD,"And yet despite selling poorly as you claim, the 4080 sold more than the entire RDNA3 lineup combined. So i guess it offered something enough people wanted.",Negative
AMD,"This is just the start of the price hikes as well. It's going to keep getting worse until at least 2028.  Plus it's not just system RAM. It's going to affect VRAM too, as well as consoles, phones etc.",Negative
AMD,Why not RX 9060 XT?,Neutral
AMD,5060 due to wattage and features.,Neutral
AMD,Why the fuck is it between the 5060 and 6800?,Negative
AMD,"If he's 1080p, you can still get away with 8GB well on 99.9% of games. Even at 1440p with upscaling only a few you might have to drop a couple settings i.e. ultra -> very high (mainly textures). I personally had a ton of driver related problems on my RX 6000 series AMD card over time, so I'd sooner go for Nvidia or possibly a newer AMD card instead.",Neutral
AMD,"that build no matter a new gpu is quite ""old"" granted probably will still hold up. best to switch if possible to a Am5 socket ryzen at least 7 series cpu. ddr5 recommend 32 gb fuckign windows 11 is chunking my 32gb ddr5 ram under minimal load as is. and i've ran debloat scripts turned off all auto start disabled fast boot i got xmp enabled a slight GPU OC like i got 7 tabs open in browser and discord on the side monitor damn near 35 percent my memory utilized at the moment. although the b650 platform was announced discontinued. well they brought it back. and now it would be a super viable option for your buddy if you have microcenter in your area i highly recommend seeing whats in the holiday bundles atm",Neutral
AMD,what about a 9060xt?,Neutral
AMD,"The 6800 isn't a great choice, since it's 2 generation old and needs at least a 600w psu.  I also wouldn't get a gpu with less than 12gb of ram, even if he only plays in 1080p.  If he's willing to search for it and spend a bit more he could look for a 9060 xt 16gb, it would work with his psu as it only has a tdp of 160w and it's also pretty good in 1440p.  He could look for a used 4070/4070 super, it would work pretty well.",Negative
AMD,I'd guess price,Neutral
AMD,"He suggested the 5060, and I suggested the 6800. It's not between the two, I'm asking if my recommendation is correct and if there's a better fit.",Neutral
AMD,"Yeah, he's running games at 1080p currently. I've heard about the RDNA2 driver issues, I was mainly suggesting the 16GB card because he'll need a CPU and RAM upgrade in a couple of years, and I wasn't sure 8GB would be a good recommendation for a 3+ year card.",Neutral
AMD,Shouldn't it be same or cheaper than a 5060? XD,Neutral
AMD,it should be between  1. 3070 Ti   2. 9060 XT   3. B580   4. 5060   5. 6750 XT,Neutral
AMD,"8gb version is, fair enough. And it does get at least the x16 pcie connector so less of an impact from going over vram. Might be one of the rare cases where the 8gb version makes sense",Neutral
AMD,"These are def more balanced options but the things that sucks is that this is strictly from a gpu standpoint, and having had my own issues with the 9060xt while using a 3700x, id say this removes at least the b580 and 9060xt. It might remove the 5060 too but i dunno how it handles with older amd hardware. I myself ended up upgrading to a 5800XT cause the 3700X couldnt quite keep up.  In this persons case, id get a 6750xt, maybe a 7700xt. Maybe.",Negative
AMD,"I don’t even really see 3070ti for reasonable prices, they are mostly going for like $600+ due to being discontinued based on what I’m seeing, you may want to update your list. You are probably better off buying a used 3080, though I’d generally not recommend buying 2 gen old hardware to better be able to take advantage of new features, efficiency improvements, etc.",Negative
AMD,What kinds of issues have you had with the 9060xt? He definitely can't upgrade both his CPU and GPU currently.,Negative
AMD,7700XT is pretty pricey,Neutral
AMD,who the fuck is selling a 3070 Ti for 600 dollars??? What the hell are you talking about? They go for sub 300,Negative
AMD,"For basic use, nothing. For gaming, i'd get screen tears, the occasional stuck frame in games with a lot of objects (survivor games or really heavy areas in elden ring) and I'm fos games like fortnite and marvel rivals, i'd get occasional latency issues.  Tbf, these went to the point where these games were unplayable. This wasn't like trying to run a 4090 with my 3700X. It wasn't a horrific imbalance, but it was enough that i noticed it and didn't like it.  Realistically if you want to future proof your friend pic, maybe do get a 9060xt first and explain to em down the line they may wanna get a 5600XT for like $150.  One more thing of note: i will say now that i game in 1080p. Gaming in 1080 puts more pressure on the cpu than the gpu. If your friend games in 1440p, the balance shifts more to the gpu, so in that scenario, their 3600X may be ok.",Neutral
AMD,"New, yeah. But ive started to see em on the used market for less than $300.",Neutral
AMD,"Now I’m seeing used ones for those prices, not sure why it didn’t show up a minute ago, but new ones I’m finding prices are cranked I guess due to being discontinued for so long. Bit of a compromise buying used with no warranty though. Whatever the case though I don’t think a 2 gen card is a good recommendation. A new 5060ti is faster and has a warranty for like $60 more vs a used 3070ti.   https://www.google.com/search?q=3070ti&sca_esv=709dfd0b212bea1b&rlz=1CDGOYI_enUS1023US1023&hl=en-US&biw=428&bih=751&udm=28&shopmd=1&ei=b95GacP0Lub9ptQP9JH2wQQ&oq=3070ti&gs_lp=Ehxtb2JpbGUtZ3dzLW1vZGVsZXNzLXNob3BwaW5nIgYzMDcwdGkyBRAAGIAEMgUQABiABDIFEAAYgAQyBRAAGIAEMgcQABiABBgKMgUQABiABDIHEAAYgAQYCjIHEAAYgAQYCkjeRlC2CFinQXAAeACQAQGYAW2gAYQFqgEDNi4yuAEDyAEA-AEBmAIHoAKpBMICDRAAGIAEGLADGEMYigXCAggQABiABBiwA8ICChAAGIAEGEMYigWYAwCIBgGQBgiSBwM1LjKgB8IesgcDNS4yuAepBMIHAzktN8gH0aoUgAgA&sclient=mobile-gws-modeless-shopping",Neutral
AMD,"He understands that it's either a CPU or GPU upgrade right now, and he's currently GPU bottlenecked in games so a CPU upgrade is secondary. He does play Marvel Rivals, though. Depending on how the RAM shortage plays out, he might end up going to a Ryzen 7600x in a year or two. The future proof way with a 16GB card was what I had in mind, it would suck if he got two years down the line and found that he needed both a CPU and a GPU.",Neutral
AMD,"I ain't really be seeing that, all I ever see is like $320",Neutral
AMD,"so they don't make 30 series cards anymore, so if you're buying one new, ofc it will be expensive, that's just supply  but, the 5060 Ti is BARELY faster and ""$60 more"" on budget builds is a LOT of money and it is not ""$60 more"", you can get a 3070 Ti for like $250, a 5060 Ti 8GB is $335, that is $85 more",Negative
AMD,"In general the 9060XT is a little weak compared to the monster 7800X3D. Normally I would recommend putting more money towards your GPU, even if you get a weaker CPU.  That said, with bundles it might be a good deal for the 7800X3D and its understandable if you then don't have the extra budget for something like a 9070XT.  What resolution are you planning to play at?",Neutral
AMD,I would probably be playing at 1080p but for games like hunter call of the wild most likely more. I have never own a pc yet so I don't know too much. This is all I know from research,Neutral
AMD,I saw it was a really good deal on the bundle and only have around a 1500 budget and I still need a monitor,Positive
AMD,"It is a decent deal considering the RAM.  It's not a super unreasonable deal, and leaves you with the option of upgrading your GPU in the future because the 7800X3D will last a very long time.  The 9060XT 16GB can do 1440p if you don't care about super high framerates, and if you use FSR4 it will go even higher. 1080p is also valid and that build will rip 1080p high quality, but many people prefer 1440p because its quite the upgrade over 1080p.   You could also opt for a Thermalright peerless assassin CPU cooler and save some money not buying an AIO, it's not needed for the 7800X3D.  Normally I would recommend something like a 9700X or 7700X instead of a 7800X3D and use the extra money to get a 9070 or 9070XT. If you can't find any good bundle deals however, your build is quite reasonable.",Positive
AMD,"There is a PSU $30 cheaper offering pretty much the same specs from a respectable brand [here](https://www.amazon.com/dp/B0C1JKHPNH?tag=pcpapi-20&linkCode=ogi&th=1&psc=1). I think a high end air cooler like [this dual tower](https://www.amazon.com/dp/B0DB861CFX?tag=pcpapi-20&linkCode=ogi&th=1&psc=1) (specifically this 140mm model, that runs cooler than the 120mm model) could work well enough around $40-45. If you really like the look of a 360mm aio, you could still save on a cheaper one, like [this one](https://pcpartpicker.com/product/jBrqqs/cooler-master-masterliquid-360l-core-argb-liquid-cpu-cooler-mlw-d36m-a18pz-r1), which offers a bit more cooling, but nothing you'll really need. You can have your build comfortably under $1300 at this point, where I'm gonna talk about a second issue. Your PC, is pretty overkill for 1080p, and more of a maxed 1440p build. There are plenty of good 1440p monitors below $200 price point, which you could use. [This here](https://www.amazon.com/dp/B0FNPSLCFL?tag=pcpapi-20&linkCode=ogi&th=1&psc=1) is an amazing example of one of those, offering a IPS panel and 200hz refresh rate for just $170.   You do have a solid understanding of PC synergy, just remember to look out for cheaper deals that can save you a lot of money.",Positive
AMD,"Two questions, what is FSR4? And what is an AIO, that would be the replacement for the cpu cooler?",Neutral
AMD,"Would you recommend buying something like [this monitor](https://www.bestbuy.com/product/aoc-27-q27g42ze-ips-qhd-240hz-g-sync-compatible-gaming-monitor-black/JX9GSKP9L3), . If I play games likes hunters call of the wild at 1440p would I be able to run 60fps?",Neutral
AMD,"FSR4 is AMDs version of DLSS, an upscaler which uses AI to basically give you more FPS in games. It's really good, but takes a few more steps to enable in games than DLSS.  AIO is your liquid CPU cooler. Most people use them because they like how it looks, but something like the Thermalright peerless assassin works just as well and like half the cost or less. Also will be more reliable.",Positive
AMD,"It's certainly good, Samsung was just a very reputable brand at similar specs and price. If you really want the extra HZ, then sure, go for it. Samsung panels are generally the highest standards tho, just so you know.",Positive
AMD,How much is fsr4? And you download it right,Neutral
AMD,Would the extra hz do that much? And what do you mean panels? I'm sorry I don't understand what some of the things are yet,Negative
AMD,"It comes free with the card! Any 9000 series AMD GPU has it.  The only caveat is that games need to add support for it, so it only works on certain games. Some games have it built right in where you just enable it in settings, but AMD also has a way to enable FSR4 in their adrenaline software, which will work for any game that supports FSR3.1 and 'upgrade' it to FSR4.  If you are playing at 1080p, you probably won't even need it since a 9060XT will perform very well on its own. However if you are playing at 1440p or you come across more demanding games, I would recommend using it as it essentially just gives you better performance for free.",Positive
AMD,"Oh so hz, the upper limit of fps, tends to do less the further away from 60 fps you go. 60 to 144hz is a very noticeable step up, while 144hz to, say, 180 or 200 is a somewhat noticeable step up. While some people think 60hz is all you need, due to the argument that a person can only ""see"" 30-60 fps, there is still a major difference in smoothness.   I don't think 240hz will do a ton compared to 200hz, to the point I'd much rather the Samsung monitor, as the colors, lighting, quality etc will most likely be held to higher standards the less known brand you mentioned.  By panels, I just meant the screen of the monitor. :)",Neutral
AMD,"Do you think the pc would be able to run around 60fps on 1440. Also thanks for all your help, I really appreciate it.",Positive
AMD,"Okay thank you that makes a lot more sense to me know. I will mostly likely go with the monitor you sent me, thanks!",Positive
AMD,"What FPS you get really depends on the game, but in general yes I would expect it to do 60+FPS. Any hard to run game should also come with FSR support so you can enable that and go higher.",Neutral
AMD,"No problem, I love helping out in this community from time to time :)",Positive
AMD,"Okay thank you. If I build this computer, should I sell my ps5? I don't really think I would need it anymore right?",Neutral
AMD,"The performance difference between the two is negligible aside from raytracing, and in a few games the 9070xt even performs a bit better. If you don't care about RT that much the AMD gpu is a clear winner in terms of value. You also won't be supporting Nvidia which is a plus.",Positive
AMD,"9070xt the extra $150 does not justify getting the same raw performance, only get the 5070ti if you need nvenc for streaming, dlss, only play raytracing heavy games or do work that needs Cuda Cores.",Negative
AMD,"If you're streaming and doing heavier workloads with Blender and stuff then get 5070Ti. If you're just gaming, get the 9070XT. You really want to make that +100 dollars worth and for gaming the performance alone doesn't justify all that extra money except maybe if you must have path tracing.",Neutral
AMD,"If you use linux or ever plan on using linux, 9070 xt is by far superior",Positive
AMD,Just bought the amd,Neutral
AMD,"9070xt, the extra Nvidia features (path tracing at <30 fps in very few game, multi frame gen) are not worth 100.",Negative
AMD,"I would say the cheap model for the 5070ti is actually better than the recommended ones.. but i might be wrong on that one..   On 9070xt vs 5070ti there is no clear winner you have to consider the facts.   -9070xt is the cheaper card that offers pretty much the same native raw performance depending on the game.. they trade blows.   On the other hand 5070ti has better software features.. like dlss that is better, same for frame gen, latency with reflex, video encoding/enhancement. It's also better for other applications other than gaming. I also liked the fact it consumes even 100W less than the 9070xt so less heat in the room and cheaper electricity bills... Rtx is also better on 5070ti.. pathtracing is much better on Nvidia   The VRAM is also better with gddr 7... Whether that will make any difference innthe future it remains to be seen....   So if you want the cheapest raw gaming performance 9070xt is the answer... If you want the better card 5070ti is what you are looking for.",Neutral
AMD,If they were both the same price: 5070 TI.  If it’s just for gaming and you’re penny pinching: 9070XT.,Neutral
AMD,"On the base MSRP cards, i’d go Asus Prime.   Heavy RT is better on Nvidia, and DLSS is in more older titles. Newer titles seem to be releasing with FSR4 (BL4 and Cronos are the most recent ones I have played) but ray reconstruction is far more mature on Nvidia as AMD is in the infancy of it there.   Something to note is that Nvidia seems to be very overclockable if you are into that.   I own a 9070xt and find it plenty of GPU at 4k, but i’m also fine with software lumen and not going wild with RT/PT. I wanted the Asus TUF OC model, and that version was $300 CAD more for the 5070ti model, which was not worth it after testing both. Also saved me from dealing with Nvidia’s two-app solution which I hated.   What do you have now? Nvidia drivers also have a higher CPU overhead.",Positive
AMD,There pretty much the same except for ray tracing and better work performance on the 5070ti resell holds better on the 5070ti also,Positive
AMD,1. Do you know what Path Tracing is?   2. Do you care about it?  If either is no - 9070XT is pretty safe bet.,Neutral
AMD,"For 100 bucks more it's a no brainer, get 5070ti  EDIT: with the new drivers that Nvidia launched these days it outperformed 9070xt in raw performance overall, and if you add the better upscaler (dlss and Multi Frame Generation) there is literally no reason to not pay 100 more, I'm not saying amd is bad, but with less than 150/200$ difference there is no reason to not get the better one, except if money is an issue of course",Positive
AMD,5070ti,Neutral
AMD,"How are there two top comments, one saying the price difference isn’t justified, and the other is saying it’s only that price difference LOL",Neutral
AMD,"9070XT. Same perf, lower cost.",Neutral
AMD,70ti,Neutral
AMD,DLSS>>>FSR,Neutral
AMD,"i find it funny why people even consider the nvidia alternative to amd when it comes to just gaming. amd doesnt use the disastrous 12vhp connector that catches fire every other day, they put reasonable vram amounts in their cards, priced much lower for comparable performance. only time in my mind to go nvidia is if you are a full time streamer or need 5090 level performance because of rt games, pushing high resolutions etc... other than that its a waste of money to go nvidia. Especially with FSR4 which is not all that bad compared to older FSR versions.",Negative
AMD,"If it is less than. $100-150 difference. Go 5070ti. If the difference is higher (cheaper), go 9070xt",Neutral
AMD,"Thanks, I have a question about raytracing - whenever I see some comparison videos, it looks like the ""sky"" in the 5070tis is much more blue than on the AMD. Example: [https://imgur.com/PM0jL1k](https://imgur.com/PM0jL1k)  Is this because of ray tracing, or is this a feature of the game? I have seen this in several different videos now where the 9070XT shows a significantly more ""gray"" sky than the 5070ti",Neutral
AMD,"Out of curiosity, why do you feel it’s good to not support Nvidia? I’m just recently back into PC gaming after 15 years and I like to know. Thanks",Neutral
AMD,Production in general,Neutral
AMD,"i agree that the extra features arent worth that much, but if someone really cares about ray tracing then they are. Full path tracing at <30fps native resolution is something an rtx 2080 does, not a 5070 ti. If someone REALLY cares about ray tracing and/or non-gaming workloads, the 5070 ti is the obvious pick. Otherwise the 9070 xt is way better value",Neutral
AMD,"\>path tracing at <30 fps in very few game,  Lol, good cope. Within last year only there was four major releases with PT (arguably three), next big one is coming within a bit more than 2 months. And there won't be LESS games with PT as time goes on.  As for sub 30fps... Do you yourself believe that?",Positive
AMD,"I know this may get me banned but... is this question asked like every few hours on this sub? 🤣  Also, I appreciate how concise you put the answer.",Neutral
AMD,Is there a longevity conversation to be had?,Neutral
AMD,"\> with the new drivers that Nvidia launched these days it outperformed 9070xt in raw performance overall  It ALWAYS outperformed 9070XT, both raster and RT. ""Fine wine"" meme was literally a single HUB video with 16 games tested, which is very small sample size. All wide scale benchmars have ALWAYS shown 5070Ti as faster card.",Positive
AMD,Are there connector issues outside of 4090 and 5090?,Neutral
AMD,"5070Ti can do PT 1440p ca 60fps avg with DLSS quality. With 9070XT you are getting sub 30fps if not sub 20-fps in: - Alan Wake 2 - Wukong - Indiana Jones  .   5070Ti can actually do 4K 90fps avg with high settings in most newest games with DLSS Performance (as it’s a half a tier better visual fidelity still, equals FSR Quality)  You also get features that you don’t get on AMD or just plainly work noticeably better, like: - Ray Reconstruction + half tier better upscaling - Multi Frame Generation for CPU bottleneck - RTX HDR - RTX Broadcast with AI plug ins for OBS - 3d modeling - longer software support 10 years vs 3 years (RX 7000 series) - holds value better for resale",Positive
AMD,"From that screenshot alone it just looks like a dynamic time of day difference. The cards won't display game visuals any different, raytracing would just run at a lower framerate. The only visual difference between the two would be DLSS vs FSR and at this point FSR4 is just as good.",Neutral
AMD,"Those screenshots are taken in very different time of day, so they are not directly comparable as far as image quality goes.  And nah, RT does not make sky more blue, it's for very different things.",Neutral
AMD,"if you want i can send you some screenshots of rt on vs off in insomniac's spider-man remastered.   You see, ray tracing doesnt make too huge of a differernce these days as most games only implement it for reflections, and sometimes lighting. Games that are full path tracers though (such as portal with rtx) sure do look different (and better) from traditional rasterization, and also are much more demanding  The reason nvidia's rt tends to also look better, is because rtx cards have dedicated ray tracing cores, so cores that are literally engineered for this ykwim? AMD doesnt have dedicated rt cores, instead they have ""ray accelerators"" which are basically tiny add-ons to some of their standard cores. A little add-on pack for a boost in rt performance on a standard core just can't keep up with dedicated cores, especially when these cores are used for everything else too while AMD's solution still leaves the cores with all the other work ontop of ray-tracing",Neutral
AMD,Tbh just for Gaming i say 9070 XT all the way Price difference is not worth it imo. 9070 XT is an amazing card.,Positive
AMD,"Nah, 5070Ti will actually struggle to hold over 30fps at native 1440p and higher, that IS true.  It is just extremely dumb to use it as an argument when upscaling exists AND provides as good of a quality as it does. Cyberpunk at DLSS Quality keeps above 60fps most of the time, with DLSS B heftily above, and image quality is pretty gorgeous on top of benefits PT provides.",Negative
AMD,Amd market share is rising and nvudia is stoping the production of non high end gpus. The only one coping is you,Neutral
AMD,"There is, but it still mostly concerns one's interest in heavy RT/PT. PT won't become universal within lifetime of 5070Ti, at least realisting lifetime. If one wants to stretch it noticeably beyond 5 years - then maybe.",Neutral
AMD,"When they came out and right after amd fixed they drivers it was just about 5% faster in 20 game overall, now it's a lot better I would say, the card got performance boost with the new drivers Nvidia released and up to 20% more fps in some games, but yea it was always faster, now its just faster they it should've been since the beginning",Positive
AMD,"Well yea usually 100 or 200$ more matters only if you are a kid that saved money for it and can't wait anymore, otherwise there is literally no reason to spend more  And yes mainly the futures and stable drivers was the thing that kept me going from Nvidia, all my life I've been using only Nvidia cards and not a single issue, my old gt730 was running 6 years every day overclocked to the max and still going strong lmao  on the other hand 5070ti is the top tier gpu for 1440p gaming, I'm amazed how this thing plays everything on max",Positive
AMD,actually yes. i’ve seen a few 5080s with melted connectors,Neutral
AMD,"Any GPU with 12vhpwr has the potential to melt  NVIDIA GPUs have a higher chance to melt (albeit still low) simply because of the way they are designed (my fathers 5070ti hasn't melted yet, hopefully it never will)  AMD GPUs can still melt, although it is FAR rarer, and I personally have not experienced it (thankfully) on my Sapphire Nitro+ 9070xt  Not all AMD GPUs have 12vhpwr, afaik it is only on some 9070xt models (could be more tho)  All NVIDIA 50 series GPUs, some of the 40 series GPUs, (I'm 100% sure the 4080 variants and 4090 use 12vhpwr, not sure about others), and the 3090ti all use 12vhpwr  The higher the wattage, the far riskier it gets",Neutral
AMD,But isn't FS4 supported only in a handful of games?,Neutral
AMD,"Would correct a bit that plain RT does not look better with NV, but runs much better. It can look better due to that, because RT is *always* temporally accumulated, but that's a moot point.  On the other hand, there is DLSS Ray Reconstruction, which DOES make RT look better. AMD until very-very recently had no alternative at all, they now have it. Somewhat worse working. In one game. That game being Call of Duty. Yeah...",Neutral
AMD,This is just simply false,Negative
AMD,"Damn really? Im basing my assumption off how my 2080 at 1080p performs, thats sure a shame (my only full path tracer was also portal with rtx so far so ots not very extensive)  And also yeah dlss is quite impressively good, and always implemented alongside rt. Ray reconstruction (the not transformer model, forgot what its called) is also good for performance but i personally think it looks worse  Anyways thank you for this info, I'll keep it in mind!",Negative
AMD,"\>Amd market share is rising  From 9 to 12%? What an achievement. Call me back when they are at least at 50%.   Also what's that even supposed to do with GPU pick in the first place? Are we supposed to buy only cards of vendor with bigger market share?  \>nvudia is stoping the production of non high end gpus  Alledgedly presumably according to rumors. And those rumors are like fifth time this year alone. Call me back when something ACTUALLY happens.  \>The only one coping is you   What the hell is this comment even then, LMFAO? You are literally arguing back because I've hurt the feelings of your favorite multibillion company. Pathetic.",Negative
AMD,"\>The higher the wattage, the far riskier it gets  Which is funny, considering two GPUs in question, since 5070Ti routinely sucks 80-100w less under load.",Negative
AMD,"In quite a few games with driver override, but still much less than DLSS, yeah. One can always use Optiscaler, but it's not a universal solution.",Neutral
AMD,"In theory it doesn't look better, but it often does due to games rt being more nvidia optimised. I should have mentioned that in theory it should be the same but just isn't cuz of that, thats my bad  And yeah dlss 3.5 really is great looking i must say",Negative
AMD,Which part?,Neutral
AMD,nvidia is the company that bought all the rams just to make everything more expensive and you tell me they dont want to kill the consumer market? Do you hear yourself?,Negative
AMD,"\>In theory it doesn't look better, but it often does due to games rt being more nvidia optimised.  That does not work that way. First - RT is not really ""NVidia-optimized"". It's just a sampling algorithm, it runs similiarly on any hardware. It just runs much faster on NVidia. Running faster in case of RT actually brings better visuals too, but not just due to ""NV vs AMD"", slower NV card will look worse than faster AMD card all the same.",Negative
AMD,"First - you really think it was SPECIFICALLY Nvidia who caused DRAM and NAND shortages? Like, noone else, only NVidia?  Second - do you geniunely think they WANT to kill consumer market? Like, you think they see some value in that? Or that they are just plain evil or something?  NVidia stayed in consumer market since first cryptoboom, solidified its position, expanded its market and continued supply the shit. They were rumored to ""exit consumer market"" for literally dozens of times. Since start of the year alone this is like fifth round of those rumors.  [https://wccftech.com/nvidia-rumored-to-reduce-rtx-50-gpus-production-in-china-in-favor-of-ai-gpus/](https://wccftech.com/nvidia-rumored-to-reduce-rtx-50-gpus-production-in-china-in-favor-of-ai-gpus/)  [https://www.techspot.com/news/108150-nvidia-could-slash-rtx-5000-gpu-production-up.html](https://www.techspot.com/news/108150-nvidia-could-slash-rtx-5000-gpu-production-up.html)  [https://hothardware.com/news/nvidia-is-allegedly-scaling-back-supply-geforce-rtx-50-series-gpus](https://hothardware.com/news/nvidia-is-allegedly-scaling-back-supply-geforce-rtx-50-series-gpus)  And so on and so forth.  And THE MOST IMPORTANT QUESTION: how the hell is that related to Path Tracing and NVidia cards ability to run it at all?! Does that ability shrink with lower market share? Or with lower supply of cards?  Like, WHAT are you even to argue here, LMAO?",Negative
AMD,Ahh okay! Thank you for explaining!,Positive
AMD,There is no ai without nvidia. Everyone is pumping money into nvidia. Youvcan say that nvidia is the shortage itself at this point,Neutral
AMD,"No problem) There's some astonishing amount of RT misconceptions on reddit, sometimes outright disinfo, which is pretty sad, especially for subs like this one which are created to help people to know stuff (and instead turn into fanboys brawl).  So it's always very pleasant to discuss stuff calmly and with someone who actually listens.",Negative
AMD,"Dude. You are spitting ideological headlines in technical discussion. Like, literally nothing of what you've wrote here is any sort of factual statement. Literally not a thing.",Negative
AMD,"Yeah, there's alot of misinformation on here that im sure everyone believes at least some, it's a shame more people don't listen to being corrected and instead argue over a brand preference   Thank you very much for telling me all that about rt, i feel like I've actually learnt alot!",Negative
AMD,"GPU is going to be the biggest issue for most modern games, then CPU. I don't know what you are looking to spend but an RTX 3060 12GB or RX 9060 XT (preferably 16GB, but if budget means 8GB ok) would be the options I would look at.  CPU wise you can update your motherboard BIOS and then slot in a 5000 series CPU, 5600/5600X/5700X/5800X/5800XT are all good options.",Neutral
AMD,"While 32GB of RAM is recommended these days, 16GB is still enough for most gaming. I would say the weakest part of your system right now is the GPU, followed by the CPU.     What motherboard and power supply do you have? That will limit what your GPU and CPU upgrade options are.",Neutral
AMD,"Not sure what speed your RAM is, but the 16gb should be totally fine. The weaknesses here are the CPU and GPU. I’d start with the GPU.",Neutral
AMD,"You don't have anything that isn't overdue for an upgrade, but I think the clearly weakest link in this pretty weak chain is that GPU.  I would say second-choice should be storage, third CPU, fourth choice RAM.  Your GPU literally can't play a \*LOT\* of modern games. If you could pickup a second-hand 12Gb 3060 your whole rig would be \*massively\* improved or if you have the budget for it the 9060XT 16Gb (avoid 8Gb graphics cards, they're all universally terrible value).  After that upgrading to a 5600 or something like that would also give you a huge boost and at that point you'd have actually a quite decent PC.  Then there's storage - playing games from a HDD is just not a good idea anymore but your SSD is so small you're only going to fit one game on it at a time.  You could do with more RAM, but actually 16Gb isn't all that big of a problem. It does hurt you at times, but if you're running games from the SSD it shouldn't hurt you a lot - and certainly it's \*MUCH\* less of a problem than everything else.",Negative
AMD,Listen here sonny boy...  *speaks in 6700k and R9 fury*,Neutral
AMD,Rtx 5060 / 5060ti or a rx 9060xt. Then a cpu upgrade to a 5000 series later or now if you can squeeze in a 5600 / 5700x depending on budget.,Neutral
AMD,AM4 and 3000 series GPU,Neutral
AMD,"RAM is stupid expensive right now, I wouldn't recommend upgrading that because it'd be very expensive, and assuming your motherboard uses DDR4, you wouldn't be able to transfer it to newer systems that use DDR5.  The main question is where is your system lacking for your needs? The size of the SSD is a bit small, so if you're using the HDD for a lot of things, getting a larger SSD will be a noticeable improvement. Otherwise you can open up task manager and see if the CPU or GPU are maxed out in the stuff you run, and update the thing that's the bottleneck.  Another option from someone rocking a 5 year old build is to upgrade your peripherals/accessories. A nicer desk, monitor, chair, mouse, and keyboard made a huge difference for me.",Negative
AMD,The best course will be a cheap 6 or 8core 5xxx series CPU ( avoid g series since they dont perform as good due to lack of L3 cache ) which means either 5600/5600x/5700x/5800x/5800XT whichever fits your price ( dont go spending more than 150-160 for one though  Gpu wise a 9060Xt will be a massive upgrade. You can find cheaper 8gb gpus but be aware of the limitations of 8gb vram   A bigger  nvme drive is also a necessity since that 238gb drive will fill after 2-3 AAA games and installing a modern AAA game on an HD is asking for trouble   With Ram you can still get away with 16gb but barely and that is if you are not multitasking while gaming so a move to 32 is still recommended though not necessary,Neutral
AMD,GPU will have the most impact because my brother in christ it is damn near 2026 and you're playing on 4gb card made almost 10 years ago.,Negative
AMD,"Honestly, unless you do a ton of multitasking, streaming and heavily modded games, RAM should be the lesser of your concerns. Sure, 32GB are better than 16, but as someone whose lazy ass has still not installed on my main PC the 32GB kit I have on my desk since summer... yeah, 16GB are still more than enough for most uses.  The GPU is probably the weakest part in your rig for modern gaming, especially considering it has just 4GB VRAM. Then next step would be the CPU, which while still fairly decent, could fall short for more recent games. I had the same 2600X until three years ago upgraded to a 5800X3D I got on a good deal. That's around the best thing you can get for your current AM4 motherboard (5700X3D, too).   Honestly, changing GPU you should already notice a nice improvement. Then CPU will nail the rest of the jump when you can afford it. About RAM... well, considering how stupid prices have become, you can always try and look for deals, but don't get crazy for it if you are on a tight budget. I'd totally prioritize GPU first.",Neutral
AMD,"Ryzen 5600 and intel arc b580.  Yes an ssd for bulk storage would be nice, but I wouldn't prioritize it in the budget.  32gb of ram would also be nice, but ram is super expensive right now and 16gb is generally enough.  As someone else pointed out. Upgrading the cpu would depend on the motherboard model. Some prebuilts don't have a bios compatible with newer gens(though some do if you know where to look) it would be helpful to know.",Neutral
AMD,"Ram is absolutely ok for this pc Just buy new gpu, something like secondhand 3060(ti)/4060",Negative
AMD,I would grab what parts you need asap. Ram prices are skyrocketting as are data storage like ssd and cpus and gpus. Every component for computers are now essentially going to ai and it has the supply/demand matrix running overtime and fucking us normies over big time,Negative
AMD,"Depend on the mother board, if the mainboard support Ryzen 5 5600 it (which usually is for AM4 system), it will be substantial upgrade, Avoid the low end Nvidia 5000 series though, they are anemic with only x8 Pcie, which is fine if your mainboard is PCie 4.0 or 5.0, but its looks like your system is 3.0 AMD GPU (6600,6700 or 9060 )is the safest bet for upgrade",Neutral
AMD,"What’s your motherboard?  Can it support 5000 series CPU’s?  Your single best upgrade will be the gpu, it was quite literally the bottom of the barrel even when new, and is 100% your biggest bottleneck.  After that, upgrading to something like a 5600x would produce a noticeable bump if your mb supports it.  Otherwise slap a gpu in it, and start saving for a platform upgrade and hope ram prices stabilize.  Your ram capacity is fine currently, so I’d only consider that as part of a whole platform upgrade.",Neutral
AMD,"I built my gaming PC in 2016. CPU i5 6600, DDR4 16GB with 1050Ti.   2 years ago, instead of building a new rig I upgraded my GPU to RTX 4070.   Most of the struggling games issues are resolved.   Even though 16GB RAM is fine, I upgraded to 64GB (because VM/Containers) early this year.   I don't have much option to upgrade my CPU without replacing mobo.",Neutral
AMD,5060ti/9060xt 16gb and a 5700x3d and you're good for a good while,Positive
AMD,"While GPU, CPU upgrades are recommended, check motherboard for what PCI-E lanes are available for GPU and M.2 slots. If you are using a low end motherboard from 2018, you might neeed to change mobo to get the most out of your new GPU. Also check PSU compatibility beefore buying GPU",Neutral
AMD,"Graphics card will be the most expensive to upgrade, but probably make the most difference - a 16gb 5060ti or 9060xt  Ram is fine, I'd leave it.   Doing the cpu isn't a bad idea as your thermal paste is probably about due to be redone anyway- a 5600x would be good. The 5700x3d and 5800x3d are now getting wild prices even on ebay so I'd just not bother.   A 2tb ssd to replace both your current drives would also be good, quite a lot of games assume they're installed on ssds these days.",Neutral
AMD,"GPU always first. Get what fits your budget,. Either new current gen GPUs (minimum 8GB, preferably 16GB) or used (cards like the 3070, 3060 12GB, RX 6600, RX6700XT). These would all be a significant step up from your current GPU.   Next step would be to upgrade the CPU to an RX 5600 or highter. This would also be a noticable improvement, but not as big as the GPU.",Positive
AMD,"Upgrade your GPU you can get a budget GPU that'll beat the pants off what you're running now.,",Neutral
AMD,"Upgrade your cpu to a 5700X, 5700XT or 5800XT. Depending on your current cpu cooler you might have to upgrade that as well, but you can get inexpensive but very good coolers like the Peerless Assassin.  32gb ram would be great, but you can still manage with 16gb. Upgrade this if you have the budget for it.  The gpu needs updating that’s for sure. If you are strapped for cash I would suggest a used Radeon 6600. Otherwise moving up from there an intel ARC B580, next GeForce 5060 ti 16gb or Radeon 9060 XT 16gb, or higher models if you have the budget.  I either one of these things would keep you going for a good while. But keep in mind you might be forced to upgrade the cpu along with a new gpu because otherwise you would be cpu bottlenecked.",Neutral
AMD,"What's your budget? Everything in your system is outdated yes, but actually RAM is the least of them all.  You could drop in a used 5800x CPU for kinda cheap, or get a new SSD for kinda cheap. A new GPU would be more expensive, but arguably more bang for your buck for games.  Or if you have a giant budget and are near a microcenter, I'd say just build a brand new computer with their 9800x3d combo - but this all dependent on your budget.  P.S. And what is your power supply?",Negative
AMD,"Sell the whole thing,  amd start fresh.  Go to microcenter.com  but at a physical store, pickup 7600X3D combo wirh motherboard and ram.  That computer would need everything new, and upgrade money can be spent better on awhile new platform.  It all depends on your budget",Neutral
AMD,What's your current budget for your upgrade,Neutral
AMD,"How much are you willing to spend?  For games, you'd want a better GPU to start with. The 16GB 9060XT Gigabyte Gaming OC is currently $379. Really good 1080p and 1440p performance, and even some fairly decent 4K performance if you compromise on graphics settings (also using FSR).   After that, the CPU and RAM will be your next restriction - which can be easily done by upgrading to a 5700 or 5800XT and find yourself a used 2x16GB 3200CL18 kit (or even 3600CL16)  Just that combination will keep your PC going for the next few years until you're really really looking to build a completely new rig.",Positive
AMD,It does depend on how much you want to spend but I would get a 5700x and a 9060xt 16gb.  If you can swing it get at least a 1TB NVME drive but they have jumped in price the past few months.,Neutral
AMD,"Depends on your budget. If it's somewhere around £300, i'd suggest going for a 5600X + ARC B580   If £200, a used 3070 or 3060 Ti is also a solid shout.",Neutral
AMD,"Well... Depending on your Mainboard, I would certainly think about a CPU update. Most pressing issue would be gpu. I do have 16 gigs of ram as well, and not yet experienced a problem due to that. Therefore I would maybe wait with ram.",Neutral
AMD,First the cpu to a i7 14 gen with integratet graphics should be good then a like 4070 on used market,Positive
AMD,Sell it for $200 ish bucks on FBM and buy a $500 used PC on FBM. Total cost to you is $300 and you can get a decent upgraded pc,Neutral
AMD,Sell it for $200 ( local prices may vary) on FBM and buy a $500 used PC on FBM. Total cost to you is $300 and you can get a decent upgraded pc,Neutral
AMD,"I did something similar to this over black Friday;  b350 pcmate MOBO, 2200g APU/CPU (no GPU build) to a $89 5600x off of aliexpress and a $330 9060xt 16gb.",Neutral
AMD,"5700x3d if used and for a decent price is a good option too. Facebook marketplace is good for that, depending on region.",Positive
AMD,"I’m rocking a 3060 12 GB with a 5800X and 2 1080p monitors rn, it’s definitely great for a low budget!  I can play most older single player games on High graphics settings, and newer ones with low-medium settings and they still look great. Stellar Blade for example I can run on decent medium to high graphics settings because of the extra VRAM.",Positive
AMD,How do you find out what type of motherboard you have? I bought a prebuilt and am pretty terrified/against opening it up and taking it apart. I got a i5 with geforce rtx 3060 is all i know. Not sure how to see my ram info either. Thanks in advance,Negative
AMD,"Dude, ram is an easy upgrade lol.  But I agree that 16gb is still generally enough. Especially for an entry level machine.",Positive
AMD,"16 GB of RAM is still enough to get by get by with for most gaming. You won't see a huge improvement going to 32 GB DDR4 in most games. With your current GPU/CPU, more ram will do almost nothing for you.",Negative
AMD,"The exact type of motherboard you have probably doesn't matter much, the main things that would matter is what type of RAM it supports and what CPU socket it supports. I believe your CPU only supports DDR4 RAM (so that's what the motherboard supports), and the socket for that CPU is AM4, so if you wanted to upgrade the CPU and keep the same motherboard, you'd just have to make sure the new CPU has the same socket.",Neutral
AMD,"RAM has become expensive as heck, tho. If OP is in a tight budget (literally said right now can probably upgrade one thing only), I wouldn't worry about RAM atm.",Negative
AMD,"I agree, I was responding about you having a ram kit sitting on his desk for half a year without putting it in.",Neutral
AMD,"Oh it was about me, damn, lol.   Yeah, I've been really busy with shit for the last months, and since I use that computer a lot I just haven't found the time to disconnect everything and do proper maintenance (clean dust, change thermal paste... you know) which I want to do altogether with the RAM upgrade. It's been a kinda hard year, so whenever I have free time with my computer I just evade myself from responsibilities and plunge into videogames or something instead. For better or for worse, that's why I know 16GB are perfectly fine for most uses tho.  Now I also need to replace my last secondary storage HDD with a SATA SSD, too, so I'll do it soon, for real this time lol.",Negative
AMD,I get you man. Especially since I have kids.,Neutral
AMD,"This is all pretty neat :) I'd say the value and synergy was very good, especially that 7900xt. I'm honestly most surprised that you were able to get the cpu-mobo-ram for less than the cheapest Micro Center bundle with at least 32 gb of ram (500$).",Positive
AMD,where the heck did you find 32 ram for $190?,Neutral
AMD,Could have just gone to Microcenter,Neutral
AMD,"I did have to go used on a majority of components, albeit most in pretty much open box condition. I’m kinda bummed on the fact I could’ve gotten a Ryzen 5 9600x for 15$ more.",Neutral
AMD,"Found a dude selling mobo and ram combo on Facebook, 290 total.",Neutral
AMD,"True, but even a CPU/RAM/MOBO bundle that is closest to my specs would have increased my budget by at least 70 dollars, still amazing deals tho.",Positive
AMD,New?,Neutral
AMD,"Ram used, Mobo open box.",Neutral
AMD,Okay that's a great deal man. I paid 300$ for one stick recently 32gb 😹,Positive
AMD,Since it's ur first build did u use standoffs ? If the board is touching the case it will be shorting out. The board can't be touching the case at all.,Neutral
AMD,Is the CPU fan on the correct header?,Neutral
AMD,Does anything happen when you try to short the power pins? Are you certain you are shorting the right pins?,Neutral
AMD,"Is the cables that run from your case into your motherboard inserted properly? Like the power switch, reset switch etc.   It's a bunch of singular connections you gotta plug in for your case buttons to work  Also known as Front Panel Feature cables.",Neutral
AMD,"What do you mean it wont power on, exactly? Is it not booting, or no power at all? Check the power switch on the back of the power supply. Your troubleshooting steps seem to be non-boot though?  If there is power, but not booting, how long are you waiting? AMD systems do memory training and can take a few minutes sometimes to finish.  Speaking of memory, check and see if your ram is seated properly. Even LED ram can have the leds lit but still not boot if they aren’t fully seated.",Negative
AMD,We need a picture or video on that one,Neutral
AMD,Make sure psu cable to motherboard is fully plugged in. I had to press it in surprisingly hard,Neutral
AMD,Did you test your power supply to make sure that it's not the problem?,Neutral
AMD,Standoffs and not switching on the power button on the PSU. Happens to us all.,Neutral
AMD,I did not use them but it does not appear to touch the case .,Neutral
AMD,Which one would that be ?,Neutral
AMD,We put it in CPU fan 1 and it didn't work,Negative
AMD,Nothing happens and yes I checked the manual .,Neutral
AMD,No power at all no leds turn on for any component,Negative
AMD,Not having standoffs is like trying to drive a car with no tires... It simply won't work... unless like the case is made out of plastic or cardboard...,Negative
AMD,It's a must it's not optional. The board is shorting out most likely. You can take the board out and try turning it on on like a cardboard box. If it turns on than you have your answer.,Neutral
AMD,"Can you share photos of the build, general photo of it all, then close ups of the status LEDS, all power connections both at the PSU and components",Neutral
AMD,Looking at the mobo probably above the RAM slots. It'll say CPU fan or something like that. Not pump or opt,Neutral
AMD,I'm very interested in seeing if we can get this working... Please make a video or pictures at the very least showing everything for us please. Show the wires and ram,Positive
AMD,Just tried it on a cardboard box with CPU and cooler and it did not work,Negative
AMD,Isn't it possible he's fried his mobo if he powered it on while screwed directly into the case? Seems to me that there could be all kinds of shorts.,Neutral
AMD,I have currently disassembled the build for trying to short the motherboard on a cardboard box but I have to sleep. Tell me what pictures or video would be helpful and I'll try to upload some tomorrow,Neutral
AMD,"If you already have a 5800X, forget it. The X3D variant has been out of production for a while and is highly sought after. Those that are available have a huge price tag that is simply not worth it. With all the hardware crisis going on, you better pucker up because you are stuck with that pc for at least two years. You are better of upgrading your gpu, or something else in your pc to increase longevity, like a new psu, better cooling. Or undervolting. The 5800X aren’t very happy with undervolting, but even a tiny bit goes a long way.  EDIT: just saw you have a 3080ti. Just keep using that. If you wanna spend money on your pc, get another 16gb ram while you still can. Or another SSD before they explode in price.",Negative
AMD,"it would be a bump up in performance, but not worth it. Grab another 16gb of ram and rock your system throughout the ram crisis.",Negative
AMD,"I don’t think so man. Unless you have $300 to burn for a used CPU with marginally better performance, that’s really the only case it makes sense.",Negative
AMD,I have a 3080TI and 16 GB DDR4 3200mhz just to add that,Neutral
AMD,What sort of performance are you looking for? I imagine you're hitting around 150 FPS on near-max settings.   Do you need more than that?,Neutral
AMD,"When you say ""optimal performance"" what do you mean exactly?  Does the game run smoothly at 60fps or above?",Neutral
AMD,"I’ve got a Ryzen 7 5700x and an rtx5060ti 16gb, depending on how much vram you have it could be that. Before upgrading my gpu I had 12gb 3060 and even with that extra vram I close all other applications when playing bf6 otherwise my vram usage was maxing out on mid-low settings. I found that upgrading the gpu fixed that issue and the 16gb vram is enough to keep my browser open when playing bf6 but watching yt at the same time does halve my fps so I stick to music",Neutral
AMD,Save up for a new system. The bump would be small.,Neutral
AMD,"At 1080p High with a **5090** the 5800x3d gets 142fps average and 106fps 1% lows. Meanwhile, the 5800 gets 111 fps advantage and 83pfs 1% lows.  How ever since a 3080 gets an advantage of 106fps at 1080p overkill with a **9800x3d** i would imagine it would be a negligible performance increase unless you play at 1080p low.   https://youtu.be/nA72xZmUSzc?si=NlbhpUm7OzP-G1Kt  https://youtu.be/RP0rfOP5iAk?si=7jt6MTWCiiU_zyQs",Neutral
AMD,"Get 32gb of ram, a 5800x should be plenty for BF6 at a solid 100fps",Positive
AMD,At the price they sell 5800x3d used now no not worth it at all.,Negative
AMD,"I just upgraded from a 5800X3D to a 9950X3D after finding a decent deal on ram on Marketplace...  There's an improvement sure, but it wasn't the game changer I was expecting for gaming. (I do a ton of video and photo editing though so it's absolutely worth the upgrade for that).  The 5800X3D is quite a chip. Ran it with my 5090 for a bit and didn't really feel very bottlenecked.  I'm throwing it up on marketplace soon as they're getting quite high prices, will offset my upgrade cost.",Positive
AMD,"5800x3d wont give you optimal performance, and its a bad time to upgrade to AM5. So you will have to sit this one out.",Negative
AMD,where do you even get it for a reasonable price,Neutral
AMD,Just turn up your graphics settings and toilet your GPU do the heavy lifting,Negative
AMD,Not really bro.  Start saving your pennies for an am5 upgrade!,Neutral
AMD,"Idk  what these replies are saying, going from a Non-X3D to an X3D, especially in CPU intensive titles will always be a reasonable upgrade. Yes the 5800X3D is expensive right now on the Used market but keep in mind, its still waaay more cheaper for you to get the 5800X3D than to upgrade to AM5 platform right now.   You have 3 options 1 . Keep the 5800X  2. Upgrade to a 5800X3D (you will spend some money)  3. Upgrade to AM5 (you will spend a SHITLOAD of money)  Choice is yours👌",Neutral
AMD,"Can you drop your thermals while gaming and how much ram is being used under typical load? ""double your ram double your ram!"" imo is terrible advice until you've ruled out undervolting your cpu/gpu. Are you worried about frame rate or latency when you're talking about ""optimal performance"" ? Do you have a gsync monitor and what resolution are you running",Negative
AMD,"Not OP but I have a 5700X with a Thermalright Phantom Spirit 120. CPU rarely goes above the 50s celcius, probably peaks at 62 degrees under high load. I’ve applied an auto overclock via Ryzen master, would I be better off undervolting instead (or alongside a clock speed increase)? I thought undervolting only helped if you were reaching your thermal ceiling and boosts weren’t maxing out as a result. Are their other benefits to it that I’m unaware of?",Neutral
AMD,"> The X3D variant has been out of production for a while and is highly sought after  Oh for real? I sprung for it awhile back because I heard the larger cache helped poorly optimized games which feels like most things these days.   Between that and me buying 64gb of ram about 8 months ago, my PC purchases are surprisingly well timed.",Positive
AMD,Double your RAM and leave the CPU alone,Neutral
AMD,"I got 32 GB RAM and the 5800X, the CPU is rarely ever at full capacity with exception of Ultimate Epic Battle Simulator. Another 16 GB of RAM would do you better probably, and it’s also cheaper than the 5800X3D.",Neutral
AMD,"I will say, since 90% of my gaming is sim racing in VR, moving from the 5900X to the 5800X3D was like a generational leap. All the stutters disappeared and the CPU frame times dropped and stayed drastically more consistent. The X3D chip is absolutely a worthy upgrade, but not at current prices. And if you aren't playing in VR and have a G-sync enabled display the frame inconsistency is way less of an issue.",Positive
AMD,"but then i gotta upgrade the cpu i gotta do the ram which is crazy expensive, motherboard and cpu :( im a student my pockets not that deep",Negative
AMD,might just wait until AM6 to buy AM5 parts lmao,Neutral
AMD,"It uses less power for starters, its not just lower temps. So that gives it more room to boost higher. Usually you would ""just"" enable PBO in the bios and a negative curve optimizer, then go down with 5 each increment until its not stable anymore, then go back up another five and thats it. The X3D cpu's can be undervolted a lot, the regular ones not as much. Most should support -5 just fine. Everything else less than that is pure luck of the draw.",Neutral
AMD,Just let it do its thing you wont gain much.  Edit: fixed game to gain.,Neutral
AMD,"With ramaggedon though it gives you plenty of time to save, the longer the ai bubble lasts the more you can save for a new system",Neutral
AMD,"This is really helpful, thank you for this. Any particular cpu stress test you recommend to test stability?",Positive
AMD,How much more performance would that get you?,Neutral
AMD,But I will game much,Positive
AMD,"Personally I would just use cinebench, but there are other more demanding and more thorough stress tests out there like OCCT. Most would recommend that.  It’s just enable PBO, set all core negative curve optimizer, start with 5, test, and keep incrementing with 5, stress testing each time. The moment it becomes unstable decrease with 5 and test again. And that’s it. You eventually settle on a number and if it ever starts to misbehave go back another 5 but at this point it SHOULD be rock solid.  You can tweak and push it even further but this is the most simple variant and it will still yield good results.",Neutral
AMD,"It’s nothing groundbreaking but it’s the easiest and safest way to gain more performance and on top of it less heat and power. The speed you gain is dependent on a lot of factors like cooling, cpu binning, PBO settings and etc.",Neutral
AMD,Gain* sorry typo.,Negative
AMD,"Back in the day, you could get a Celeron to run almost as good a a Pentium with overclocking.  Early Athlons were also pretty good when overclocked.  And it wasn't that hard to do.  I overclocked a bit back then and it paid off.  But today, from what I've seen, it seems to be a bit more complicated with a lot less return.  But I have some free time over the holidays, so maybe I'll look into it more.",Positive
AMD,"Its not really a case of “new thing bad”, it’s that the philosophy behind the releases that’s changed. If we take a part like the ryzen 5800X, you don’t get a lot of overclocking headroom because it’s basically already running into the ceiling of the architecture out of the box. That’s why you have to go the route of undervolting to push it further. Heck, I’ve heard of some unlucky folks who had to OVERvolt their CPU’s to get the system stable.   If you take the Athlon 1700+, a legendary cpu for its overclocking potential, it could overclocking so good because it was perfectly good too end models that was simply downclocked. The demand for low and mid end cpus was so high amd (and Intel) had to resort to this.   Now on the other hand, it’s more controlled and low to mid end CPUs are almost always high end CPUs that didn’t muster it, and here is the most important part: if it’s a good high end that has to live as a 5500x for example, then the extra cores etc are fused off on a hardware level.",Neutral
AMD,">If you take the Athlon 1700+, a legendary cpu for its overclocking potential,  I do believe that was one that I overclocked.  So easy and so much more performance.  I had a Celeron before that that was equally overclockable.  Ah, the good old days.  Yeah, I understand about the binning and stuff.  That aspect hasn't really changed that much, but your options for over-volting and under-volting and setting power curves and frequency manipulation, most, if not all, of which can be done in BIOS.  We had a jumper that gave you two or maybe three options for the base clock and another jumper for frequency multiplier and after that it was just having a cooler good enough to keep the CPU from overheating.  I knew one guy that built his own liquid cooler and drilled holes in his case for the tubing and mounted a radiator on the top of his case.",Positive
AMD,"No, Amd has been pretty good with drivers for the last 5 or 6 years ever since vega. 9060xt 16gb is the best bang for the buck all around.",Positive
AMD,"I've had a 9060xt since it released and have zero driver issues.  It's an amazing card.  With that being said, Walmart has a 5060 ti for sale right now for $369 which is the better value over a 9060xt for $389.",Positive
AMD,I have a newer AMD card(7900XT) and a newer Nvidia card(5060) and I’ve had zero issues with the AMD card over the last 3 ish years but my 5060 I’ve had for less than a year I’ve had to roll back driver updates twice. Nvidia is nice for productivity features but otherwise I’d just go with best bang for you buck that fits your budget and the 9060XT is damn good bang for your buck barring some 5060Ti sales lately,Positive
AMD,"Amd driver issues are so so exaggerated! You don't even need to update them every month for them to go wrong, and for 95% they're no problem upgrading every 6 months.",Negative
AMD,I have had a 6700xt and a 9070 now. I have never had any driver issues with them. They are good cards. Honestly if anything Nvidia has been notorious for more issues recently.,Positive
AMD,If you have a microcenter close to you:  https://www.microcenter.com/product/696272/asrock-amd-radeon-rx-9060-xt-challenger-overclocked-dual-fan-16gb-gddr6-pcie-50-graphics-card,Neutral
AMD,Im using RX9060XT 16GB rn and yes I had some crashes when playing BF6 and Ghost of Tsushima. But overall I think that it's a good choice for whatever budget you are running. Also AMD has improved vastly in recent years.,Positive
AMD,"The 9060 xt 16gb is a pretty good card for both 1080p and 1440p, and it can be found for around 400€, There's also the 5060 ti, but that usually cost about 100€ more than the 9060 xt without having much better performance.  There's also the 9070 non xt, but that goes for around 600€, so that may be out of your budget.",Positive
AMD,"If you have the money, go for the 9070 non XT. It is a significant performance upgrade over the 9060xt.",Positive
AMD,>The main thing i want from the new pc is to be able to run the latest final fantasy on at least high graphics.  What is your budget and where are you living?,Neutral
AMD,if anything nvidia is the one with the worse drivers now (lol black screen),Negative
AMD,"Upgraded from 2080 super to 9070 xt almost 2 months ago, drivers are stable for the moment and card runs well  The AMD driver problems were mostly from 5000 and 6000 series afaik, and they fixed them over time!",Positive
AMD,It's the reverse in the last few years. Amd drivers/software has been solid while nvidia drivers/software has been buggy.  I just bought an Asus Prime 9070 XT 16GB and have no complaints but I debated buying the 9060XT 16GB instead.,Positive
AMD,"My 6900XT was very solid. The few times I did have new driver issues, I just rolled it back and carried on. Easy fix.",Positive
AMD,"The only thing painful about and AMD driver updates is convincing windows to not undo them. If you trust yourself to do it through regedit it's easy, but if you want to do it the safer way you have to install the group policy editor that Microsoft pulled from home versions.",Negative
AMD,"Radeons drivers have been pretty solid this generation. Surprisingly nvidia has actually struggled more.    https://youtu.be/NTXoUsdSAnA?si=zQsPrFuhfnwkt6Ew  Now the nvidia card is still going to be better than its Radeon equivalent, DLSS is a superior upscaler and their feature set is more widely supported, but if the Radeon card fits in your budget and does what you need it to do, you really don’t need to worry about driver issues",Positive
AMD,"I built my friend an AMD build about 6 months ago and he hasn't had any issues. I, like many others are still very afraid of their drivers after some of the worst pc experiences of my life. It's hard to just forget that stuff lol.  I personally would never put an AMD anything in my machine, but I'm based af.",Negative
AMD,Grab an 9070xt they are or that expensive  Arc raiders on cinematic 2k gives 80 fps with a kinda weak cpu 5600x  On ultra I play over 120 in every map   Just some examples and no frame generation,Neutral
AMD,Rx9070 non xt is ur choise,Neutral
AMD,"just save more and get a 9070xt or 5070ti. dont be impulsive, it will give you alot better performance and longevity.",Positive
AMD,"AMD driver issues haven't been a thing since the 6000 series, your IT guy is living in the past. I've owned a 6950 XT, a 7800 XT and a 9070 XT, all ran and worked fine.  Nvidia on the other hand, had such a rocky launch of the 50-series when it came to drivers that it was their turn.",Negative
AMD,"eeeh, Vega and Polaris definitely had some issues, but every card after is great as far as I've heard.  I remember with my RX 580 there was an issue where the GPU wouldn't downclock itself at idle when at 144hz, but setting a custom resolution at 143 fixed it.",Positive
AMD,[https://www.walmart.com/ip/RTX-5060-TI-16G-SHADOW-2X-OC/16603867637](https://www.walmart.com/ip/RTX-5060-TI-16G-SHADOW-2X-OC/16603867637)  Here you go.  It's about 8% faster than 9060XT.,Neutral
AMD,Strange in india 5060Ti is like 20-30% more expensive than rx9060XT.,Negative
AMD,Probably an 8gb 5060ti for that price even if the ad says it is 16gb,Neutral
AMD,"I've always just left autoupdate on.  The only driver issue I ever had was a major patch on a flightsim I play that added some new stuff, a compatible version of the drivers were released I a week.",Neutral
AMD,"My budget is not that high about 1500eur, I'm in EU, i wont share exact location im sorry🙏 But I have most items available to buy I'm pretty sure",Negative
AMD,And she gone.  Lasted 39 mins.  Sorry,Negative
AMD,Think it’s like that most places outside of the us. In Canada for a while the 9070xt was the same price as a 5070 but the 5070ti was $4-600 more.,Neutral
AMD,It's the same in the UK Nvidia cards have always been out of my price range. That being said I've never felt the need to question the value of the AMD cards I've bought.   For example the rx5700 I bought at the start of lockdown was a real trooper.,Neutral
AMD,[https://pcpartpicker.com/list/XMKLv4](https://pcpartpicker.com/list/XMKLv4)   Something like this could work.,Neutral
AMD,"Hahah still thank you! Unfortunatly I'm not located in US, though I appreciate the advice!",Positive
AMD,"This definitely looks good!! Thank you so much! It's very kind of you to take time to do this, thank you! I will think about getting it :)",Positive
AMD,7600,Neutral
AMD,You could also opt for a Ryzen 7500F from AliExpress for £115. It's pretty similar to the 7600 with the one major caveat of no built in igpu so you will need a graphics card of some sort for display out from your pc and it won't come with a cpu cooler.   As it's below £130 you won't face any import fees when getting it delivered to the UK.   Ive bought CPUs from AliExpress a few times 5700X3D / 7500F for maximum price / performance builds for a few people and haven't had any issues yet.   Lack of warranty on the AliExpress 7500F is obviously another concern,Neutral
AMD,"7600  You'd be on AM5, which has an upgrade path to better 7000 series to Ryzen 9000 as well as whatever the next gen of Ryzen CPUs on the AM5 socket should you be able to want/need a CPU upgrade.   With Intel, you're stuck with the Core 200 series unless you replace the motherboard for whatever new socket they want to introduce.",Neutral
AMD,If you are mainly gaming then the 7600 hands down. The £7 is worth the performance increase over the 245k,Positive
AMD,7500F,Neutral
AMD,I wouldn’t say for 7£ the obvious answer is intel. Thats nothing and its slower for games.,Negative
AMD,"For what exactly?  You should really do an entire build comparison because a CPU vs CPU price point isn't the whole picture  Intel mobos can be cheaper depending on the feature sets you want as it's a very unpopular platform  It can also be more expensive than similar featured older am5 boards if the specific criteria are met  Personally I favor the ultra series for thunderbolt5, for the iGPU utilization in secondary builds after the main rig is retired and replaced for encode/decode in Linux on site smart home/camera footage storage, video preservation etc but that's not exactly a common usecase",Neutral
AMD,Tom's hardware has the 245k as slightly better for gaming,Positive
AMD,I would do something like that The only issue is that i plan to get a gpu later and to use integrated graphics for the time being,Neutral
AMD,https://www.techpowerup.com/review/intel-core-ultra-5-245k/19.html  What performance increase? They are perfectly tied.,Neutral
AMD,Ah that's a shame there's a discount to knock off another £7 that can be applied when ordering too.,Negative
AMD,You could always nab something like a £10-20 used GPU off ebay or CEX (like a GTX 960 2GB or R9 280X) to bide you over.,Neutral
AMD,According to this gamersnexus no there not. https://youtu.be/WxXZlONu4Ig?si=zyVXOsIwZ6XxiANP they trade blows.,Negative
AMD,you ve been finessed to buy a gpu off of him.. the funniest thing is that IN FACT RTX cards at least the high end ones get burnt because of their idiotic cable solution.,Negative
AMD,"OP, are you interested in bridges? I have one I can sell you.",Neutral
AMD,"Guessing he either gets paid commission or he makes higher margins of the 5060 ti 16gb given the much higher price...  Guy finessed you, ignore what the salesman says in store, they're wrong fairly often.  Last time I went to a store, the guy who ""builds all the PCs in the store"" said that each GPU model makes a huge difference in performance... I was talking to him about why I'd get a used asus tuf 5070 ti for the same price as a new 5070 ti from PNY or something, he said the asus tuf 5070 ti competed with the 5090 lol.",Neutral
AMD,"Ya boi was just getting a sale off you using what he had available and you got got.  I've been on ATI/AMD cards since like 2010 when my 8800GTs died, and I've never had one fail. Hell, I used my Radeon VII for mining til it paid for itself and a 5950X, then sold it for near double what I paid, then the guy I sold it to mined with it for a couple years, aaaand it's still fine and I know this because I bought it back.",Neutral
AMD,He's fucking lying.,Negative
AMD,The only GPUs that grill their cables are high end Nvidia GPUs. AMD doesnt have that issue,Negative
AMD,"If any cards are going to be ""burning"" It'll be the NVIDIA ones.",Neutral
AMD,"Its Nvidia cards that can have a problem with burning because of the connector they use but its still fairly uncommon. However the fact that he told you to buy Nvidia because AMD cards are susceptible to this fault, dude lied to you.",Negative
AMD,he upsold you. 9060 not catching on fire anymore so than a 5060.,Neutral
AMD,"That might have been true 5-10 years ago, but modern AMD cards are really fantastic value.  Save $100 and get the 9060xt 16gb, its on par with the 5060ti in terms of performance and will have zero issues with 1080p or 1440p gaming.",Positive
AMD,"Dunno, but at least from what I’ve heard 9060xt is better value than 5060ti at MSRP.",Neutral
AMD,mine 9060xt doesnt even reach 60 degrees celsius.... nvidia glazer scammed you,Negative
AMD,"Return the RTX. Dude straight up lied to you. The only known cases of modern Radeons burning so far are a couple of Sapphire Nitro 9070xt. The model that uses Nvidia’s 12v high power connector, which has been known to be prone to burning when used in high end cards since they were first introduced.",Negative
AMD,Return it and get a monitor lol,Neutral
AMD,Never listen to anybody who randomly walks up or even store employees. Lol,Negative
AMD,“that product I currently don’t stock and therefore can’t sell you definitely has problems. You should buy this one I conveniently have available instead and spend more money with me.”,Negative
AMD,"Yeah he was definitely trying to sell you a more expensive GPU. Low to mid range cards don't usually have issues burning regardless of vendor because the TDP isn't that high. Although you may want to consider the 5060ti if you can spring for it. DLSS4 is highly adopted throughout the gaming world and still looks better than FSR4 in most titles(not by much but still). End of the day, Nvidia has better ""tech"" for their cards. 9060xt is still very good though.",Neutral
AMD,You got got,Neutral
AMD,I've never seen an AMD card melt its connector.,Negative
AMD,"LOL fuck no, its the other way round.  The power connector design on more recent gen nvidia cards is well known for melting, even though its pretty rare there are more than enough photos and videos around of unhappy nvidia owners with melted connectors.",Negative
AMD,I watch a lot of GPU repair channels and I've been put right off AMD cards. The repair guys say they get the weirdest issues or have problems that take forever to diagnose.,Negative
AMD,Return it,Neutral
AMD,It's the other way around. Nvidia cards use 12VHPWR connector that has a tendency to melt and brick the card. This is pretty well documented.,Neutral
AMD,The guy was lying,Neutral
AMD,"You 5060Ti is probably the better card for a number of reasons.  Don't feel too bad, unless it cost alot more.",Positive
AMD,"Neither of those cards will catch fire but contrary to all the comments here, there are a lot of limitations to AMD cards if you aren't just gaming.",Neutral
AMD,Sometimes AMD drivers are difficult for me but he pretty much got ya. The RTX 5060 Ti is a better card IMO.,Positive
AMD,"Have had a 7800XT for just over a year now, guess how many problems/issues I've had with it, if you said zero, you're damn right",Negative
AMD,"NVIDIA is the one that have melting connector issues LMFAO, their 5080,4090,5090 have these issue, AMD was known for driver issue back then but it was solved, nowadays their issue is their card aren't the best for productivity work and the amount of games supported aren't as many as NVIDIA card",Negative
AMD,Well you wouldnt think to make this post BEFORE you got finessed?,Neutral
AMD,"Return the card and get a 9060 XT 16 GB at a different store. These people don't deserve your money.  And as others have said, the only cards that are going up in flames are those using the 6x2 12VHPWR connector or whatever that's called nowadays, and those are basically all Nvidia.",Negative
AMD,"Wrong move, guy just wanted to sell you something most likely",Negative
AMD,"lol salesmen doing salesmen things, you got taken. return it.",Neutral
AMD,"I wouldnt exactly say you got swindled since both GPUs are in the same tier. But the 9060XT/5060Ti cards do not have a peak power consumption high enough, or the fire hazard cables to start burning up.  What I wanna know now is how much more did he charge you for the 5060Ti 16GB over the 9060XT 16GB because the latter is supposed to be the cheaper card. Also, I hope he was at least honest enough to actually give you the 16GB variant over the 8GB",Negative
AMD,"I got a top end amd card with more memory than the then current RTX card that cost less than half the price and has nearly identical performance, excluding ray tracing. I'd say worth it.",Positive
AMD,I just got a rx7900 xtx just because of the 24g v ram. I play heavily modded games. And with the ram Armageddon happening I don’t think we’ll see any card with over 16g under 4000$ for a long time.,Negative
AMD,"Hey, the 9060xt 16gb only uses one 2x6/8 pin connector. It’s quiet, cool, and has a low power draw. I have one and it’s not struggling at 1440p although full disclosure I had to drop some settings to medium/low in borderlands 4 for acceptable fps, but it’ll run high at 1080p.  Its price to performance ratio can’t be beat; I recommend the card to anybody on a budget.",Positive
AMD,of course he was lying ... they gotta move product to make money.,Negative
AMD,"Actually, I think it's a good choice. Many people are now crazy about AMD graphics cards because of their price and pure power, but if we look at the modern gaming industry, many titles are designed to work with DLSS or FSR out of the box. And DLSS still produces a better image than FSR. A nice bonus is RTX, which drops the FPS too much to use, but you can admire it a little.",Positive
AMD,don't EVER listen to reddit users about PC parts.,Negative
AMD,"I've had two Amd gpus past 3 years a RX 6600 and RX 7600 both were plagued with driver issues and I had to use display driver uninstaller multiple times because of driver timeouts on certain games, sometimes I would have up to 5 timeouts a session usually at worse times in competitive games like Counterstrike lol, Great performing cards but the driver issues were a nail in the coffin for me, now when  all is said and done I could’ve just had terrible luck but now currently running a 5070 ti flawlessly. Never ever heard of AMD cards burning though..",Negative
AMD,"Not burnt. They seem to have other weird issues, though. Coil whine, HDMI turning off the pc issue, artefacting, sudden reboots, fan bearings noisy...    Just my personal experience from two cards, so not really representative.",Negative
AMD,Highly doubt he did it for a better sale.  There are legit NVIDIA fan boys. Most likely case he’s one and he’s probably just sold you his prejudiced idea more than anything.,Negative
AMD,"Nah, it sounds like he wanted to get you to buy the more expensive card.  If anything nvidia gpus have been melting lately because of that new 12 pin power connector they've been pushing on their most expensive cards.",Negative
AMD,"I've only owned 5 dedicated gpus in my life. A r9 390, which I replaced with an Rx 580. Then I replaced the 580 with a GTX 1070 which ended up dying within a year or two. I swapped my Rx 580 into an old PC and it still works perfectly, honestly better because of modern drivers. I now have an rx 5700xt and rx 9060 xt. All of which still run perfectly. I'm never buying Nvidia again. All my amd cards got far more abuse than my 1070 ever did.",Neutral
AMD,Get the amd I have 2 6900xts 2 6800s 1 5700 and a 9060xt zero issues with all them,Positive
AMD,"My 5700XT has an overheating problem that exists across all of them as far as I understood from my initial googling years back, but it's never burnt. Like other commenters have said, RTX cards have been pretty notorious for burning the connector cables from the power supply--bit of a fire hazard, I would think.",Neutral
AMD,If that was true no one would have AMD cards. I have one and it's great.,Positive
AMD,"Burn? No. Some models have more problems than others? Yes. But the situation is much much, absolutely infinitely much better than it was during the polaris days, when you either lucked out or you didnt. Current amd gpus dont have many problems, and I know I will be downvoted only for this part of the comment, but except sapphire gpus which have very very bad quality control. Before you reply with sapphire good blah blah, just look at the most problems reported on reddit with amd gpus, most of them are sapphire cards. And I stand by this, having owned 2 of them. They do not check paste or mounting pressure correctly and it screws with the experience later. Avoid buying from them and you are golden with an amd gpu.",Negative
AMD,U got burned dude,Neutral
AMD,"I agree with the majority of comments here. However, I can also say in the last... 2 decades I've been using, building and upgrading PCs. I was all AMD cards, intel chips. Only in the past 5 years did I switch to an AMD chip and Nvidia card. I don't know if it's the card mix, or one or the other in particular. But so many weird problems I would have on random games (seemingly gpu problems) no longer exist.   I used to get so many impossible to solve, intermittent, moderately annoying issues. The last 5 years with my PC have been lovely.",Neutral
AMD,no,Neutral
AMD,Google any GPU and add crashing to the search. That's the issue with PCs. Every component has issues. It just depends on if the one you receive has problems.,Negative
AMD,"Agree with the rest here. Burning issues are caused by (afaik) any variation of these stupidly undersized ""high performance"" power connectors. The AMD 9000 cards are also better value right now and AMD isn't such a bootlicker for Palantir (although just barely better as a company).",Negative
AMD,Same thing happened to me. I went in for the same gpu and the guy tried to upsell me to the 5070ti,Neutral
AMD,The real life lesson is never trust what a salesman says,Negative
AMD,"I'd keep the 5060ti, if you can afford it, for a couple of reasons. It's a bit more performant than the 9060xt (around 10%), it has better upscaling and framegen (if that's something you're interested in) and better raytracing performance. Neither card is going to burn because both have very low power draw.",Positive
AMD,"I ran nvidia until this year, then switched to AMD because of the RTX burning issues.",Neutral
AMD,Lmfao the nvidia cards are the ones which burn,Neutral
AMD,"He did you a favor, but for the wrong reason",Negative
AMD,"I got an rx6600 when building my stepmom's PC, and we had nothing but issues with it. Any Bethesda game wouldn't run right. Skyrim wouldn't load into the main menu, Fallout: New Vegas couldn't render half of the models. I don't remember what other games had issues, but almost nothing worked. We spent weeks messing with settings and drivers before giving up and buying a used 3060. I don't know if I got a bad card or what, but I saw a bunch of posts about the same issues. I'm not saying this guy wasn't pulling your leg, but at least for me, Nvidia is the better one. (I do a lot of AI work too, so I do have a little bias due to compatibility with my software)",Negative
AMD,Yeah there's a real confirmed problem: Lower sales margins for the store.  You've been had lol.,Negative
AMD,I have a 9060xt and its awesome for me. Crushes 1080 and does surprisingly well with 1440. One day I'll move on to a 9070xt or an xtx when it comes out but the 9060 was a great deal for my build.  I was tempted with the 5060ti but the price difference for slightly better ray tracing was not worth it for me,Positive
AMD,> luckily they didn't put the gpu in the pc yet  i would not trust them to build a pc with how they tried to scam you.   there is a beginner's guide in the sidebar and this whole subreddit to help you build it yourself.,Negative
AMD,"been using a old 5600xt since it came out, did have driver crashes at launch but last few years i havnt had many issues really, i game every single day, at most maybe 1 adrenaline software crash a month if even that.  Ive also noticed, if adrenaline crashs, my (gpu)fan curves get defaulted to what appears to be a quiet mode, and fans barely work at all(will cause excessive heat) but as long as you apply your fan curve my temps never over 80 degrees while 100%(and is overclocked with xfx vbios update)  I Think amds pretty solid now adays i really dont even have to worry anymore, i just make sure my adrenaline profile is loaded and go",Neutral
AMD,Rtx cards are the ones that burn do to the 12vhp connector. amd Cards typically dont have them,Neutral
AMD,You’ve posted this same question in no less than 8 subs. Are you just trying to earn karma or what?,Negative
AMD,"Another shitty deceptive salesperson taking advantage of ignorant customers, it seems.",Negative
AMD,Was that the admin of userbenchmarkdotcom? lol,Neutral
AMD,"Lol, dude you got finessed. The only problem I've seen with AMD cards is high VRAM clock rates because the refresh rates on multiple monitors are different or they have bad blanking times.   [https://www.reddit.com/r/Amd/comments/14t7otk/ive\_finally\_found\_the\_source\_of\_the\_issue\_with/](https://www.reddit.com/r/Amd/comments/14t7otk/ive_finally_found_the_source_of_the_issue_with/)",Negative
AMD,"He needs to mention what problems, if say its the latest drivers causing games to run in low fps and all. It does not happen to rx9060xt only those higher end cards do.  Source: I have the same gpu",Negative
AMD,I heard about these problem for AMD cards are mostly drivers issue. BUT AGAIN that was like what? 10-15 years ago ?,Negative
AMD,"What a scamm. Tricking you to buy rtx insted of rx. Rtx GPUs like rtx 3000 have burned chips in 2021 becuze of a gsme and 4080-90 to 5080-90 still burn out today when power cable not plugged in correctly. Amd CPUs had issue with soc voltage too high and issues to burn up becuze of asrock/asus motherboard issue not CPU. Intel CPUs 12900k (13th 14th gen included)and equal wattage have been killed by micro code they got fixed in 2025 that late. Rtx have driver issues more than amd but got more games that support frame generation. Amd have better price to performance and ray tracing not even playable on rtx 5060ti or below (same for amd 9070 and below). Dlss4 have more game support too makes it ideal if you need a gimmick in a game, sure its good and I have tested it out but blurry when going below quality. Nvidia smooth motions I tested on my cousin 5070ti sucks vs my 6950xt afmf2.1 when we did test for nightreign that have 60fps cap and we used frame generation driver based thrn amd wins for clarity and stable 120fps.   5060ti-16gb sure its 10% faster than 9060xt 16gb but price diffrance is not worth depending on what you will use it for. Story games sure go 5060ti-16gb but all round then 9060xt price to performance better pick when pair ith with amd CPU becuze of SAM.",Negative
AMD,"The only realistic ""problem"" for AMD cards, is the lack of CUDA, and maybe some rendering problems in old games.  For everything else, they work just as good.",Neutral
AMD,They both have issues and burn.,Neutral
AMD,"Only the older AMD cards had problems, and most of them were driver issues too. The 5000 series was one of the most notorious for problems but by 7000 they had most of the issues sorted and the latest generation doesn't have many issues besides the occasional lemon. There have been some issues with the 90xx cards that use the 12 pin power cable, but even those issues are pretty rare and are usually user error.  I have a friend who got a 5700 XT and had such a bad experience with it that he swore he will never get another AMD card and will never recommend them to anyone else, which with his situation was completely fair. I guarantee if he used a PC that has a 9070 XT he'd enjoy it more than his current PC with a 3080 Ti, but I doubt he'd enjoy it enough to trust AMD again.  There's a pretty high chance that the employee was just trying to get an easy sale since they get commission on some products and would make a bigger commission selling you a 5060 Ti PC than he would selling you a GPU-less PC. There's also a small chance that he was someone who bought a 5000 series AMD card and got burnt like my friend did.",Negative
AMD,"No but there is an issue with windows 11 being trash and Nvidias stupid power connector burning up due to it not being made correctly. I have both a rx 6750xt and a 7900xtx. The 6750xt is in its original box incase I need too test a setup or a backup card. I'm sticking with my current gpu 24gb I don't care about any of nvidias crap the past two generations. Stopped playing a majority of games after 2019, MegaBonk is one that is newer I'll play but it's not really intensive on graphics.",Negative
AMD,Went from a 3080 to the 7900xtx the driver support and innovation from amd has come a long way I think the nvidia boys will start to move soon most of the features for amd works at the driver level not software level so it’s just really nice to not have overhead and amd anti lag 2.0 eliminates latency on v sync and most other things from what I notice when you have any of them on with anti lag instead of getting tons and tons of delay it’s only about 2-3ms more so it’s crazy good,Positive
AMD,Never ever get advise from a tech store staff. Most of them get promotions from specific brands or products.,Negative
AMD,"It’s not a gpu issue, it’s the cable and how many watts are drawn. Either way you got a modern gpu, have fun",Positive
AMD,"I just got my first AMD GPU this time around with the 9070XT. I can confirm that most of the time it works without issue, but you will run into games that do not like AMD GPUs. Two games I've run into issues with GPU crashes and hangs where when tested with my 4080S it had no issue, Fallout 4 and Arc Raiders. Outside of those two examples, it has been smooth sailing. Arc Raiders is known to have issues with AMD GPUs. Devs and AMD haven't really done much to rectify the issues which is making me think of switching back to NVidia in the future.   I do think that as more devs adopt AMD features that it'll eventually be the same experience with either, but as of right now there are still some issues with AMD that I have never had with NVidia.",Neutral
AMD,"I’m still rocking the 6950XT. Never got a lot of press when it launched, maybe because it smoked the 3090 for a couple hundred less but who knows",Positive
AMD,I've had a couple of AMD cards with zero issues.&#10; Probably trying to get you to buy a more expensive GPU.,Negative
AMD,AMD GPU? Get burnt?? Unheard of.,Neutral
AMD,Only problem with AMD line up is they have no response for the 5090,Negative
AMD,AMD only has their age old driver problems.,Negative
AMD,Had 1 AMD card in my life and it burnt. My2nd ever GPU was a rtx1660 and never ever had a issue with it,Positive
AMD,"Glad you could cancel it, actually RTX cards are the ones who get burned.  I got a RX 470 since 2018 and it still works really fine, so I can assure you AMD cards are as hard as steel.",Positive
AMD,"Return the RTX and get the RX. I own both Nvidia and AMD gpus for years. Recently, I have decided to boycott Nvidia for their business decisions on pricing, customer service to gamers, and their response to the cable adapter defects. Sure the ray tracing and lack of dlss is an aspect of AMD, but the price to performance is better.",Negative
AMD,"I have 9070 XT and I am very tech -oriented so I go with that. I also built my friend a PC with 9070 XT in it, because I know he is between illiterate and modest to PC's. But for anyone who is illiterate to PC's I recommend them  Nvidia's just because their drivers cause less hassle for them.  So ask yourself, are you illiterate or modest with PC's, if illiterate keep Nvidia, if modest go with AMD",Positive
AMD,I've been using an RX 6650 XT in 1080p ultra since 2022.,Neutral
AMD,"I think you might've found the guy who runs UserBenchmark in the wild. AMD might not hold a candle to NVIDIA's top-performance cards like the 5080 and 90, but their mid-range ones like the 9060 XT tend to punch above their price point. The contest of ""worst drivers"" changes hands from time to time, but overall the notion that AMD cards have bad driver support is a decade-old accusation that even then didn't really hold water to the extent it was described. I used AMD GPUs all the way from 2012 up to 2023 without any major problems, and earlier this year I got to experience some of the instability that NVIDIA's v591 driver caused with the 4070 Ti I got a few years ago; I ended up having to roll back to an earlier one to stop getting random black screens. This also isn't to say ""NVIDIA bad"", but just to show that there's no perfect brand. The 9060 XT also just doesn't draw enough power to really be a ""burn"" hazard.  Generally speaking, if you don't know what CUDA is and why you might need it, then the 9060 XT is a great option for the price. If you *do* know that you need CUDA, then you'd usually want a more powerful card than the 5060 Ti in the first place to best utilize it. I will note that NVIDIA still has better ray-tracing performance, so you can weigh that to determine if you care about it for the increase in price.",Neutral
AMD,"For gaming it shouldn’t matter. Imo nvidia is superior. I wouldn’t buy amd l, just because it’s never worked in the equipment I use. Everything is intel chipsets and nvidia gpu. We tried putting in new parts to our machines using amd and it wouldn’t work right. Software was slow as hell and the gpus gave errors. Again not gaming computers but equipment/machines at work. But since it don’t work there I’ll never buy it because I use the software at home and it causes conflicts. I’ve never seen any mfg equipment at all in 30 years come with amd chips or chipsets. Always intel and nvidia.",Negative
AMD,I have a 9060xt 16gb runs smooth and cool..this dude wanted to sell u that 5060,Neutral
AMD,"Not true, but you got the better GPU anyway, so as long as you didn't overpay a lot it worked out in the end.",Positive
AMD,"Rare to happen for either brand but idk why everyone is glazing AMD so much these days.. out of the multiple pc’s I’ve built and owned, RTX cards have been WAY better in my experience over AMD.",Neutral
AMD,Yeah I'm looking into a 6750 and catching hell reading the reviews lol,Positive
AMD,"Realistically it’s not just an Nvidia issue, it’s a cable issue. Any card using that damned cable has the possibility.",Negative
AMD,Love the 3 beefy 8 pins feeding my 9070XT. Looks great with braided extensions.,Positive
AMD,"Only issue i have with my 6750 is just trying to get adrenaline to pop up and then sometimes afmf can be wonky to get ""working""",Negative
AMD,I think there was a sapphire 9070 card that used the burn cables but idk if there was a 9060 one using them,Neutral
AMD,"Rtx burn, 9070xt also burn. Both cards fire hazards lmao. Any card that uses the 12vhpwr cable is a risk…",Negative
AMD,5060TIs do not have 12VHPWR connector.,Neutral
AMD,There are AMD cards with the 12VHPWR connector as well,Neutral
AMD,Is it made out of burning cables?,Neutral
AMD,SLI bridges could make a comeback.,Neutral
AMD,It's any GPU that uses those 12v cables. There was a post on here of a 9070xt with a 12v connector that melted.,Neutral
AMD,"There are less examples of AMD 12V Cards burning, but they exist, it's not only nVidia",Neutral
AMD,A few of them do but only the ones that use the same connector nvidia keeps trying to force down our throats.,Neutral
AMD,"Idk, it’s still rather rare",Neutral
AMD,"Biggest lie of the century, the primary issue of AMD gpus is driver issues",Negative
AMD,"Its any card using the 12vhpwr, which several AMD cards from various manufacturers have.",Neutral
AMD,"Very good lesson in PC Building and maybe life regardless.   What was bad 10 years ago, may not be bad now. What was good 10 years ago, may not be good now. A decent chunk of what you encounter on the subject tends to be either dated or user error and is only relevant if you're noticing patterns on a specific model.   I remember back in 2013 when I built my i5 4670k Haswell build, ASRock motherboards were gold standard Seasonic level for quality motherboards, AMD CPUs were bug-riddled and unstable, and after I built that PC I was so happy with its stability that I swore I'd never touch AMD again.   When I built my 5700 AM4 build, everyone swore by Ryzens recent reliability so I gave AMD another chance with a 3600. Barely unstable that I couldn't complain. Had you asked me then, I'd probably have said ""yes, another AMD build that's not as stable as my previous Intel/Radeon build."" Eventually figured out that the culprit was the 5700, not AMD CPU when I upgraded to a 4070 Ti Super that had not one instability issue in 10 months.   When I built my AM5 build (it's too recent to really report back), ASRock was the only motherboard vendor that I refused to touch and 8 months prior Intels were cooking themselves prior to the microcode fix (largely resolved and Intel handled it very well with affected customers).   Further, on the AM4/AM5 build I've ran cheaper, non-TUF ASUS motherboards... which I've seen some people knock as being not great. My ASUS motherboards have both been phenomenal, but they also were better chipsets (Prime X570-P and B650E Max Gaming) so I cannot speak of their other chipsets. Again, sometimes advice is just flat out bad.  In these current generations, I'd have no problem with AMD/Intel or Radeon/nVidia. Just be careful with especially 12vpwr (I have one and had no issues) and 12v 2+6 and make sure to thoroughly inspect each square in the power connector with a flashlight for plastic manufacturing residue before plugging them in and that the power connector is fully engaged. The PSU manufacturers seem to have caught on, but it should be a concern.",Positive
AMD,"OP already splashed for 5060ti. I think a return is in order.  Also wtf is a 90660xt, get me one of them rn.",Negative
AMD,What's your experience like with amd drivers?,Neutral
AMD,"If anything, much as it sucks that consumers have to bear the burden of it, I'm glad at least one AMD card uses the 12VHPWR connector, because it serves as conclusive proof that it's not a platform-specific thing; it's literally just an awful connector. I've seen a few people argue that the cable only burns because NVIDIA cards are *so very* capable of bigger and better things - a ""suffering from success"" situation (which is still a stupid defense) - and while it's true they have the performance lead in the high end, the fact that even a high-powered AMD card can still have the connector melts proves that 12VHPWR needs to be taken off the market, pronto. It's simply not fit for purpose no matter who's using it.",Negative
AMD,It's happened as only a few models use the new HPR12 pin connector like nvidia does.,Neutral
AMD,"To be fair this was nearly 15 years ago, but I had an AMD card in a pre-built Dell desktop that would occasionally lock its HDMI output at 640x480 for no reason. I'm not sure if it was a weird driver issue, but I never had this problem on Nvidia or Intel graphics even with the same monitor and cable(s).",Neutral
AMD,That's what I'm afraid of. Does Nvidia have less driver problems? And is that enough to go Nvidia?,Negative
AMD,Amd vendors have a few cars that also use that connector which have also had the same issue as nvidia.,Neutral
AMD,Nice! And do your run into any problems with amd drivers?,Positive
AMD,Like what? I don't have them.,Neutral
AMD,Nice! Also are the drivers really bad in amd?,Negative
AMD,Might be cause of personal preference and nvidias anti consumer behavior recently. But same experience still prefer nvidia over amd with the dlss and mfg.,Neutral
AMD,"I've had a 6750XT for the last few months. It's been phenomenal. I also only paid $200 USD for it. At that price, you literally can't beat it. I am incredibly happy with it.",Positive
AMD,"I have owned one for 3 years atp ,  It is good , peak intermediate what can I say more than that , unless you can buy a 4060 at MRSP, it's the best GPU at that price range , super easy to disassemble if you want to change thermal paste, never had a problem.",Positive
AMD,"Try looking into the 6800 instead. I don't know what the prices are like in your neck of the woods, but 6800 kinda flew under the radar.   At the time of release it didn't offer as great of a value compared to 6700XT or 6800XT so most people went with one of those, but in the used market its a gem.    It's the most underclocked and cut down version of the full sized Navi 21 chip which might sound like a bad thing, but it retains full 16 gigs of VRAM and was the most power efficient card in the entire gpu generation, both AMD and Nvidia.   That means it's fairly quiet (depends on the manufacturer ofc), has decent headroom for overclocking and chances are it'll last longer as it isn't driven so hard.    Fair disclaimer - I'm obviously biased because I own one, but I couldn't be happier with it. I've bought it used few years ago just after the crypto fever cooled down (supposedly it wasn't mined on but... yeah right).   I considered getting 9070XT but honestly I don't play that many new games and 6800 serves me just fine. It's pretty close to 9060XT *in raster* performance. If you care about RT, path tracing, FSR or now Redstone it fails badly, but in raster performance its a great value proposition.",Neutral
AMD,"I got a 6800 over a year ago and its been great, coming from a 3060ti. Love the 16gigs of vram and I got it for 330$ from best buy and sold my 3060ti for 230$. Its a huge 3 fan xfx model too and runs like a beast.",Positive
AMD,"A lot of reviews don't take into account the current pricing of them, a 6750 at $550 is a very much different value than at the $250-300 price they are used currently. Sometimes it can be good to look into a newer lower end gpu that they might compare to the 6700/6800 for a better idea of it's performance compared to brand new $300 cards.",Neutral
AMD,Love mine. Rock solid. Performance is great at 2k resolution. Paired with a 5700x CPU.,Positive
AMD,Yep and there are AMD cards that have that same power connector,Neutral
AMD,I'm the upcoming GPU economy?,Neutral
AMD,"Yup, I've seen some Sapphire cards that now use that connector, and also there are pics online of some of them being damaged now too :).  It's still rare, but unless you have to take a risk, why would you?",Neutral
AMD,"To be fair, most of those have come from pig tailing adapter as far as I’ve seen. Still a fucked situation but I don’t think people realize how much juice runs through their PC, hell I know I don’t but I know it’s enough to snap crackle and pop",Negative
AMD,"Yes. It's not an Nvidia OR AMD issue, it's a garbage connector issue.",Negative
AMD,because they have 1/10th of the market share  of course you’re going to see less amd cards burning???,Neutral
AMD,"Ok yes, there is like 1 or 2 specific versions of 1 AMD card that use that connector. And then then you have to get extremely unlucky for such a comparably low wattage GPU to actually fry the cable.  Meanwhile every upper end Nvidia GPU uses the connector and many have a higher wattage",Negative
AMD,It is rare but they're the only who has that possibility.  He isn't lying.,Neutral
AMD,"And lately, the ""bad driver award"" has been going to NVIDIA. Their drivers a few months ago were dogshit with all sorts of bugs.  The ""AMD drivers bad"" trope is pretty much ten years out of date by now.",Negative
AMD,Linux folks would argue with this,Neutral
AMD,Are we just going to pretend that every single nvidia driver releasee in the past year hasn't been botched in one way or another?,Negative
AMD,"Not several, exactly two; both 9070 XT models: The AsRock Taichi, and the Sapphire Nitro+.",Neutral
AMD,"Just my 2 Cents for the TUF ASUS mainboards. Running a TUF GAMING X670E-PLUS since 2 Years and no Problems at all.    Specs going 7800x3D with a 7800XT. I just had the Problem with the 7200Mhz Ram, that resolved a couple of Day ago after a Bios Update and some tweaking and Bios - Rabbit Hole ;)    And a tip for all: Do NOT change the fucking Chassis Interuption Option, brought me to a Loopwhole .",Positive
AMD,"I fixed the ""90660xt"" before you even posted the comment, you were fast on the draw though lmao.  But yeah, that ""salesman"" did rake him over the coals, the 9060xt is very close with the 5060ti for gaming performance. Shame on that guy.   Generally though local repair shops do just suck due to margins being so razor thin they need to ""oversell"" and bend the truth to maximize their profit. He probably was able to make more on the 5060ti sale this time so now its ""AMD cards are bad"".  He either is painfully uninformed on the current generation of cards or is a bold face liar, as a computer tech I don't really know which is worse lmao.",Negative
AMD,"Apart from Cyberpunk path tracing crashing, it's as good as was my Nvidia GPU. One minor instance where it's better, is that when the PC goes to sleep (the monitor turns off) and I immediately wake it up the screen won't get messed up. I had a 4070 where it would go into a weird aspect ratio, and only a restart fixed it.   Since we here, anyone reading it can confirm if it's fixed, if you have the time and can make out what I'm talking about?",Positive
AMD,"I have no issues at all except with battlefield 6 , but a lot of people has issues with that game even nvdia users",Negative
AMD,Look at Northwest repairs on YouTube he talks about it at length and it's why he refuses to repair them any more,Negative
AMD,"You spent more money most likely, but it is a better card anyway. Yes, no driver problems. I would rather have the Nvidia card. Budget constraints would be the only reason I'd go AMD personally.",Positive
AMD,"Sometimes but I wouldn't find it to be outside of the norm, I'm a streamer and mainly play indie games so I'm expecting things to be buggy.",Neutral
AMD,"I had a problem recently with the Q3 security update that caused some system instability and some BSOD, but rolled back to previous and everything worked fine again.   But usually AMD drivers give no problem at all. They have the longest support on their hardware (up to 5 years + security updates after that) and give native support to Linux.",Neutral
AMD,"Yeah I suppose I get that, glad you agree though. Might change my mind one day but that day is not today :)",Positive
AMD,It's better than a 4060 lol,Positive
AMD,I have a red devil 6900xt from 2020.. we won't discuss prices but im still very happy with the card,Positive
AMD,"Yes, would you like to buy a bridge?",Neutral
AMD,"It's rare with the Sapphire cards because the 9070XT doesn't draw enough power to really melt the connector unless you **really** mess up plugging it in, or the cable itself has a massive defect. (For example, the 5090 can draw 550-600W at peak. The 9070XT draws ~340-360W peak.)  The 12VHPWR cable woes are seriously overblown. Most of the issues are user error. That said, any connector that allows for user error that sets itself on fire is badly designed.",Negative
AMD,No more then the rate of that failure mode with the old connectors. The 80 and above nVidias are a real problem tho.,Negative
AMD,"Exactly, I call it an ATX Standard issue",Neutral
AMD,"It's also because every high-end Nvidia GPU uses 12VHPWR, whereas AFAIK it's only two lines of a single AMD card (the 'top trim' models of the 9070 XT from Sapphire and AsRock) that use it. So it's not just that there's 9 Nvidia cards for every 1 AMD card, it's that only like maybe 10% of those AMD cards are the ones that use 12VHPWR.",Neutral
AMD,Oh definitely agree,Positive
AMD,"I maintain that the folks keeping it alive are either paid folks, Nvidia fanboys, or people that have defective cards (as happens with literally every manufactured product, including Nvidia) and instead of just getting a warrantied card, they blame drivers.  Meanwhile, I am NOT saying that one company should be chosen over another. AMD can be every bit as bad as Nvidia. But the whole ""AMD has bad drivers!!"" crap needs to die.",Negative
AMD,"~6.5 years out of date. The last time AMD had driver woes was mid-2019, with the 5700XT launch.  For comparison, Nvidia has had 2 separate times with issues since then, the 2000 series launch and the 5000 series launch.",Neutral
AMD,"Nvidia really had a big brain move like 9 years ago when they finessed r/Nvidia mods to close r/Nvidiahelp and then contain all driver complaints into one megathread. NOBODY sees any Nvidia driver complaints because you'd have to go into the megathread, while AMD owners (and not even owners, *anyone*) can post any AMD driver complaints in r/AMDhelp so visually, AMD has driver issues while NVidia has none. But if you look at how many driver issues the changelog has for both Nvidia and AMD, Nvidia has way more known issues and fixed issues the past 4 years (especially 5000-series when they partially have AI write driver code).",Neutral
AMD,"Fixed, my 4070tiS used to do it, I’m guessing the driver updates fixed it",Neutral
AMD,I saw that it is equivalent to a 4060ti but realistically you won't find that at MRSP.,Neutral
AMD,"It can happen with any card that has this connector because none of them can regulate the amps per wire due to the lack of protection / shunts.   The 12VHPWR spec per wire/pin is 9.5a x 12v = 114w. So any card that draws above 114w is in danger of melting if all the power goes through one wire, and the others are low powered / failing. All wires failing except one is probably extremely rare, but it’s possible.   But of course it’s easier to happen on higher wattage cards like the 5090 because the margin of error is so thin. It’s pushed to the maximum limit of this connectors spec.",Negative
AMD,"Yep, only the Sapphire Nitro and the ASRock Taichi use the connector.",Neutral
AMD,> It can happen with any card that has this connector because none of them can regulate the amps per wire due to the lack of protection / shunts.   Incorrect. 3090's used it and had no issues with it. Thats because 3090 split the cables into 3 and made sure to evenly distribute the power between em. They completly got rid of that design starting with the 4090.,Neutral
AMD,Yea should have clarified starting after the 3000 series.,Neutral
AMD,It'll be masive upgrade lol,Positive
AMD,if you live near a Micro Center you can get it in a bundle with a motherboard and ddr5 ram for 399,Neutral
AMD,Yes,Neutral
AMD,"you'd need new mobo and new RAM, and considering ram prices now, i'd rather not  can't help but notice you have a Z490 mobo with an i5 10400 and only one stick of RAM, interesting choice  i've seen the I5 14400 go for pretty cheap (in my country atleast) and a new mobo (get a B series, Z series are usually if you have power hungry chips and wanna OC)   also you want 2 sticks of RAM for best performance",Neutral
AMD,"I think you should configure your CPU better. Run a stress test with CPU-Z and see what frequency it stabilizes at. Since you have a Z-series motherboard, you should be able to get the most out of that CPU. It seems to me you left it at factory settings and didn't change anything in the BIOS. Let me know the results and I'll help you.",Neutral
AMD,"The question isn't if it's worth the upgrade. It's a question of is it worth it to you to buy a new DDR5 RAM kit at inflated prices, along with a new motherboard and CPU.   If that's within your budget, yes. The CPU Comparison is day and night, especially with that 9060xt backing it up.",Neutral
AMD,just wait out the craziness in the ram prices.,Neutral
AMD,Sadly I’m from Europe. That seems like an amazing deal! Here it will cost me 550€-600€ to get the same I think :(,Negative
AMD,"I bought one stick to save some money in the beginning, thinking I would buy a second stick soon. Well, until now I never felt the need to do so and now they’re 4x as expensive :|.",Negative
AMD,All cores stabilise at 4000 MHz according to CPU-Z. Thanks for your help!,Positive
AMD,"then it probably isnt worth it for you, an LGA1700 or AM4 upgrade would make more sense for your budget",Negative
AMD,Mindfactory in Germany though,Neutral
AMD,"yea fair enough i bought ram for my homeserver and when i wanted to buy some for my gaming machine poof, prices skyrocket  still, an I5 14400 and a new mobo ( make sure it's a ddr4 mobo so you can reuse your ram) should give you a great uplift",Positive
AMD,"There aren’t any bundle deals from Mindfactory, right? And since they’ve been insolvent, their prices haven’t been that great I think.",Negative
AMD,"Yeah, I mainly meant that you could get the 7600X3D from there (I have one). It’s not fully a microcenter exclusive in that respect.",Neutral
AMD,I've had mine for about 6 months. It's been great so far and I'd consider getting another AMD card in the future. I've exclusively used Nvidia cards for over 2 decades before I bought this card.,Positive
AMD,Immaculate. Honestly 0 issues with anything,Positive
AMD,I have a non XT 9070 and it's been nothing but smooth sailing. I did have one GPU crash but I think that was a Linux specific issue.,Positive
AMD,"I've been using both AMD amd Nvidia simultaneously for the past 7 or 8 years.  I've never had any significant issues with either.  From 1060 to 4070 with Nvidia and R9 290, 5600xt,  6900 and 6950 with AMD(current main rigs for me/spouse).  DLSS has been better than FSR until 4, but i play almost everything natively at 1440p on the 6950 and have had nothing but great results.  Fwiw, I prefer the AMD cards now because of better Linux performance in general but will continue to use Nvidia in the laptops until AMD has a good option again.",Positive
AMD,Your colleagues are about 10 years out of date,Neutral
AMD,3080 -> 9070xt. Massive performance uplift. Only real driver issue is the fact that adrenaline is super slow to load up and every once in a while the recording software turns itself for an unknown reason.  Very rare that I get crashes or anything of the like.,Neutral
AMD,"I play a fair bit of games. I've had 0 issues running stuff on ultra settings with ray tracing on. The one game that won't work no matter what I've tried is fallout 4. Seems like a bit of a driver issue and the game being unoptimized garbage is the fault. Anything else has been fine. Control, Forza Horizon 4 + 5, doom eternal and so on.",Neutral
AMD,It's a monster,Neutral
AMD,"Unfortunately I'm one of those folks. I've had a 970 4GB GPU for the past... 15 years? I put it into the brand new PC I built last year and have used it up until now, flawlessly. I bought the AMD 9070 XT, DDU'ed everything Nvidia from my PC (in safe mode and everything). Installed AMD's drivers and Adrenaline. But almost every time I play Arc Raiders it crashes. It really sucks, because as far as performance is concerned I'm loving the card. But coming from literally zero issues to a crash per almost every AR gaming session...  The game played flawlessly with my 970, though obviously with trash graphical settings haha. How does the 9070XT do with other games? Honestly I haven't dove in yet on other games to see if it also crashes there, or if it's UE5 causing the problems.",Negative
AMD,Love it. The best thing is that there are many ways to get FSR4 to work on games that don't currently support it. It's also whisper quite and cool.,Positive
AMD,Just got a new rig with a 9070XT after upgrading from a 7950 (that lasted 12 years without issue too)  Perfectly happy with it so far,Positive
AMD,Red Devil in the house.  No issues here.,Neutral
AMD,"From a 2070 Super to 9070XT, this puppy can render so many frames! Been mostly playing MH: Wilds. The 2070S could do Low-Med @50 FPS. Now I'm playing Ultra 110 FPS. (No ray tracing, No FG).   I haven't set it up yet, but I see it does have a replay system to record those sick moments. It's something I used alot with Nvidia's software. So I'm glad I didn't lose that with the switch.   I chose the reaper variant just so I can sleep at night knowing the power connector won't melt on me. I've been told the 9070 XT doesn't draw enough power for that to be an issue, but why risk it.   Something it can't do well is switch emulation. It has the power to pump out frames but it struggles to compile shaders while playing. Specifically Xenoblade 3, the 2070 S can play it no problem. But the 9070 XT has a bunch of artifacts and studders. Even when I change from Vulcan to openGL and removing old shaders. It's a shame because I play switch games on my computer alot.",Positive
AMD,"Amazing. My issues have largely been with Windows, not AMD.",Positive
AMD,"Not nearly as great as people have made it out to be. Performance was great, and many games were fine, but I had seemingly random crashes at least once every couple of days or so. They seemed to happen more often after waking from sleep instead of fully restarting and whenever I would interact with the Adrenaline software(especially the overlay and instant replay feature). Just this week I switched it out for a 5080 and have been having a much better experience.  I gave the 9070XT to my SO as a Christmas present in hopes she might have a better experience on her rig, but she's also got issues too. For her, Space Marine 2 just flat out refuses to launch, even after completely wiping all drivers and reinstalling everything.",Negative
AMD,Went from a 6800xt to a 9070xt. Haven't had issues with either card.  Adrenaline software is nice. FSR4 (Redstone) is nice.,Positive
AMD,Awesome. Now i feel like nvidia is marketing and nothing more,Positive
AMD,S tier. No issues. Upgraded from a 3060ti,Positive
AMD,just picked one up today because i only hear good things,Positive
AMD,Got a 9070 XT on Black Friday and it’s been a rock. Games funning great and zero issues. Got an XFX Quicksilver.,Positive
AMD,"I like it, but I had some issues.   Oblivion remastered was crashing a lot. Which can be a problem with the game. But also cyberpunk did this as well, which isn't that expected.   Other than that it is fine. It's a good gpu",Negative
AMD,"15 years Nvidia user switched to 9070xt in April.  LOVE IT.   Adrenalin better (for me) Great performance. Greta price. A gazillion vRams.   I run mine at -25% power limit and it's literally inaudible sitting right next to me on the table.   UE5 Games are actually super playable due to a gazillion vRams and loads of raster power. Knowing that I can switch to Linux any moment I want is a huge bonus as well  0 driver issues, 0 crashes that were weird, 100% satisfied.  Won't be upgrading anytime soon.",Positive
AMD,"Flawless, went from a long line of Nvidia GPUs with the most recent being the 3080 10gb which itself was a great card, but yeah super impressed with the 9070xt's performance.  Ended up building my partner a PC and another for my brother just in time before the RAM-pocalypse hit, both also using 9070xt's and no issues at all.",Positive
AMD,"My main issue with the card has been how sparse FSR 4 support is, it has gotten better since the driver can automatically upgrade FSR 3.1 to 4 but if you’re planning on playing games that only use FSR 2 you’re going to have to learn to use OptiScaler which really isn’t that hard but it’s an additional step",Neutral
AMD,Absolutely zero issues. And I no longer need to update those stupid Nvidia Game Ready Drivers every single time I start up a game.,Positive
AMD,I know on reddit anything but glazing on amd gpus but personally I had a lot of small issues. personally I would go back to nvidia gpus next time around,Neutral
AMD,"No issues, both on Windows and Linux. Granted most games I play aren't *super* demanding, with PoE2 being the most intensive.",Positive
AMD,I had one issue at the start. I do 3d printing and my slicer software wouldn't recognize the open gl on the amd driver. All I had to do was download the driver directly instead of letting adrenaline pick and now it works fine.,Neutral
AMD,"I've had zero issues running games. I play everything at 1440p, ultra settings with no issues. But every other time I start my PC the video and audio stutters if I open you tube. It has to do with windows and the amd drivers. I've done all the things... fresh windows, bios updates, ddu, changes to settings, turning off random windows features you've never heard of before because a random reddit post said it solved all his problems when he did. Nothing seems to stick. Never had any of these issues with my 3070.",Neutral
AMD,"I've had no issues with mine that I didn't cause myself. My specific unit does poorly with undervolting. Once I reined in my expectations for UV/OC, I haven't had a single hiccup.",Neutral
AMD,Absolutely eats anything in 4K that I throw at it. 100% happy and would recommend.,Positive
AMD,Mine has worked fine. Haven't encountered any problems so far and have no complaints.,Positive
AMD,Been great so far. Even so much as to recommend a 9060XT to a friend of mine. AMD is doing well this round imo.,Positive
AMD,Adrenalin sucks imo and I have had a few crashes in some games but that's about it,Negative
AMD,"I actually had tons of issues when I first got it, and drove myself crazy thinking something was wrong with the card, but eventually I thought to update bios and it's been flawless since then.",Negative
AMD,I had an 7900xt now i have a 4070.  I wont buy an amd again,Neutral
AMD,It's been incredible except I get micro stuttering playing games still with insane frame rates when I'm watching a video on my second monitor. I have a feeling it's because the monitor frame rates are not the same,Positive
AMD,"I'm very happy with the raw performance and FSR4(in the games it's available) but AMD software has certainly deserved its bad reputation in my experience so far.  I've only had one case of bad drivers that luckily was fixed with a simple rollback but even something as simple as opening the software is often a challenge, and if you listen to any music in your browser you can forget about recording/clipping using Adrenalin due to overzealous anti-piracy measures that are included.  Overall the card is great value but the software has been very disappointing for me. I would highly recommend avoiding the AORUS model as it seems to have serious thermal issues, the unit i had would reach 110°c with minecraft shaders and shut off within minutes of launching.",Positive
AMD,"I have a 4090 and a 9070 XT, and I have to say, the XT is pretty nice. The recent redstone update has made the anti-aliasing/upscaling comparable to nvidia, but i haven't gotten to try the frame gen just yet because it was just rolled out last week, and I had to use dlss swapper to even make that work. I'm optimistic   There *is* still a certain ""just works"" experience with nvidia that still isn't present with AMD. Every game has some tweaking to be done so it runs nicely, and some games seem to ignore frame rate caps, which might be a bug.",Positive
AMD,"I've owned one since late March and it's my first AMD card ever. When the 9070XT is $950CAD and the 5070Ti was $1300CAD I decided to give it a shot.  The good news: It's a good card. 1440p165hz monitor pairs perfectly with it and FSR4 is competitive with DLSS's latest models. When you can't tell if you have an upscaler on, you're winning. I've had one driver hiccup since and that was updating to 25.12.1 where it didn't kick back in after installation (pc restart fixed it) so that's absolutely blasting away Nvidia's driver track record of 2025.  The meh news: FSR4 isn't supported in older titles in the same way that DLSS is, and that sucks for a few games where I'd still like to use the upscaler. Most older titles, though, don't need an upacaler with this class of card but I do miss the ability to kick on FSR4 in Native as a better AA method.  The bad news: While Raytracing is performant, it lacks the nice-to-haves that Nvidia has been pushing for years, like Ray Reconstruction and denoising. Many games has RT perform decently but it's obviously lower quality than nvidia cards. AMD recently published Redstone to start addressing that, but it's going to be another case of ""only in new games moving forward, or if the devs are nice guys"".  All and all, I don't regret the card, but it sucks watching AMD be slow on the uptick to match in features and support.",Positive
AMD,Banger,Neutral
AMD,Real banger and they are stepping up big time on the driver side of things. The Amd fluid frame generation 2.1 whatever it is called is simply amazing even in multiplayer fps gsmes,Positive
AMD,Went from a 2070 Super to a 9070 XT and the difference is amazing. I’m playing through Red Dead Redemption 2 on 1440p max settings and it stays above 60fps with no issues. The extra VRAM is more than enough,Positive
AMD,I have a 4090 in one rig and a 9070xt in another. I’ve had no more issues with AMD than I have with nvidia … which is to say not many problems with either of them.,Positive
AMD,Upgraded from a 5700XT Red Devil to a 9070XT a little over a year ago. It is stable and performs great with any game I have thrown at it both on Linux and Windows.,Positive
AMD,"I had one, it was great and a large upgrade from my 3080. I had a few issues with VR so I returned it and ended up with a 5070 Ti.  I’ll be honest, I was a bit surprised at how close they are in performance BUT shocked by how much smoother the software works on the nvidia.",Positive
AMD,I undervolted mine and used radeon chill. I use around 50-90 watts of power for 180 fps of gameplay on most games.,Neutral
AMD,"Bad drivers are more of Nvidia's thing right now. A guy I work with was having a bunch of problems with his 4070 earlier this year. Any ""driver"" issues I've had with my 6800 XT or 9070 XT were strictly power delivery issues. I made custom PSU cables for my last build but botched some of the crimps, and that was causing issues with my 6800 XT. My 9070 XT was crashing a bunch back in June, and the issue there was 2 of the pins in one of the PCIE connectors wasn't in all the way. Easy fix. My Aorus 9070 XT has been great.",Negative
AMD,"The 5700xt was a disaster card for me. Upgraded to 9700xt finally over a year ago and the only crashes I've had were probably because of my old second monitor only being 60 while my main was 240. Upgraded that, and smooth sailing for months now.",Negative
AMD,"Well, when everyone was complaining nonstop about Borderlands 4, my wife's 9070xt and my 9060xt just went to work and had absolutely zero issues.  Which wasnt to surprising, one of the reasons I got us Red cards was because Borderlands has usually performed better on those cards.    Every other game has also worked great.  Only issue we had was with the 9070xt computer... my wife cleaned it out and plugged everything back into the motherboard and I didnt check that first because im an idiot and thought ""who would ever plug it into the motherboard, it must be all those AMD driver issues i heard about, better reinstall everything""",Positive
AMD,"Excellent. Early on there were some issues with the Linux drivers, but I haven't had any problems in a while.",Positive
AMD,I love it and got mine for 570 at microcenter. I don’t think a 5070 ti could be that price and I’m getting great value,Positive
AMD,I'll get to you when I finally get to build my new PC. Waiting for my SSD to ship.,Neutral
AMD,Never had a single issue with mine except for in AC Evo ( which is an early access  unoptimized graphical mess at the moment). Never had any driver issues,Negative
AMD,"Been using this card for a while now. Zero issues. The only thing I miss about team green was the Nvidia game capture stuff, its more user friendly, and more accepted, but its a non issue.",Positive
AMD,I don't have a 9070XT but i do have the 9060XT (16GB) and it works awesome. No game is running below 120 fps and i have my settings on the max,Positive
AMD,"7900XT here. All good, no issues",Positive
AMD,I like mine very much. Its just a little sad that not many games have fsr 4 support,Positive
AMD,"I've been using mine for almost two months without any real problems. I did have some weird stuttering that happened with games that had a 60 fps lock like dark souls 2 and persona 5 strikers, it was easy to fix by disabling some settings in the adrenalin software. Fsr 4 is amazing. I'm using the XTX Quicksilver, it doesn't ever go above 70 degrees. I can run pretty much everything at max settings at 1440 p. I had weird driver problems with my 4070 ti and when it randomly died I was fed up and went with amd. So far I don't regret it.",Positive
AMD,my daughter hasn't had a single issue with hers and its pretty fast too,Positive
AMD,"the GPU itself is a beast. Ive had 1 or 2 driver issues, when I play very recent game updates, which are usually ironned out a couple of days later- Oh and the AMD software is absolute trash compared to nvidia control panel. It feels exactly like one of those gaming peripheral softwares that never properly work.",Negative
AMD,Mine has been rock solid. I used to use it in Windows 11. Now I use it in Fedora Workstation 43 (Gnome). Works excellent on all of my games. No crashes at all. Anecdotal feeling is I get higher frames in Linux than Windows.,Positive
AMD,"Experiences stutters in some older games and crashes in BF6, but other than that, it performs well.",Neutral
AMD,Fantastic,Neutral
AMD,It's been excellent.,Positive
AMD,I’m still rocking a 6900XT and it’s a killer GPU.,Positive
AMD,"Got a 9070 non-XT, zero issues. Nothing bad to say about it. It just works.",Positive
AMD,"It's fantastic, also running VR flight sim. I can get stable 72-90 fps with a mix of high and ultra settings",Positive
AMD,"2070 -> 9070xt. It shreds. Price is good. No nvidia supply bs issues. Some of the adrenaline features like frame gen or anti lag don’t seem to work right, but it’s such a powerful card, I don’t seem to ever need it. And FSR4 is really good.",Positive
AMD,"No problems encountered, bit the bullet after hearing about all the shortages and moved from a 3080ti to a Red Devil. Still figuring out the Adrenalin software but the performance increase is pretty good. FSR4 does seem to have a little worse image quality than DLSS but I am getting much more stable frames.",Negative
AMD,"Using for 7 months now, great card, zero issues.   The only thing is that with every Adrenalin update I have to reinstall the drivers instead of update them.  That's 3 minutes wveru2 months that I'm perfectly fine with and has 0 to do with the card.",Positive
AMD,"Love my 9070xt Nitro+, absolutely no issues",Positive
AMD,"Loving it, except Forza Horizon 5 stutters and has refused my attempts to fix. Everything else is running on high settings like a champ.",Negative
AMD,Fantastic.,Neutral
AMD,Just bought a 9070 non XT today and reading through this thread is really reassuring.,Positive
AMD,"Went from a 2070S to a 9070XT. No issues that I can recall, other than Adrenalin is not as user-friendly as GeForce Experience (in particular the video recording aspects aren't as easy to use in my opinion), but that's a pretty minor thing. Also I can't get Fan Control (the software) to work with my GPU fans, which is annoying but I can just set the fan curves in Adrenalin.   As far as performance and stability is concerned, no issues, even in games that have a reputation of not playing nice with AMD cards (Darktide in particular.)",Negative
AMD,Ive had no trouble.,Neutral
AMD,"I played on one for a whole month and then gave it to my sister,  drivers were stable,  unlike my 5070ti at the time.   You can buy it with confidence,  they gotten much better over the years and still improving",Positive
AMD,I've been using it for a week now (previously nvidia my whole life) and I hadn't encountered any issues although i was very sceptical coming into this,Positive
AMD,I love my 9070xt. Best gpu I've ever owned!,Positive
AMD,It's amazing 👍,Positive
AMD,"I just upgraded from a 3060 Ti (FE) to a 9070 XT because of that paypal cashback and capital one shopping cash back double dip, and it's worked perfectly. I have continued to play at 1440p on a VRR monitor, but happened to score an upgrade on a Lenovo deal a couple weeks prior to getting the GPU, and i've seen massive improvement in every game without any sort of stability concerns or anything like that.",Positive
AMD,"Came from RX580 to a 9070xt Sapphire Pulse. Still using 3700x sadly so about half the games I have are cpu bound at UWD 1440p. Still appreciate the gpu pushing frames that max out the cpu w/o the heat associated with high end cards. Sadly the AM4 x3d cpus are mostly out of stock where I live so my setup will be cpu bound for a long time, unless I go 4k on the monitor.. Hmm, might not be a bad idea since I also use the monitor for WFH stuff..  I had lots more crashes on the rx580.",Negative
AMD,Mine works great. Had problems with drivers one time where I’d get small visual artifacts in games behind UI stuff. Reinstalled driver and since then had no problems at all.,Positive
AMD,Been having issues with streaming on discord and the screen turning into straight cable static when on the main screen but goes to normal when i click off the main monitor,Negative
AMD,"XFX Quicksilver 9070XT is working real well here for 1440p (I want higher frame rate not 60fps 4K nonsense). Great temps and not the worst sound levels.   In STALKER 2 @ -160 fps VRAM is 37C and GPU 57C  Also have a 7900XTX which is way more noisy. FSR4 ...RedStone is just so much better than FSR3, just remember VSYNC ON.  PS: Owned A LOT of Nvidia cards as well.",Positive
AMD,Was the best purchase I made this year.   Upgraded from a 1070ti lol,Positive
AMD,I like mine,Neutral
AMD,"The last time I experienced actual problems was the HD 5850 in my brothers comp. It'd crash just watching videos.  Since then I've had HD 6850, R9 290 (then I got another so I ran 2 in crossfire), Vega 64, 6900xt, 9070xt. Can't say I really remember having real problems.",Negative
AMD,Bought a 9070XT 4 days ago and I don't have any issues. I'm very happy with it.,Positive
AMD,I haven’t noticed a single issue. I did stress test it when I first installed it by fully maxing out cyberpunk and it spins up LOUD with the rtx options maxed out but no normal game has struggled or crashed it. I also moved from 1080 to 1440 when I upgraded from a 5770xt which has just been a really nice quality of life thing.   The closest to a problem is that mine isn’t even one of the larger ones and it barely fits in my fractal case so you gotta really measure things if you like smaller cases at all,Positive
AMD,Minipc + 7600m egpu --> 9070xt mff build. Went from 1080p aircraft noisy to 4k almost everything. No issue whatsoever.,Negative
AMD,It works amazing. The only complaint I have is FSR4 is barely implemented in any game. It was supposed to slowly increase the games it was implemented on but that has been slower than I expected. Almost none of the games I've ended up playing has had FSR4. Some new games are releasing with it but older games have not been receiving it as much as NVIDIA has been getting DLSS support.,Negative
AMD,Fantastic card.  Rock solid on a b650 WiFi plus alongside a 9950x3d. Can you link these claims? I've not seen anything like that apart from usual fanboy drivel linked with brand politics.,Positive
AMD,Their info is outdated for at least 4 years. Especially anything released in the last 3 had no wide spread issues.,Negative
AMD,"Bought a buddy of mine a sapphire pulse (£500) 9070xt for Christmas, it runs fantastic. After benchmarking it for him, it does everything near identical with my 5070ti (£670) at £170 less than what I paid haha, I was taken aback by how well it did RT and as he's not the type to play heavy graphically demanding games I think he'll be flying for a while after dropping his gtx 1080.  Ironically Nvidia has been a rollercoaster in terms of drivers bugging out, lost phys X  and Nvidia Broadcast 2.0 is forced on 50 series and Broadcast 2.0 basically doesn't work.",Positive
AMD,"Got an xfx 9070xt in May. It was good while it lasted, but died on me about two weeks ago. Needed a graphics card quick to meet some deadlines at the time, so I ended up grabbing a 5070ti at microcenter  I'll probably try and get an RMA for the card once the holiday rush is over and figure things out from there",Positive
AMD,"3 weeks in, latest drivers, zero issues. Still running a 5700X3D so a little behind in CPU, but running pretty intense games with zero issues. Battlefield 6 max settings 100+ FPS. Star Citizen fairly high setings, vulcan rendering, averaging 120FPS but still having 99% lows in the 30s, I blame star citizen optimization for that one. The finals on ultra is 144+.",Positive
AMD,Love mine. I heard lots of horror stories before buying about bad drivers but I haven’t had a single issue yet. I had an rtx 2060 previous and that was far more temperamental with drivers especially once I made the switch over to Linux,Positive
AMD,"I've had nvidia GPU in my system for the last 10-11 years. Recently got 9070XT as my 3080 started dying. I guess the best review I can give is that... I forgot I have an AMD GPU now. No driver issues, no crashes, great performance. It's just there and works very well.",Positive
AMD,"Got a slight discount on an open box ASRock 9079xt Taichi from micro center 2 months ago, $655. Put in a new PSU and hooked it up. Runs great with my 5700x3D, very capable card. I'm playing Helldivers 2 and Arc Raiders on ultra, no FSR, 1440p getting 70+ FPS consistently, sometimes 120+. Drivers gave me issues consistently until I turned off auto-update and installed the stable version myself. Was getting crashes and freezes and otherwise seamless performance prior. A few times my screen just cut out and went black, 1-4 seconds at a time. Haven't had that issue since.   But great value out of the card, especially non-OC versions at $599-$650? Beats the 5070Ti easily, unless you need to do AI or some non-gaming demanding processing, or if Ray Tracing is ultra important, though it's been great in Arc Raiders for me so far.",Positive
AMD,I have it since day one. The only thing I didn’t do at the beginning was the DDU. Just clean your drivers! And since then it’s a dream,Neutral
AMD,"I built my pc recently and i had debated between it and a 5070ti, and I went with the 9070xt (Sapphire), I've been using it for the past 2 months and I have no complaints, it runs flawlessly, it never overheats (again my pc has 6 intake fans with the AC mostly on), no stutters or crashes so far, and I already undervolted and overclocked it.",Positive
AMD,"Can't speak for 9070, but I got a 7800xt shortly after release (first time going AMD) and it's been nothing but regret since. Learned my lesson...ish (might give intel the benefit of the doubt someday, probably repeat the experience).  tearing, constant driver issues & crashes, noisy-ass fan that can't respect curve, adrenalin pain in the ass, horrible for local AI. Hotspot started reaching 113C at some point before I decided to undervolt it by 100, and probably contributed to the SMART ""failure imminent"" of the M.2 SSD which is directly beneath the card. I'll never buy another AMD card again; I'd rather pay a $500+ premium just to avoid them. Awful first experience, but I'm stuck with it until a suitable NVIDIA card releases, and bad timing because of RAM prices.",Negative
AMD,My son has one and he hasn't had any issues with it at all.    There were issues many years ago but they have been good for a long time.,Positive
AMD,"Been using it since Nov and I love its performance. But I gotta be honest, from time to time the Adrenaline software would crash randomly",Positive
AMD,I've had the 9070xt for about 5 months now and couldn't be more happy. No issues whatsoever and every game runs very well,Positive
AMD,"150 fps in stalker 2 on average - high/ultra settings for example. Card memory is getting 95-97 degrees celsius on full load which is fine, my 3080  is getting even hotter sometimes. All in all, more than satisfied for 500 euros I paid, the only thing that worries me is that plastic bracket thing falling of that people reported more than once here.",Positive
AMD,Quick guide to AMD driver issues  Is your GPU an RX 5700 xt?  No: No driver issues  Yes: Mostly no driver issues,Neutral
AMD,"Good overall, but some really weird issues here and there like my displays went all black and wouldn’t come back, after restarting my PC I found out the AMD Adrenaline software had completely bricked itself(no longer showed up in my start search) and I had to do a repair installation. But I think that’s a issue with the game I play more than anything, Escape From Tarkov which has very bad optimisation and hangs a lot going in and out of menus.",Negative
AMD,"2070s -> 9070 xt, insane difference. Basically 3 times the fps in red dead redemption.   The only problem is getting used to the new software, and I honestly don't know if freesync is active or not. Something that actually confirmed it in the software would be nice.",Negative
AMD,"Mines great, only annoying thing is the noise suppression not working.",Negative
AMD,"I've had an occasional crashing issue where I've needed to clean install the drivers, but I have a feeling that's an issues specific to the ASUS model. Other than that, it's been perfectly grand. I should note I had similar small issues with NVIDIA cards too, I kinda just attract issues",Neutral
AMD,"Main complaint: no easy way to get 1440x1920@480hz stretched to work. I somehow got some hacky method which worked for a bit, but an update ruined it. Going back to nvidia for a 5090 and it took 3 seconds to add the resolution and then it just worked.  This is also something most people won't have to worry about. Few people run 1440x1920, and fewer at 480hz.  Otherwise minimal complains. Idle wattage is higher than my 5090 fwiw, but not a large enough difference, and energy is cheap where I'm at. (48w vs 38w)  Also I'm running 1440p 480hz main, 1440p 240hz secondary, 1080p 240hz tertiary monitors. Removing or lowering these refresh rates would reduce idle power draw for both cards. I usually sleep or power off my pc, so more of an observation than a real complaint.  Overall pretty good experience, and close enough to ""just working"".",Negative
AMD,"Honestly, went to upgrade from a 6700xt. My primary use case is flight sim.  I had to return it for now. Msfs24 does not play well at night with any of the 9 series. https://forums.flightsimulator.com/t/weird-graphical-artifacts-around-cockpit-at-night-on-amd-rx-9000-series-gpus/715067",Negative
AMD,I like it. No issues with what your colleagues described.,Neutral
AMD,"Had one rma after 5 mos.Rma was no problem .eplacement has operated near flawless for 9 months on 1440p. Helldivers runs ultra with high gpu utilization, no heat problem.   Easily in hundreds fps reported. For 4k it is serviceable but 1440p is the sweet spot. For gaming still a great card",Positive
AMD,"I have the sapphire pure, but I quite like it, the only issues is that some games are a bit weird and there is minor screen tearing, but not enough to seriously downplay the card. It is capable of getting 165 FPS on most games at 3440x1440",Negative
AMD,"Two 9070xt’s at home, several pc’s build with them for friend and family. Not one issue apart from one friend having to disable windows driver update on win11 and reinstall proper driver once.",Neutral
AMD,"I had issues at the beginning, but it works great now. If you come from nvidia you need to use DDU in safe mode",Positive
AMD,"Had no big issue at all.  The only minor issue i had was that i once needed to backroll the drivers since there were some problem with EAC and vulkan in a game i played, and also that the memory get somewhat hotter, over 90°C, but that's only when using particular software, when gaming it doesn't go past 85°C.  Beside that i never had any problem with anything, it was always pretty smooth.",Neutral
AMD,"Went from 1080 to the 9070xt so far been good relativity smooth, had one game with light stutters but that could be any reason. Considering I couldn't run the game previously lol. But a prob would get a 5070ti if I could change, just because more games have less issue with them",Neutral
AMD,It works every day consecutively. It does a great job at that,Positive
AMD,"Using 5090 here , just wanted to say that nvidia's recent drivers ever since the end of 2024 are completely trash.  So anyone who still chanting AMD drivers sucks, they really should have a reality check.  My friend has 9070XT with no issues, great card by all means.",Negative
AMD,"Upgraded two weeks ago from 6600xt, before was 5500xt, rx570. I'm not a fan of ATi radeons, but three previous cards were used in hackintosh system also.   Finally, 9070xt incredibly powerful and efficient. I'm able to decrease power for 55w and keep performance the same. I didn't consider to buy 5070ti because price difference in EU is about 150€ so buying nvidia is a waste of money.",Neutral
AMD,"I’m using mine on Fedora, I came from a 3060 12GB. Performance is night and day which is not too surprising, and while both amd and nvidia worked without any noticeable bugs, the nvidia driver requires repackaging and signing on each kernel update, which is has proved to be brittle over the years. The AMD driver is baked in to the kernel, which means kernel updates are much less of a nuisance now.",Positive
AMD,"I’ve had mine since June of this year and it’s been doing great. Almost had no issues with it, but if anything it was the Adrenalin software that’s a bit funky sometimes. It deleted itself off my PC and I hadn’t realised it, but steam basically refused to open because of it. Had to redownload it and everything was working after.  It’s honestly just a nitpick, it works amazing for 1440p",Positive
AMD,best 600 quid I've spent in my life,Positive
AMD,"Yust upgraded from nvidia 1660 super to 9060 xt 16gb after been a nvidia guy for many years. It is working great, a little screen tear in one game, but i coreected it wit same fps or lower than my screen hz. Tried also VS. All in all no drivers issues and other glitches in games.",Positive
AMD,The best GPU I have ever purchased,Positive
AMD,"I had a 3060 TI since early 2021, then decided to upgrade for a new build this year. Grabbed a 9070 XT back in August, and aside from crashes during the first week, it's been fine ever since.",Positive
AMD,"I have a 9070xt nitro, so far no game crashes. I did some undervolting (-50mV) to lower down a bit of temp.  No burnt cables as well.",Positive
AMD,Sht slap,Neutral
AMD,Bad drivers still exist.   I downloaded drivers for a friend. I recommended this gpu to and it couldn't even launch after installation and a restart. How embarrassing.,Negative
AMD,"Awesome!  I game at 3440x1440@120, but my monitor's native resolution is 5140x1440@120.  I have a 9800X3D, 64GB@6000, and a 2TB NVMe.  I recently switched my FE 3090 for the 9070 XT Sapphire+ and the frame jump was solid.  Same settings in The Finals (no ray tracing) was averaging 100 fps and it is 150 on the 9070.  I mainly play this game, but similar experience across my other games.  Glad I picked one up during that brief window prices normalized.",Positive
AMD,1080>3070>9700xt  Each upgrade has felt like a monster upgrade. Have only good things to say about all three cards. 9700xt ultras anything I want it to without even getting that hot. What more could I want.,Positive
AMD,"Im ATI/AMD  user since the Power full card:  ATI 9800 PRO, now i have RX9070XT,  I still have an R9 280X in an old computer for retro gaming.",Neutral
AMD,"This has been my first AMD card, and I swear I've had less issues with it (none) than I've had with any of my past Nvidia cards.",Positive
AMD,"0 issues and I have 2 of them. No crashes, no bad drivers. The fps are high on 1440p. I recommend the XFX Mercury, that’s what I’ve been using.",Positive
AMD,Its struggling a little at 4K depending on what game you play.. At 1440p its the best value card..,Positive
AMD,Only problem I have is my R7 3800x is bottlenecking all my fckn games.   Which... is my own fault for not buying the 5800x3d when I should. Now I'm kinda trapped :(,Negative
AMD,Extremely happy with 9070xt on linux,Positive
AMD,"I initially had an XFX Swift 9070xt, which had issues with just not spinning up the fans that neither I nor the shop I got it from could fix. It got replaced with a gigabyte gaming OC one and it's been very solid without any issues since.",Positive
AMD,"Perfect so far, had mine since start of October and had no issues at all",Positive
AMD,"I have the XFX Quick from May 2025, very happy with it. Got a great deal on an open box , 640 euros , 36 months warranty. I keep mine on a custom setting, + 10 power limit, -55 MV, 2640 MHz memory, and a custom fac curve.",Positive
AMD,"I very recently bought a 9070xt, so far no problem.   Before that, I had a 6700xt for 2,5 years. I did start to receive a few crashes just before the upgrade, but only for one specific game (elite dangerous), and under very specific circumstances (while in a loading screen just before finishing loading, tabbing out of game to webbrowser).   Other than that, it has been very stable.",Positive
AMD,"Only issue l've had is the coil whine, but it goes away after it has warmed up a bit, my game room is quite cold.",Negative
AMD,9060xt is good enough,Positive
AMD,"Some crashes but none in months, I love my card. At least I didn’t pay 1000+ to get a fire hazard",Positive
AMD,Only issue I've had so far is windows one day decided to delete my driver so I spent about 10 minutes running on my 7800x3d igpu confused as hell until I reinstalled it.   May I just emphasize that was very much a windows problem and not an amd problem. Worked in the morning and then didn't by noon so at some time between then windows decided I should go f myself,Negative
AMD,"AMD's cards are perfectly fine as long as you use it to play games. Ignore the bandwagon saying that Nvidia's cards play games better than AMD's ones because unless you're going for the absolute best of Nvidia there's no significant difference between these two on most games. Nvidia only got the upper hand in certain gaming situations because they usually introduce new tech (like RayTracing) before AMD could adapt, and most people would turn that off anyway to juice for some fps. If they come up with \*Oh I used to have issue with this game\* it's mostly because of a terrible driver version, and you can experience the same thing with Nvidia's as well.  When you need to render or do something else like AI learning, then go for Nvidia because the softwares for those are tuned for Nvidia's GPU.",Neutral
AMD,Flawless for me. Very happy with it.,Positive
AMD,Zero issues. Only time I've had a crash is when I went a bit far with undervolting. Pulled it back a bit and not been a problem on any of the drivers.,Negative
AMD,I've bought a 9070 XT and so far the experience has been a pain in certain games like ARC Raiders where the latest driver caused crashing even if they said those were fixed in the amd patch notes.,Negative
AMD,"my first gpu, got it in october. zero problems ever, only issue i got was with windows automatically updating adrenalin drivers",Positive
AMD,"My coworker has preached the same thing to me, I've had no issues with mine since I got it months ago",Positive
AMD,"Lifelong AMD user here, super happy with the performance of the card but I must say I've had more driver crashes than any other card in 20 years. Which at least they seem to patch reasonably quickly but then I get a new one that's really random like currently I can't use my GPU for Illustrator which is... mildly annoying.   0% regret buying it though, and FSR4 was an unexpected win. I had no real desire to use it, until I did.",Positive
AMD,"Bought Sapphire Pure as soon as it was available for a new rig. Only issues I had were Windows related (overwriting drivers, changing to iGPU), after dealing with those none whatsoever. Card undervolts beautifully, you can cut 100w with minimal performance loss or 50 with performance gains while maintaining stability even under a heavy load. Adrenalina gives you a ton of tinkering options. Temps are great, no coil whine, silent while idle. One minor gripe I have is that the RGB shade is slightly different from other parts I got but that's on me. Oh and performance is next level, pushing 4k60fps in UE5 games on high settings or ultra with slight upscaling. For the price it's unmatched. Never going back to Nvidia unless for work.",Positive
AMD,And at what resolution! That's important info!,Neutral
AMD,"No issues so far. I was nervous making the switch from Nvidia having been team green since my gtx 970. Its been 2 weeks and no problems in BG3, Arc Raiders, and dota 2. I'll be testing it on other games as soon as I spend all my money at the steam winter sale.",Positive
AMD,"I’ve got some instability issues while undervolting my Taichi, but I figured out it was due to Windows. I also had problems with DDR4 XMP profile on Windows so I had to stick to slower fclk.  As soon as I installed Bazzite Linux I solved all of this.",Negative
AMD,"Love the card and was a huge upgrade for me. Has generally been great, but I am experiencing some kind of driver issue. Now and then when I turn my PC on it fails to POST and the debug light for video comes on. I have to restart a couple times with just the HDMI in for it to resolve. On latest drivers etc.",Positive
AMD,"Great, before this I had a 6800XT and it was great as well. Before that I had 3 or 4 Nvidia cards and then made the switch to full AMD. I haven't had issues at all, though I'm still using W10.  Apparently a lot of people having issues are using W11 and recently there has been a Windows update that fixed a few things is what I read.",Positive
AMD,"I pulled the trigger and some friends helped me built a PC during the summer. ASUS TUF 9070xt, 9700x + a Phantom Spirit 120 EVO, 32GB RAM 6000MT/s and it's been great. Started with 1080p gaming then decided to buy a second monitor , but  1440p. Overall very good performance on both resolutions. I honestly didn't expect the RAMpocalypse, still wonder if I should have bought a 64GB kit though.",Positive
AMD,"Best card I've ever owned, never even broke a sweat",Positive
AMD,"I have it for a bit more than a month. Great at everything I throw at it in 1440P or 4K because I have my TV hooked up to the PC. Haven't even tried 1080p, even tho my main monitor is 1080p lol.  Only one downside, the fans are a bit noisy for my liking. I'll probably have to turn down the fan speed a bit. I guess as long as the temps are under 80, then it will all be fine.",Positive
AMD,"This has been my experience with the 9070 XT so far:  Great perfomance, great price.  They unvervolt well.   They oveclock well.  AMD software is much better than the NVIDIA app. for tuning the card (no need for MSI Afterburner).  However, I did go through 4 models with horrible coil whine (udervolt and power limiting did not help). Finally, I settled on the Sapphire Pulse model.   Great card honestly (if you don't care about Ray tracing and just want raw performance) and it's been a ton of fun tuning it using AMD's software. Very stable and no crashes.",Positive
AMD,Upgraded from 4070 ti aero gigabyte to 9070 xt Asus prime like 3 weeks ago  Aero was already quiet and had low temps. 9070 xt uses more power so I was sure it'd run hotter.  It doesn't. At least while playing FIFA or League of Legends.   Haven't tried it in any GPU heavy games yet.   I like the soft do far as well. Can't say anything bad about it but like I said I'm haven't had it for long,Neutral
AMD,Only issue is that AMD Adrenalin sometimes refuses to stay open or stay on my 2nd monitor when launching a game.  0 issues with the hardware itself,Negative
AMD,"RX9070 (non XT) on Linux here, absolutely flawless, no issues. Also tried it on Windows for a short time too and it also worked perfectly.   Drivers and stability aren't a issue anymore 99% of the time for 99% of the people. Only difference is that Nvidia has stuff like Cuda, which makes it better for productivity. Besides that, all commonly used features like ray tracing, image upscaling and frame gen all have a equivalent feature on both Nvidia and Amd, you really aren't compromising anything imo.",Positive
AMD,"I can't say much because my wife mostly plays CS2 but every time we play I have some weird feeling her game is like smoother vs mine and we have the same fps(both locked to 160fps). I know CS2 is kinda bad game to have this comparison but she doesn't play anything else so...  Her specs: 5900X, 9070XT, 32GB RAM, 165HZ monitor   My specs: 9800X3D, 4070TiS, 64GB RAM, 165Hz monitor  The only reason I am with 4070TiS is that I play PC VR games and those works better with Nvidia.",Neutral
AMD,"Had a bad motherboard CMOS battery even though the voltage was good, and it caused heaps of unexpected crashes and instability multiple times a week that i thought was other components.  So far my system has been stable since the latest Adrenaline driver as that's when i did the battery replacement.   Not playing many raytraced games.",Negative
AMD,amazing,Positive
AMD,Amazing actually.  Even plays Star Citizen very well,Positive
AMD,"I got my first AMD card an about 2 months ago after being with Nvidia since the GTX 960, granted it’s a non XT 9070 so not exactly answering the question but it’s close enough. Honestly the only ‘bad’ thing, and I’m stressing the quotations, is having to adjust to the AMD software when you’ve used Nvidia for the past 10+ years but even that didn’t take long.  Other than that the card performance is exactly what I expected and is a substantial upgrade to my 3060Ti.",Positive
AMD,"GTX 760 -> GTX 780 -> GTX 980Ti -> GTX1080Ti -> 9070XT user here, it does pretty well! I have had a few stumbling blocks with it compared to my nVidia years, but not too much.   Really, it’s just one game that seems like it is perpetually better optimized for nVidia than AMD. Just so happens to be the game I have ended up playing most. Issues have been: 1. Visual artifacts on loading screens. 2. Corrupted shaders - I have had this happen 3 times since 9070XT launch day, never with my 1080Ti.  I can’t complain too much. I like my 9070XT, and I haven’t used an RTX series nVidia GPU to compare against, but I do find myself wondering if I would have those issues if I had just stuck it out for a 5070Ti or spend the money on a 5080.",Positive
AMD,"So I just got a 9060xt 16gb and I have been loving it and the software is actually pretty good, I've experienced no crashes or any issues",Positive
AMD,"I had mine since it came out. I had a few crashes here and there.. but i can count them on one hand, they are often due to driver issues, that's all.  Performance is great. (Except monster hunter wilds, i love the game, but hate the performance.)",Positive
AMD,"Have a got it for almost a year, been playing games at near 200 fps and above. HD2, Arc Raiders... cyber punk and such.",Neutral
AMD,Been great,Positive
AMD,I never had an nvidia card. Also my problems with drivers were only driver timeouts and occasional game crash but that wasn't very often and happended only a few times with my 6700xt.  I bought sapphire pulse 9070xt and I've been having a blast. It's a beast for its price and runs pretty cool. Also zero issues so far.,Positive
AMD,It's awesome,Positive
AMD,I bought mine like two weeks ago. Before that i only had NVIDIA ones. There is no real difference exzept for the overlay interface wich is not that great for both. But the amd one is just a little bit worse. But for more than to 100€ less and just a little less performance than the 5070ti it is defenitely worth the money if youre not a huge raytracing fan.,Negative
AMD,Great gpu love it,Positive
AMD,I love mine! No issues whatsoever.,Positive
AMD,"I've been having a good time with mine. Got it recently on november, and since then the only game that gave me some trouble was Hitman (the game doesn't like the match-up between High FPS and AMD cards).",Positive
AMD,I do have 9070XT. So far all good. Fps is high no lag no freeze..playing bf6 on Ultra,Positive
AMD,"Yeah, drivers crashed for me a few times. But like last few generations didn't have any issues idk what are they doing right now. But for me the price difference is worth it. Depends what you're gonna use the card for. If mostly gaming and you care how much you spend go for AMD. For working go with Nvidia. But it's all about price for me.",Neutral
AMD,"I've had mine for about 3 days now  No complaints. I dont really play super demanding games, so I've noticed no change from my 1080  But I bought it for Ace Combat 8 and the new Tomb Raider games, so remind me later",Positive
AMD,I've only had mine a few weeks. So far the only issue I've had is a direct x 12 conflict with playing bf6. I reinstalled drivers and direct x and so far I haven't had it again.   I'm only playing at 1080p so it's been smashing every game I've been playing so far. Pretty big upgrade from a gtx 1060.,Positive
AMD,"Upgrade from a RX 6650XT. The card was nice, but I sent it back (some issues in metro Exodus and I don't want to be a beta tester on my time and money). I'll buy a 5070ti or wait for the next gen.",Positive
AMD,Pretty good. I run a 9070xt on Linux and get the occasional stutter when stuff is loading but 99% of the time it’s pretty flawless. And fsr4 is pretty great.,Positive
AMD,Great Just need a bit of work to disable the RGB in my XFX but other than that it has been excellent,Positive
AMD,"I’ve had the 9070xt for about a month on a 4k tv (60hz).   First thing I do on a game is chuck everything to ultra high, it hardly spins up the fans and fps doesn’t drop below 59. I’m excited to get a new display so I can really rinse it",Positive
AMD,"No issues with it at all. Drivers have needed manual updates once or twice but thats because AMD software always hates me.  Otherwise fab, performs excellently, best performance from a GPU i've had in years.",Positive
AMD,I've had a 9070XT Hellhound OC since launch day and it's an absolute beast. No regrets at all and it handles everything I throw at it at 3440x1440p. Coming from a former lifelong nVidia user,Positive
AMD,"I just built a system with a 9070XT and a 9800X3D. Coming from a 5800X with a 3080. The 9070XT build feels far superior! I feels smoother, less micro-stuttering. I am playing in 4K so it obviously would perform better than the 3080 in that regard.   I am very happy with it! Currently playing ARC Raiders at High settings and getting a solid 120fps in 4K, it's phenomenal!",Positive
AMD,10/10,Neutral
AMD,I’ve been getting a stutter/tear when using browers for anything like when loading multiple things back to back or like YouTube but if I’m watching a movie I’m good playing cod I’m good I’m not sure why I get the thing it’s like one part of the screen is movable with the scroll wheel but the rest of it becomes a overlay for 2/3 seconds before it adjusts itself but other than that I’ve been gaming in 1440p 160fps no problem,Neutral
AMD,"Been on Team Red since the 5700XT in 2019 and just upgraded to the 9070XT earlier this year. Late 2019 was pretty bad for drivers with weird crashes for no apparent reason but it didn't last all that long, maybe around mid-2020. I also discovered that my 6+2 was loose at some point so that definitely contributed to the issue.  After mid-2020 the AMD experience has been pretty good. No problems at all on the 9070XT besides the fact that I had to get a new 8-pin because it wouldn't boot when I used my pig-tailed connector.",Neutral
AMD,No issues. I switched to a 9070XT from a 4080 Super because there were too many issues with the 4080 Super in Linux. Been loving it so far. I lost a little performance but I'm pretty happy with it.,Positive
AMD,Yall are tripping I had a 580 then a 5700xt red devil now a 6750xt no driver problems yall tripping,Negative
AMD,"Bought it when it came out, had 4-5 crashes in the first 2 months, two of them could have been from my under volt. Didn't have any issues since then",Neutral
AMD,I had the 6800xt for about four years until recently when I got the 9070xt. Absolutely no issues with either and the software has been rock solid. Yay AMD.,Positive
AMD,I've been running mine for 5 months or so and it's been awesome. No issues whatsoever.,Positive
AMD,"I have a non-XT 9070 and zero issues, bought at launch.",Positive
AMD,Perfect.,Positive
AMD,Great  Expect the other day where the latest driver update broke both OpenGL and Vulkan Support so I just rolled back to the driver before  Other than that it's been great,Positive
AMD,Bought the 9070 XT for my wife open box from bestbuy. It goes fucking BRRRRRR. On par with my 4080 in some games.,Negative
AMD,It's been fine. Not had random issues with monitor dropouts in multi monitor setups that I had on my last few Nvidia cards. It's been rock solid and great performance.,Positive
AMD,"Have been using AMD gpus since 2017, started with a RX 580, in 2020 a RX 6600 XT and now a RX 9070 XT. Never had any issues of compatibility or driver problem. Those who say AMD cpus are getting hot or gpus have drivers that crash constantly they sure live in the past.",Negative
AMD,I have had the 9070 non XT for the last year I think no issues. Before I had a 7800XT and again no issues. Only used nvidia before that.,Positive
AMD,"It's been great, for the most part. Great performance,and the only time I had driver issues was actually the game dev's fault for pushing an EAC build that was incompatible with the latest amd drivers.   The overlay is a bit buggy, sometimes it won't open when hitting the key combo, but it's nice being able to mess with things like afmf and the like while in game.   My only major issue, to be honest, was whenever I was playing a game, if I were to tab out and use another application on my computer, often the app window wouldn't properly update, visually, and I'd have to maximize/resize the window to force it to update. Essentially the app would be visually frozen. Turning off MPO (multiplane overlay) in windows fixed that, though.",Positive
AMD,A set it and forget it experience. As should be the case with all GPUs,Neutral
AMD,"Had mine since March, not one issue.",Neutral
AMD,"I’m on a 9060xt 16GB as of December coming from a decade old nvidia. Never looking back. Running resident evil 4 remake at 1080p with all options plus ray tracing. AMD is the right choice for gaming for the foreseeable future, friend.",Positive
AMD,I upgraded from a 7900gre and have been getting crashes every day… I ordered a 5070ti to see if it’s the card or something else in my pc causing the crashes,Negative
AMD,Yeah all good for me.  I run 1440 and can play everything I have at max quality.,Positive
AMD,AMD gpu drivers pmo. Had my gpu 2 years. Windows auto updating breaking things.,Neutral
AMD,I have a 6800 and it never freezer..,Neutral
AMD,"Amazing, no issues at all",Positive
AMD,"I switched both my GPU and my OS (3070Ti -> 9070 non-XT and Windows -> Kubuntu), and I am actually surprised at how smooth and painless my experience has been.",Positive
AMD,I've had mine since release. No issues whatsoever. Works great for 3440x1440 on my ultra wide and 4k for couch gaming on my TV.,Positive
AMD,my first build ever and i went with 9070xt and i haven't experienced any issues at all.,Positive
AMD,"Bought one in April. I've had no issues with any game, everything runs smooth. I'm happy with the performance in games (I replaced a 3080). My card (Sapphire Pulse) has now developed a squeaky fan, and I am debating whether to try to RMA it for that. That's unrelated to it being a Radeon though.  Three times in the last 8 months, the AMD Arenalin software has seemingly uninstalled itself, blackscreening the computer and forcing a restart and reinstallation of the software. I don't know what causes that, but it's annoying.",Positive
AMD,My experience with the sapphire pulse is great. No noise or crash issues. Fps are great. Would recommend.,Positive
AMD,Pretty good overall. I've had a few driver crashes but not enough to consider it a problem.,Positive
AMD,"I upgraded from a 2060 to a Sapphire 9070XT other than it running a little warmer, the thing kicks ass and AMD's software is a long ways ahead of where it used to be",Positive
AMD,"Pretty great so far, got one a few days ago.",Positive
AMD,"Preformace is amazing, but I've had a couple gpu driver crashes. Not really a problem tho it doesnt happen super often, and might've stopped after the recent update",Positive
AMD,"Not bad but my issue is AMD is wisely not supported. Nvidia be having all the new feature for every game afap while I'm here praying AMD will get the treatment soon.  And yes, I know optiscaler exist but it is not an excuse that I have to fix every game that I play, especially some live service game may ban you for using optiscaler.  I also have some crashing issues but this mostly may come from the fact that I upgrade just gpu on my rig so it bottleneck with the current cpu.   If you plan to buy this card, be ready to use optiscaler",Negative
AMD,Just bought mine on black friday couldn't be happier,Neutral
AMD,"Amazing, low temperatures, everything in very high quality, no complaints at all, glad that I bought It as soon It was available in my country!",Positive
AMD,Excellent 0 issues got it for black Friday.,Positive
AMD,Butter smooth high fps,Neutral
AMD,"First 2 weeks were a bit ropey. I had a rogue nvidia file in my system that I couldn't delete and it kept causing crashes. After i reset my pc, it started working IMMACULATELY.   It's a brilliant card. I can run cyberpunk at max settings in 1440p (except path tracing) and I get 90fps without framegen, or 45fps with path tracing.  Other than that, when I start playing a new game i just crank the settings as high as possible because it will have a playable framerate",Positive
AMD,"Mine's been doing great! Runs smooth, looks nice in the case and finally let me max out the settings in Cyberpunk.",Positive
AMD,"I went from a 1080 to 9070xt and it's queite the jump     also ditched the 22"" IPS and got myself a 27 inch oled.     games look and run so good, it's dope",Positive
AMD,"I had a 3070 laptop for 4 years, maybe a crash. Now I have a 9070xt, maybe 10 crashes in the first week and a half. Otherwise I’m happy",Positive
AMD,"Got my 9070xt on launch day as my first AMD gpu, and I have had a flawless experience.",Positive
AMD,had mine for a month only had one crash but it was in dragons dogma 2 which is terribly optimised so take it with a grain of salt everything else runs like a dream,Neutral
AMD,No issues with my Asus Prime 9070 that was not the game fault (Arc Raiders) or my undervolt profile (Cyberpunk 2077) where it was too low to be stable with RT on.,Neutral
AMD,No issues with my 9070 except the hdmi idle reboot bug in the previous drivers that is now fixed.,Neutral
AMD,Love it. It's quiet and stays cool under load. I would seriously consider the card if you're looking to upgrade.,Positive
AMD,Love it went from 3570k 1080ti to 5700x and 9070xt. Unbelievable,Positive
AMD,"Love it. Went from a 3070 to this  I’ve had one driver issue so far, just some very minor screen flickers that were fixed with a quick driver update   Can’t complain whatsoever",Positive
AMD,"Upgraded from a 1080, couldn’t be happier and runs great on windows and linux",Positive
AMD,"After six months, I will never purchase an Intel anything again. Plus, having something with heavy support in Linux is a bonus.",Negative
AMD,"I have a 9060xt so I just want to comment that the stuttering thing is real but just affects a very small percentage of 9060xt/9070xt users. As soon as did my first boot after switching to the 9060xt I noticed that mouse movement was inconsistent. It would stutter, feel like it's moving underwater, and jump around. Also have a horizontal tear that becomes very noticeable when scrolling or watching videos and playing some games.  Otherwise the card is very good and my only regret is just not biting the bullet and getting a 9070xt. Never had any crashes caused by the card or bad drivers. Just sucks I got unlucky with this issue.",Negative
AMD,Very pleased with mine which is paired with my 7800X3D,Positive
AMD,No issues. AMD since they were ATI. The only card I ever had an issue with was a 512 MB AGP Sapphire,Positive
AMD,Built my first red team machine a month ago with one and loved every minute of it so far!,Positive
AMD,Card is absolute cinema. Have had 0 issues across both Windows and Arch. Had way more issues with my reference 2080.,Positive
AMD,Only issue is that mine is benchmarking at 27% for some reason. But i havent done any tweaking or digging into it. Mainly because it still runs everything i play damn well.,Negative
AMD,its fine for me so far,Positive
AMD,the stuttering and crashes tend to come from people enabling too many setting in the amd adrenaline app alongside ingame frame gen settings to where it just feels like a sloppy mess and not the good kind.,Negative
AMD,On linux it's been very solid. I haven't done too much windows testing but from what I've experienced it's solid there too. I have an ultrawide monitor on the way so I can really push this thing and take some load off my 10th gen i7.,Positive
AMD,"Zero issues, had the card for 3 weeks now and I'm on Linux",Positive
AMD,"Can’t vouch for the 9000 series cards.  But I had a 7900 XTX for a few months after it launched.  Performance was fine, even in ray tracing (but I was not running 4k).  I did have a number of driver crashes though in quite a few games (AC Valhalla, Jedi survivor, etc…).   I think Jedi survivors crashed were more due to its poor launch state than anything, but Valhalla crashes stopped when I swapped to a 4090.  That was just my experience though. A friend of mine has a 9070 and has said it’s been flawless.",Neutral
AMD,"I just upgraded to 9070XT hoping I will be finally able to play games in 1080p on full graphics with fluid and silky smooth frames. I was wrong! Stuttering is killing me no matter what I configure and finetune, spent days fixing stuttering and nothing helps",Negative
AMD,"Got a 9070xt Sapphire Pure in my first ever build paired up with a 9800x3d.  It's handled everything I throw at it in 2k. Haven't tried anything higher than that.  Consistently getting >200 fps in Cyberpunk on Ultra with raytracing on and a handful of mods. (I'm assuming this is good, I have very little PC experience).  At first I thought I choose wrong because CP2077 kept crashing but I just needed to update some drivers and the BIOS and reinstall. Been smooth sailing since.",Positive
AMD,"The only issues I've had has been that my AMD Adrenaline gpu fan curves seemed to reset from time to time for unknown reason: maybe windows updates, didn't research futher. So what I've done is set the profile of curves for individual games separately. So far the profile settings have stayed correctly. Yes it takes a bit of work especially if you play a lot of different games.  Hardware issues seem to be a bit of coil whine when playing games that reach very high fps, however so did my previous rtx2070 so it didn't really bother me. Gpu in question: Powercolor Hellhound.",Neutral
AMD,"Perfectly happy. Last had AMD GPU in 2011. Nvidia since 2015. I couldn't justify Nvidias prices. What I wanted to upgrade my 3080. Got my 9070XT for MSRP, $600. FSR and frame gen are really good now. I kind of miss really good ray tracing but let's be honest not many games use it",Positive
AMD,I have PowerColor Hellhound version. Its fantastic.  Only drawback is lack of FSR 4 support in many games.  For Single player games you need to use Optiscaler for best expierience possible,Positive
AMD,"I've had it for about a week now and it's been pretty amazing. I came from a 6950xt and a Ryzen 5800x to now a 9070xt and a Ryzen 7600x and it's been pretty awesome. Cyberpunk has new life now with optiscaler, and Ray tracing.",Positive
AMD,"No problems here, i have 9070xt and 9800x3d both undervolted and overclocked. Consumes less power and runs colder for the same performance i fcking love amd. Before i had intel and nvidia (4080 14700k).",Positive
AMD,"fantastic card, good vram amount and fsr4 is amazing  probably gonna hold onto it for many years",Positive
AMD,"Mostly smooth, went from 1080ti to 9070xt.   There are some game crashes, but very, very few and i dont think they were from the gpu as i was doing some other stuff while playing kcd 2  Even if it isz it prolly got solved from the new chipset update.",Positive
AMD,I just switched to a 9070xt from an rtx3060ti. Switching was a bit of an hassle but after figuring everything out its amazing. Its such a strong and fast card. It can easily run baldurs gate 3 on 1440p.,Positive
AMD,So good that I ordered one for my wife,Positive
AMD,"I've had a lot of experience with a 7900xt and 9070xt. They're both great and I've had zero driver or timeout issues. These complaints come from 2 main largely uninformed places.    1) 8 or 9 years ago AMD had driver issues! And those issues were common.    2) since nvidia has such a big market share, the majority of prebuilt and hand-me-down pcs have nvidia gpus in them. When someone who isn't very knowledgeable upgrades to an amd card, there's a non-zero chance that they don't understand that Nvidia drivers will seriously mess with you if you're running an amd card. These people just download new drivers and don't wipe the old drivers and complain when everything is a train wreck. Any time you switch between companies, run DDU to completely uninstall previous drivers or my preference, just use a fresh Windows install.",Positive
AMD,"Powercolor Red Devil. I’m quite happy with it. My only issue is Dolby Atmos/Windows is rather buggy, so I’m “living with 5.1” on the livingroom pc :)  The old 1080ti didn’t suffer from b0rken Atmos issues and “downmixed” 5.1.4 to 5.1 nicely.  Most likely an AMD driver issue, but apart from that, I can recommend the 907xt.",Positive
AMD,"It’s bullshit. I currently use 5 gaming PCs. 3080 12gb, 3080ti, 4080, rx6800 and rx5700xt. None of them have any noticeable driver issues. Over the last year, each card probably had one driver crash, if at all.   If there was a problem with maybe one driver lowering fps a little, I didn’t notice. I never look at fps counters, I play games and see how they feel.   Also Reddit is no benchmark. They will tell you 3080ies are 1080p or at best 1440p now, while i happily have my ti on my 4K/60 tv and never notice any problems except in the handful most demanding games. So I play cyberpunk on the 4080. It has an ultrawide monitor anyway.   The TV setup will handily beat the steam machine when it comes out. It’s a 5800x3d with 32gb and said 3080ti, and it cost me all used parts slightly under 900€, which is what I expect the steam machine to cost.  Steam machine thus is a good indicator about the way we are heading. If the next console gen doesn’t make a huge jump in performance, games will probably stagnate at the level they are right now. So if your machine can handle AAA right now, it probably will for the foreseeable future.   Get the 9070xt, or a 4070ti super or a 5070ti, it probably won’t matter much, other than what you pay for it. You will have a similar experience and enjoyment.",Negative
AMD,"I upgraded to a 5070ti when they first came out and had nothing but issues with it, unstable drivers and constant crashes. So I sold it and picked up a 9070xt. It’s been flawless ever since, I was team green since GTX 760 but now I think I’m switching sides",Positive
AMD,"We own two 9070 XTs.  My girlfriend has one in her windows 11 machine, I have never ever heard her complain about performance.   My secondary gaming PC (Linux) also has a 9070 XT And it runs anything I throw at it. No stutters at all.",Positive
AMD,"My only real gripe is with the AMD software itself; specifically when the process just randomly stops sometimes. Like I'll try to look at the metrics page or go to tweak a setting and realise that it stopped running at some point without any warning or message etc and so I have to relaunch it. It's not a huge deal, just slightly annoying.",Negative
AMD,A lot of people just repeat what they have heard. If every person owned an AMD card to know they were bad then Nvidia wouldn't have 92% of the GPU market.   It's a solid card.,Negative
AMD,"The GPU game is all b******* AMD versus Nvidia just comes down to what game or what program you're running. It seems to me most of the time and nvidos ahead of the curb and they're just better, but I don't think that AMD is bad at all. I have a 9070 XT currently but only games developed with AMD are doing well.   That said, it's amazing and I mean that because my 9070 IT is not a red devil or anything fancy. It came in a pre-build and it's pretty basic but if I'm just undervolt it by like like -5, whatever that means on the scale. But if I just barely undervolt it, the clock speed goes from a base clock of like $2,900 up to 31.32 all the way up to like 34 when it'll crash the game. It's crazy how well it will overclock naturally if you just undervolt it a little bit.   Now that's that. I can't account for the quality of the product on anything because it all comes down to the company making the card and my previous card was a 3070 FTW ultra or whatever. It was called from EVGA and that thing was awesome but it just got to be a little out of date and I could read the writing on the wall with all the other system issues, an i7, 9700 etc etc. But that said, if you have the money to do it, I would recommend getting a 5070 TI over a 9070 XT, but if you're in a pinch and you play at anything greater than 1080P, I would absolutely make sure you get the 9070 XT for the additional vram.  Forgive the grammar but reddit's voice to text is garbage and I'm not going to go back and edit a bunch of minor errors.",Neutral
AMD,"Had a 9070xt pulse. It would crash about one in three times I loaded up alan wake 2. Clean install, drivers up to date. No OC. SF750 psu  So I returned it and got a 5080FE. This is quieter, smaller, and gives a smoother experience in AWII",Negative
AMD,"Funny this showed up as I started to have issues, PC recognizes second monitor but my monitor shows ""cable not connected"" but recognizes the cable (it'll blink trying to connect). Used DDU to reinstall AMD Drivers and that didn't work. Gonna test hdmi... (left and continuing comment)    After testing another cable DP (monitor) to HDMI (pc) and that NOT working, I put the old cable back and it works again. Frustrating and would like to understand what went wrong lol",Negative
AMD,"I had a lot of trouble with lag in chrome and discord, ended up having to edit the registry to disable some windows setting to fix that.  There was an issue where my PC couldn't idle when plugged into a 4k HDR tv that was turned off, it would cause a reboot, this issue was fixed in the most recent driver update.  To this day if I zoom quickly in paint.net or gimp my GPU drivers crash.  It hasn't been a perfect experience for me so far, but it has been slowly improving over the months, I feel like I sold my 3080 10GB at the right time as it paid for half of the new card, I also didn't want to reward Nvidia for their insane pricing and fire hazard power connector.",Negative
AMD,"I have a 6800xt and while not a 9070xt like you're asking it's still a recent AMD card. With a 1000W PSU  random transient spikes hard crash my system in any game I play. BIOS chipset, adrenaline all updated with the same results over numerous versions. Multiple fresh driver installs using DDU, complete windows reinstalls nothing fixed it. Now I'm able to minimize my crashes with some very specific bios settings and throttling the card to 90% power and max frequency.  I like AMD and want to support them, but fml is my particular card a headache. I think I would still buy another AMD card since I still believe this is an isolated incident, but my buddy has a 6950xt and he has crashing issues too. Disabling multiplane overlay in the registry seems to have solved his issues though.",Negative
AMD,"Frequent driver crashes, and I mean once a week ish with daily use. Other than that, it functions without issues and i would say it's a worthwhile purchase. I think I just need to do a clean install and that might resolve my issues. Also intending to switch to Linux because my sanity can't take windows 11 much longer, but I am not sure if that affects the GPU in any way",Neutral
AMD,The drivers are shit - I side graded from a 4070 to the 9070XT and it was horrible. Purchased a 5080 and haven't looked back. Can't even mail this gpu to a friend because the drivers fking suck.,Negative
AMD,Absolutely shit,Negative
AMD,"the only amd card I had came defective, constant whea id 18 errors in games, safe to say i am most likely never going for an amd gpu again, or in a long time...",Negative
AMD,"multiplayer games always crash, not sure why. other than that it’s great",Negative
AMD,"9070xt pure stock eveything is fine**. Had to kill most overlays and do a few registry changes to keep it stable and the driver hangs are down to once every week. No hard crashes. Had to configurre the igp as gpu for my brave browser due to hangs. No problem in games, still rocking the 25.9 1 driver. Overclock is meh, undervolt is good, software is okish if you kill the bloat (metrics and ai)  Definitively a worse user experience than my ex 3060, 1070 and 970. Similar exp to the old 7950HD vaporX but totally different from the 5570HD by sentey that crappy card overclocked like crazy with a toy cooler.  I don't think much changed there. Nv still king for windows, amd still king for linux. Good enough for mid-high(ish) end rigs, just not the best out there.  Would I trade in my 9070xt for a 5070? Heck no. A 5070Ti at same price? Yup. Would I pay up the 200euro extra it cost where I live? Nope. That was a 4TB ssd, or extra 32gb ram before everything went boom past october  I used to be fine with the NV 70' tier, but since the 3000 nv has been really overpriced and skimming on vram that's why I jumped to red team. I would not think twice if the 5070ti was at the same price. So it's well priced, makes sense at 650 for me. I still get to crank everything except RT to ultra without hesitation at 2560x1080 uwide. But that's more on UE5 side than the gpu itself.  Fsr4 is serviceable I don't like it though but it's getting to a point where I don't realize is there. So all good.   What can I say I feel right back at my 7950 era where it traded blows with the 6070 but lost to the 680. They are still there. Between the 70 and the 80. At least this time I picked one version without toasty vrm / mem.  No major complains but, i would not doubt to go back to green team. Cuda is still better almost in eveything relevant for me and vulkan/rocm is serviceable.",Neutral
AMD,Some coil whine on specific games and areas.  Perfect otherwise.,Neutral
AMD,"My ""feelings"" are a little bit mixed. I bought my 9070 XT Sapphire Pulse during BF sales.  Anyway I upgraded from  2080 Strix(non-Ti) to 9070 XT, and at the time I was playing Star Wars Outlaws. I ran the SWO on medium settings in 4K and it ran fine, very little FPS drops etc, but when I installed 9070 XT and of course cranked the SWO settings to max, and well the FPS pretty much tanked. Even with FSR(whatever versio SWO supports), the game actually ran worse with FSR. I then tinkered little with the SWO settings and got it to run ok, but honestly I was kind of disappointed at this point. Sure ubi game is ubi game but still. After I finished SWO the newest game I have played is Atomfall, but that game isn't exactly system taxing game, and I am pretty sure, if I remember correctly, that I am running it on max settings. One thing I can say about the Sapphire Pulse is that it is quiet as a mouse compared to the 2080 I had. I guess I should run some 3Dmark etc tests just to see what that ""thing"" can and cannot do, or once again install CP2077 😁. But do I think 9070 XT Sapphire Pulse is a bad GPU absolutely not, and I would still buy it again. Also it was reasonably priced, and if all the hype about the next gen AMD GPUs holds up, the 9070 XT will nicely carry me the next gen. I just that memory shortage does mess up the next gen AMD GPU pricing. Also the only problem that I've had with my 9070 XT is that when I wake up screen/GPU, not my entire PC(never use sleep mode), the screen ""freezes"" kind of. I can click things and hear thing happening, but nothing changes on the screen. I guess the drivers, so to speak, get confused which of my monitors is actually in use since I have PC monitor and TV hooked up to my GPU. It is easy to fix by turning on my TV, and then turning my monitor from power button, so that the GPU output automatically switches to my TV, and then I just turn my monitors back on from power button and a voila the screen ""jumps"" back to my PC monitor and everything works as it should.",Neutral
AMD,Bahahaha they’re probably the same AMD shills who say that driver issues are a thing of the past.,Negative
AMD,"Had mine since release - the only negatives that come to mind is that I had to reinstall the drivers... Twice? Maybe three times? due to the automatic installation causing corruption and with the brand I got (Sapphire Nitro+) has some major league coil whine issues. My PC sounds like an angry hornet under load of anything above 100/120fps.  However performance barring the whine has been great, RT is fairly solid, all my games now run at minimum 90fps on 1440p, it's a great undervolt card and all for a fraction of the price Nvidia wanted for their cards.",Negative
AMD,It didn't work upon first install... Because I screwed up the install. After that it's been flawless and the driver supports been good.,Negative
AMD,"I had to devolt and lower frequency on mine cos it was boosting to frequencies that the card was not made for. That stopped crashes, and now it works like a charm. Outside of that it’s great.",Positive
AMD,How the 9070XT performs in Handbrake for gpu acceleration encoding hevc ? I had very good results with a 3080 10gb and I just bought a used 4080s just for this task (and gaming ahah) But I was tempted by team red but couldn’t make the move regarding my experience with the Nvidia gpus.,Positive
AMD,"They are living in the past - it’s been a very smooth experience so far. Not a single crash, Radeon works like a charm for me and gives me fantastic performances",Positive
AMD,"2 main reasons for that:  * usually only people who have problem would care to make a complain post or seek help  * lots of people really have little idea of what they are doing, and blindly go undervolting just according to some random post they saw. As every card is different, there's no guarantee my UV setting will work on your card. Not to mention people straight up lie/inflate their result just to be superior.",Negative
AMD,"Same, but sitting at 10 months here. No issues in games, rocked cyberpunk just without the ray tracing, 33 runs great but then bg3 did as well...and I got the oc version cheap at microcenter because the person who bought it brought it back to buy an Nvidia card cause the recipient got pissed seeing it not being Nvidia. Tis a great card.   And with the 9060xt competing directly with the 5070ti why pay more just for ray tracing when FSR is working pretty sweetly?",Positive
AMD,Same here. I’m very happy with mine.,Positive
AMD,"Yeup. Same. “AMD has bad drivers” stopped being true years ago, but the meme can never be killed. Funny enough, Nvidia has bad drivers now.",Negative
AMD,Same and my 6900XT before that as well,Neutral
AMD,"Same. Got mine a couple of weeks after launch and it was supposed to be a ""bad model"" (Gigabyte MSRP base model).  Absolutely zero issues in any games with it at stock and with some small fan curve edits and an undervolt it is on footing with some of the higher clocked models.  Interestingly I also have less issues than some of my friends who are rocking last gen Nvidias (4070 and 4070S) which had surprised me.",Neutral
AMD,How is inference?,Neutral
AMD,When Borderlands 4 came out - zero driver/bug/performance issues.  It was nice to have a card that let me enjoy that game in particular on day one.,Positive
AMD,not even adrenalin? 7900gre owner here,Negative
AMD,"What was your crash from? Mine's a 9070XT and I've been using it on Fedora. Just recently, I've started having a particular shader in Minecraft that doesn't work well with it, at least in Linux, and I've gotten a lot of crashes the last couple of days. I think it has something to do with an update, and this particular shader, because it wasn't doing that before and it runs other shaders (even at very high settings) perfectly. It hasn't had any issues outside of that game with that shader.",Negative
AMD,"The only problem with FSR4 is that (unlike FSR3 and prior, but like DLSS) it needs to be implemented game by game. And since it's a lot newer than DLSS (DLSS 2.0 came out about 6 years ago), and it's exclusive to the newer generation of Radeon, there aren't many games supporting it...",Negative
AMD,"10 years ago is about the release date of rx480, which is quite reliable to me.",Neutral
AMD,"It's not quite that long ago. The rx 5700 was the series that seemed to have significant driver issues, but the point is still very much valid - the drivers were an issue for a short while, and haven't been for quite some time.",Neutral
AMD,"Nah man. I've had a 7900xtx for a few years, I'll never buy amd again due to the driver timeouts I've been dealing with since I bought it. I have a friend with the same card and he feels the same.",Negative
AMD,Stuttering and driver issues were still very much present with my RX 6800. Immediately fixed after swapping to an RTX 4070.,Negative
AMD,Not really. The 7900XTX has driver timeouts and crashes to this day on multiple games.,Negative
AMD,more 13 years,Neutral
AMD,"I don't know about the 9000 series but I've had driver issues I could ultimately never fix with my RX 6600.  They got better, but even as of 3 years ago driver issues weren't completely a thing of the past.",Negative
AMD,That’s not true. Just one datapoint but amd GPU didn’t work for me when I built my system two years ago. All new amd cpu and gpu and was super unstable for months. Required me to play with settings until stable then any updates would break it again.  Felt it was unnacceptable for 1000+ usd card and returned it for a 4090 and been rock steady ever since.,Negative
AMD,Or they just read userbenchmark reviews.,Neutral
AMD,Observational but I find it’s still true. Most issues on Reddit I see are AMD 6000/9000 series trouble.  If you are comfortable tinkering a bit the 9000 series is fine. Expectations should be: it will work fine for most games but you’re more likely to experience issues compared to Nvidia  Again: this is observational and not fact,Negative
AMD,"I have a 5700xt, it's been awful. Only the last driver update sorted years of crashing in Hitman - which was caused by a prior update. Still get weekly green screens, sudden locks at 3FPS (on desktop too). The radeon software is completely unreliable for recording, and can't do basic things like remember settings (why is my C drive full, oh AMD has decided to change my recording folder again).     Oh and the driver timeouts.. good lord. Nightmare of a card. AMD don't seem to give a single shit though.    Not to mention the unbelievable noise and heat. I would be saying worse than their colleagues.",Negative
AMD,"Bro, I had a discussion with my buddy about the 9070 xt and he talks about how he worries about driver issues and games running poorly. Then today he said I should get an intel arc because there drivers are better. His mind is fried",Negative
AMD,"Nah, I bought a 7900xtx brand new and its given me some problems for sure. Drivers are STILL unreliable and every so often I have to roll back a driver to a previous version because its more stable in a game",Negative
AMD,It's really that noticeable? I'm on a 3080 and I've been flirting with the idea of jumping to the 9070xt especially now that I'm on Linux full time but I didn't think the gains were that significant.,Neutral
AMD,"that is the same upgrade I did, but at 3 x 4K monitors.  Only game on the 1 but.   The 9070XT is great update for me. Been getting close to 100fps on nearly everything I play.   The only weird quirk I had was a reinstall of windows to update the AMD drivers. The release drivers worked when I switched over, but any other GPU driver update would cause render issues. DDU and everything did not fix it. In the end, it was over 12 months on a windows install, so reinstalling windows fixed that quirk.   Everything Linux, I have thrown at it, has been fine.",Positive
AMD,My exact upgrade recently. AMD and Nvidia have the same amount (if any).,Positive
AMD,> and every once in a while the recording software turns itself for an unknown reason  Maybe you're unknowingly hitting the default hotkeys.,Neutral
AMD,"Damn, exactly what I've been debating. I have a 10GB 3080 and it's still going strong at 1440p, but been scoping out possible upgrade paths eventually",Positive
AMD,Yea fallout 4 barely worked on my 3070 without modding and setting hard frame caps,Negative
AMD,"Unfortunately there's really no way to play Fallout properly without modding.  The previous two updates broke everything, so you may have to revert to a previous version. It fucking sucks, but it is a good game. Just takes some work to get running well.",Negative
AMD,"Yeah need the high fps mod fix and couple others really, playing through fallout London currently",Neutral
AMD,"the only real issue i have is / were driver drashes.  depending on what i did, even frequent. i uninstalled AMD adrenaline for now which made it stop completely, cant tell you what it caused tbh.  the driver crash ONLY happened in fallout 4, modded, light modded, heavy modded, no mods, fallout london, doesnt matter. undervolted, basic performance, i tried EVERYTHING (even modder discord with crashlogs and whatnot). pretty much crashed in the first 5 minutes of the game. i can play fallout new vegas and fallout 76 just fine.  and then once a week or so the driver crashed randomly, made my screens go black, i wait for 10 seconds and the driver is back up. mainly in path of exile, which i played at that time.  thats the only issues i had so far. other than that, buttery smooth. have it since launch. no crash on any other AAA or indie title.",Negative
AMD,Try disabling weapon debris in the settings.,Neutral
AMD,"For me what changed everything was turning off god rays. Open console and type ""gr off"" without the quotes and voilá, zero crashes so far. You could edit .ini files and set them to read only for permanent solution though.",Positive
AMD,"The 9070 XT has an issue where it overclocks above spec in some scenarios. Get GPU-Z or HWinfo and monitor your clock speed while gaming.  If it exceeds ~3300 and gets up in the 3400 range, and then crashes, you should lower the card's max boost clock in Adrenaline by 100 or 200.  I had the same issue, specifically in cutscenes, and when I undervolted + power limited my card. It would shoot up to over 3400 MHz in cutscenes, and combined with the undervolt + lower power limit would make it unstable.  It would boost differently from game to game, and could handle different boost clocks differently, so I just made individual game profiles for the affected games. Some I'd set a -300 max boost, some -200 etc.",Neutral
AMD,Are you using path tracing at all?,Neutral
AMD,"What is it with extraction shooters and having weird issues?  Darktide, Helldivers and now Arc Raiders all just occasionally have the game break in horrific ways for people.",Negative
AMD,Are you overclocking? Follow a video on YT or something and make sure your settings are optimal.   I've been playing Arc Raiders for hours on end with all video options set to ultra and haven't had a single crash.,Positive
AMD,"weird, I have that card, and play a wide variety of games, and even Arc raiders during the free weekend, and never have issues",Negative
AMD,how fat is your PSU? I recently had a friend that did the same as you but he was on a 400w PSU and his 9070 did NOT like that.,Negative
AMD,"could be a windows issue too. forbidding windows to automatically download video drivers seems to help sometimes. back when i swapped to my 6950XT i had a few hitches here and there that disappeared completely when i did a clean reinstall of the OS. so if you haven't done that in a while that would be a good idea, even if it's a bit annoying.",Neutral
AMD,Same here.  My only issue is that it’s too big to downsize my case like I was planning to but that’s kind of on me.,Negative
AMD,"they include this anti sag thing, do you bother using it? should I ?",Neutral
AMD,"Is that emulation in Linux? Given some of your statements, I read it that way. Just verifying. I am about to start my emulation journey and am very curious.",Neutral
AMD,"This is at 1080p, I'm guessing??",Neutral
AMD,What cpu are you using? Wilds run like shit for me even with the 9070xt. I get around 60fps at 1440p with extreme stutters and lows around 30-35fps.  I managed to tweek the settings and get a somewhat static 45fps locked but that's about as good as I could get it to work.,Negative
AMD,"Funny I have this same issue and particularly its with BF6 it likes to crash, I have to make sure my PC doesnt wake up from sleep mode in order to minimize the crashing.",Negative
AMD,Too late to RMA the card? That's not normal.,Neutral
AMD,How big of a jump was it? I am also on 3060ti but debating if I should also upgrade,Neutral
AMD,tbf unless your running 4k upscaling isnt really needed imo its  a very strong 1440p native card unless some unoptimized slop comes out you wanna play lmao,Neutral
AMD,How dare you! /s,Neutral
AMD,why not? anything specific or overall experience?,Neutral
AMD,Driver crashes i eventually resolved after hours of debugging Pubg didnt work at all with this card even with fresh drivers  Never had any issues with Nvidia Had to rerurn to Nvidia despite their high prices  Dont want to bash amd i really wanted to make the switch,Neutral
AMD,"Just wanted to chime in, I’ve had my 7900xtx for two years. I’ve never had a single issue. May never go back to Nvidia. So there’s that.",Neutral
AMD,"Windows related afaik, especially if monitors are different resolutions",Neutral
AMD,">There *is* still a certain ""just works"" experience with nvidia that still isn't present with AMD. Every game has some tweaking to be done so it runs nicely, and some games seem to ignore frame rate caps, which might be a bug.  My experience as well (9060xt 16GB). Whatever issues I've had were Adrenalin/Chill related. Like for example, I fired up Vampire Survivors, and for whatever reason I was getting major framepacing issues. Turns out I needed to either turn off Chill, or enable the overlay (wtf) - either of these solutions instantly fixed the pacing issues.  Still, 8.5/10, would buy AMD again.",Neutral
AMD,ENJOY YOUR new RX9070 card..,Neutral
AMD,"I had to DUU first the nvidia drivers then restart, then same with amd. The install fresh amd drivers. Then no issues.",Neutral
AMD,"With Valve and Linux heavily favoring AMD and nvidia cutting down production for consumers, I can only see AMD winning this",Neutral
AMD,Do you have one?,Neutral
AMD,Try optiscaler with bg3. Fsr4 native AA looks great compared to fsr 2.,Positive
AMD,i have 7900gre and for every driver update i have to see users reporting for stability before updating. i am on oct driver still. maybe i am in a parallel universe,Neutral
AMD,"To be fair there were issues with the latest drivers which caused pc’s to fully crash when the computer put the screen to sleep, if you were using a high refresh rate monitor.",Negative
AMD,Haha. How to lie 101.,Neutral
AMD,"Check your connections. My 9070 XT was crashing a bunch when I played Stellar Blade back in June. 2 of the pins in one of my PCIE connectors weren't in all the way. Once I fixed that, the crashes were gone.",Negative
AMD,That was 10 years ago..? Oh god..,Negative
AMD,I've still got an rx480 running my old machine. I was still using it up until a few months ago with no complaints.,Positive
AMD,No they are spot on lol usually people dont know how bad it is till they come from a more stable system lol my wife has the 9060xt and it has been nothing but problems since day one,Negative
AMD,"Didn’t realize how many issues they had until I switched to something else. I had a 5700XT and ended up getting a really good deal on a 3060Ti a while back. When I installed the new card in my system, it shocked me how much better things felt. Not just a higher framerate but a much, much more stable one.",Positive
AMD,"I returned this card due to said issues (got the card and it crashed non-stop) and I regret the super overpriced EVGA RTX 3060 I got after (which either that or motherboard are creating some lock ups with idling/ low use in my system, but nothing too impactful. Also regret not having it to dump in crypto craze number X when they were in demand to get a RX 67/6800. I guess I'll stay 3060 for a long while unless I get in on a price error card.",Negative
AMD,"I had a 5700xt at release and within a few months all the major problems had been resolved.  But that's what makes it so hard to talk about GPU drivers. Experiences can be so mixed based on other hardware, settings, and the games you play.  I have no trouble believing some people had a ton of trouble with them, but I had a really good experience.",Neutral
AMD,"I newver had issues. I had issues just before win 11 release and no gaming avaialble. I moved to 11 (PR) . It worked fine (quicker than a fresh install to previous version (yes explorer crashed on right click at the time, but total commander)).",Neutral
AMD,"> The rx 5700 was the series that seemed to have significant driver issues  This is technically true - it had some issues at release and generated some negative headlines about them - but also not really true at all from the perspective of actual experience.  Almost all of the issues were patched in a month or two at most, and I suspect the vast majority of people who own the card never even experienced the bad drivers in question.  Some of the ""issues"" were also entirely on the game side of things and were fixed quickly via game patches.",Negative
AMD,Which is why it’s baffling to me that AMD tried to EOL driver support for 2 year old cards recently. Luckily they backtracked. But they only just recently repaired their driver reputation.,Negative
AMD,"Mine worked perfectly, never had any problems with my XFX RX 6800; it's the best value for money and power consumption I've ever had.",Positive
AMD,Are you on Linux? They just fixed the Linux drivers a few weeks ago.,Neutral
AMD,"Agree. Recently had to roll back a driver for the 7700xt in Arc Raiders.  Also during the time Helldivers 2 came out, my 7900xtx was crashing all-the-time on that game. Has been having no problem since, but I also didn’t run any launch titles anymore, except Arc Raiders beta.",Negative
AMD,It depends if you are on 1080p or 1440p,Neutral
AMD,On linux the difference will be even bigger when using proton. Rt works kinda bad but it's not great on nvidia either and valve is working to fixing raytracing so it might get better than nvidia despite worse rt hardware.,Negative
AMD,I play at 1440p and shoot for 120fps so this change was significant in that it let me achieve this in basically every game I play.,Neutral
AMD,"it's like 30 percent faster unless you're talking about call of duty or something which is an exception and does so much better than nvidia in general. I could notice a 30 percent faster gpu but would I upgrade my system just for that? Hell no. That's why I got a 5080 which at least gave me a 55 percent jump. I'd prefer more like 80 percent but the 5080 is actually a 5070 going off of what those numbers used to mean. I was fine with it because of the paypal 20 percent cash back thing that got it to $800. So i get the appeal of the 9070 xt. I just feel like most of the reason to upgrade from a 3080 (most of them being 10 gb) is the VRAM. In games where you were running out of VRAM, that's a huge difference in how the textures look  performance numbers from hardware unboxed 9070 xt launch review",Positive
AMD,I had no issues with my rtx 3060 other than turning off God rays or it would crash. I've seen others write online having the same issue with 9070xt. One Redditor I saw said it was a vulcan issue and a mod fixed it for them. I just do not feel like fiddling with mods to have the game work and then crash again because of an update,Negative
AMD,I do have it for xbox as it is what I originally played it on. So i always have that as backup. The pc version I have was a giveaway or a sale buy so not a big deal,Neutral
AMD,"Yep that's been it for me too. Forza 4 crashed a bit, but i set an underclock in custom driver settings. It still happened and then I realized it was due to my LED light software. Turned it to a static colour and it was fine. No idea if it was the gpu, signal rgb or both, but that fixed the odd crash that would occur. Other than that everything has just worked",Neutral
AMD,Didnt work with my 9070xt. I tried all of the edits without going full mod route and nothing works. The game would work for like a minute at best and the drivers would crash,Negative
AMD,"I had to do this at launch until drivers eventually resolved the issue for me naturally, but yeah this was a thing that was rolling around quite a bit at launch that didn't get talked about enough.",Neutral
AMD,Same Arc Raiders works perfectly fine for me,Positive
AMD,My PC rig:    \- MSI MAG B650 Tomahawk WIFI   \- PowerColor Hellhound AMD Radeon RX 9070 XT 16GB GDDR6   \- Samsung 980 Pro 2 TB NVMe   \- AMD Ryzen 7 9700X + Thermalright Phantom Spirit 120 SE Black CPU Air Cooler   \- CORSAIR Vengeance 32GB (2 x 16GB)   \- CORSAIR RMx Shift Series RM850x Shift Fully Modular 80PLUS Gold ATX Power Supply   \- FD Pop Air RGB case,Neutral
AMD,"Yeah had to buy a new case lol. Went for the LianLi O11 vision compact. Great case, plenty of room",Positive
AMD,My card sags for sure. I used one,Neutral
AMD,Currently on windows 11. As for the emulator previous I was using Yuzu but I'm currently using Eden. I don't know if Yuzu had the option to use AMD's FSR technology but Eden does have it. Let me tell you xenoblade 3 looks so good with it. In the near future I'm going to switch to Linux. But if emulation is unplayable I'll just dual boot.,Positive
AMD,I been trying to see if its an issue I can remedy or hope its a driver issue. I been reading some of the comments where the card likes to overclock by itself or something but yeah I can see about potentially RMA it with the manufacture if there's no fix for it.,Negative
AMD,Substantial,Neutral
AMD,"For ppl who use pc a lot lot and dint upgrade frequently,its  a no brainer to just pay nvidea more 100€ or so",Neutral
AMD,"must have been a faulty gpu or software issue, 9070 xt since october and it was a beast, this is my first ever gpu and i swear to god, i am way satisfied than ever.",Positive
AMD,"AMD definitely requires tweaking so it's not as plug and play and I can't exactly call the occasional issues that I have out for being an AMD problem, especially since I had arguably more problems on an intel nvidia pc and they were bigger problems where the pc would crash for no reason.   Now when I crash, it's due to an unstable undervolt (my fault) or me messing around with something like ai models and rocm being immature causing no display occasionally when vram gets exceeded (doing it for fun so it doesn't bother me). So for pure gaming, it is basically unproblematic if you don't mess with the gpu voltage and clock speess.  So, would buy AMD again if they offer alot better price to performance like they usually do in Australia (800$ 9070 vs 870$ 5070).",Negative
AMD,"It's not that they favour AMD as such, AMD drivers went open source nearly 20 years ago but Nvidia only went open source a year ago.  It's a no brainer to go AMD if you plan on using Linux, sometimes AMD performs better on linux than windows but this is never the case with Nvidia currently.",Neutral
AMD,Yes hated every minute of using it,Neutral
AMD,"Tbh I have a 5080 and it's the exact same story here. I check the reddit thread to see the issues, and last driver I had to DDU and reinstall because I had a flicker issue.  Both sides are equally bad.",Negative
AMD,"I’m on a 6950xt, and I’ve never had to pay attention to what others are saying about stability. I just let it update, zero issues ever.   Sounds like a you issue rather then a driver issue.",Negative
AMD,I haven't updated my driver since last year after I first installed the GPU. Is there anything I'm missing?,Negative
AMD,The majority of complaints that I see about AMD drivers these days seem to involve the Navi 31 based cards for whatever reason.,Neutral
AMD,"Ah that’s a good one, then. I do have a high refresh rate display but I disable Windows putting the screen to standby because the screen does that itself if the image becomes static for so long. So, I had never encountered that issue.",Positive
AMD,"Same dude, same.",Neutral
AMD,I remember when I wanted to buy 580 and never have.,Negative
AMD,And next year it will be 15 years from release of Skyrim. Sorry buddy.,Negative
AMD,"Yeah that was my first real GPU back in 2016, in my first personal PC build. I had helped other friends with it since my Dad used to build PCs for a living when I was a kid, but I never had the money to build my own until the RX480 8GB came out.",Positive
AMD,I've got an rx460 running in a bazzite machine rn. Ran bg3 at a somewhat playable framerate,Neutral
AMD,Same - only retired mine for a friend's cast-off 1080 Ti last year.,Neutral
AMD,Evga cards resell for a decent amount of money due to the 30 series being their last nvidia cards.,Neutral
AMD,"Agree got the 5700xt in 2019 the only game I got crashes in were csgo. Card worked great upgraded to the 6800xt 2 years ago with 0 issues on that card. I just upgraded to the 7800x3d and 9070xt 0 issues. I have only used AMD CPUs and gpus do to them typically being the best price to performance, it's been a pretty good experience for the most part.",Positive
AMD,"They recently put RDNA 1 (2019) and RDNA 2 (2020) into a lower tier of driver support without Game Ready drivers or newer features.  That’s not EOL.  EOL is “here’s the last version, we’re done here”.    RDNA 3/4 have full support and the latest version is December 2025.   RDNA 1/2 have “maintenance support” and the latest version is December 2025.   GCN 4/5 (Polaris, Vega) have “extended support” and latest version is August 2025.   GCN before Polaris is EOL and latest version is June 2022.",Neutral
AMD,I'm on 1440p,Neutral
AMD,"I played re4re with it and ran pretty well  Unfortunately i wasnt able to play pubg with this card, when i swapped it with 4070 everything worked fine.",Positive
AMD,https://www.reddit.com/r/AMDHelp/s/4Ufw3qc02Z  looks like everyone is this thread share the me issue,Neutral
AMD,"Lucky you, my 6900xt has some occasional microstutters which I occasionally have been troubleshooting for the past 3 years...",Positive
AMD,"Zero issues with stability of the 6950XT since I got it a couple years back. Only two issues I've encountered. Frame gen on Spiderman Remastered caused subtle lighting problems, and anti-lag on BF6 caused dog shit frame pacing. Both simple fixes of just turning them off.",Negative
AMD,That was me with the R9 290...       I ended up with a Fury Nano though.,Neutral
AMD,"You’re right, it’s not EOL.   In the original patch notes, they said they were putting RDNA 1&2 into bug fixes only, no day one game ready support. But some of those RDNA 2 cards could still be bought as recently as January 2024. So someone with a 23 month old card could stop getting day one driver support for new games.   I didn’t follow it too closely at the time, but Hardware Unboxed had a whole podcast episode about it which I listened to a few weeks ago. I’m interested in AMD doing well because I want healthy competition in the GPU space. I was enjoying seeing their current generation of GPUs do so well and got bummed out that they had made this decision because I feared it would hurt their reputation. They have a lot of catching up to do.",Neutral
AMD,Then you will notice,Neutral
AMD,"You're not alone. I decided to give AMD a 2nd try (last one was the 295x2 years ago and buggy as hell) when a 5070ti was impossible to get earlier this year. Had a 7900XT and for the most part, yeah it worked. But I had a couple games with issues, namely OW2 would stutter for a few minutes at the start of any session. So nothing huge, but they were annoying, and after crawling forums, I could ever find a fix.   Drivers were another issue, and like you, I was remaining on a driver that was months old after reading that others with my card were having issues with the latest driver etc.   Never had an issue with my 1080ti for the 8 years I had it. And the past month since snagging a 5070ti for $630, I'm back to the days of just being able to play my games and update the drivers without fear of something breaking that was just working.  Now, I will give them credit, my 7900XT experience was miles better than the first try I gave AMD, but the 295x2 was a dual gpu card, so I'm sure crossfire/SLI always had more issues than single gpu setups. I just never tried SLI and now that setup isn't possible (or wanted IMO).",Negative
AMD,"1) This is the Internet. You get these paranoid threads after every update of iOS too, all of which apparently are terrible, destroy your battery life, and cause every ill known to man  2) Loads of people on that thread pointing out the update was fine. You just self select to feed your own anxiety.",Negative
AMD,Yeah at the time it's waiting game and then mining hits and then it's never been the same. I still don't have GPU.,Negative
AMD,"I don’t really consider the late in the generation follow on cards to use weird bins for support time, nor do I consider whether you can still buy them since I can find some *old* cards marked as new stock on Newegg (like a HD 5450 for the low low price of $350).  Like the GT 1010 came out in 2022 and I can buy a GT 1030 on Newegg BNIB today and those are in NVidia’s equivalent of extended support.    I am really hoping that they continue game optimizations, especially since there are RDNA 2 hand held devices out there that need as much support as they can get.    I understand not promising new features like Redstone  though.  A significant chunk of new features now rely on RT and ML hardware and every generation of RDNA has made massive leaps in those areas.  RDNA 1 lacked it entirely, RDNA 2’s work but are clearly a first attempt, RDNA 3 is their first competent implementation and RDNA 4 is rather solid at it.  It’s the same way DLSS isn’t supported in 16 series and DLSS Frame Gen is 40 series and up.",Neutral
AMD,"honestly at the age of 34 with a kid, a job and post grad program i want to spend the remaining time i have to actually play games i enjoy, instead of dealing with stuipd driver issues.",Negative
AMD,"I play ow2, I've played on a 3070, 6800xt and now 9070xt red devil, never had that, although i have had issues where it says I've got high ping for one game and I can only replicate it if I use a riser cable, make it make sense!",Negative
AMD,"people experiencing are real issues when they update to the new drivers. you are lucky to not have experiences with any of these. when i got the card a year ago ""the final"" would not stop crashing until after at least two driver updates. it will suck when people listen to your preach and buy an amd card not knowing what the potential risks are.",Negative
AMD,"I think they can be. It’s not that there’s never an issue across various platforms with a driver update. There are certainly some notorious ones. But by and large driver updates are fine and most people across most operating platforms update without issue, and the fact you find a handful of people complaining about a driver attributing any issue they have to whatever they updated last isn’t indicative of an actual issue. It happens across platforms with nearly every update. Mostly it’s guff. When it’s real you certainly know it as the volume of complaints distinguishes the actual issues.   I’m neither team green nor red. I find being a fanboy for a company seriously weird. Buy the best card you can afford that suits your needs. I got the 6950xt for £333 last summer which was a steal, but I’ll happily buy Nvidea also. But if you think Nvidea currently has more stable driver updates than AMD that is simply bonkers. If anything from reporting of late it has been the reverse.",Neutral
AMD,Tbh i think am5 rn is a pretty bad buy considering the price of ram unless you can find it cheap or buy used. I would just upgrade your cpu and stick to am4 for the foreseeable future,Negative
AMD,I'm about to upgrade from the same processor. I think the 5700x is the better deal for the performance and value. GPU might still be a little lacking but a Ryzen 7 processor should definitely make a difference.,Positive
AMD,Only if:   1. The 5800X is a good deal relative to a 5700X.  2. You have a good motherboard and you're confident it will last years.,Positive
AMD,"I would just stick with the 3500 or upgrade to another am4 cpu, as the other comment mentions ddr5 ram prices are going to hurt.",Negative
AMD,"Yeah the 5800X or 5800XT are good upgrades. Sometimes the 5700X is substantially cheaper and the performance difference is pretty minimal. The 3500 in addition to being an older architecture is only 6 cores and 6 threads, all of the upgrade options are 8 cores and 16 threads, and zen 3 has substantially better performance, even a 5600 would be notably better.",Positive
AMD,"Yeah it is a good decision. You should see significant performance boosts, especially in cpu heavy multiplayer games like counter strike and valorant. I went from a 3400g to a 5700x and my frame rates basically doubled in most games combined with the rtx 3060. Couple of months ago everybody would've told you to spend some more and upgrade to am5, but today its a completely different story because of ram prices. Get the 5800x, get a new gpu if you have to and you should be good for 2 to 3 years easily.",Positive
AMD,"5800x is not going to give you much performance over a 5600, if the latter is considerably cheaper.  It’s a good idea to upgrade if you’re being cpu bottlenecked. If your 2060super is hitting near 100% GPU load at an unlocked frame rate, then you need a better GPU… which would likely need the better cpu anyways to be honest.",Neutral
AMD,"Top shelf CPU for AM4 are 5700x3D, 5800x3D and 5950x, if you can snuggle one of these for cheap price you should be good to go for quite a while (or at least I hope as I have a 5700x3D 😅)",Positive
AMD,"Am5 is better, 5800 is cheaper",Positive
AMD,"Thr 5800X is a Ryzen 7, not Ryzen 5😅",Neutral
AMD,Huge jump.,Positive
AMD,"I went from ryzen 5 3600 to ryzen 7 5700x3d and im very setisfied, I dont think am5 is worth it except if you wanna spent more money and upgrade everything",Negative
AMD,"i upgraded from a Ryzen 5 3600 to a Ryzen 7 5700X(bought it USED for 100$) exactly 1 month ago, and i usually play competitive CPU heavy titles and i can assure you, its worth it. I see 80-120FPS increase in most of the games i play @1080p  Just a reminder : the 5800X is a hot chip unless you do something to it from the Bios, so if you are planning to get it, get a Good/Decent CPU cooler as well. Also when i bought the 5700X it was around 58-65 degree celcius while gaming with the Stock AMD cooler, i bought a Deepcool AG400 white, for like 18$ and temps during gaming dropped to 38-40 degree celcius. I also used the Thermal Grizzly Kryonaut thermal paste(its cheap af).",Positive
AMD,"If you can get an x3d cheaply, then yes.",Neutral
AMD,"5700X is better, you'll lose a miniscule amount of performance (under 5% at the very worst or not even noticeable) for much better thermals and TDP.",Negative
AMD,"5800x is a bit of a messy CPU compared to the 5700x...     It runs much hotter, has a much higher TDP (power draw), and as a result OC headroom is worse....",Negative
AMD,"I just upgraded form 3600 to 5800X and it's a great improvement, i think i'll just wait for AM6 and skip AM5",Positive
AMD,"its a good uprade, if u have the money to invest more i would get a 5700x3d/5800x3d and call it a day until am6. Only go for am5 if u want to spend even more and to get atleast a 7800x3d or if u plan to get a even better cpu in the near future.   Also all depends on what games u play, cpu bound games will run fine with any decent gpu. But if u plan to play AAA games and get a better gpu in the future, the 5800x will do just fine.",Positive
AMD,the 5800X is faster by like 2-4% on games. Its literally unnoticeable. The 5700X is a 5800X but with better temps and Lower power consumption,Positive
AMD,Yes it will,Positive
AMD,"AM5 is not cost effective for the performance increase. Need new MoBo, new Ram, and the new CPU. Versus just a CPU if staying AM4",Neutral
AMD,"Was going to say the same thing. OP should go for a 5600 or a 5700X, given that AM4 X3D are now rare and overly expensive.",Neutral
AMD,2-4% for 1.5 times the TDP. There's a reason they called the 5700X what the 5800X should have been.,Neutral
AMD,"https://youtu.be/keCFPCJswuk?si=xRG-CecUThmvCKQE  Not really. 4 year old video but still holds true today, the extra cores really don’t do much for you in the grand majority of games",Neutral
AMD,"Yeah, but am5 CPU are quite a bit faster. Depends on your needs",Positive
AMD,Yes sir!,Neutral
AMD,Who said anything about games,Neutral
AMD,"I'm a huge proponent of the 5600 and throwing a $600 GPU at it, but there have been some canaries going off. With data we had available in 2024, back then I'd fully agree with ""go with the 5600"" and I do still think the 5600 is incredible price/performance. But in 2025 we've seen some oddly CPU intensive titles that have been favoring the older AM4 8-cores as of late and I think that's something builders should be aware of.   Whether they're just outliers or it's becoming a trend is still too early to tell. And if you doubt me, here are BF6 benches of the 6-cores vs. 8-cores. But again: that's just one of small number of titles.   https://www.youtube.com/watch?v=VQ6Udki1rK8&t=6m  Again, this only affects a small number of titles on the market that really utilize those two extra cores. Where it gets even more interesting is that [the 3700x is actually holding its own and seems to outperform vs. the 5600 specifically in their 8-core utilization titles](https://www.youtube.com/watch?v=8uB2GNeYUt8). The nearly identical 3600 without access to those two cores is around 50-60FPS) which highlights how much these extra cores can affect framerate increases. Again, this is a small number of titles *currently.* On most titles, where those extra cores cannot be utilized, the 5600 should beat out the 3700x.  I'd also consider the 5600's 80-90FPS to still be an *excellent* experience especially for someone migrating from console or for an entry level build especially for those still using 60hz monitors. For how long and how many games an 8-core variant vs. a 6-core will stay relevant for I do not know - looking at prior CPUs it typically isn't many releases. But if we do see 8-core standardization for AAA's it's certainly something to at least note.   Comparative to let's say a $100 5600/5600x are the 5700x/5800x/5800XT worth it at $170? Hard to say and it's user/software dependent. If it's a build you *have to stretch out for 4 years*, I'd definitely be inclined to say do it because it might sneak you past reqs that you otherwise would not be able to run. On the same note, by the time the 5600x starts struggling it may be about as old as the 3700x we're seeing for $80 (I wouldn't count on this though, given that it's the 2nd-tier of a best-in-socket CPU that will always remain the 2nd most desirable 'set' of AM4 CPUs). If it's just a short-term CPU intended to carry you long enough to jump off the AM4 bus and buy you time to save up/hope RAM drops/find the right RAM deal, I'd probably lean towards a 5600.",Positive
AMD,"Considering price of RAM alone, not worth it for gaming with a 2060",Negative
AMD,"Thought it was obvious OP is likely talking about gaming performance, since they mentioned what GPU they have",Neutral
AMD,True. Better to go for 5600 to 5800 and bug a better GPU,Neutral
AMD,"5600 and 5800x are very different. Gpu:s  are used in photo/video/3d work, and ai.  But yes with some use cases the differences won't be that noticeable.",Neutral
AMD,"9070XT is really good for 1440p when paired with your 5600X. It's only rated for about 130W more than your 3060 so won't actually use that much power, but you can power limit it and get nearly full performance at a significantly lower power usage.",Positive
AMD,You can get it,Neutral
AMD,"9070 xt is a little much for 1080p gaming  I’d keep cpu, get 9070 xt, and get 1440p monitor and deal with a little bit of a bottleneck. Or…  Get 9070 xt, keep current cpu, and buy 4k monitor. That way those frames you’d be losing from bottleneck you get back in resolution, in a sense   Either way im not buying a 9070 xt to game 1080p",Neutral
AMD,"Yes, that RX 9070 XT Mercury is a strong upgrade for you and will pair fine with your 5600X for 1440p, especially given your rare chance to buy at EU pricing.  RX 9070 XT is roughly ~90–100% faster than an RTX 3060 at 1440p across modern games, so you are getting a huge step up and a real “last dance” upgrade.  For a 3‑year 1440p target and big jump from a 3060, this is a very solid move; the only main reason to wait would be if you specifically want Nvidia for DLSS/RT instead.  People running 5600X with RX 9070xt at 1440p report that it’s a good combo and generally GPU‑limited in modern AAA games, with minor CPU caps mainly in very high‑FPS esports titles.  RX 9070 XT class cards can draw ~300–330 W stock in heavy gaming on many AIB models, including XFX; that’s fine for your 750 W Gold PSU but not ideal for power bills or noise.  Multiple tests on RX 9070 XT show that a mild undervolt and reduced power limit can cut ~20–30% power (often 60–100 W less) with ~0–5% FPS loss:  Example settings often used: around −60 to −85 mV core offset and −15–30% power limit, sometimes with a small memory OC and “fast timings”.   Given your concern about electricity and noise, yes, plan to undervolt in AMD Adrenalin once you confirm stability. Your 750 W PSU is ample, but undervolting will reduce heat, fan noise, and long‑term stress on the card.  RX 9070 XT is designed as a high‑end 1440p card and is within a few percent of Nvidia’s competing 5070/5070 Ti class in raw performance at that resolution, so it has ample headroom for future games.  XFX Mercury models are typically triple‑fan, large‑heatsink cards that run cooler and quieter than reference, and owners are specifically using the Mercury 9070 XT for efficient undervolt profiles without instability.",Positive
AMD,Ahh that's good news ! thank you,Positive
AMD,Thank you will do :pray:,Positive
AMD,"thank you , i forgot to mention that i will get a 2K monitor for sure if i get the gpu",Positive
AMD,What would you buy now for a couple more years of 1080p?,Neutral
AMD,Xiaomi Curved Gaming Monitor G34WQi,Neutral
AMD,I would think the 5600x + 3060 12GB would be perfect for a couple more years of 1080p  Should get 60 fps minimum in most titles at medium setting 1080p,Positive
AMD,"Definitely get the 9070 XT for ultrawide gaming. It's pretty intense on the gpu. Also please research ultrawide gaming before you commit, because there are some downfalls especially if you play a lot of competitive fps games.",Positive
AMD,"If you are only looking for pure gaming performance and want to maximize your FPS, go with the R5 5500X3D.",Neutral
AMD,Ngl it’d be easier giving a budget range and the country you’re in vs this very specific comparison that I somewhat doubt is giving the most value you can find  Also what games you’re trying to play potentially bc different parts have different outcomes,Neutral
AMD,X3d all day.,Neutral
AMD,The 5500x3d was made for competitive games. Get the cheapest one.,Neutral
AMD,"don’t get a 5060 get a 9060xt… there are so many reviews on this man idk why people still think thst the 5060 is a good card. If your solely focused on gaming, get a ryzen 5 7600 for am5 or if you really want to do am4 get the 5500x3d",Negative
AMD,"https://gamersnexus.net/cpus/am4-lives-amd-ryzen-5-5500x3d-cpu-review-benchmarks  The 5500X3D occasionally beats the **14900K.**  If you stream or occasionally do professional work like rendering or video editing then the 14400F can be a better choice, but for pure gaming, X3D chips tend to perform at least one generation ahead of non-X3D chips in their product line.  One asterisk is the motherboard though, if the 5500X3D uses a 300- or 400-series motherboard with PCI-E 3.0, the RTX 5060 could be hurting big time in modern games. The 14400F almost certainly uses PCI-E 4.0 or 5.0 which will be better for that GPU.",Neutral
AMD,"It's obvious from their wording that these are 2 prebuilts they are considering, so the 5060 is the only option.",Neutral
AMD,Hurting mostly happens when you run out of vram so just dont use settings that run out of vram.,Neutral
AMD,"Then find a different prebuilt, not that complicated tbh",Neutral
AMD,"It's always the ""just do this"" people who tend to think the least.  What makes you think they haven't?",Negative
AMD,The 5060 by a fair margin because it’s more recent and has access to newer technology. What price are they though? What about something on sale like this  https://www.walmart.com/ip/iBUYPOWER-Element-SE-Gaming-PC-Desktop-AMD-Ryzen-7-8700F-NVIDIA-GeForce-RTX-5060Ti-8GB-32GB-DDR5-RGB-RAM-1TB-NVMe-SSD-ESA7N56T01/17048165781,Neutral
AMD,"Intel + RTX Combo, i don't trust longetivity of RX Radeon driver update unless it's Radeon 9000 series",Negative
AMD,"I don't like both of them. If the Case is the one pictured then im pretty sure the PCs are overpriced.  On the Ryzen CPU side i dont like the Desktop 8000 Series. So like it doesnt exist for me😂 Oh and i never heard of the RX 7700 16GB, i only know the RX 7700XT 12GB but could be me who missed out on that.  On the Intel side i dont like the RTX 5060. Would be to weak for me. When i buy a PC it should last a bit longer than 2-3 years in terms of upgrading.  And on BOTH PCs i dont like the 16GB RAM. I know it expensive atm but 16GB is nowadays the Office PC recommendation and 32GB+ for everything else.  Would love to see the prices tho cuz you cant recommend without knowing the price.",Negative
AMD,If you have some selfrespect you would choose AMD and your going to start using Linux as operating system.,Neutral
AMD,Have you considered this? its a $899 but you get the 5060 ti and 32gb ram. [https://www.walmart.com/ip/iBUYPOWER-Element-SE-Gaming-PC-Desktop-AMD-Ryzen-7-8700F-NVIDIA-GeForce-RTX-5060Ti-8GB-32GB-DDR5-RGB-RAM-1TB-NVMe-SSD-ESA7N56T01/17048165781?classType=REGULAR&athbdg=L1103&from=/search](https://www.walmart.com/ip/iBUYPOWER-Element-SE-Gaming-PC-Desktop-AMD-Ryzen-7-8700F-NVIDIA-GeForce-RTX-5060Ti-8GB-32GB-DDR5-RGB-RAM-1TB-NVMe-SSD-ESA7N56T01/17048165781?classType=REGULAR&athbdg=L1103&from=/search),Neutral
AMD,The Ryzen was 849.99 and Intel was 779.99,Neutral
AMD,The Ryzen was 849.99 and Intel was 779.99,Neutral
AMD,"Okay for only 70 more i would definetly stick with the Ryzen System. CPU is ~10% faster based on what you play, sometimes the Intel is faster, but overall Ryzen +~10%. The RX 7700 and RTX 5060 should be on the same performance level. But the RX 7700 has double the VRAM of the RTX 5060.  If you want to buy one of the 2 PCs:  -Raw Performance + VRAM Headroom: Ryzen 8700F + RX 7700  -If you care about CUDA, more modern Upscaling (DLSS 4, Multi FrameGen) and better RT: i5 14400F + RTX 5060   Edit: On both PCs i recommend upgrading to 32GB. Both PCs come with only 1x 16GB Stick of RAM. Thats a bottleneck in itself for the CPU.",Positive
AMD,"Given DDR5 is driving consumers away from AM5, it'd be an interesting way to get money out of AM4 owners who need an in place upgrade.",Positive
AMD,"That would make me feel very dumb for recently buying one on the used market 🥲 but holy shit, would be nice if it happened.   Problem is how the process is and how the chips are binned, could they really get enough yield to make it worth their while vs paying to flood the market with lower binned chips nobody wants to buy",Negative
AMD,Hell yeah. This and 5700X3D too.,Negative
AMD,That would be great/hilarious.  Keeping am4 as the all time GOAT platform.,Positive
AMD,Just give is the 5950x3d or 5850x3d.,Neutral
AMD,I'm here just waiting for a 5950X3D.,Neutral
AMD,Me with a 5700x from last year  https://preview.redd.it/6d5l1z4lh38g1.jpeg?width=640&format=pjpg&auto=webp&s=805759206ded804d65b7757c9c01b50b673c9a8a,Neutral
AMD,"Just crazy though they make new chip 5950x3d, make am4 great again.",Positive
AMD,I have the 5900x. Wonder if it’s worth it to get a 5800x3d if the release more,Neutral
AMD,Can’t believe I got $120 AliExpress 5700X3D a year ago,Neutral
AMD,What's that going to do outside of busy fabs that are already committed to other products?   The issue is that we don't have enough chip making things. Going back and making a line that's already retired doesn't solve the issue of not enough chip factories.   All it does is repurpose already strained to meet demand factories to meet some new demand that wasn't planned on.,Negative
AMD,i think that's why they launched the 5800XT. but yeah if they re-release a AM4 x3d chip would be cool. would be nice for my secondary system.,Positive
AMD,Here I am stuck with a 5600x. Next time I upgrade I'm buying the top fucking cpu. This bs will never happen to me again.,Negative
AMD,Fabs already moved on.. why would they go backwards.    They won't  Deal with it.,Neutral
AMD,"AMD, don't fumble ffs! You have a golden opportunity.",Negative
AMD,will there even be capacity to make new ones?  At best maybe they can find a few pallets of those saved up for warranty and sell them.,Negative
AMD,Are we/they?,Neutral
AMD,The 1080 of the CPUs.,Neutral
AMD,I couldn't find a 5800x3d or 5700x3d last year. Not sure if it's worth putting money into a new cpu if i already have a 5800x.,Negative
AMD,"In a normal world where business make products well so that people will buy them, this is a no-brainer. As a chef, if we don't have enough of one ingredient but a lot of another, guess which ingredient I'm running a special on.   However, this is not a normal world. Surely there is some reason why this option can't be done.",Neutral
AMD,this is not happening lol,Negative
AMD,It's not gonna happen. Buy a 5800XT while you still can.,Negative
AMD,that’s like the struggle of my life trying to upgrade without going broke as hell,Negative
AMD,"I doubt it. That would also mean reversing their directive to wind down AM4 motherboards while the ram cartel winds down DDR4 production altogether.  They already reversed the wind down of the B650, so the win-win scenario for them, board makers, and the ram cartel would be for everyone to still switch to the lower AM5 CPUs.",Neutral
AMD,And to think i was gonna go with a 5600 before is said fuck it and pulled then trigger on the 5800x3d,Negative
AMD,"Damn, what number they calling?  I got some grievances of my own",Negative
AMD,Best chip of all time.,Positive
AMD,Well I guess it's more worth it that I still use 5800x3d which I bought close to 2 years ago?,Neutral
AMD,I would buy one tomorrow if they came back out,Neutral
AMD,"Dear AMD, bring us a 5950x3d for Xmas. Thank you",Positive
AMD,BAHAHA AM4 WILL NEVER DIE,Positive
AMD,"I mean, from AMDs perspective they could, but how long would it take to actually get them back in production again? Will there still be demand for them in that time, and would it not be more cost effective to just keep making AM5 chips, or make an AM5 refresh that can use DDR4?",Neutral
AMD,Ddr4 stock will be out by  the end of the month... this is just false hope imo.,Negative
AMD,"No, please don't, because I want to schlep my old one on Facebook for a thousand bucks.  After I do that then yeah, start manufacturing them again.",Negative
AMD,"That chip is fucking immortal :D   Glad I switched my 2600 to 5800X3D back in a day, and held myself on jumping to AM5",Positive
AMD,They could make a 5850x3d but instead of being a binned version like 9850x3d make it like the 9800x3d with the cache on the bottom and unlock overclocking.  Would probably sell for the price of a 9800x3d,Neutral
AMD,Ryzen 9 5900X3D would be Fire XD,Neutral
AMD,I was called a madman when i bought a 5700x3d so late into am4's life,Neutral
AMD,"It’s good and all, but to do that would take time, and their yields might be questionable too. It’s not like they can just click a button and there’s a million 5800X3Ds 😅",Neutral
AMD,I thought about buying a 5800X3D last year so I could upgrade my main CPU and put my current one in a machine still running a 1700 but I figured I'd wait a little longer for the price to go down. Boy that sure was a mistake.,Negative
AMD,would be nice if they could shove zen4/5/6 while they are at it.,Neutral
AMD,"Yeah with RAM/GPU and other prices I might upgrade to this for the near future, but not at silly prices.",Neutral
AMD,Man I sure did want an AM4 X3D chip but... My 5600x is doing me good anyway.,Positive
AMD,"Would be a good move, RAM makes upgrading very very difficult.",Positive
AMD,I don’t know if there is enough demand for AMD do this.   It’s like wagons in the us a small group love wagons but not enough to justify production,Negative
AMD,I just ordered a 5500x3d hours ago since there are no 5700x3d in my country... Would be kinda unlucky if it happens but I'll just enjoy what I can I guess,Negative
AMD,"So happy I decided to splurge on a 5800X3D, it's gonna keep me afloat through these turbulent times",Positive
AMD,"No, no they aren’t.",Neutral
AMD,Hahahaha,Neutral
AMD,It's not like there's spare capacity at fabs: why go back to making 5000-series CPU chiplets when they can keep cranking out modern EPYCs and Threadrippers instead?,Negative
AMD,"Meanwhile I’m trying to sell my 5800x3d, 32gb of ram and a rog Strix motherboard for around $500 and am getting absolutely zero interest lol",Negative
AMD,I sure could use an update to it. This cpu broke XMP on my machine,Negative
AMD,"Three years ago, whenever I brought up chip makers reviving old models I got downvoted to hades for it.  “iT’s iMpoSsiBle whY wOulD they do tHaT?”  Funny.",Negative
AMD,"Ja, in den Rechnern der Kids werkeln aus Kostengründen auch noch 5800x3d. :D",Neutral
AMD,"Ich habe einen 5600x und hatte eigentlich vor, auf AM5 aufzurüsten, weil meine AIO jetzt auch schon 7 Jahre alt ist und ich gleich ein neues Gehäuse haben wollte. Durch den DDR5 Wahnsinn setze ich das jetzt aus. Jetzt hätte ich schon gerne zumindest ein weniger umfassendes Upgrade mit neuem Gehäuse, AiO und einem 8 Kerner.",Neutral
AMD,"Why? Nobody is making ddr4 anymore.  Once inventory is out, it will. E more expensive than ddr5.",Negative
AMD,I feel that it makes more sense to just buy ddr5 and a mid tier am5 chip than build on outdated sockets. Seems like a short term fix for a long term endeavour. It takes 3 core components being outdated for relatively minuscule savings overall.,Negative
AMD,I honestly cannot fathom how people are paying 400+ for this when you can just sell your current AM4 cpu/mobo and buy a 14600k/Z690 DDR4 board for way less than the 5800x3d.,Negative
AMD,"Yeah DDR5 prices are still pretty brutal, makes sense why people want to squeeze more life out of their AM4 builds instead of doing a full platform jump",Negative
AMD,I already have a 5800X3D but i think i would upgrade to a 5950X3D,Neutral
AMD,DDR4 is also stupid expensive and not that much cheaper though,Negative
AMD,Seems to me that chips that don’t make the 5700/5800x3d bins will still be in demand considering the tech shortages could last years.,Negative
AMD,The 5700x3d's are just 5800x3d's that didn't meet the performance benchmark. If they revived the 5800x3d they'd also be reviving the 5700x3d as a result of that.,Negative
AMD,Ddr4 is really not being produced at scale anymore. Once people start buying DDR4 systems the price will explode just like ddr5.,Negative
AMD,why not both?,Neutral
AMD,Or even take a play from AM5 and make a 5850X3D,Neutral
AMD,I had a 5950x and went to a 5800x3d and it improved my 1% lows in most games and some games got a boost of average FPS too. It is just for gaming not for any other tasks so it was absolutely worth it for me.,Positive
AMD,"Is the x3d variant really that much better? I stood in line day 1 for my 5800x, 3d didn’t exist then",Neutral
AMD,If only there was a free search engine you could type that into?,Neutral
AMD,That was a good price. I paid around $165 in January. Crazy what they are going for now,Positive
AMD,"I almost bought one myself. I hesitated and then the price started going up.  Oh well, my 5600x is good enough for me I guess. If AMD wants to make more at that price range, I'm totally in, however.",Neutral
AMD,Demand for ryzen 9000 series will drop off if ddr5 ram is either prohibitively expensive or not available for a good while,Negative
AMD,I am doing my first PC build and I went with the 5800XT. It would be awesome to see more AM4 X3d chips with how RAM prices are.,Positive
AMD,Are you having issues with it? My 5600X still runs everything I can possibly throw at it without any issue.,Neutral
AMD,I stood in line day 1 for a 5800x. No 3d existed yet. Is the x3d variant that much better?,Neutral
AMD,"Me too, might have to find a discounted pre built",Neutral
AMD,I'm just here on a i3 12100F. I'd love to go AM4 but the cost of doing so puts me at i5 13400/i5 14400 prices just for a sidegrade to an R5 5500 + B550.,Neutral
AMD,Because ddr5 ram shortage is going to eat into demand for am5 processors in new builds... Meanwhile there's a healthy amount of am4 boards that already have ddr4 ram in them and could use an upgrade.   The design work is already done and the nodes they are made on are still in operation.,Neutral
AMD,"I did too, but prices had already went stupid and not worth it for a fractional upgrade.",Negative
AMD,"It's about getting some cash out of existing AM4 users. They largely wouldn't be buying new or more memory, just replacing an existing CPU because upgrading to AM5 and DDR5 is going to be incredibly expensive for the next few years at least.  So they can get no money in the next few years from people like myself where I will opt to sit on my 3600 because DDR5 prices are insane, or they could manufacture some AM4 chips, and maybe it's worth upgrading to that. Re-starting manufacturing isn't cheap though so I'm sure they'd really have to run the numbers to decide if it's worth it. Maybe once AM5 sales start to plummet they will.",Neutral
AMD,Because a LOT of people are still on AM4. I'd bet more than AM5.,Neutral
AMD,"Oh just buy vastly overpriced DD5, oh and a whole new motherboard. Do you even know how PC building works?",Negative
AMD,Because the real world does not work like that.,Negative
AMD,I'm trying to squeeze more life out of my Intel I7-11700k!,Positive
AMD,"if you are primarily gaming not worth it, technically a downgrade for gaming if memory serves right",Negative
AMD,"There are rumors of a 9850x3d with dual v-cache. They could make a dual v-cache am4 chip as well and cash out on the am4 renaissance.  I don't know if it's possible, but if they could do bottom-side V-cache on an am4 like they do on am5, it could let the am4 x3d chips reach 5ghz instead of their current 4ghz. Would be a MASSIVE boost to everyone in am4. Honestly I think a 5850x3D with bottom side cache could probably match a 7800x3d at least.",Neutral
AMD,Prob 5900x3d would be more practical.,Neutral
AMD,"Yeah, but people who already own AM4 systems also already own DDR4 memory.",Neutral
AMD,"Not necessarily.  The 5800X3D became economically irrelevant for AMD. It was expensive *and* it was being beaten by the 7700X on AM5, which was both easier to make, and outselling it.  From analysing assembly dates, AMD appears to have ordered two, possibly three, batches of Vermeer-X/Milan-X. AMD used pre-tested, known good, Vermeer dies for the chip stacking process: **Every last one of them** was 100%, high clocking, the best dies TSMC had made.  I bought this 5700X3D in June 2025 for just over £200, it went end of life not long later. The date code on it was week 9 **2024**. AMD was making these in batches, not in serial production. Go google up some pics of the CPUs, you'll see the date codes cluster in two major groups with a possible third one.  Given that they were pre-tested dies, all of them able to clock and scale voltage at the very best of what Vermeer was ever capable of, the choice of whether it was sold as a 5600X3D, 5700X3D, or 5800X3D was a purely economic one.",Neutral
AMD,"5700x3d is exteme hard to overclock even to 4,2ghz.",Negative
AMD,"It's really for people who already have an AM4 system with a slow CPU, and already have DDR4 RAM who can no longer afford an AM5/DDR5 system. It's a big performance jump over the the 1000-3000 series and 5000 non-x3D",Neutral
AMD,Consumer DDR5 also isn't being produced at much of a scale anymore either so may as well get what performance is possible from the socket people already have memory for.,Neutral
AMD,"Reviving the 5800x3d would offer a good upgrade for people on AM4. No new DDR4 required. Additionally, there's still a decent supply of used DDR4.",Positive
AMD,Im on a 5800x3d. Id welcome a new am4 with open arms. Riding the am4 wave till 2030….,Neutral
AMD,"All I can contribute is the 5700x3D is the most consistent and stable system I've ever had, love it.",Positive
AMD,Certain games really benefit from the extra L3 cache - Escape from Tarkov and Star Citizen being two that instantly come to mind.,Neutral
AMD,Go check out some of the charts form Gamers Nexus from when it came out. You can find charts with both chips.  I have what you have and found out about the x3d from it being announced they were stopping production.  It's a clear gain. I think people say around 20%. Which was pretty nice at the pre-scarcity price.,Neutral
AMD,Same. Bought the 5900x before x3d even exists. FPS wise it doesn’t look much of a difference but I think the 1% improves which I’m hoping improve monster hunters wild,Neutral
AMD,"I went from a 5600X to a 5700X3D - average FPS was a little bit better, but nothing eye opening. However the 1% lows improved massively and general frame time was much more consistent.   I especially noticed this in VR performance for sim racing where inconsistent frame rates and frame times are immediately noticeable.   While the 5600X is technically a faster CPU, in both base and turbo clock frequencies, the additional cache of the 5700X3D made a huge difference for the games I play. The fact that I also gained two cores really isn’t that much of a factor. I originally went from a 3800X to a 5600X and the reduction of cores, from 8 to 6, had no negative impact on gaming performance.",Positive
AMD,"https://preview.redd.it/sm4hfifukg8g1.jpeg?width=4212&format=pjpg&auto=webp&s=93f5a9b2614c7c7c6c1a86784826781e80618fac  Six months ago, at exactly the right time...",Neutral
AMD,"Got my 5800x3d for $300 last month and that was a fair price on used market. Still expensive for last gen but less then nee cpu, mobo and ram",Neutral
AMD,"The issue is bringing back 5700x3d on say the back of the current running 5500x3d would change binning.  And it might not come back at a price point people are willing to purchase.  It really depends on the facility.  If the fab they're doing 5500x3d can't produce enough yield to bin a decent amount of 5700x3d, there's no point, supply would be so low that price wouldn't match consumer expectation.  It really depends on where each line is being fabricated, but if the machine they're doing the 5500x3d can't hit yields high enough to produce enough bin for another SKU, it's just not going to be smart to start binning on something that the final price to consumer will just drive them away.",Negative
AMD,"Outside of workstation tasks and some specific CPU-heavy games, 5600X is just fine. I mostly game, so I made the small upgrade to a 5700X3D to make my AM4 build relevant for a little while longer, but it’s not that big of a lift over a 5600X, 10-20% lift at most, not worth it unless you’re just trying to eek out some more performance and you can justify the price over swapping to AM5, which is different for each person.",Positive
AMD,I listened to all the tech tubers saying a 5600x is all you need for gaming like 5 years ago. Now I realize cpus have a longer shelf life and I would be in a much better place if I had at least gotten a 5800x or 5900x.  It just sucks wanting to upgrading but not being able to because of ram. Sometime in the next 2 years when ram prices drop I'm building a beast.,Negative
AMD,Same. I’m actually on the regular 5600 and it does just fine with my 9060 XT. Hardware Canucks actually has a couple of videos comparing how older CPU’s hold up with modern GPU’s and the results lean towards pretty well with the 5000-series.  [Against the 9060 XT and 5060 Ti](https://youtu.be/NqRTVzk2PXs?si=rPbJ63sy1vGa2GUr)  [Against the 9070 XT and 5070](https://youtu.be/TXKyQYiLro8?si=lJhigcH8-o2E5NtI),Neutral
AMD,Nah bud save your money. I also got a 5800x on release before X3D you won’t get massive gains. Just ride the 5800X into the sunset and pray the market cools.,Neutral
AMD,"No huge gains outside of some asset-heavy games like simulations and anything that needs to preload large textures or other intensive processes, so some capability gain for workstation tasks. The downside is a lower TDP and a lower overall clock speed.",Negative
AMD,"I did the same, and upgraded to an x3d the day it came out; For me it's night and day in MMO's, where developers can't fully control scenes, and thus, there's an increased chance of dips.  It's a bit slower on top the top end, but much, much more stable in the 1% and .1% lows.",Neutral
AMD,I'm sure hoping since Im standing on AM4 board and I'm planning on upgrading CPU and GPU to get through these rough times.,Positive
AMD,Wafers costs fuckton more than any newly produced AM4 would sale.,Neutral
AMD,"Yep, I’m talking about people building from scratch.",Neutral
AMD,It very much does,Neutral
AMD,I’m still rocking my heater/ i9 9900kf from 2019!,Positive
AMD,"If we're talking about the regular X version, yes. The 5950X3D doesn't exist but one could dream.",Neutral
AMD,Exactly. I’m sitting on 6 or 7 sets of DDR4 from when it was cheap cheap. Also my itx portable is still AM4 so an upgrade for it would be nice.,Positive
AMD,"It is, considering how long and in quantity the 5800X3D was made. The 5800X3D launched first, and was produced for months before the 5700X3D hit the market",Neutral
AMD,"Cache affects game performance kinda like not having enough RAM does. In some games it makes almost no difference, but in a few games it's like 50-70% improvement. If there are specific games you play a lot, try looking up benchmarks for the X3D chips for those games specifically.  If you play a lot of different games, then I'd say it's probably not worth worrying about it.",Neutral
AMD,Even if am4 chips would cost same as am5 people would buy it cause they don’t need new ram and mobo for it. And for instance if you go from 3600 to 5800x3d that’s a huge boost without platform change.,Neutral
AMD,"It's not a problem if the 5600X still does everything you need it to, which for me it does. Obsessing over tiny gains and upgrades isn't worth it.",Neutral
AMD,"A 5950X3D would essentially be one 5800X3D and one regular 5800X glued together. For gaming, you'd get the best performance by disabling the non-X3D die which would turn the CPU into a 5800X3D.  The only advantage of such a part would be that you get 16 cores for productivity tasks, while still getting access to 8 cores with 3D V-Cache for gaming.",Positive
AMD,"> Even if am4 chips would cost same as am5  I mean that's a big if.  It really depends on what kind of yields they can get on the machine that does the 5500x3d.  If they're getting low yields for 5700x3d on the the machine, there's not much point to binning as such.  Given the 5500 74mm² and the 800 standard wafer, that's like ~890 dies per wafer if none are bad.  If each wafer is only yielding 10% or less dies that can be binned 5700x3d, it's going to be more than the AM5 equal in cost.  So at least 90 or more of those have to meet the 5700 testing.  And my guess is that they settled on 5500x3d because they were hitting high 90% yield for that chip and not much more for anything else.  The Zen3 is arrange in 2x4 for the cores, so they must be hitting high for 6 of 8 coming through and really, really low for all eight.  The difference between 5700x3d and 5800x3d binning is the later hits higher stable clocks.  The 5700x3d is just functionally a 5800x3d that's got a few issues so running it at lower clocks keeps it stable.  But the 5500x3d is two cores died during production.  So it might be hard for them to hit 8 for 8 enough times to bin 10% of the wafer into a 5700x3d SKU on the machine they have the 5500x3d on.  And the 5500 isn't being produced on the main money maker fabs, that's always the latest gen that it's tooled for.  So 6 for 8 is maybe as best this machine they're running on can do and 8 for 8 is pushing it too hard.  I don't know, depends on the location and what they've got going.  But to retool one of the big boys that have great batting averages to run an AM4 platform is silly talk.  It'd take a ton of time to do so and disrupt so much we'd have another disruptive chip stocking issue.  I don't know AMD's specific numbers, but I can only imagine that they'd need some high 8 for 8 yield for it to be equal in price to the current AM5.  There's likely a specific reason they selected the 5500x3d as 6 for 8 and at the price point they've put it at.  My guess is that they're getting high yields on the machine they're using for two busted cores, high enough that the 5500 can be at the price it's at and it make sense to keep an AM4 around still.  Again, if the DDR5 makes an issue for the 9000s and AMD actually cares about the consumer platform, then it's just easier to use some dollars to have some of the higher demand consumers for DDR5 to ease up a bit and let some flow into consumers.  A whole lot easier than trying to push a machine to justify binning the 5700 SKU.  And vastly easier than trying to retool one of the machines it used to be on, back to doing that platform.",Neutral
AMD,"A perfect machine for programing + gaming, *at the same time*",Positive
AMD,Is disabling the cores a simple process?,Neutral
AMD,"Ngl id buy a couple of those. Have shit tons of DDR4 RAM and a few server rack cases I could use to add a few more units to my home lab.   X3D CCD for cloud gaming VMs and the regular die+ for productivity workloads  Edit; not a hardware Eng, could be misusing CCD as a term and I apologize.",Neutral
AMD,Awesome reply. I don’t believe it’ll happen but if it would and would give folks a bit of a boost until ddr drama ends.,Positive
AMD,"Something something ""Screeps"".",Neutral
AMD,"Restart, head into bios, make change, boot back up again",Neutral
AMD,"Yeah, it really depends on the length of the ""DDR5 drama"" which that's what I'm going to call this now.  I've taken it, you can't stop me.  But the thing is, given what I've heard with folks like Micron, Samsung, and SK Hynix, these aren't temporary changes, these are structural shifts in production.  *Money into data centers has crossed some sort of threshold that things like consumer PCs aren't profitable any more.  There just simply isn't enough sales in consumer PCs to justify the production before.*  Honestly, this gets into the economics part that I have very little knowledge on.  So, take anything I have to say in the above with a massive grain of salt.  But the main makers of DDR5 RAM are doing things that are big shifts away from the way it used to be done.  Reasons for why they're doing that I can only guess, but those may be bad guesses.  So the DDR5 drama may have a much longer duration.  I don't know, but the kinds of shifts they're doing aren't common.  But at the same time, it may mean a massive shift for the AM5 platform and software developers.  Chip makers issue a thing called ""Last Time To Buy"" LTBs.  The main producers of DDR4 have LTBs already.  The last ""raw"" DDR4 chips will be sold by major makers up to Dec. 31st.  After that, there's no more being made by the major makers.  But the big three aren't the only ones who make these.  There's Nanya, Winbond, and the Chinese CXMT.  They have yet to announce LTBs for their chips.  Again, I'm guessing here.  Just pulling shit out my ass here.  *But we all know how like Chinese CPUs are like a gen or two behind the leading gen?  I have a feeling that's going to be the entire consumer PC market.  Like what they're using in data centers is like two gens ahead of everyone else.  I think the DDR5 drama is just the correction to that state, but I have a sinking feeling, the situation that China has with their homegrown CPUs, will be what everyone who is a consumer will be in soon enough.*",Neutral
AMD,"I would say okayish, not a steal",Neutral
AMD,I have thesame specs but all brand new with 6700XT. Built it for around 800-ish USD  https://preview.redd.it/fmbjsu3pkb8g1.jpeg?width=4080&format=pjpg&auto=webp&s=67640a84ce338c300098709e1a6a9363e70f6e6c  You might have overpaid a bit,Neutral
AMD,Considering ram pricing it’s alright. Can easily upgrade the GPU later for a chunky performance boost,Positive
AMD,Yeah i asked  if he would sell without cpu and gpu and i woulda got a 7700x and 6800xt sadly not tho,Negative
AMD,Yeah I've noticed it across all of my games since switching GPUs,Neutral
AMD,This is how a 5800x should look:  https://preview.redd.it/p6jt2g5chc7g1.png?width=997&format=png&auto=webp&s=69b2669d3b0ad0332044a35a688e373c0e0d5d48     100% got scammed.,Neutral
AMD,"That is an AM2 or AM3 era CPU, possibly as old as Socket 939. I hope you didn't damage the mobo when you forced that thing in.",Negative
AMD,"Thank you everyone, I'm requesting a refund ASAP",Positive
AMD,Yes you've been scammed. No doubt about it.,Negative
AMD,"OP, I suggest you check tweakers for the best Dutch pricings/specs etc for this specific CPU > [https://tweakers.net/pricewatch/1618234/amd-ryzen-7-5800x-boxed.html](https://tweakers.net/pricewatch/1618234/amd-ryzen-7-5800x-boxed.html)  With Bol more being a sellers platform at this point I suggest you steer clear of purchasing PC parts from them.   If you need some help you can DM me :)",Neutral
AMD,That's an AM2 CPU. Sorry bud.,Negative
AMD,"It's an AM2 CPU, so yep, you've been scammed. Wait how the fuck have you put it and CPU light came on? Does this CPU pattern match your motherboard's pattern?",Negative
AMD,Absolutely. Ryzen 5000 have a middle that's not populated with pins. I might have a very old Sempron nearby that I think might be the same as this... will check and get back.  Yup: https://prnt.sc/NTE8FWqgZSZA  What you have there is a CPU for AM3 slot.,Neutral
AMD,"What does the top look like?  Just curious because yes, counterfeit Ryzen heatspreaders have become quite common but they used to only be faking the X3D chips.",Neutral
AMD,Yeah the ‘missing’ pins in the 5800x are parallel and symmetrical.  You should put in for a refund if that’s possible.,Neutral
AMD,"On the other side it is literally engraved in the metal what it is. The pins are definitely from an older socket tho. Also Bol is not an official vendor, it's a marketplace just like Amazon",Neutral
AMD,"You managed to shove an AM3 CPU into an AM4 socket? How? And uh... Don't you have eyes? All it would take is a single look at the socket and CPU to know they aren't compatible... Like you don't even need common sense to see that, you just need working eyes...",Negative
AMD,"Yes, thats an AM3+ Pin layout. Refund it and buy from actually legit sites next time.",Neutral
AMD,"Yep 100% scammed. This is an AM3 CPU, pre Ryzen era FX.",Neutral
AMD,"Yes, luckily its official vendor and RMA it if you have unpacking video",Neutral
AMD,Why show the bottom? what does it say on the top?,Neutral
AMD,This kind of stuff happens too much.  I'm driving to the closest MicroCenter (3 hours away) and buying in person.  I plan on opening each box there in store to make sure theres no funny business.  I don't plan on driving back.,Negative
AMD,"I'm curious, what does the top of the cpu look like, any photos?     It's clearly a different CPU, but I wanna see if it's a fake or if they just put the wrong part in the package.",Neutral
AMD,So why not just googling yourself how that CPU is supposed to look like?,Negative
AMD,It appears to gave 940 pins - vintage am2 cpu,Neutral
AMD,![gif](giphy|wPb0Er6MG6d9K),Neutral
AMD,That is not am4 dudee,Neutral
AMD,"You did get scammed but, if you squint your eyes hard enough it looks like am4.",Negative
AMD,"OP: since you're in the Netherlands, the only two stores I recommend buying pc parts from are: [azerty](https://azerty.nl) and [megekko](https://megekko.nl). Two reputable online stores that have yet to let me down in the 10 years I've bought pc parts.",Positive
AMD,Hopen dat bol.com die verkoper ff een goeie tik op de vingers geeft,Neutral
AMD,I refuse to believe this is not ragebait,Negative
AMD,"Does the cpu have its model etched into the lid? Should have its SKU number on it at least. It looks like an AM3 socket cpu, definately request a refund!",Neutral
AMD,Looks like an AM3 socket CPU so yes.,Neutral
AMD,"That’s an AM2 or AM3 chip, probably the cheapest Athlon they could find and slapped an AM4 5800X IHS on it because it’s the same size, definitely got scammed brother. Don’t know where you got it from or if it’s a reputable site which I’ve never heard of BOL, I’d get your money back. Tell them this: “I bought what specifically said a Ryzen 7 5800X which is an AM4 CPU but what I got is clearly an AM2 or AM3 CPU with a 5800X IHS slapped on the top and I demand I get my money back.” And if they say there’s nothing they can do then tell them “I’ll get the law involved for false advertising and fraud.” Then they’ll give you your money back.",Negative
AMD,Did it come with lube?,Neutral
AMD,Looks like a phenom II,Neutral
AMD,yes. how did you even get it in the motherboard? 💀,Negative
AMD,"Go back, kiss him as hard as you would the love of your life",Positive
AMD,How much did you pay for it?,Neutral
AMD,"siorry bug, you got scammed     once you refund it buy yourself a 5800xt or a 5800x3d",Neutral
AMD,Ryzen 1000 / 2000 / 3000 / 5000 series (AM4),Neutral
AMD,that looks similar to the socket to my old ass athlon 64x2 in my retro computer,Neutral
AMD,His pins bend out the box 😂,Neutral
AMD,I have a ryzen 7 5800x and mine didnt look like that,Neutral
AMD,That looks like some kid ordered the AM4 chip and then claimed it didn’t work and returned his old AM2 chip. The good old return scam “free upgrade thanks to Bezos”,Negative
AMD,Poor you,Negative
AMD,"At best it could be an FX, but could also be some older Athlon II or Phenom II.  You might have been third-party scammed where someone bought a 5800x, but returned it with an older cpu and retailer did not check the return properly (or returner did a great job making it look unopened). Were any of the seals questionable/not present?",Negative
AMD,I'm afraid you damaged your mobo as well.,Neutral
AMD,Yeah fr you lost thatttt,Neutral
AMD,Lmao it’s always so strange to me when people make a post answering their own question IN the question.,Negative
AMD,"Yeah mine when I was on am4 didn’t have any missing pins, sorry to say bud looks like you were scammed",Negative
AMD,Yes. That will never work.,Positive
AMD,Dropshipping company.,Neutral
AMD,"Go back, kiss him as hard as you would the live of your life",Positive
AMD,"Go back, kiss him as hard as you would the love of your life",Positive
AMD,"Go to bios and check what CPU it sees.  I have myself bought a 7800X3D which doesn't look like in photos but was told on PCMR Discord that multiple places make these and that they look slightly different. BIOS sees it, windows sees it and gaming is amazing as expected so I don't care 😁",Neutral
AMD,![gif](giphy|hpAMh2sBYpsmFhSRPI),Neutral
AMD,"Just fyi bol.com isn't really an official vendor for anything, it's more comparable to amazon so you should check the original source where the CPU came from.   Still sucks you got scammed like that, hope you get that refund quickly.",Negative
AMD,"OP if you can’t get a refund and paid with ur credit card, challenge the charge with ur bank with the pretense that you’ve been scammed.",Neutral
AMD,They are good to order from but you need to cueck who is selling it. If bol it self is selling it 99% good to go if any other seller it really depends on which one but then it is a 50/50.,Neutral
AMD,"saves 17 bucks over Bol as well.  Bol is really good for more mainstream things like controllers or consoles, but for pc parts, rather anyone else, unless sold directly by them",Positive
AMD,Oh God I'm picturing the mashed pins underneath 😭,Neutral
AMD,Unless someone swapped the IHS,Neutral
AMD,Common sense? In this economy?,Neutral
AMD,"If his motherboard is AM4, there is no fucking possible way he can go to bios to check this CPU as this one will not ever fit in AM4 socket, this one won't fit AM3 socket. You've been told wrong or you think of wrong thing.",Negative
AMD,Yeah that's just not true,Neutral
AMD,![gif](giphy|RkuynGGVvthiV8cUMK),Neutral
AMD,"Yeah, but bol does handle as a middle man when you ask for a refund, you should contact Bol and not the seller directly.",Neutral
AMD,"That's not true. Bol is very much an official reseller it just depends on product category. You have Bol.com retail ( direct)  and Bol Plaza ( marketplace).   But for the 2 resellers selling the product currently, I'm surprised you received a bad product as they are official retailers ( Paradigit and Informatique).",Negative
AMD,"This. Its called a ""chargeback""",Neutral
AMD,Bol really isn't really that good for anything anymore. Almost everything is dropshipped nowadays. It's even worse than Amazon.,Negative
AMD,His pins are 10000% fucked,Negative
AMD,Nope I am just a retard not reading it properly...my case was completely different from OPs,Negative
AMD,"While the OP image isn't a 5800X3D - this poster is correct in that it is manufactured (or assembled) in different places, so the PCB colour may vary slightly for example...",Neutral
AMD,Sorry I am just a retard not reading it properly...my case was completely different from OPs,Negative
AMD,This is not always the case. Depends on what the seller is paying for.,Neutral
AMD,"The funny thing was that at the time of writing my comment I blanked on what it actually was called, I sat there for a while going hmmm.",Neutral
AMD,"Damn dude, I know the feeling, happens to me often hahahaha",Negative
AMD,"Definitely always, contact third party first - right? Like there is so much possibility, into the positive if contacting the deller directly as well. But the rule of thumb is to always go to the company you have a direct contact to. When you buy anything in a shop, you don't go to the brand for most problems, right? Especially if it is about being scammed, you would go to the shop and explain to them what happened so they can contact their delivery, which is gonna do their own inspection etc rtv etc etc",Neutral
AMD,It should be much more powerful than the 2070 in the ballpark of 70-80% faster with more vram. If it's not faster then something is probably wrong.  A couple of basic things: Are you plugged into the GPU? Do you have any other programs eating up GPU or even cpu/ram? When opening task manager in a game (control + shift + escape) is the GPU 100%?,Positive
AMD,Are you using a hard drive or ssd,Neutral
AMD,"Try rolling back to older drivers if you are using new one, cause sometime new drivers do bring improvement for new games, but messing up on older titles, try 25.9.1 version, don't forget to DDU from safe mode just in case",Neutral
AMD,"Hey, what's the PSU rated for?",Neutral
AMD,"Thanks for the replay!!  - I believe I am plugged into the gpu, as it's directly plugged into where the gpu is anchored into the back plating.  -only steam and discord are open  - nothing appears to be background processing  - while running nighrein my gpu is running at 67% and my internal gpu is 0, further confirming I'm plugged into the right one!  Hope this helps!",Positive
AMD,ssd,Neutral
AMD,"Okay, well if the GPU isn't 100% it means something else is limiting it, I'm not familiar with Nightrein, so I'm not sure what it could be, an FPS cap, cpu or ram being the limiting factor etc. But if you're GPU is the limiting factor it should be over 95%, usually 99/100%.",Neutral
AMD,"That's great to know, I will check it out. So generally we want the gpu to be maxed out?",Positive
AMD,"Yes, if it's not maxed out then something else is limiting performance, since your CPU is so high end it's most likely something software related.",Negative
AMD,"It is a terrible motherboard, potentially its throttling the CPU, very unlikely though    The 5500 isn't a power hungry chip   If the bios is outdated, update that and see how it goes. It won't be your PSU",Negative
AMD,the A320 doesnt support OC. only B and X series motherboards support OC.,Neutral
AMD,"Yes I know this isn't the issue, the base speed of my cpu is supposed to be 3,6ghz but it's only running at 3,2ghz",Neutral
AMD,I thought I was the luckiest guy in the world when my lady bought me a PS5 two years ago. Turns out we are both the luckiest men in the world !,Neutral
AMD,N0ice,Neutral
AMD,She wants you to spend less time with her. Sus... /s,Neutral
AMD,"the radiator is going to have air bubbles in it pretty soon, you have to change the orientation of the pc.",Neutral
AMD,It's the same lady!,Neutral
AMD,https://preview.redd.it/q8l83qvl4a8g1.jpeg?width=3024&format=pjpg&auto=webp&s=8268a0820139be05355049116ad463e845836869  Exactly the aquarium look I was going for!,Neutral
AMD,"![gif](giphy|QiCCluutpecs8)  Why would she want that, when this is what I look like. /s",Negative
AMD,Can you please explain further? Do you mean to turn the radiator upside down?,Neutral
AMD,![gif](giphy|t6cn3lRhDZtBjdAjKN),Neutral
AMD,[https://www.youtube.com/watch?v=BbGomv195sk](https://www.youtube.com/watch?v=BbGomv195sk)   i know its long but its the best explainer out there.,Neutral
AMD,"You'll be OK the way it is now. Jayztwo cents made a video that greatly simplifies the GN video and explains what you've done is just fine but not the best. [The Classic](https://i.redd.it/b5i5e59f7k861.jpg)  Also,   https://youtu.be/DKwA7ygTJn0",Neutral
AMD,Her vagine hang like sleeve of wizard,Neutral
AMD,"Thank you, I’ll watch it.",Positive
AMD,Mankind never fails to be even more greedy than King Midas should the opportunity arrise.,Negative
AMD,Buy a 5800X3D for an extra $500 or pay an extra $200-$300 for DDR5  Makes sense,Neutral
AMD,"Its like the housing situation. Sure I could sell all the parts in my current PC (5800x, 64gb DDR4, 7900xt) but that profit wouldn't be enough to buy those parts again.  What a crazy time man. If this AI trash continues, home computing is effectively going to die after this generation, with people just up keeping their current systems.  They've priced us all out. Gpus, Cpus, ram, and storage are all outrageously expensive.",Negative
AMD,"Headline is trying to mislead you. 5800x3D has been out of production for a while so it's been pretty pricey for a new one. But it's only up about $70 or so in the past few weeks on ebay used. So yeah it's going up steadily and people are choosing DDR4 in a lot of cases but ofc 5800x3D is expensive af new, there is 0 stock.",Negative
AMD,Or just buy a 5800xt for 150 and call it good.,Neutral
AMD,Wow I was doing really good the other week to get fisted for $500 for one of these  All because I wanted to save $150 when I did the build in the first place with a 5600X  ![gif](giphy|d2lcHJTG5Tscg),Positive
AMD,Hell yeah     Horrible for new users but those with 32gb ddr4 and a 5800x3d with a 9070xt great news its like owning a gmc jimmy the older they get the sell value goes up.,Positive
AMD,"This is crazy. I upgraded to a 9800x3d but still have my 5800x3d laying around. Had no idea it went up in value so much.  Edit: No I'm not selling it, gonna use it in another build.",Negative
AMD,The old Athlon days are coming back in an unexpected way I guess lmao,Neutral
AMD,We live in interesting times…,Positive
AMD,Looks like I'm sticking with my Ryzen 5 3600 for even longer,Negative
AMD,So pissed I missed the last aliexpress sale of the 5700x3d before it went out of stock,Negative
AMD,I just listed mine on eBay with 32gb 3600 cl16 ddr4 for $550. Way better price than all the chips that have sold for $525-$575 just for the CPU.,Positive
AMD,Lol. This entire market is fucked.,Negative
AMD,"5800x3d was going for $800 at our retail 2 years ago, so to me this price tag doesnt seem out of place ... thats how expensive PC parts can often get here in eastern europe",Negative
AMD,I bought it when it first came out for around 300 I think?  Got super lucky I guess.  Been a fantastic CPU,Positive
AMD,"Well shit, I've got a 5800x3d I'm not using, I'll sell it for 700 since I did use it for a little while.  Lol",Negative
AMD,May have to blow the dust off the ol Phenom X4 965 Black edition soon,Neutral
AMD,I just picked up a pair of 14700K's and DDR4 boards to upgrade my wife's and my PC's. After selling the old pieces I expect the cost to me to be about $250 per system. DDR5 to match what I have would be over $500 on its own.,Neutral
AMD,"Hell. Looks like I upgraded my home lab at the last moment (August). I still don't know why everyone goes GPU route when ASICS are twice as efficient and maybe someone will finallly implement SNNs models, because they are 30 times more efficient than GPUs... You can build a monkey brain equivalent in single server rack with them and it will be using 20kW, so not sipping power yet, but similar complexity built on Nvidia chips eats just over a half of giga watt.",Negative
AMD,![gif](giphy|HChtj3gzcVsXK)  Me with my 5800X3D I've had since 2022.,Neutral
AMD,"Ive been seeing 5800XDs for $550 on new egg and I thought that was a scam, jeez.  I think Im just gonna be the old fart gaming on the 5800X if thats how its gonna be :/",Negative
AMD,Well my 5600x will have to carry my 9070xt for 2 more years I guess.,Neutral
AMD,"For all the hype on AI and the colossal spend, I haven’t seen it produce anything good or new. It makes images/videos and does programming but for all that investment you can hire an army of people to do that too.",Negative
AMD,Does that cover the difference for ddr5 memory?,Neutral
AMD,Fuck this timeline. Fuck AI and corporate greed.,Negative
AMD,"Sold my 5800X3D, B350 motherboard, and 32GB of RAM for 420**€** a month ago.   Would have gotten more for it these days, it seems, but then 32GB of DDR5 would have eaten heavily into and over that.   The market pricing is currently seriously broken, don't upgrade unless you really have to...",Negative
AMD,I’m rocking a 5700xt. If it was reasonable I’d upgrade to a am4 x3d chip but it’s not worth what it costs right now.,Negative
AMD,My brother picked one up like a week ago for $150 to upgrade from his 3600... Insane change,Neutral
AMD,Damn. I got lucky. I upgraded from a 5600x toward the end (around when the 5700x3d was available) to a 5800x3d and it’s awesome.  Bought the 5800x3d for $520 AUD at the time brand new. And then sold my 5600x on eBay for $170 AUD from bids (originally bought the 5600x new at $220 AUD),Positive
AMD,Here I am still rocking my amd 3950x happy as a clam with no plans to upgrade any time soon!,Positive
AMD,Called it!  https://www.reddit.com/r/pcmasterrace/comments/1pg1ugc/comment/nsodc3o/,Neutral
AMD,"ddr4 is like 5-10x the price it was a year ago too though lol  i bought 384gb of server ddr4 for <300 CAD in february, not even a year ago. Same thing sells (as in successfully) on ebay now for $1600+. Sheer insanity.",Negative
AMD,"Yep, my 5800x3D - 7800x3D upgrade made me £200, people are wildly overpaying for it",Negative
AMD,"Damn ,this is what I paid when I bought it  https://preview.redd.it/gldhv9rnb48g1.png?width=1220&format=png&auto=webp&s=0d4f991efbe980055fdc14cada0bb65e371c9315",Negative
AMD,So happy for my 5700x3d and 32gb DDR4 bought at normal prices last year.,Positive
AMD,There honestly hasn't been a huge increase in requirements for games in the past 5 year or so. At this point with dlss I will probably run my 3080 for the next 5 years or so. Maybe longer if the current nonsense continues. Which it probably will unfortunately.,Negative
AMD,"Next up, computers just becoming whatever spare parts we can find, mad max style, retro gaming going to make a comeback, and NVidia will find away to take that away from us too",Neutral
AMD,Im good with my Ryzen 9 5950X for 300 Dollars,Positive
AMD,"I don’t care how good a CPU is for gaming, if it’s not for a workstation it ain’t worth $800",Negative
AMD,https://preview.redd.it/l415qxi0a18g1.jpeg?width=1320&format=pjpg&auto=webp&s=98747d624684e5e11007abf66ba9e61b6a160f71  ￼​Good thing that I already have mine lol,Positive
AMD,Got in under the wire.,Neutral
AMD,certainly we all saw that coming.,Neutral
AMD,"Just started disassembling my 5800X3D & B450 ready to install a 9800X3D and B850. I was hoping to get £400 for the 5800X3D, B450 & 2x16gb 3200MHz.",Neutral
AMD,If we cant even afford a couple modern ram modules how are we going to buy a quantum computer in a decade o two?,Negative
AMD,Up to $800 doesn’t mean you will get that you can buy one on eBay for $400 right now,Neutral
AMD,I got a 5950x for around 240$,Neutral
AMD,Microcenter is killing it,Neutral
AMD,Is now the time to sell my old 5600X? Me thinks it's time.,Neutral
AMD,5900xt is up to $319 which is a bit higher than when I purchase at summer. \~250   Seems only the X3D processors took off in price (comparatively speaking).,Neutral
AMD,And I was happy to sell mine a year ago for what I paid new.,Positive
AMD,"About 2 months ago, I managed to grab a used 11700KF for only $140. Very nice upgrade over my i5 10400F, especially in the CPU heavy games that I play. Also threw in another M.2 since the other slot was now available.   I built this rig in 2021 and now it looks like she'll be running into the 2030s...",Positive
AMD,"I am beyond grateful I found a 5800x, motherboard and 32gb of ddr4 for only 300 last week. Also picked up a 3070ti. Market place is great. The ram kit the guy sold me is going for 200$ online…",Positive
AMD,Man I wish so much I bought one last year when I was thinking about it. I got the 5800xt recently at least and I’m really happy with it.,Positive
AMD,Im glad I was able to snag this cpu for 398CAD a few years ago,Positive
AMD,Now I know how someone felt when they bought RAM in September. Buy your pc parts now fellers.,Neutral
AMD,I nearly got one last year but the purchase fell through. Ended up getting a Ryzen 9 5900X instead which I'm happy with. The extra cores come in handy for non-gaming and I can live with slightly lower 0.1% lows.,Positive
AMD,Well shit I shouldn't have postponed upgrading my 1600. Whats another year or two with that anyways.,Negative
AMD,Never getting rid of my 5950x! I'm going to undervolt it right now and replace the thermal paste!,Negative
AMD,you would be a dubmass to buy this,Neutral
AMD,I can't believe I'm one of the lucky ones to have an AM5 system. I feel like my PC is worth way more than it is actually worth right now because of this stupid AI shit.,Negative
AMD,I did the one thing I swore I would never do this Christmas. I got my kid a prebuilt desktop. Costco got me. I got tired of ever changing prices and nonsense.,Negative
AMD,The 5800X3D is no longer in production (sadly cause I want one). That combined with people not wanting to upgrade is why it is so expensive,Negative
AMD,Should I buy a 64gb DDR4 kit if I already have a 5800x3d? I only have 32gb now. GPU has 24gb,Neutral
AMD,"Shit. Just when I'm considering to upgrade my 2017 cpu/motherboard combo to something else that is still old, yet more modern than my current setup.",Negative
AMD,LAN centers making a come back,Neutral
AMD,"I used to play on a 3rd gen intel i7 with no dedicated graphics. I averaged about 15-20 fps in Skyrim on low settings - I managed. My 5800x3d and 7900xtx and 64gbs of ram will be fine until this bubble pops, and no one is getting any money from me until msrp.",Neutral
AMD,"okay questioon for people here because ive been out of the loop for a while now. im rocking a 3600x but im being fairly cpu limited in the applications i use. mostly CAD, some light gaming. best AM4 cpu for the price right now? Thinking 5800XT but i really haven't been keeping up with things for a while. definitely not moving to AM5 though.",Neutral
AMD,The market when i want to upgrade:,Neutral
AMD,Never been more glad to have purchased one a year ago,Positive
AMD,"I upgraded my pc last month. I was not in the mood to change the mobo so i just upgraded my ryzen 5 2600 and got another 16go of ddr 4. The price of x3d cpu are insane, and so is ddr5. So i ended up with a ryzen 5700x. Still a massive upgrade",Positive
AMD,"fucking called it. The next major increase would be CPUs, and they will probably jump AM5 too seeing people are willing to pay big bucks for hardware.",Negative
AMD,I see I can get a bundle with the 9800x3d for like $680 with a mobo and ram.,Neutral
AMD,Wonder what my chip is worth.  Ryzen 9 5900X,Neutral
AMD,"I'm seeing it at a range for about $500 on ebay and under $400 on amazon. Why are people paying extra?  I do have an extra new 5800x3d that I was planning on swapping into one of my PCs, but maybe I'll set it since I don't really need it.",Neutral
AMD,So I was right to get my 5700x3d after all !,Positive
AMD,"Sigh, I regret not buying when they were in the ~$250 range. Now I have to make do with a 2700x.",Neutral
AMD,It's crazy to think I bought all my pc parts a week before the pricing explodes,Neutral
AMD,My 5700X does everything I could ask of it.,Neutral
AMD,"I bought a R7 5800X from Amazon for $189 a few weeks ago. Just checked and it's up to $219 already.   Same with the DDR4 32gb I bought for $144, up to $199 now.   Glad I bought when I did. Now to get the 9060XT I've been looking at before that goes up....",Positive
AMD,It seems that i was fucking lucky to upgrade from a R5 2600 to a R7 5700X3D for about 250€ last year,Negative
AMD,"is this all FOMO honestly?  I know the PC part market is in a bad place currently with this AI bubble and manufacturers turning 100% attention towards it and cutting off personal consumers...  but like, this is INSANE",Negative
AMD,Damn…. What is this timeline?  I’m second guessing selling my pc…. How is Ryzen 7 w/ DDR4 in again?   I’m scared if I sell I won’t be able to replace it…,Negative
AMD,My 3950x with 64GB RAM continues to be one of the best investments in computing I've made. Renders video and photo quickly and plays all but the most badly optimized ports at 120+ FPS.,Positive
AMD,Holy crap i paid 290$ for mine!,Negative
AMD,Glad I got one on prime day,Positive
AMD,"I have 64gb 4 sticks of 3600mhz ddr4, a 5900x, 2070 rtx  Send me your offers. Its my old pc. LOL",Neutral
AMD,I just spent 20 minutes laughing like the joker at this utter lunacy.,Negative
AMD,"I've just gotten all my parts ordered to upgrade from a 5800X3D to a 9800X3D since I don't like what I'm reading about the future of the market. 32gb 6000 CL30 RAM at $275 when I bought it stung a bit but I think it will prove to be the right choice, and apparently I can sell my 5800X3D and make half of it back...",Neutral
AMD,motherfucker I knew I should have done the X3D version a year ago when I got my normal 5800x,Neutral
AMD,"I have a 5600X3D I am about to try to unload, wonder if the interest is the same. (evil mustache twirl?)",Neutral
AMD,dang man got a free 5800x3D gave it a buddy since I'm on AM5,Positive
AMD,Hilariously broken market.,Neutral
AMD,Maybe I should part out my old mid range pc,Neutral
AMD,At this rate people will go back to consoles. An ok build will be like 2500 if it keeps going like thuan,Neutral
AMD,"If you're building a budget system today, go for Chinese X99 with quad channel, like the MR9A Pro.  Get 4x8GB ECC Reg memory, it's still dirt cheap. Look locally.   An, E5 2697 v3 is like $15. Turbo unlocked (custom bios), it'll perform near Ryzen 5 5600. It's good enough to hold you over a few years.  Do so before the prices reach that part too. Upgrade properly in 2-3 years. For now, put some money in a good PSU and GPU. You can upgrade SSD and platform later.",Positive
AMD,"I have a spare i7-4790k, will that be worth anything to anyone? It treated me well for nearly 10 years.",Positive
AMD,"If you live near a Microcenter, the DDR5 RAM prices arent too bad.   $200 for 32GB, if you buy a processor.",Negative
AMD,its going for $400 ish.,Neutral
AMD,I got a 5600x what that cost if buy today?,Neutral
AMD,Earlier this year picked up one of the last 5700x3D chips at my local Microcenter and sold my 5600x that’s in my living room HTPC.  Paid $220 and sold the 5600x for $120.  I’m glad I did that when I did.,Positive
AMD,Man I sold my 5800x3d last year to some dude on marketplace for like $300 bucks and I threw in 32GB ddr4 Ram for free 😭 guess I shoulda waited lol.   But tbf I wasn’t tryna scalp nobody,Negative
AMD,same here. lol.,Neutral
AMD,Wow. I really did build a PC at the perfect time,Positive
AMD,"CPU manufacturer should be cool and do legacy 5800x3d manufacturing run to help gamers out.  I upgraded to 5 because of the cost of that legacy cpu being more than newer better cpus, but if they offered it again I would buy it to just upgrade my old rig to something more capable.",Positive
AMD,Somehow am4 is alive again!       (not that it wasn't viable post am5).,Neutral
AMD,So anyone selling 9800x3d cheap?,Neutral
AMD,With all this price hikes it seems like cloud game streaming services will become more popular due to being priced out of PC components. Economically it will make more sense with prices of these components being worth about 2-3 years of GeForce NOW,Negative
AMD,"Hmmm maybe I can be pursuaded to downgrade my desktop to a 3700 if I can find my old CPU and sell the 5800x3D  Nah, the performance hit would suck",Neutral
AMD,I listed a 5800X3D on ebay today for 399 and it still hasn't sold.,Neutral
AMD,I got so lucky buying my 5800X3D and 64Gb DDR4 before the price increases,Positive
AMD,"Eh, 5600 is great too!.. yeah?",Positive
AMD,There goes my upgrade. Tf,Neutral
AMD,"me, with a 5900x and 96gb DDR4: let's wait for 2029",Neutral
AMD,Wow i thought it was a good deal back in the days but so good. Silicon 🐂 market,Positive
AMD,"I just bought a 5800x with a MSI b550-a pro, 32gb vengeance ddr4 for 285. I also have some ddr4 ram stockpiled so I can sell when the time is right",Neutral
AMD,I’m sad to have missed the 5800x3d boat. Seems wrong to only be able to get the 5700…,Negative
AMD,"Soooo, I just ordered a refurbished 5800X3d off of Newegg and it’s shipping from China. Got it for like $480 plus shipping. Did I get scammed? Hasn’t came in yet.",Neutral
AMD,this is crazy,Negative
AMD,The price of them was high before the RAM shortage. Chinese sellers saw the demand and raised the price.,Neutral
AMD,Stop paying for things you don't need.,Negative
AMD,ill probably just keep my regular 5800X until i finish my degree in late 2028  then decide if moving to AM6 is worth it,Neutral
AMD,I'm now afraid to ask what a 5600X costs right now.,Neutral
AMD,"while it make sense, its still sad... what a dark timeline to live in..",Negative
AMD,Hehe boii. Bought mine used for 350 a few mo ths back. That and this 3080ti holding on for the long haul. Unless I need money for some reason.,Neutral
AMD,😈,Neutral
AMD,Anyone wanna buy mine?,Neutral
AMD,Even the 5700x3D is selling on ebay for $300 plus 😬,Negative
AMD,So how do I cash in on this opportunity,Neutral
AMD,"Pretty glad I bought my 5700x3d during the am4 slump when everyone was hopping on the am5 bandwagon. Have no regrets getting it cheap and the games I play are all gpu dependent at 1440p. Also got a 5070ti below msrp which was a surprise but looks like gpu prices are going up again and again. What a shitty world we live in. Reminds me of that game Metal Fatigue when 3 Corponations took over the world and fight with giant robots. Maybe soon, there will be the mag7 Corponations duking it out with giant gundam-like mechs to battle for supremacy.",Positive
AMD,I got one in my PC. I'll let it go for $3000. This deal is valid for today only.,Neutral
AMD,Damn I got my 5800 wen it was 500$ lucky me.,Positive
AMD,"I am ashamed of myself got one for 450 usd brand new, should I rip the sigil or sell it for 800? 😂 Its WOF or whatever without the cooler. Haven't put it inside the rig yet bcs I have to stay at my parents house this night :'( God i wanna install it  Used are 400 ish so maybe I got a good deal? Idk, în Europe the prices differ.",Negative
AMD,"Or, or, I know it's crazy.. don't buy a x3d cpu? Plenty of the other 5000 series works completely fine. I'm on AM4 and have zero reasons to leave right now. An x3d is tempting but I certainly will not pay for an overpriced cpu. I'll stick with my 5600 and be happy.",Positive
AMD,This is so fucking annoying. Why are these idiots buying all of this shit for no reason.,Negative
AMD,"If you're going to build a ddr4 rig now, intel 12-14th gen support it. Just get the latest bios fixes and you won't even have to worry about the 13th and 14th gen bugs. 14700k is half the price and as good or better performance with more cores for multi-thread applications.",Positive
AMD,I was confused why anyone would buy 5800x3d over the 9800x3d when it's cheaper but according to this it's because most people are on AM4. Have motherboard prices increased as well to where they can't just buy AM5 boards?,Negative
AMD,Huh? Just looked the 5800x3d up on ebay since that's my current chip and it's going for 400$-500$. No clue where they're getting 800$ from.,Negative
AMD,"https://preview.redd.it/xva4jiz0i18g1.png?width=680&format=png&auto=webp&s=8222cd5067e8de0ec6926a0e5475754d445b9448  Me having a 5800x3d, EVGA 3090 ftw and 64 gigs of 3200mhz cl16 ram.",Neutral
AMD,Humans are locusts,Neutral
AMD,"At that point, it's cheaper to upgrade to an Intel 14th gen + ddr4 lga 1700 mobo than it is to simply upgrade to a Ryzen 7 5800x3d",Neutral
AMD,No one is buying that for 800 let alone more than a 9800x3d,Negative
AMD,We’re so susceptible to hype it’s crazy,Neutral
AMD,... Can't you just use DDR4 with a 9800X3D?,Neutral
AMD,Plenty of AM5 DDR4 boards available,Neutral
AMD,King Midas? Those are rookie numbers,Neutral
AMD,5800X3D is a beast CPU for AM4. It'll last through the AM5 generation easily.,Positive
AMD,By 5800X3D for $800 or just buy 14600k for $160 and have 10-30% better performance.,Neutral
AMD,And then buy new mobo and CPU on top of that.,Neutral
AMD,I mean assuming you got a micro center near you that 800 will get you a 7800x3d or hell even 9800x3d bundle with 32gb of ram.,Neutral
AMD,"I got a 9800x3D + 32GB of RAM for $800 last month... toss in a new motherboard for $150 if you want to keep things cheap, and I don't see why you'd pay $800 for an old chip when $150 more gets you a new DDR5 setup entirely.  (just checked, DDR5 kits are still available at the same price)",Neutral
AMD,Or you can buy a 5700 for $150 and not miss out on much for gaming.,Neutral
AMD,It does seem like they’d perfer to go the phone model of 2 yr contract you then upgrade to the new version down the line.,Neutral
AMD,"It's going to go the way of everything else.  X as a service.  You won't have a physical computer anymore, but will have a speedy internet service that connects you to a high end server that acts as your pc that you basically rent.   Amazon Luna pretty much does this now.",Neutral
AMD,Getting us cloud computing is the end goal here.,Neutral
AMD,"oh come on, the AI spend can't go on forever.  Eventually HBM production will be so saturated that the prices will come down to consumer levels, and then we might start seeing PCs with HBM instead.  There's too much money sitting there for someone to not jump on the opportunity. Non-HBM memory has too much demand and necessity all over the world for businesses and countless businesses can't just absorb another 1k per laptop, especially outside of the US and other very wealthy nations.",Neutral
AMD,"I got a strategic supply of unopened 128GB micro SD cards (1TB total). If things get worse, I got some more space to datahoard. If things get better, I got a lifetime supply of cards for my mirrorless camera.   It only costed me like $20 with coupons and coins from AliExpress.",Negative
AMD,Time for the used market to shine! We don’t need to buy the newest every single year. I only upgraded my last PC from 2016 (mid range) last year,Neutral
AMD,"The thing is if demand is threatened, companies that make part of the value chain will expand vertically and horizontally to fill the space.  If Nvidia wants to surrender 30% of GPU production, and demand exhausts all available GPUs, buyers will be forced to adopt substitutes. Like AMD GPU products. Or Intel. All that matters is that these companies recognize the available supply and move quickly to fill it.  The only other option is to let the market collapse as you said, which would be profoundly stupid. It obviously could happen, but given AMD has been playing with producing GPUs in the past, I see no reason they wouldn’t just pick it right back up.",Neutral
AMD,The bubble will pop... or maybe the next admin in 3 years will not allow this price manipulation,Neutral
AMD,Flagship GPU used to cost $500 tops..   Then fast forward a few years and they are $3k    But ram going up is end times?,Neutral
AMD,I am so glad. I did my dream build in 2023.  https://preview.redd.it/c9xbpbmon38g1.jpeg?width=3024&format=pjpg&auto=webp&s=acee621b60145a051dd7b6c2d761df1b5274f1ba,Positive
AMD,Will game arcades and gaming rooms make a comeback?,Neutral
AMD,When that bubble will burst and it will burst. You also be paying to save the companies.,Neutral
AMD,Companies don't want regular people with strong computers.  They can make more money if you have to pay to stream everything remotely.,Negative
AMD,If you are not a gamer and don't use operating systems that unnecessarily force upgrades this won't affect you for a long time. My system was a typical high end system in 2012. It is still extremely fast.,Neutral
AMD,"Yep if this happens, then they stop making PC games as well. If the hardware market dies then the game market dies.",Negative
AMD,Hardware optimization and architecture innovation is a thing,Neutral
AMD,"> gpus, CPUs, ram, and storage are all outrageously expensive     Lol what are you even on about? Besides ram, that’s not even true",Negative
AMD,Yup.  It wasn't available when I got my 5800x. I learned about it when they announced it would no longer be made. And it was already shooting up in price. I've been looking on and off for a year or two.,Negative
AMD,"DDR4 is also out of production, so this is nothing more than a short blip. Once the supply is gone, that's it",Negative
AMD,The article is based on sold ebay listings. That was my first thought too.,Neutral
AMD,Or a normal 5700. People overthink things. A half decent 8 core cpu is great for gaming even if it's a few years old. It's all that's needed.,Positive
AMD,Did that earlier this year.  5800xt for $120 on amazon and sold 3600 for $60.   Sucks I missed the cheap x3d from Ali but what can you do.  I'm just hoping my 4x8gb sticks don't fail.  But with no oc and loose timings they shouldn't.,Negative
AMD,Or a 5950X/5900XT if you like moar cores,Neutral
AMD,"Did it right before the ram crazyness. From AMD 3600 to 5800x, also upgraded my ram from 16 to 32gb, gpu from gtx 1660 to 4070ti and lastly from 550w psu to 1000w (only because it was cheaper that 750 or 850w ones)   This shit plays everything at 1440p and it was around 700€ (only gpu was used).",Neutral
AMD,Yeah I would not buy one for $250 earlier in the year and kind of regret it  I *definitely* wouldn’t spend $500 on a 5600x -> 5800X3D though,Negative
AMD,I have those exact parts except only 16gb of ddr4. The ram hasn't been an issue yet but I'm really hoping that remains the case for the next few years!,Positive
AMD,The air cooled Porsche market of CPU’s 🫡,Neutral
AMD,"I have AMD Ryzen 5 7500F and a regular 9070, how come only the drr4 system is going up?",Neutral
AMD,Ooh I was looking for one to replace my 3900x if you are looking to sell it?,Neutral
AMD,I'll buy it for a reasonable price to replace my 5800x.,Neutral
AMD,"Crypto: gpu prices quadrupled  COVID: pc prices doubled, cars prices increased after few years of being used  Ai boom: memory prices quadrupled, old platform  prices cost as much as when they released  Its been constant 10 freaking years of some kind of mayhem for PC market.  At this point just buy a console if you want to only game.",Negative
AMD,How do you think it performs nowadays?  I got the same,Neutral
AMD,I tried ordering one on aliexpress and the seller told me it's out of stock and asked me to cancel the order. Then they kept the listing and doubled the price. I said fuck that and got a 5950X from an ebay auction.,Negative
AMD,Did it still?,Neutral
AMD,Why not hardware swap?,Neutral
AMD,This is USD prices so 682 euros if you do the conversion,Neutral
AMD,I upgraded to one from my 5600x once I learned about AM5. The final form for my AM4 system.   It's a great CPU but hot!,Positive
AMD,I used to have one of these back in the day. 🤣,Neutral
AMD,"At 4k the difference is about 2% avg.  At 4k with a 9800x3d the uplift is roughly.... 5% avg.  I am using a 4790k to run my 4k TV PC, and am upgrading it to a 2700x mainly just for fun. You don't need to worry with a beast like a 5800.",Neutral
AMD,I'm with you...5800X gang! Make sure you use PBO2 and the curve optimizer. 💪🏼💪🏼,Neutral
AMD,Were you considering upgrading from 5800X to 5800X3D?,Neutral
AMD,16 threads @ 4.8 is nothing to sneeze at even without the cool 3d cache,Neutral
AMD,Looks at my gf build with a 2600. Yeah... I should have upgraded last year.,Neutral
AMD,Also nice deepfakes if you are a man of culture as well,Positive
AMD,I saw someone playing battefield 6 with that CPU on YouTube and that shit was getting over 130 fps. Like wtf how is Zen 2 architecture putting up numbers like that? Chip must have been a monster.,Negative
AMD,https://preview.redd.it/4t2r3uotf18g1.jpeg?width=2000&format=pjpg&auto=webp&s=4720ec6aaf3242b5d114081aa3422636572739d8  mines in a box rotting :),Neutral
AMD,The article is based on sold ebay listings.,Neutral
AMD,"Yeah it’s weird that I could get way more than I paid right now. My 3090 was 465 (and maybe 80 in fan and thermal pad upgrades), 7950x3d was 400, and 96gb 6400cl32 ram was $300.",Negative
AMD,Kind of sad that AMD didn't make longer runs of the AM4 x3d chips.,Negative
AMD,Consoles will also run out and need price increases. They also require memory that must be paid for.,Neutral
AMD,I personally wouldn't order anything direct from there. It's a dice roll.,Negative
AMD,">Or, or, I know it's crazy.. don't buy a x3d cpu? Plenty of the other 5000 series works completely fine. I'm on AM4 and have zero reasons to leave right now.  okay? Good for you?   This is such a stupid take that it's hard to believe a real person actually posted it lmfao. So just because you personally dont do anything that requires an upgrade that means no one else in the world does and they are crazy for buying a x3d chip?",Positive
AMD,"I think something that people don't talk about much is how much an x3d is worth it at higher than 1080p. Its more noticable at 1080p and below but 1440p and above is harder to see as much benefit. HUB or somebody else also put out a video recently showing benchmarks of different CPUs performance when using a 5090 vs lower tier GPUs.   All in all leads me to think.   - If you have a 5090, money is probably not a big issue, then go get that x3d   - if you have a lower tier card but play at 1440p and above, don't bother unless you find a really good deal.",Neutral
AMD,"I'm 100% sure that people more likely to change cpu on already owned am4 board, than buy problematic cpu and motherboard",Neutral
AMD,"It's because AM4 supports DDR4. There are no AM5 boards supporting DDR4 (at least to my knowledge). The thing is, altho DDR4 are cheaper than DDR5, they are also rising in price, so idk if that'd worth it.  Now, if you already are on AM4 with decent DDR4 sticks, trying to ""max out"" the platform with 5800x3D might be something to consider. Issue is, again, this particular CPU also is expensive.  Maybe, this situation, it'd better to upgrade to some cheaper, non-x3D AM4 processor instead?",Neutral
AMD,I wonder if there would be a scenerio where AMD would consider starting to produce x3d chips for AM4 again. If there's enough people out there stuck on AM4 that are itching to upgrade it might be worth it.,Neutral
AMD,"$800 new, not used.",Neutral
AMD,That sounds exactly like how people treat others.,Negative
AMD,Lmao,Neutral
AMD,Would you say the same for someone with a 5700X3D?,Neutral
AMD,"That was exactly my reasoning for buying it a couple of months after it was released, it is a beast",Positive
AMD,Any X3D CPU will last quite a while. Even the rarer and recently released 5500x3D is pretty good.,Positive
AMD,"At this point, I'm praying my system lasts me until 2030!",Negative
AMD,"depends on your use case, but the 9800x3d is quite a bit faster than the 5800x3d, i personally wouldn't upgrade but i also dont play any demanding games/care about high fps in mmos",Neutral
AMD,But it's leagues slower than a 9800X3D,Neutral
AMD,"All 13th and 14th Gen intel cpus are and have always been defective.  Largest cpu class action lawsuit to ever occur still in talks.  Chips were manufactured with specs that make them overvolt themselves and the bios patches dont fix it, just make the problem less harsh all while killing your performance by underclocking your cpu, so you lose that performance anyways.",Negative
AMD,Missed the point. Paying 500 extra for a downgrade cpu instead of pahing 200-300 extra for ddr5 ram is dumb,Negative
AMD,That often necessitates buying a new motherboard as well though,Neutral
AMD,Just did that this week. Nice little christmas upgrade for myself.,Positive
AMD,14600k is mad underrated. Beast of a CPU.,Positive
AMD,And 30% higher change for the processor to rot,Neutral
AMD,Makes me tempted to make an sff all blue build ngl,Neutral
AMD,"This is the way, during black Friday I picked up this CPU and a motherboard for $220 together. Upgrading from my 8700k to a 14600k felt amazing, and I got to keep my 64GB of G.skill as well.",Positive
AMD,"I was considering a switch to AM4, but for the same amount i would be paying for a sidegrade from my i3 12100F, I may as well nab an i5 13400 or i5 14400 & relax, though a CPU upgrade isn't a top tier priority for me though, as i'm going to nab a GPU upgrade first before doing a CPU jump.",Neutral
AMD,screw this man I’ll just stick with my QX9650,Negative
AMD,"The 14600k is literally slower in games, you’re hallucinating.",Neutral
AMD,Says who?,Neutral
AMD,Not that much faster with DDR4.,Neutral
AMD,Intel? Disgusting.,Neutral
AMD,"It’s the memory my man, cheapest ddr5 32gb 6000c36 kit are 400€ in Europe and 6000c30 are like 550€.",Neutral
AMD,You could get a 7600X bundle for <$400 and it will Perform about the same as the X3D and still be upgradable down the line.,Neutral
AMD,They'll just give you a free 'PC' that you can use to subscribe to their cloud computing.,Neutral
AMD,Phones use CPUs and ram. They will effectively be going down in specs to maintain the same price for what we pay now for better specs.  Society needs to accept an pre 2000 era lifestyle and let all these tech companies crumble.,Neutral
AMD,"This is already a thing. Cloud computing. You will just rent your computer time now. Own nothing, be a slave to the corporations for everything.",Negative
AMD,you'll own nothing and be happy.,Positive
AMD,Nope you will be able to lease a thin client pc to access your subscriptions to Geforce Now.,Neutral
AMD,"Dumb terminals and mainframes, here we come!",Neutral
AMD,Hell is real,Negative
AMD,"Not gonna fly, not everything I do tolerates the eyes of others.",Negative
AMD,"How would that even work? You have a mouse, keyboard, and monitor but you connect everything to your modem? Sounds impractical",Negative
AMD,"We are headed there. Customer hw is getting expensive and while cloud hw also increases in price, when the AI bubble pops and demand settles on a more realistic level, the remaining computing power can be rented out to customers.",Neutral
AMD,Private equity is going to want their money back too lol. Especially when AI doesn't produce a tangible product 90% of the time.,Negative
AMD,"Quantum computing just messaged....Said it's leaving now, should be with you sometime soon after the AI bubble bursts.",Neutral
AMD,What are you going to datahoard with a few micro sd cards?,Neutral
AMD,Why wouldn’t an old mechanical HDD work? Those things are a dine a dozen and you could have several terabytes lasting a decade or more in cheap enclosures.,Neutral
AMD,1TB isn't a lot of space dude.,Neutral
AMD,"You can build a pretty strong PC with older GPUs. But when the current platforms only support 2 types of ram, your options become far more limited. Sure DDR4 is still plentiful and ""cheap,"" but it's no longer produced. When the supply runs out, it'll rocket in price too.   Everything uses ram, and it's all gone 5x in price across the board. Not everyone needs or uses a top-of-the-line GPU.",Negative
AMD,"My pc was also typical high end system by that time (2600k) and it definitely started to show its age/not be ""extremely fast"" at least ~5-7 years ago.",Neutral
AMD,"No no no. Not the regular 5700. It’s not the same as a 5700x. It has less cache, pcie3 only, and is worse than a 5600. It was terrible naming. It’s basically a 5700G with the gpu disabled.   https://www.techspot.com/review/2802-amd-ryzen-5700/",Negative
AMD,Just got a 5600x! Happy with it.,Positive
AMD,"Yup, still rocking my 11700k",Positive
AMD,Well I’m regarded,Neutral
AMD,I went from 16 to 32 so my old kit its not being sold     I agree i want years of work from my ram,Negative
AMD,This year ddr4 was suppose to end production but it was extended to next year     So last year of ddr4 production with the AM4 platform still relevant and ram shortages do too greedy companies.     A perfect storm.,Neutral
AMD,buy a 500-600 dollar console so you can pay 10-20 a month to play the online games you already purchased  just to effectively spend more money in the long run.. right,Neutral
AMD,"My 3600 is holding up well. I use it every day for content creation and gaming, and even vr gaming.",Negative
AMD,"Doesn't bring the same price, there's honestly a $200 difference. If I have to ship - might as well be on eBay. Nobody local wanted to meet.",Negative
AMD,"no no, I already converted it to USD when I was making that comment  bought it for 50% off at the time, thought I was getting a good deal ... I mean, it was, but not as good as if it was 50% off of MSRP (taking the tax into account)",Negative
AMD,"Right now I have a 3800X and Im getting away with pretty much everything. I don’t know what kind of improve I would get with the 5800, but I figure I should upgrade one last time on my AM4 motherboard.",Positive
AMD,"No. I have a 3800X, and I figured maybe I should upgrade to the last AM4 chip before getting an entirely new motherboard. Im relatively happy as things are, but I figure if I upgrade I may see a performance improvement. Especially with an X3D line.",Positive
AMD,"Maybe, but the specs just look the same to me between 3800X and 5800X once you take it away.",Neutral
AMD,IT has a ton of cores but games typically only use a few. it's a completely viable system. I've upgraded the vid card only since then and I likely don't need another upgrade for years.,Positive
AMD,"Damn, well there’s a chance. I guess I can just refund it if it’s a scam. I just don’t want it shorting anything out on my motherboard. I’m thinking worst case scenario, it’ll be a different CPU.",Negative
AMD,Go buy a 500 to 900$ marked up cpu then. Most people will have 0 reason to upgrade to a x3d cpu. Go back to your elitist cave.,Negative
AMD,Oh gotcha that makes sense. I forgot how ddr5 is linked to AM5. Much more cheaper getting a new compatible CPU then having to change your setup and deal with RAM prices.,Neutral
AMD,"At least with ddr4, there is probably a lot sitting there on the market unused after people upgraded or whatever, the price for the longest time was low enough where many sizes of ram were barely worth the effort to sell. For current prices many will dig old ram out of their closet though.",Negative
AMD,It's doubtful. They won't risk cannibalizing sales of AM5.,Neutral
AMD,Yes.,Neutral
AMD,Smart Man. I took the plunge too just wondering what the potential of X3D chips would be. I'm so glad it paid off and wasn't an unstable mess.   Perhaps the single best upgrade decision I have ever made.,Positive
AMD,"Hey, I still have Sandy bridge systems running strong. Just set your expectations, and don't try to ride the bleed edge, the hardware can last quite awhile and still be very useable.",Positive
AMD,You can get a 9800X3D/32GB/Mobo for less than $800,Neutral
AMD,$90 gets you a B760 Asus AYW DDR4 board.,Neutral
AMD,And you can find plenty of decent motherboards for well under $150 for that CPU like mine. That's still less than the cost of the 5800x3D alone. The math makes sense when you sit down and think about it,Positive
AMD,"It's never been a real issue on the 14600k but if you are paranoid, just update bios and if you are really really paranoid, lock vcore max to 1.45v",Neutral
AMD,Almost all affected cpus were i7 and i9,Neutral
AMD,"Sorry glazer, every single review/benchmark says otherwise.",Negative
AMD,And the DDR5 RAM...?,Neutral
AMD,Then at that point just get like a 5800x.,Neutral
AMD,Imagine not being able to play a single player game because too many people want to play it at the same time.,Negative
AMD,Just like phones where you “upgrade for free” every year they’ll just put you on a subscription for a pc for $24.99 and every year it’s only $5 more for the newest model.   You’ll never own it and you’ll always get a crap tier pc,Negative
AMD,"In the industry it's called a ""Thin Client""...",Neutral
AMD,Game streaming says hi.,Neutral
AMD,They’ll like shitty ass Chromebooks too just enough storage for the os and client apps.,Negative
AMD,Not everyone has fiber internet (with low latency.) I will never partake of cloud gaming.,Negative
AMD,"Free loadout, coming soon to a home near you.",Neutral
AMD,It’s just like streaming video. Heck people use online spreadsheet now.,Neutral
AMD,You have a very light end pc that just runs an OS and can connect to the internet.   It's cloud computing.,Neutral
AMD,The return on investment is already horrible  and they keep pumping money at it hoping it works but... nope,Negative
AMD,Tbf the crypto bubble burst and although crypto is still a thing its not nearly as prolific as we thought it would. AI is here to stay but it probably won't be shoved into everything in a few years,Neutral
AMD,"Doesn't really have to be a big bubble pop. Just has to have a shareholder mood change on AI investment, which we're starting to already see. Right now investing in AI has a negative pressure on share valuation, at least short term. I think eventually it'll cause the capital investments to peter out. Doesn't necessarily mean a crash unless these companies invest with debt though.  I do suspect we'll see a crash in the datacenter market once the investments calm down though, unless something else computing wise comes along to use the buildings. Anything deployed today is obsolete in 5 years or so.",Negative
AMD,Some gems form pre-AI internet.,Neutral
AMD,"I’ve got one 1TB drive taken out of an Xbox waiting for an enclosure. It’s showing “Caution” on crystaldisk, so I don’t know how long it will last, but I’m using all I can get.  MicroSD cards are just very versatile. You can use them for older phones, iPod modding, cameras and a whole lot more.   I’m not buying much rn because prices are ridiculous. The same 2TB hard drive I bought 2 years ago for $50 is now $70. I don’t even want to talk about SSDs.  I also have about 8TB total between HDDs, SSDs, USB drives and SD cards. Got some M-discs for stuff I want to really preserve like personal photos.",Neutral
AMD,I got a 5800X and it heats my house,Neutral
AMD,*sigh* this is the same chipset inside of the 5800H paired with my 3070 Laptop. Always bottlenecked my 3070 in moderately complex games and I was dying for a while until AMD came out with FSR 3 frame generation. Saved me so much headache. Pairing a weak APU with a powerful dedicated GPU was dumb.,Neutral
AMD,"New to this. Question: is the 5600x that much better than the 5600g? Most of the comparison sites I've checked seem to suggest they're similar, with the x being only 10-15% better. I wouldn't know better though, so are they that far apart, in actuality?",Neutral
AMD,"I mean, not every game requires PS+, marvel rivals for example doesn’t.   But most people play single player games and you don’t need PS+ for that.  I also don’t enjoy playing online shooter games on consoles. Rather don’t play them at all.  If you play fighter games you want to play on consoles anyway. If you play racing games, then GT7 is great title and iRacing also needs subscription.  PlayStation plus is 9€ and extra is 14€ in Europe and you get access to many PlayStation games if you have long backlog it’s god send instead of spending money on every game.  So sure there are some games you may want to play online like GTA. But you aren’t grinding that either, so you buy PS+ once in a while.  Consoles paying for internet twice is overblown.",Neutral
AMD,Ah ok     I got mine around the $350-400 mark years ago but its amd there prices are basically a roller coaster.,Neutral
AMD,"Even rolling the dice with Amazon these days by way of return scams. Right now the only way to definitely get your stuff is to buy it in person and have someone at the store open it in front of you.   Chinese sellers will swap out the heat spreader plates from different model CPUs, put a different bios on gpus and other nefarious stuff. I just don't trust it. Good luck.",Negative
AMD,"Just got one several months ago thankfully. Was a substantial upgrade that was absolutely necessary that I wish I had gotten sooner. Now I am happy I'm able to enjoy my favorite hobby again with 0 problems or frustration.  Isn't that crazy that someone else could desperately need and value that upgrade even tho you personally dont need it? How elitist of me, right?",Positive
AMD,What if they aren't cannibalizing sales though? What if so many people put off upgrading that there's actually more money to be made with Am4? Hypothetical of course but I dunno doesn't seem insane to me.,Neutral
AMD,5700x3d also. I will ride this badboy 2-3 years more,Positive
AMD,5600? 🙏,Neutral
AMD,I won't overclock or anything like that. I currently run a 5800x3D and a 7900 XTX. Just gotta hope nothing big goes down.,Neutral
AMD,"Microcenter gotta have some bundle deals, yea??",Neutral
AMD,I already had to exchange the 13600kf twice,Neutral
AMD,Not sure what you’re looking at but benchmarks from reputable sources clearly say otherwise,Neutral
AMD,Bundle includes 16GB,Neutral
AMD,"Oh you probably will, but for a premium upgrade.",Neutral
AMD,Ahem EA   Oh you want to play skate 4 by yourself? Wait in this queue,Neutral
AMD,"Already losing Anthem. . . and while it's technically an online multiplayer RPG, the single-player storyline is the best part of that game and should have held as a standalone.",Positive
AMD,“You will own nothing and be happy.”,Positive
AMD,"Yeah, im out dog",Neutral
AMD,They don’t give a shit about you then.  They’ll just take away your affordable local compute options and leave you with nothing because you’re too marginalized for them to care.  It really is heart braking that this is how PC gaming begins its slow death.,Negative
AMD,"So you would still have a PC, got it",Neutral
AMD,Best scenario AI is dropped like a bad habit. Worst case and sadly most realistic is this probably.,Negative
AMD,"I bought a 5700X and it absolutely destroys almost anything I throw at it while consuming fuck-all power.  The other day I was testing Immich which has **local** machine learning with smart search (you give it ""cat sitting on chair"" and it returns every photo of a cat sitting on a chair it can find), facial recognition and OCR, and I gave it a set of 1400 photos to analyze, with no hardware acceleration (CPU only), and a multilingual search model, something more complex than the default.   It chewed through it in like 10 minutes. Sure, an even more modern or more powerful CPU could run circles around that, but my point is, if you're willing to be a bit patient, do people *really* need that much CPU power unless they're chasing the absolute most FPS in gaming?",Negative
AMD,5600g is the APU version so it has less cache but includes a gpu in it. 5600 and 5600x perform basically the same when you turn on PBO for the 5600. both better than a 5600g the techspot link i posted has a 5600 vs 5600g in it. the g is about 10% slower and it has less pcie lanes too. always better to get the non g version if you can unless you're building a small no discrete gpu build.,Neutral
AMD,"Your right, how inconsiderate of me to stop you from getting ripped off. But please do go on, I have a pretty neat car to sell you, sounds like its right up your alley. I don't think you comprehend that value from AM4 is awful specifically compared to AM5. You could get a cheaper and more powerful cpu that's more efficient as well under the 7 and 9 series. But you chose a far more expensive path with zero upgrades. I'm the idiot though, right?   Let's point out that normal people aren't 'streamers' if i can call you that. So yes, normal people and even those with lower grade hardware won't see much benefit in a x3d cpu specifically meant for gaming.",Negative
AMD,I grabbed one during the $110 glory days on AliExpress.  Simpler times.,Neutral
AMD,5800x3d until the frames aren’t good enough anymore.,Positive
AMD,"I'll be using mine until it dies most likely. I essentially have endless games to choose from that'll run fine on my current hardware (5700x3d + RTX 5070). The only games my PC won't be able to handle are future AAAs, and I'm fine skipping those.",Positive
AMD,"yeh I'm on a standard 5800, non 3d, quite happy with it, not changing anything for 3 years I think, at least.",Positive
AMD,More like a decade. Gaming isn't moving away from 8c/16t any time soon so it's gonna be the sweet spot for a long time.,Positive
AMD,"It all depends on your expectations, but if it works then it works.",Positive
AMD,"For games, the X3d part is really key.",Neutral
AMD,"I use my 5800x for a lot of geospatial computational work. Is definitely still a beast and handles tons of data with ease. Same with all the games I play. (Now I don't play the newest titles. But anything current or older I easily get 120+ and that's with other background processing and stuff going on.) All that to say. Unless your trying to keep up with the Jones, don't sweat it. That rig will last a good while. And the part that's most likely to fail (the motherboard) is still the overall cheapest part.",Positive
AMD,bruh,Neutral
AMD,"Yes I just saw a 32GB RAM/7800X3D/MSI Mobo deal for $579 (up from the $400s like 5 months ago) which I would do in a heartbeat over just a 5800X3D for even the same price so I find it hard to believe “people” are actually paying $800 for a 5800X3D  Now, I do wish I had bought a 5800X3D in march for like $250 but I felt the same then- I couldn’t justify the upgrade over just building a new PC with AM5 parts (which now I wish I had done)",Negative
AMD,"My 13600k has been rock steady from the time it was bought, and I'm the second owner. I know the original owner upgraded their bios at some point along the way, but neither him nor I had any instability issues.  Just out of curiosity, what issues were you specifically noticing with yours?",Neutral
AMD,https://www.techpowerup.com/review/intel-core-i5-14600k/18.html  cLeArLy,Neutral
AMD,"That's a good deal, then!",Positive
AMD,"They won't manage it, take a look at some successful MMO's, shit hits the fan on each patch/expansion release",Negative
AMD,Nope. Amazon Luna is via their Firestick. You just stream the games and connect a wireless controller.,Neutral
AMD,"I dont' mind AI for some things although it has completely fucked most artists by stealing and training on their work.   It is quite useful, I just hate that CEO's and shareholders are salivating at making billions of people redundant. We could genuinely use it to improve people's quality of life but no....Let's surveil the shit out of everyone and create some awful big brother society without privacy and where nobody owns anything anymore.",Negative
AMD,"I’m gonna try that I’ve enjoyed messing with oolaama and local models but most use my GPU.   So I have a 5800X and my wife has a 7800X3D and both work fine for us. Heck my kid has my old 3600 and it works fine too paired with my old 3060ti.  People are overly panicking, the shortage is going to delay PC sales to both enterprise and consumers and software isn’t going to assume as rapid performance growth in hardware as it might have. That includes games. And I’m fine with that because games look good now. Heck they looked good in 2014. A stall out on hardware progression forces studios to learn to optimize.",Positive
AMD,Ah gotcha. Thanks for the info!,Positive
AMD,You're setting up an awful lot of strawman arguments and false assumptions to pointlessly try to ignore the reality that other people may need the upgrade regardless of if you don't.  With the current state of the market if you have a weak AM4 chip this is still cheaper and much more stress/hassle free than having to upgrade to AM5 and buy DDR5 at its current price on top of a new motherboard and cpu and rebuild/rewire your entire PC,Negative
AMD,I still don’t understand how that happened but I’m glad I took advantage,Negative
AMD,Sames.,Neutral
AMD,Cheap 5700x3d best buy ever,Positive
AMD,"Yeppers. My 5800x3d, 4070ti and 80 odd gigs of RAM will be the foundation for my PC for some time yet I reckon",Neutral
AMD,I'll just keep playing helldivers for the next 5 years,Neutral
AMD,"God willing. I'll be honest, I'm really eyeing the new Resident Evil game but I mainly play fighting games which aren't really demanding. I got stupidly lucky when I went to Vegas and won a Sony Inzone 4K monitor so that makes things a bit more demanding. I'm hoping to replace it with a 1440p OLED monitor since it would be a lighter load for my PC.",Positive
AMD,"The upside of going for that 9800x3D also means you won't have to upgrade your CPU for like a decade, too. My local Microcenter is still offering a 9800x3D, Asus B650E-E, and 32GB ram for $679. That's a god damn steal, seeing as that same 32gb ram without the bundle is $348. The 7800x3d bundle has the same memory options and is only $399. So you're getting a CPU a step down from the 9800x3D and a motherboard for only an extra $51. There is zero reason to build a 5800x3D just because DDR4 RAM is slightly cheaper than DDR5. Not when Microcenter is offering their current AM5 bundles that aren't inflated with current DDR5 prices.",Positive
AMD,"The z690/790 boards in my experience push such an aggressive LLC that unfortunately the bios updates don’t address. My 13600k out the box in R32 was pushing over 1.5v! MSI Lite Load adjustment to level 3-6 will easily fix that problem, I don’t see more than 1.3v now.   Coming from a 8700k, whenever I see 1.5v on Intel, alarm bells start going off in my head.   So yea if you just plug and play these chips, even the 13600k, you can have problems IMO.",Negative
AMD,"The first one would blue screen randomly when playing games, specially when I had YouTube on second monitor but it was kind of random. Before the whole Intel drama I thought it was windows issue and kept formatting the computer and installing again but never fixed it. When this news came I got it exchange under warranty. The second one would freeze the computer at times, it would happen more often this time when alt+tab out of a game. Either games would crash or the whole system would became like in this frozen state thar I would need to force reset. This time I didn't try much, I just got the processor replaced again. So far the 3rd one is holding up, but it has been 6 months only.",Negative
AMD,So 8% at 1080p with a 4090? What were you smoking when you said 30%,Neutral
AMD,That's not what OP was insinuating. They used Amazon Luna as an example. They claimed nobody would have PCs anymore and I said that was impractical and didn't make sense to me.,Negative
AMD,"This. If AI is used for science, medicine, repetitive  or dangerous  tasks then sure, go for it.   But for gen AI/ creative stuff or government surveillance/ military apps? Fuck that shit, crash and burn",Negative
AMD,For me it was like $180 on new egg with 32 gb of ram free.,Neutral
AMD,">The upside of going for that 9800x3D also means you won't have to upgrade your CPU for like a decade, too.   Maybe not a decade, but at least for the next 5 years. At least that was my thoughts when upgrading from AM4 to AM5 at the beginning of this year.",Neutral
AMD,"Man, that's bad. I'm grateful for how smooth mine has run, but now I'll panic if I ever see anything close to those symptoms. Lol.   Hey, maybe third time's the charm. If mine can survive two people, surely you can eventually get one to survive you.",Negative
AMD,I was smoking some RT heavy games and multi thread heavy games where it pulls so much further ahead. I don't expect glazers to understand that but that's ok.,Neutral
AMD,"What makes sense to you is of absolutely no concern to them.   You will be provided the services they want you to have, through the cheapest device and the most expensive subscription they deem necessary to increase shareholder value.",Negative
AMD,"This i got mine for 500 Canadian, but it was a Newegg combo deal that came with a good MOBO and 32GB of 3600 DDR4",Positive
AMD,"Hopefully. The most annoying thing is that both times I had to disassemble everything and wait about a month, and I don't really have any extra processor laying around that fits this motherboard. I have an older 10400 but I did not really want to go buy a second hand motherboard and go to trouble of further assembly and disassemble. Well, I hope this time at least nothing more happens, maybe the bios updates helped, who knows.",Negative
AMD,"Ah, so you were cherry picking benchmarks. Does intel pay you to be a shill or do you just have too much free time?",Neutral
AMD,"Who is they? Last I checked there are multiple companies that produce memory chips and CPUs. Companies that aren't even in North America. So who is they and who all is working together? Taiwan, Vietnam, and Western government? I think you guys are fear mongering and nothing more... It's obvious the prices of components will be high for the next 1-3 years but they will come down again because that's not a sustainable business model. People get tired of paying $600 for RAM kits even big government has a budget.",Neutral
AMD,"Lmao I literally gave you the biggest benchmark site that slightly favors AMD. I'm sorry that proved your argument wrong and invalidated your feelings. Instead of throwing tantrums trying to get validations from CPU benchmarks, you should be trying that from your daddy or a therapist instead.",Negative
AMD,"You'll have a good time. Depending on what you do, the 1TB might get eaten up quick though",Positive
AMD,Yea my plan is to store anything I can on an external 1tb drive I already have. I have used this same strategy on my steam deck and my kids Xbox. I think this is am5 too so my upgrade path should be pretty long right?,Neutral
AMD,"Check event viewer for why it shut down. Your PSU is within spec for your components, no problem there unless there's a defect.",Neutral
AMD,Look at that cable management. Impressive,Neutral
AMD,I had something similar happening with an older 750W power supply. It was under rated on the 12V rail and would have some wicked coil wine before shutting down. It was just an abrupt power off. Got an 850W supply and all the problems and symptoms went away.,Negative
AMD,"OCCT power stress test helped me identify my PSU was faulty, where up to that point I was just getting occasional reboots running stressful games for hours, that test made it reboot in under a minute.",Neutral
AMD,"Same happens to me just an hour ago. Pc just crashed and tried to reboot, with the VGA light going white. Has to hold down reset button and it went up again.   Running a 6950XT and a corsair Rm1000x. Must be something with AMD drivers/windows 11",Negative
AMD,"did it hard reset ? could be power supply .. 750watt is right on the red line for my 9800 x3d and 9070 xt  looks like that psu"" Efficiency drops under heat"" and has"" Risk of overload with 12V-2x6""",Neutral
AMD,"I checked it just shows critical event system shut down unexpectedly. No other warning or error signs. It was instantaneous, heard a click as well (probably from the PSU). As to why it happened I have no idea. It wouldn't turn on afterwards so I had to unplug the power connector and re-plug it back to turn it on. It didn't happen to me before in games like Arc Raiders, Kingdom come deliverance etc.",Negative
AMD,"In your case it's most likely driver related. In my case while gaming, the PC just instantaneously turns off completely and the monitor goes black with lost signal. I am unable to turn it on from the power button and have to manually unplug the power chord and re-plug to be able to turn it on again. Most people told me to replace my PSU with a 1000w one like Corsair rm1000e or MSI mag a1000gl pcie5",Negative
AMD,"psu. ⬆️  have similar build I'm running [https://www.amazon.com/dp/B0FBY3F1NT?th=1](https://www.amazon.com/dp/B0FBY3F1NT?th=1) 850watt  I will say though OP, after AMD did some driver updating last week I had a similiar issue but it has not reoccured.",Neutral
AMD,ID 41 or 6008?  6008 would be power issue/crash.  41 is usually power loss/hardware crash  Could be BIOS RAM profile. Any hardware OC you may be running. Loss of outlet power momentarily. Symptoms vague. Wouldn't worry too much unless the issues persist.,Negative
AMD,"In administrative events I saw the following:  - Event 6008, EventLog - The previous system shutdown was unexpected  - Event 41, Kernel-Power - The system has rebooted without cleanly shutting down first.  - Event 1101, EventLog - Audit events have been dropped by the transport. 0  - Error setting traits on provider {8444a4fb-d8d3-4f38-84f8-89960a1ef12f}. Error: 0xC0000001  Then bunch of warnings  Then error event 1023, Perflib: Windows cannot load the extensible counter DLL sysmain.dll",Neutral
AMD,were you OC'd?,Neutral
AMD,"I tried to run a GPU stress test earlier in adrenaline app, the pc shut itself down again. It could be a PSU problem idk",Negative
AMD,Good find!.  I have the same card.  I ran it for 30 minutes OC'd on my 850watt I posted above and it was solid.  Just for your testing and diagnostic purposes.  good luck.,Positive
AMD,Yes,Neutral
AMD,Most definitely!,Positive
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,"I don’t understand why they won’t make one with 6 cores and 16cus with no npu for gaming handhelds. We don’t need 12 freaking cores in our handhelds, it’s actually detrimental considering it pulls power from the tdp starved gpu.",Negative
AMD,Rebadged ryzen 5 340.,Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,"This doesn't deserve the AI7 naming with 2+4 cpu cores and 4 cu graphics.  I think this is the uncut Krackan Point 2 die. If it's Gorgon Point, then they could have given it 3+3 cores, just like the 340 predecessor, and not make it worse than it.",Negative
AMD,Soo rdna3.5? How much the improvement from 740m?,Neutral
AMD,a 6/16 with RDNA 4 (for the FSR4) would make a fantastic successor Steam Deck 2,Neutral
AMD,"Isn't this the Z2E? It's an 8 core IIRC, but 5/8 are Zen 5c cores (granted, it's cut down from Strix Point).",Neutral
AMD,"Not a ton, but measurable. Per-CU rdna3.5 isn't much better in typical gaming, but it is better. Check out 880M vs 780M benchmarks to see the top end of each.",Positive
AMD,They will def skip rdna4 for some stupid reason.  I get it’s a stop gap before rdna 5…but it’s also a 2 year gap.  Maybe longer now with memory prices.  I think steam deck 2 will be 6 zen 6 cores and 12-16 rdna 5 CUs in 2027.  But again…memory :/,Negative
AMD,In 2028.,Neutral
AMD,"Z2E is rdna 3.5, which is just a refined rdna 3.  Rdna 4 is significantly more efficient than rdna 3.  Example: the 9070xt is a 64CU chip that outperforms the 84cu 7900xt at similar TDP (~300w)  If that holds true at lower tdp’s, then an RDNA 4 16cu chip would perform around 25% better than the Z2E, which is already basically 80% better than the steamdecks apu.  So a hypothetical rdna4+ 6core 16cu handheld would have >2x performance to the OG steamdeck.",Positive
AMD,"AFAIK Valve said they're not going to make Steam Deck 2 until there's a very noticeable advance in APU technology. IIRC even +50% performance is not enough for them which is a shame on one hand, but on the other, I totally get it.",Negative
AMD,My comment was not about graphics.,Neutral
AMD,Rdna 5 would be well beyond 50% considering rdna 4 would deliver almost 50% uplift.,Neutral
AMD,"You can't push for devs to make console-like optimization for your device if you keep releasing new devices everytime there is +10% power available !  SD2 should be like Switch 2 vs Switch 1, something that really feels like a different generation, that will run games that the first one can't even hope of launching.",Neutral
AMD,"Isn't it like there would be no RDNA 5? AMD is planning to merge RDNA (consumer branch) with CDNA (professional branch) into UDNA. Nonetheless, the rumors suggest that APU based on Zen 6 (which would be the next generation) could power the Steam Deck 2 with its launch in 2028 ([article](https://www.linuxjournal.com/content/steam-deck-2-rumors-ignite-new-era-linux-gaming)). Fingers crossed for Valve",Neutral
AMD,">You can't push for devs to make console-like optimization for your device if you keep releasing new devices everytime there is +10% power available !  The problem with Steam Deck is that it's just a PC packed into a handheld. It uses the same games library as your ""normal"" PC, someone's else ""normal"" PC, my ""normal"" PC, so there's no chance we'll get the console-like optimization since it's impossible. PCs are just too different.   Although I totally agree that there's no point in releasing a new handheld every year or two because there's a new APU with +15% more performance.",Negative
AMD,I read that as zen6 being 2028 and got a little concerned. I’m planning to upgrade to zen6 when “AM6” drops and that timeline was putting it at 2032 lol.,Neutral
AMD,"Yes it will likely not be called rdna 5, rumors have said udna for a while.  I think 2028 is way too late.  Next gen consoles and PlayStations portable will be 2027 which will all be zen 6 and udna.  Unless memory pricing pushes that back.",Neutral
AMD,"I think AM6 will be revealed in late 2027 or in 2028 as it'll be around 5 years since AM5 release, so it's at least 2 years of waiting.  I also wait for AM6 to upgrade my PC, but if the current situation will stay with us for longer (or, hopefully not, become even worse) mg rig will have to stay with me for a while.",Neutral
AMD,I’m rocking an AM5 platform now and my plan is to upgrade when they EoL the socket to an end stage processor.,Positive
AMD,"Yet another ""gaming"" laptop with 16 core 9955hx with 9955hx3d option, but the max gpu is 5070ti. Because making a 6-8 core x3d with laptop 5080 or 5090 would make too much sense.",Neutral
AMD,Yeah it happens. Its why I went with the legion pro for the x3d CPU at 100w allowance and 5080 at 175w tgp.,Neutral
AMD,Plan to rock mine until AM6,Positive
AMD,got my 5800x3d years ago best purchase ever. I was ever so hesitant from coming from the 3700x cpu but the magic that was 3d cache can not be underestimated. now if we can just get a big amd gpu i wouldn't have to worry for a while.  sad fact that pc parts are getting artificial scarcity cause ram makers well we don't want to scale growing demand we just want to price gouge.   Just hope this ai bubble pops in all of these companies faces,Positive
AMD,"I feel like they won’t considering production has been shut down for over a year. The 5700x3D, 5600x3D and 5500x3D are made from already existing 5800x3D chips that failed QA. I don’t think any AM4 x3D silicon has rolled off the assembly line since last September.   Maybe they could do a Rocket Lake style back port and get a newer Zen running on AM4. Again it’s probably not worth the effort unless next year’s market is catastrophic. Intel allegedly still has Bartlett Lake in the pipeline which would be a 12 P-Core DDR4 CPU with no E-Cores.",Negative
AMD,Long live AM4.,Positive
AMD,Can AMD make a 5950X3d while they're at it too?,Neutral
AMD,I've been sitting on a 5800X for a while since I upgraded my old rig to the 3d version. Guess I should pick up an AM4 board and some DDR4 for some home server needs.  ...or maybe I should just sell the 5800x3d because apparently these things are still going for over $400 on ebay used? wtf?,Neutral
AMD,"Even if AMD wanted to, this would have a lead time of easily 6 months, and nobody knows if the RAM price problems will persist until then – or if DDR4 will stay affordable until then.",Negative
AMD,The PC market is trash—and will stay trash for a while yet.,Negative
AMD,Guna ride my 57003dx and 4090 all the to AM6. I’m gpu bound at 4K anyways,Neutral
AMD,My 5800x3d still isn't the limiting factor and I've had it ages.,Neutral
AMD,I'm glad I got a 5700x3d while they were still affordable. I'm gonna be set on my x370 launch day board for the next 4 years lol.,Positive
AMD,"I want to see 5950X3D with two 3D cache or nothing.      the true AM4 behemoth, the beast that was never born",Neutral
AMD,"I got mine when they came out, its a champ. The 3dcache makes it perform like its a gen newer in most games. I plan to ride it out until am6 at least lol",Neutral
AMD,"Zen3 X3D Production has been closed for over a year, everything sold now is just parts that failed QA and are binned down.",Negative
AMD,I'd rather see the 5700x3d. Like 2% less performance for a notably lower cost.,Neutral
AMD,"It’s the 1080Ti of CPUs, no chance AMD brings it back",Neutral
AMD,With the price of RAM these days this would be a smart move. 5800X3D DDR4 totally still viable these days.,Negative
AMD,Honestly? Im for it.  Rocking the 5800X3D/507012gb/32gb DDR4 and I can confidently say this is a system that'll last me until 2030 and maybe even beyond.,Positive
AMD,"I've got a 5800X3D, 4090 and 64GB ram. I think I'll be holding onto this setup for a LONG time.",Positive
AMD,So thankful I got my hands on one of these when I did,Neutral
AMD,Mine's still going strong along with a 7900 GRE. The GOAT.,Positive
AMD,Going to rock the AM4 and 5900x until AM6.  There’s no need to upgrade a socket change when you don’t really need it,Neutral
AMD,"Ever since i got mine and i forgot what bottleneck is. seriously lol, pairing it with a 9070 xt and they both fight like champs. love it!",Positive
AMD,It’s crazy that these things are talked about like they’re an ancient relic. Grabbed mine at the initial release and it feels like I just got it the other day.,Neutral
AMD,Dies in my 5950x,Neutral
AMD,If they did return it to production I would snatch one so fast. Looking to upgrade and with RAM prices jumping to AM5 doesn’t exactly look appealing,Negative
AMD,If it _really_ came to it I don't think there's any actual technical reason they couldn't slap a Zen5X3D chiplet on a package with the AM4/DDR4 IO die.  That's probably a product they tested internally and found it just didn't make sense (Zen5's are kinda bandwidth starved with DDR5-6000... there'd be a lot of wheel spinning with DDR4-3200) but that'd have been before ram costs went to Narnia.,Negative
AMD,"I miss the days of legit £125 5700x3d's off sellers on AliExpress, now you gotta drop around £250 for a 5700X3D  I don't see AMD reviving the line now tho",Negative
AMD,"5800X3D , 32GB ddr4-3600  and 7900XTX  This will need to last me till AM6 arrives.   Might grab a 5800x3d for my sons rig if I can ever find one  .",Neutral
AMD,That will help only those who already are on AM4 and want to upgrade CPU and already own DDR4 ram. But prices of DDR4 have already been increasing in fast pace and are already almost 300% what they have been just in October here in Norway and I believe similar situation is in other places too. Anyone who thought that stable DDR 4 prices will stay down or not get extremely expensive just wait. Everyone is now trying to make money on desperate PC people. Look just the prices of standard HDD. Everyone who thought of going over from SSDs or M.2 to HDD for mass storage should know HDD prices are also going up in incredible pace. Yeah PC users are FUCKED big time.,Negative
AMD,"I have had this same thought the last few days after seeing X3D prices skyrocket. There is obviously demand, but is it worth it enough for them to restart production, even if they have the capability. Not to mention the risk of said chip instantly selling out if the batch is too small.  I personally would not be willing to pay more than $200 for an AM4 X3D upgrade, especially since I have a 5600 so it feels like a 5700X3D restart or refresh would be most likely.",Negative
AMD,my fear is that the industry aims to remove the consumer (us) of having his own HW and to force subscriptions on us,Negative
AMD,"I've seen the 5800XT still being sold. I have a 7950X3D, and I barely use the cache specific cores for anything but gaming. Is it worse to just install the XT as an upgrade in an old build? A friend of mine installed it last week on his PC because he couldn't find the 3d version and he doesn't want to upgrade everything yet (too expensive for him). I think he was quite happy with it even though it wasn't the 3d version",Neutral
AMD,Gamers and reviewers must not really understand business operations then. I cannot think of a single time anyone has spun up foundries(which are already very competitive themselves) because customers were facing a hard time.,Negative
AMD,"I have 5600, 9070XT with 1440p display. Is 5800X3D is good upgrade? I won't upgrade to AM5",Neutral
AMD,"I kinda wish I had gotten a 5700X3D or 5800X3D. They are the best AM4 has to offer, and would have been an affordable upgrade to my old 2700X. That being said, I have recently upgraded to the 7800X3D and it has been amazing.",Positive
AMD,"I would unironically buy on day one. I'd love to buy a used one too, but as far as I can tell, nobody is selling their Zen 3 x3D CPUs on the used market yet (because why would you).",Neutral
AMD,Went for a 5700x3d from a 3600 about 9 months ago. I'm happy I made that decision. The x3d chips are just so impressive in terms of gaming performance. It was almost never what was limiting my system in games.,Positive
AMD,Is my 5600x new again?,Neutral
AMD,I would love it if they made a new higher tier AM4 x3d chip. That would be amazing.,Positive
AMD,"i gave my workstation 3700x/x570 to my niece. tbh it was pretty power-thirsty and it didn't age as well as i thought '8 cores' would outside productivity, but it's a nice rig.  currently using a 12700k ddr4 setup, which is fine for me but... eh.  it's not exciting at all.  it wasn't interesting when i built it.  it'll carry me through the AI horror show though.",Positive
AMD,I will buy it 200% if it becomes available.,Positive
AMD,"I did make the step to upgrade from a 5600X to 5800X3D, it was the best decision years later , combined with PBO and a peerless assassin, I have a beast running better and cooler , as I play with a 4070Ti @1440p I feel like I'm in the best sweet spot",Positive
AMD,"Buying myself a 5800x3d a couple of years ago, even at Corona prices, might have been one of the best decisions i made, tech wise  This CPU just...goes. I play games, i work from home, i dabble with AI stuff. I've changed out a total of 4 GPUs since then, a 1660 super, an Rx6700xt, an Rx 6900xt and now rocking an RX 7900XTX. Still the same CPU, never saying no to me, no matter what i throw at it.",Positive
AMD,I miss the good old days of $120 5700x3d on AliExpress,Positive
AMD,Wait until people learn you can game on non x3d CPUs,Neutral
AMD,"Please do this, AMD.",Neutral
AMD,"if AMD doesn't capitalize on AM4's resurgence in popularity and the general reliance on DDR4, then Intel will, because LGA1700.  While it wasn't looking very impressive against the 5800X3D, the i9-12900K was still a contender and right now you can get an i9-12900KF for slightly less than an i5-14600KF. Obviously doesn't work for people who already have AM4, but people who are in the market for a cheaper platform than AM5 can get somewhere with LGA1700 and the performance ceiling is still higher than AM4 as long as AMD doesn't raise it.",Neutral
AMD,God I hope so,Positive
AMD,Plan on upgrading mine sometime in 2027. Don’t really see the need. Especially with all future games being made with handhelds in mind.,Neutral
AMD,still rocking with R7 5700X3D,Positive
AMD,"I check eBay and Facebook daily, it’s a miserable time",Neutral
AMD,If they did return it to production I would snatch one so fast. Looking to upgrade and with RAM prices jumping to AM5 doesn’t exactly look appealing,Negative
AMD,"I’ve been using that chip for a while now, it’s a great work horse",Positive
AMD,Please let this happen 🙏 when I finally went to get one the price had skyrocketed so I ended up buying a used 5950x because it was far cheaper.,Positive
AMD,Best purchase I ever made back in 2023,Positive
AMD,"5600 with 4080s, x3d tempts me",Neutral
AMD,"YES PLEASE. I’d upgrade in a heartbeat. Hell, try and revive that 5950X3D",Positive
AMD,My 3600 will have to truck on until well into AM6 probably 🫠,Neutral
AMD,The GOAT,Neutral
AMD,"I am kicking myself for not picking one up when they were readily available.   Would be the biggest boon for my rig, going from a 5600x to a 5800x3d",Neutral
AMD,Had a 5700x3d in my hands returned it for a 7600x    Worth it yea but at the time ram prices where good    Kinda wish I keep both I was expecting to upgrade my kids PC with a 5700x3d I doubt that will happen now,Positive
AMD,Here I am still rocking my amd 3950x happy as a clam with no plans to upgrade any time soon!,Positive
AMD,I remember knocking this chip when it came out because it wasn’t unlocked for overclocking and now I look like a doofus,Negative
AMD,Sad day when tech enthusiasts ask for almost obsolete tech due to current tech prices.,Negative
AMD,"Best CPU I've ever purchased by far. Paid 300, 3 years ago and there's still nothing it struggles with, not even a bit.",Positive
AMD,Would the effort to improve upon the Zen 3 chips (Zen 3+/Zen 4) be worth it over simply restarting the production of already existing designs like the 5800X3D? Or using a more advanced node like 6nm or 5nm?,Neutral
AMD,Yep would really like to buy one atm.,Positive
AMD,Got 5700x3d this year and it’s awesome. It’s essentially same speed as modern CPUs gaming at 4k,Positive
AMD,Was thinking of am5. But with all these BS. Ill rather stay am4 and get a 5800x3d,Neutral
AMD,"I upgraded from a 3600 to a 5800X3D that I got on sale, best upgrade ever. I then upgraded to the 9070XT, which I got for $30 over MSRP, so I will be skipping AM5 altogether, especially now since RAM pricing went insane. Fuck..., maybe I will die using this setup if the tech bros just keep fucking over consumers.",Positive
AMD,I fully expect the RAM market to return to normalcy by August,Neutral
AMD,"Very happy to have grabbed my 5700X3D when I did, upgraded my GPU to 5070Ti shortly after. Yesterday my APC Smart-UPS made a very loud pop sound and threw up an error code as my system immediately went dark. Smelling burning I scrambled for a power bar and all I could do was pray my computer didn't get fried because hitting PC Part Picker is NOT very fun these days!",Positive
AMD,Please just 5850X3d or maybe 5955X3d for us. I will even buy it original price,Positive
AMD,5950x3d would be fucking sick,Negative
AMD,*revive ddr4,Neutral
AMD,Hail the king baby.,Neutral
AMD,I'd buy two for my kids.,Neutral
AMD,I had to settle for the 5700x3d…. I actually swapped from a 5800x because I mostly do 90% gaming 10% everything else /sad,Negative
AMD,I would absolutely get one. I'd give my kid the 5600x3d,Positive
AMD,"I would get one. And if there are bundle deals with an AM4 mobo, I would too. Not that my current mobo can't handle it but I broke the USB 3.0 header on it and I can't use the front panel USB ports on my case. Stupid reason I know but well.",Negative
AMD,"5800X3D the GOAT, right after it is 5700X3D",Positive
AMD,My old 5800xed and my old R7 make a good steam machine.  In a small ITX box.,Positive
AMD,"I actually upgraded at the beginning of the year, from a 3700x to the 5800x3d, 16 to 32gb at around 65€ and got the 9070xt, don't regret it at all seeing what again happened, next step will be a monitor upgrade :)",Positive
AMD,"I have a 5900X, I would ‘downgrade’ my cpu to one of these and move my current chip to my server (currently an i5 6500)",Neutral
AMD,Or make somehow make the new processors/mbs work with DDR4 lol. That's the only way I'd upgrade. I'm not an engineer so I'm not sure if that's even possible.,Neutral
AMD,5**9**00X3D.  Dr. Su literally had one in her hands when she unveiled 3D V-Cache,Neutral
AMD,OMG please!,Neutral
AMD,seems gamers are not paying attention to market forces   AI makes more money to sell to,Neutral
AMD,Looks like we're going the distance buddy.,Neutral
AMD,But I just got a 5700x...,Neutral
AMD,"went from 3700x to 5800x3D 2 years ago, best investment ever  I am planning to keep this and skip AM5+DDR5 going directly to AM6+DDR6 :P",Positive
AMD,5700x3d would be cheaper for 10% less perf max. But yeah the idea is good!,Positive
AMD,I'd love to see a 5900X3D or 5950X3D.,Positive
AMD,"Release a 5950X3D with 3D cache on both dies, and I'd buy it.",Neutral
AMD,Be careful what you ask for. Seems like a perfect opportunity for AMD to come out scalping with it costing $600 or more.,Neutral
AMD,"Rocking 5800x non 3D, i won't replace it anytime took tbh",Neutral
AMD,"I just sold a 5800x3D for more than I bought it 3 years ago, which was also more than it cost me for the 9800x3D lol",Neutral
AMD,I bought a 5700x3d off aliexpress years ago for $120. Feelsgoodman,Positive
AMD,On paper it sounds like a good idea for them to spend some time backporting new archs to AM4 if this memory shortage will last a while. But I still doubt they have enough time to do that if they were to start today.,Neutral
AMD,"I regret not getting a 5800 or 5700x3d because I thought I’d have enough saved up to do a whole mobo, RAM, and CPU upgrade for my rig.  Waited too long then the RAM shortage hit and now I’m stuck with my 3700x for the next few years.",Negative
AMD,"I upgraded mine a while back, keeping it until AM6 at least",Neutral
AMD,"Been trying to find one, but $400+ for a used, 5 year old CPU from a random source on eBay is just a terrible deal. Fuck this bullshit.",Negative
AMD,"I don't see this happening, because the current memory shortages will not be fixed by switching back to DDR4. DDR4 production will be ending very soon and memory manufacturers will be more than happy to shift all production to DDR5.   The current memory shortage problem is a result of an artificial surge in demand while manufacturers remain cautious about ramping up production too hard, lest the bubble bursts and they have to drop prices through the floor. It's going to be a while before either side gives in.",Negative
AMD,"I would love that. By the time I got my savings in a better state, I only found a store selling the last of the stock at premium price. And switching to AM5 is too expensive for my taste. So yeah, it would get in line to buy one.",Positive
AMD,No 5800x better more ram performance gain if the produce them with higher quality they could all run 5ghz allcore and 2x16gb 4000 1:1 mode,Positive
AMD,"AM4 changed the game but none of us has predicted for how long yet! Went with a full custom watercooled AM4 platform since 2017 on 8xcores CPUs on a Crosshair VI Extreme paired with a GTX1080Ti and a 2x8GB kit of B-Die DDR4 (@3800C16currently)  : 1700X 4.1GHz / 2700X 4.2Ghz / 3800X 4.3 Ghz / 5800X3D Undervolted 4.45 Ghz... And with the addition of an RTX3080Ti recently, that 7 years-old AM4 platform feels like it's gonna keep taking it for years to come like a champ!",Positive
AMD,I hope they make more 5800x3d. They stopped by choice.,Neutral
AMD,Where can I get one nowdays? Seems sold out everywhere.,Neutral
AMD,"Yes please!!... Currently I'm still rocking a 5800x3d and have 2 other AM4 pc's waiting for an upgrade. At that time the original 5800x3d had a restriction on clocks and temps due to the 3d v-cache being on top... With the current 9800x3d using the v-cache at the bottom to take advantage of better coolers and temps, I would definitely buy at least 2 more and know some partners that would make the final upgrade for their long forgotten AM4 still rocking ryzen 3000 cpu's.",Positive
AMD,"5800x3d is starting to show it's age, but I'll run it till AM6 I reckon. Same with my 7900XTX. CPU bound on BF6 is kinda annoying. 5800x3d can't really deliver more than 120-130FPS in BF6, and makes me CPU bound which is annoying.",Negative
AMD,"5700x3D (on my Ryzen 1 cheap motherboard) and RX 9070, I'm ready for the years to come.",Positive
AMD,Which gamers and which reviewers exactly? 🤔,Neutral
AMD,yes please,Neutral
AMD,Releasing 5850X3D (with the compute die on top of 3D cache just like 9800X3D) would be better.,Neutral
AMD,One of the best buys ever for me,Positive
AMD,It's definitely the 1080 Ti of CPUs.,Neutral
AMD,300 well spend bucks 3 years ago...,Neutral
AMD,Yeah Id be willing to pay a pretty penny if they would be willing to sell it for a slight mark up.,Neutral
AMD,"Same, i'm skipping AM5 all together",Neutral
AMD,"Bring back the 5800X3D, please!",Neutral
AMD,"Made the jump from 1800x to 5800x3d, was incredible. Not sure how much the older x370 motherboard chipset is a limiter at this point.",Neutral
AMD,Picked up a brand new 5800X3d for $280.00 two years ago and sold my 5800X for $160.  Social media haters said it was a side grade and not an upgrade.  Look who is laughing now.  Ha ha ha.  My 5800X3d paired with my rtx 5070 ti is playing everything on Ultra and high with no bottlenecking.  Will probably be still doing the same for the next 3 years.  And I was also able to pick up 64Gb of G.Skill tridentZ (4X16) 16 18 18 39 for $124 in June this year.  So happy I purchased before this rampocalypse,Positive
AMD,"Upgraded my 3600X to 5800X3D kind of on a ($600) whim. I didn't really have any good reason to. I guess my logic at the time was to get the 'last, and best version' of AM4 and ride out this build. Other than paying full price for it, I've been happy with it. Can't say the same about the buggy Radeon driver experience however.",Neutral
AMD,"this is stupid af, i'm for progress and more performance my 9800x3d is light years better than my old 5800x3d",Negative
AMD,Damn I’d replace my 5900x so fast…,Negative
AMD,"I've had my 5800X3D for just over 3 years now. It's going to my wife and I'm upgrading to a 9800X3D. I bought 64gb of DDR5-6000 for $500 CDN less than it is today. Still more than at its lowest, but I knew I had to pull the trigger while I had the money.  Edit: Canadian dollars not US dollars",Neutral
AMD,"Well, instead of reviving the 5800X3D, isn't it just easier to make a DDR4 AM5 motherboard?",Neutral
AMD,Ditto.,Neutral
AMD,I will rock it forever  if they bring it back,Positive
AMD,"Mine died under warranty, after waiting 6 months for a replacement I gave up and had to settle with a 5700x. Pretty sad and annoyed by it as this was my plan too.",Negative
AMD,Same. Still going strong.,Positive
AMD,Plan to rock mine until it can no longer play MMO FFXIV. I'll leave to the Japanese to support the game on a GTX 970.,Neutral
AMD,5700x3d til ATLEAST Second gen AM6.,Neutral
AMD,"Yep, not gonna start considering an upgrade until long enough into AM6 to not be an early adopter.",Neutral
AMD,Same here. Getting in on the end of AM5 isn’t anywhere as appealing as the start of AM6. My next motherboard is gonna get a long time of use…,Negative
AMD,Same brother !,Neutral
AMD,Same!,Neutral
AMD,Yeppers,Neutral
AMD,When is AM6 expected?,Neutral
AMD,"I was thinking about getting a 9800x3d at some point but it's just not even worth it. At this point I have a 9070xt, 5800x3d, and 32gb of 3600 ddr4 ram. I mainly game at 4k, so I'll rock the 5800x3d for a while and just upgrade my GPU in like 2-3 years.",Negative
AMD,"I have a 5700x3d, 32 GB DDR4 4000, a 2 TB SSD, and a 4080 Super.  I’m well situated to outlast the AI boom, figure I should be able to get playable framerates on new games for the next 5 years.",Positive
AMD,Rocking mine til AM7,Positive
AMD,The chip makers are making so much money off of AI that they don't even want to serve the consumer market at all anymore because the margins are lower.,Negative
AMD,"Bought a 5700X3D for $120 USD brand new and sold my 3700x for $75. Such a solid upgrade. Paired with my 5080, I should get another few years on my system.",Positive
AMD,Likewise mate! Also switched out the 3700x for the 5800x3d. Also switched a 2080s out for the. 6800XT at the time.   The 9070XT has been cheaper here as compared to what I paid for the 6800xt. Still can't decide if it's worth it now or worth waiting it all out,Neutral
AMD,"Same!  I got it in a discount bundle from microcenter.  Like 350 for the chip, a new mobo and 32 gigs of ram.  Crazy amazing deal",Positive
AMD,"I don't even blame the RAM manufacturers **this** time.   I think the RAM manufacturers know AI is a bubble. It takes multiple years to build new manufacturing plants to expand production. Meanwhile, the AI bubble could pop in like 6 months.  I blame Sam Altman buying up all the RAM.",Negative
AMD,"I rotationally upgraded my current PC from the 1800x on a 370 platform, up to a 3800x & finally a 5800X 3D & 570, glad I got 32GB of RAM when I did, because the G.Skill modules and latency haven't been available for quite a long while.",Positive
AMD,Same. Got my 5800x3D when rumors were that it was a very limited run so paid £337 in January 2023. It’s nearly 3 years old and it’s been a beast. Was an upgrade from a 3600,Positive
AMD,"Likewise, still feel no need to upgrade from it.",Negative
AMD,I made the same upgrade last year. Amazing performance along with a 4070 Ti.,Positive
AMD,">got my 5800x3d years ago best purchase ever. I was ever so hesitant from coming from the 3700x cpu but the magic that was 3d cache can not be underestimated.  I made the same upgrade and I totally concur, going from a 3700X to a 5800X3D definitely felt like a bigger upgrade than going from an i5 4670k to a 3700X.  >now if we can just get a big amd gpu i wouldn't have to worry for a while.  As far as I'm concerned my 6900XT still delivers on 1440p on most games, but 16GB of VRAM is beginning to feel a little cramped for the newer games.",Positive
AMD,Exact same 3700x -> 5800x3d for me,Neutral
AMD,How much is the difference now ? And what gpu you have?,Neutral
AMD,"If anything prices on CPUs will drop thanks to lower sales due to increased memory prices, people will keep from updating for a while.  DDR4 production has been completely shut down since 6 months so AM4 parts availability will drop anyway and production plans has already been phased down in favor of other platforms.",Negative
AMD,"Switched to the x3d while using a 3700x as well. 330 at microcenter on the first black friday after its release and I was floored at how much more performance I got. Double the fps in wow was completely unexpected, spikes in fortnite were nowhere to be found.",Positive
AMD,Why would ram manufacturers scale for artificial demand tied to an industry everyone knows is a bubble,Negative
AMD,"We just need to wait for the fabs to catchup on production and eventually there will be some available for consumers and bring prices back to more reasonable levels. This ai bubble popping will just hurt gamers and consumers because it's basically fueling future technology development and advancement. If we hold through the expensive price hikes, we'll benefit from rapid growth in the end.",Neutral
AMD,"I kinda regret my purchase of the 5800X3D. The 5700X3D got announced within literal weeks for hundreds of $ less for like \~3-5% less performance. Couldn't even return it as I was not in the country at the time, and by the time I came back I was well beyond the return window.",Negative
AMD,Also went from 3700X to 5800x3d early on. The difference was significant even with a 1080Ti at 1440p.  Eventually I swapped it for a 6800XT and now an 9070XT since release. Works great.,Positive
AMD,"Managed to buy a new 5700x3d , the last one , few months ago.",Positive
AMD,"It takes time to scale up, guaranteed they're already in the process of doing so",Neutral
AMD,The scarcity isn’t artificial at all.  The memory makers found buyers willing to spend far more than those that make consumer memory.,Neutral
AMD,"> I don’t think any AM4 x3D silicon has rolled off the assembly line since last September.  SKUs haven't, but AMD extended Milan availability to 2026, which means the silicon is there.",Neutral
AMD,"The beauty of it is they wouldn't need to do a Rocketlake style backport, just put newer chiplets alongside the older IO die on the package.",Neutral
AMD,Almost definitely easier to release an AM5 motherboard that supports DDR4 than a newer Zen on AM4.,Positive
AMD,"Bartlett Lake is allegedly for the embedded market only, no?",Neutral
AMD,"Even if next years market is catastrophic, I just dont see how spinning up an entirely new hybrid SKU (So eg Zen 4 or 5 but with DDR4 Support) would make any economical sense when looking at the cost and also the fact theyre already 2 Gens in on AM5.",Negative
AMD,"I'm up to 4 AM4 machines in my house now.   3600x, 3700x, 3900x, and my 5800x3d.  I'll be running AM4 for probably the next 10 years until I retire the last of them   Edit:  also have a laptop with a 5600h, but that's not Am4",Neutral
AMD,"Oh man, I fucking wish. I’d be all over it.",Negative
AMD,THIS!!!  I really enjoy my 5950x but it could use a little more oomph in the gaming department.,Positive
AMD,Pfft just have them make a 5950X3DX edition with a 32GB HBM MALL cache with DDR4.     Problem solved!,Neutral
AMD,"It was in the labs, Lisa Su even showed a prototype at Computex 2021 lol.",Negative
AMD,"They actually did...but it was just an engineering sample that never made it to market 😞. They maxed it out with 3DV-cache for each CCD, too.  https://youtu.be/RTA3Ls-WAcw?si=\_6NcQQG798fTH8aR&t=1072]  How awesome would that be tho, if they finally crowned the AM4 platform with a 5950X3D (with the cache on both CCDs, flipped in the right position this time for higher clocks, and windows cache optimizer chipset drivers).  Do it AMD!!!",Positive
AMD,Yes  And maybe give us 2 x3d dies,Neutral
AMD,If they did that it might straight up cannibalize themselves for how too good it would be,Neutral
AMD,I’ve been waiting for this since X3D has been a thing.  I will unironically pay $400 or more for that chip so I don’t have to upgrade my entire PC when my 5900X is no longer cutting it. Getting an extra 3 years or so would be worth it.,Positive
AMD,Gimmi.,Neutral
AMD,Or bring back dual CPU motherboards to the DIY market,Neutral
AMD,"Did you check the new 9950x3d-2?... It has v-cache over each CCD, instead of only one of them... A 5950x3d-2 would be the ultimate and final upgrade for an AM4 setup... I will definitely do the jump to AM5 when RAM prices get closer to normal next year and with ryzen 10000 jump in performance.",Positive
AMD,They don't make them anymore and they're one of the best DDR4 cpu's available. I bought a 5700x3d at the beginning of the year and it had already risen to $250. I imagine it's higher than that now.,Positive
AMD,Man i would sell my 5800x3d now if it's fucking $400 so I can upgrade but ram prices are ridiculous. Maybe I'll wait til AM7 now looking at hardware prices... Was previously gonna do AM6 upgrade.,Negative
AMD,RAM price is expected to come down in 2028.,Neutral
AMD,"> stay affordable  lmao, even bottom of the barrel ddr4 is like triple the price it was a few months ago.",Neutral
AMD,Got a 5700x3d on cyber Monday from AliExpress and they’ve shot up since. Can’t wait to slot it in to replace my 2600. My 1080ti will be happy,Positive
AMD,AM4 Boards a true troopers.,Neutral
AMD,"The reason the 5700x3d released so late, compared to the 5800x3d, is that it's just 5800x3ds that didn't make the cut in clockspeed.  I got one towards the end of last year for a great price, and the performance difference is usually a little bit more than 2%, but it was literally less than half the cost of a 5800x3d.",Neutral
AMD,Same as me. I bought 128GB RAM but two of the sticks stopped working. 64 is enough though.,Neutral
AMD,Thankful about being two generations behind?,Neutral
AMD,They will or they will starve for sales and their Mobo partners will as well.,Neutral
AMD,I noticed a difference going from a 5600x to 5800x3d. Mostly in the 1% lows.,Neutral
AMD,Unless you're playing cpu bound games it's probably not worth it. If you can get it for cheap then sure.,Negative
AMD,"Yes, if you can find it for a reasonable price. I went from a 5600x to a 5700x3d and it was worth it to me, but I paid like $130 on AliExpress a bit over a year ago.",Positive
AMD,DDR4 production is over. This product would only be for people already on the AM4 platform.,Neutral
AMD,"The AM5 Chips only have ram controllers for DDR5. You could make a motherboard with DDR4 slots but I seriously doubt it would work. Before, the ram controller was on the motherboard but now it's inside the CPU itself so we'd just be back to square one",Negative
AMD,"Not going to happen, Zen 3 X3D Production ceased like a Year ago and every x3d product based on Zen3 sold are just downbinned 5800X3Ds that failed QA Testing. Even if they now decided to buy both 7nm production capacity and Front side Hybrid Bonding it would take at least a year until you would be able to buy new 5800X3Ds, and all of which would be pretty expensive for what to them is a last last gen niche product. I mean yeah it would be pretty nice but economically this wouldnt make any sense even if they tried to be nice.",Negative
AMD,How’d it die?,Neutral
AMD,If this happens to me I'll probably just sell the rest of my PC at this point lol,Negative
AMD,I just got my hands on GTX 970 to replace 660Ti on my second scrap PC and it's surprisingly capable card running Valheim at 1080p max settings,Positive
AMD,"Same here, upgraded my 5600x to a 5700x3D before they became expensive AF. Upgraded my 6800 to a 9070XT a few weeks ago, initially wanted to do a full upgrade somewhere in early '26, but at the moment I can play everything on ultra with 150+ and beyond fps on 1440p, so I'm good for at least 3-4 years, I reckon.",Neutral
AMD,"I bought in 2020, at what I thought was the end of AM4–but I had the money (and time, lol) to upgrade. I got a 5600x figuring maybe down the road I could get a 5900x or 5950x if I wanted something faster. Instead, I got a 5700x3D cheap last year which has been a nice upgrade!  I’m hoping AM6 has more cores per CCD, I’ll probably start with an x600 and then upgrade to x3d on the 2nd or 3rd gen.",Positive
AMD,Not anytime soon. 28 or 29 or something .,Neutral
AMD,I have a 5800x3d and 4090.  I built a newer pc with a 9800x3d and 5090.  Performance feels incredibly similar.,Neutral
AMD,It would use fabs that aren't currently being used to make AI chips (for the most part I would assume) so it would be more additional revenue than one eating into the other.,Neutral
AMD,greedy bas\*\*\*\*s. But nothing new,Neutral
AMD,"For AMD their AI gross margins are actually below their corporate average. Meanwhile, gaming CPUs for AMD have always been above their corporate average margin.   There's also nothing that stops AMD from doing this. There's no conflict of wafer supply or other packaging shortages that overlap with AI. The hardest part might just be trying to start back up those old packaging lines and old wafer starts. It could be pretty difficult to do it. And as soon as they could have any products out would be in 6 months which would probably still be worth it. But who knows.",Neutral
AMD,yeah i came off a 5600x for a 5700x3d and paired with a 9070xt its really a solid combo. I got so lucky doing a minor refresh on my pc earlier this year.,Positive
AMD,"I swapped my 2700x for 5700X3D and Vega 56 for RX 6800 XT, works wonders :3",Positive
AMD,I bought my 5700X a year ago for 140€ new. I hate the market.,Negative
AMD,Same cpu paired with my 5070 ti. How much longer will this config last?,Neutral
AMD,"If you can afford the 9070xt now, do it and sell your 6800.  Shits only going to get more expensive the more you wait and it’s a solid card.",Negative
AMD,"I upgraded my CPU 3 years ago and my GPU 2 years ago. Went from 3700X to 5800X3D and RX 5700XT to RX 7900XTX. Managed to sell my old CPU, GPU and some other stuff and got enough to upgrade my RAM few months ago. I started doing a lot of 3D work and game dev stuff, so 16GB wasn't cutting it, got an upgrade to 64GB for €250. Seeing the current shit show, im so glad I made the jump to 64GB",Neutral
AMD,Supposed leaks put it at 2028 before prices come down due to backlog demand even with increased production.,Neutral
AMD,Why scale up? If the bubble pops. They would be stuck holding the bag and ram would drop even more than if it just pops with less equipment,Negative
AMD,"Except it is artificial.  Both Samsung and SK Hynix have just come out and said it lol  https://wccftech.com/two-of-the-biggest-dram-suppliers-are-skeptical-about-increasing-production/  This is all due to OpenAI buying insane amounts of ram production to keep others from getting it, artificially inflating prices.",Negative
AMD,"The extension was over 2 years ago already with availability up until 2026 theoretically only for certain CPUs. AMD likely ended manufacturing this year.  The peak run for TSMC N7 was 2019 - 2021, with most capacity dropping off by 2023. This is a 9 year old node now.  By now TSMC N7 is a small shadow of what it was with most remaining capacity bought by 2nd tier chip designers like Huawei.",Neutral
AMD,It’s a bit of a mystery. Probably should have been released by now but they haven’t officially cancelled it,Neutral
AMD,"Kinda same, as I'm now on AM5 but my bf inherited my 3900X and my server runs a 1600X lol",Neutral
AMD,"Got 5950X, 3800XT, 2700X and 3200G systems in our home. And my old 1700X laying in a box.   I just today was looking for 5800X3Ds to replace my sister's 2700X and living room 3800XT. Guess I will just get some 5800Xs instead, as the X3Ds on eBay are over 400€ while the 5800X is everywhere for around 160€ new... The 3200G will also be replaced with a 5700G soon.",Neutral
AMD,"Right there with you. I kept buying all of my co-workers ""old"" PC parts for my home servers and wife and kids machines. Up to 6 AM4 machines including mine.",Neutral
AMD,"3 here with 5800x, 5700g, 1600x in systems and a 2400g laying around.",Neutral
AMD,"I tried switching out my 5950x with a 5800X3D for a little bit. The difference in gaming wasn't all that noticable for me and the games I play. I ended up giving the 5800X3D to my nephew and going back to the 5950X. I need all the core for blender, code compilation and other multi-threaded workstation stuff, so I found the 5800X3D too slow for embarassingly parallel workloads.      Guess I'm waiting out the next few years before it makes any sense to upgrade. If I were to buy anything it would be the 395+ AI MAX.",Neutral
AMD,Process lasoo,Neutral
AMD,BLACK EDITION,Neutral
AMD,I wonder if anyone watched that video... He said they did the 7950x on AM4 before they made the 5800x3d... https://youtu.be/RTA3Ls-WAcw?t=1101... I would guess that is the 7950x3d... Which means they can just duplicate the 7950X3D on AM4 using the 7000 architecture.,Neutral
AMD,That would hurt productivity while gaining nothing for gaming.,Negative
AMD,"If it is too expensive to upgrade AMD will starve... They can support newer features on new MOBOs for the AM4 line while pushing newer CPUs for the AM4 line. Let people reuse the DDR4. Helps customers, helps mobo manu and helps customers. Win/Win/Win in an otherwise screwed situation that is expected to last until 2028 at the earliest.",Neutral
AMD,That doesn't make any sense when there are 16 core consumer CPUs. Most people don't need that much and anyone who needs more can get threadripper or epyc.,Negative
AMD,For gaming latency would get worse. Trust me we've already had problems with cross CPU memory talk in big data apps. Gaming would be much worse.,Negative
AMD,I definitely am not used to my hardware appreciating in value.,Negative
AMD,Yea they’re like $330 now. I’m just rocking a 5600x. Not much point upgrading to non-x3d chip and the x3ds are too expensive =/,Negative
AMD,Check ebay. Nothing goes for under $400 without damage like bent pins.,Neutral
AMD,By whose estimate?,Neutral
AMD,"Nobody knows when the prices will come down. They are expected to NOT come down in 2026 because that DRAM manufacturing capacity is already sold out.  When prices come down depends on when demand growth from AI slows down. Some people think it's a bubble and will soon pop. I don't think it's a bubble. RAM, electricity and permits seem to be the big bottlenecks for data centers. Financing could become an issue if the markets turn bearish on AI.",Negative
AMD,"I fully expect it to come down by August next year, as the AI bubble starts bursting",Neutral
AMD,You upgraded your CPU before that old ass GPU?,Negative
AMD,"You should check the warranty on your busted sticks--a lot of manufacturers give a lifetime warranty.  I had two sticks die, and they were replaced under warranty...but I also only had 2 RAM sticks, so I had to shell out for replacement RAM while I waited for them to come back, which was annoying--but it beat just not using my PC, lol.",Negative
AMD,Yeah because I would have needed to build an entirely new system to upgrade past a 5800x3d dipshit,Neutral
AMD,Couldn't they just create a conversion chip within the motherboard to make the CPU think it's DDR5 ram?  There might be a minute drop in speed from the conversion but it's better than paying half a grand in Ram.,Neutral
AMD,I had two 9950x die on me for no reason. Just happened I guess,Negative
AMD,Just went to turn on the pc one morning and it wouldn’t boot,Negative
AMD,I had two 9950x die on me for no reason. Just happened I guess,Negative
AMD,Same here with the 5600x to 5700x3d upgrade when it was cheap on alibaba last year. So glad that I made that upgrade.,Positive
AMD,"That's what I was worried about if I did make the jump, glad to hear I'm not super crazy",Neutral
AMD,"You can't easily refit a fab for an entirely new and very different process node. You basically would prefer to build a whole new one. In particular, the most advanced chips have very specialized processes / devices, EUV litho and Gate all around FETs.",Neutral
AMD,It’s the same fabs tho.,Neutral
AMD,When I saw it at that cheap of a price I bought 3 of them and gifted them to people that were on am4 still in my friendsgroup. I knew it was too good to pass up,Positive
AMD,At 1440p? 4-5 years at decent settings probably!,Neutral
AMD,Or keep the 6800 as a backup. Who knows what availability looks like for the next 12 months?,Neutral
AMD,"Money.  Just because the bubble pops doesn't mean the technology will be going away. Homes and the internet still exist.  Samsung, micron, sk hynix have all already reportedly begun to scale up and it's going to take years. Probably be 2029-2030 before these new production lines are ready.",Neutral
AMD,I can see industry wanting to use their existing DDR4 and upgrade to high core count EPYC 7003 high core count SKUs. That might be incentive enough for AMD to restart production.,Neutral
AMD,My server is the 3900x.   I picked it up from Facebook marketplace.  Came with a x570 motherboard and 64gb of ram for $250.   Couldn't pass it up.     It replaced my old Fm2+ motherboard with an athlon X4 860k,Positive
AMD,Thanks for that. I got the 5950x when I thought it was gonna be the halo chip. This helps allay my fomo.,Positive
AMD,I went 9800x3d and threw my 5950x in another PC and the difference in gaming can be massive but it depends on the game and how CPU dependent it is. The heavier ones are going to see the biggest differences. 5800x3d was basically on par with 1st gen AM5 non x3d stuff.,Neutral
AMD,I use that when I game and stream simultaneously.  It's why I bought the 5950x.  Like two PCs in one.  :D,Negative
AMD,Depends on the game and depends on the productivity  There are reasons to want a 16 core 3d vcache CPU,Neutral
AMD,"Youre ignoring the fact that backporting newer Zen Architectures to work an AM4 and support DDR4 would require completely redesigned silicon for a product that would cannibalize their newer chips, for an publicly traded company, this just doesnt make any economical sense.",Negative
AMD,Yea its a weird feeling. My ram went from 200 to 700,Negative
AMD,"If you haven't been paying attention, EVERYONE in the industry.  https://tech4gamers.com/memory-shortage-till-2028/  https://www.tomshardware.com/pc-components/dram/the-ram-pricing-crisis-has-only-just-started-team-group-gm-warns-says-problem-will-get-worse-in-2026-as-dram-and-nand-prices-double-in-one-month  Mircon wouldn't leave the consumer market if it was just a flash in the pan..  It's just price fixing at it's finest..  https://www.tomshardware.com/pc-components/dram/memory-makers-have-no-plans-to-increase-production-despite-crushing-ram-shortages-modest-2026-increase-predicted-as-dram-makers-hedge-their-ai-bets",Neutral
AMD,Mine!,Neutral
AMD,Yes it is old ass but it still kicks ass. The cpu and 32gigs ram will keep it running. Next upgrade will be gpu when i see the need,Positive
AMD,"Thanks. Maybe I can get them replaced on warranty, sell them and retire early :)",Positive
AMD,chill down pal lmao,Neutral
AMD,I really don't think that's how it works. Ram is electrically incompatible between generations. The fastest DDR4 is also slower than base DDR5 meaning that it probably couldn't run at all (3600MT/s for DDR4 highest VS 4800MT/s for DDR5 lowest),Negative
AMD,"I had my first 5800x3d arrive DOA, Amazon thankfully replaced it for me after a 5 min chat",Positive
AMD,Did you upgrade the Bios? Apparently MOBOs are overvolting.,Neutral
AMD,"At first I wasn't sure if it was a big enough upgrade, but it certainly was. Again, good for at least 4 more years.",Positive
AMD,"Yeah I made a machine to play PoE in 4K in 2024 summer with a 7900XT and an 5700X3D. It delivers. And it'll deliver for quite some time. Although I am upgrading the video card in a roundabout way: I will move from Europe to Canada next summer, I bought a 9070XT in Canada during Black Friday and sent it to a friend, will sell the 7900XT locally next summer and that's how I will get a practically free upgrade. I will take my CPU, RAM and SSD with me -- small, lightweight and by now super valuable.",Positive
AMD,"5800x3d are 7nm, those fabs aren't pumping out bleeding edge AI chips.   There is still use of those fabs for other purposes, but I hardly see them at capacity",Neutral
AMD,"I wish I had friends like you 😭  That CPU here was like double the price I spent on the normal one, and I had to save for RX9060XT so...  It is what it is.",Neutral
AMD,Much like how I kept the 2080s as a backup.   If the 9070XT is going to fail within a single year it's not a good buy though.,Negative
AMD,Maybe vene hopefully update them as well while they are at it?,Neutral
AMD,Nice! Daddy has the 9950x3d and it is nice having so many threads for sure,Positive
AMD,"The ccd to ccd latency kills any benefit for gaming.   Some games doesnt even benefit from the extra cache and would rather run on the non 3d cache ccd.  And because zen3 has the cache on top of the ccd, there is a frequency penalty which hurts alot of productivity. There are ofc some that can use the extra cache, and in that case it would benefit. But that starts to get very niche on a desktop cpu, and gives up performance in most other use cases.",Neutral
AMD,"You are ignoring the fact they already mated the 7950X to am AM4 platform before they released the 5800x3d. It was an engineering sample.;)  [https://youtu.be/RTA3Ls-WAcw?t=1101](https://youtu.be/RTA3Ls-WAcw?t=1101)  At least the AMD Engineers claimed they did it. But what do I know, I am ignoring facts. I am also ignoring the fact that the 7000 series chips are still being produced.  ""Key Details from AMD Engineers  During a visit by Gamers Nexus to AMD’s testing labs, engineers clarified several points about these ""hybrid"" samples:  * **Internal Proof of Concept:** AMD engineers explicitly stated they had the  **Ryzen 9 7950X**  (Zen 4) running on **AM4 boards** internally. This was done primarily to test the Zen 4 compute dies (CCDs) and architecture using the existing, mature AM4 infrastructure before the AM5 platform was fully ready. * **The ""Hybrid"" Design:** To make this work, engineers likely paired the **Zen 4 CCDs** (5nm) with a compatible **Zen 3-era I/O die** (12nm) that supported DDR4 memory and the AM4 socket. * **Decision to Cancel:** Despite having functional 16-core Zen 4 samples on AM4, AMD chose not to bring them to market. They instead focused on the  **Ryzen 7 5800X3D**  as the final high-performance gaming upgrade for the AM4 platform because the 3D V-Cache provided a more significant gaming benefit on the older socket than a core-count increase."" * [https://www.youtube.com/watch?v=iM1NXHQ8YTA&t=2s](https://www.youtube.com/watch?v=iM1NXHQ8YTA&t=2s)",Neutral
AMD,"The shortage lasting until 2028 is probably the best case scenario here, unless the bottom falls out of the AI market entirely. Chip manufacturers might be able to catch up with current demand by 2028, but as long as the AI companies have effectively infinite money to throw around they can just keep buying up the new production capacity",Neutral
AMD,"And that's assuming OpenAI doesnt buy even more, or their competitors don't scoop up more to make sure they have enough.  The only thing that's fixes this is the AI bubble popping.",Negative
AMD,There are people who went to jail for the last price fixing scandal. I doubt anyone in power has forgotten. It was a pretty big deal,Negative
AMD,"TSMC upgraded several of the 7nm fabs to 4nm tho, so those production lines don't exist anymore.  Edit: and they are converting more currently: https://www.tomshardware.com/tech-industry/semiconductors/tmsc-ponders-upgrading-2nd-japan-fab-to-4nm-could-pave-the-way-for-more-advanced-chips-for-japanese-customers",Neutral
AMD,x3D production is not limited by lithography process but packaging which is a bottleneck right now and will be for at least several years.,Neutral
AMD,Anything capable of 7nm is probably capable of manufacturing ddr5. Ddr5 is 10-12nm lithography  I imagine it's backwards compatible and lower nm manufacturing is capable of producing higher nm components still.,Neutral
AMD,"I'm just paranoid about my tech failing when replacements aren't readily available. Also, any random piece of tech can fail regardless of the overall quality of the product line.",Negative
AMD,"Yes. We all are aware of that  Keep in mind, there are reasons why some epics have 3d vcache. Simulations and AI aren't ""very niche"" on desktop. They're not common, but it would be nice to have the option.  I don't know why anything optimized outside of gaming and streaming gets so many of you guys bothered when people say they want it.  Also, there are even games that want the cores AND the cache",Neutral
AMD,"First of all, please spare me your AI written SumUps, second of all there is still a difference between prrof of concept and production silicon. You can bet this combination was probably full of architecture bugs. Like they literally said in the snippet you send that they didnt go through with that for a variety of reasons, not only the narrow statement of the 5800x3d being better like you said.",Negative
AMD,There is a possibility of the bubble popping on AI.,Neutral
AMD,"We know it's nothing but bullshit. they knew about these contracts for quite some time, but still slowed production because of ""oversupply"".   Anyone thinking otherwise doesn't have a functioning brain.",Negative
AMD,"Doesn't matter as the current administration in the USA is letting them do whatever they want. If we want any action, the EU will need to bring them to justice.",Neutral
AMD,"thats not how it works. the companies making DDR5 are SK Hynix, Samsung, and Micron (and a bunch of smaller players). TSMC's fabs aren't designed to produce DRAM or NAND, just more specialized ones like MRAM or RRAM",Neutral
AMD,> I imagine it's backwards compatible   Might want to look into things instead of using your imagination when facts are involved. Too much nonsense being spread online as if it's reality because people assume. Imagination should be left for creating things.,Negative
AMD,"No it isn't. TSMC 7nm afaik is an entirely new node and not a refresh thus it will need a redesign instead of just use the same design that they use for 10nm.  Also you typically don't use 7nm or lower for RAM since there is a lot less benefit vs making it on the bigger nodes. Basically RAM doesn't scale as well as logic (the density improvement for making RAM on smaller node is not great) thus why you don't see companies making DRAM on 7nm.  So when if someone want to make RAM on 7nm, they need to design it first and do all of the testing and validation process and then potentially have a product that is a lot more expensive. I will say that with the current price and assuming this kind of pricing last long, it might be viable (as in not losing money) to sell RAM that is made in 7nm process but still doing that would only add very little available RAM to the pool and the product itself is not going to be better.",Negative
AMD,"You’re imagining wrong, ram manufacturing uses completely different processes and TSMC doesn’t produce ram at all, it’s made in specialized factories by different manufacturers.  Samsung is the only manufacturer who has both logic foundry processes and memory manufacturing but in different fabs.",Neutral
AMD,Confidently incorrect,Neutral
AMD,Wat,Neutral
AMD,Cores AND the cache is exactly what you get with 5950x3d...  Which games btw?,Neutral
AMD,"I knew odds were high you would be too lazy to watch the video and the AI summarizing the video would prove I was not pulling it from my arse. But I think the real problem is it pissed on your oversized ego.   ""(W)ould require completely redesigned silicon"" is the problem with your claim. I was proving you were wrong. It does not need to be ""completely"" redesigned, but repurposed and bug stomped. Maybe figure out the meaning of the words you are using?  You also did not bother addressing the fact in my original statement that there is nothing to cannibalize if people cannot afford Ram for it. ""If it is too expensive to upgrade AMD will starve...""  The people with money that can afford the expensive DDR5 setups will benefit from those and the performance they offer while the potential customers will be able to afford AM4 setups that cannibalize old DDR4 builds. Think Ryzen 1000 series, Ryzen 2000 series and Ryzen 3000 and even Ryzen 5000 series that wanted upgrades but were waiting for a little longer and now cannot afford them because of DDR5 shooting up over 300%. Well they cannot afford to upgrade anymore, but they want to. This would give them a reasonable path. Think of the $100 steak vs the $500 cheeseburger. The $100 steak is reasonable and logical  when compared to the $500 cheeseburger.  It also builds customer loyalty and customer good will.  I really hate explaining logic to arrogant people that have proven there is nothing to be proud of. AS it stands I am probably not breaking it down small enough and I am sorry for that, but I am only willing to do so much. I hope you have a good night and please ponder on what I am saying without flying off the handle or accusing others of ignoring facts when they are things that are not even facts but illogical opinions you have.",Negative
AMD,This.  If the bubble pops before 2028 prices will normalize faster.,Neutral
AMD,If the AI bubble pops the US economy is screwed seeing how it's driven by tech companies as the majority of it's valuation,Neutral
AMD,If the ai bubble pops we’re going to have bigger problems then buying ram. GPD growth is like .2% when you don’t count AI investments.,Negative
AMD,"In wich scenario AI bubble ""bursts""? all of a sudden millions and millions of people simply stop using it?  AI is here to stay, prices will level down, but it is going to be gradual.",Negative
AMD,"> TSMC's fabs aren't designed to produce DRAM or NAND  DRAM is much simpler than what TSMC's fabs are designed for.    I think the gotcha would be the economics. DRAM might not necessarily benefit from TSMC's 7nm process very much, and it could be significantly more expensive to manufacture.   Those 10-12nm manufacturing processes used for DRAM don't need to be as advanced, but probably do need higher volume, and would therefore be more optimised for the cost of production over having the smallest features, or the highest performance.",Neutral
AMD,"I'm no expert, but I'm not spewing nonsense.  Perhaps you should take your own advice.  [This is basically confirming what I've said. New nodes have some amount of backwards compatibility built in to enable old designs to be migrated to new nodes more easily.](https://i.imgur.com/Q5zXzXp.png)",Neutral
AMD,"SRAM (used for cache on logic processes) is the least scaling part of a logic focused process that’s correct, but Dram doesn’t use SRAM its much simpler but slower and still scales down rather well.  DRAM manufacturing however is quite different from logic manufacturing and focuses on another set of characteristics.",Neutral
AMD,maybe ram prices will crash then :D,Negative
AMD,"""US economy"" is a bit more complex than market indexes. The money will just distribute to other industries",Neutral
AMD,"oh no the rich people number that doesn't actually go to anyone not in the top 5% will go down, how horrible",Negative
AMD,"It's worse than that, it's 0.2% when you exclude only data centers, not even the full AI bubble.  So even just stopping the production of data centers would send the US in an atrocious recession.",Negative
AMD,the money will just move from AI to gold/bonds to other stocks allowing better industry/market development outside AI,Neutral
AMD,"Which is still not how this works, producing dram is a completely different beast from producing CPU/GPU/whatever chips that actively compute.",Negative
AMD,Using AI summaries isnt reliable. It told me that Amari Cooper didn't retire and pulled Facebook articles as reference while linking an ESPN article where Amari Cooper stated he was retiring. It said there was no reliable source. Those AI summaries pull bad information and make shit up all of the time,Negative
AMD,"Holy crap, I'm so embarrassed for you right now.",Negative
AMD,Your source is AI generated text? For real? WTF,Negative
AMD,"Yes, because the crash of a critical sector has never led to a catastrophic recession before. Oh wait, that’s exactly what happened in 2008 with the housing market crash genius.",Negative
AMD,"Except those rich poeple/companies employ everyone else. And what do yoit think all those mutual funds, pension funds, 401ks are investing in right now?  It will be worse than 2001 and 2008.",Negative
AMD,"Google's AI is also so insanely bad, feels like all it does is hallucinate.",Negative
AMD,"AI is not critical to anything, housing and (""too big to fail"") banking is",Neutral
AMD,"I'm OK with it being worse than 2001 or 2008. It's not like the boom economy right now is helping anyone - rents are at all time highs, food costs too much, people are still getting laid off regardless anyway, and we can't even afford distractions like PC gaming because they're stealing all the wafers for Sam Altman's ego",Negative
AMD,"I wish this was a truly low power version, 75w Max. Then we could have slot powered, single slot/short but single fan or low profile cards.  I still hold hope that AMD will make whatever a modern RX6400 would be, RX9400? LP and slot powered options are few and far between, a compelling 12+ gigabyte option would be a winner in that market.",Neutral
AMD,"I hope this increases the chance of having a low profile design if it ever releases outside china. Would be enticing for <10L case builds to have a small card with 16GB VRAM but I am not seeing any partner AIBs, I can see first from Gigabyte and Yeston that can provide smaller low profile designs for AMD cards.",Neutral
AMD,"> The standard RX 9060 XT is rated at **180W** TBP and requires a 450W minimum power supply.  False. RX 9060 XT is rated at **160W** TBP: [https://www.amd.com/en/products/graphics/desktops/radeon/9000-series/amd-radeon-rx-9060xt.html](https://www.amd.com/en/products/graphics/desktops/radeon/9000-series/amd-radeon-rx-9060xt.html)  >Chinese media reports suggest a 140W TBP  The source link that was provided confirms ""up to **140W**"" TBP: [https://www.amd.com/en/products/graphics/desktops/radeon/9000-series/amd-radeon-rx-9060xt-lp.html](https://www.amd.com/en/products/graphics/desktops/radeon/9000-series/amd-radeon-rx-9060xt-lp.html)",Neutral
AMD,"amd should release a 9400 and 9500.  sure nobody wants to see just 8gb, but it would be fine for this use case.  9400:1 slot low profile slim card like the old 6400, no added power connectors. 9500:2 slot low profile card, extra power connector but overall low tdp for thermals.",Neutral
AMD,Gamers can rest assured that at least from AI it won't have demand lol,Neutral
AMD,"so more or less a factory UV 9060XT, guess they have produced enough silicon and can use the better one with better UV capability here.  If that's the case, these cards would actually be best fit for a BIOS mod that unlock the power limit.",Neutral
AMD,Will this have 3 DisplayPort ports? Or just 2?,Neutral
AMD,I love low power consumption with same performance,Positive
AMD,"So they had some silicon not quite up to par to hit advertised boost clocks, but they didn't want to cut it down to RX 9060 (non XT)",Neutral
AMD,How about a laptop GPU 😡,Neutral
AMD,"I never understood product/price segmentation by AMD, Intel - and all the crap that the system builders bring on us.  My question - I hope someone better informed will explain - doesn't all these add to operational complexity, costs, etc.   Especially for AMD with a binning strategy - why add these complexities, while the system builders never seem to prioritize AMD products",Negative
AMD,Honestly that's impressive uses much less than my 6750xt lol.  I'm just using a 550w PSU for my setup.,Positive
AMD,"Did anyone even ask for this? My 9060XT really doesn't want to touch 180W even with the power limit maxed out, temperatures are more than controlled and any half decent PSU can handle it",Negative
AMD,What? I already have the 9060XT,Neutral
AMD,"Damnit, Chinese market only again.",Neutral
AMD,Cool. It's about time.,Positive
AMD,9050 XT somewhwere down the line once most of the binning has been done,Neutral
AMD,Yeah would love to have a RDNA4 single slot GPU for a tiny Bazzite build.,Positive
AMD,"I honestly think AMD should release the RX 7400 to consumers, it's 43W & will work well for SFF systems as it can just be a single slot card a-la the GT 1030/RX 6400.",Positive
AMD,"At least a 5060 lp competitor, 75w looks a bit constraining for what most people would want",Neutral
AMD,"well, you can limit the power yourself if that is the interesting part. i'm running my 9060XT with a 40W limit right now as that's enough for the game i'm playing. though i have the 8GB version, so the power consumption is likely a bit lower. most of the time the fans don't spin when gaming.",Neutral
AMD,I’m getting decent 1080p performance from a B50 pro. Not good. But decent. I don’t normally game on it but I had to try.,Positive
AMD,"[9060 XT LP](https://www.amd.com/en/products/graphics/desktops/radeon/9000-series/amd-radeon-rx-9060xt-lp.html) will be an international release because the  product page is available on the English page for AMD, while all China-only products are listed just on the Chinese page (like the 9070 GRE, 7650 GRE,  6750 GRE 12GB,  6750 GRE 10GB)  English: [https://www.amd.com/en/products/specifications/graphics.html](https://www.amd.com/en/products/specifications/graphics.html)   Chinese: [https://www.amd.com/zh-cn/products/specifications/graphics.html](https://www.amd.com/zh-cn/products/specifications/graphics.html)",Neutral
AMD,or plopping in SFF office PC builds. ...or maybe i just get Gabe Cube this time.,Neutral
AMD,"Very helpful, thanks",Positive
AMD,Anything wrong with gigabyte? Their builds for 30 series cards which were notoriously power hungry and spike heavy were well prepared for the task. Don't see why they would be bad for lp cards.,Negative
AMD,"If Yeston is able to, I wouldn't be surprised to see a [Cute Pet](https://yestonstore.com/collections/cute-pet) 9060",Neutral
AMD,">False. RX 9060 XT is rated at **160W** TBP  To provide more context for anyone reading this, the partner models can have three different TBPs, 160, 170, 182, some information on which card uses which is provided in this google sheet: [https://docs.google.com/spreadsheets/d/111K2xkO2-ExNq8NgQsPULDEShBoBqFf-9WQFeorjFHY/edit?gid=755628141#gid=755628141](https://docs.google.com/spreadsheets/d/111K2xkO2-ExNq8NgQsPULDEShBoBqFf-9WQFeorjFHY/edit?gid=755628141#gid=755628141) so it's both right and wrong depending on which model is being talked about since AMD has no 'reference' board/card. Although obviously 160W is the official number with it being on their website.",Neutral
AMD,"easier to sell one of each variant than several of one variant, in other words their sales team probably asked for this, operations be damned lol",Neutral
AMD,"I didn't ask for it, but I'll take it. It'll hopefully stay even more under TBP compared to other cards – I have my doubts though. My 1660 Super could use an upgrade, but I refuse to install a card that'll double the power draw of my entire apartment, just to play video games :D",Positive
AMD,But why buy such a card? It has so little performance that some APUs have more power. If you wanna go SFF go for a G suffix CPU and you will save space and power and still have more performance.,Neutral
AMD,I agree. AMD really missed out on the gold mine that is the low profile gpu market here (that Nvidia ignored). Lots of enthusiasts will be glad to grab a RX7400 for SFF or mini ITX builds. I will certainly buy one in a flash to upgrade my RX6400 that I have installed in my Lenovo M920x Tiny. I also have a few HP & Dell sff PCs that will appreciate a low profile GPU upgrade!,Positive
AMD,"This would require being able to boot though, right? I don't know of a way to change the power limit of a GPU outside of windows.  A lot of GPUs simply won't boot without the pcie power connectors plugged in",Negative
AMD,:),Neutral
AMD,"I understand - I look at this as a operations guy and seeing all these SKUs and thinking about the BOM and the associated operational complexity - really wondering about the product segmentation, and wondering is it all worth it or just pandering to marketing beliefs.  I see the same with HP, Lenovo - needlessly having some many SKUs and I am convinced their senior  product managers don't have a clue - which product for which segment, etc - just MBA idiots thinking they have mastered the consumer surplus curve.  Dell has been a surprise, rationalizing somewhat superficially - superficially  because if you go and pick a product and customize - like at Apple store - it is all warnings that this chassis does not fit that choice - blah blah blah. At least given them a couple of years to figure out this as they are on the right track.  Apple is really the king - fanbois eat them up without blinking. I am willing to bet - Apple people can correct me - their operational/marketing complexity is way less.  / end rant",Negative
AMD,"RX 7400 seems to be around RTX 3050 8GB - RX 6600 performance though. It's not actually a bad performing gaming card at all (especially for 43 watts), especially if the MSRP is right.    It's a good option theorectically if you are on something like a old Dell Optiplex SFF, where the best option is a 3050 6GB or RX 6400.",Positive
AMD,Home Theater.,Neutral
AMD,"yeah i at least assume it won't boot without the connector, if that is what is limiting your use case.",Neutral
AMD,"If it is just for video output: all AM5 CPUs have an integrated GPU that can drive 4k60 no problem (not in 3D, but 2D stuff like video playback is no problem at all). Only those with a 'F' suffix have the GPU disabled.  I.e. a Ryzen 5 7400 goes for 130€ (tax included, so about 120 USD without tax), which is more than enough for video playback. For 140€ you can get a Ryzen 5 8500G, which has a little less CPU power (like 5% less) but has a way more powerful integrated GPU. You can play most games on low settings on it, if it is an older game you can probably max it out no problem. It is about on par with a GTX 1050. So if your home theater should also function as an emulation machine for everything PS2 and older, even some PS3 era games, go for this CPU.  A dedicated GPU wouldn't be cheaper than about 100USD, so there is just no room to sell those if the cheapest CPU+GPU combo goes for 120USD.  Nvidia is still selling the GT 710 for 50USD, but the integrated GPU in the Ryzen CPUs is about 3 times as powerful. The G version is more than 40 times as powerful, just to compare them. They did have the RTX 1650, but you can't get it new anymore, was 160SUD at launch but went up to over 500USD towards the end of its life cycle. So nothing people that wanna be cheap would buy.",Neutral
AMD,"Is anyone really doing HTPCs still? Even with all the issues Windows and Linux have with HDR and stuff? I feel like a Plex server capable of AV1 transcode plus a box that supports AV1 like Fire TV 4K, onn. 4K box, or even your TV's built-in chip would be better than an HTPC.   Hell the Plex machine could have the RX 7400 because that has AV1 encode and decode. That would justify AMD releasing it.   Then use a PS5 or something for UHD Blu-Rays.",Neutral
AMD,That is pretty much the exact use case for most of these low power GPU requests unfortunately.   The ability to get power from the PCIE slot only makes for a VERY high amount of compatibility.  Makes you wonder why a lot of the mobile GPU chips aren't released as low power desktop GPU's too,Negative
AMD,"yea AMD's low end gpu devision has been entirely absorbed by APU's and while i like that the apus have good gpus, sometimes i wish they would release more low end cards for systems that are already established and just need a gpu      i always wanted AMD to release a 12 or so CU Vega thats passive cooled and single slot or low profile back in the day when vega was relevant",Negative
AMD,'existing' home theatre.,Neutral
AMD,existing setup.,Neutral
AMD,"There isn't a market here, this is a few thousand possible customers, not enough to create a product.",Negative
AMD,I don't understand. Like a Windows Vista PC with Windows Media Center?,Negative
AMD,windows 10 with an Rx460.,Neutral
AMD,Right but how do you use it? There's no remote control so you need to always have a mouse and keyboard ready? How do you have the audio routed?,Neutral
AMD,"K/M works great, Amp.",Positive
AMD,"AMD will do anything but hire more software developers to fix their stuff. Running a goddamn lottery here, this is beyond embarrassing.",Negative
AMD,sure we'd all like to see amd pay lots of devs to improve / fix rocm but    I think they also need to do stuff like this to get people interested.    After all you'll always need people to tinker with it because they want to and    not because they are forced to in the end,Neutral
AMD,Smart move,Neutral
AMD,"This has been their MO for awhile. They release something way after Nvidia that isn’t as good, then hope the community does free work to make it useful.",Negative
AMD,Going back to individual integer schedulers makes me think the 88 entry unified scheduler in Zen 5 might be suspected as a root bottleneck for why Zen 5 didn't do better against Zen 4 despite being so dramatically wider.,Negative
AMD,Looking forward to building an EPYC Zen6 with 8GB of RAM in 2026,Positive
AMD,Original article: https://www.phoronix.com/news/AMD-Zen-6-znver6-GCC-16,Neutral
AMD,time for AMD to make a new memory standard that doesnt need the memory makers input and amd can make it all in house,Neutral
AMD,For what? Everything is still gpu limited with my 7800x3d,Negative
AMD,I can't wait for the inevitable meltdown from gamers if Zen6 requires DDR6 RAM lol...,Neutral
AMD,I doubt availability of consumer zen 6. Wafer availability on the newest nodes is getting tight and expensive. Totally wouldnt put it past them to re release the old archs for a few years.,Negative
AMD,Nobody will buy this shit if it's 700 USD bro,Negative
AMD,"I thought it was well considered that the IO die was the main culprit? Hence why the x3D parts were so much better than the standard, and why even the 9800x3d has serious issues in heavy RT workloads once the cache runs out and it has to dip into main memory.",Negative
AMD,"Zen 5 did better in most non gaming tasks,  in Games, we already seen multiple times that wider cores dont really help with gaming performance if you dont increase the cache/get a faster IMC and memory  Rocket lake is another good example, its much beefier than Skylake, but Skylake had lower latencies so the 10900k/10700k outperformed the 11900k/11700k in games  you can also take ""gimped"" Zen 3 APUs that have 16MB of L3 per CCD  and compare them to Zen 2 with 16MB per CCX, the difference is quite small in games     i suspect that Zen 6 with 48MB per CCD (if rumors are true) will do 10 - 15% better than Vanilla Zen 5 in games, but still slower than a 9800x3d  and 10800X3D is where Zen 6 is going to shine  also, if the 7GHz rumors have any truth to them, we might see bigger gains but i doubt 2nm can reach 7ghz",Positive
AMD,Performance profiling didn't show that though. Being backend bound is overwhelmingly the smallest bottleneck for perf in almost all the specint2017 subtests.,Negative
AMD,I just grabbed the $159 Newegg bundle for a future/spare parts build just to have the 16gb ram on hand. It happened to come with a pretty solid motherboard so can’t complain. Very tempted to throw a 6 core x3d cpu in and and be off to the races.,Positive
AMD,Just need 8gb of cache,Neutral
AMD,I can smell those 4 slots with 2 GBs RAM each,Neutral
AMD,"If they had stupid amounts of money and fab capacity, which they don't, that'd still be a bad idea.  Basically Rambus all over again but from AMD this time around.",Negative
AMD,"Would be cool if they made a threadripper sized socket that has the CPU die, GPU die, and HBM dies for unified memory all next to each other. The Vega cards already had the GPU and HBM2 dies organized this way years ago, and the APUs AMD has been putting out have been great, so the pieces are all there. Would admittedly suck for upgradability, but would probably have pretty big efficiency and performance gains",Positive
AMD,Yeah we don't need another competing standard..  https://xkcd.com/927/,Neutral
AMD,"Man, I have flashbacks from the mining boom and overpriced gpus. It just isn’t fun to build a rig rn",Negative
AMD,AMD has confirmed that it is for AM5,Neutral
AMD,DDR6 isn't coming for years.  Earliest dates you can find are sometime in 2027.  And that is primarily for server use since the cost for it is supposed to be REAL high.  Its probably coming to desktop much later in 2028 or so and Intel will probably launch it first in PC just like they did pretty much DDR2 onwards (IIRC they actually supported RDRAM before DDR).  AMD also tends to delay adopting the latest memory standards by a year or more.  Been that way since before DDR3 came out so they're pretty consistent here.  If they follow that pattern again you won't see AMD requiring DDR6 support until sometime in 2029.  That is why rumors having been coming out that Zen7 will also be on AM5.  Either DDR6 won't be ready in time or AMD won't be ready with a new socket in time for Zen7.  Given the caches they're putting on their chips these days its not like it matters much anyways what RAM they use past a certain point.  You can get more benefit from cranking the IF bus and improving latency then going high bandwidth on AM5 anyways.  And that is for the non-X3D chips.  The X3D chips care even less about main system bandwidth.    Where it would matter is for a high(er) performance iGPU but everyone seems to be going for soldered RAM of some sort with those anyways so its of questionable benefit any which way you look at it for AM5's expected lifespan.,Neutral
AMD,Top-ish end Zen6 could easily be 700USD+.  But its also rumored that 32C/64T is coming Zen6 to very top end so its hard to judge what is really going to happen price + core-wise here.  For reference a 16C/32T Zen5 9950X is about $540 right now though so I'd expect prices to drop a bit lower baring tarrifs or other market BS.,Neutral
AMD,"Depends on the workload.  Games and various other apps are memory bandwidth/latency sensitive with low/maximized ILP (instruction level parallelism) potential due to stalls (waiting for data).  Those are being restricted by the IMC/IOD and fabric interconnection limits.  However, most simple benchmarks happily fit within the core caches and even those showed much lower than expected gains.  Zen 4 has four integer schedulers (96 entries, 24x4), Zen 5 has 1 unified scheduler (88 entries).  There are scenarios where each should have an advantage over the other, but the unified scheduler really should win more often than not... but it seems it wasn't able to maximize the additional execution units as fully as smaller individual schedulers could in Zen 4.  Zen 6 has now, it appears, gone back to individual schedulers, so it seems like an area AMD couldn't make the unified scheduler (which Intel uses) work as well as desired.",Neutral
AMD,">Hence why the x3D parts were so much better than the standard  They weren't though, the gains were coming from the higher frequencies new X3D parts had relative to previous X3D parts, while standard Zen 5 vs standard Zen 4 didn't see that.",Positive
AMD,"The 3D parts don't seem relatively any better between Zen 3, 4 or 5. Gaming wise 7800X3D and 9800X3D seem almost identical",Neutral
AMD,"> Zen 5 did better in most non gaming tasks  For context, it posted a very respectful average 18% gains on wide assortment of Linux workloads (i.e., mainly server focused):  https://www.phoronix.com/review/amd-ryzen-9950x-9900x/15",Positive
AMD,">Zen 5 did better in most non gaming tasks,  A \~10% gain in specint2017 is not impressive at all.",Neutral
AMD,RAM BUS go vroom vroom,Neutral
AMD,"I think that you are very unlikely to see HBM memory in the regular consumer market again unless the AI industry collapses.  It's too valuable for use in consumer products when they need all they can get for compute oriented products.  There's a reason they went back to GDDR relatively quick.  Granted you can get something sorta like that with the Ryzen AI chips where they're using DDR5X like the Gmktec Evo 2, or the Framework desktop with 128GB RAM.  The price is a bit breathtaking though.",Neutral
AMD,"competing standard?  the memory makers are already taking their ball and leaving, who cares about their standard if they are just gonna up and leave",Negative
AMD,If you want to upgrade. I would not wait for next year. Honestly. I cant see the ram and gpu situation getting better in the upcoming next 3-4 years.,Negative
AMD,Gaming has hit the mainstream is what happened. You know it's real bad when consoles are getting price hikes every year now.,Negative
AMD,"Yeah, there needs to be a reason for things to be faster. We have ideas and standards for all kinds of things, but only really start moving when it's actually required.   Take pcie for example. We were 7 years on pcie 3.0. Because it was enough. 16 lanes was enough for basically everything.   Then we got 2 years on 4.0 and 2 on 5.0. Tho consumer is still using 5.0. Because suddenly GPU's and SSD's were able to saturate it and we needed more for our highspeed networking.   Networking is also funny, we had the ability for 400 gigabit for ages. We just didn't need it so it was barely relevant. Same with Gigabit. Man we are JUST barely getting 2.5g devices. But suddenly you can get 400g for under 2k€.   If there is no need for ddr6, then there will not be a ddr6. Some maybe even sidestep ddr for hbm. Not entirely impossible. Just like gddr5 was around a long time, then suddenly we needed more so hbm, dddr6 and gddr6x came around really quickly.",Neutral
AMD,I sincerely hope zen7 lands on am5. I went super budget with my build (7900x for $200) and while it works fine I want to throw an end stage cpu in it from the last support generation when we know for sure that is.,Positive
AMD,"So, $999 for the 32 core cpu.",Neutral
AMD,What sort of rumor is that... They are expanding to 12 core CCDs so I imagine the top would be 24. How do you expect to make up 32 cores with 12core CCDs??? That would mean having 4 CCDs and disabling 4 cores per CCD which I don't see happening... Maybe we'll get 36 cores with 3 CCDs though?,Neutral
AMD,What's the point of this comment lmao,Negative
AMD,"This is probably true, but believe it’s also the case that they have leapfrogging design teams, e.g Z4 team will have moved onto develop Z6, which might have a bearing on the outcome here.",Neutral
AMD,"yeah thanks, Zen 5 is very good  the ""gaming"" side is just AMD not changing anything in the cache subsystem and IMC being the same  the 48MB L3 on the vanilla Zen 6 is going to be better than Vanilla Zen 5 for sure, if the high clocks rumors turn true, then we might even see really nice gains (unlike wider core, higher clocks have more effect on games)",Positive
AMD,"It’s purely a price to performance issue for me. I’m already using a 3090 so it’s hard to justify those fully integrated boards, but if I was building from scratch today it would probably be a different story.",Negative
AMD,"It's literally not going to matter as AMD can't get fab space/time.  The only way to fix it is to have everything integrated on the processor with HBM, which again, won't happen as AMD can't get fab space/time.  Intel has sadly gone the correct route with their process of integrating the ram into the processor, but it does suck as you can't upgrade it.",Negative
AMD,I bought the 7600 on release and planned to “max-out” the socket once I get a new GPU but I’m still happy-ish with my 7900XT.,Positive
AMD,It will be better in 18 months,Positive
AMD,Zen7 is the first one that might be DDR6.  How is Zen6 not enough of a performance uplift that you need to wait two generations?,Neutral
AMD,Possibly.  If they do a X3D version it could easily go for $1k or more I think.  Few would buy it but that is normal for these halo prestige parts.  Its more or less a HEDT chip anyways.,Neutral
AMD,Consumer 32 core CPUs are around $2500-3499 right now so 999 would be a huge discount.,Neutral
AMD,i imagine them trying to compete with Nova lake on core counts  i expect to see Zen 6c used in desktop as a 2nd die   maybe 12C Zen 6 X3D + 32C Zen 6c die,Neutral
AMD,"12C/24T is rumored for the ""big"" Zen6 but something like a Zen6c (I dunno what it'll actually be called) will fit more cores in the same die space.  Note that I mentioned rumored in this post and in the post you were originally replying to.  No one knows for sure what AMD is planning here, and I don't claim special insider knowledge, but nothing I'm stating is outlandishly impossible either.",Neutral
AMD,To flex lmao,Neutral
AMD,That's absolutely something I didn't think about and could absolutely be related... maybe a split between what the teams think is the better solution...,Negative
AMD,Aren't there actually 3 teams working on Zen?,Neutral
AMD,How so?,Neutral
AMD,The 7900x works fine for me is the thing. 4-5 years when we see am6 might be a different story though,Positive
AMD,"I think AMD need to return to lower the HEDT entry price. AM5 platform is fine with topping out at max 16cores.  HEDT used to be $549-749 only, if you inflation adjusted it is still no way near the base price of the lowest HEDT CPU now. The extra PCIE lanes is even more relevant now thanks to nvme ssd.",Neutral
AMD,We’ll find out if this is the case when Zen 7 releases.,Neutral
AMD,Because demand will catch up.,Neutral
AMD,"Intel may get entry level ish HEDT next time around. But probably not. Buying a cascade lake or old TR is still all that’s available for most people’s budget. Which, they still work ok. Just slow on single core and missing 512",Neutral
AMD,Nice! Planning to achieve Rx7600 + 5 5700x using Pop! Os,Positive
AMD,didn't go 9070xt due to vram?,Neutral
AMD,"Hold on to it dearly, the next couple of years are gonna be rough",Negative
AMD,"Thanks :)  popOS is really nice!      I chose Arch because I need the native ROCm 7.1 package, and Arch supports it. It's a truly bleeding edge distro.   Obviously, with its pros and cons, I believe each distro has its own features for each type of user.",Positive
AMD,"Hmm... Several reasons:  1. VRAM 2. I'm planning on getting a liquid cooler for the GPU soon and installing the 7900XTX Aqua Elite BIOS on it for a massive performance boost. 3. I have a soft spot for hardware architectures, and RDNA3 is one of my favorites after the cell broadband engine architecture. In fact, I think the 7900XTX will be one of the cards  I won't sell, but will sit on my lab shelf. 4. I get for now better desktop support, as well as better productivity and AI support.",Positive
AMD,"Yes, unfortunately yes. We hope the tech world can be accessible to everyone again",Negative
AMD,"Arch is a nice choice tho, might as well try endevour OS one day!",Positive
AMD,What does it take to install the aqua elite bios on a reference card?,Neutral
AMD,Why RDNA3 and Cell specifically?,Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,the higher rated cpu is more than the lower rated cpu? shocker...,Neutral
AMD,"I don't usually do this, but: ""Obviously"" - Professor Snape",Neutral
AMD,"In other words, grab the 9800x3d on sale while you still can before AMD increases the price of 9800x3d to make 9850x3d more appealing.",Neutral
AMD,About to get 9800x3d for 325 at Microcenter.,Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,Calling it now they're discontinuing 9800X3D and only selling this to raise prices in a less obvious way.,Negative
AMD,you telling me it's more expansive than the 9800x3d? whaaaat?!,Neutral
AMD,Kinda wish I held out for a 9800 or higher instead of getting the 9700X,Neutral
AMD,"Will this one cook Asrock boards, or will Asrock boards cook this cpu?",Neutral
AMD,"> before AMD increases the price of 9800x3d to make 9850x3d more appealing.  That's not how that works...  If 9800X3D goes EOL because 9850X3D has good-enough yields, then the price increase is from **retailers** artifically inflating prices of remaining stock due to high demand.  And again, why would AMD do this?   CPUs that can't reach 5.6GHz but reach 5.2 are still valuable.",Neutral
AMD,I have 4 of them lol,Neutral
AMD,325?!!! I’ll sacrifice my pp to be micro for one to open in my city,Neutral
AMD,How the fuck is that possible? Insane deal! Open box?,Negative
AMD,Not normally how the 850 class chips work   This is nothing new. AMD has done this for nearly 10 years now,Neutral
AMD,"Considering 7800x3d is still for sale, doubtful.",Neutral
AMD,9850X3D clocks higher than 9800X3D.  That means any binned chips that can't reach 5.6GHz will still have value as 9800X3D if they can reach 5.2GHz.,Neutral
AMD,When have they ever done this with previous x800x3d chips?,Neutral
AMD,"Ehh, you saved like $200 and unless you have a really high end GPU on a relatively low resolution the difference is very minimal, besides in two years or so you can just throw a zen 6 chip in there.",Neutral
AMD,Maybe in games because of the clockspeed increase. but even the any 9000 x3D chip will be marginal of error when it comes to games,Neutral
AMD,"So you're telling me it was the retailers that increased the prices of 7800x3d couple weeks/months before the 9800x3d released? Because that's certainly not what anybody is saying.   9800x3d is most likely not going away or ending production. They'll just increase the price of 9800x3d because well they can. This will bridge the pricegap between 9800x3d and 9850x3d and make people think ""Oh well 9850x3d is just 50-80usd more, might as well get that.""",Negative
AMD,Sounds like open box.,Neutral
AMD,![gif](giphy|TbfII8ChaSS6PlZFvv),Neutral
AMD,"399 is the price without a bundle with the motherboard, but the good thing is that my friend needs a new PC so I'm just going to give her my 7800x3D for about 175 (bought it for 325 in August of 2024) + the motherboard in the bundle.",Positive
AMD,So you get one at no change to you? Doesn’t seem very fair,Negative
AMD,"Combos with the motherboard, and then have my friend build a PC on the motherboard + 7800x3D that I'm going to give to her.  399 is the price without combos.",Neutral
AMD,"Yeah, that is the price without motherboard bundle.",Neutral
AMD,"It is kinda of how it works when you consider this chip is a perfectly functional 9800X3D that they withheld from the market to apply an unnecessary overclock so they can sell it a higher price. Tho obviously they won't stop production of 9800X3D completely.     They do that all the time tho to their credit this time the overclock is somewhat substantial which is not usual, let's wait and see. Still i think most people would rather this be a normal 9800X3D that they can try overclocking themselves than a pre-overclocked one for 20% higher price, I'm calling it right now this thing will NOT be 20% faster than 9800X3D, probably not even 5%.",Neutral
AMD,"I agree that it’s money saved. I’m just FOMOing a little bit, plus the whole “well I’ve spent a ton of money on components anyway…”",Neutral
AMD,The difference comes in cpu heavy titles ofc in non cpu heavy games the difference is margin of error since you aren’t using it,Neutral
AMD,The 7800x3d went ballistic the second reviews came out to show that they are still a really good CPU choice. Supply and demand did the rest.,Positive
AMD,"AMD didn't increase the MSRP on their store, so yes - it was retailers.",Neutral
AMD,"It's not. There is a Combo deal right now. I got the same thing. 9800X3D, motherboard, and 32GB CL36 DDR for $680",Neutral
AMD,"As a bonus, they're running a special right now that they'll throw in 32GB of DDR5 for $200 with any CPU.  If you need RAM for one of those systems buy it with the CPU, otherwise you're looking at more like $350 for the RAM.",Positive
AMD,As of now it’s not fully even reached 5% lol only 4.7% margin of error likely in most benchmarks. I would assume at most in 1080p and select games they’ll be a 3-5fps improvement maybe. Sometimes it will prolly be the same.,Neutral
AMD,"I was thinking the same thing as you are now, but I gave in. Build a new PC in February with a 9700X and 9070 XT when 9800X3D prices where almost double of the 9700X.   During Black Friday I upgraded to the 9800X3D as it was it was a good price and I couldn't fight my FOMO any longer.   In 1440P I don't think I would notice a difference if you would swap my 9800X3D back to a 9700X. If it makes you happy and you got money to spare go ahead and get the 9800X3D, but the 9700X is still a great CPU. The 1% lows are somewhat better in some games (that  I play) with the 9800X3D, but that's about it. The biggest upgrade is the name of the CPU and ""bragging"" rights.",Positive
AMD,"Now imagine how ballistic it would have been if the price of 7800x3d wouldn't rise to it's original MSRP, but stayed where it was during summer of 2024. 8% (or something I don't remember) worse performance for over 40% less (If my math is right).   How AMD managed to raise the price, if it was with slowing down the production, leading to decreased supply or with just selling it for more is kind of irrelevant in this context, as we're just interested in price. The point is that they can do the same here, making 9850x3d artificially more compelling product by simply raising the price of 9800x3d just before 9850x3d launches.    The price of 7800x3d reached an all time high just about when the 9800x3d was released. I don't think that's a coincidence. Looking at price history in December 2024 when the 9800x3d was barely available the price of 7800x3d went back to it's original launch price (at least here in Sweden) with the price creeping up since August of 2024.   Sure it's how market works, but It's not *all* market. AMD did something, and they might do it again, that's my point.",Negative
AMD,"Ok, but did they ever lower it?   The price listed on their website is not the price wholesale or retailers pay. Just because they didn't raise or lower it on their website doesn't mean they didn't change the price they sell at to wholesale. It's not like many consumers buy directly from AMD anyway.",Neutral
AMD,I have been eyeing a pretty  budget upgrade for sub 500 with 7600x3d wondering how dumb an idea  that would be.  Right it’s  between 400 and 420 at micro center. Would be for a mostly gaming setup would be upgrading from 3800x.,Neutral
AMD,"That doesn’t mean he’s not getting one open-box. I’ve seen them at Microcenter for around the price previously mentioned. There was no mention of a bundle, just the processor.   Even if this is the bundle offer, they’re not getting a 9800x3D for $325, they’re getting a bundle offer for $680.",Negative
AMD,"when did that start?? i just bought a whole system, cpu memory ram gpu board, i didn’t get 32 gigs for 200 bucks!",Negative
AMD,Currently playing things at 1440p. And yea I suppose in the end it’s not a huge difference,Neutral
AMD,Pepperidge Farm remembers when microcenter used to give 32gb ddr5 for free with purchase of any 7000 series CPU,Neutral
AMD,"This week, I guess.  I bought a CPU last Thursday and the offer wasn't there at the time, and so I bought the cheapest ram I could find for $250 (Crucial DDR5 5600MT/s, not even a heat spreader!).  That same kit today is $300.99, and probably was $75 3-4 months ago.",Neutral
AMD,Haha i was talking about fall 2022.,Neutral
AMD,Oh…back then the 32GB of RAM was way cheaper anyway.,Neutral
AMD,AMD still not shipping their discrete graphics in laptops in 2025 is an own goal.,Negative
AMD,"bring back the z16 line, with haltic touchpads and no numpads. Hx 370 or strix halo",Neutral
AMD,Still no Stix Halo 128gb laptop other than that single 14 inch HP one. I'd call it a phantom launch but there are many mini PCs with that chip. It makes no sense.,Negative
AMD,"How is this the most powerful?  It doesn’t have the most powerful apu, it doesn’t have the most powerful mobile cpu  from amd, and it doesn’t have a gpu.  It doesn’t even have a high end screen.",Negative
AMD,Because Lenovo refused to make:  - P1 - P16v - P16s/P14s with the better chassis and dGPU  - P16 no suffix  with AMD.,Neutral
AMD,"2.4k euro 2.6k usd is that because its got 64gig ddr5 and has ""AI"" in it? dont see the point in that high of a price other wise",Negative
AMD,"Screen resolution 1920x1200. Is it a machine from 2015?    Crippled AMD versions as usual, thanks Lenovo.",Negative
AMD,"The ""16-inch"" is doing a lot of work here, as ASUS and HP already have 13"" and 14"" models with Ryzen AI Max+ 395  Also a 16"" model with that processor was [previewed by ETA Prime](/r/Amd/comments/1medko7/this_laptop_has_amds_most_powerful_igpu/) a while back, I understand those were supposed to launch in time for the holiday season but the memory market threw a spanner into these plans.",Neutral
AMD,And dumping their unwanted last gen 7600m dies with absolutely zero mainstream oem demand en mass (steam machine),Positive
AMD,"And no 6-8 core x3d laptops with 5090, and no AMD 2-1 OLED laptops, etc.",Neutral
AMD,If they make a strix halo laptop it won’t be a Z16. It will be a different much thicker lineup. AMD also doesn’t have anything to succeed the 6550m and the Z13/Z16 was overall a thinkpad nobody asked for. I predict the same fate for the X9.,Neutral
AMD,>there are many mini PCs   Even this is a massive overstatement because practically ALL of them (sans the high priced HP Z2 mini) are from small local OEMs and many of which in weird obviously non-mainstream form factors.  I would be very surprised if total shipment of all of these mini PCs combined even reached a few hundred thousands worldwide.,Negative
AMD,"Yeah, it's strange there is no 16 inch laptop with strix halo yet, I have seen online from multiple sites that gaming laptops with strix halo are coming as part of CES 2026 and there was the sixunited leak of a possible Ryzen AI max 388 and 392, the ram shortage will of course sting here as they still need at least 32GB ram to be viable but hopefully will be offset by the longevity of having more than enough ram.  My guess is it's timing, the mini pcs and handhelds have been coming from small venders that just release when they are ready while big OEMs like Asus or Lenovo like to wait for Computex or CES to launch laptops, especially high end laptops like the kind that strix halo will be powering.",Neutral
AMD,"It’s the most powerful 16”, AMD Lenovo laptop.",Positive
AMD,As I bitterly pound away at my hot & loud corporate provided intel lenovo thinkpad,Neutral
AMD,If only there was a P1 with Strix Halo...,Neutral
AMD,Its 1.8k usd when you buy this specific model from Lenovo directly,Neutral
AMD,"The thing I hate about thinkpad is the shitty starting screen resolution. Then different screens are locked behind arbitrary hardware configurations, or just randomly not available in your country for some reason.",Negative
AMD,"[theunknownforeigner]   > Screen resolution 1920x1200. Is it a machine from 2015?   > Crippled AMD versions as usual, thanks Lenovo.  &nbsp;  Please read this before you make yourself look even more stupid:  https://psref.lenovo.com/Product/ThinkPad/ThinkPad_P16s_Gen_4_AMD?tab=spec",Negative
AMD,"> ASUS and HP already have 13"" and 14"" models with Ryzen AI Max+ 395  The headline is specifically about Lenovo",Neutral
AMD,"> ASUS and HP already have 13"" and 14"" model**s**  Drop the plural.",Neutral
AMD,I think at this point it's worth them spinning up a small company to contract manufacture laptops from Pegatron or Tongfang and sell those into the market.   Make some kind of Surface-like halo product.    There are so few AMD Advantage systems today that I can count them on one hand.,Negative
AMD,They don’t even make any X3D chip for laptop less than 16 cores. Laptop vendors always want to force bundle the highest priced combos and not allow sensible options.,Negative
AMD,"I was the owner of z16. I agree they made so many stupid decisions with that laptop and no wonder why it didnt have many sales. First of all that 6500m was barely faster than 780m igpu, 60hz screen, small battery. But with all those flaws it was actually so so close to being proper macbook competitor. Haptic touchpad  really nice, no numpad, awesome build quality, full amd system meaning linux experiece was 10/10. I would pay any amount to buy fixed z16.",Negative
AMD,"> while big OEMs like Asus or Lenovo like to wait for Computex or CES to launch laptops  This is just not true. They launch intel models left and right at any time in the year. They only ""reserve"" model launch for big events when new chips get unveiled at those events.",Neutral
AMD,"Interesting, specially since my AMD Lenovo laptop has a ryzen 9955HX3D 16 inch oled laptop is far stronger, but I guess it doesn't count because its got an Nvidia GPU instead of being an APU.",Neutral
AMD,low bar lmao,Neutral
AMD,Tbf I don’t think you got a lunar lake one.,Neutral
AMD,"Forget P1, Lenovo has zero strix halo laptop across their entire lineup.",Neutral
AMD,Tbf that’s a corporate laptop thing not a Lenovo specific thing. Elitebook/Probook and Dell Pro are the same.,Neutral
AMD,"Read the review, you're stuck with the 1920x1200 if you want Ryzen 9. Same with the P14s Gen 6 AMD.",Neutral
AMD,There is not a single zen 5 amd advantage system. Zero. Not one. I’m pretty sure even the zen 5 framework 16” isn’t an official advantage laptop anymore.,Negative
AMD,"Yet for some reason there is no laptop 5080 or 5090 with even the 16 core x3d, which is insane considering that is the best gaming chip, and the extra efficiency of x3d would benefit power and thermally limited laptop dgpus.",Negative
AMD,"I agree it looks great, and I actually think the faux leather feels nice. But “fixing” it requires a hypothetical die smaller than Navi 44 (9600 desktop die) or powergating Navi 44 to 50w TGP. Amd is not showing any care in laptops atm. They still don’t even have any mobile rdna4. 2026 is all coast 0 new things.",Positive
AMD,"That’s mostly true, you’re correct. The difference is that flagship laptops, like Strix Halo systems, RTX 5080/5090 models, or more extreme designs such as the Zephyrus Duo, are usually saved for CES or Computex announcements rather than dropped randomly.  More budget‑oriented devices, like ASUS TUF or the recent Gigabyte A16 Pro, tend to launch whenever. But I’d be surprised to see “budget” Strix Halo laptops in 2026, especially with the current RAM shortage. There will probably be more options than in 2025, but I wouldn’t expect them to lose their premium pricing since Nvidia faces the same memory constraints with their competing products.",Neutral
AMD,"They should’ve just said most powerful AMD Thinkpad laptop instead of this, because it is confusingly worded. I actually have this P16S laptop for work, and it’s honestly a pretty solid workstation with excellent battery life, but yeah the 890m isn’t exactly going to win any awards.",Neutral
AMD,Exactly.. which is a shame,Neutral
AMD,There are 3 in total,Neutral
AMD,I would prefer to not have dgpu. Strix point or strix halo. I currently. Have strix point tuxedo laptop with slow slotted ram i stead of lpddr5x and it runs games like pubg and cs2. Cant even image what good of experience it would be with halo.,Negative
AMD,"Lenovo dropped the AMD variant of the Legion Pro 7 randomly a few weeks back.   I have a different prediction.  I would post nudes on main if there is a mainstream (by mainstream I mean mainstream form factor, NOT mainstream budget) 16"" clamshell laptop with Strix Halo from the big three vendor.  There are usually leaks. There aren't any.",Neutral
AMD,"I actually like the P series for workstation, so honestly its easier to recommend them rather than trying to use catchy titles from their side honestly.  I agree with you",Positive
AMD,"> most powerful AMD Thinkpad laptop  How many hoops until the title is basically ""The most powerful AMD ThinkPad P16s launched in the USA in 2025""?",Neutral
AMD,Halo is never happening in Z16-like chassis.,Negative
AMD,Due to thermals? Why not? They put it in a tablet,Neutral
AMD,"An ROG is not the same philosophy as a thinkpad at all. Thinkpads have never tolerated high temps or high fan noise and have always been extremely conservative with power limits relative to size. The only thing comparable is HP's Zbook Ultra 14, and that is thick for a 14 incher. A 16 incher (you want to be able to hold 80w) would be much thinker than a Z16.",Negative
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,"Ok everybody, say it with me:  it’s just a slightly higher clocked version of the 300 series with the same Zen4 and RDNA 3.5 cores. There’s not anything to get excited about.",Neutral
AMD,Their naming conventions are dumb.,Neutral
AMD,Gonna be a slaughter by Panther. AMD not crossing that 20-25% threshold on laptop market share they've been at since Renoir era.,Negative
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,It’s just a slightly higher clocked version of the 300 series with the same Zen4 and RDNA 3.5 cores. There’s nothing to get excited about. How’d I do daddy?,Neutral
AMD,It’s not going to be a slaughter because of the price difference. Panther is not going to be priced just as Ryzen 400. It’s going to much more expensive because of the new high tech,Neutral
AMD,"Nothing gets slaughtered by anything. If current ram and ssd prices continue to soar, people will be looking at the cheapest CPUs to offset that 16GB.",Negative
AMD,It’s just a slightly higher clocked version of the 300 series with the same Zen4 and RDNA 3.5 cores. There’s nothing to get excited about. How’d I do brother?,Neutral
AMD,"Somehow the supposedly expensive lunar lake with on package ram can be found in laptops with similar price to kracken point, not just strix point. Whether that is intel eating a lower margin or laptop oems eating a lower margin, I don’t know, and isn’t for consumers to worry about.   There will also be 10x as many laptops using Panther than rebrand strix point just like there was way more laptops using meteor than phoenix rebrand (hawk point) in 2024.",Neutral
AMD,The volume will once again be corporate bulk purchase via long term contracts as usual.,Neutral
AMD,"Ok everybody, say it with me: it’s just a slightly higher clocked version of the 300 series with the same Zen4 and RDNA 3.5 cores. They should've named it RDNA 3+.",Neutral
AMD,"Can you find me a laptop with Lunar Lake and Krackan, with exactly the same specs, and at the same price? Used Geizhals and did not succeed.",Neutral
AMD,And it will have to compete with long term contacts set by AI data centers backed by trillion dollar companies,Neutral
AMD,It’s just a slightly higher clocked version of the 300 series with the same Zen4 and RDNA 3.5 cores. There’s not anything to get excited about.  Are we… are we still doing this?,Neutral
AMD,"https://www.dell.com/en-uk/shop/laptops-2-in-1-pcs/dell-pro-14-plus-laptop-or-2-in-1/spd/dell-pro-pb14255-2-in-1-laptop/gcto_pb14255_emea?redirectto=SOC&configurationid=8c519bdc-ad7a-4b90-8b7a-01e5a3bc99bc   https://www.dell.com/en-uk/shop/laptops-2-in-1-pcs/dell-pro-14-plus-laptop-or-2-in-1/spd/dell-pro-pb14250-2-in-1-laptop/gcto_pb14250_emea?redirectto=SOC&configurationid=d4e36ddc-2337-4a13-9372-6f34a11838c0  These two Dell Pro 14 Plus with Ryzen Pro 340 or Core Ultra 236v both with 16GB ram, 512GB SSD, same display and all other options as closely match as possible. Difference came out to be £25 cheaper on the LNL side.",Neutral
AMD,"And just like in the past years, the worldwide PC shipment had never dropped apart from post-Covid adjustment in 2023. Businesses on a scheduled upgrade cycle will eat the cost amortised over the useful life cycle of hardware as usual.",Neutral
AMD,Too bad it has the firestarter connector,Negative
AMD,"Got my 9070 XT Pure a week ago, this Asrock card looks nice as well",Positive
AMD,I rather have the Miku version,Neutral
AMD,That thing should be at least $50 cheaper than the regular version.,Neutral
AMD,Perfect for when something goes wrong ASUS will be of no help in the customer service department,Negative
AMD,What a total miscalculation to make this instead of a battlefield 6 or arc raiders 9070xt.   No one's gonna buy this.,Negative
AMD,"ROG 9070, but nvidia have ROG brand only for them.",Neutral
AMD,The 9070 XT is a $599 card.,Neutral
AMD,"They are doing everything now just to get people playing this shit game, because nobody wants to play it lmao",Negative
AMD,My basic reaper 9070 will do thanks,Positive
AMD,"I am building a Linux mini-ITX PC, and that card would not fit, and also the software mentioned doesn't work on linux. It looks really cool though. I don't play CoD, but I would still buy it if I was in the market for a mid-tower ATX PC",Positive
AMD,Ew,Neutral
AMD,This isn't the most exciting game to base a custom edition card off of. Blacks Ops 7 is far from what BO1 and BO2 were.,Negative
AMD,I also keep by memory at 8008mhz so it burns itself to death,Neutral
AMD,The sales are down bad for activision this year,Negative
AMD,sry i wont put that thing on my desktop. A cod version? ??,Neutral
AMD,"Instead of fixing the fidelity CAS upscaler where it’s blurry when you move in game, they make a new COD branded gpu. Fidelity CAS was the best graphics option for competitive play. It was a good blend of performance and visibility. The only way to see in the game now is to have DLSS transformer (which requires an NVIDIA gpu) or FSR 4 (which requires a new 9000s series AMD gpu). The rest of AMD gpu users are stuck with a broken graphics option which is weird because BO6 it was working fine.",Negative
AMD,There’s a Miku version woot,Neutral
AMD,We have REVA  https://www.powercolor.com/news-detail71.htm,Neutral
AMD,it should been included in the game pass Subscription,Neutral
AMD,it’ll sell out,Neutral
AMD,It’s almost like releasing a Madden 2026 edition.,Neutral
AMD,Nobody? It’s COD bruh 🤣,Neutral
AMD,"I used to work at a 4-letter company that made a killer COD Black Ops card back in the day, but I would also say that this is a really nice-looking design for our TUF Gaming models, which typically cater to solid tones. Of course, if it doesn't work for your system in terms of colors - or a preference towards franchises, then I also understand that too.",Positive
AMD,"The Miku version is on the NVIDIA side, unfortunately. Personally, I still really like the color design of this model as it's used as a colorway on the TUF Gaming design.",Neutral
AMD,"Kinda meh, just one small picture on the edge and too focused on the ass, I like the Asus Miku, Manli Polar Fox and MSI MLG design more. They could put Ruby on it, similar to VTX3D box design.  https://i.redd.it/0xgy09dr6b8g1.gif",Neutral
AMD,Im just saying B07 is shit and gets a lot of hate cuz of it,Negative
AMD,I think its more a judgment of a steaming pile of a video game that had been universally panned and having that represented on a video card is a tough sell. Its like when Apple got blasted for including the U2 album on all of their devices and people couldn't delete it.,Negative
AMD,Powercolor has you beat with the power of waifu  https://www.powercolor.com/news-detail71.htm,Neutral
AMD,"I'm sad the there no Prime series gpu with Miku design, TUF has too much coil whine for my liking even though the fans are better.",Negative
AMD,"COD gets tons of hate annually and it isn’t due to “quality” (people would have to play the game to assess that). It’s due to being the highest selling franchise of the past 2 decades.   The same reason Justin Bieber got hated on, the same reason LeBron gets hated on, the same reason Tom Brady gets hated on, etc.   People hate to see the same thing succeeding over and over and will pray on its downfall.  Black Ops 7 players love it very much, non-players hate it very much. A tale as old as time.",Negative
AMD,"I get where you and others are coming from, but I'm still skeptical when I hear that a game is universally disliked - especially franchises like COD and Battlefield. I gamed on the original MW for a long time and remember the build-up for the so-called ""boycott"" of MW2, which barely lasted beyond the launch of the game. Many people still didn't play the game, and I'm sure that's true here as well, but a lot of the most public and loudest critics were still queueing up as soon as the game launched. It was a fairly instructive moment in how the Internet works.  However, I don't think the analogy works here. We still offer our standard TUF Gaming Radeon RX 9700 XT without the COD tie-in and design, whereas the Apple situation involved everyone using the service and didn't have an opportunity (initially) to choose if they wanted the album or remove it.",Neutral
AMD,Oh wow that looks really good,Positive
AMD,That doesn't seem as exciting as Yeston's waifu cards.,Neutral
AMD,"Since bo7 came out cod has regularly had on average around 45 thousand players on steamdb losing at least 100,000 players. They're now doing literally anything they can to get people to play their game hence all the promotions and free weekends",Neutral
AMD,It'd hard to beat ocean water scented weeaboo power,Neutral
AMD,"that’s Steam. COD consistently does poorly on Steam.   According to Circana, COD is #2 on both consoles just behind Fortnite (was #1 on launch month though).   COD is on 3 PC clients: BattleNet, PC Game Pass, and Steam.   Steam has always been COD’s smallest platform.",Neutral
AMD,Finally someone saying what I've been saying for days. Steam players are convinced that Steam holds like 90% of PC player base. They don't know that some games are played much more on other plaforms. For example Diablo 4 has like 10X more players on [Battle.net](http://Battle.net) than on Steam. CoD is played more on GamePass AND [Battle.net](http://Battle.net) than on Steam. They just can't accept that we are not all Steam glazers.,Neutral
AMD,Steam does have most of the pc player base on it without question though. The one point you made here is invalidated by the fact that diablo 4 isn't a game that was doing well anyway so when it shadow dropped on steam it naturally wasn't gonna have a huge player base and the few that are left on [battle.net](http://battle.net) are all people who got really into it or people who spent too much on it to quit.   Ironically you just did the exact thing that Throwaway was initially talking about by hating on something just for the sake of it not to mention that glazing [battle.net](http://battle.net) of all launchers is just crazy work. What next? Ubisoft Connect?,Negative
AMD,"Some games are just not played on Steam,  reason doesn't matter. Saying that Diablo 4 have only ""few"" players is so disingenuous. The game is really popular, especially now more than ever with current season and Paladin class added.",Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,"Nothing much to be excited about, since Gorgon Point is just a Strix Point refresh.  You get slightly higher clocks but otherwise it's mostly the same.",Neutral
AMD,"If these rumors are true, I am wondering why they haven't moved onto RNDA 4 based iGPs instead of the RDNA 3.5 they've been using for a while. Makes me wonder if they are just waiting for RDNA5/UDNA before updating their iGPs.",Neutral
AMD,"Damn, AMD core stagnation is REAL  Quad core Ryzen 5 in 2026 is insane",Negative
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,Gorgon is a refresh of Strix. Not new silicon. It’s still RDNA3.5 for that reason alone.,Neutral
AMD,"[3.5 was specifically made for mobile](https://www.pcgamer.com/hardware/graphics-cards/amds-tweaked-rdna-35-gpu-is-solely-focused-on-improving-mobile-gaming-performance/) and 4 as per leaks and rumors, didn’t have any plan to release on the mobile space (either scrapped alongside top dies or never was, don’t know which to believe)  A “good enough” strategy until they deem the next arch to be viable and feasible for mobile requirements.  Also, 3.5 is still new and these upcoming are refreshes.",Neutral
AMD,I think your guess is right. I suspect they will update their entire line with UDNA when it releases,Neutral
AMD,"Remember, the main limit for an igpu is the memory bandwidth and latency since it uses ram. Strix halo has a wider bus thus higher bandwidth, but all the others use normal lpddr5x, for example in handhelds. It might be that a rdna4 igpu isn't really faster than a 3.5 one due to this aspect alone, even if it would have nice features like fsr4. They probably are waiting for ddr6 memory to increase the bandwidth and put stronger igpus in devices, or at least this is my theory.",Neutral
AMD,This is just a refresh of the current lineup. It is heavily rumoured that Zen6 will feature more cores.,Neutral
AMD,Isn't Lunar Lake eating their iGPU lunch? I certainly hope their next arch is viable and feasible for mobile.,Neutral
AMD,LPDDR5X 10667 is in mass production and matches the introductory data rate of LPDDR6 (if we ignore 1.5X channel/ECC overhead). They can brute force GB/s issue.   Also RDNA 3.5 has many of RAM BW saving tech as so we'll see how much they actually need to push this. See C&C breakdown and LLVM post u/Defeqel.  But it's an easy win for higher end configs for sure as they're prob BW starved rn.  AMD better have FSR4 INT8 ready by 2026 because RDNA 3.5 products are going nowhere. Even standard SoCs with Medusa Point (without dGPU die) will reuse RDNA 3.5 for another gen according to Kepler\_L2. No UDNA except for premium mobile SKUs (with seperate dGPU die bolted on) it seems u/SAUCEYOLOSWAG and u/Hero_The_Zero   But it's good enough for entry level as it doesn't need ML power of RDNA 4 or GFX13 and like u/J05A3 said it's new and it'll prob be a repeat of Vega 7nm iGPU except this time even more prolonged.,Neutral
AMD,"RDNA4 introduced new compression tech, which is why it gets away with rather small bandwidth, though perhaps 3.5 already has that",Neutral
AMD,Hopefully   AMD has been on 6/8 core Ryzen 5/7 longer than Intel was on 4 cores lol,Neutral
AMD,"did you forget that the x950 parts launched in 2019? sure the cheaper mainstream is still 8 cores, but Intel was selling 8 cores for $1000 until Zen. at least AMD isn't selling 16 cores for that still though I hope they increase mainstream to 32 soon.",Neutral
AMD,I did not. But did you forget that those were a different price class and basically HEDT and not really mainstream?  AMD has had the same core counts on their mainstream CPUs for almost 9 years LOL.,Neutral
AMD,"they're on the mainstream platform without HEDT features. though it's proving that without competition from Intel, AMD will do the same thing as them. it's actually even longer if you consider Bulldozer an 8-core lol",Neutral
AMD,"Your assumption is most likely correct, that is why benchmarks try to stay above the L3 size.  I think it was the benchmark SuperPI that was very popular, but once cache sizes on CPU's got so big that the whole program fits in it, the results reflected more the cache size than the raw CPU performance. Similar to what you see here in your results.",Neutral
AMD,"I'm no expert, but I remember the new Arrow Lake chips lacking AVX-512 support which may or may not contribute to this difference.",Neutral
AMD,"The E5-2680 v4 is a low-clock Broadwell, it's going to be horrible in singlethread and the L3 is scattered across the die in a slow ringbus. The i7-12700H scales uncore frequencies depending on load, so it probably had the L3 downclock after a second or two, making latency horrible. The EPYC 7773X likely runs at a low clock speed and has *horrible* memory latency, so it would fall to memory. The 285K has a very slow L3 and slow interconnects.  Meanwhile, the R7-5700X and 9900X\* can fit it in their 32MB L3 easily and can put it in memory if needed, and the M2 can fit it in its 16+8MB SoC cache.  \*AMD chips can *sometimes* reference memory across CCDs, but they prefer not to, and when a cache line is evicted from a CCD it gets sent to RAM (instead of being sent to a different CCD), so functionally AMD cores can only use a single L3. Maybe someday we'll see an SoC cache on the IOD.",Negative
AMD,"Anything other than firing up a profiler is just guesswork, why bother when you can just find out?",Negative
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,Will this be compatible with AM5?,Neutral
AMD,Dont tell userbenchmarks,Neutral
AMD,Got any ram?,Neutral
AMD,Just give us back 5800x3d lol,Neutral
AMD,I'm going to ride out my 5800X3D until AM6 AND cheap chinese ram.  I think by then I'll finally go for a SFF machine.,Positive
AMD,Zen 6....on ddr4 right? Right?,Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,It’s about FP16 support. I believe this has already been posted before?,Neutral
AMD,Finally more cores/threads to end that Intel big/little competition in multicore applications.,Neutral
AMD,> Another leak claims that AMD is targeting 7 GHz clock speeds with its Zen 7 CPUs  doubt,Neutral
AMD,"It’s adequately cooled, and a 7950X3D is not performing slower than a 5800X, period.",Neutral
AMD,When am6???  I have 7900xtx and 7950x3d. Waiting for next generation shit before I blast 3k,Neutral
AMD,Thanks none of us will afford it due to stupid corpos and AI ruining everything.,Negative
AMD,"That's the plan, but it'll be up to motherboard manufacturers to provide an updated BIOS.",Neutral
AMD,short answer: yes      long answer: [https://www.techspot.com/news/109821-amd-zen-6-cpus-confirmed-work-existing-am5.html](https://www.techspot.com/news/109821-amd-zen-6-cpus-confirmed-work-existing-am5.html),Neutral
AMD,Yes Zen 6 will be compatible,Positive
AMD,"Even if they launched AM6 next year, DDR6 is not commercially available until after 2027-2028 and there isn't much consumer demand for something beyond PCIe 5.0.",Neutral
AMD,"""this may be the fastest CPU ever, but it never beats intel! So: -500 points!""",Negative
AMD,"""The Ryzen X 1800 is the most powerful processor recorded in the UserBenchmark database, handily beating out even most Arrow Lake outings. That being said, this is yet another instance of AMD's ""Advanced Marketing"", peddling low-quality silicon at premium prices in a fancy box. With Intel's marketing team asleep at the wheel, this is the perfect opportunity for AMD to awaken its sleeper agent techtubers to convince you that lower dollar-per-FPS is better. buy a pentium""  edit: I meant ""*higher* dollar-per-FPS"" (more expensive) but I think UB would have that footgun himself",Positive
AMD,Is that still a thing with them?,Neutral
AMD,Frothing at the mouth already,Neutral
AMD,r/TechHardware 🤣,Neutral
AMD,Yesssss  I regret not getting the 5700x3d when i upgraded to my 5600 now,Positive
AMD,The gift that keeps on giving.  My use cases still doesn't need more than a 5800x3d.   I guess once I go back to ultrawide or 4k I'll jump over for a new built on am6 if the market isn't fucked up.,Neutral
AMD,"I'm actually surprised people didn't switch to a 5000x3d years ago... That was the first thing I did when it released back then. After I had a taste of it, I upgraded to 7000x3d when the ram prices dropped. People waited almost 3 generation before deciding to switch to Ddr5 which I thought was a little long...",Neutral
AMD,"5800X3D is a beast still, been continually upgrading around it. I’ve seen zero reason to go AM5.",Positive
AMD,And make a 5950x3d !!,Neutral
AMD,U dont need 5800x3d.you just need 5800x.its good enough. New ssd and 4k screen monitor,Neutral
AMD,"The way to go. I had a 2080ti/9700k matx that lasted me 8ish years (still have it actually, was planning to sell it but shortages scared me lol). I had upgraded to 9070xt/7800x3d sff, I see another 8 years with this. This is the way. Sff is a bit of a challenge to work on but I enjoyed building it so much.",Positive
AMD,You probably won’t have to replace the 5800x3d for years,Neutral
AMD,DDR4 production ended like a year ago...,Neutral
AMD,"This is my thought too \^\^ But the enthusiasts will ""happily"" pay $4000 for a 32 GB DDR5 9000 Kit",Neutral
AMD,Ddr4 has more than doubled in price as well...,Neutral
AMD,Why would they go back a generation?,Neutral
AMD,It will be HBM,Positive
AMD,"Ehhh, if rumors are true then Intel is increasing core counts by a lot as well. Likely It'll be the same situation as now.",Neutral
AMD,"I swear these leaks come out of random tech tubers asses. Some Zen6 CPUs will probably hit 6ghz though, considering the node shrink. 9000 series can hit 6ghz with a decent OC and cooling. But we’re probably 10 years away from 7ghz, it took a lot of time to get to 6ghz, while 5ghz is achievable on 10+ year old CPUs. But chasing clocks is physically pointless since it requires exponentially more power. Ipc increase is much more feasible.",Neutral
AMD,My 7950x hauls the mail.  But I’m over cooled.  Open loops WC with 3 big rads,Neutral
AMD,"3 years or more for am6. Zen6 will be on am5 and will release end of next year most likely. Zen7 is a toss up, but I think will be on am5. DDR6 ram will release around 2029ish most likely, so zen8 will be on am6 with ddr6. Unless am6 comes out on ddr5 but that would mean only 1-2gens before ddr6 and am7. Amd seems to do at least 3 gens on a platform.",Neutral
AMD,"if we are looking at REAL WORLD performance, intel i5 12400f is still the the king - userbenchmark",Positive
AMD,"So, I haven’t checked their site in years. I went there to see if they managed to find an “argument” against the 9800X3D and they in fact did. Crazy shit.",Negative
AMD,"I have a real time (max stutter I can survive is 100 ms) scientific application I wrote at work. I can't use Intel cpus for it, for the dumbest reason.  It has 13 threads and each thread is eating 40% of an AMD9950X. When I tried to use Intel cpus, it constantly had underflow faults because ... Intel cpus have 8 performance cores and *16* efficiency cores. When my high demand threads get scheduled on an efficiency core, it gets behind and slowly the whole system gets behind.  Amd cpus, on the other hand, are a simple flat 16 performance cores.  I would use server cpus for the job, but they're actually much worse than a 9950x. Server cpus have lots of cores, but each core is fairly slow. On passmark they're like half the capacity of a 9950x core. And because server cpus are so focused on IO, they're insanely expensive. I need a trickle of disk io and st most 16 GB of ram, so their cost is poorly justified.",Negative
AMD,I was researching getting a 9070 and was wondering if anyone else had noticed how weirdly petty user benchmarks gets lol,Neutral
AMD,"Bro I make my upgrades in December, it's the birthday month, first time around they discontinued 5800x3d but 5700x3d was not in the market by then. Next year they had already discontinued 5700x3d. Lol I just totally missed the apportunity to get one.  Now looking at ram prices no way I am moving to am5 anytime soon.",Negative
AMD,"Indeed, you might find a used one if you're lucky, if they are not on sale any ore in your region.   I believe that cpu is good for at least another 5 years if you're not after the highest of framerates with a 5090.",Positive
AMD,Honestly going up in resolution sounds like the least necessary moment to upgrade from your 5800x3d.,Negative
AMD,I might legit keep this cpu till it dies. With the games I play it seems like it could last me 10 more years.,Positive
AMD,I got a 5900x somewhat before it was released and regretted it. A few years later I corrected that mistake with 9800x3D. I have been most pleased.,Positive
AMD,"I had a 9700K instead of early Zen 1/+ lol. I totally would slotted a 5800x3D in my old system if I had AM4 but I just waited for Arrow Lake, saw how it was literally negative in gaming and went and got a launch 9800x3D instead.",Neutral
AMD,"I don't see much of a performance boost between the 5600X in my PC right now and the 5800X to justify buying that... X3d though, I do...",Negative
AMD,There's some stuff the 3d cache is great for. Like paradox games,Positive
AMD,"I had 3700X and a 3060TI, I have upgraded recently to 5800X and 9070XT. The CPU is not even getting half of usage, so pretty good combination IMHO :)  Playing at 2K on ultra on most games.",Positive
AMD,"GPUs are just getting bigger and more power hungry, which as a SFF enjoyer is frustrating.  However, my current build is a 5700x3d and 4080 Super, so I should be good for a while.",Negative
AMD,I had the same build until earlier this year! Got myself a 5080 Vanguard and 9800x3d,Neutral
AMD,Yeah all I really want to do is eventually upgrade my cooler from a corsair H100i and then ride it out for another few years!,Positive
AMD,We'll keep passing sticks around anyway like a defacto currency,Neutral
AMD,"Nah motherboard sales have crashed, very few people are building new pcs. Sure an argument can be made that consider we have monopoly we will reduce avaliabllity and increase prices. This never works in the long run, someone else is bound to come into the market.",Negative
AMD,Ddr5 ram price,Neutral
AMD,Remember when Intel promised that the Pentium 4 would do 10Ghz?,Neutral
AMD,"Funnily enough, ARM cores (stock, Apple, and Qualcomm) are all currently chasing clocks rn. But their cores are already very high IPC, so it might be harder for them to increase IPC than it is for Intel or AMD.   Unfortunately AMD and Intel have *also* been struggling to increase IPC with their most recent gens.",Neutral
AMD,"I’m on a NH D-15, thing never really makes it past 65c when gaming.",Negative
AMD,Hauls the mail? Haven’t heard that one,Neutral
AMD,That and several paragraphs calling any positive AMD news a bunch of Neanderthal fanboys,Neutral
AMD,"Pfft...if you look at the *actual* benchmarks and not the ones those AMD shills pulled out of their asses, the Celeron 300A is still clearly better than every AMD processor currently in production /s",Negative
AMD,It’s not the 9900K and 9400f anymore?!   Shocker.   Those must be number 2 and 3.,Negative
AMD,Actually how it this better? Performance per watt? Raw speed? Performance/price? Or jellyfin transcoding? Wonder how can this beat zen 5? Say for Ryzen 7 9700X I am using  For current RAM scarcity and for people that just need to browse internet and Microsoft suit. Probably is already too excessive of raw power.   https://www.cpu-monkey.com/en/compare_cpu-amd_ryzen_7_9700x-vs-intel_core_i5_12400f,Neutral
AMD,"I had an issue with Intel CPUs because they removed AVX512 😑 Months lost to realise the issue, with the lab work basically blocked.",Negative
AMD,We had software that had similar issues. Needless to say Intel was out of the picture on that project.,Neutral
AMD,It sounds like the fabled dual-X3C CCD would be the perfect CPU for you.,Positive
AMD,"Yea, definitely feel fortunate I pulled the trigger on it. Originally had a 5600 and jumped with the hype to a 5800x3d but it has lived up and more. I'm after high end visuals not insane frame rates since I'm on a 120hz OLED so it has worked out great.",Positive
AMD,"5800X3D works very well with a 5070 Ti in FS2024 in VR, I am continually impressed with how far they have been able to take AM4.",Positive
AMD,"Yeah I know, but I plan to donate the 5800x3d to my wife. She's using a laptop to game ATM but I want her battlestaion next to mine.",Neutral
AMD,You dont see any significant difference with x3d from 5600x.better pay for gpu ssd or ram with low cl upgrade,Neutral
AMD,Very true. My cores dont even max,Negative
AMD,That's because games generally don't use all cores. At 1440P you are most definitely getting CPU bottlenecked in quite a lot of games.,Negative
AMD,Well the fact AMD announced they were stopping the production on the B650 boards but last month now said they will carry on producing them because of the current climate says it all really,Neutral
AMD,"Normally yes, but these are not normal industries that can easily be broken into by startups. To be a ddr5 manufacturer you likely need a minimum 10k employees, 1000 of them highly skilled, billions in property, plant, equipment, and even then you are waiting in a line years long to buy the lithography machines which are only made by a few companies in the world. You could be building your plant and staffing while waiting for the lithography equipment but you could have 10 billion cash today and still need 3 years to produce your first chips. There is no one else to enter the market, those that could are bailing for AI $ or exited the bleeding edge market and can't catch up e.g. global foundries, IBM, TI etc.",Neutral
AMD,Mobos are so expensive anyways. I know they are good kit but still.,Negative
AMD,"Yes, in the long term, I suppose, most people will only use kind of terminals (monthly rent $99,99 + $39,99 for game-streaming) and only a few will have own computers (mostly old computers), but I am kind of paranoid.",Negative
AMD,"I do, lol.",Neutral
AMD,"""Advanced Marketing Devices""   Lol.",Neutral
AMD,Still the only known CPU that can overclock +50% of it's own frequency and still be rock stable.,Positive
AMD,They were quoting userbenchmark   They werent actually saying the 12400f is better,Neutral
AMD,"It’s a quote/sarcasm, but being this offended about your cpu is interesting. 9700x is good man, no one said otherwise",Positive
AMD,Intel used to have AVX512 when no one used it. Now that people do use it they took it out :(.,Negative
AMD,"How come it cost months? Can't you use avx256 instead? I am, for all my complex-valued math.",Negative
AMD,"You don't know if their work is cache intensive, though",Neutral
AMD,"Indeed. Can't beat an oled. I got a 45"" LG model a few years ago, absolutely amazing, even the 3440x1440 resolution is perfect for gaming (does not require such a powerful PC as 4k).",Positive
AMD,Very valid point!,Positive
AMD,"Maybe you should look up performance numbers, 5600x struggles with heavy ray tracing, I get cpu bottleneck. Where as 5800x3d handles them comfortablely. Just a simple Google search will be enough...",Neutral
AMD,What does it mean? That people are not buying new ones because they are expensive,Negative
AMD,"No one knows what the future holds but there are several issues with shifting all personal computational work to the cloud. It's very expensive, it's environmentally terrible and sure there not much regulation right now, because  policy changes take time, but at some point I feel there is will greater checks on how many data centers can be built and where.  The ai bubble, though I feel there are loads of cool innovations in this space, will at some point burst.. Things may look very different after that..",Negative
AMD,"So you just decribed Stadium, the failed business venture as at the end of the day, computing is cheap but fast and reliable internet is extremely expensive. No, we will not have monthly subscription terminals, no one wants those. People will sooner go back to consoles",Negative
AMD,"this is just how everything works in the history of computing: a pendulum between local and remote.  from thin clients and datacenters to personal computerà, from local to Cloud and back again.  i wouldn't be surprised of we flop to think clients.  Also for the price of a whole new gaming rig you would be fine for years of remote instances.  edit: i can't write.",Neutral
AMD,Fucking kek. Needed a good laugh tonight. bless,Negative
AMD,That's honestly more on the manufacturer leaving that much headroom than the chip being insane.,Negative
AMD,"Hmm...   I had an Opteron 146. Clocked it from 2ghz to 3ghz, and ran it like that, no issues. Switched it out with an Opteron 165 that i clocked from 1.8ghz to 3ghz. But because of the locked multiplier, that needed 333fsb, which not many mother boards could do. Could push it to around 3150 for benching, but for gaming i usually just settled for 2.8 something ghz. 2.7ghz was just as easy as 3ghz on the 146. So an Opteron 170 was the more popular choice at that time because of the 10x multipler rather than 9x.       Core 2 duo was also an easy 50%+ overclock btw. And before the 300A, 486 40 could easily be changed to 66mhz...    But yeah, the 300A isnt legendary without reason.",Neutral
AMD,Indeed!,Neutral
AMD,"Your gpu card struggles with ray tracing. My 5800x was a significant upgrade for me from 3600. Didnt occur to get x3d then. X570 board, low CL ram, good non ramless ssd all have better impact.",Positive
AMD,">5600x struggles with heavy ray tracing  ""heavy ray tracing"" is purely a GPU load and uses less of your CPU because your fps tank into nothing even with a 5090",Negative
AMD,"Stadia mostly failed because Google did an absolutely horrible job at marketing it and you had to get a subscription and still had to buy most of the games on top of that.  Since shrinking nodes will get more expensive relatively to the past, so computing power will not be cheaper in the future either.  I think streaming will become more popular in the future, as better internet is becoming more popular all over the world in general as well.  Do I like it? No, but it think that's what the situation is realistically.",Negative
AMD,"Remote processing of 360fps 8k screen data? I don't think so.   Also, most edge devices nowadays are not even tied to the wall.",Negative
AMD,Me seeing a significant increase in RT games going from a 10700K to a 7950X3D with the same GPU says otherwise.,Neutral
AMD,"You’re wrong, RT will most of the time tax your CPU a lot too. The GPU is definitely the primary compute, but CPU will matter a lot too.",Neutral
AMD,6700xt entry level ray racing. Almost got one. 5600x should only struggle with 3080ti and above,Neutral
AMD,"RT uses the CPU to build the bounding volume structure/hierarchy, the more objects you put there, the more the CPU has to work.",Neutral
AMD,typical average scenario LOL,Neutral
AMD,"7950x3d on what board and cooler?. On many ratings for single thread games, it performed significantly lower. . Am5 ddr5 setup and clockspeed. Ssd play more significant role.",Neutral
AMD,6700xt..cpu 5600x should be never be bottleneck,Neutral
AMD,I sure hope so it will be very soon.   Because why else to upgrade from now?      AMD started promising proper 8k support 5 years before Zen.,Positive
AMD,"Does intel 10A still come out as scheduled in 2027? I googled it and found out intel said the 10A will come out in 2027, but this was old news in 2024.",Neutral
AMD,"GFHK also has 14a for Razor and Coral Rapids in 2H 2027, so I'm taking what they are saying with very little credibility.   Plus, we had very similar rumors during 18A, and that went nowhere. Fool me once...",Negative
AMD,I wonder how intel and other companies are going to manage for next year? Prices for memory and SSD’s are predicted to go even higher putting off many buyers from getting a new PC build or laptop.   This makes me concerned Nova Lake won’t sell as well because of this.,Negative
AMD,It's shameful to see LBT posing with 14A wafers when all the groundwork for this was setup by Pat Gelsinger. The entire Intel board should have been sacked instead of Pat.,Negative
AMD,Unbelievable till official announcement,Neutral
AMD,can't they use it to make more ram ?,Neutral
AMD,good news,Positive
AMD,"Lisa So Sue Me wants a taste of the Lip? Am I living in a different dimension? I callled out So Sue Me on X, is she jumping on Big Blue’s Back?  Is anyone Dollar Cost Averaging INTC? It will still be awhile before IFS is firing on all cylinders. The Lip said he would stop high end chip production for external customers (If No One Took A Byte) in order to get $$$ to build out Ohio Fab.   Let’s get it done. I’m driving distance from the Ohio Fab, any chance Intel will give me a tour?",Neutral
AMD,"If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  [https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots](https://www.tomshardware.com/pc-components/cpus/intel-puts-1nm-process-10a-on-the-roadmap-for-2027-aiming-for-fully-ai-automated-factories-with-cobots)  *""Intel's previously-unannounced Intel 10A (analogous to 1nm) will enter production/development in late 2027, marking the arrival of the company's first 1nm node, and its 14A (1.4nm) node will enter production in 2026.*  ***\[Edit: to be clear, this means 10A is beginning development, not entering high volume manufacturing, in 2027\]*** *The company is also working to create fully autonomous AI-powered fabs in the future.""*",Neutral
AMD,10A & 7A are in R&D phase,Neutral
AMD,It's gonna be 2026 soon and 18A is launching at the very start of 2026. A double node shrink in like 2 years doesn't exactly sound very possible.,Neutral
AMD,"14A probably won't be ready for 2027, much less 10A.",Neutral
AMD,"Remember, these are just names/nicknames. 10A? The difference between 14A and 10A is probably equivalent to the difference between 14nm and 14nm+",Neutral
AMD,And yet here you are.,Neutral
AMD,"Brother, don't hint at your place of employment when you have your full face in your profile as well as you commenting in NSFW subs.",Neutral
AMD,There will probably still be another of layoffs next month 😂,Negative
AMD,"Yes, perhaps it’s better if you post it on the r/intelstock subreddit instead 🤪",Neutral
AMD,"The entire Intel board probably should have been sacked, but Gelsinger as well. He failed at his main mission and drove the company into a crisis. That kind of thing should have consequences.",Negative
AMD,Who was it that decided to exit the SSD business.  They sold off a cash cow for pennies on the dollar.,Negative
AMD,Nvidia is at least some what believable. AMD though?,Neutral
AMD,"I thought that too. At least they'd have some money coming in. But apparently it takes years to rejig the plants to churn out RAM instead of CPUs. And they're heavily invested in getting the next gen CPU fabs working.   Pivoting to RAM just doesn't make sense, unless they magic'd up a new type of RAM that's cheap to make and has super low latency - which is one thing I've always thought they ought to do.   Imagine if external RAM ran with super low latencies like CL1 or CL2 or something. You wouldn't even need branch prediction and prefetch and massive caches in the CPUs.",Neutral
AMD,"""news"" needs a lot of quotes around it...",Neutral
AMD,This isn't wallstreetbets. We don't talk like that here.,Negative
AMD,">If you are referring to an article like the one linked below, they later clarified that 10A was supposed to begin development in 2027, not production.  Yup, and to make it even more obvious, the same graph also has Intel 14A showing up early 2026, and 20/18A showing up at the start of 2023*,* so clearly it's not the date of when the node is going to come out (or even start HVM).",Neutral
AMD,"Dunno why this is being downvoted, the CEO of Intel himself said that 14A is a 28-29 node in the Q2 2025 earnings call.",Neutral
AMD,enough info about how intel names products exists to know. if it didn't increase in transistor density per mm it would not be called 10A.,Neutral
AMD,"I think everyone knows there will be continued Q1 and possibly Q2 layoffs.   Return to office didn't lead to enough voluntary attrition. Leadership wants to hit a magic number which sounds good for financial reports, not what is actually viable to run things.",Negative
AMD,The thing intel is doing rn is literally pat's groundwork isn't it?,Neutral
AMD,Still a tall order imo unless it's some defense chip for RAMP-C,Neutral
AMD,"If they're following industry standards I'd say it depends on how good AMD's next gen is. Intel doesn't need direct access to AMD designs to etch chips for them, and designers make way more than per wafer than foundries do.  If AMD has superior designs to intel again they could finally ship out some damn chips for laptop OEMs. It would hurt intel more than the revenue would benefit them imo since client has really been carrying intel for the last six years and demand for AMD chips has been high despite the drip feed of strix chips. honestly I'm considering an AIO/NUC/whatever the new name is with strix halo and unified LPDDR5 to upscale old footage without having to use my daily desktop. imagine if it was available at scale.",Neutral
AMD,"they don't have to make faster ram, just make it, right now, some ppl don't really care about speed",Negative
AMD,"So, risk production in late 27/early 28 and HVM in 2029 I suppose?",Neutral
AMD,YEs it is. He did make mistakes. He was hiring like crazy at the beginning of his term. And he should have started cutting sooner. But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.,Negative
AMD,"Nothing they're doing *right now* is a success story. Remember that they don't actually have customers, and that is first and foremost what got Gelsinger fired. As things stand, the foundry as a whole is a failure. If things turn around, that will have to be under Lip Bu.",Negative
AMD,I wasn’t aware 14A is part of the RAMP-C initiative. I thought it was only Intel 16 & 18A that are currently covered by RAMP-C?,Neutral
AMD,I think so. Maybe optimistically we see a 14A product in late 28'.,Neutral
AMD,"> But he doubled down on EUV lithography and tried to get orders in for the most advanced litho machines ASML made before TSMC started buying those machines. This is why 18A and 14A even exist at Intel.  No, that was just more wasted money. 18A doesn't even use the high-NA machines Intel bragged so much about. It seems they tried blaming their struggles in foundry on the equipment instead of the broader org culture and talent.",Negative
AMD,It can expand in future ? My point is how can we believe such stuff at face value without actual proof.,Neutral
AMD,14A does use the High-NA machines. They didn't buy them with no plan to use them That would be stupid.,Negative
AMD,"It can expand in the future but this is a trial, it’s not yet a long term commitment until the outcome of the project is known (final evaluation won’t be until 2026/2027). 14A is not part of RAMP-C, it’s still in phase III trial with 18A. There’s been no additional RAMP-C design calls via NSTXL that I’m aware of",Neutral
AMD,"> 14A does use the High-NA machines. They didn't buy them with no plan to use them  They bought the very first high-NA machines, claiming it was for 18A. Now they won't be used until a node that hits volume in '28/'29, by which point TSMC will have (or rather, already has) much better machines. So what exactly was the point?  > That would be stupid.  Is that not a perfectly apt description for Intel's foundry strategy in recent years? It sounds like they really drunk the coolaid with their attempts to blame the 10nm failures on the lack of EUV.",Negative
AMD,14A will have volume production in 2027.,Neutral
AMD,"Lip Bu himself is saying '28-'29. At this point, there isn't a chance in hell it's ready for volume in '27.",Negative
AMD,"The fact 890M is that much faster than 140V shows this benchmark is terrible anyway. In real gaming performance, 140V performs very close to 890M and does so at usually superior efficiency.",Negative
AMD,Is panther lake on the intel process considered better perf than lunar lake on tsmc process? Or is it lateral,Neutral
AMD,I hope it comes to desktop CPUs,Positive
AMD,Almost 7600m performance ie stream machine. From a igpu . Hoping a handheld with this igpu under 1000usd,Positive
AMD,Yeah this headline doesn't add up based on my own testing,Negative
AMD,"yeah it says   ""We should also make it clear that these benchmarks seem to undermine the performance of Intel's Xe2 architecture. The Arc 140V is shown much slower than the Radeon 890M, but in reality, it ends up close to or faster in actual games. So it looks like this benchmark suite is not optimized for older Arc GPUs, but the new Arc Xe3 architecture is doing well, and we can see further improvement once the finalized drivers roll out.""",Negative
AMD,Yeah that's a strange result. Makes me think the 16% will be for PL improvement over LL.,Negative
AMD,"So imagine how much faster it is in actual practice.  These iGPUs Intel are putting out are great, it's a good time for lower-power handhelds!  And insane power handhelds too, with Strix Halo getting in them, the Ryzen 388 (8c16t with 40CU iGPU) allegedly coming, and I'm sure Intel is working on an answer to Strix Halo which if it uses this kind of uArch, will probably be deadly.  Good friggin times.",Positive
AMD,concur  some benchmarks are biased,Neutral
AMD,lateral,Neutral
AMD,If it’s just “16% faster than 890m” it’s nowhere close to 7600m. You have to be over twice as fast as the 890m.,Neutral
AMD,"Isn't 140T also faster than 140V in benchmarks, despite being Xe+?",Neutral
AMD,"Yeah the 388 makes a lot of sense, a worthy sacrifice of a cpu tile for cheaper more efficient gaming cpu.",Positive
AMD,Answer to strix halo was the partnership with nvidia,Neutral
AMD,Did is you see the link? Passmark graphics score? 10999 for 7600m and 9500 for b390.,Neutral
AMD,"since the 140T has 20 watts for the GPU itself, how can it be otherwise?",Neutral
AMD,"I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.     I mean, if they actually launch something, cool. But as of now, we don't really have any information that directly points to a competing product.      In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.",Neutral
AMD,"I mean no offense, but Passmark is irrelevant.  Even comparing the 890m to the 7600m (non-xt), the 7600m is usually twice as fast, with dips down to ~60% faster, and lifts up to ~170% faster.     NoteBookCheck has an extremely robust dataset of benchmarks in games for both the 890m and the 7600m (non-xt) at various resolutions, and they show not only a clear winner, but a very large difference in the performance of these devices.      Now I'm not trashing what the B390 will be, because we need an iGPU fight here.  But thinking that the 7600m (non-xt) is only ~15.7% faster than the B390 ((new-old)/old gives percent change) because of Passmark is erroneous.",Neutral
AMD,">I feel like the partnership was an answer to a much broader question concerning both companies, none of it really being an implicit answer to Strix Halo beyond a vague promise to develop custom chips with Intel cores and RTX cores fuse together using nvlink.  They explicitly talk about a client product that will have Intel cores and Nvidia iGPU tiles. It's not especially vague.   >In fact, I might be crazy but I feel like it is more likely that the actual answer to Strix Halo will be all Intel silicon, because Nvidia is very horny for all things ai and all things datacenter.  Despite that, Nvidia has already provided a custom iGPU tile for their Mediatek + Nvidia iGPU solution.   They have both the resources and financial incentive to do this. Plus, this should be better than any all intel silicon solution anyway.",Neutral
AMD,Yup and Jensen himself said the high powered SOCs is a $30 billion untapped market,Neutral
AMD,"I guess we'll see more when we get actual info about the potential devices.  Right now, I haven't read about a device coming to market.",Neutral
AMD,Back in the day you could overclock a 2600k from 3.4Ghz to 4.5Ghz on a $25 Hyper212 cooler. The performance gains were incredible as Sandy Bridge scaled very well at higher clocks.   Now days CPUs come overclocked already.,Positive
AMD,"""It's crazy to think that a cpu from 2009 can be easily overclocked.. 2.9Ghz to 4.1Ghz is crazy !""  You could overclock huge amounts on earlier generations - I used to run Pentium 4 1.6GHz chips at 3.2GHz on air-cooling, more on phase-change cooling.",Neutral
AMD,"I ran my i5 750 2.67Ghz for years at 4Ghz without any issues. I benched it some at 4.2Ghz even, but it was not fully stable.  The X58 CPU are even better tho. And even if you had insane OC potential back in the days it was not as good as it sounds, since the turboboost was higher than the stock frequency that is listed.",Negative
AMD,"Lol a 15 year old computer running Windows 11, meanwhile Microsoft telling people to upgrade 5 year old laptops for win10 being EOL.",Neutral
AMD,X5690@4.6GHz on Rampage III Extreme 😘,Neutral
AMD,it is crazy that intel sold you same technology at downclocked speeds to make a nice model range with different prices.,Negative
AMD,Sick stuff. I still got my i7 930 at 4.2Ghz running just fine. These types of chips overclock like crazy.,Positive
AMD,Be nice. Give it another stick of ram!,Positive
AMD,"Cool. Glad it worked for you. I have dual xeon server, maybe i should try it. But its production server dont wana break my apps. Lol",Positive
AMD,My 2500k did ~4.8 ghz and my 6950x did 5.2 ghz. Its base clock was like 3.2ghz and this was using 128GB of quad channel DDR4.  It was “stable”,Neutral
AMD,45nm is crazy in 2025,Neutral
AMD,Q6600 G0,Neutral
AMD,500W power draw when,Neutral
AMD,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Neutral
AMD,"I used to run my i3-540 at 4.2GHz, air cooled on what is effectively worse than a Hyper 212 Evo. I miss the old days when I could overclock the snot out of them. These days I guess they're binned to almost their max potential out of the factory so most of the time I'm undervolting them.",Negative
AMD,Well done. Still using two H55m machines with OC (x3450 and i5 661).  They also OC decently at stock voltage keeping turbo and all power savings. My X3450 does 2.6 -> 3.3Ghz(3.8 turbo). The advantage is that it idles quite low at 50-60W.   But for gaming and rendering it's better to go all in as you did. Most chips can do anywhere from 3.8 to 4.2 all cores IME.,Positive
AMD,nah my 40 logical processors would smash through it all  x2 xeon e5-2680 v2,Neutral
AMD,"is that better? I dont need to dive into setting anymore, the CPU maker do it for me with warranty.",Neutral
AMD,"There is still more to work with, especially if one does not fossilize on static all core OC, but does 2-step TVB fueled dynamic OC, Ecores are Aldo the source of much happiness on arrow",Neutral
AMD,I miss overclocking. Felt like you were getting a bargain. Now I don’t even try.,Negative
AMD,"Not as big an OC as yours, but I had a pre-built from FutureShop.  It was their home brand name.  Found a BIOS for the board that wasn’t theirs.  Managed to get 3.2GHz out of a 2.4GHz Pentium 4 on pre-built from FutureShop cooling.",Neutral
AMD,"Wow, soo cool",Positive
AMD,The motherboard doesn't accept other stick of ram. Only my corsair ram work,Neutral
AMD,How did you even get a 6950x to boot at 5.2ghz? Most of them hit a wall around 4.3ghz,Negative
AMD,"Has its ups and downs. Now that I'm older and have less time to tweak things and mostly just want shit to be stable, I see ""pre-overclocked with maybe 5% performance left on the table"" as a pro. The con is that chipmakers just jam a ton of power through it to make it happen, and the option of buying a half-price chip and spending an entire sleepless weekend tweaking it yourself to get 95% of the more expensive chip's performance is gone.",Negative
AMD,"Same - the complexity and heat rose a lot and the gains because less significant - with multi-core chips and turbo frequencies there just isn't much headroom in them.  That and I work fixing issues with computers all day, I just want my own PC to work.",Negative
AMD,"Thats because these older CPUs were surprisingly energy efficient. Also mostly because now modern CPUs are powerful enough where overclocking is pointless. Even my i3-12100 being overclocked would be pointless, even if its only a 80 watt CPU",Negative
AMD,He couldn't without LN2.,Neutral
AMD,"It was short lived, over ~7 years I had to pull back the multiplier from 52 to 44 to keep it stable.  I retired the system this year.  It was a full open loop from EK.  2x Pascal Titan X in SLi",Neutral
AMD,"I used to overclock everything, now I undervolt everything lol",Neutral
AMD,"The most important characteristics of a laptop are battery life ( power efficiency) and screen quality. That is what sells ( non apple ) clamshells.    All the other crap they test in various reviews are mostly meaningless to the actual user. There is a very small %age of the market for high power/perf laptops and even smaller market for gaming.   Most of the reason a laptop is “slow” is bloatware and has nothing to do with the cpu choice anyway.  And Intel is already “beating” AMD in laptop cpu sales, by a substantial margin. People incorrectly assume AMD has most of the market share in all segments because of the very noisy and super-biased gaming reviewers, who mostly focus on $3000+ gaming desktop builds. Yes AMD is handily winning there.",Neutral
AMD,"I have a T14s Gen 5 Core Ultra 5 135U and recently I saw \~9h of battery life, browsing websites. It's quite nice piece of hardware so with Lunar Lake it would be just perfect. Of course not for heavy workload because for this we will have Panther Lake. I work in tech for years, not an expert in laptops area but I can assure you that new CPUs from Intel that I mentioned earlier are way, way, way better than previous generations.",Positive
AMD,"I've gotten one and honestly it's amazing, easily the best laptop I've ever used so far.   I was skeptical about the battery life claims but I've genuinely found that using it for about 8 hours straight for coding, only drains the battery maybe 50%.  I've set it to only charge up to 80% max for battery health conservation, and I've regularly coded for 12 hours straight on the medium performance profile and haven't needed to charge until I got back home.  (This is for the Ultra 7 258v cpu variant btw)  Also this is while running Fedora with KDE Plasma which makes the battery life even more impressive as it's one of the heavier distros running cutting edge hardware and I've heard that Linux has less battery optimization compared to windows.    Screen isn't anything to write home about but the 100% srgb one looks good enough and is bright, 60hz looks kind of bad but I know that it saves a lot on battery.   Keyboard feels very nice as far as laptop keyboards go, having it be easily swappable is lovely as I wore out the keys on my old laptop, and I want this thing to last.   Linux hardware compatibility is perfect so far, even the fingerprint sensor works out of the box on fedora.   My only real complaint is that the plastic it is made out of is a major grease magnet and if I touch it without having immediately washed my hands, even if my hands weren't dirty, it'll leave dark patches from oils. Also it would be nice to have swappable RAM but I think 32gb ought to last a very long time anyway.   Genuinely seems like arguably one of the, if not the, best laptops for actually getting work done. Maybe it's not as fancy or sleek, but it just works. It's like the 2001 Toyota of the laptop world, it's not winning prizes for looks, but it'll never die, gets good mileage (battery life), and is easily repairable. Maybe not the laptop you want, but definitely the one you need (excluding people who need something like a dedicated GPU or really need super high CPU performance).",Positive
AMD,"Intel beats AMD in software (drivers, firmware) … I got think pad 780M laptop by company I work for. Randomly display won’t get detected. Randomly audio device goes missing. Not fun thing to reboot your laptop and miss 10 minutes of meeting.",Negative
AMD,"Soldered RAM sucks.  Nothing beats popping out the standard 8GB stick(s) a notebook may come with, installing a couple 2x32GB sticks yourself and having it actually work.",Negative
AMD,lol. Even in the cons it says weaker multicore than AMD….?   This article seems like AI wrote it,Negative
AMD,"Unfortunately Intel abandoned the on-package RAM after Lunar Lake again, which is the primary reason for the great efficiency and low power usage. I kind of understand why, it's expensive and not very flexible, plus apparently the market doesn't actually care that much about long battery runtimes. Only a small minority of people are ready to pay premium for this.",Negative
AMD,At work we are a Lenovo shop and recently swapped from Intel to AMD T14 laptops. Too many issues with Intel and the AMD models offer the same performance for less money.,Negative
AMD,Suck at gaming.,Neutral
AMD,Try a modern mac and tell me its not the CPU holding the UX back. It's all about the cpu's.,Neutral
AMD,"That would be nice. We have a bunch of laptops with Intel's i5, 13th gen I believe it was. 2 p-cores, some e-cores. They are all slow as fuck. I mean it. The CPU is so extremely slow and goes into tdp limit right away. Most users hate them.  Battery life is ok, but they are really bad in terms of performance.  So - I wouldn't say CPU doesn't matter.",Negative
AMD,"AMD is held back in laptops by some shady deals of laptop manufacturers with Intel. It's impossible to get a 4k AMD laptop with 5090 for example, all of those are Intel (I found literally one AMD laptop like that compared to 25 from Intel). That's utterly ridiculous.",Negative
AMD,Isn’t the keyboard one of the most important characteristics?,Neutral
AMD,"Yeah, so much of laptop performance is dictated by things other than the cpu. Its kinda wild.   Intel does a way better job getting good laptop designs. Amd historically has just been a cpu seller, telling oems to go wild and do whatever . . . And it always ends very badly.   The biggest sign amd is taking share is not cpu benchmarks, but will be things like having premium screens, good thermals, lack of bloatware, dual channel memory, good SSDs, good colors, etc etc. and . . . ACTUAL AVAILABILITY. I dunno how many reviews i see where they review and intel and amd parts. Usually there is some way intel has a better premium finish. And then amd just has zero ways to actually buy their model. Its the weirdest thing.",Negative
AMD,"Hey I’m looking at the exact same laptop that you have. Can you tell me about the build quality and if there’s any keyboard flex when pressing down on it? Please tell me. I’m going to use it for word, excel, reading lots of pdf files and ebooks and watch movies. Will it be enough for that?",Neutral
AMD,"But haven't you heard, AMD beats Nvidia slightly at linux gaming benchmarks.  That means AMD has the best software support.",Positive
AMD,"I miss the times when laptops were far more upgradeable. I got a budget laptop for college with a low-end dual core, a spinning HDD, and 1 stick of 2GB RAM. By the time I retired it ~6 years later I've upgraded the CPU, replaced the HDD with a SSD, and added 2x4GB sticks of RAM. I also could've swapped out the network card and even the DVD drive for another SATA drive, but never got around to those.",Neutral
AMD,Soldered ram is a lot faster. So no.,Neutral
AMD,Yes but now RAM costs a ton of money,Neutral
AMD,Is multicore performance the only consideration when buying a laptop?,Neutral
AMD,What kind of issues?,Neutral
AMD,What kind of issues with Intel? I thought it was the AMD that had tons of issues,Negative
AMD,It is not a gaming laptop,Neutral
AMD,"Yep. More specifically, it's mostly the single thread performance and efficiency.  It's how a MacBook Air can be fanless, run super fast, and stay cool at the same time while you're doing work with it.",Positive
AMD,It's an enterprise grade product you buffoon.,Neutral
AMD,Exactly. Thinkpads are not targeting average Joes. They are targeting business and enterprise customers. Their Yoga and Ideapads are targeting the regular Joes.,Neutral
AMD,Build quality.    Thinkpads are solid machines that are easy to fix.    It's one of the few laptops that comes close to MacBook quality and everything judt working.,Positive
AMD,"So build quality will be subjective, from what I can tell, it's got very good build quality in terms of ""real"" factors such as durability. But it definitely feels less ""premium"" than similarly priced consumer grade laptops. The plastic is plastic so it will flex a little bit, but the parts all seem very well put together and it does feel ""solid"" overall.   I haven't really noticed keyboard flex, but I have noticed a slight amount of flex where my palms rest, particularly on the right side, where the smart card reader is, which makes sense as it is just a big hole in the side of the laptop. I plan on getting a dummy smart card to fill the gap and hopefully that should reduce it.   Overall whilst the internal chassis is metal, the outside is just plastic. I imagine that is good for durability, as it ought to be able to absorb shocks, but, as I said, it definitely makes it feel less ""premium"". They key press feel of the keyboard definitely does feel very nice as far as laptops go though. Obviously it's still nothing compared to a good mechanical keyboard but for a laptop it's very nice.   I bought this laptop for longevity and durability, so given that It's only just come out, I can't really say much about that, but the prestige of thinkpads of previous generations kind of speaks to their reliability. Plus it's apparent that they are still quite easy to repair and Lenovo has video guides on replacing loads of the parts.   And for your use case the battery life should be very good. It seems the Intel chip was designed to be very efficient during periods of downtime and something like viewing a PDF or editing a document has a LOT of downtime for the CPU",Positive
AMD,"Because only on Linux, Valve heavily funds AMD driver development and they also get other community contributions. The commonly used RADV vulkan driver was started as a community effort without AMD involvement.",Neutral
AMD,"I hope you're joking(sorry if you are), cos Nvidia isn't actually a good benchmark for software support on Linux. Intel is so much better at Linux software support than Nvidia",Negative
AMD,"The question is, is the soldered ram you're buying in a laptop faster than the ram you can buy and install yourself?",Neutral
AMD,All the more reason to make it upgradable,Positive
AMD,Lunar Lake isn’t how Intel beats Amd lol. Panther lake will stop a lot of the bleeding for sure. AMD is so far making mostly good moves and Intel is as well with LBT. The goal for Intel over the next 3 years is stop losing customers base. I will point out Intel still has about 75% of all x86 customers.,Neutral
AMD,"I mean, the only other pro is, that you cannot get the 2.8K panel on the AMD version for some reason..... sooo",Negative
AMD,This was back when the 14th generation were having issues.,Neutral
AMD,"It's $2,000 so no excuse.",Neutral
AMD,Thank you so much for this valuable and comprehensive information! I really appreciate it:),Positive
AMD,"Usually yes, soldered ram will be faster. And in case of lunar lake, it is faster and more efficient due to it being packaged with the cpu. Like Apple's unified memory.",Positive
AMD,"Lunar Lake already beat AMD, nobody buys AMD laptops",Neutral
AMD,i stopped at $2100 for a Thinkpad T14,Neutral
AMD,"Ah okay, got it thanks.",Positive
AMD,wrong  Nobody Supply AMD laptop     There fixed for u,Negative
AMD,"I do, and many of the people I know do.",Neutral
AMD,Nobody pays that much.,Neutral
AMD,"Not anymore, there are plenty of AMD laptops on the market, of course - depending on region.",Neutral
AMD,Try the shunt mod,Neutral
AMD,"Cool, errr...  icy",Neutral
AMD,"I used to work in an inter chip testing lab in Ronler Acres Beaverton. We would test them in an oven, test them at room temp, and test the chips with liquid nitrogen. Cold had the highest failure rate, hot had the highest success rate.  Chips are designed to love heat.",Neutral
AMD,Great work man. Brings back the memories from the good 'ol early 2000's.   You need a power mod and more voltage.,Positive
AMD,"mine with B570, everything stock, no any mod   [https://imgur.com/a/i5wVgi4](https://imgur.com/a/i5wVgi4)",Neutral
AMD,did you use dry ice? how did you hit sub-ambient?,Neutral
AMD,I ordered a steel legend B580 and I’m going to clamp an LN2 pot to it and run it on Dice to see what happens. Probably going to shunt mod it as well because of what you did. Might as well add an Intel card to my mix and see what it does.,Neutral
AMD,Are you in the US? If so how were you able to get Maxsun?,Neutral
AMD,Oh... for sure 😁,Positive
AMD,"I know! If I could of modded the power table I would have, shunt mod is on the ""card"" for sure.",Positive
AMD,Great work dude! Only 200MHz to go 😉,Positive
AMD,Car coolant in the freezer 😁,Neutral
AMD,That's the way! Let us all know the results.,Positive
AMD,I am in Australia.,Neutral
AMD,I di it myself but it seemed to only add like 20% more power not really the unlimited I expected.,Neutral
AMD,"Yes, but car coolant doesn't enable sub-zero. What else did you have in the freezer, how was the liquid cooled?",Neutral
AMD,Should be here in a few days and I’ll tear it down and prep it. I’ll see how it goes when it’s down to -70c and do some scores and then shunt it. I think it can probably handle 30% more wattage just fine. My HWBOT is Forsaken7. I’ll let you know.,Positive
AMD,So do you have outlets in Australia where you can buy Maxsun Gpus?  Does Maxsun have an outlet in Australia where you can RMA to?  I am in the US and I want to look into getting the dual Arc card with 48gb of vram.,Neutral
AMD,"Okay, household freezers get to -18C. Water freezes at 0C, antifreeze freezes at about -25C. So, car coolant in a freezer will get to and hold -18C while staying liquid. So, that's how I did it.",Neutral
AMD,"Just be aware (from my experience anyway) when Intel crashes it doesn't just crash the driver, it crashes into a full reboot, almost every time. It's a real effing pain in the arse.",Negative
AMD,Oh! You put the car coolant to run through a freezer? Wow! Nice,Positive
AMD,And largely against the non-x3d lmfao.,Neutral
AMD,Aren't they just showing that AMDs CPUs are better for gaming?,Neutral
AMD,"Whoever downvoted my comment, why not run your own benchmarks and compare your results with mine? You could make posts asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming. Or how about go to tech sites and tech tubers asking them to provide their benchmark scores. its disappointing  to know people prefer to stay in ignorance and prefer people getting scammed over knowing the truth, just face it reviews adjusted to satisfy the sponsor how do you think they make money.",Neutral
AMD,"I haven’t tried ARL processors yet, but based on my own tests, 14700K with DDR5-7200 is 30–40% faster in gaming compared to Zen 4/5 non-3D chips. In the five games I tested, 14700k matched 9800X3D in three, lost in one, and won in one. I have no idea how tech sites and tech YouTubers show non 3ds as being just as fast or only slightly behind when they are actually far behind.  14700k vs 9700x/7700x  [https://ibb.co/cSCZ6fdX](https://ibb.co/cSCZ6fdX)  [https://ibb.co/999WzW4T](https://ibb.co/999WzW4T)  [https://ibb.co/Gjw1nXX](https://ibb.co/Gjw1nXX)  14700k vs 9800x3d  [https://ibb.co/FbMNL40q](https://ibb.co/FbMNL40q)  [https://ibb.co/8nQXpYZ5](https://ibb.co/8nQXpYZ5)  [https://ibb.co/vvcpKsGY](https://ibb.co/vvcpKsGY)  [https://ibb.co/67Jn8tKr](https://ibb.co/67Jn8tKr)  RE4  9800x3d 179FPS  14700k 159FPS",Neutral
AMD,"i dont think proving you can do more work faster is going to turn around sells. more people exclusively game or game and do light work, than those who do heavy work at home on computers.  i dont relive there are enough youtubers and streamers out there to capture as there are general gamers",Negative
AMD,"I mean, yeah. A 9800X3D costs way more than 265K, while a 9700X is within 10 bucks of it.",Neutral
AMD,I assume they compared with CPUs in a similar price range,Neutral
AMD,I don't think these slides are as good as intel thinks they are. They are just an advertisement to buy x3d ryzen cpus lol.,Negative
AMD,I guess the point Intel is trying to make is that it's great for gaming and much better for content creation (aka work). It's also cheaper.  X3d CPU's are only better for gaming (and not in all games). Doesn't mean they are crap but you pay a premium for x3d.,Positive
AMD,"if you think you're 100% correct - go out and buy an x3d and run the tests, post a video with the results, and put an amazon affiliate link in the comments for a 14700k build that overall is better than an x3d.  there's nothing wrong with the 14700k, it's a beast. i think there's a few select games where it even beats the 7800x3d. but there's a reason that you can find hundreds of youtube channels with sub counts up and down the spectrum from 0 subs to 10M subs showing hardware side by sides with the x3d's beating the 14700k - and most of them don't have advertisers to please.  in my opinion, you're getting downvoted because you don't want to provide proof, you're out here saying it's tech tubers fault that you have no proof. you can also find a good amount of open source benchmark data at openbenchmarking.org, it's not pretty - but there's useful data there as well.  but hey - you're only hurting yourself by not breaking open the youtube techfluencer conspiracy around AMD processors. go buy an x3d, do all the testing, and use it as a springboard to free us from the lies of big tech. i'd love for you to be correct, but until you can provide proof, it honestly just looks like you have buyers remorse with your 14700k.",Neutral
AMD,">asking others what scores they’re getting in games. Or If you can afford it, CPUs like 7700X, 14600K, or 14700K aren’t that expensive you can buy them and test for yourself. Just a few games are enough to show RPL processors are far ahead of Zen 4/5 non 3ds CPUs in gaming.  [but it's not true?](https://youtu.be/IeBruhhigPI?t=13m3s) 1%low on 9700X is identical to 14700k/14900k, and better than on 14600k, AVG. FPS is slightly higher on 14700k/14900k, or is 9.7% higher average FPS on 14900k considered ""far ahead""? Plus, 14900K is not a CPU for gaming, it's a CPU for production workloads plus gaming, which makes it noticeably more expensive than 9700X, so realistically we should compare 9700X to 14600K/14700K.",Neutral
AMD,"Cherry picked results or just outright fake.  A 14700k isn't 30-40% faster compared to zen 4/5 non-3d chips.  Come up with a decent game sample first first before coming up with stupid conclusions.  Every reviewer I've found just happens to have different results than yours so everyone must be wrong except ""me"" right?  Even if the 14700k is slightly faster it still consumes a lot more power so theres that.  The numbers you can find from reviewers point to the 9700x being around 5 % slower, and you here are talking about 30-40% lul, if it was 30-40% faster it would be faster than a fking 9800x3d lmao.",Negative
AMD,I just built a 265K machine with 64GB 6800 RAM and it is no slouch. I haven't really played much on it yet as I've been working out of town a lot lately but my 4080S is now the limiting factor rather than the 10850K I had before.,Positive
AMD,RE4  9700x vs 14700k   [https://www.youtube.com/watch?v=FcDM07sgohY](https://www.youtube.com/watch?v=FcDM07sgohY)  [https://www.youtube.com/watch?v=-WO0HqajShY](https://www.youtube.com/watch?v=-WO0HqajShY),Neutral
AMD,"I did the same comparison, but it was 13900k vs 7800x3d and the Intel was much smoother. It's sad that you are getting down voted by bots who have never tried it and repeat tech tuber numbers. It's sad because ultimately the consumer is the loser when they are believing in lies.",Negative
AMD,"Now install windows 11, lol",Neutral
AMD,"> I haven’t tried ARL processors yet, but based on my own tests  Then whatever you're about to say has 0 relevance to the topic at hand.",Neutral
AMD,"Clearly not as sometimes they use the 9950x3d, and sometimes the 9900x to compare to the ultra 9 285k.   One of those is the half the price of the other.  (And guess which one they used for 1080p gaming comparison)",Neutral
AMD,"And to add to that, which often gets omitted. Is that they are only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, intel in my mind actually makes more sense.",Neutral
AMD,"Why not tech sites provide built in benchmarks scores and let everyone veryfy them, and let us decide which processors better, why should we believe in charts blindly, why should I stay silent about what i found.",Neutral
AMD,"I sell PC hardware and get to test a wide range of components. I’ve built 7950X3D and 9800X3D systems for my customers, and when I tested five games back then, their performance was very close to the results I got with the 14700K. I’ll be able to test more games in the future if another customer asks for an X3D build. I never claimed the 14700K was better than the X3D in general I only said it scored higher in one game, Horizon Zero Dawn Remastered, and matched it in others. At first, I thought something was wrong with the build, so I made a post asking 4090 owners with X3D processors to benchmark HZD, and they confirmed my result. If I were to test 10 or 20 games, the 9800X3D might turn out to be about 5% faster, but I’m still not sure. Since I’m testing on 4090s, I can buy whatever processor I want, but based on the five games I tested, it doesn’t seem worth it. Also, the minimum frame rate was always a bit higher on the 14700K. X3Ds are generally good, but my main concern is that non-3D chips are 30–40% slower than the 14700K. Why does it feel like I’m the only one who knows this?  As for proof, I’ve provided built-in benchmark screenshots, phone-recorded videos (since OBS uses 15% of GPU resources), and OBS captures as well, which should give very close idea of processors performance. I’ve shown full system specs and game settings every time, and provided more evidence than any tech site or tech YouTuber ever has, yet some people are still in denial. I want just one tech site or YouTuber to show benchmark screenshots or disclose full system specs before testing. None of them do, and people keep trusting charts with zero evidence.",Positive
AMD,"Ive given my evidence by videos recorded by phone and obs,  scores screenshots prove 14700k is 30% faster than 9700x where is your evidence, talk is cheap, you either provide hard undeniable evidence or stay silent.",Neutral
AMD,"14700k vs 9700x  CP 2077 171FPS vs 134FPS 30%  spider man2 200FPS vs 145FPS 35%  SoTR 20%  RE4 160FPS vs 130FPS 25%  horizon zero dawn 172FPS vs 136FPS 30%  30% on average vs 9700x, 35%-40% vs 7700x  here is video recorded testing 5 games by OBS for 14700k vs 7700k  RE4 160FPS vs 110FPS  SM2 200FPS vs 145FPS  [https://www.youtube.com/watch?v=oeRGYwcqaDU](https://www.youtube.com/watch?v=oeRGYwcqaDU)  [https://www.youtube.com/watch?v=u6EPzOGR2vo](https://www.youtube.com/watch?v=u6EPzOGR2vo)",Neutral
AMD,"can you do a test for horizon zero dawn and CP built in benchmarks, also if you have RE4 can you go to the same place of the game I tested on and take screen shot showing what frames you are getting, I want all tests 1080P very high, if its not much trouble for you.",Neutral
AMD,"I only tested 5 games, and the two processors were very close. I’d probably need to test 10 or more titles to know for sure if the 9800X3D is actually faster. The fact that the 14700K came out ahead in Horizon Zero Dawn shows that there are games where Intel performs better.",Neutral
AMD,I did same performance on all processors.,Neutral
AMD,"It does, just like RPL being shown on corrupt tech sites as being as fast as Zen 5 in gaming, in the real world ARL could be much faster than they make it out to be.",Neutral
AMD,That sounds like an AMD Stan argument circa 2020,Negative
AMD,Lmao   https://youtu.be/KjDe-l4-QQo?si=GsvzFon8HYcrzrMc  theres no conspiracy mate,Neutral
AMD,"You need to grow up, evidence from some rude slime on Reddit is irrelevant for me, I value data from techpowerup, hardware unboxed or gamers nexus x100000 times more than anything coming from you.  I tried making this conversation constructive, but it seems that you're not adequate enough to even try. Good luck.",Negative
AMD,Can you explain me why in the video you are running a 7700x on 6000 cl40 RAM?,Neutral
AMD,"I challenge all tech sites and tech tubers to publicly share their built-in benchmark scores so everyone can verify them. By the way, I posted these scores on all the popular tech tubers pages on X (twitter), but none of them replied. I wish at least one of them would debate these results and prove me wrong.",Neutral
AMD,"Are your results with or without APO? APO supports SOTR and Cyberpunk, so give it a try if you haven't yet to see if it makes any difference.   https://www.intel.com/content/www/us/en/support/articles/000095419/processors.html",Neutral
AMD,"I can do CP 2077 later. I have a 265k with a 9070, but only 6400 ram  (Mostly replying so I remember to do it)",Neutral
AMD,I only have Cyberpunk of the three.  [https://imgur.com/a/L3qMGva](https://imgur.com/a/L3qMGva)     I think the 4080S is holding me back too much in this benchmark.,Neutral
AMD,"Your tests on win10, win 11 24H2 improved performance. All of your tests are complete nonsense anyway",Neutral
AMD,I was just thinking the same. We have come full circle.  Time to buy some more intel stock,Neutral
AMD,It worked for them didn't it.  Why is it loads of people think AMD has dominated since original Zen CPU?,Neutral
AMD,Expand ?,Neutral
AMD,"I want elegato unedited video from start to finish i did that and on cp 2077 9800x3d matched 14700k, and it managed to beat it on horizon zero dawn, i want built in benchmark scores so everyone can easily double check the numbers, forget everything online buy the hardware and run built in benchmarks RPL i7/i9s on ddr5 7200 are equivalent to zen 4/5 x3ds much faster than non 3ds, if you can't afford it make posts asking people for thier scores in games.",Positive
AMD,"It is a common knowledge at this point, that HUB, LTT and GN are working for YouTube algo, not the costumers. YT algo promotes saying AMD is king, so their reviews are exactly that (Every single thumbnail or tittle with Intel being dead or destroyed). Let's pair Intel with mediocre ram and loose timings, let's power throttle them, put on some weird power profile, turn off whatever bios setting, focus on specific benchmarks that have very little to do with real usage etc. In worst case scenario they can always say that Intel won, but it's not enough or that Amd will crush them in 6 months or so. They've been doing it for years now.   Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong. It's really not about being right or win some CPU wars, it's more about not being screwed by that giant YT racket that once again works for sponsors and YT bucks, not the costumers.",Negative
AMD,"I spammed my score screenshots on every popular tech tuber’s page on X (Twitter), and none of them had the courage to face me. I wish one of them would come out of hiding and tell me what I did wrong in my tests. Show me the scores they’re getting. Reviews are sold to the highest bidder deal with it. Also, PCGH showed the 14600K beating the 9700X, so why isn’t that the case in other tech site charts?",Negative
AMD,"at first I tested 4080 1080P ultra on these processors on the same DDR5 7200 im using on the 14700k but it didn't make a difference tried the 2 profiles 7200 xmp and expo CL38, so when I got the 4090 I decided to use whatever available but that wouldn't make a difference, I could use DDR4 on 14700k and still win in every game against non 3ds,   here is the test on 14700k 5600 CL46 which got me around 185FPS vs 145FPS for zen4/5 non 3ds.  [https://www.youtube.com/watch?v=OjEB7igphdM](https://www.youtube.com/watch?v=OjEB7igphdM)  also here is 14700k +DDR4 3866 +4080 for RE4 still faster than 9700x+4090+DDR5 6000  143FPS vs 130FPS  [https://www.youtube.com/watch?v=lxZSMJnkYNA](https://www.youtube.com/watch?v=lxZSMJnkYNA)",Neutral
AMD,"Did you get any more FPS when you tried it? I’m getting good frames without it, so I don’t think it’s worth it. Also, in SoTR, even though the 9800X3D scored higher in the benchmark, there weren’t any extra frames compared to the 14700K during actual gameplay. I tested multiple locations, and the performance was the same, the higher score on the 9800X3D came mainly from the sky section of the benchmark.",Negative
AMD,did you do it,Neutral
AMD,I was at:  RT Ultra: 118.78 Ultra: 182.42 High: 203.76  https://imgur.com/a/edaNmHQ,Neutral
AMD,can you reset settings then choose ray tracing ultra preset.,Neutral
AMD,here is w10 vs w11 same scores in every single game.  [https://www.youtube.com/watch?v=vQB4RrNDzyM](https://www.youtube.com/watch?v=vQB4RrNDzyM)  [https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s](https://www.youtube.com/watch?v=wMfkGTMTJdo&t=8s),Neutral
AMD,because they exclusively exist in DIY build your pc enthusiast bubble,Neutral
AMD,Pricing was aggressive. A 12 core 3900x was 400 usd.,Neutral
AMD,">And to add to that, which often gets omitted. Is that Comet Lake is only better at gaming when not GPU bound, when the GPU is heavily loaded like with most games running at 4K, the difference is negligible or non existent. So if you are a 4K gamer and want the best productivity, Zen 2 in my mind actually makes more sense.",Neutral
AMD,literally multiple videos from him even with a 7800x3d and still 14900ks can only eek out wins in like 1-2 games like congrats you dropped 500$+ on the RAM alone and 720$ on hte motherboard to run that RAM just to lsoe to a 7800X3D,Neutral
AMD,">It is a common knowledge at this point,  It's not an argument.  Only objective way that you have to prove that those sources are spreading misinformation, is to prove it and share it with others - do it, make your own research and share it on Reddit and YouTube, if you're really correct and your data is accurate, it will hurt public reception of these sources(HUB, LTT, GN) and will damage their credibility.  >Every dude that actually verified their claims and build systems how they should be built, will tell you that GN/LTT/HUB reviews are bad and more often maliciously wrong.   [No true Scotsman - ](https://en.wikipedia.org/wiki/No_true_Scotsman)sorry dude, but your argument is logically incorrect, for pretty obvious reasons - it's unfalsifiable, it lacks evidence and it assumes the conclusion **- ""every dude"" argument is a way to sound like you have the backing of a consensus without having to provide a single piece of evidence for it -** as I said, real evidence in your case would be substantial evidence that proves that sources that you named in your comment are spreading misinformation, which shouldn't be hard to test and release in public.  I don't like to be a part of emotional conversations, I honestly hate those - if we want to speak facts, share facts with me, if you want to speak emotions - find another person to discuss them, I don't like emotions when it comes to constructive discussions.",Negative
AMD,"Okay, I did it",Neutral
AMD,"No, I didn’t remember good",Positive
AMD,"I did. I ran the benchmark a couple more times and the results were similar. APO on, 200S Boost enabled, no background tasks running except Steam. I think it's a GPU bottleneck.",Neutral
AMD,Thanks for solidifying opinion that your benchmarks are fake,Negative
AMD,"Ddr5 7200 cost as much as 6000, i bought mine for 110$ from newegg, I should have got the 7600 for 135$ but I was a bit worried about compatibility, my z790 mb cost me 200$, and on my 14700k on a 4090 it matched 9800x3d in 3 games lost in re4 and won in HZD remastered, ill test more games in the future but since i won in one game to me 14700k+D5 7200=9800x3d, 14900k+ddr5 7600 could be faster, i dont care about any video online or chart, i only believe in built in benchmarks scores and what my eyes see.",Neutral
AMD,"if there isnt cpu bottleneck your score should be around 135fps, can you give it one last try with sharpness set to 0.",Neutral
AMD,"What’s fake about it? What scores are you getting? Prove me wrong with score screenshots, not with fanboyism false accusations. If you’re comparing score screenshots to OBS-recorded videos, you shouldn’t, because OBS uses about 15% of GPU resources, which reduces the scores slightly. I’ve been talking about this and calling out tech sites and tech tubers on Twitter and Reddit for over a month. Do you really think they’d stay silent all this time if I were wrong? If I were faking benchmark scores, they would have publicly shown their results and shut me up for good, especially since I’ve been accusing them of corruption on a daily basis. Just yesterday Tech power up is the first techsite to block me on X for sharing my benchmark screenshot on their page, cowards, shouldn't they discuss what did i do wrong in my benchmark, then again what are they gonna say AMD paid us too much money that we need 10 years in hard work to make that much.",Negative
AMD,Cam someone confirm or is this gas lighting?,Neutral
AMD,This is nice honestly. Points out some nice stuff about ARL besides the usual hate on the internet,Positive
AMD,"The Ultra 7 265K is absolutely a better deal than the non-X3D 9700X or 9900X. The Ultra 9 285K is competitive with non-X3D 9950X. If 3D-Vcache offerings from AMD didn't exist, Intel would likely be considered the go-to for this gen.",Neutral
AMD,Intel comeback real?,Neutral
AMD,"How come the non-k variants of ultra 9,7, and 5 have a base power of 65 watt, but the K variants have a base power of 125 watts? The only difference I see between both variants, besides base power, is the 0.1-0.2MHz clock speed increase. Am I missing something here?",Neutral
AMD,3D v-cache has entered the chat.,Neutral
AMD,Take it as a grain of salt. Intel marketing LOL,Neutral
AMD,Does anyone have the Intel Ultra 9 285k and can test all of this ?,Neutral
AMD,Thats cool ...but lets talk about better pricing.,Positive
AMD,"I got the 7800x3d and the 265K, the 265K got higher fps. Also when i watch twitch streamer i Look at the fps if the streamer is useing a 9800X3D. In CS2 the 265k got 50-80 fps less then the 9800x3d  (500 vs 580 fps) in cs2 Benchmark the 265K is consuming about 100 watts",Neutral
AMD,Tech Jesus has entered chat :).,Neutral
AMD,"Intel still trying to sell their already abandoned platform, they should learn something good from AMD and provide support for three generations per socket (and yes, Zen 6 will use AM5, already confirmed by AMD).",Neutral
AMD,"Two things can be true at the same time. Arrow lake has legitimately improved since it launched last year, and Intel is portraying it in the best, most optimal light they can that is probably not representative of a broad spectrum of games reviewed by a third party.   It would be interesting to get a retest of arrow lake now, but I dunno if it is worth the time investment for some reviewer like TPU or HWUB to re-review a relatively poor platform that’s already halfway out the door.",Positive
AMD,"Arrow Lake performs quite well at general compute and power efficiency compared to prior generatons ans even against AMD.  Where they have had trouble is in specifically gaming apps, particularly against the X3D variants (which it should be said drastically perform non-X3D AMD parts in gaming). And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  This led to Intel being dismissed out of hand by gaming focused YouTube reviewers like HUB, GN and LTT despite them being perfectly capable parts outside of gaming, simply because they did not place well in gaming benchmarks in CPU limited scenarios (e. g. with a literal 5090).  HUBs own benchmarks, btw, show an utterly inconsequential difference between a 9700X part versus a 9800X3D part when paired with anything less than a 5080/4090 in gaming. Most people with most graphics cards wouldnt see the difference with their card between the various high end CPU parts unless theyre literally using a $1000+ GPU. Ironically the 9070 performance was identical with a 9700X and a 9800X3D.  So it's actually kind of the reverse, gaming reviewers painted Arrow Lake in the worst possible light.",Neutral
AMD,What do you mean by gaslighting in this case?,Neutral
AMD,"The answer depends on whether or not you're a ""next-gen AI gamer"" (apparently that means 1080p avg fps with a RTX 5090).",Neutral
AMD,I doubt they would give false numbers. Those are probably what you get with the setup they list in fine print. Also they pretty clearly show the 3dx parts are faster in gaming. But they have probably picked examples that show them in a good light.   There is nothing wrong per se in the arrow lake lineup. In a vacuum if AMD and intel launched their current lineup now they would both look pretty competitive. AMD zen5 was also a bit of a disappointment after all. The issue that destroyed the image of arrow lake is that the previous generation was often faster in gaming. That made every reviewer give it bad overall reviews.,Neutral
AMD,Wait till refresh comes out in December... Memory latency lower and performance boosted. Price also lower... Wish it could come to the old chips but it architecturally cant. :(,Negative
AMD,"generally they cherry pick, so maybe the demo they use is process heavy with minimal I/O (arrow's biggest performance hit is memory movement). if you manage to keep memory movement low core performance is actually really good on arrow lake. it just gets massacred by d2d and NGU latency, which they don't get a pass for but its unfortunate that decent core designs are held back severely by packaging.",Negative
AMD,"there is just no way dude lol  even their comparisons make no sense, 265K = 9800X3D / 265KF = 9700X? lol? how would that even be possible",Negative
AMD,Honestly it's hard to get real conversation with people who fanboying Amd so hard. Why people can't be normal when we talk about Intel and Nvidia? *smh,Negative
AMD,"The base clock rates, my assumption how it boosts during heavily multi-threaded tasks, is like 60%-80% higher on both core types.",Neutral
AMD,Nova Lake bLLC about to ruin Amd X3D party.,Neutral
AMD,Look at the last slide. Intel's source is TPU's review data which if anything is a less than ideal config for arrow considering it's only DDR5-6000. I'm honestly surprised they didn't pull from one of the outlets that did 8000 1.4v XMP+200S boost coverage.,Negative
AMD,I always wondered if Intel marketing budget is higher than the R&D budget,Neutral
AMD,Intel Arrow Lake is much cheaper than Amd Zen 5.,Neutral
AMD,Genuine question how often do you or even the average person keep their motherboards when upgrading the CPU? IMO the platform longevity argument is stupid,Negative
AMD,only an AMD fan would worry about replacing their shit CPUs under 3 years,Negative
AMD,Hardware unboxed isn't a reliable source.,Neutral
AMD,"As a real world example, I upgraded my primary system from a 14700k to a 265k recently. My other desktop is a ryzen 7900 (non x, 65 watt tdp)  Compiling my os takes 6 minutes on the 7900, 10 minutes on the 265k, 10 minutes on a 3950x, and 16 minutes on the 14700k.   That is not in windows, but my os which does not have thread director or custom scheduling for e cores. (So going to be worse than windows or Linux)   All the numbers except the 7900 are with the same ssd, psu, custom water loop.  The motherboard and cpu changed between Intel builds and the 3950x was the previous build before the 14700k.   The e core performance significantly improved in arrow lake.  It’s now keeping up with 16 core am4 parts in the worst possible scenario for it.  If the scheduler was smarter, I think it would be very close to the 7900.   In windows, it’s pretty close in games I play on frame rate to the old chip. A few are actually faster.",Neutral
AMD,"> And secondly that in improving power efficiency compared to 14th gen it appears they suffered in raw performance compared to high end power hungry parts like the 14900K.  Yeah, 10% less performance when frequency matched is quite a lot.",Neutral
AMD,Telling people that its performance is better than it actually is?,Negative
AMD,The ones with similar pricing not performance,Neutral
AMD,"True, except AM5 supported all cpus 7000, 9000 and will support the newest one. So not exactly true per se, because you need a new motherboard every time and those on AM5 can upgrade cpu without buying a board. Imagine if someone was on 13400F and wanted to buy lets say 285K? Well, you need a new board. So it depends what “cheaper” is seen as.",Neutral
AMD,"Yep. We all see the patterns here. Gamers nexus, HUB, LTT and most big name they all keep mocking Intel, even when Intel did good job to bring improvements to their products like fixing Arrow Lake gaming performance but those ""tech reviewer"" decided to be blind, they don't even want to revisit Intel chip and benchmark it, they don't even want to educate their viewers.    At the same time they keep overhyping Amd products to the moon like they own the stock, it's obvious now who is the real reviewer and who is fraud reviewer using their own popularity and big headline to spread their propaganda for their own benefits. They are just as worse as shady company!",Negative
AMD,"Platform longevity isn't stupid lol. There's a reason why am4 is one of the best selling and one of the best platforms of all time. The fact that you can go from a 1700x to a 5800x3d on the same mobo for 85% more performance is crazy good value. You can say that all you want but there's a reason why in the diy market everyone is buying amd over intel. People know arrow lake is doa where as am5 actually has a future, which is why everyone is flocking to the 9800x3d in droves.  Zen 7 is rumored to be on am5 as well. Meaning that am6 would come out in 2030/2031. If that's true am5 lived for around 8/9 years.  That's insane value to get out of one socket.",Positive
AMD,Quite common for AM4 in my experience.,Neutral
AMD,"My brother went from 1700X to 3700X to 5800X3D on AM4, so 10 years next year? I plan on upgrading to 3D cpu probably 9600X3D once its out or maybe 7800X3D (something more affordable but still somewhat better). Kinda want better 1% lows tbh, but maybe I’ll just wait for next ones unsure.",Neutral
AMD,"How is it stupid? On one hand, intel sockets used only 3 gens from 12th-14th while on the other hand amd has not only used 7000-9000 but will also be using zen 6. I don't think you realize just how big of a hassle changing motherboards are",Negative
AMD,"I upgrade my motherboard usually because of cpu socket. Thinking of going AMD. I build my PCs for myself, my wife and my kid. Lots of floating parts in this house could use the platform longevity.",Positive
AMD,I originally had a Ryzen 5 3600 and am now on a 5700x3d so it’s pretty relevant I’d say. Of course it’s anecdotal but I don’t think it’s too uncommon,Negative
AMD,"I agree with you, it’s daft, no doubt about it. But you’ve got to factor in that people can be daft too. Most tend to skimp, only to end up buying another middling CPU the following year, and then again after that. And so the cycle trundles on.  When I buy PC parts, I usually go for the best available. Provided the price is reasonably sane. I picked up a 9900K seven years ago and it’s still holding its own. Is it on par with the 9800X3D? Of course not. But it’s still a very capable bit of kit. Next time I upgrade, I’ll go top shelf again.",Neutral
AMD,"They frequently revisit products and test old hardware, but naturally that's not going to be their focus. Channels with that focus like RandomGaminginHD fill that niche.",Neutral
AMD,Sooo they are in the YouTube space for the money not for the love of tech,Neutral
AMD,"So true. After all they are reviewer, it's their job to revisit something if it's worth to benchmark due to improvements.   Didn't they said they want to educate their viewers? If so then why being lazy? This is obvious they just care about making big headline which makes more money for them. It's BS!",Negative
AMD,"If only the Arc B770 can beat the RX 9070XT with performance and features, then I definitely stay on Intel's side, but RN I doubt it",Neutral
AMD,"Even accepting that as true, I don’t think it changes the main point I was making. Arrow Lake has probably improved versus a year ago. It’s probably not as good as intel is making out. A good third party reviewer would be needed to determine by how much, yet it’s probably not worth their money and time to test it.   So the truth is probably in between what Intel is saying here and what the state of things was 6 months ago.",Negative
AMD,Isn’t that just either lying or exaggerating?   I know those words don’t “go hard” and are “low key” boring.   But I think some internet buzzwords are just overused or badly applied.,Negative
AMD,"Rule 5: AyyMD-style content & memes are not allowed.   Please visit /r/AyyMD, or it's Intel counterpart - /r/Intelmao - for memes. This includes comments like ""mUh gAeMiNg kInG""",Negative
AMD,The 5800x 3d costs anything between 450 (time of launch) and 300. With that kind of money - you can literally buy a cpu + a mobo bundle. There is no value in the platform upgradability argument. You can literally buy a 13600k and a brand new mobo for cheaper than the cost of the 5800x 3d alone.,Negative
AMD,YouTube has created an entire industry not just for gaming but for everything else where all these people rely on the YouTube algorithm and have figured out how to exploit it and bitch and complain whenever YouTube tries to change things up.  We need a new platform based on YouTube but built by technology like ipfs and run by the internet community.,Negative
AMD,"No other card in recent memory has been more overhyped than the 9070 XT , we should be getting a B770 before 2025 ends. Make sacrifices to the Silicon Gods and perhaps your wishes will come true.",Neutral
AMD,"The only things that have changed since launch is the bugged ppm driver on windows causing extremely low clocks under moderate load (games being the main example), and 200S boost making d2d/NGU OC a single click for people not willing to type in 32 in the NGU/D2D ratio boxes. The current performance on arrow is achievable on the launch microcode.",Neutral
AMD,Sure but charts seem about right to me,Positive
AMD,APO is game specific. I'm referring to what has changed overall.,Neutral
AMD,"I'm unable to claim my copy of Battlefield 6 from the recent Intel promotion even after redeeming the code.  I redeemed my promotional code last month which added the ""Intel Gamer Days"" bundle to my account that includes Battlefield 6. The instructions say to return October 3rd to claim the key. I've been attempting daily to claim the key but just keep getting the message ""We are temporarily out of codes and are working to reload our system. Please check back soon.""  The game launches tomorrow so this is pretty frustrating. Is anyone else having an issue claiming their BF6 key?  I put a ticket in with Intel support but haven't heard anything yet.",Negative
AMD,"bought the newegg bundle. I got a master key but my PC is running linux. The linux HST tool doesn't even have an extension, no .exe no nothing. I don't know how to run it.",Negative
AMD,"Hello there, it seems that after updating my Windows 11 I can't scan for new intel drivers by using the Intel Driver & Support Assistant. It scans for a while but then it gives me the message: ""Sorry, something went wrong while trying to scan""; it seems to affect the Intel Graphics Software as well as it keeps looking for updates without success. I've already tried to reinstall DSA and deleting any folders associated with the program, rebooting Windows and also uninstalling the Windows Update. Any thoughts?",Negative
AMD,"After updating to W11, these awful KILLER apps and processes have returned and reduced my upload speed to less than 1mb!!! Normally I get 30mbs up.   Dell PC, 32 gbs RAM, Intel i9. x64 W11 Home 24H2, OS 26100.6584, WIRED Ethernet direct to router/modem.  The Killer thread is closed and directs users to post in this thread.   The same thing happened in W10 on same machine but problem was fully solved by the KillerSoftwareUninstaller tool, which also disabled Killer permanently from coming back on every single update.  However, I tried the same tool after the update to W11 and it removes Killer, and everything works better for a day or so, then suddenly everything slows down and Killer is back.  PLEASE help me get this awful suite of useless Killer tools off of my machine forever!   Thank you!",Negative
AMD,"I have a i7-13700 that may be the root of my issue. Long story short, MATLAB is getting an Access Denied error when trying to link to an Imperx framegrabber using videoinput. Imperx FrameLink works fine. The function that it crashes at works fine when called from a different way (getAvailHW), and only crashes when trying to call it through videoinput. The exact same setup works just fine on another computer. Even when no cameralink cable is attached, it crashes on this one. BIOS is fully updated. The CPU passed the checking program Intel provided.   Looking up this issue, people had similar problems, and they linked it to the CPU issues with 13th and 14th gen. Intel released a chart for what settings you should have in BIOS, but it does not cover the 13700, only the K variant. What settings should I have in BIOS to rule out Intel?  Chart: https://community.intel.com/t5/image/serverpage/image-id/56057i81282C3BCB9162A9",Negative
AMD,"Is this normal activity for a 265K at Idle?  https://imgur.com/SSSOB9F      The multiplier and clock speed keeps jumping around all over.  There are no background processes running, nothing in task tray, closed all applications except HWINFO to record this.      I am running into an issue where I feel like either my CPU or my AIO is bad.  The build is new, completed yesterday.  Idle temps hang around 30-40C; which seem fine.  But as soon as tasks start happening, it jumps all over up to 80C.  I have a 360 AIO on it.  Gaming even, it hit 83C earlier.  It doesn't make sense to me.  I have built a few 265K rigs and not had this kind of fluctuation in temperature before.  Even running cinebench on previous builds, the CPU never went above mid 70s.  But this newer machine is hitting 80+ in games.  Previous 265K PCs have ran mid 50s in games.  Sometimes 65 depending on the load, but never 80+.",Negative
AMD,"Dear Intel team,  according to **desktop** [**Core 14th datasheet**](https://edc.intel.com/content/www/us/en/design/products/platforms/details/raptor-lake-s/13th-generation-core-processors-datasheet-volume-1-of-2/system-memory-timing-support/) \-> memory timings support:  \- only **CL50 DDR5 5600MT/s** **is supported**  \- only **16Gbit / 24Gbit die density is supported**    **Can I safely use following DDR5-5600 CL36 kit from Kingston KF556C36BBE2K2-64 with 32Gbit die density?**  Will it cause some memory troubles / issues on memory controller ?  System is not overclocked (and never was/will be), everything set to Intel defaults, bios has latest in 0x12b microcode from ASUS (board Q670M-C CSM).",Neutral
AMD,"Hi, I’m at my wit's end with my build and would really appreciate some advice.  My PC has been plagued by random crashes, CRC errors, and installation failures for months:  - Random application crashes, often citing `KERNELBASE.dll`, `ntdll.dll`, or `ucrtbase.dll`. - Frequent CRC errors when extracting large ZIP or RAR files. Retrying usually works. - Software installations fail with data corruption or unpacking errors, only to succeed when I try again. - Games crash or stutter randomly. - Very rare BSODs.  Specs:  - **CPU**: Intel Core i9-13900K - **Motherboard**: ASUS ROG STRIX Z790-H GAMING WIFI - **RAM**: 32GB (2x16GB) G.Skill Trident Z5 RGB (DDR5-6400 CL32, Model: F5-6400J3239G16GX) - **GPU**: MSI RTX 4090 Gaming X Trio - **Storage (OS Drive)**: Crucial P5 Plus 2TB NVMe SSD - **OS**: Windows 11   This is what I've already tried (everything passed):  *  **Memtest86:** Completed multiple passes with no errors. *  **Prime95 & Intel XTU:** Seems to be stable. *  **FurMark:** GPU stress test is stable. *  **Storage Health:** All drives pass SMART and manufacturer-specific self-tests. *  **System Integrity:** `sfc /scannow` and `DISM /RestoreHealth` complete successfully. *  **Updates:** All drivers, firmware, BIOS, and Windows are fully up-to-date. *  **Physical:** Cleaned the case, re-seated components, and replaced thermal paste. * **XMP**: I've disabled/re-enabled XMP multiple times, doesn't make a difference.  I have trouble finding a consistent way of reproducing the issue. Today I tried a 7-Zip benchmark which failed once with a ""decoding error"", but I wasn't able to reproduce it afterwards. I couldn't get Intel Processor Diagnostic Tool to fail after multiple hours. So...  *   **Is this a memory instability issue?** Could the RAM be faulty?  *   **Or, could this be a faulty CPU core?** I found [this post](https://www.reddit.com/r/intel/comments/15mflva/tech_support13900k_problems_when_using_multiple/) where a user had identical symptoms (CRC/7-Zip errors) that were only resolved by replacing a faulty 13900K, even with XMP off.  Thanks in advance for any help.",Negative
AMD,"# Alienware 34 - AW3425DWM resolution issues  [](https://www.reddit.com/r/ultrawidemasterrace/?f=flair_name%3A%22Tech%20Support%22)  I just got the AW3425DWM, and my laptop is a Dell Inspiron 15 5510, which is not a gaming laptop. I'm not a gamer.  When connecting through the HDMI port on the laptop and monitor, I can't set the resolution to 3440x1440; I can go up to 3840x2160, but not the monitor's native resolution. However, when I connect through the USB-C port on the laptop to the DisplayPort on the monitor, I can set the resolution to 3440x1440 without any issues. The downside is that I've lost the only Thunderbolt port I have available.  Is there a workaround for this issue? If I use an HDMI to DisplayPort adapter, will I be able to set the resolution to 3440x1440?  I understand that the HDMI 1.4 port usually can handle 21:9 resolutions, but with the latest Intel drivers, it isn't giving me the option for 3440x1440",Negative
AMD,"Hi, can someone help me please? I'm trying to generate a video on AI playground after installing it but it just keeps loading for videos and images don't show up. That's my it it'll specs:   Processor: Intel(R) N95 (1.70 GHz) Installed RAM: 32.0 GB (31.7 GB usable) System Type: 64-bit operating system, x64-based processor Graphics Card: Intel(R) UHD Graphics   -That's a part of what's in the console:    No key found for setting negativePrompt. Stopping generation oa @ index-fOc02QH8.js:61 w @ index-fOc02QH8.js:61 await in w V @ index-fOc02QH8.js:22 pt @ index-fOc02QH8.js:61 await in pt V @ index-fOc02QH8.js:22 c @ index-fOc02QH8.js:61 await in c Il @ index-fOc02QH8.js:14 Sr @ index-fOc02QH8.js:14 n @ index-fOc02QH8.js:18 index-fOc02QH8.js:61 uploadImageName b99fb28ea4440a2f51ce53cd5c529554e5965b66f5f5a8506b9b973b66e754bd.png index-fOc02QH8.js:251 [comfyui-backend] got prompt (anonymous) @ index-fOc02QH8.js:251 (anonymous) @ VM5:2 emit @ VM4 sandbox_bundle:2 onMessage @ VM4 sandbox_bundle:2 index-fOc02QH8.js:61 updating image {id: '6eb88f1b-f433-4541-a421-40619ac9fdc2', imageUrl: 'data:image/svg+xml,%3C%2Fpath%3E%3C%2Fsvg%3E', state: 'generating', settings: {…}, dynamicSettings: Array(3)}       With: RuntimeError: UR error     (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:251 [comfyui-backend] Prompt executed in 116.86 seconds   (anonymous) @ index-fOc02QH8.js:251 index-fOc02QH8.js:61 executing Object",Neutral
AMD,Will XeSS 3 and Intel multi framegen be available for Iris xe graphics igpus?,Neutral
AMD,"Putting together a 4k gaming 5090 machine, deciding between the 285k or 265k. Does the extra L3 cache of the 285k make any difference, or does the ~5ns less latency of the 265k make more of a difference?   I plan on a 2dimm board to push memory and OC slightly, but nothing crazy.",Neutral
AMD,"Hi all,  I ordered a contact frame from Thermal Grizzly because the temperature of my CPU wasn’t great, and I was tired of the fan noise.   I installed the contact frame in August, and everything was fine until two weeks ago, when the desktop suddenly stopped booting. The fans were spinning, but there was no POST, no debug LED — nothing. I thought the motherboard was dead.   However, when I removed the AIO head and tried to boot again, it worked!  Since then, I’ve been experiencing intermittent no-POST issues, especially after gaming sessions.   To get it to boot, I always have to adjust the screws on the AIO head. If it’s too tight: no POST. If it’s too loose: CPU temperatures are high. For example, in Hogwarts Legacy, I get around 60 FPS with an RTX 4090, while the CPU runs at around 18% usage @ 85°C. Which is not normal at all, as I used to run the game at around 100–110 FPS.  I tightened the screws on the contact frame using my thumb and index fingers.   I’m looking for advice from anyone who has encountered a similar issue, because at this point my only idea is to remove the contact frame and reinstall the original one.  For context, my setup is from November 2022, and I installed the contact frame in August 2025. I have never removed the CPU from the socket since the first installation.  Thank you all.",Negative
AMD,Anyone know why the performance of my 13700k is so much better when using a pre micro code bios on a z790 board? This even after applying the latest micro code update through windows,Negative
AMD,"My hp elitebook 830 g8 notebook, had some problems booting up and it shows blinking lights (caps key) and it keeps trying to boot up without success, but i after getting it repaired now it boots up but sometimes it still does the same thing and boots up after awhile  I was already using the latest versions of the BIOS and ME firmware, and trying to download and update them again did nothing. However, avter updating the BIOS (to the same version), it showed a message saying ""HP Sure Start detected that the Intel Management Engine Firmware is corrupted"", but it only did so once.  What it does consistently is it restarts once or twice when i boot it. It shows the logo, then turns off, and then boots normally.  In the BIOS, it shows in system information that ""ME Firmware Mode: Recovery Mode"".  Windows also takes much longer to boot than usual, can take up to a full minute.  I can recall that the whole system was super slow at some point, but that was before I started diagnosing any of these details. It works fine now after boot.  Tried all sorts of things, installing ME drivers doesnt seem to do anything, and IDK what to do now.  help",Negative
AMD,"My processor with integrated 11th Generation Intel graphics (Intel Core i3-1115G4) completely lost Vulkan support after installing the latest version, 7080 (coming directly from the previously installed version, 6987). I'm testing the drivers one by one, but it looks like Vulkan support has been completely removed. Did this only happen with my processor, or with all 11th Generation processors?",Negative
AMD,"I'm on Win11 with Killer WiFi 7 BE1750x 320MHz Wireless Network Adapter. PC keeps crashing/restarting. Here is an error:  The computer has rebooted from a bugcheck.  The bugcheck was: 0x00020001 (0x0000000000000011, 0x0000000000210720, 0x0000000000001005, 0xffffe700010059a0). A dump was saved in: C:\\WINDOWS\\Minidump\\121225-18625-01.dmp. Report Id: 189a9fba-5969-4b6c-8199-d8b6a03a1a34.  Copilot had me disable all Killer services except Killer Network Service, but the issue persists. Now it tells me to uninstall the drivers and block them from reinstalling and just use the generic drivers.",Negative
AMD,"I recently bought a 2025 LG Gram 17 (Intel 258V + 140V). Great laptop for productivity tasks which is its primary use for me. I might occasionally game on it if I'm traveling. I only really play Final Fantasy XI these days which was released 2003 so this processor has no problem running it at max settings and \~60 FPS even in Silent mode.  That's when it is plugged in though.  As soon as it is unplugged the performance drops like 80%. I don't mind the performance hit when I'm doing productivity tasks, but would like control over it when I want.  I've tried various things to improve this while on battery:  * Making sure it is designated as ""High Performance"" in Windows. * Making sure Advanced Power settings are the same between plugged-in and battery. * Making sure in Intel Graphics Command Center that Display Power Savings are off. * Making sure in My gram that performance is set to high. * Changing anything anywhere I can find that might be limiting performance on battery.  I haven't started digging through the BIOS yet and don't know that I will. At that point it is too much of a hassle as opposed to some quick toggle(s) in the OS.  Any other places I should be checking or suggestions to address this related to the CPU itself?",Positive
AMD,"Hey everyone, looking for second opinions because this behavior doesn’t seem normal.  Specs:  CPU: Intel i5-14600KF  Cooler: Scythe Fuma 2 (dual tower, dual fan) with oficial LGA 1700 mounting kit.  Motherboard: ASUS TUF Gaming B760M-PLUS WiFi II  Case airflow is fine.  Darkflash DLX case with 9 fans.  Ambient temp \~25–30°C (Brazil)  Before this build I had a Ryzen 7 5800X on an ASUS TUF board with the same case and airflow, and temps were completely normal. No overheating issues like this.  but now, I'm facing the following:  Idle temps sit around 60–75°C  Any moderate load (opening anything, even the bios) causes instant spikes to 95–100°C. When acessing the Bios, the temp shown is 78-88°C  CPU thermal throttles immediately  Heatsink stays barely warm, even when CPU reports 95–100°C  Fans ramp up correctly  I’ve tried:  Re-mounted the Scythe Fuma 2  3 times (check [https://imgur.com/a/ehBnxLn](https://imgur.com/a/ehBnxLn) for how the termal paste was when I remounted  the last time)  Re-mounted the CPU.  Checked power limits  Limited PL1/PL2 to 65W → temps drop but CPU becomes extremely slow, 1˜3GHZ, but still hitting 50-60ºC on idle and 80-90ºC on the rest, 88 on the bios as well.  Undervolted –0.06V, didn't solve.  Has anyone else seen this behavior with 14600KF + ASUS B760 TUF boards?  My friend (chatgpt) recommended a contact frame and said that it would fix it. What do you think about this?  Any input appreciated. Thanks!",Neutral
AMD,"This isn't tech support, but my thread got locked and the mods said to post it here.  My 14700k is starting to give instability symptoms, so I opened a ticket with Intel.  The next day, I got an email asking for more information and was told they would call yesterday if they hadn’t received it. I replied to the email with information and my order confirmation, but as of 4:00 yesterday, I didn’t get a reply.  So, I went into the ticketing system and didn’t see my reply. I added a new comment with the information from my email and received almost an immediate email from the support person granting the RMA.  He asked for shipping information and if I’d choose option 1 or 2. For option 2, he asked for my agreement to the process and my billing address, name on the credit card, and expiration. He said he’d call to get the credit card number.  I replied to the email and pasted the information in my ticket. Oddly enough, I didn’t see his second response in the ticket.  Is that really how this goes down? There’s not an order system where I can input my credit card information? I have to wait for a call from some random support person and give him my credit card number over the phone?",Negative
AMD,"Hi u/Mydst For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues. They'll be able to assist with your Battlefield 6 code availability and on how to resolve this issue.   You may also check these links:  [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Neutral
AMD,"Hi u/afyaff For this kind of inquiry, please contact directly our [Software Advantage Program](https://softwareoffer.intel.com/) team. They are the one who handles bundle related inquiry and they may help you on how to input the master key in your PC.   You may also check this link for additional information:  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)",Neutral
AMD,"u/Adrian-The-Great  For bundle promotions and game code redemption problems, please coordinate directly with our [Software Advantage Program](https://softwareoffer.intel.com/) team. They handle all promotional game codes and can resolve redemption issues .They'll be able to assist with your non-working BF6 code from the Newegg bundle promotion.     Additional Information:   [Battlefield 6 Redemption Information – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/38908354715533-Battlefield-6-Redemption-Information)  [Intel® Software Advantage Program – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/categories/115001220623-Intel-Software-Advantage-Program)  [Sorry, this promotion has expired. Please contact support for more details. – Software Advantage Program Support Center](https://tgahelp.zendesk.com/hc/en-us/articles/13532109226125-Sorry-this-promotion-has-expired-Please-contact-support-for-more-details)",Neutral
AMD,"u/Broad_Remote3101 This could be a compatibility issue or something with the installation process. Just to help figure this out - are you using a laptop or desktop? If it's a laptop, could you let me know the exact model? Also, what graphics card and processor do you have?  You might want to try following this article too and see if that does the trick. Let me know how it goes!  [Intel® Driver & Support Assistant (Intel® DSA) Results in “Sorry,...](https://www.intel.com/content/www/us/en/support/articles/000026895/software/software-applications.html)",Neutral
AMD,"u/musicmafia77 Based on what you're describing, this sounds like a classic case of Windows 11 being overly ""helpful"" with driver management. To get to the bottom of this and find a lasting solution, I'll need some specific details about your setup.   First, could you check Device Manager and tell me the exact model of your network adapter and current driver version? Since you mentioned you're using wired Ethernet, I'm curious if this is actually a Killer-branded Ethernet adapter. Also, when Killer inevitably returns, which specific processes are you seeing in Task Manager - things like ""Killer Control Center"" or ""Killer Network Service""?  Here's what's likely happening behind the scenes: Windows 11 maintains a driver store cache that can automatically reinstall what it considers ""preferred"" drivers, even after you've removed them. The system recognizes your Killer hardware ID and keeps pulling that branded driver through Windows Update, completely overriding your manual removal efforts. This explains why the KillerSoftwareUninstaller works temporarily but then everything reverts after a day or so.   To truly fix this, we'll probably need to perform a clean driver installation process that involves purging the entire driver store of Killer-related files, blocking Windows Update from auto-reinstalling them, and potentially replacing them with Dell provided drivers rather than Intel generic drivers (since Killer hardware is actually based on Intel chipsets). It's more comprehensive than just running the uninstaller, but it should give you that permanent solution you're looking for.",Neutral
AMD,"Update your bios to the latest version and set it to intel default settings. If the issue is the same, request a replacement",Neutral
AMD,"PL1 should be  65W , PL2 is 219 W you can cross reference this value on this specs https://www.intel.com/content/www/us/en/products/sku/230490/intel-core-i713700-processor-30m-cache-up-to-5-20-ghz/specifications.html   However following intel recommended settings once you load BIOS with microcode patch it should follow the default, you may check also with your motherboard manufacturer",Neutral
AMD,"May I kindly ask you what components are you using (especially MoBo)?      I have HP Z2 Tower G1i with 265K too and what is interesting is that placement of performance and efficient cores is not contiguous (e.g. 8P and then 12E), but somehow ""alternating"".   I observed this on my HP Z2 G1i and thought that it is something special defined by HP Z engineers in ACPI tables (maybe DSDT..), but looks like that it's how things works.",Neutral
AMD,"u/vincococka ,Yes you can use that memory kit safely as long as you don’t enable XMP and keep everything at Intel default settings. Intel officially supports 16Gbit and 24Gbit dies, but many users have successfully used 32Gbit dies without issues. However your memory will likely run at (JEDEC default), which is fully supported. Just make sure, You monitor system stability after installation. Avoid enabling XMP unless you confirm stability with the 32Gbit modules.     [Compatibility of Intel® Core™ Desktop Processors (14th gen)](https://www.intel.com/content/www/us/en/support/articles/000096847/processors.html)",Neutral
AMD,"u/SuperV1234 If the motherboard BIOS allows, disable Turbo and run the system to see if the instability continues.  If the instability ceases with Turbo disabled, it is likely that the processor  need a replacement.",Neutral
AMD,"u/triptoasturias Before I share any recommendations, could you confirm if this is your exact system-[Inspiron 15 5510 Setup and Specifications | Dell](https://www.dell.com/support/manuals/en-my/inspiron-15-5510-laptop/inspiron-5510-setup-and-specifications/specifications-of-inspiron-15-5510?guid=guid-7c9f07ce-626e-44ca-be3a-a1fb036413f9&lang=en-us)? Also, may I know which driver version you’re using and where you downloaded it from—was it from Dell or the Intel Download Center?",Neutral
AMD,"u/triptoasturias Your concern is related to **port and bandwidth limitations**. Most Inspiron models only support **HDMI 1.4**, which is limited to 4K at 30Hz or 2560×1440 at 60Hz. Ultra-wide resolutions like 3440×1440 often aren’t exposed because they’re outside the standard HDMI 1.4 spec. **USB-C to DisplayPort**: This works because DisplayPort has much higher bandwidth and supports ultra-wide resolutions natively. HDMI-to-DisplayPort Adapter, Unfortunately, **passive adapters won’t work** because HDMI and DisplayPort use different signaling. You’d need an **active HDMI-to-DisplayPort converter**, but even then It will still be limited by the HDMI 1.4 bandwidth from your laptop. So, you likely **won’t get 3440×1440 at 144Hz,** maybe 3440×1440 at 30Hz or 50Hz at best.",Neutral
AMD,"u/mercurianbrat This spec can run **basic image generation workflows** (CPU mode or lightweight models), but **video generation and heavy diffusion models will struggle or fail** on this setup. AI Playground’s minimum requirements are currently tied to Intel Arc GPUs with 8GB or more of allocated VRAM.  Currently you can download the installer for discrete GPUs.  We will also publish an installer that will run on Intel Core Ultra-H with built-in Intel Arc GPU (please keep in mind that [Windows allocates half of the system RAM as VRAM](https://www.intel.com/content/www/us/en/support/articles/000020962/graphics.html) for integrated GPUs, so 16GB or more of system RAM are required)  and Intel Arc GPU discrete add in cards with 8GB or more of memory. AI Playground takes up 8GBs of  HDD/SDD requirements: 8GB w/o models,  \~50GB with all models installed.",Neutral
AMD,"u/mano109 As a general corporate policy, Intel Support does not comment on information about products that have not been released yet.  **Visit** our [Newsroom](https://newsroom.intel.com/) for the most recent announcements and news releases.",Neutral
AMD,"**265K all the way.** At 4K with a 5090, you're GPU-bound anyway. The 265K's lower memory latency beats the 285K's extra cache at that resolution, plus you save money for better RAM or cooling.",Neutral
AMD,"u/hus1030  The mounting pressure from your AIO cooler can directly affect whether the system successfully completes POST. When the cooler is tightened too much, it can cause the CPU or motherboard to bend slightly, which may lead to poor or lost contact between the CPU and the socket pins. This prevents the processor from initializing properly, resulting in a no-POST condition. Installing a contact frame changes the pressure distribution compared to the stock retention mechanism, so overtightening the AIO screws can amplify this issue. On the other hand, if the screws are too loose, the CPU temperatures will rise because the cooler is not making proper thermal contact. To avoid these problems, ensure the AIO screws are tightened evenly in a cross pattern and do not exceed the manufacturer’s torque specifications. If the issue persists, you may need to verify that the contact frame is correctly installed or temporarily revert to the original retention bracket to rule out pressure-related problems.",Neutral
AMD,"u/BudgetPractical8748   Intel Default Settings may impact system performance in certain workloads as compared to unlocked or overclocked settings.  As always, system performance is dependent on configuration and several other factors.",Neutral
AMD,Nope got cash back,Neutral
AMD,"u/Any_Information429 Your HP EliteBook 830 G8 is experiencing boot issues due to corrupted Intel Management Engine (ME) firmware, which is a critical low-level system component that manages hardware initialization. This corruption is causing the blinking caps lock light, multiple restart attempts before successful boot, and the extended Windows startup times you've been experiencing. The BIOS showing ""ME Firmware Mode: Recovery Mode"" confirms this diagnosis. Since these issues began after your recent repair, it's likely that the Management Engine chip connections were disturbed or the firmware became corrupted during the service process.  To resolve this, you need to perform a forced recovery of the ME firmware by downloading the specific firmware version for your EliteBook model from HP's support website-[HP EliteBook 830 G8 Notebook PC Software and Driver Downloads | HP® Support](https://support.hp.com/us-en/drivers/hp-elitebook-830-g8-notebook-pc/38216726) and using specialized recovery tools to reflash the Management Engine. You also have check BIOS settings to ensure proper ME configuration and temporarily disable fast boot to allow complete initialization. If the firmware recovery doesn't resolve the issue, this may indicate hardware-level damage to the ME controller that occurred during the previous repair, which would require professional chip-level service or potentially warranty coverage since the problem originated after authorized service work. The good news is that once the ME firmware is properly restored, your system should return to normal boot times and eliminate the restart cycles you're currently experiencing.  USB flash recovery method is definitely worth trying first - it's designed specifically for these types of firmware corruption issues and should get your laptop back to normal boot times without all those frustrating restarts. Check here: [Support Search Results | HP®️ Support](https://support.hp.com/us-en/search/videos?q=BIOS) BIOS Videos",Negative
AMD,"Hi @[Content\_Magician51](https://www.reddit.com/user/Content_Magician51/) Upon checking, there is a new driver version available which is 32.0.101.7082. You may try this and use DDU method to make sure that you performed a clean driver installation. Here are the links of the latest driver and the steps on how to perform DDU.  [Intel® 11th – 14th Gen Processor Graphics - Windows\*](https://www.intel.com/content/www/us/en/download/864990/intel-11th-14th-gen-processor-graphics-windows.html)  [How to Use the Display Driver Uninstaller (DDU) to Uninstall a...](https://www.intel.com/content/www/us/en/support/articles/000091878/graphics.html)",Neutral
AMD,"u/MISINFORMEDDNA  I took a look at your crash error, and here's what's going on. That error code you're seeing (0x00020001) is actually what's called a ""hypervisor error,"" which basically means it's a problem with Windows' virtualization stuff rather than directly being caused by your WiFi drivers.  The real culprits are more likely things like memory issues, BIOS problems, or conflicts with virtualization features like Hyper-V. I'd suggest running a memory test first (just search for ""Windows Memory Diagnostic"" in your start menu), and if you have any virtual machines or Docker running, try shutting those down temporarily, we'll probably need to dig deeper into hardware or system-level issues to really fix this one.",Negative
AMD,"u/strumpystrudel So what you're experiencing is actually pretty normal behavior for your laptop when it's unplugged - that 80% performance drop is totally expected and here's why. When your laptop is plugged into the wall, your CPU can run at much higher power levels (probably around 28W or more), but when you switch to battery, it gets severely limited to maybe 8-15W to preserve battery life. This is especially true for ultrabooks like the Gram that prioritize being thin and light over raw performance. The thing is, a lot of this power management happens at the hardware level with Intel's built-in systems, which is why all those Windows power settings you tweaked aren't really making a difference - the CPU is basically ignoring them and doing its own thing to save battery.  Now, for a game like Final Fantasy XI, you should still be able to get it running decently on battery with some tweaks, but expecting that same smooth 60 FPS at max settings is probably unrealistic given the fundamental power constraints of ultrabook design. Most ultrabooks see this kind of 60-80% performance hit on battery for any sustained workload, so you're definitely not alone in this.   But honestly, this is just how these thin and light laptops are designed to work - they're amazing when plugged in, but they have to make compromises when running on battery to actually give you decent battery life.",Neutral
AMD,"u/Aggravating_Gap_203 I'd recommend running Intel's Processor Diagnostic Tool first to rule out any hardware defects with the CPU itself. Just download it from Intel's website, run the test, and let us know if it passes or fails. While you're at it, try loading your BIOS defaults and make sure your power settings are at Intel's recommended specs - PL1 should be around 125W and PL2 around 181W for your 14600KF-[Intel® Core™ i5 processor 14600KF](https://www.intel.com/content/www/us/en/products/sku/236778/intel-core-i5-processor-14600kf-24m-cache-up-to-5-30-ghz/specifications.html)  [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html)  Your friend is actually spot on about the contact frame recommendation. Before you buy one though, try remounting your cooler one more time - make sure you're tightening the screws in an X-pattern and that everything is perfectly aligned. Sometimes it just takes that perfect mount to get things working right. Let us know what the Intel diagnostic shows and we can go from there!",Neutral
AMD,Hi [RadioFr33Europe](https://www.reddit.com/user/RadioFr33Europe/) I sent a direct message to gather more details for me to review the case and check the status of your replacement request.,Neutral
AMD,"Thank you so much for your help!  The ethernet adapter is Killer E2400 Gigabit Ethernet Controller.  I am not super tech savvy, but what you are describing makes sense and is consistent with what I've read about this problem. I will need simple step by step instructions to fix as you suggest. Thanks again!",Positive
AMD,"Intel 265K.  Motherboard was a Gigabye Z890 Aorus Elite X Ice.  32GB Corsair DDR5-6000 CL30.  Gigabye AORUS RTX 5080 MASTER.  Corsair 1000W PSU.      I had ordered another AIO cooler, and when I took the current one apart to repaste and install the new cooler, to test if the cooler was bad, I found that the pin contacts on the CPU had several dark spots.  Either the CPU or motherboard was bad from the get go.  I took it all back and exchanged it for an AMD 9800x3d and X870E motherboard, and it runs flawlessly.  Good stable temps.  35C idle, no jumpiung around, and gaming averages 57C, never passing over 65C.",Neutral
AMD,"Hi, I've tested disabling turbo and I still experienced issues. I've created a support request, case number 06728608, please take a look ASAP.",Negative
AMD,"Actually, this is not the case. After spending hours on this issue, I've finally found the 31.0.101.4502\_A14 drive at Dell's website that is working without any problems with the 3440x1440. So your claim that HDMI 1.4 is limited to 2560x1440 is false. HDMI 1.4 can go further to 3840 x 2160 without any problems. It's unacceptable the lack of support for UWM from Intel graphics cards and drives. Many previous drivers let you choose 3440X1440 resolution. Why did Intel stop supporting this resolution in the last graphics drivers?",Negative
AMD,I was going with the 265K over the 9800X3D since the Intel stuff seems to get better 1% lows and smoother experience at 4k and above. But does DLSS change that? Does DLSS lowering the render resolution push the 9800X3D back into the lead?,Neutral
AMD,"u/musicmafia77 After checking our records, I found that the unit you're inquiring about has been discontinued-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html). While direct support is limited to self-help resources and community insights from other users, I'd be happy to extend some steps that should help resolve your issue.     Based on your initial description, here's what I recommend:     First, please review this article-[Customer Support Options for Discontinued Intel® Killer™ Wireless and...](https://www.intel.com/content/www/us/en/support/articles/000059296/wireless.html) and follow the troubleshooting steps provided here-[Clean Installation of Wireless Drivers](https://www.intel.com/content/www/us/en/support/articles/000022173/wireless.html). Once you've updated to a newer wireless driver, I'd suggest reaching out to Dell Technologies for assistance with downloading the appropriate OEM driver for your specific system.     If you prefer a more direct approach, you can:  * Visit Dell's website and download the wireless driver for your adapter by entering your service tag in their support portal * Alternatively, run Dell SupportAssist after completing a clean installation of the wireless driver     For additional troubleshooting, here's another helpful article that covers complete removal of Killer software issues, which may be relevant to your situation.-[How to Perform a Clean Installation to Solve Most Intel® Killer™...](https://www.intel.com/content/www/us/en/support/articles/000058906/wireless/wireless-software.html)     If you've tried these steps and the issue persists, I'd recommend coordinating directly with [Dell's support team](https://www.dell.com/support/home/en-us), as they're best equipped to assist with OEM-specific products and can provide more specialized guidance for your particular system configuration.",Neutral
AMD,"u/SuperV1234 Hi, thanks for the update. I’ve reviewed case number and confirmed that it’s currently being handled by our **warranty team** for replacement. You should be receiving further instructions from them shortly.     [Guide to pack your faulty CPU](https://www.intel.com/content/www/us/en/content-details/841997/guide-to-pack-your-faulty-cpu.html)",Neutral
AMD,"u/triptoasturias this explains, The generic Intel® driver provides users with the latest and greatest feature enhancements and bug fixes that computer manufacturers (OEMs) might not have customized yet. OEM drivers are handpicked and include customized features and solutions to platform-specific needs. Installing Intel generic graphics driver will overwrite your handpicked OEM graphics driver (in your case Dell driver). Users can check for matching OEM driver versions at OEM websites. For more information on how the installation of this driver may impact your OEM customizations, see this [article.](https://www.intel.com/content/www/us/en/support/articles/000096252/graphics.html)",Neutral
AMD,Unfortunately my Dell warranty support has ended and so far the forum there has not been able to help either. The removal tool works but Killer just keeps coming back.,Negative
AMD,"I also went to Dell, typed in my service tag # and cant find any other ethernet drivers.",Negative
AMD,"Most commenters are wrong or didn’t read all the articles. Nvidia wants some custom SKUs of server chips to pair with its data center GPUs, and Intel wanted a better solution for pc parts for the GPU tiles.   NVIDIA is NOT getting an x86 license, nor do they want one. In case you haven’t noticed, Nvidia is a $4T company largely on sales of data center GPUs, sales of which were 10x the gaming segment. Profit/earnings ratio of data center is even higher than the 10x.   In my opinion, the only reason for Intel to use RTX chiplets on pc parts is so they can have a few skus for mobile gaming. Lower end laptop chips would still use Xe cores.  This solves NONE of intel’s structural issues - they need, in this order  1. Major foundry customers  2. Regain server market share 3. Data center GPU offerings 4. Move all pc chiplets back to Intel nodes and start to fill up the fabs again",Neutral
AMD,An Intel + nVidia handheld would be something I would welcome just for DLSS4 alone.,Neutral
AMD,"I don’t think it will change that much overall.  It’s no secret that NVIDIA wanted a x86 license for decades now and now they basically got it. This will ""hurt"" AMD by offering competition to markets where AMD currently holds a monopoly like with Strix Halo, most handhelds and consoles.  I don’t think we’ll see big changes on the Intel side.  I don’t think this will affect neither the iGPUs nor ARC because why would it.",Negative
AMD,"this jointventure is NOT about consumer gpu and does not change anythig about gaming gpu of intel, amd or nvidia.  this is instantly obvioiusly from just reading the headlines of the news announcing this.   this tumbnail is just hub using this news to suggest jet another time that maybe amd might just be screwed. they do this because they are neutral in every way.",Negative
AMD,"i think it's bad for us, consumers",Negative
AMD,Was the team up really to crush AMD or Nvidia's answer to enter China?,Neutral
AMD,AMDware unboxed only cares about AMD anyway,Neutral
AMD,This hurts the arc division way more than this could ever hurt amd.,Negative
AMD,They will crush user's wallet,Neutral
AMD,you guys realize Nvidia buys all their stuff from TSMC the same company that makes all of AMD's stuff...So now intel will buy from them too since they cant compete? Because no amount of Intel+Nvidia humping is going to make a TSMC baby...you got to pay to play.,Neutral
AMD,"Even assuming they made monster co-op AMD still has own ecosystem of GPU and CPU, along with consumer support. It's Radeon and Ryzen are still very strong consumer wise. So no, AMD is not screwed",Positive
AMD,Remember Kaby Lake G? No? This will also be forgotten soon.,Negative
AMD,Yes.,Neutral
AMD,Intel partners with UMC of Taiwan the third largest foundry in the world.    What does it mean for UMC and Intel?   How will it affect transfer / exchange of technology between Taiwan and Intel?   How does this affect TSMC position with USAG?,Neutral
AMD,Foveros baby!,Neutral
AMD,AMDUnboxed on suicide watch.,Neutral
AMD,"For mobile/laptop performance, yes AMD is in the dust, maybe? Makes sense since AMD GPU division is mainly focused on the console partnership with Sony/Microsoft--leaves the mobile platform wide open.",Neutral
AMD,"Who knows, time will tell. But if AMD does find its back against the wall, this is when it does its best. This should be interesting to say the least.",Positive
AMD,"The last time NVIDIA made onboard graphics for a PC, they could hardly be considered graphics. The 3D performance was better than Intel, but features like CUDA, and a good handful of video decoding capabilities were missing. You were much better off getting ATi onboard graphics back then inside of an FX or AMD Bullerdozer chip.  In more recent times, this also applies to laptops with the low end MX chips. Many lack encode/proper decode support, and they are effectively just 3D accelerators. The Intel Onboard Graphics would be a much better choice overall. Whereas an AMD APU would give you the full suite of features.   I'm not entirely worried for AMD. It just means if Intel and NVIDIA manage to not screw up onboard video, AMD will just need to make sure Radeon can remain competitive. I know they have a lot of tricks up their sleeves still, like designing HBM memory onto the die (how Vega was supposed to be originally) and unified memory (soldered down, see the new AI branded chips).",Negative
AMD,Ohh noooerrrrrrrrr,Neutral
AMD,"Actually this may be good for AMD and I will tell you why: Open source drivers. Nvidia if they had a say in intels graphics division would make the intel GPU stuff proprietary and locked down and when support ends its gonna hurt. Meanwhile AMD has open source drivers and in the age that linux is gaining popularity will make it the preferred platform for anyone interested in the growing linux ecosystem. Nvidia will NEED to change how they handle things or intel may not be as attractive as they once were, intel has already suffered a massive knock on thier reputation and this may be the mortal wound that can work in AMD's favor.",Neutral
AMD,"I don't see how this means anything at all for AMD in desktop. This is borderline irrelevant for desktop gaming. Most users pick up an Intel or AMD CPU and an NVIDIA dGPU.  APUs don't really exist in desktop gaming.  In mobile? I think most people won't care much. Intel and AMD APUs both offer compelling and reasonable alternatives for everyday usage. Anyone hoping to game on a laptop as their primary device will choose a model with an NVIDIA dGPU.   On handheld/console, AMD already has the largest market saturation, except for Switch, where Nintendo/NVIDIA opted for an ARM solution.  I don't see this APU being much more than a side choice that offers at best a reasonable alternative to a high end AMD APU, perhaps in the relatively small handheld market, if the performance is significantly better than existing AMD and Intel APUs, due to the DLSS support in a mobile form factor where a dGPU is not a realistic option, and upscaling is necessary for reasonable performance in a limited power package.  In all other cases I dont see this being a compelling option.",Neutral
AMD,So either a 200w igpu with a 300w intel cpu or an intel cpu with a 5030 slapped on it?  I think amd will be fine,Neutral
AMD,welcome to the Nvidia and amd duopoly,Neutral
AMD,"I’m really curious what the power efficiency ends up looking like here - mobile performance/W has continued to be very good in recent Intel platforms, and Nvidia is the market leader in that regard among gpu IHVs. But until now, Nvidia chips being intrinsically dGPUs has completely neutered any potential mobile power efficiency or handheld capability - but not anymore if such a thing can be a chiplet/tile! Interesting days ahead…",Positive
AMD,"By that time amd will have already made an FSR4 capbable handheld, so no significant change",Neutral
AMD,"It won’t affect either cause on parts that don’t need a nvidia igpu why raise the price with it when arc igpu is really good as is, maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive",Neutral
AMD,a partnership doesnt mean they get free reign over license lol,Neutral
AMD,"AMD already has the next gen of consoles. Except for the Switch, which NVIDIA already has (with ARM, not with Intel).",Neutral
AMD,"None, It's a way for NVidia to have another product line and stop Intel from having a competitive product line.",Neutral
AMD,"Not to crush amd, but this essentially just kills arc and hurts the consumer market more. You won't be getting any of those 250 dollar 12gb gpus anymore.",Negative
AMD,Why would it?,Neutral
AMD,"It has nothing to do with the dgpu product stack. Nvidia makes very little from the diypc market. Way more comes from prebuilts, laptops and 90+% from data centers and other enterprise use. In a very real way, the arc cards are not a competitor to Nvidia.",Negative
AMD,"Unfortunately no matter what given Intel's current state the division is going to suffer. Intel cannot afford to invest in products that will take a very long time to generate profit or may never generate profit. If even AMD cannot compete with Nvidia, then Intel will likely never be able too either",Negative
AMD,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Neutral
AMD,Past != Future,Neutral
AMD,"nvidia also used to make motherboard chipset, with mixed success.",Neutral
AMD,FSR 4 looks like the later versions of Dlss 2 did,Neutral
AMD,"This ""insignificant change"" we definitely want though.  Competition is good in a stagnant market...",Positive
AMD,"Other than Halo SKU which we are yet to establish any trend of, AMD notoriously drags their feet. RDNA3 will be with us for a while",Negative
AMD,Wont this be targetting the ML workstation space where you have the iGPU with access to >128GB of RAM like Strix Halo with DMA and/or NVLink banswidth?,Neutral
AMD,">maybe for the upper end they will have nvidia but what’s really stopping them from shipping one with an intel arc tile, gives customers more options and nvidia s Option may end up being more expensive  Possibly an overlap like this would be a waste of money developing, and tbh I highly doubt customers would go for the Intel option rather than a Nvidia one, *even if* the Intel option is better performing, because of Nvidia's insane mind share.",Neutral
AMD,Never said that.,Neutral
AMD,Also a way for Nvidia to invest some of their funny money (which is essentially infinite) into a cheap company. Intel under $20 was ludicrous. Intel is still at book value at $29.,Neutral
AMD,Why do so many people think that this will kill ARC?,Negative
AMD,The market for Arc is the same as for Nvidia.,Neutral
AMD,Was that back in the MacBooks having the NVIDIA 9400m paired with a Core2Duo? Those were solid.,Positive
AMD,"No, but it is a reminder on shortcomings to watch out for. The Intel + AMD Partnership with the Intel NUCs several years ago made for some pretty compelling hardware. I still have a NUC8i7HVK in service which works like a champ.  If we are talking about current day NVIDIA with their Discrete Graphics (not the MX budget crap) then their GPUs are obviously smoking anything AMD and Intel. That doesn't resolve some of the Linux problems that still remain with NVIDIA, and that's not going to stop some of the BS NVIDIA does with their hardware around NVENC/NVDEC stream limits. But they have raised those limits, and the open NVIDIA drivers are getting better in time.",Positive
AMD,"Yup. Their nForce boards were great for overclocking, but rather buggy too.",Positive
AMD,"It's not a later version of dlss 2, it's between dlss 3 and 4. Just go watch some videos comparing both.   And btw redstone is coming probably at the end of this year which will improve ray tracing quality and frame gen performance.",Neutral
AMD,"No reputable reviewer thinks that. Most I've seen say it's better than DLSS 3, and very slightly behind DLSS 4.  Why lie about this?",Negative
AMD,They literally haven't said a word about it everyone's just speculating off of Moore's laws anti Intel videos lol consumer is the last thing they care for atm and might be server first cause that makes the most financial sense,Negative
AMD,If they do that in the case the two companies deal fall apart Intel is screwed with no graphics to compete with even the basic amd ones,Negative
AMD,"Well phrased, They now have a seat at the table of Intel to sway votes enough to dissuade them from making competitive products",Neutral
AMD,"Too many people are peeping that MLID podcast. Dude there is very confrontative about his theory that ARC is being ""effectively"" canceled because of some very early roadmap not paying out. Everything is a confirmation bias at this point.   I on the other hand don't get why would Intel forfeit it's competitive advantage with GPU accelerated AI, XeSS or QuickSync and made itself dependant on nVidia. Doesn't make sense especially now, when they have significant influx of cash from both Nvidia and government.",Negative
AMD,"Arc was already not doing well long before. Pouring millions into their gpu division every year with no returns. Their own road maps had their gpus coming out a year sooner but everything got delayed with them resizing the company. Like this deal is not exactly what killed arc, but arc was already struggling beforehand so this was most likely the nail in the coffin.",Negative
AMD,Nvidia does not have an A310 competitor.,Neutral
AMD,But I still remember how many motherboards with Nvidia chipsets I replaced way back.  They used to reliably overheat and need a reflow.  I think it was the HP DV6000 and DV9000 series.,Neutral
AMD,I have not lied,Neutral
AMD,"While workstations dont directly contribute a big chunk of the sales, I suspect they dont want to yield their foothold on the ML/Data Science guys.  If they start getting familiar with non-nVIDIA ecosystems, it could mean indirect losses when these guys start procuring non-nVIDIA dGPUs in servers when they do port their workstation experiments to ""larger"" hardware now that AI is all the rage.  B60 is gaining momentum on homelabs, and Strix Halo to some extent getting popular for those needing more memory for their larger datasets.",Neutral
AMD,"MLID has used clickbait trash thumbnails/titles way too many times, no one should watch that garbage channel to begin with.",Negative
AMD,"I think Intel priced in multiple generations of taking losses when they decided to enter the GPU market. Essentially impossible to walk in to a mature duopoly market and immediately start turning profit as a new player, even with Intel's resources.  Of course I think they also had much rosier projections for their other ventures when they started this journey, so who knows if the GPU losses are still tolerable.",Negative
AMD,Intel doesn't have a current gen A310 competitor either.,Neutral
AMD,Theres no way that nvidia is gonna let intel keep producing entry level gpus that directly compete with their own. I know their deals are in regards to apus and to compete with strix halo but nvidia is not gonna invest into a company with a competitor to their gpus. Like a b580 destroys the 5060 and 5050 while costing less or the same amount. There's no way nvidia invests 5 billion without some agreement that kills off arc as long as they are together.,Negative
AMD,Their nForce boards were excellent for overclocking and tinkering! That's for sure. I also remember them being a bit buggy lol.,Positive
AMD,IIRC that wss the AMD Turion X2 + NVIDIA 7300Go laptops. Those were notorious for dying.   I also remember some budget laptops with the Turion X2 + ATi 3200 chopsrt graphics would die due to graphics desoldering off the board. The GPU would sit at the end of the heatsink and burn up.   The 9400M MacBooks I remember would get hot enough to start leaving powder from the chassis on my desk lol.,Neutral
AMD,"You did tho. It doesn't look like ""later versions of DLSS 2"". Every reputable source has said it's better than DLSS 3 and close to DLSS 4",Positive
AMD,"They're already at 2, going on 3, generations of losses though (Intel discrete GPU's basically have 0 marketshare, even worse than AMD who are also doing really badly) so even if that were true that isn't a reason to be optimistic.  As a company they also seem to be desperate to get income as well and so there is no reason to assume they're going to tolerate that situation for all that much longer.    I doubt they'll entirely kill off their GPU division, they can still do iGPU's which will be good enough and cheaper than paying NV a royalty, but the discrete GPU line is clearly in trouble.",Negative
AMD,A310 is cheap enough and good enough for it's purpose. If it ain't broke why fix it,Positive
AMD,"Wrong man. You just don’t understand how small the diypc market is compared to prebuilts, laptops, and enterprise- it’s a fraction of one percent.",Negative
AMD,The later versions of Dlss 2 look like Dlss 3,Neutral
AMD,Nvidia probably feels the same about their low end SKUs.,Neutral
AMD,"The DIYPC market largely drives the “prebuilt” market, with a 1-3 generation lag.  Many people who buy “prebuilts” have people they go to for advice (i.e. they talk to their friends/family members who DIY). When there’s a big swing in the DIY market, it typically ripples out to the prebuilt market over a few years/generations.",Neutral
AMD,"Lol, what an explanation hahahaha   And that's still not ""better than DLSS 3"" as most say",Negative
AMD,Yeah lol,Neutral
AMD,AMD Unboxed say anything about that? What about Linus Cheap Tips!,Neutral
AMD,"""Everyone I don't like is biased""-ass answer",Negative
AMD,"can someone explain how this might effect me, as a long term holder, as if I am a golden retriever?",Neutral
AMD,The MTL/ARL/LNL Dates are all wrong for CPU launch it's an OEM roadmap not Intel.,Negative
AMD,Medusa Ryzen... Sounds like something I'd rather avoid in real life.,Positive
AMD,Panther Lake is slipping to Q2 2026 now?    It was originally supposed to launch in Q4 2025,Neutral
AMD,Just hodl until you get the biscuits,Neutral
AMD,"If leak is correct it's bad news.   Panther lake q2 2026, Nova lake q2 2027.    It means 18A will arrive later than TSMC 2N.    Nova lake will likely arrive after Ryzen successors.    Combine that with other performance rumors where there isn't a huge performance uplift. It's bad news.",Negative
AMD,"The dates aren't wrong. As you said, It's the OEM roadmap, and they follow a Q2 release cadence.",Neutral
AMD,Gate All around and Backside power was supposed to be in 2024...  Now it's q2 2026. At this point it's 4N7Y... Which is what we call TSMC cadence.,Neutral
AMD,"This isn't Intel and AMD's road map, this is an ""undisclosed OEM's"" road map.   This doesnt tell us anything about PTL's launch. Just when this OEM will refresh their laptops to have PTL, and its normal for OEMs to launch refreshed models at certain times.   Dell, for example, will usually do Spring, so this may be them. Surface is notorious for being 3 or so quarters after CPU launch.  Look at where MTL is on this schedule, for example. Also around Q2, even though it had been on store shelves technically since the previous year's Q4.  Edit: Looking at the roadmap a second time, this OEM seems to always target a Q2 launch cycle: ADL, RPL, MTL, ARL, PTL, and NVL all follow it. LNL is the only outlier.",Neutral
AMD,I thought Arrow Lake refresh was in the cards for 2025.,Neutral
AMD,Wasn’t Panther Lake going to use TSMC for a portion of the volume?,Neutral
AMD,Yeah I forget to to mention wrong In the sense that it is not Intel CPU Launch OEM roadmap.,Negative
AMD,This entire thing is a mobile roadmap so why are you here?,Negative
AMD,"This ""undisclosed OEM"" is Lenovo by the looks of the font used. Also the timeline makes sense. For example Lunar Lake is shown to be in very late 2024. Lenovo had only one model of Lunar Lake available at that time - the Yoga Aura Edition.",Neutral
AMD,"Yeah if it's Surface roadmap, it's a nothing burger.",Neutral
AMD,Yeah some OEMs don't refresh their lineups until the new generation is already launched,Neutral
AMD,I think it was for the iGPU but I don’t remember. I just know the majority of PTL is 18A. I saw a rumor that they were using Intel 3 for the iGPU also so idk.,Neutral
AMD,I also think it's Lenovo. Q2 is when they refresh their ThinkPad line,Neutral
AMD,The 4 Xe3 core iGPU will use Intel 3. The 12 Xe3 core iGPU will use N3E.,Neutral
AMD,"Wonder if Intel will do stacking two bLLCs on a single CPU tile like rumoured as an option with AMD's Zen 6. I iknow this is basically both Zen 5 CCDs having each an 3D v-Cache and NVL seemingly going to have that option now. But two bLLCs meaning 288MBs of stacked cache + cache in compute die would be insane and crazy for gaming.  Regardless I'd love a P Core only RZL SKU with much stacked cache as possible, or heck just 48MB 12P Griffin Cove + 144MB L3 bLLC would be sweet.",Neutral
AMD,"I like how they just throw random words around to pad their ""article"".",Neutral
AMD,The only reason I don't have a dual CCD ryzen is the lack of X3D on the 2nd CCD.  I would 100% be in for a 16 or 24 (when they go 12 core CCD) core dual X3D.,Neutral
AMD,"""leaks""",Neutral
AMD,"When are people going to realize ""Leaks"" are marketing ploys to get eyes on product. Come on people.",Negative
AMD,"Considering the latency is not even expected to be any good with BLLC, AMD is seemingly quite a careful opponent for Intel indeed",Negative
AMD,Scared of their rumor?  Lets release our rumor!,Neutral
AMD,I don't think 2x bLLC could fit on the package. Sounds like nonsense to me.,Negative
AMD,it seems as if right now the best option for consumers is to just build a cheap 1700 or AM4 system and call it a day for the next 4-5 years or so. but that's just me,Neutral
AMD,"My god.  The only reason huge caches benefit gaming, is to benefit rushes to shipment by the publisher, so that game engineers either don't have to optimize, or they can use garbage like Unity, or both.  This is going to make so many mediocre developers better... except no it won't.  It'll just get the market flooded with more garbage.  Also... diminishing returns, and I doubt AMD will put this in Zen 5 -- seems like a trash rumor, to me.",Negative
AMD,"3D cache will eventually become full of assets and dip, if and onlyif IMC is bottlenecked by random memory access.  - the ability for cache to transfer data from memory, to IMC, to cache has a FPS hit (cache hit-rate). This is due to the bottleneck of a chiplet-based IOD: compromising memory latency with cache hit-rate. Eventually the cache fills and relying on IMC to feed the cores anyway.  You can easily test this by monitoring 1% lows and increasing memory latency (lowering DDR4 B-die clocks/DDR5 hynix and increasing timings) to see how Cache holds up when full of assets while memory is being bottlenecked-   What games need is 10-12 P-Cores and an IMC capable of high speeds. Within a monolithic die to reduce relying on cache hit-rate and IOD chiplet bottlenecks.",Neutral
AMD,This is all they needed to sell for gaming again and yet they gave us all the e cores no one asked for jfc.,Negative
AMD,bLLC is not stacked cache,Neutral
AMD,What do you consider random? The article was perfectly clear.,Neutral
AMD,Do you have a specific application in mind? I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.,Neutral
AMD,Some rumors of 16-core dual X3D chip actually surfaced yesterday...,Neutral
AMD,"Isn't the latency similar? The advantage of the 3d cache is an even larger cache size, while the advantage of what intel is going to do with bllc is that there is less of a thermal issue so the clocks can be higher.",Neutral
AMD,Probably only on the skus with less cores.,Neutral
AMD,"They would have trouble fitting just 1 bLLC onto a package, 2 bLLC might only be viable for something like a server chip with a much larger socket.   It may be possible once Intel starts 3D stacking they're CPUs though, but we have to wait and see.",Neutral
AMD,Yeah. Definitely just you,Neutral
AMD,You could literally make that claim with any CPU performance increase.,Neutral
AMD,Speak for yourself. Skymont ecores have raptor lake IPC. We need more of those,Neutral
AMD,"What CPU do you have? If its Zen 4 or slower, your cores are same or weaker than Arrowlake E cores in IPC",Neutral
AMD,This is correct. Stacked cashe comes later. I'd say around 1 to 2 years after. And that comes with stacked cores as well if they go with that path.,Neutral
AMD,"Correlating asset pop in with memory latency is utterly nonsensical.   Edit: Lmao, blocked.  Also, to the comment below, the creator of that video clearly has no clue what there're talking about. There's no ""bottleneck between cache and memory"" with v-cache. The opposite, if anything. Just another ragebait youtuber who doesn't know how to run a test.",Neutral
AMD,"> I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.  It does, but i'd rather every core be equal in terms of per-core cache in a local sense. Thread scheduler can handle the CCD split (usually)  There are definitely workloads that benefit from X3D but either scale to many threads or are let down by poor OS scheduling.  It could even be a boon to gaming with well thought out core pinning of game threads.",Neutral
AMD,"No, because it would be the same die. Just won't fit.",Negative
AMD,He has a fair argument with the horrible optimization of modern video games but any innovation in the CPU space is a good innovation at this fucking point,Negative
AMD,Id love a 200 e core cpu with lots of pcie lanes for my home server. But I do not want mixed cores.,Neutral
AMD,Yea but I’m not paying for e cores that are barely faster than my current skylake cores. I just want p cores only.,Negative
AMD,"IPC is just 1 factor  Zen 4 clocks much higher, and got a far superior low latency caching subsystem   No one sane thinks that Skymont can actually beat Zen 4 in most workloads Only if you downclock Zen 4... and guve it slow ram to increase latency",Neutral
AMD,"Now pay attention to the memory bottleneck of chiplet IMC architectures on x3d chips:P https://youtu.be/Q-1W-VxWgsw?si=JVokm7iLScb7xynU&t=700  This simple 1% lows test indicates how texture-pop can impact frametime, directly. Since assets are first stored in memory, then fed to cache, any bottleneck between Cache - to memory will cause delays when loading asset thus 1% FPS... measured temporally(in this case).",Neutral
AMD,The bad optimisation critics aremostly people trying to sound smart by rambling things they dont understand.  Game engines are very optimized these days.   Its funny they mostly pick on Unreal Engine 5 as the prime example... That is probably the most complete game engine out there. So naturally its hard to make all that stuff work well togetter... They have litterally some of the best coders and researchers in the business ... Id love to see you do a better job.,Negative
AMD,Mixed cores are awesome if the scheduler would do what i want and there lies the problem and i dont think even AI can solve that yet.,Negative
AMD,"I said IPC of Zen 4 is the SAME and weaker than Zen 4 is weaker than Skymont in IPC. No more no less. As for clocks, the core scales to max 5.2ghz (default 4.6ghz), certainly overall slower than Zen 4. IPC is a comparison about throughput at a given clockspeed. Its a give to compare you will need to match the clocks.  Funny you you should mention latencies, considering that core to core (even accross clusters), latencies of Skymont are better than Lion Cove? So if latency is the issue, it has nothing to do with Skymont and everything to do with the trash uncore they have based on the Meteorlake SOC. Even workstation Crestmont on a different package performs better. Furthermore skymont IPC is measured based on Arrowlake. Arrowlake is worse latency than Raptorlake and Zen already. Latencies have been accounted for.",Negative
AMD,"I play Warthunder almost daily. It’s ridiculously fucking optimized.   I got cyberpunk when it came out, it was a shit show. Elden ring? Mediocre.  Escape from tarkov? Fucking laughable.  Call of duty? 500 terabytes. Besides this small list,  I’ll wait for battlefield 6 (which by the way also has a history of releasing unoptimized)  The one thing I will say is that developers do work hard after releases to optimize their games. I’m just so use to playing Warthunder which almost always works, everyday, perfectly, with no complaints. lol",Negative
AMD,"Do tell how a bigger cache makes games faster, if you are aware of some details.  And, of course, how doubling the already tripled cache will yield perf improvements worth the huge amount of extra money to make such chips.",Neutral
AMD,Any efforts of Intel to improve efficiency and even performance on old products is a pro-consumer move. Good to see.,Positive
AMD,It's not sorcery. Its just Intel doing the game developers work.,Neutral
AMD,"Is it updated? Last time I tried it, it didn't have an uninstaller and not many games were supported. In the ones I tried, there was no difference at all.",Negative
AMD,"It's very specific scenarios where this can unlock new performance, most games it won't do anything or even lower performance.     So they can't *really expand it* since it's an uncommon scenario. That's why we have so few games that benefit from it.",Negative
AMD,Never count out Intel. They have some very talented people over there.,Positive
AMD,"Regarding the cache optimizations you pointed out, I have a question. Can you test all APO supported games you own and capture the utilization of the E-cores ?  My understanding is that Intel is keeping 1 E-core per cluster active and parking the remaining 3 E-cores. So the lone E-core of each cluster has full access to the 4mb shared L2. For the i9 they can have 4 such solitary E-cores that have their own cache and hence do not touch the L3 cache. The 4 E-cores handle background task while the 8 P-cores with the full 36mb L3 cache deal with the games.  Please correct me if I am wrong.  Also, what cpu cooler do you use ?",Neutral
AMD,What about this post saying APO havent gotten anything in over a year.. https://www.reddit.com/r/intel/s/tTKCn1CnDZ,Neutral
AMD,Does anyone know if changing the Windows power plan affects performance? What Windows power plan do you recommend for the i7 14700kf? Balanced? High performance?,Neutral
AMD,I wonder why APO isn't just tied to game mode. That way it would work for all games from ADL to ARL.,Neutral
AMD,"or just get rid of E cores. I know all the non engineers in here think they are great, but real world they are crap, this is just another in a long line of examples of why intel needs to ditch them.   anyway go ahead and downvoted me becuse being correct in this sub is looked down upon.",Negative
AMD,A few years ago you'd have to tweak the hell out of AMD CPUs to get comparable performance to Intel equivalents. Now the tables have turned.  In either case those who know how to tweak will get top performance not matter the platform.     Kudos,Positive
AMD,Let me teach you a trick.... there is a hack to disable e cores for every game using Intel compatibility tool.... you can try yourself to see if it works,Neutral
AMD,I will follow all!,Positive
AMD,Congratulations on getting a CPU that didn’t fry itself.  Consider yourself one of the few lucky ones.,Positive
AMD,"It looks like Intel APO almost completely disables the efficiency cores for the game when you look at the screenshots I posted below with per core activity.  But this performance boost likely cannot just be gained by turning off the efficiency cores in the BIOS yourself I wager, because it would be too simple.",Negative
AMD,How is this doing the game developers work?,Neutral
AMD,"It's updated as far as I know, but not for every platform.  If you're on Arrow Lake you have the new games that were added, but if you're still on Raptor Lake then you only have the original titles.     This sucks because I own BG3, which is one of the new titles they added recently along with Cyberpunk 2077.",Negative
AMD,"They have added a few new games, but it appears you need to have Arrow Lake to be able to apply APO on them which is BS!  They need to have a better system than this.       The DTT driver should be downloadable from Intel, and not from the motherboard manufacturers who cannot be trusted to make the latest version available.",Negative
AMD,The intel software team is pure black magic when they allowed to work on crack.,Neutral
AMD,"Here is a screenshot showing CPU activity across all cores.  This is with HT disabled BTW, and the cooler is an Id FROZN A720.  It looks like the efficiency cores are not engaged in this particular game when APO is enabled.  And Metro Exodus is the only game that I own in the current lineup for the 14900K.  I know they've added some games for Arrow Lake (like Cyberpunk, Baldur's Gate 3) but those games still aren't available yet for the 14900K     [https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png](https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png)",Neutral
AMD,"Raptor Lake isn't getting new games for APO.  Which strikes me as greedy, since I'm sure it would work fine. Intel originally said that APO wouldn't work at all on 13th gen, which turned out to be completely false.",Negative
AMD,"When combined with Nova Lake's bLLC, it could give Intel a major edge over AMD's X3D.       And yeah, limiting it to certain games and certain CPUs is a real downer for the technology, so hopefully Intel can find a way to streamline and expand it when Nova Lake launches.",Neutral
AMD,"AFAIK, it's recommended to use Balanced mode for proper use of the efficiency cores if you have a Raptor Lake or Arrow Lake CPU",Neutral
AMD,Balanced in full load will just do the same thing as High Performance.,Neutral
AMD,"It's capable of a few things, but the main mechanism from what I've seen is that it temporarily disables three out of four cores in each E-core cluster by forcing them into the lowest power state. The result is that the remaining active core has access to each cluster's full L2 cache (2MB on 12th-gen, 4MB on 13th/14th-gen). L2 cache isn't shared with P-cores (L3 is global), so this can really minimize E-core cache evictions before they're forced into slower memory, and games do love themselves some cache.  It's a genuinely clever way of maximizing available resources and I really wish they'd allow user control over its features, but it seems to be pretty tightly leashed to/by the team that developed it. It obviously wouldn't benefit every situation, such as particularly low/high thread occupancy situations, but it's pretty rough to have the option tied to quarterly updates for one or two specific games.",Neutral
AMD,Or... Just process lasso.,Neutral
AMD,Because it’s optimizations on how it can efficiently use the cpu.,Neutral
AMD,Must the App be downloaded? Or it's automatic after installing the PPM and DTT drivers?,Neutral
AMD,"All of this stuff is super situational. For when it’s supported for my 14th G, I love it.  To call it better than X3D though? That’s a bit much.",Positive
AMD,They are they came up with ML for their upscaller before the red team did and even better than FSR as a dump upscaler and they also did a good RT implementation. But the hardware still has raw horse power issues and they already use a big die for what it can do.  In the 80's or they shammed The DEC Alpha team on subnormal floating point operations with the 8087 FPU. Intel was doing way more precession without having to round so earlier on a consumer FPU while the Alpha CPU was a Mainframe one. Intel also gave it up to help estandardize the way CPUs handle floating point types.  Intel compilers were also black magic becuase they optimized so much C and C++ for Intel specifically but there was always controversy causeless AMD x86 couldn't take part of those optimizations,Neutral
AMD,Were there any background tasks running when the screenshot was captured ? I want to see how the E-cores are assigned for the background tasks under APO.,Neutral
AMD,In certain games. It's not universal at all. There would be scenarios where it gets an edge but if AMD is pushing as hard as rumors state they may still have the better CPUs overall.,Neutral
AMD,"Great explanation that makes sense!  It definitely has to do with the cache, that much is true.   A Raptor Lake CPU with 8 P cores and no efficiency cores, with 36MB of L3 would perform exceptionally well in games.   But the efficiency cores have their uses as well in highly parallel tasks.",Positive
AMD,"Yeah it's a pretty cool, yet simple idea. It would be really nice to give users more granular control natively.",Positive
AMD,Are the e cores clustered in order so we would be able to manually achieve this with process lasso?,Neutral
AMD,Why should game devs be automatically expected to specifically optimize their game for specific architectures of Intel's CPUs? It's not on them to do so.,Negative
AMD,You need to download the Intel Application Optimization app from the Windows store,Neutral
AMD,"I never said it was better than X3D, only that it is useful as a foil.       Remember that Nova Lake will have bLLC, which is the answer to X3D.  But in conjunction with APO, it may give Intel the edge and help them regain the gaming crown in the next cycle provided they can streamline and expand the technology.",Positive
AMD,"Actually, it is better than X3D in almost every GPU/CPU combo where the GPU throttles. Intel 14th gen almost universally wins 1% lows and frequently FPS - almost universally in 4k gaming, but as I said, when the GPU throttles at any resolution, which is almost always, Intel wins.",Positive
AMD,"Eh, they're very good but they benefited vastly from AMDs mistakes and the lessons learned from it. Remember AMD didn't have any ai hardware for their first 2 gens on RDNA and RDNA 3 didn't have enough for FSR4 either. It's not that intel were better at producing a ML upscaler. They just had the hardware for it from the get go so they could implement it quicker.  They did make Xess though which is miles ahead of FSR 3 even on AMD systems (on average. FSR3 is pretty good when modded and can be competitive) so they do have some extremely talented people there. I hope they fund them properly and don't go the AMD route of having Ryzen and Radeon not work together due to pride.",Positive
AMD,"Just  Steam, MSI Afterburner, Windows Security, Nvidia App and my sound card command center",Neutral
AMD,Where to download this APO,Neutral
AMD,Except its THE game devs job to optimize games for multiple cpus and gpus.,Neutral
AMD,It’s literally their job to do so? wtf you talking about?,Negative
AMD,AMD also has Heterogeneous CPUs and even more so ARM cpus.  Devs should start to do this optimizations to schedule the stronger cores.,Neutral
AMD,"Will do, I got a 265K. Performance is already great tbh.",Positive
AMD,I love Intel but with how the company is doing right now I’m in a “I’ll believe it when I see it” type of skepticism.,Neutral
AMD,"Sorry I am making unreasonable requests. Can you test the game with APO on, and a few background tasks ? Like a few browser tabs, a youtube video playing, discord etc ? I am interested in the E-core usage, particularly the spread of E-core usage.   Thanks.",Neutral
AMD,"You need to enable it in the BIOS, me ale sure the DTT driver is installed and then download the app from the Windows 11 store",Neutral
AMD,"Which they already do, and are often overworked and underappreciated for.   But sure, it's their fault they didn't optimize the game even more for a subset of already relatively high end CPUs....",Negative
AMD,"You are massively oversimplifying. A lot of games that support APO, like Metro Exodus EE OP used as an example, were released before Intel released CPUs with P and E cores. Expecting studios to rework and optimize old games for new CPUs is very naive.",Negative
AMD,"They should be focusing on optimizations that help the vast majority of all CPUs, especially older/lower end ones.   It's not their job to specifically help one companies latest CPU architectures because they couldn't figure out how to create a core that has both good area and performance.",Negative
AMD,That's not simply what APO does.,Neutral
AMD,"You should have the full list of current APO optimized titles, unlike myself which only has the original list LOL!",Neutral
AMD,"Everyone should be skeptical, sure, but I choose to be hopefully optimistic also.",Neutral
AMD,"I booted it up again, this time with three Chrome tabs open including one that had an active YouTube video playing.  The efficiency cores were definitely being utilized for the background tasks I believe as I could see a small bit of load going from core to core.     [https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png](https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png)",Neutral
AMD,i am not talking about older games. i am talking about newer games.... really dude?,Neutral
AMD,Ok…all CPUs include Intel CPUs…I don’t get your thought process,Neutral
AMD,"According to Intel, the app is entirely optional. The DTT driver already includes APO, the app is just an interface to control it and disable APO, if needed.",Neutral
AMD,Thanks a lot.,Positive
AMD,> i am not talking about older games. i am talking about newer games.... really dude?  New games seldom have problems with P and E cores. That is why if you took at minute to look at games that APO support you would notice almost all were released before 2022 before release of Alder lake CPUs. Because older P and E unaware games struggle. Don't really dude me if you are this clueless.,Negative
AMD,"APO includes specific optimizations made for specific Intel skus, not general optimizations that help pretty much all processors.",Neutral
AMD,That is not true whatsoever. Games do not properly utilize the p and e cores and unreal 5 is very known for this.,Negative
AMD,Ok…? If there is a specific problem then they need to optimize it for that hardware. Literally a devs job. Still don’t get your thought process,Negative
AMD,"Dude, Unreal 5 runs like crap on pretty much all hardware. It stutters on AMD and nVidia graphics. Unreal 5 doesn't count.",Negative
AMD,"There isn't a specific problem, it still works lol, just not as well as it could on a specific architecture.   They shouldn't be wasting their finite resources/time on optimizing specifically for an already relatively high end and well performing architecture, but rather attempting more generalized optimizations that help all users.   It really shouldn't be too hard to understand.",Neutral
AMD,"Very cool. Cheers Intel! As a 6900XT owner, I use Xess whenever it's available. The DP4A version got very solid once V1.3 hit.  Hopefully more devs implement it.",Positive
AMD,"I like how Intel still support DP4A model. My meteor lake chip gonna love this XeSS2 with FG and LL.  Meanwhile Amd give middle finger to their consumer who don't have radeon 9000 series, FSR4 doesn't works at all on their previous GPU including the flagship rx 7000 series.",Positive
AMD,Did the DP4A version also improve from 1.3 to 2.0?,Neutral
AMD,"I thought it was a mix of both shader model 6.4+ and DP4a, not either or",Neutral
AMD,"i don't get the performance i got with FSR, but damn, FSR looks like crap in every aspect, i got less performance with xess in comparison, but at least got better framerats than native and get a better image quality",Negative
AMD,Okay but why would I want to use that instead of NVIDIA DLSS?,Neutral
AMD,It’s the least you should get after not getting FSR4.,Negative
AMD,You'd use it over FSR if that's available too?,Neutral
AMD,Except you NEVER got acces to the DP4a version tho. Intel cross vendor locked the DP4a path to their igpus and only allowed Shader Model 6.4 version for other gpus.,Neutral
AMD,"I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4.   On Linux they managed to emulate it using the FP16 instruction, which is available on RDNA3, but it seriously impacts performance and introduces 1,5ms of input latency, compared to the 0,5/0,8ms of latency on RDNA4 with hardware FP8.",Negative
AMD,And they expect people who bought previous RDNA to buy more RDNA,Neutral
AMD,Not by a significant amount.,Neutral
AMD,DP4a only relevant for intel igpus as its crpss vendor locked by intel. Other gpus only get the Shader Model 6.4 version.,Neutral
AMD,for the games that dont support DLSS,Neutral
AMD,"If you own a card that doesn't support dlss.   This is an extra option, it doesn't affect your choice to run dlss or not.",Negative
AMD,10 series cards will benefit from this,Positive
AMD,It's great news for gpu that can't use dlss because fsr3 is a blurry shitfest.,Positive
AMD,"I mean it gives better image quality than FSR 3.1, so I know I definitely do.",Positive
AMD,Sure. The image quality is generally better. I'd only use FSR if I was trying to squeeze every last drop of performance out of my GPU since it's faster than Xess DP4a. But Xess will be tried first.,Positive
AMD,Do you have a link? Anything I have seen points to the GPU using Xess DP4A unless it's an ARC GPU. So it supports RDNA2 and newer on AMD and Turing and newer on Nvidia.  https://www.pcgamer.com/intel-xess-compatibility-nvidia-amd/  And I do literally run Xess on my 6900XT (currently using it for F7 Rebirth through Optiscaler). So if I'm not running Xess DP4a then what am I running?,Neutral
AMD,">I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4   Sure, but they can make FSR4 for non hardware accelerated like Intel right? FSR 3.1 is so bad even when compared to Intel XeSS 1.3 DP4A, but Amd refuse to improve it because they want to force people to buy radeon 9000 series.",Negative
AMD,"Amd really all in when it comes to copying Nvidia. Guess what? The more you buy, the more you save!",Neutral
AMD,DP4a is cross vendor.   XMX is Arc only.,Neutral
AMD,DLSS is the widely adopted upscaler in the current market. I'd be shocked if a major game in 2025 releases without DLSS at the very least.,Neutral
AMD,1080ti heard no bell,Neutral
AMD,"You run the Shader Model 6.4 version which i stated. There was a discussiona bout DP4a being cross vendor locked but for the life of me i cannot find it again. The article you linked is from 2021, in early 2022 intel cross vendor locked DP4a. The main article of the post clearly states Shader Model 6.4.",Neutral
AMD,where did amd touch you bud?,Neutral
AMD,"Yeah more and more games are adding DLSS support, so I'm not sure when I would need Xess",Neutral
AMD,"That's true but in edge cases where even the lightest Xess option is still too heavy, FSR may still deliver a better overall experience.   E.g. CP2077 on the steam Deck in Dog Town. When I was testing some stuff out (trying to see how close I could get my Deck to the Switch 2 build) I found Xess to be a unsuitable. Even on the lightest Xess mode, the performance hit was a bit too noticeable. FSR doesn't look as good as Xess but it runs better and you can get away with a bit more upscaling artifacting on the Decks relatively small screen. Just rendering at a lower res and letting the screen upscale was also a solid option in that scenario.  As I say though, that's an edge case where you really just need outright speed. If your GPU doesn't support DLSS or FSR4 then using Xess makes sense in the vast majority of cases.",Neutral
AMD,Are you sure you're not getting the DP4A/Shader Model 6.4 and Arc exclusive version mixed up? DP4a was added to Shader Model 6.4. https://github.com/microsoft/DirectXShaderCompiler/wiki/Shader-Model-6.4.  There is a separate path of Xess that is better than DP4A but that only works on Arc Cards.,Neutral
AMD,"It's good to see finally Intel working with game developer to bring optimization to Intel platform. This time not only optimization for Arc GPU, but also for Intel Core CPU like proper CPU scheduling for P and E cores to make sure the game runs on the right thread.  I think this also the reason why they never update APO game support again. Obviously optimizing their platform by working directly with game developer is better than doing it by themself.  I really hope we got optimization like this for more titles.",Positive
AMD,News like this will never been posted on Yahoo Finance or Stocktwits... Instead the news from Short-Sellers and Paid Bashers ...,Negative
AMD,"Im not impressed, my i9 12900k is bottlenecking my 4080 like crazy in bf6 beta :s  Im only getting 100-140 fps on my cpu while my gpu wants me to have 200. Any kind soul who can explain how i increase this?  My cpu is right out the box, i have not done any OC what so ever",Negative
AMD,13700k vs 9800x3d BF6 Beta menu cpu fps comparison (105 vs 370 fps) [https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8](https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8),Neutral
AMD,You sound like an intel stock holder lmao The markets don't care about gaming at all.  They only care about AI slop,Negative
AMD,"the game is just really latency sensitive, i dont think it will even change in the full release sadly  i kinda wonder what makes the game so demanding on CPUs, the mechanics and physics arent much better compared to older BF (like BF4) that barely use modern CPUs beyond 20 - 30%",Negative
AMD,"Dealing with same issue on a 285k + 5090. dipping to 125 fps curing certain scenarios specifically on the cairo map at point C. MY system should not struggle at all at 1440p to reach 144 fps, but the 285k is getting absolutely destroyed by bf6 and I have no idea what the issue is,",Negative
AMD,"It's not like game runs bad on Intel CPU which has P and E cores, i will be lying if i say that. It's about maximizing CPU performance. CPU demanding game will runs with much higher FPS when it knows which thread should be utilized.    This is why game with APO enable like R6S on i9-14900K at 1080p low settings, it got performance increase up to 100FPS which is massive.",Neutral
AMD,"Most games are optimized for consoles which are natively eight cores, and a lot are ported over to PC and so you basically need eight fast cores in whatever CPU you happen to have, and the rest don't really do much. That's why Intel HEDT and AMD's Threadripper aren't of much use in this regard.",Neutral
AMD,"No, that's because and isn't really better for gaming unless you get their x3d variants. Those have a different style of memory that is better for gaming in particular but can underperform when it comes to workload tasks. Idk why 🤷",Negative
AMD,"Yeah and in a game like bf, I only care about the 1% low staying at minimum my monitor refresh rate 🫠 But I have not spend that much time optimizing my settings so I think I can work around it",Neutral
AMD,"I found a ""fix""   Dynamic render scale, I set it to 75, so when shit really hits the fan, it just lowers the render scale for just enough time for me to not drop below my monitor refreshrate, and it happens so fast I don't see the change in gfx",Negative
AMD,"From what I understand, many games are generally fairly latency sensitive. The fast cache on the X3D chips basically helps with that.",Neutral
AMD,Thank you :),Positive
AMD,Wouldn't the 300 series actually be Arrow Lake Refresh?,Neutral
AMD,"Videocardz is wrong      This chip will likely be called ""Core Ultra 5 445k"" it's an 8+16 die with bLLC + 4LPe   (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")    Ultra 3 has 4P + 4E + 4LPe  Ultra 5 has the 1x 8+16 tile + 4LPE   Ultra 7 has 8+6 P-Cores and 16+8 E-Cores +4LPe   Ultra 9 had 2x 8+16 tiles + 4LPe tile   Ultra 300 series will be reserved for mobile only Panther Lake and/or Arrow Lake Refresh   Ultra 400 will almost certainly be Nova Lake.",Neutral
AMD,I think this competition will tight up alot in the next gen for both intel and AMD.  Intel has had a superior memory controller for awhile now and AMD will finally address this in Zen 6.  AMD has better cache setup with VCache which Intel will finally address in NL.  May the superior arch win!,Positive
AMD,This is pat's work,Neutral
AMD,This is normal Nova Lake or the bLLC version? I heard the bLLC version has separate SKUs.,Neutral
AMD,8 Big cores with loads of cache and then a couple of E cores with their own cache for background tasks would be great.,Positive
AMD,Is Intel finally making a comeback with their cpus? I hope so,Positive
AMD,Large L3 cache without reducing latency will be fun to watch.,Positive
AMD,the specs sure do shift a lot..,Neutral
AMD,"Now we are talking Intel! If they can hold on to these chips, not melting themselves, then it's gonna be a good fight",Positive
AMD,it's just a bunch of cores glued together - intel circa 2016 probably,Neutral
AMD,Not sure I can trust Intel again after the BS around Arrow Lake and the lack of ongoing LGA 1851 upgrade path.,Negative
AMD,Bout time,Neutral
AMD,"Like I commented before, hope (but really don't believe) this is NOT the product that is only 10% faster than arrowlake because liquidation awaits them otherwise",Negative
AMD,Will it be available in fall of this year or 26Q1?,Neutral
AMD,So it only took them like 5 years to come up with a luke-warm response. cool cool. It's especially bad as Intel actually introduced something similar (eDRAM LLC) into the x86 space many years ago.,Negative
AMD,Noob question:  Is this their 16th gen chips?,Neutral
AMD,Planning on upgrading from my 13700K to whatever the top end Nova Lake is,Neutral
AMD,Only Core i3 can have 65watt ? no more i5 at 65Watt ?,Neutral
AMD,"For some reason it seems to me that all these new different micro cores are cores from ten years ago at the level of Broadwell or Lynnfield, but made on thin nm to fit everything under one lid lmao",Neutral
AMD,"What is it with Intel and 8 ""pcores""  how about just do 24 cores 48 threads",Neutral
AMD,Cool.  I have said for a while now that is want both the extra cache AND a few lesser cores to offload stuff to.  The only alternative that has this atm is the 9900x3d and the 9950x3d.  Next gen when im buying new stuff then perhaps intel will have an alternative to those then.,Neutral
AMD,"The 52 cores is exciting, but I would probably use Linux as my main OS and run Windows in a VM. Plenty of cores and plenty of RAM and it would be a great machine.  A 52 core desktop beast with an Intel graphics card with 128 GB VRAM would be a killer machine. LLMs would run like lightening.",Positive
AMD,"Intel mocking the chiplet design, describing it as gluing processors together and then doing it themselves - this screams that energy once again.",Negative
AMD,So Nova Lake IS happening? This talk about skipping to next node had me anxious..,Negative
AMD,"So alder lake on cache steroids ?  If the heavier core count chips don’t have the LLc cache, and if this chip is faster at gaming than the heavier core count chips, how will Intel market it?   If they call it a core ultra 5 or 7, then that will make the core ultra 9 seem less of a halo product.  Will be interesting to see how they market it.",Neutral
AMD,"E core need to get lost for gaming, we need 9800x3d rival, purely for gaming !",Neutral
AMD,"""E-Cores"",  Ewww, Gross.",Neutral
AMD,"**Hi,**  I’m currently running a 9700K paired with a 5070 Ti. Do you think I should upgrade to Arrow Lake Refresh now, or hold out for Nova Lake?  The CPU’s clearly starting to bottleneck a bit, but it’s not super noticeable yet.  Is it worth waiting for Nova Lake, or better to just go with Arrow Lake Refresh when it drops?",Neutral
AMD,NVL will definitely be the 400 series. PTL is 300.,Neutral
AMD,"Naming can be changed, but the most important thing is, this isn't the upcoming arrow lake refresh  I hope arrow lake refresh fix the memory controller latency tho",Neutral
AMD,"Yes, I expect the 300 series to be Arrow Lake refresh and Panther Lake.",Neutral
AMD,"> (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")  Maybe they will bring back the ""C"" from i7-5775C.   Behold, the Core Ultra 7 465KFC",Neutral
AMD,I suspect it'll be like:  495K: 16+32.  485K: 12/14+24/32.  475K: 8+16.  465K: 8+12.  Etc.,Neutral
AMD,Seeing all those different cores on a chip does look cool. But I presume this idea will end up being butchered by Windows not being able to schedule things properly,Neutral
AMD,"So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake? Or in 2027? Either way Zen 6 vs Nova Lake will be interesting.",Positive
AMD,Taking the 'L' to their own CPUs would generally be considered a bad marketing move.,Negative
AMD,"We'll see how far Zen 6 takes us on memory. Strix has improvements, hopefully that wasn't all of it though.",Neutral
AMD,"If intel doesn't screw itself with stupid tdp, reliability issues, or bad relative performance",Negative
AMD,More like *despite* him.,Neutral
AMD,All the SKUs rumored so far are BLLC,Neutral
AMD,"They have graduated from background tasks. At this point, even LPE cores can handle apps like Microsoft teams by themselves with no issue never mind the E cores on the ring bus",Neutral
AMD,"If the scheduler can really support this. As soon as a game would hit the e cores, it could turn bad.",Negative
AMD,Why not 8 P cores with HT + even more cache and no e cores at all,Neutral
AMD,"A large L3 will help a ton with latency. It's why AMD has so much success with it using chiplets which have a ton of extra latency. Reduces all those higher latency trips out to RAM. Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile and if that's the case this L3 might be a really big hit for gaming, etc.",Positive
AMD,If latency slightly increases or stays the same due to adopting Pantherlake optimizations and the newer Foveros then that might be enough to narrow the gap,Neutral
AMD,I assume the cache is based on Lunar Lake's memory side cache which doesn't bring much if any noticeable latency over Arrowlake H,Neutral
AMD,"Also done with them. Changing socket every \~3 years was also bad, but this is disgusting.",Negative
AMD,"Not sure how it could be, given all the extra cores.  That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  I expect NVL to be a big power/thermals/multithreading win, than anything else.",Neutral
AMD,Nova lake? More like 26Q4.,Neutral
AMD,Only Pantherlake for mobile,Neutral
AMD,"It doesn’t matter so long as all major vendors will automatically ship with whatever Intel puts out. Lenovo, dell, and HP carry a grand total of *zero* desktop Zen 5 (granite ridge) machine across their consumer, gaming, and business line. You heard that right, *zero* among the global big three.",Neutral
AMD,"Since when they are ""mocking chiplet design""? Have you forgot Intel is the first when it comes to gluing chip together? Intel did this with Core 2 Quad. Meanwhile Amd copying that with ryzen but sadly people don't want to admit it.",Neutral
AMD,It is really happening. The only question is when? Or can they release it on the next year without delay?,Neutral
AMD,"Core Ultra 9 has 48 cores (and 4 LPE cores). It slaps you in the face as halo SKU (unless it becomes entry level threadripper target?).  Currently rumored:  (8+16E+BLLC, 8+16E)  8+16E+BLLC  8+12E+BLLC",Neutral
AMD,The i9 sku is rumored to be two 8+16 tiles. So slotting in an extra bLLC 8+16 tile instead of a normal 8+16 one for the top sku doesn't sound impossible.,Neutral
AMD,Probably same way AMD did the 5800X3D 8 core which was slower than the 5800X at productivity and had less cores than the 12 core 5900X and 16 core 5950X.,Neutral
AMD,"E cores are the future, P cores days are numbered.",Neutral
AMD,ARL refresh is not worth it. Go AMD or wait for NVL if you want.,Neutral
AMD,If you have to upgrade now for gaming get a 9800x3d from AMD otherwise wait and see what AMD Zen 6 and Nova Lake can do.,Neutral
AMD,"You just missed some pretty killer deals around Prime Day; that would have been the ideal time to go with Arrow Lake.  The 'refresh' will offer better performance, but I highly doubt the value difference will be significant.     You can definitely wait a bit.",Negative
AMD,"I would like the naming with suffix ""C"" for cache at the end or maybe ""G"" for gaming.",Neutral
AMD,"W11 has (mostly) fixed the scheduling issues, in fact the scheduler was already pretty good by the time Raptor Lake launched. Sometimes it can mess things up but it's quite rare nowadays. The only time I have seen some small issues is with legacy single threaded software.",Neutral
AMD,"Why would it be butchered? the issues with big/little are long gone now - but I guess to your type none of that matters - getting upvotes for blind anti-MS zealotry is all that matters.  but given that this is an intel sub, i am surprised its not merely blind anti intel zealotry instead.",Negative
AMD,">So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake?  Yes. Intel has confirmed NVL will launch in 2026.",Neutral
AMD,The former ceo,Neutral
AMD,"Raichu has repeatedly talked about BLLC ""variants"", so I wouldn't assume that all skus have the large L3.   Plus, it seems extremely wasteful to have all the skus have BLLC.",Neutral
AMD,"Yeah, Skymont are more like space efficient P cores that are down clocked and don't have all of the features. IPC is really good.",Positive
AMD,This is wrong. Arrow Lake already uses E cores for gaming to no performance hit.   12-14th Gen sure. But not with the new E cores and no HT. You’re using dated info.,Negative
AMD,"A number of public tests have shown that, even with all the P-cores disabled, ARL is still pretty good in gaming.  It's not the E-cores.",Positive
AMD,No different to 12-14th gen then.,Neutral
AMD,"windows pretty much gave up on this and just allows you to set to performance mode, which always tries to grab a P core first.",Negative
AMD,"Or that I forgot they don't have HT on the newer chips.   To be honest I was meaning 14th gen P and E cores with more cache not the new gen stuff. We know the IPC, memory controller and core speed is enough it's just the cache that makes it fall behind AMD in gaming.",Neutral
AMD,E core is 30% faster than hyper threading,Neutral
AMD,HT is worse than E cores,Negative
AMD,>Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile  They aren't,Neutral
AMD,It will enlarge the ontile L3 cache most likely with a cache only tile with high performance die to die connect. No 3d stacking until 18A-PT is available for production which will likely be well after Nova Lake tapeout which should be done now or with in the next few months in order to ramp up production in early 2026 and be on the market by late 2026.,Neutral
AMD,">That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  If you are in the lead, maybe this argument is valid. However Intel is behind Apple in ST and behind AMD in gaming. They have plenty of room to improve.",Neutral
AMD,"> That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point  And yet we see that constantly.",Negative
AMD,"Ze3vsZen2, Zen 4vsZen3, Zen 5 X3D vs Zen 4 X3D, Alderlake vs RocketLake, Raptorlake vs AlderLake, Lunarlake/Arrowlake-H (Mobile) vs Meteorlake U and H, Gracemont vs Tremont, Skymont vs Gracemont and Crestmont, Many Arm or Arm based designs, etc",Neutral
AMD,ARM chips regularly do this sometimes on a yearly basis.,Neutral
AMD,A lot of gamers have no clue that Intel is actually doing well with ARL for OEM laptops/dekstops as ARL is basically a laptop design warmed over for desktop chip usage.,Neutral
AMD,Core 2 quads and Ryzen chiplets are totally different ways of doing MCMs  C2Q didnt have an IO die and it pretty much worked like having a 2 socket motherboard      IO die is what enabled much lower latency and the ability to connect much more CPU dies together (EPYC 9005 got 16 CCDs),Neutral
AMD,https://www.pcgamer.com/intel-slide-criticizes-amd-for-using-glued-together-dies-in-epyc-processors/,Neutral
AMD,"> They stopped trying to sell capacity to external customers and they will try again with 14a. It's not like they had a lot of capacity to spare. It's not even enough for themselves  Intel claims they can double to 1.5x their EUV capacity in arizona and ireland, if customers asked for it, in 1-2 years.   They built out all they needed for their own internal 18A demand too.",Neutral
AMD,"It shouldn't become an entry level workstation part since the Ryzen 9 will have 24c/48t to go against it.   48 cores (plus the 4 LPe, but those shouldn't factor in too much) should handily beat 48 threads, but not by enough to reach Threadripper territory.",Neutral
AMD,Most likely the bLLC variant wil be single compute tile and single cache tile as this would be cheaper for now than putting bLLC cache in base tile like Amd did. They can do that when 18A-PT is avaliable which will probably be after Nova Lake goes into production with 18A or maybe 18A-P,Neutral
AMD,"Lmao, some of my games still runs on e cores",Neutral
AMD,People are still disabling e cores for more performance.,Neutral
AMD,"Varients because there are two tiles. One with BLLV, one without. Full chip had 1 BLLC chip and 1 without. No actual SKU (not tile) without BLLC in some way so far",Neutral
AMD,"They are nothing like P-core as designed with no hyperthreading, area/space efficienct and integrated into clusters of 4 e-cores with shared L2 cache",Neutral
AMD,fym no different they're 50% faster,Neutral
AMD,No. ARL architecture is definitely not optimized for latency but hiding memory access which favours multithreading instead of gaming. Hopefully Nova Lake will optimize all the latecy issues with ARL arch and decrease latency to L3 considerably aswell,Negative
AMD,"Yes, but in games, it's the P-cores that are meant to be utilized. Also, more than eight cores or 16 threads aren't really necessary for gaming. The 9950x3d isn't faster, than the 9900x3d.  My point is that the space taken up by the E-cores could instead be used for additional cache, not?",Neutral
AMD,I had heard this both ways and never really know who to trust until I see it. Anyhow if that's true the large L3 should be even more helpful for things like gaming as it will keep those higher latency hits down. AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help.,Neutral
AMD,"As a advice, Nova Lake will further increase memory latency.",Neutral
AMD,In which gen iteration?,Neutral
AMD,Editorialized. But its funny because said quote is attributed to AMD when Intel made Core2Duo (or Quad?),Neutral
AMD,"If its is with 10% of threadripper 9960x at half the price, it will definitely hurt the low end threadripper cpus a lot for productivity/creator types who would love a more reasonable priced high performance entry level workstaion. Low end workstation is probably bigger market than high end gaming desktops anyways.",Negative
AMD,"Another factor is that Apple has booked out all the N2 capacity for the better part of a year before AMD can touch it.  Nova Lake will have been out for a while, before AMD shows up",Neutral
AMD,Process lasso can let you just designate by task so the E cores can do background and side jobs. People use process lasso on the 7950x3d to make the X3D chiplet run games with those 8 cores and the other 8 core chiplet for everything else.,Neutral
AMD,What issue did you run into with a 9800x3d? It has one CCD and no small cores,Neutral
AMD,"Nowadays there are very few cases where disabling E cores is extremely beneficial, sometimes they are a detriment to performance but more often than not they are useful. Not to mention the E core design team is more competent than the P core team and are quickly catching up in performance.    The E cores in Alder Lake had relatively poor ST performance but in Arrow Lake they are much faster and closer to the P cores in raw performance. Every new generation the gap keeps shrinking and it will make the disabling of E cores less and less relevant.    Sometimes the Windows scheduler will wrongly assign an E core to a ST bound task, even if a P core is available, but it doesn't happen often enough to be a serious issue.",Neutral
AMD,"yes, but not because of windows scheduling - its because you can OC higher with them off - better temps and easier stability - some people disable HT for the same.",Neutral
AMD,They don't pay attention,Negative
AMD,"No. Intel needs to do what Amd did and add seperate bLLC just for gaming. As long as latency is reduced to the L3 cache including bLLC, gaming performance will definitely increase as 285K and 9950x have similar performance in both gaming and productivity with small lead going to Amd. But x3d totally dominates ARL beacsue of the obvious huge cache difference and lower overall latency to their cache implementation.",Neutral
AMD,That is more so bc the 9950x3d isn’t really a 16 core cpu it is two 8 cores,Neutral
AMD,"There was one article a while back that reported something like ""intel goes back to integrated mc"", not understanding that panther lake is a fundamentally different product than the tile based desktop products.",Neutral
AMD,How do you think they're doing 2 compute tiles if the memory controller is on one?,Neutral
AMD,">I had heard this both ways and never really know who to trust until I see it.  I'm curious, which leakers have said otherwise?   You could still have mem latency improvements without the mem controller being on the compute tile too, so the situation there can improve either way.   >AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help  Yup, and ARL has higher mem latency than AMD even, so obviously there is room to improve even without having to move the mem controller back.",Neutral
AMD,"Intel will definitely decrease latency with Nova lake and as long as they add a bigger L3 cache variant, gaming performance will increase further to challenge AMD",Positive
AMD,"If you go straight to the source, the quote they used is directly on Intel's slides. Intel literally wrote that, regardless of who said it first.   [https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg](https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg)",Neutral
AMD,"I mean that it won't be priced like an HEDT part. It'll definitely perform like current generation lower end Threadrippers, but so will the 11950x.",Neutral
AMD,"NVL high end desktop skus are rumored to be on N2 as well. Intel has confirmed NVL desktop will be on TSMC, and Intel has also confirmed some NVL compute tiles will be external.   It's very possible Zen 6 DT is out before NVL-S, and it's very likely that NVL-S and Zen 6 launch within a few months of each other, if that even.   Also, if AMD wants to pay for TSMC N2 capacity, TSMC will build it out. Just a couple days ago we had a [report](https://www.digitimes.com/news/a20250724PD213/tsmc-2nm-capacity-2028-3nm.html) about TSMC being very aggressive with N2 scale out, and the reasoning here is two fold- first of all, 2026 is technically the 2nd year of N2 HVM, and also because a *lot* of major SOCs, other than just Apple and AMD, will be on N2- demand for N2 is high, and build out will be too.",Neutral
AMD,AMD is one of the leading N2 early adopters. Both TSMC and AMD announced this.,Neutral
AMD,That didn't stop Intel with N3B,Neutral
AMD,The recent Warhammer space marine game is a game that benefited from disabling e cores. I am not sure the disparity between arrow and alder in that game tho.,Neutral
AMD,">Intel needs to do what Amd did and add seperate bLLC just for gaming.  Thats what I tried to say.  >My point is that the space taken up by the E-cores could instead be used for additional cache, not?  Or isn't that enough? AMD stacks the cache directly on top (9000 Series) or under (9000 series) the CPU cores, right?",Neutral
AMD,"Yes but IF Intel decides to do quad channel for AI nonsense at least for flagship model, Amd threadripper will be stomped over as I will definitely buy the Nova Lake part just for the price/performance compared to 9960x threadripper. The motherboard selection alone for Z890 is absolutely amazing and RDIMM prices aretwice as expensive for the same amount of memory. I expect with CAMM2 that max ram capaciity will go for 512GB and maybe 1TB for Nova Lake aswell. Even with 2 channel, cudimm will go above 10,000 MTS and maybe 12000 if we are lucky on CAMM2 modules by 2027 as CAMM2 definitely being used for DDR6   I do feel Intel has I/O advantage beyong gaming and that market is getting bigger than hardcore gamers all the time!! A decked out 256GB Arrow Lake z890 based computer is way cheaper than same memory sized threadripper 9960x.",Positive
AMD,"I don't recall Intel stating that NVL core chips would be at TSMC, only that some of the eventual package would be... which implies iGPU, to me, and that wouldn't be N2 (possibly N4, since Battlemage is N5, and N4 would be open and cheaper than N3 nodes).  Do you have a source, where they directly claim the NVL cores are coming from TSMC?",Neutral
AMD,"The game is a dumpster fire when it comes to CPU utilization, it hammers even top tier gaming chips like the 9800X3D. I'm not surprised if the E cores can bottleneck the performance even further, most likely some of the game's heaviest threads get (wrongly) assigned to the E cores.    The scheduling in W11 has been significantly improved over the last few years but it's not 100% perfect either. Legacy software in particular, which typically utilizes 1 core, often ends up running in an E core and as a result you can't take advantage of the higher ST performance a P core provides. In that case you need stuff like Process Lasso to fix the problem. It doesn't happen very often but when it does it's very annoying.",Negative
AMD,"Yup.   >. Then as you look forward, to our next-generation product for client after that, Nova Lake will actually have die both inside and outside for that process. So, you'll actually see compute tiles inside and outside.  Q4 2024 earnings call.",Neutral
AMD,"That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.  Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  That seems like something they couldn't have known in 2024.",Neutral
AMD,"18A is about as good clockspeed as N2, but density is still around N3 level. To make a new architecture, it makes sense for them to target N2 if AMD does the same which they have",Neutral
AMD,">That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.   20A, and only the lower end 6+8 dies are rumored to be fabbed on that node.   >Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Not competitive enough.   >Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  Low end client products like WLC, and even the lower end 4+8 NVL compute tile, is rumored to be on 18A still.",Neutral
AMD,"It doesn't make sense, for either of them, due to the cost of being an early adopter",Negative
