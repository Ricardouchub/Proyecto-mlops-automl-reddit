brand,text,sentiment
Intel,I would imagine intel has to at least get celestial out the door before cutting off their GPU line. it's been so long in development that it would be more of a waste of money to cancel it then finish it and see how it sells. at least getting it to the public means some kind of ROI and a solid position in the lower end of the GPU market that AMD and Nvidia are abandoning to the used market.,Negative
Intel,"Okay then, what's the Xe GPU roadmap looking like then?",Neutral
Intel,"Hopefully Intel still thinks it needs discrete GPUs to compete in DataCenters and such.  Also GPUs for the ""edge""",Positive
Intel,I can't claim to know the future or fate of Arc but I think if Nvidia wanted to kill it they could just price it out of the market instead of buying a stake in a company with a broadly uncertain future,Negative
Intel,They barely lived on before the deal.,Negative
Intel,"It'll live on our hearts, yes.",Neutral
Intel,Lol if you believe that I have a bridge to sell you in Brooklyn,Neutral
Intel,"You have to look at this from both sides.  This would give NVidia the ability to offer OEMs a tightly integrated on SOC gfx solution vs a separate discrete gfx package as it is now. This would lower costs for NVidia's customers as well as improve performance and Intel's cpu's get to ride that train.   Intel has a long track record of using integrated gfx so it's certainly right up their alley to take this on.  Could Xe gfx coexist with NVidia? Yep, I think so.  Xe could be the budget igfx option and iNVidia is the premium integrated solution. That's basically how it is now with Intel Arc/Battlemage & GTX.   On the DC side, Intel has absolutely no gfx solution at this time and won't have anything for at least two years.  Intel already sells NVidia when a gfx solutions are needed so tighter integrated products will benefit both sides in that DC space as well.",Positive
Intel,"What's weirdest to me about this deal is. They specifically mention NVIDIA chiplets being packaged with Intel CPUs.   Now, with Meteor Lake and Arrow Lake, the CPU and the GPU are different tiles packaged together. But for Lunar Lake, the GPU was included in the same tile as the CPU, called the compute tile, because that was more efficient.   Supposedly, Panther Lake is using a different tile setup, with the GPU being separate again, or with the memory controller being on the tile with the CPU, but.   Either way, it feels to me like relying on separate NVIDIA chiplets for the integrated graphics is a bit of a limitation to how the overall architecture of the SoC can be designed.   My understanding is that, if another company buys Intel, that messes up the cross licensing agreement, and Intel might lose x86-64. So, even if NVIDIA keeps buying Intel stock, the most they can get is 49%.   And NVIDIA is too big now for Intel to buy, so the other way around wouldn't work. And even if NVIDIA wants to keep Intel's fabs open, I doubt they want to run them either.",Neutral
Intel,Wonder if this investment is purely to keep intel going to avoid a monopoly issue in a few years.,Neutral
Intel,AMD is not going to release a next gen gaming gpu it just leaked and that’s what I’m worried about intel arc are ramping up.,Negative
Intel,"There's also a better then not chance this alliance would have gone the same way as most other nVidia semicustom things by the time Druid tapes out anyhow, and the leadership at Chipzilla likely know this.",Neutral
Intel,"While that might be true in the long term, it's not necessarily true for whatever quarter the execs are caring about at any particular moment.",Neutral
Intel,"Probably a chronologically changed version of what is known now.  No one with any savvy expects this alliance to last with nVidia's track record, so carrying it on makes sense in both being ready for the inevitable burnt bridge and possibly pushing it back.",Neutral
Intel,It's looking for 6 years of Xe3 in integrated graphics I'd guess.,Neutral
Intel,"Always selling out actually, they just can't produce that much",Neutral
Intel,And I have another if you think nVidia is capable of keeping this deal running for that long...,Neutral
Intel,"Im not sure what you are implying, but nvidia partnerships have historically not broken from their side as far as I know.   Apple gave them the boot, not the other way around.   AMD are the ones who rejected the Nvidia-AMD merger back in 2005.   Tesla kicked nvidia out as well, because they did not want to keep paying them to develop the tech.   If you are referring to AIB Partners, then that can hardly be called an actual partnership. That's more of a contracted labor and distribution agreement. Even then, XFX only got blacklisted because they were a premier partner who were seen as jumping ship when the going got a little rough with nvidia stalling between 2009-2010.",Neutral
Intel,Intel's arc is dead with or without nvidia deal.,Neutral
Intel,"Microsoft went with Nvidia for Xbox and it ended in a lawsuit.   Sony went with Nvidia once, and never again.  Apple ceased to bundle Nvidia GPUs because Nvidia sold them (and HP and Dell) huge quantities of faulty 8000M GPUs.",Negative
Intel,There is a reason why every nvidia partnership ends up breaking apart pretty quickly...,Negative
Intel,"Nvidia seems to make working with Nvidia too difficult for their partners to continue. It's the business version of someone emotionally bailing, and abusing you until you leave them.  They did that with smartphones, consoles, Linux, Apple, and likely countless other projects I didn't follow in as much detail.",Negative
Intel,"Hardly. Even after the bubble, Intel needs in house GPU, for both APUs and data center... which means they'll make a gaming version if the work is already mostly there.",Neutral
Intel,"In the case of Intel and Nvidia chipsets, it was Intel that decidedly ended the relationship, paying Nvidia off to exit the chipset business so that they could make integrated graphics.",Neutral
Intel,"Yeah, if nVidia has an Achilles' heel, it's their company culture's near total inability to do semicustom without fuckery, and it's the reason AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops.",Negative
Intel,"They've already cancelled every data centre GPU after Ponte Vecchio (which was forced out by DOE contract) despite reassuring us Rialto Bridge and Falcon Shore were on track as little as months before the chop, and their next is apparently Jagure Shore in 2028.   I have 0 hope that will actually happen.",Negative
Intel,">AMD dominates basically every high performance gaming graphics application outside of Windows desktops and laptops  And how much money is there in your ""everything else outside of ..."" vs Nvidia's entirety of consumer prebuilt desktop/laptop?",Neutral
Intel,"*looks at effectively every current gaming system that isn't a PC, Intel claw, a phone, or the Switch*  A fair bit, and frankly, even if it's not that profitable, it defines more of the dynamics of this sector then you'd think if you're a PC type.",Neutral
Intel,"Not only is it not profitable for the chipmaker, the revenue isn't even anything to brag about looking at AMD's gaming section of their financial statements. 1.1 billion for Q2 2025, and that includes whatever miniscule sales there were for Radeon dGPUs.",Negative
Intel,"> If using NVIDIA RTX iGPU in Intel SoC, that will leave only discrete Intel Arc designs to be sold independently.  Assumes without evidence that Nvidia's GPU will immediately replace Xe in every SKU.",Neutral
Intel,Oh those sweet promises. How credible it is!,Neutral
Intel,"Sure it does..... Ignore all the signs, we are fine, nothing to see here.",Neutral
Intel,I wonder what this will mean for running llms locally. Could I buy a laptop with 128 gb of ram and an Nvidia iGPU and have that memory unified with the GPU to run the models?,Neutral
Intel,If the roadmaps haven't changed does that mean there's still a chance we'll see Celestial gpus in 2024?,Neutral
Intel,Rtx “igpus” are probably in a different power envelope to igpus..,Neutral
Intel,"Xe will live in Desktops and high end HX which uses the same chips. Xe3 is done. Xe4 is probably done too. So i expect them to be used here like HD Graphics and will get very little improvements from Xe5 onwards. Just to keep display and needed functions.  In a way they are still commited to GPUs   H, U and V are almost certainly getting their Arc GPUs replaced with RTX ones.  We all know with RTX tech those Laptop CPUs will sell a whole lot more and make intel way more profit. Anything with Nvidia's tech sells like pancakes now.  So going forward they can't recover Xe development costs through the laptop iGPUs using like they did before. Remember laptop iGPUs are the highest volume ARC sales.  Without those sale it's hard to justify full on Xe spending needed for dGPUs. Especially with Intel's financial situation.",Neutral
Intel,"This deal seems crazy.  For Nvidia that's an easy way to get custom x86 CPUs for datacenters and a way to directly compete with Strix Halo tier products.   If the products suck they can just blame Intel and walk out as if nothing ever happened. They don't really have much to lose here.  Even if they cancel their own CPU cores as a result of this deal, they can still use stock ARM cores in the future when/if the partnership ends...  But for Intel? For Intel this seems like a complete nightmare.  What did it take for them to convince Nvidia? It's hard to believe this won't have any impact on Arc... In the worse case scenario they'll be giving up on high-end graphics, just for Nvidia to abandon them right after when x86 isn't as important anymore...  This sounds like a desperate gamble, and it's difficult to understand where it came from because Intel does have pretty good iGPUs... Why are they so desperate?",Negative
Intel,Intel's market cap is 141 billion.  Nvidia's 5 billion investment doesn't give Nvidia enough ownership to start calling all the shots.,Neutral
Intel,Why are we not linking to the original source?  https://www.pcworld.com/article/2913872/intel-nvidia-deal-doesnt-change-its-roadmap.html,Negative
Intel,"I wonder why MLID seems to want so much that ARC be cancelled...  How many times has he brought out ""leaks"" about ARC getting axed in the last few years? I stopped counting.",Negative
Intel,Remember how well it went when Intel shipped an AMD iGPU?,Neutral
Intel,With the announcement I imagined Nvidia branded products. Mini-PCs where the CPU and GPU aren't replaceable. Nvidia having Intel and Mediatek products. The integrated GPUs for Intel have seemed pretty solid for a very long time. The discrete cards are competitive in price at least. I wouldn't bet on them quitting discrete cards. Don't think they'd want to be caught with nothing again if another major novel GPU algorithm pops up and starts a frenzy again,Positive
Intel,"Intel iGPUs are equal or better than AMD outside of the one on the 395. 140v is performant and efficient, 140t is on par with the 890m but paired with a better CPU.",Positive
Intel,"Well, I hope they will have them, but I have a feel they will only be available for servers.",Neutral
Intel,Yeah right.. this will age well... Nvidia definitely wants more competition...,Positive
Intel,Intel has struggled with GPU for decades which is insane when look what Apple has accomplished with their iGPUs. Intel was decimated from the insides out when the C-suite decided asset stripping via sharebuybacks and gutting employees/R&D etc.. in addition to taking in debt.  Not sure if Intel can recover but interesting to see all this corporate heroic medicine trying to save the patient.,Negative
Intel,Which is also a big reach to assume when for a large portion of intel's market the current intel iGPU is already faster than they need or will need for the forseable future.,Neutral
Intel,"Much more likely the intel/Nvidia collab is for a few special new products, not a complete change in how intel has done graphics for decades.",Neutral
Intel,"Not immediately, but Intel knows CPUs with RTX GPUs will sell a whole lot more than CPUs with ARC.. Money is what Intel desperately needs.  So don't be surprised if they do it fast.",Positive
Intel,There are definitely more Intel ARC iGPU users than Arc GPU users. This will lead to a cut in workforce (already happened) and more in the future. This also means less investment into things like XeSS.,Negative
Intel,The thing is that Intel barely sells any dedicated ARC cards. Almost all of their graphics userbase comes from Intel integrated graphics with their new XE tiles on them. These will be replaced by the Nvidia tiles.      The result of this is that basically every Intel laptop will now have Nvidia inside.,Negative
Intel,"Yes, I doubt Nvidia would ""give"" iGPU without any licensing/fees.",Neutral
Intel,Reminds me of Stadia. All the signs were there that Google would pull the plug but many were optimistic. Google even said they are committed.  Then it got killed.  Edit - found a article that lists how many times Google said they were committed.  https://www.theverge.com/2022/9/30/23378757/google-stadia-commitments-shutdown-rumors,Negative
Intel,If the bandwidth is as low as most PC laptops currently have it will be slow. Even Strix Halo and DGX have this problem and they are better than most laptops.,Negative
Intel,Nvidia already has this with the the DGX Spark and that same chip is coming to laptops.,Neutral
Intel,"That's likely the plan, and they should be able to pull it off. Just a question of whether they can beat AMD and Apple at it.",Neutral
Intel,Intel really needs to use a unified memory design like Apple or they will always look like a joke. They really should be buying a company that makes memory.,Negative
Intel,"Yes, I think we'll see them next year.",Neutral
Intel,"Doubt they’ll remove the option for Arc completely from the H, U and V series. NVIDIA is most likely charging a pretty penny for their GPUs, which will make Intel completely non competitive in a lot of markets if they were to just switch to NVIDIA.  At minimum, it will stick around for the H and U series as a complimentary option.  Not everything magically sells better just because it has NVIDIA on it",Neutral
Intel,I don't see how Intel might be getting more profit from these tbh... Revenue? Yes. Profit? From where?  Especially if Nvidia will continue to use exclusively TSMC nodes.,Neutral
Intel,"Killing Xe3 and Xe4 is so weird to me when Panther Lake is supposed to launch this year, and Nova Lake (which is rumored to to use the Xe4 media block and use Xe3 for the graphics/shaders/compute block) is supposed to launch sometime in 2026.   Like, the volume for both ""launches"" is supposed to be low, actual devices probably won't be available to purchase until the next year, but. Companies are supposed to be receiving samples of Panther Lake. Now. Presently.   For them to say their roadmap isn't affected, isn't changed at all. Unless that's a massive lie, that's only possible if samples of integrated Xe3 have already been fabbed, are already in working silicon.   ...Like, not to get too into the rumor mill, but supposedly the early Panther Lake samples aren't doing great. The CPU side isn't very efficient compared to Lunar Lake, and the GPUs sometimes don't work. But, when they do work, supposedly, the performance is really good, and, again, rumors, but the problem is supposedly more about Intel's drivers than the hardware or architecture itself.   ...I can completely believe that Intel would panic and would rather kill their GPU division entirely than. Invest in their software stack. Developing good technology and then abandoning it instead of advancing, because abandoning it is cheaper and easier is an extremely Intel move.   But if nothing else, the sunk costs for Xe3 at least make me feel like. They sorta have to figure it out, if only because they don't have time to replace it with NVIDIA, in the time frame Panther Lake has to come out.",Neutral
Intel,"There is just simply no way Intel is going to rely on Nvidia as the sole supplier of their iGPUs when their product lines. Theres even less chance that the U series, their low cost volume product line, would switch to Nvidia sourcing.   Nvidia iGPUs are going to be their own separate, lower volume product line, with its own designation. Maybe -G, or -N, or -AX",Negative
Intel,I dont think so.  Why would they cut themselves from hundreds of bilions of dollars market this easily?  It doesnt make sense,Negative
Intel,">We all know with RTX tech those Laptop CPUs will sell a whole lot more and make intel way more profit. Anything with Nvidia's tech sells like pancakes now.  this makes no sense, it will sell more as opposed to what? the igpu needs a cpu anyway and nvidia's gpus mostly get paired with Intel's cpus anyway. Intel controls 80%+ of laptop cpu market and 65% of ALL igpu + dgpu market, why would they give up such a big lead? If anything this has the potential to cannibalize Nvidia's lower end 4050/4060 market since thats probably the performance these RTX SOC's will be",Neutral
Intel,"Intel has already given up on high end graphics, this deal has no impact on Arc cause Arc already has no future.",Negative
Intel,"Also Nvidia is not a company Intel would want to be stuck relying on indefinitely, they make money off of selling advanced chips, not consoles. I don't know why there is all this presumptive gravedancers here proclaiming the death of ARC, but its really stupid for Intel's long term market viability as a large independent chipmaker. I would honestly spin off the fabs before going to that.",Negative
Intel,Intel will want the nvidia name on everything regardless as they know it will sell,Neutral
Intel,A lot of AMD's are using outdated nodes and architecture.,Neutral
Intel,Intel's biggest struggle with GPUs is gaming compatability and drivers. Apple certainly isnt doing great their either.,Positive
Intel,"I love my laptop with an Intel iGPU, it can actually last me through a work call compare to my other laptop with an AMD chip plus a damn 4070Ti in it.",Positive
Intel,Sure but a lot of Intel's laptop chip business has no need for (presumably) more expensive integrated RTX does it?  Of course for gamers and maybe even workstation laptops (which is a pretty tiny market) these chips will be very appealing but everything else... whatever flavour of integrated ARC Intel are currently developing will continue to be the norm.,Neutral
Intel,"I would assume that the RTX iGPUs cover all the ""gaming"" SKUs, leaving the intel iGPUs for all the low-end and business ones.",Neutral
Intel,"Why would they sell more? We already have laptops with RTX dGPUs and they sell less than Intel iGPU laptops.   The Nvidia iGPU versions are going to be more expensive chips and the laptop prices will reflect that just as they currently do. There's going to be a *reason* to pay extra for the Nvidia versions, and thats going to be measurable more GPU performance. Not just low end Nvidia GPUs on-par with Arc iGPUs.",Neutral
Intel,"Eh. 90% of people and especially companies don't have any need for high GPU performance in a laptop, so CPUs with cheaper Xe iGPUs will likely continue to make up the vast majority of their sales.",Neutral
Intel,"> Not immediately, but Intel knows CPUs with RTX GPUs will sell a whole lot more than CPUs with ARC.. Money is what Intel desperately needs.  Among gamers and people who need the power? Agreed!  Majority of users who don't game or need the power/expense.... nope!  What I like about this is that it pushes innovation further. Hopefully it brings innovation to the sub-$600 laptop market.",Positive
Intel,Average person doesn't care.    Most people don't play games on their computers all of the time.,Negative
Intel,Businesses do not care about GPU's in their laptops it will not sell a whole lot more.,Negative
Intel,Intel iGPUs are drastically better than Nvidia stuff for anything except Halo-type parts.,Positive
Intel,"I think it may come down to the fact that the executives within a particular business unit probably ARE very committed to making it succeed and sticking with it, but the ultimate decision to pull the plug comes from higher up",Neutral
Intel,Ahh yes reminds me of this lovely Google Graveyard.   https://killedbygoogle.com/,Positive
Intel,"The reason why Stadia failed was because they never implemented ""negative latency""",Negative
Intel,"Because a Laptop is a sum of its part.  Lets say for $400 IntelArc CPU and $500 IntelRTX.  When a laptop is built, if the IntelArc costs $1000 the  IntelRTX will cost $1100  The higher you go the closer it gets percentage wise.  So when the difference is less 10% which one do you think sell way more?",Neutral
Intel,Nvidia is always looking for more fabs as they seem to be significantly supply limited at TSMC. If 18A/14A turn out good it's a safe bet they'll at least try it out.  Remember how Ampere customer cards were done on Samsung so they could use all of their TSMC allocation for A100. They would probably have continued that Samsung partnership if internal corruption at Samsung didn't bomb their 4nm process.,Neutral
Intel,"Even **if** Intel were to completely cancel Xe and their entire business becomes dependent on Nvidia selling them iGPU chiplets (for some odd reason), the timeline for these Nvidia chiplet CPUs would be after NVL",Neutral
Intel,They've spent the past 1-2 years laying off much of their driver team.,Neutral
Intel,Excuse me but Hundreds of billions?   Since when has gaming GPUs make 100 of billions.  Nvidia makes just over 10 billion on Gaming GPUs a year.,Neutral
Intel,"They don't see themselves having a credible chance of capturing that market (GeForce + CUDA moat is deep) without having to trash their margins, they see short-term gain in being able to have the best gaming laptops on the market for a few generations, and being the choice to provide the CPUs for NVIDIA AI platforms.",Neutral
Intel,If you haven't noticed intel hasnt been doing so hot.  Dell recently added AMD for XPS for the first time. Many OEMs similarly are doing amd versions for the first time. Something unthinkable even 5 years back   This way they losing OEM sales on the laptop space. They are no longer the preferred CPU brand.  So what do you do? Well having their CPUs with RTX would give them a surefire boost,Negative
Intel,Intel is worried that they'll lose this market to the now superior AMD chips.,Neutral
Intel,"Well AMD does make 4 billion each year from their console deals, and Intel cannot enter that space because they don't offer a product that is as compelling as AMD.      With this partnership they could do this.",Neutral
Intel,Oh we know they are trying to sell us outdated modes and architectures. And they will continue to do so for the entirety of 2026 in mobile.,Negative
Intel,Apple GPUs are significantly more powerful than any intel iGPU. I can play cyberpunk 2077 at 60fps 4K and nice visuals. Also I can run large AI LLMs as well. I can run heavy AI workloads all day long and total wattage 150-160w for the entire Mac Studio.  Intel iGPUs would melt/die. Its amazing how far Apple has pushed Apple Silicon while Intel got stuck in the mud.  Sad to see how the mercenary C-suite came in like a plague of locusts and gutted Intel leaving a husk that requires intensive care to resuscitate. I hope they do rise back up because competition is good and pushes progress forwards.,Positive
Intel,"If Optimus is working properly, your AMD laptop should not be consuming any more power than if it were APU-only during normal use.",Neutral
Intel,this is likely more to compete with amd's apu's and less for business,Neutral
Intel,"I can see a consumer marketing reason and a business professional reason for RTX throughout: driver support. Anything with the RTX branding will get more support on both fronts, especially if the integrated parts default to studio level drivers. A large part of Arc’s inefficiencies are due to the lack of software support, so streamlining this aspect would be a huge boon for the “it just works” marketing coming back online for Intel.",Positive
Intel,That could still mean Intel dGPU business will go away.,Neutral
Intel,"It has been talked about a lot but SOCs will be replacing the lower end systems completely. For example, the Strix Halo chip that AMD just released is better than a RTX 4060. That may seem weak, but it's cheaper to provide an APU than a CPU+GPU.",Positive
Intel,"I said profit.  Even if RTX CPUS cost more, they result in significantly more sales resulting in greater profits.  We know from the market, that even if its more expensive, Nvidia GPUs will still sell more than equivalent cheaper counterparts",Positive
Intel,"You mean the business sectors which are most likely to use AI? You know, what Nvidia is famous for.  And low-end don't exist for new CPUs for laptops anymore. Its all older generations rebarded.  Intel use older alderlake and raptor lake CPUs rebarded with UHD graphics and AMD sells Zen 2 CPUs.",Neutral
Intel,Makes me wonder if we will see lower price devices with Nvidia iGPU V vs roughly equivalent dGPU models. There should be some saving I think but will also be interesting to see if there remains a choice or if Nvidia will try and move most of it's mobile lineup to this unified product.  I think on the very highest end like 80 and 90 tier mobile chips they may well be too powerful to adequately cool as part of a combined package.,Neutral
Intel,"I honestly want a work laptop that has a good keyboard, screen, and battery life. (and light!)   I'm mostly doing emails, anything else I'm using a workstation for number crunching.",Positive
Intel,And how do you know this?  Cause last i remember Nvidia doesn't have iGPUs,Neutral
Intel,"While we have yet to see good comparisons, it's provable that Nvidia produces more efficient graphics cards than Intel does. The die size on the Intel chips is massive compared to the similar performing Nvidia ones. Larger the die the more power it's using. Switching to Nvidia will provide more performance at the same power usage.",Positive
Intel,Latency on stadia was actually good. I tried it for a bit with with their pro subscription for a month.  But never renewed or bought anything cause of the awful business models.,Negative
Intel,"And why would Intel do that? Theyre dropping MoP because they act as a middleman, moving MoP at cost to OEMs.  A $100 BOM increase per unit rarely only comes with a $100 product increase. And even then, it's corporate suicide to make Nvidia a critical sub-supplier when you've dont need them to be. Intel would never go in 100% on Nvidia being the sole iGPU option.   The Nvidia option is going to be a separate, lower volume, more premium product line.",Negative
Intel,"Apparently they have been working on this for a year at this point, so a product with an Nvidia tile is likely going to launch by 2027.",Positive
Intel,Arc covers also non gaming segments like B50 product,Neutral
Intel,"Gaming laptops?  Ive watched the webcast with Jensen and as far as I understand this is about datacenter because customers dont want to switch to arm, so partnership with Intel will allow them to get x86 cpus with features Nvidia needs for their datacenter and hpc customers",Neutral
Intel,"Also, the partnership potentially could get Nvidia using their fabs. Nvidia will also now hold a 4% stake in Intel.",Neutral
Intel,"LNL(Xe2) solos every AMD offering in igpu out there and matches the AI 395 with lower power, even ARL(Xe+ Alchemist) isnt that behind, igpu perf is the last thing Intel is concerned about, these high perf RTX laptop apus will only damage the lower end offering from nvidia  and here look at the laptop market share, Intel's share has been pretty constant between 75-80% for the past 3-4 years, idk where the notion of Intel struggling in laptop space is coming from, they even clawed some back after the launch of Lunar Lake: [https://www.tomshardware.com/pc-components/cpus/amds-desktop-pc-market-share-hits-a-new-high-as-server-gains-slow-down-intel-now-only-outsells-amd-2-1-down-from-9-1-a-few-years-ago](https://www.tomshardware.com/pc-components/cpus/amds-desktop-pc-market-share-hits-a-new-high-as-server-gains-slow-down-intel-now-only-outsells-amd-2-1-down-from-9-1-a-few-years-ago)",Neutral
Intel,It’s only been five years since people said it when Renoir APU launched. Maybe in another 5 AMD can cross 30% market share on laptop (it’s been stuck at 25% for about 3 years)?,Neutral
Intel,That’s really nothing. In Q2 2025 AMD made 1.1B from gaming (which includes semi custom and Radeon) out of 7.7B.,Neutral
Intel,"I know, it's just that I think we'll keep seeing Arc iGPUs for the business and lower end consumer market primarily. Cheaper or more power efficiency focused laptops mainly.",Neutral
Intel,They don’t need that to compete with AMD’s regular iGP. 140v/140T do well enough.   Strix Halo is irrelevant as it was near zero mainstream OEM presence and,Negative
Intel,"It's possible.  Arc might be carving a workstation niche though? their B50 certainly seems like that might be the focus, offers a lot of pro features for a low price.  As far as enthusiast goes I wouldn't be surprised, it's a money sink Intel probably doesn't need even if the payoff might be worth it long term. They say it will make no difference though so who knows.",Positive
Intel,Demonstrably false in any currently existent device. Any $1500 laptop with a 5070mobile  blow the pants off Strix halo.   Continue your APU delusion in r/amd,Negative
Intel,"Will they really profit more by adding RTX GPUs where there were none previously? Especially considering that NVIDIA will likely not make it any cheaper.  Like ultrabooks aren’t going to magically sell more and more, or more expensive in all categories just because they have NVIDIA GPUs.  Only place I can see it really make a lot of sense and add more profit is in place that already had NVIDIA GPUs, like with gaming laptops or enterprise, or as an option to complement Arc.",Neutral
Intel,Are you aware that most enterprise computing is cloud based?,Neutral
Intel,"I think client inference is kind of stupid. It's limited in capability by your local memory amount, and it's economically inefficient because you can't batch requests.  No AI system that exists is ""there"" yet, all of them can be improved enough that people would switch over to a better one. The current top of the line commercial models are hundreds of GB for the weights. To use client inference today you need to use a severely castrated model. Alternatively, if you ship all those hundreds of GB of DRAM on every device, they will be very inefficiently utilized because a single user rarely has the token flow to keep them working, and when they do it's all serial so you can't even batch, you just have to do a linear read over the whole model to get a single token out. And when there's an improved model next year that takes twice the ram you have to roll out whole new machines to the whole fleet.  In contrast, centralized inference can fit as much memory as you put on it, there are no power constraints, you can batch hundreds of requests in one go, and you can update the whole system much more easily. Client inference won't even win in latency because even though you have to pay for network latency, the centralized solution is probably much faster.  The only real advantage client inference has is privacy, and that's not a problem in business, they just get their own inference server. For office work, that even makes latency very fast.",Negative
Intel,"I truly think you'll start to see lower tier offerings, like 50 - 60 class because large iGPUs. Maybe even 70, and a separate, discrete graphics chip become increasingly rare of the next decade.   Nvidia has a similar deal going on with Mediatek to bring their graphics there too.   This is Nvidia shoring up their lower end market in client laptops as the APU wars begin",Neutral
Intel,"You don’t have to wonder, laptops with 7600m are already far better deal for gaming than any equivalent system with Strix halo for gaming.",Positive
Intel,"That kind of proves my point. Intel iGPUs exist, and therefore are better.  Trying to scale-down discrete graphics into integrated graphics will take quite a bit of development effort. AMD has done pretty well with it, but it took them years.  PC gaming / PC building hobbyists seem to drastically undervalue how good Intel graphics really are. They're low power, stable, and have sufficient performance to make the entire category of discrete graphics a niche market in PCs. For most applications that currently use discrete graphics, there would be no reason to even consider Nvidia if it were an option.",Positive
Intel,Quicksync and power management if I were to guess. Spinning up a dGPU uses a lot of power.,Neutral
Intel,Quick sync on a $150 Intel CPU is better at transcoding then anything Nvidia offers under $600.   A SIGNIFICANT portion of intel sales are because of features like this. Otherwise the entire industry would just switch to AMD. Which is superior in every single way EXCEPT it's IGPU support.,Positive
Intel,"Meanwhile back in the real world the switch 2 exists with its nvidia SOC and all those SOC's inside cars exist, all those SOC's in Jetson's etc exist.   Lol literally knows nothing out side of PC gaming hardware.",Neutral
Intel,Nvidia actually does have iGPUs. Cars that use AGX Drive will have one.   The Volvo EX90 is using [AGX Orin](https://www.techpowerup.com/gpu-specs/jetson-agx-orin-64-gb.c4085) which has an Ampere-based IGPU with 2048 CUDA cores and [owners are being given free a upgrade to a Dual Orin setup](https://arstechnica.com/cars/2025/09/forget-ota-volvo-is-upgrading-the-core-computer-in-the-ex90-for-free/).  The latest AGX Thor has a Blackwell IGPU with 2560 CUDA cores.  Either way it seems easy enough for Nvidia to package Blackwell into a tile that Intel can replace their Arc graphics tile with.,Neutral
Intel,"Again, you're confusing yourself with current-generation dGPU comparisons. Yes, discrete Arc is behind Nvidia, because it's new.  An iGPU isn't just a dGPU slapped next to a CPU, nor are different process nodes really comparable.",Neutral
Intel,"Pay for a sub, still have to pay again to ""buy"" games that you don't own. What could possibly be unappealing about that?",Negative
Intel,You are kidding yourself if you think Nvidia will let their iGPUs be a low volume products.,Neutral
Intel,I am pretty sure intel made the arc pro at a low price to sell of most remaining stock and capacity bookings.  This Nvidia deal was in talks from a year ago. And isn't it a funny coincidence they announce the deal after the Arc Pro has sold out in a lot places?,Neutral
Intel,"There were two things announced. A client partnership for gaming laptops (primarily), and a datacenter partnership for custom Xeons with NVLink.",Neutral
Intel,"> LNL(Xe2) solos every AMD offering in igpu out there   Except in, you know, actual workloads.",Neutral
Intel,Well intel is and will still be high in volume since they have very low end on lockdown. They maybe high volume but they are the lower margin chips.  Here is whole range of intel core (not ultra) that use older architectures  https://www.intel.com/content/www/us/en/products/details/processors/core.html  They even revived intel 14nm AGAIN  https://www.intel.com/content/www/us/en/products/sku/244818/intel-core-i5110-processor-12m-cache-up-to-4-30-ghz/specifications.html  Amd cant quite match this yet  However the higher margin products is where they getting hurt by AMD. So that's not good for them,Neutral
Intel,Luner Lake is for handhelds are very lower powered laptops. It's not for work.,Neutral
Intel,"That's likely the case for now, but Intel probably can't fight against the market.   Now that Nvidia iGPU is an option, OEMs are going to express interest, enterprises are going to express interest. It's the AI era, nobody is going to get fired for buying Nvidia.  Intel's CEO is a customer-pleaser, so if there's demand for mainstream SKUs w/ RTX, I can't imagine he'll say no. And Nvidia winning more of Intel's business? Maybe they'll have to port Nvidia IP to Intel foundry to service the higher volume.  I think Intel Xe will stick around for a while as a legacy platform, Intel does have a bit of a software moat, but I think this goes much further than Kaby Lake-G.",Neutral
Intel,> Arc might be carving a workstation niche though  Not enough to keep it alive. Even smaller market than gaming.   > offers a lot of pro features for a low price  Well that's kind of the problem. They're forced to compete on price.,Negative
Intel,"Let me tell you why intel rtx laptops wont be significantly more expensive.  A $400 Intel+ARC and $500 Intel+RTX. Same performance, but Nvidia has their features.  Now the Intel+RTX looks more expensive right by costing 25%? WRONG   Cause if the two laptops are built with exact same components and Intel+ARC laptop costs $1000, the Intel+RTX laptop would cost $1100.  Suddenly the Intel+RTX becomes the obvious choice at only just 10% more money.  It gets worse the higher you. $1500 intel+arc v $1600 Inte+rtx. Or $2000 v $2100  This is also why AMD sucks at OEM and Prebuilts  If a 9060xt built desktop costs $900 the equivalent RTX 5060 Ti desktop would just cost $970.  So suddenly instead of the RTX 5060 TI costing 20% more, it becomes only 8% more expensive.",Negative
Intel,Which runs on Nvidia hardware.  Nvidia laptop GPUs dont have enough memory to run ai locally.  However an Nvidia iGPU with access high capacity DDR5 and you see where we are going?,Neutral
Intel,"I'm really looking forward to it. I guess ODMs are probably pretty pleased as well, this should simplify laptop motherboard layout, feasibly improve reliability and given it's already so rare for a gaming laptop to have anything other than an Intel CPU + Nvidia GPU they're probably going to find all of this very easy to design around.",Positive
Intel,"Nvidia's been doing iGPUs for *years* in Tegra.  > For most applications that currently use discrete graphics, there would be no reason to even consider Nvidia if it were an option.  Nvidia's IP and drivers are simply better.",Positive
Intel,"You know, i think an iGPU does exist. The switch 2.  And I don't need to tell how efficient that is despite using the awful Samsung node",Neutral
Intel,Intel Xe Media Engine is the SOC tile. Not the GPU tile  So Arc GPU tile etting replaced will not effect that at all.  Edit - here is how it works  https://www.intel.com/content/www/us/en/support/articles/000097683/graphics.html  The display is also there too. So the GPU tile cam be completely idle. Arc or RTX,Neutral
Intel,> Quick sync on a $150 Intel CPU is better at transcoding then anything Nvidia offers under $600.  That's just how they spec the encoders across the lineup.,Positive
Intel,Good thing the Arc tile getting replaced wont effect it.  If anything now you get both Qsycn and Nvenc,Neutral
Intel,Intel sales are mostly laptops and most businesses do not give a single shit about iGPU or quicksync.   Hardly anyone actually buys intel for quicksync lol.,Negative
Intel,I actually did reply the switch 2 just below because i got tunnelvisioned on windows  He never replied after that because the switch is far more efficient than anything intel has and that too using a worse mode,Negative
Intel,Here is the kicker. You didn't need to sub and then buy games.  If you bought a game you can play it without a sub.  But google failed spectacularly to market it that as you didn't even know that.,Negative
Intel,"well, their dGPUs are a niche product in laptops already. Why would an agreement to co-develop an APU with Intel change that?",Neutral
Intel,"The Arc Pro B stuff hasn't even released to consumers yet.  B50 is shipping the 25th, B60 hopefully not to long after.  If Intel can really ship B60's at MSRP this year they will sell every single one they can make. There's nothing out there that comes even close to being competition for it.",Neutral
Intel,"The deal was negotiated for months and finalized / signed on September 13th (per Greg Ernst on LinkedIn). That means they announced the deal just 5 days after making the deal.  And even then, these parts arent coming out for years.",Neutral
Intel,"what actual workloads? In gaming they are basically tied, and that's what most consumers care about. The workstation laptop market is very small and niche and most people use a discrete gpu there anyway",Neutral
Intel,What “actual workloads” on an integrated graphics would you be referring to?,Neutral
Intel,"What kind of work are we discussing here you think the bulk of office laptop from dell, hp, and Lenovo (which is the majority of worldwide pc shipment) have to do?",Neutral
Intel,Whether MT matters or not seems to depend on if Intel is food or not.  Don't people always talk about how gaming is king?,Neutral
Intel,We will see. For the moment all we can go on is what Intel is saying and they're saying it's not going anywhere.,Neutral
Intel,It's really funny how you assume that OEMs and ODMs won't also charge more on top of the component cost.,Negative
Intel,"No I don't, actually. Why would enterprise customers bother running anything on laptops? There are no reasonable use cases for this. The only ""AI"" thing that needs to exist on a laptop is advanced search and windows recall, which the existing Intel hardware is perfectly capable of handling. Plus, you are aware Microsoft copilot+ requires a separate NPU right? Which makes the whole idea of running ""AI"" on said Nvidia IGP quite redundant.",Negative
Intel,"Yea, gamers definitely live in some sort of mirror universe.  From my perspective, Intel drivers are significantly better than AMD drivers, which are worlds ahead of Nvidia drivers.",Positive
Intel,"Samsung's 8LPP was one of their best nodes lmao, what is with this revisionism",Positive
Intel,"90 percent of servers using Intel for Quicksync are not going to waste their time buying an Intel ARC GPU for $300 that they can't even source.   If Intel makes this mistake, literally every single person in the industry would just switch to AMD CPU's. As they are superior in virtually every single aspect of computing. If you are forcing people to buy an Intel ARC GPU, then the CPU they own will not matter.   Contributors to things like Jellyfin already stated they will just spend all their development time optimizing for AMD APU's.   I have worked in Comp Eng for 20 years. I can assure you Intel will not survive if they do this. They will likely be bought like the first company I worked for, ATI.",Negative
Intel,"AFAIK, the 265KF for example, does not support quick sync.",Neutral
Intel,Nvidia laptops are niche products....HUH????  Have you seen the steam charts? Their laptops are big Even rivaling desktop numbers.  https://store.steampowered.com/hwsurvey/videocard/,Neutral
Intel,Its bait to sell off remaining stock and bookings.,Neutral
Intel,> In gaming they are basically tied  LNL vs Strix Halo? Absolutely not.,Neutral
Intel,"Gaming, content creation. Your choice, really.",Neutral
Intel,CPU performance was never discussed in this thread only iGP.,Neutral
Intel,"> and they're saying it's not going anywhere  They've said nothing about dGPUs in particular. If anything, likely died before this deal.",Negative
Intel,"Of course they will, Because you and i both know which one will be in more demand and sell more.",Neutral
Intel,"> Plus, you are aware Microsoft copilot+ requires a separate NPU right? Which makes the whole idea of running ""AI"" on said Nvidia IGP quite redundant.  That bit seems to be changing, fwiw.",Neutral
Intel,"Why are you of the belief that people care about CoPilot+ at scale?  There are plenty of reasons a company would want local compute options instead of compute in the datacenter. Data security, data ingress/egress costs, local AI compute for appliance-type deployments, edge inference, network constraints, etc.  There is not a one-size-fits-all approach to AI compute. NPUs are not sufficiently powerful enough for the workloads today, let alone where AI compute is heading.",Negative
Intel,Wouldn't change anything as the NPU is im the SOC tiles. Not the GPU tile.,Neutral
Intel,"> Intel drivers are significantly better than AMD drivers, which are worlds ahead of Nvidia drivers  What?",Positive
Intel,It's not even a current *Samsung* node. It's like 3 full gens behind state of the art.,Neutral
Intel,It is awful compared to modern tsmc 5nm. Which all CPU and GPUs use,Negative
Intel,I think the smarter move would be to do what amd did. A VPU for servers  https://www.amd.com/en/products/accelerators/alveo/ma35d/a-ma35d-p16g-pq-g.html  Now that will sell. Nvidia isnt competing there either.,Neutral
Intel,"> Contributors to things like Jellyfin...  Unless they are contributors to FFmpeg, their statements are meaningless. Jellyfin (and projects like it) has zero features that are gpu related that aren't routed through FFmpeg.",Negative
Intel,"They aren't saying you'll need a gpu, they are saying the quicksync and other media engine features are on the cpu SOC tile which will still be present in hybrid nvidia igpu tile designs. They could still stuff it up by not supporting it but the hardware will be there.",Neutral
Intel,F = no IGPU,Neutral
Intel,Less than 25% of PCs sold have Nvidia graphics at all. Don't care about the Steam hardware survey lol. Intel iGPU outsells Nvidia 2.5 to 1 in client.,Negative
Intel,"Nah.  The AI market is too hype for Intel to just drop it, and Nvidia is exploiting enough market power that there's certainly space to profitably undercut them in that segment.",Neutral
Intel,What content creation are people buying AMD iGPU laptops for specifically? This is the weirdest lie someone has ever told on this website lmfao,Negative
Intel,"140v does not lose to 890m in either of these, what are you on about?",Neutral
Intel,"Yeah, the one that has an almost identical featureset for $500 less",Neutral
Intel,"Companies are not clamoring to get AI compute more powerful than what the NPU can provide, locally, and at scale across their whole fleet.   If they were, P series and Precisions would've supplanted E/T and Latitudes by now.   Like, what's the usecase of that much local AI on individual workstations? Presumably any data being inferenced will he in some shared location, and with it, so will the inferencing hardware.",Negative
Intel,"OK, what kind of workload do you think they will actually run on laptops that is too powerful for existing and future Intel hardware, not too powerful for whatever Nvidia IPG will be bundled, and they don't want to do with an on-prem solution?",Neutral
Intel,You still haven't addressed the why,Negative
Intel,"No shit, but compared to its contemporaries it did pretty well.",Negative
Intel,"Huh, weird, last time I checked Blackwell, Navi 48 and Zen 5 were all on TSMC N4, Arrow Lake on N3, and none of them on N5. There seems to be an awful lot missing from ""all CPU and GPU""",Neutral
Intel,"Of course they are. Even upstream FFmpeg couldn't keep up with the demand and they had to write additional code themselves to drive the hardware transcoding.  https://github.com/jellyfin/jellyfin-ffmpeg/wiki  It's not true that they spent all their time on APUs, they did spend some time, but it was much better than their competitors spending almost no time on AMD.",Neutral
Intel,Right. The person I'm responding to is saying that Intel CPUs would still support Quicksync if Intel removed the Arc iGPU tile.,Neutral
Intel,"Nvidia's gaming division makes over 4 billion a quarter. And its a safe bet more than a 1 billion comes from Laptops alone.  That not a niche market at all. Yes, its not market dominant if you include iGPUs which even exists in celerons, but if you call over 1 billion a quarter niche, we need to rewrite the meaning of ""niche""",Neutral
Intel,Intel is going to get Ai market by being the exclusive x86 supplier of Nvidia.  Both on server and client.,Neutral
Intel,"Intel's too late to the AI market, Arc flopped, Gaudi flopped harder, broadcom is doing custom solutions, AMD is number 2. China is all in with full state backing. What is Intel going to do? Nobody needs a third place.",Negative
Intel,> This is the weirdest lie someone has ever told on this website lmfao  Weirder than claiming LNL outperforms Strix Halo in GPU?,Negative
Intel,They claimed it even beats Strix Halo.,Negative
Intel,"""identical"" lol",Neutral
Intel,"I think you're missing the point because you seem to believe that all users or departments have unconstrained budgets and can just buy whatever the ""perfect"" solution is for their workload. If that were the case, all workloads would already be handled in the datacenter, and no one would ever need a local GPU for compute, graphics, or AI workloads, which is not reality.  It's a hell of a lot cheaper to go buy a dozen laptops than to go buy a single B200.  Look at the entire entry laptop workstation market. There's a reason why those products exist, or else OEMs wouldn't make them.",Neutral
Intel,Won't matter anyway because of how poor NPU support is.  ROCm doesn't support Ryzen NPUs. Don't think Intel OneAPI does either. And CUDA obviously doesn't.  There was good post about NPUs here  https://www.reddit.com/r/hardware/s/JGJ45bpbjN  There is very little reason to support NPUs.,Negative
Intel,Intel 14nm did not only “pretty well” but amazingly compared to its 2014 contemporaries.,Positive
Intel,I think the F series has both Display engine and Media Engines disabled cause they assume you are going to be using a requirered dGPU for that.,Negative
Intel,"Intel iGPU outsells Nvidia dGPU 2.5 to 1. There's no need to redefine niche, which means ""a specialized segment of the market for a particular kind of product or service."" I think being a high end upsell product that's found in less than 25% of computers qualifies.    >You are kidding yourself if you think Nvidia will let their iGPUs be a low volume products.  That's what you said. They already *are* a comparatively smaller market than Arc in client. The Nvidia x Intel collab product will be the same: A lower volume specialized part that costs more and performs better, that some people will pay extra to upgrade to. But it will not form the bulk of Intel's volume.",Neutral
Intel,They are very clearly not including Strix Halo. You're somehow the only one who is thinking of Strix Halo.,Neutral
Intel,This is the card Intel should have released instead of the gaming version(s). Took them 2 years,Negative
Intel,"Not worth it at $599, slightly worth it at $499  This has 1/2 the memory bandwidth of a 3090 and 70% the compute performance.",Negative
Intel,Far more than I expected them to come out at. Damn.,Negative
Intel,"Hello WarEagleGo! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,I’m getting one when it releases in Australia,Neutral
Intel,Thank goodness it's not an unprofessional gpu,Neutral
Intel,"The gaming cards require significantly less robust driver and application support. Releasing them first makes sense when they still needed to work on said driver and application support, the timeline likely wouldn't have meaningfully changed.",Negative
Intel,b580 should've allowed SR-IOV upon release. That's all the card really needs to sell.,Neutral
Intel,"Even at $599 it's significantly cheaper than a used 3090, comes with a full warranty, and can be bulk purchased new.  If you just need the VRAM, which is what this is targetting, I think it can definitely make sense.",Positive
Intel,"This is worth it at any price point.  I am getting tired of gamers not understanding that SR-IOV support and 200W are very important to some people. If there were a 24GB AMD card out there for 200W and $600 I would buy that too regardless of SR-IOV support.  Neither Nvidia nor AMD have a card at this price point with this feature set. You are looking at $600 for 24GB of VRAM and SR-IOV support. AMD's SR-IOV offerings are in the ""thousands of dollars"" and Nvidia's cards in this segment do not meet the 200W and 24GB of VRAM targets. The closest thing is the P40 and it's a decade old, has way higher power consumption and is about to be end of lifed.  Please tell me the card that exists in the same segment as the B60. I will wait.",Negative
Intel,"This is $150 cheaper than NVIDIA's competing A2000, which has less VRAM.  3090s do not get certified drivers, which are a business requirement for certain workloads.",Neutral
Intel,The 3090 does not have ECC on it's VRAM nor certified drivers,Neutral
Intel,it's worth it for enthusiast collectors who want to own a rare Arc GPU. Especially if Intel abandons dGPUs. I'm sure Raja Koduri will buy it.,Positive
Intel,Totally not worth it.,Negative
Intel,"I'm more looking forward to the B50, but obviously local pricing is everything.",Positive
Intel,"Graphics cards are in fact seperated into two broad categories, consumer and professional, used by all the main manufacturers. They have completely different approaches to their design, different use cases, different drivers, different memory etc.",Neutral
Intel,"It has SR-IOV, certified drivers and other professional features...",Neutral
Intel,Both nvidia and amd only take a few months between releasing gaming and pro lines of new gpus. Intel managed to do the same with their first gen (only a few months). How come the second gen takes 2 years!?,Neutral
Intel,"Are used 3090s that expensive?  Here in Chile they go for about 550-600USD. Got one (luckily) at 450USD some weeks ago for AI/ML, without the box and such but it works fine.  Asked a friend from Spain and it seems there they go for about 600 EUR there.",Neutral
Intel,Do not underestimate the lack of CUDA.,Negative
Intel,A used 3090 is only $100 more.,Neutral
Intel,"It barely makes sense, at $600.  3090s are $700 now and the new 5070Ti Super (faster than a 3090) is expected to come out at $750 soon. And you’d have CUDA for either the Nvidia 3090 or 5070Ti Super.   I strongly believe that $600 is the price Intel set to ward off the scalpers. The niche of the B60 is pretty tiny at $600.   It’ll make a lot more sense at $500, which I suspect is the “real” price a month or two after it goes on sale.",Negative
Intel,Used 3090's are cheaper than this lol. Did you check the sold prices on Ebay at all? I can literally buy them at buy it now prices that are less than this.   94+ upvotes lol. 3090's sell for about the same price as 4070 supers as they have the same gaming performance.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1,Neutral
Intel,yeah it's got a good feature set for its price. people looking at it from a gaming product frame of reference rather than a professional product frame of reference are doing it wrong,Positive
Intel,"SR-IOV alone makes this a substantially different offering than a used 3090. If you know, you know.",Neutral
Intel,It is an competitive edge and hope Intel pushes on it hard. Same with AMD and its Strix iGPUs. I will buy unified memory on a desktop for a premium too - the M5 chip is going to make all 3 Intel/Nvidia/AMD wake up hard,Positive
Intel,Atlas 300i duo,Neutral
Intel,"Also 3090s are $200 more, are used, consume twice as much power (400W card BTW), and do not have SR-IOV support.",Neutral
Intel,Businesses aren't buying these cards these are destined to hobbyists home labs. The extra money to buy a CUDA supporting card is trivial for businesses they aren't going to buy Intel.  Certified or not the 3090 works wonderfully with all AI software out there while Intel does not.,Neutral
Intel,"It's cheaper than Nvidia's RTX Pro 2000, but it's also slower, but have more VRAM.",Neutral
Intel,The 3090 are one of the few GPUs with ECC.,Neutral
Intel,It was meant to be a joke. Not so funny I guess.,Negative
Intel,"> R-IOV, certified drivers and other professional features  Did Intel ship the SR-IOV part? Last I read (been a while) it was ""coming soon"".",Neutral
Intel,Why does this matter?,Neutral
Intel,AMD and Nvidia have been having extensive driver support for years  ARC is a much newer platform requiring more careful hand holding,Neutral
Intel,Was seeing one sold locally recently for $800-$900. They very much are still expensive here in the US.,Negative
Intel,"Yup 600 Euro is about (or slightly above) what used 3090s are sold for here (Sweden) as well. I would rather buy a significantly better card used than a worse one new, if sold at approximately the same price point.",Negative
Intel,"Used site jawa.gg shows 3090s priced from $750 to $950 (and of course, some people asking $999 or $1200)  https://www.jawa.gg/shop/pc-parts-and-components/geforce-rtx-3090-KNRXB30U",Neutral
Intel,Used market is freaking insane. It is better to grab new or open box.,Negative
Intel,Over here used 3090s are sold for 500-600€.,Neutral
Intel,"Well sure if you specifically need cuda then this won't work, but there's also a large subset of people who just need vram for as cheap as they can get it. And for that purpose this Intel card is interesting at least.",Neutral
Intel,I hope this advances well for the future hardware and software releases -[MLIR] (https://www.phoronix.com/news/Intel-XeVM-MLIR-In-LLVM),Positive
Intel,Just keep on digging the hole man.  A 5070Ti or 5070Ti Super or 5090Ti Super Extreme Titan do not have Support for SR-IOV. You do not know why people want these products.  > Muh niche  **Gamers** are the niche. Have you been living under a rock for the past 5 years?!,Negative
Intel,the super gpus are not expected to release soon?,Neutral
Intel,"Why have you posted this same garbage about a 5 year old used card so many times in this thread? It's a valid point, to some people I'm sure, but it's like you're weirdly obsessed with making it until someone hears you.",Negative
Intel,"....are you mistaking the listings of broken cards as new, working ones?  because when you take those out you'll see pretty quick that all of the 3090's here are selling for FAR above $600 lol.",Negative
Intel,Used 4070 Supers dont sell for nearly $700+ on the low dude.,Neutral
Intel,Yes running business of used cards is how its done.....,Neutral
Intel,Typically run at 250W though to be fair.,Neutral
Intel,"They are less not more, no idea why people are making up used prices when we can check ebay sold listings.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1",Negative
Intel,not on the Vram like professional cards,Neutral
Intel,Because they're super late to the party.,Negative
Intel,"Wow, 800-900USD after 5 years is more than I would have expected.",Positive
Intel,"No one on hardware swap is willing to go above $600 for a 3090, trying to sell one now. This reminded me to just put it up on Ebay.",Negative
Intel,I have no idea why people make up stuff like this when we have ebay sold listing search.  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  Most recent one sold for $439.00 including delivery.  Again why are people making up used prices?,Negative
Intel,For what exactly do they need vram without cuda ?,Neutral
Intel,"Look at my profile. I have 0 comments in gaming and hundreds of AI comments. What do you know about running inference locally? Why do you think I mentioned memory bandwidth? Gamers don’t care about that. I haven’t played a game in years.   This is not a good deal for AI running consumers at this price, and a mediocre option for enterprise.",Negative
Intel,"False, they’re releasing in december or jan. So about 3 months from now",Neutral
Intel,"> Typically run at 250W though to be fair.  At which point we have to derate the performance. You know, to be fair.",Neutral
Intel,"The 3090 Ti did, but the standard 3090 did not.",Neutral
Intel,People do not understand AI.  People buy up used 3080s and use 4 of them in an AI Rig to run MoE models. 96GB of VRAM can run you basically anything that's not the full 500GB models like Kimi K2. The power consumption doesn't matter because they're making more money off of the inferencing than the cost of electricity and it's cheaper than cloud providers.,Neutral
Intel,"Where did you find that? FB? Im saving for a GPU here in chile too, was aiming for a 5060ti 16gb but the model I want is 630k in ... Idk the top result in solotodo",Neutral
Intel,https://www.ebay.com/itm/156720971107  That's a 3080 not a 3090 lol.,Neutral
Intel,"rendering, working on big BIM / CAD models, medical imaging,...",Neutral
Intel,CUDA isnt the only backend used by AI frameworks,Neutral
Intel,You mean your 0 profile history because you have it private?,Neutral
Intel,"And AI isn't the only use for a workstation card either, jfc.  3090s and 5070 Ti Supers don't have SR-IOV  3090s and 5070 Ti Supers don't have ECC VRAM.  AutoDesk for example will tell you to go pound sand if you try to get support and you aren't running a workstation GPU with ECC VRAM. So for businesses running AutoDesk software in a professional capacity, or businesses needing SR-IOV, you could offer them a 3090 or a 5070 Ti Super to them for $100 and it still won't be an option for them, because for their needs they might as well be paperweights.",Neutral
Intel,I thought we were talking about actual hardware that was available in the referenced country (US) and not a banned Chinese product that may or may not even exist and if it existed would cost nearly 10x as much as this product.,Negative
Intel,last leak on them is that they’re delayed. Haven’t seen any reports since then that they’re releasing in q4,Negative
Intel,You still get about 85% performance compared to stock settings.,Neutral
Intel,What are they doing that's making them money? Or are theu selling the compute somehow?,Neutral
Intel,"People do not understand Ebay sold listing search showing them selling for way way less than the values you guys just pulled out of your assholes?  https://www.ebay.com/sch/i.html?_nkw=3090+RTX&LH_Complete=1&LH_Sold=1  They sell for roughly the same price as 4070's as they have roughly the same performance, AI don't matter gaming is still king in this market.",Negative
Intel,"Sip, marketplace.  Si buscas por RTX 3090 vendidas, es una PNY 3090 a 450K, y todo perfect.  La 5060Ti es más lenta que la 3090 para juegos, la 5070 es un poco mejor o igual.",Neutral
Intel,"It's the only one that's always well supported and expected to work with decent performance.  We're months after RDNA4 was released and ROCm still doesn't have any winograd kernels for it, which means 3x3 convs, the most common kernel size, are 2 to 3 times slower than they could potentially be...  I've close to no experience with OneAPI and SYSCL, but I'd imagine AMD should be ahead considering they've been playing the game for way longer… And if ROCm is a nightmare then I can only expect the experience with Intel to be as bad, if not worse.",Neutral
Intel,"Oh good, I’m glad I set that private. You can literally google the username anyhow and see 1-2 example posts that underscores my point.",Positive
Intel,0 people are using autodesk with an intel arc gpu lmao,Negative
Intel,"You know nothing about the ai market, huh?  https://www.reddit.com/r/LocalLLaMA/comments/1n46ify/finally_china_entering_the_gpu_market_to_destroy/",Negative
Intel,Which is 36% higher performance per watt.       ... to be fair.,Neutral
Intel,"Gamers Nexus has a documentary on the GPU smuggling business, where even the 3060 12GBs are being used: https://www.youtube.com/watch?v=1H3xQaf7BFI  The 4090s, 3090s and other cards? There's such a demand for those in China that people will buy them up and smuggle them.",Neutral
Intel,Software Development,Neutral
Intel,Why did you spam my replies about cherrypicked results. The cards are worth $800-$900 and the listings you linked literally prove htat besides a few outliers that are $600-$700 to tank the prices.,Negative
Intel,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",Neutral
Intel,"No voy a jugar ni nada parecido, más que todo por los 16GB para blender y porque aún ando con una 1650 que ya no da para más. Pero tampoco querría irme por una 3090 o algo así, gastan demasiado power. Cómo es la experiencia de comprar usado? Me da miedo que me quieran joder 😅",Neutral
Intel,"Intel is a bit better for common models as they actually pay frameworks and model makers to optimize for it, unlike AMD.  I use flux and sd models and it’s much better on my arc than my Radeon 9600Xt",Positive
Intel,Private profile = Complete troll.  You're not an exception to this rule.,Negative
Intel,I too like to promote Chinese propaganda which isn't available in the United States due to the whole AI cold war going on and the fact that Huawei products have been banned in the US for what? 15 years?,Negative
Intel,EILI7? How are software engineers using GPU ~~imprints~~ inference on their home machines for software development? What am I missing?  E: a word.,Neutral
Intel,"Hell, one of them was just the cooler without the actual GPU.",Neutral
Intel,"En ese claro entonces tal vez valga la pena esperar a la 5070 Super, que supuestamente tendrá 18GB de VRAM.  Sobre comprar usado, todo bien en general, pero siempre hay que tener ojo y ojalá siempre probado. Yo fui a la casa del vendedor para revisar y todo bien, pero igual puede ser peligroso, por eso revisar bien los perfiles.  He comprado hartas 3090s usadas por FB Marketplace.",Neutral
Intel,"you’re wrong about cost, availability, performance, and just plain wrong overall lol",Negative
Intel,"The Chinese are busy making hardware, not propaganda. These are old cards that have been around for a while, I can assure you they are quite real.",Neutral
Intel,"The many hours of trial and error getting things to work, and building expertise with said labor would be my guess.  This isn’t home software engineers buying up all the inventory, its people getting around sanctions.",Neutral
Intel,And the one directly under that was the actual PCB...,Neutral
Intel,And you're a Chinese nationalist spreading Chinese propaganda.,Neutral
Intel,"> Neweggâs  I know this is mojibake, but this kinda sounds like a Lithuanian versions of Newegg lol",Neutral
Intel,The competing product is sometimes slower while also being twice the price.  If this wasn't a success then Nvidia would be  unbeatable,Negative
Intel,"What's the word on the B60? Even more VRAM (24GB), and double the memory bandwidth. I see it listed as ""released"" in various places, but can't figure out where to actually buy one.",Neutral
Intel,"My RTX a4000 doesn't support SR-IOV. I don't know about newer series, but at the time you had to buy the A5000($2500) or A6000 and then there are some crazy licence fees to use it.  For 350 i will buy it when it gets available just for this.",Negative
Intel,L1 techs had a great feature on these.,Positive
Intel,"Profitable product for Intel, wouldn't suprise me if Xe3P and onwards for dGPUs happens because stuff like this can do easy returns.",Positive
Intel,1:4 ratio of FP64 performance is a pleasant surprise,Neutral
Intel,"Honest question here: what makes it a ""workstation gpu"" that does it differently than say like a low end 5060/AMD equivalent?   Iis it just outputting 1080p ""faster""?",Neutral
Intel,"Its also just a whole 95 cards sold. (Past month, I’m unsure if its been up longer)",Neutral
Intel,"It will never be in stock again. It’s good for AI, hosting pass through srvio to VMs without licensing and a number of other things outside of gaming.",Negative
Intel,"Hello 79215185-1feb-44c6! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Let me know when it shows up on the steam hardware survey. That's the only barometer for success that true hardware enthusiasts care about.,Negative
Intel,what's up my Neweggâs,Neutral
Intel,My favorite game  Fallout Neweggas,Neutral
Intel,It also sounds like an ikea lamp name or something,Neutral
Intel,Geriau nei Bilas Gatesas,Neutral
Intel,I gotta spell Neweggâs?!?,Neutral
Intel,"The B50 appears to be an decent low-end workstation GPU, at least as long as the intended workloads don't effectively require CUDA in any way, shape, or fashion.    My one lingering question is what use cases actually *require* the certifications of a workstation-class GPU (which would rule out something like a relatively similar consumer-tier RTX 5060 Ti / 16GB) but wouldn't benefit from CUDA?  Then again I'm not exactly an expert in the field, so I could be completely off-base here.",Neutral
Intel,"I'm disappointed in Nvidia's inability to put out a stable driver since last December, I'm waiting to see if a competitor card will come out that meets my wants for an upgrade.",Neutral
Intel,Intel might be using the B50 as a pipe cleaner for the B60's drivers to prepare it for a retail launch in Q1 2026    IF they're doing this then it's a sound strategy,Neutral
Intel,"Double the memory bandwidth of trash is still trash.  Edit: Y'all can downvote me all you want, but 250GB/s is just slightly more than the 200GB/s of my low-profile 70W GTX1650 GDDR6 that I bought for €140 in 2019. Its absolutely pathetic and should be unacceptable for a new product in 2025, let alone a product of $350 !!!. Even double of this (\~500GB/s) of the B60 is less than a RTX3060. Pathetic products.",Negative
Intel,SR-IOV is the selling feature for me and why I have one ordered. Getting a Tesla P4 with nvidias vgpu licensing working is a pain in the ass and expensive.  I'll get it and sit on it until SR-IOV is released in case of scalpers/stock issues. If it doesn't pan out I'll either just sell it on or drop it into my home media server for the AV1 encoding/basic AI stuff.,Neutral
Intel,"Last time I checked GRID licensing can be faked out, but yes, only Quadro/Tesla and Turing/Pascal(IIRC) through driver mods can use Nvidia's vGPU.",Neutral
Intel,"The professional market is smaller than gaming and even more slanted towards Nvidia. This might be a nice side business, but can't remotely justify developing these cards.   Not even clear it's profitable either. The numbers here are negligible so far.",Negative
Intel,"Do people actually need and use FP64 at all anymore? I've got one or two original Titan cards that I haven't thrown out although I've never used them for this purpose either, because they apparently have very high FP64 numbers and if I recall correctly can operate in ECC mode as well.",Neutral
Intel,"iirc, SR-IOV and VDI support in the coming months, toggleable ECC support, and it is ISV certified",Neutral
Intel,"So I spec our PCs at work. We do anything from traditional office work, to intense engineering tasks. On our engineering computers we run MatLAB, Ansys, Solidworks, MathCAD, LTSpice, Xilinx, Altium and other such apps. Lots of programming, VMs, design work, simulation testing, number crunching, and on occasion AI work.   This means we spec systems like with RTX Pro 500, RTX Pro 2000, RTX A4000, A4500, A6000s. The reason we have these rather than cheaper GeForce cards is mostly 3 things. Power/form factor, Driver certification, pro GPU features.   So typically Nvidia keeps the top binned chips for their professional cards meaning the power efficiency to performance is top tier. So we can get high performance single slot or low profile cards, or get some serious GPU performance in relatively small laptops. Drivers usually are validated better than the GeForce drivers, so they include better bug testing, and the apps we use validate performance with the cards which helps us evaluate performance. They also have way more vram like the RTX A4000 has 20GB of vram while being just a supped up 4070. Then from a feature perspective they have better VM passthrough support, or you can enable the vram to run in ECC mode for error correction. Very important when running 24-48 hour simulations.",Neutral
Intel,Software support is a thing. CAD applications like solidworks and inventor don’t officially support the GeForce rtx or radeon rx line of gpus and they’re considered untested unsupported options. You can’t get any tech support if you’re using them. For a business that needs those apps you need a workstation gpu. They also come with ECC vram,Negative
Intel,ECC memory.,Neutral
Intel,"That kind of puts it into perspective.  Also, let my take a guess:  Newegg sells them well because of how dirt cheap they are, people buying actually expensive Pro cards will more likely do it directly via their system integrator.",Neutral
Intel,Oh when they get enough enterprise customers they will definitely charge licensing fees,Neutral
Intel,How many A2000s show on the hw survey? Because that's the Nvidia variant and it has been around for a long time..,Neutral
Intel,Lmao,Neutral
Intel,Komentarą skaitai per tapšnoklį ar vaizduoklį?,Neutral
Intel,I am no expert but don't these gpus have ECC vram. That's a enough to get labs/professionals to buy them.   You don't want the headache of lacking error correctiin in a professional environment.,Negative
Intel,"I seriously considered getting one for my homelab. I would really like some SR-IOV, and giving multiple VMs access to transcoding would be very useful. Ultimately decided against it because at the moment my CPU alone is powerful enough, I have other uses for the PCIe slot, and I would have to import one. But it's something I'm going to check in on whenever I'm browsing for new hardware from now on.",Positive
Intel,"I know in my field of work, solidworks certified hardware is one such application where certain features are gated behind workstation class cards.",Neutral
Intel,Vulcan does fine with inferencing.,Neutral
Intel,people are buying B60's  https://www.reddit.com/r/LocalLLaMA/comments/1nesqlt/maxsun_intel_b60s/,Neutral
Intel,"Most Zen-1 parts had much worse single core performance than Kaby Lake,    People still cheered on the competition anyway despite it's shortcomongs",Negative
Intel,"GTX 1650 has only 4GB of RAM at 128 GB/s; RTX 3060 is only 360 GB/s, and only 12 GB--or maybe just 8 GB for some cards--of RAM. But thanks for playing.   Edit: relevant username. Up voting you for jebaiting the crap out of all of us.",Neutral
Intel,you really dont want to fuck around with software licensing as a business. vendors do inventory audits to ensure nobody's exceeding their license allocations. piracy would automatically invite a lawsuit.,Negative
Intel,Grid licensing can be faked if you depend on a sketchy github driver that only works on Turing GPUs. You certainly don't want to be doing that in a professional setting where licensing costs are not a massive expense anyways.,Negative
Intel,"I believe mobile is the main reason they continue developing ARC IP, highly integrated SoC are crucial for lower power consumption and performance per watt, as more and more mobile designs are becoming more integrated (see strix halo for example) Intel knows it has to continue developing graphics IP that is competitive with competition. As for discrete cards, this is a battle in the long run to win, but it will take serious investment, we can hope that they won't axe as part of cost cutting measure.",Positive
Intel,"The B50 (16Xe cores) is pretty cut down compared to the full G21 (20Xe)die, it has 2600mhz boost clocks instead of the 2850mhz on the gaming cards, it uses 14GB/s memory (19Gbps on gaming cards) and it has a 128bit bus with 8 memory chips (B580 has 192bit bus with 6 memory chips)  The only costly thing about is the 2 additional memory chips.   I'm not saying it's extremely profitable but it can't be too expensive to make since a portion of the volume is likely faulty G21 dies that can't make a B580 or B580.   If Intel can sell the B580 for $250 without too much pain, then the B50 is probably making a profit",Neutral
Intel,"Yes, to the point where I’m considering picking up a Titan V on eBay. It’s a must for scientific computing, single precision floats accumulate errors fast in iterative processes.",Positive
Intel,I recognize those as words...,Neutral
Intel,I think it was obvious I was being facetious.,Neutral
Intel,> You don't want the headache of lacking error correctiin in a professional environment.  I think Autodesk tech support will tell users to piss off if they encountered software problems with a consumer GPU. That was the explanation that I got back in the university years when the IT department would use the cheapest possible professional GPUs instead of high end consumer GPUs.,Negative
Intel,"Yeah, if ECC is a hard requirement for whatever reason then that would certainly rule out all the GeForce-branded RTX cards.    Of course this then begs the question of what kind of labs / professionals are so cash-strapped as to not be able to afford something like a [RTX PRO 2000 Blackwell](https://www.nvidia.com/en-us/products/workstations/professional-desktop-gpus/rtx-pro-2000/) instead, which fits the same niche as the B50 (i.e. low-profile 2-slot @ <75w with 16GB of VRAM) while being faster and having a far superior feature set.",Neutral
Intel,I’ve been working in a professional aechtiectural environment for 5 years and haven’t seen the need for ECC once.   Can you explain situations where it’s needed? I’ve always wondered.,Neutral
Intel,"SR-IOV for (license-free!) vGPU is IMHO the killer feature here, perhaps along with being able to get 16GB of VRAM per card relatively cheaply and without needing auxiliary power.  Both open up interesting server and workstation use cases that can't be had cheaply from the competition.",Positive
Intel,Have you tried GPU paravirtualization?,Neutral
Intel,"You can buy it off AIB Partners but you can't buy it at retail (i.e. microcenter, newegg] and it doesn't have an official MSRP yet.   The prices you see now are what AIB's want to charge in bulk orders.    If you want to know how much let's say 5 B60's cost you have to get a quote from a distributor",Neutral
Intel,They need GPU IP for two things: client and AI. Anything else is expendable.,Neutral
Intel,"Yes, my point was *if* they have the gaming cards, they can justify the professional line, but it's not nearly big enough to justify making a dGPU to begin with.",Neutral
Intel,"SR-IOV is Virtual GPU (SR-IOV is IO Virtualization used to split PCIe lanes into virtual functions so their physical function can be shared between VMs). No consumer cards support Virtual GPU right now besides Pascal/Turing with driver hacks. AMD's SR-IOV offerings are [very limited](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#virtualization-support), [And Nvidia has a bigger selection](https://docs.nvidia.com/vgpu/gpus-supported-by-vgpu.html) but their budget VGPU options are being phased out (P40).  I believe VDI is Microsoft's implementation. I believe I've done VDI on my RTX 2070 before (I have done seamless sharing between host and VM), but I don't know if it's possible with AMD. Someone please correct me if I'm wrong here, I'm more familiar with the Linux side / vGPU than VDI.  ECC is Error Correcting RAM. I generally don't understand the use case for ECC either, but it is ubiquitous in HPC. All server boards support ECC RAM.  In modern environments most of these features need 16GB of VRAM minimum, but if you ever wanted to try it on a consumer card, you could get an old RTX 20 series and try it out with some driver mods. Optionally, the P40 is still pretty cheap ($250 used) and doesn't need those hacks at the cost of drawing a lot of power, which Intel has solved with their Battlemage Pro platform (by far the cheapest VRAM/$/W you can get).",Neutral
Intel,>Autodesk tech support will tell users to piss off if they encountered software problems with a consumer GPU.   A380 and A770 is also on the certified gpu list. But otherwise that statement is correct.,Negative
Intel,Can you buy an rtx pro 2000?  If i had to guess what percentage of wafers are for b200 chips i would say 90%.  I don't think there are enough pro 2000s around. I don't think there are enough gpus around in most cases.,Negative
Intel,"> Of course this then begs the question of what kind of labs / professionals are so cash-strapped as to not be able to afford something like a RTX PRO 2000 Blackwell instead  It is interesting. IDK how common it is, but one of my university labs had computers donated from nvidia with nice quadro GPUs for their time.",Neutral
Intel,Pathetic 1:64 ratio of FP64 flops,Neutral
Intel,RTX 3090 Ti and RTX 4090 have ecc. Not that they're cheap.,Neutral
Intel,"It's needed whenever the work you're doing matters and a single-bit error could cause significant harm.  Something like audio or video, a single-bit error probably isn't very noticeable.  Calculations, it absolutely depends on what you're calculating and which bit gets flipped; flipping the low-order bit in a number might not matter much and flipping the high-order bit could cause a big error.  Networking, it depends on whether the protocols you use have more error checks at a higher level (TCP does; UDP does not).  If in doubt, you want ECC, but market segmentation to mostly restrict ECC support to ""server"" chips and boards and charge more for ECC memory means you'll overpay for it.",Negative
Intel,"For rendering and most graphics use cases, ECC really doesn't matter that much.   But for a few very specific compute cases. Like long running numeric simulations, it can make a difference. This is for stuff that is going to be long running and relies on numeric/data accuracy, ECC tends to be appropriate.",Neutral
Intel,"They're so good, I wish there was a single slot variant.   I want to put them in my MS-01s. The Sparkle A310, being the main candidate for deployment in those machines, only has 4GB and its maximum h264 encoding throughput actually drops below the iGPU (although its h265 and AV1 throughput slaps the 12900H/13900H). It's just a little too low to comfortably handle the Plex server usage I have, so the iGPU remains in service until a suitable competitor arrives.",Positive
Intel,"IIRC, that requires a Windows host right? That's a non starter for many people unfortunately",Negative
Intel,my guy sr-iov is a type of gpu paravirtualization.,Neutral
Intel,The Asrock b60s are $599,Neutral
Intel,AI doesn't even need a GPU; it can have its own accelerators - see Gaudi.,Positive
Intel,"One use case for ECC, is when the data is critical and can’t be lost.",Neutral
Intel,>  I generally don't understand the use case for ECC either  Its for when you don't want errors to just be ignored?   How is that hard to understand?,Negative
Intel,ECC should be in literally all memory.,Negative
Intel,"\>but their budget VGPU options are being phased out (P40).     I mean, the T4, L4 , and A16 exists...     I'm also not sure why low end workstation GPU needs SRIOV support.",Negative
Intel,I see them at least available in my stores. although mostly as backordres via remote warehouses but they seem readily available with some shipment time.,Neutral
Intel,"but you can still do gpu paravirtualization without sr-iov using Mediated Passthrough, API Forwarding (RemoteFX) or Dedicated Device Assignment",Neutral
Intel,"Where can you get them? And are they for sale yet, or pre-orders, or...?",Neutral
Intel,"The problem with Gaudi (I know, I've written code and run training runs on it) is simply that the programming model is not oneAPI, or whatever oneAPI becomes. Yes, pytorch works, but people care a lot about software longevity and long term vision when buying $5mm+ of GPUs (and these are the purchases Intel cares about that can actually start to offset the cost of development).   The whole purpose behind Falcon Shores (and now Jaguar Shores, if it will even happen) is to put Gaudi performance (i.e. tensor cores) in an Xe-HPC package. Unifying graphics and compute packages is what NVIDIA was able to achieve but not yet AMD, and it's really great for encouraging ML development in oneAPI.  See this post to see where Intel would like to be: https://pytorch.org/blog/pytorch-2-8-brings-native-xccl-support-to-intel-gpus-case-studies-from-argonne-national-laboratory/ (they don't mention the ""XPU"" because it's Ponte Vecchio, which are iiuc worse than A100s).",Neutral
Intel,"Intel can't get people even in an AI shortage. No one wants to deal with an ASIC. That's why their AI solution is GPUs, starting with (hopefully) Jaguar Shores. So it's that or bust.",Negative
Intel,I spit my coffee reading that. Gaudi? The platform that nobody uses that Intel has to revise their sales estimates down each half quarter?,Negative
Intel,"Yup. For example you are doing a structural integrity physics simulation, and a single flipped bit can ruin your 1 week long run (and your liability insurer will reject your claim, a lot of them have standards requiring calculations to be done only on ECC for sensible reasons).",Neutral
Intel,"Great example of why certain people shouldn't reply if they don't have knowledge in the area.  - Tesla T4 is $650 Used and has 16GB of VRAM. - Tesla L4 is $2000 Used and has 24GB of VRAM. - Tesla A16 is $3000 Used and has 64GB of VRAM.  Compared to:  - Arc Pro B50 is $350 new and comes with 16GB of VRAM. - Tesla P40 is $275 used and comes with 24GB of VRAM.  If all you care is vGPU / VDI for a small amount of hosts, then no, you're not getting a Tesla A16. What kind of joke suggestion is that?",Neutral
Intel,"Mediated pass though requires big ass license fees vGPU/MxGPU, and isn't FOSS other than Intel's currently broken support that they abandoned for SR-IOV support.  API forwarding only support limited host/guest setups, and even more limited API support. The only FOSS support is VirGL, which only support Linux host/guest and only OpenGL.  Obviously fixed pass though is an option, but even that isn't without issue. NVIDIA only recently removed the driver restriction, they could add it back at any time. Plus you are limited on virtual machine by the physical GPU count. It works with Intel GPUs and is FOSS with them.  SR-IOV on Intel fixes all of that. It works amazingly well with their iGPUs, has no license issues, and is fully FOSS.",Negative
Intel,Hey no need to be aggressive towards the other user. Your comments are very helpful and I appreciated them a lot but keep it constructive please!,Positive
Intel,"LMAO, I actually have quite a bit of knowledge in this area.  If all you care for is VDI for a small number of VMs, then you'd go GPU passthrough. vGPU / MxGPU often requires higher levels of hypervisor software tier (i.e. VMware vSphere Enterprise Plus), requiring more money. For KVM hosts, setting up vGPU is a lot more difficult and time consuming than just straight up GPU passthrough.  Only two groups of people would be interested in GPU virtualization / splitting:  * Enterprise, in which they wouldn't care about the used card prices.  * Enthusiasts, in which they wouldn't want to pay for vGPU prices anyway. So why bother catering to this crowd?",Neutral
Intel,"Full GPU passthrough is not a solution that many people would consider because it is clumsier than using sr-iov (or potentially VirtIO GPU Venus). Plus for each extra passthrough instance I would have to add in another GPU and this greatly increases power consumption, heat output and cooling requirements. The process is not all that much more complicated at least on Turing GPUs with a hacked driver on KVM guests at least. Plus for passthrough, you probably still need an NVIDIA card because last I checked AMD cards still had a random kernel panic issue after being passed through.  My assumption is that sr-iov on the b50 will allow users an affordable way to have multiple guests on one host GPU without increasing power draw and paying for expensive alternatives and expensive vGPU subscriptions.",Negative
Intel,"...first time I heard people prefer SRIOV over GPU passthrough because it's ""clumsier"" lol. I'm sure setting up mdev devices in KVM, finding the correct corresponding GPU instances, making them persistent through reboot, then edit virsh xml for each individual VM is a lot easier than just doing IOMMU passthrough. /s     Again, enthusiasts don't care about power consumption / heat output / cooling requirements for their lab environment. Enterprise that do care about them are very willing to pay extra cost to get a production ready driver. You're creating a hypothetical situation that simply does not exist in the real world.",Neutral
Intel,"It's good they launching this, this card adds some competition to the landscape, but before anyone buys it they should figure out if their drivers are lighter weight.  It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  The more ARC cards out there the more developers get familiar with them, the more XeSS v2 gets added to titles, the more the drivers get matured and the better future ARC cards will be.  I'd happily pick up an ARC card... once they've proved themselves in terms of driver maturity and overhead.",Positive
Intel,This should be a great upgrade from my A750 if B580 performance is anything to go by. I hope it's under $400.,Positive
Intel,"Unless it's super cheap, it's not gonna be sold well at all.  Even here with all the ""enthusiast"" and people are saying ""make sure you have this hardware combo, that driver, these settings,..."". The average buyer would just simply pay 50$ more for an nvidia card and not have to worry about all that.",Negative
Intel,I hope they fixed the drivers CPU overhead problem or that GPU's gonna need a 7800X3D or 9800X3D to feed it fully.,Neutral
Intel,Too late for me I already went with a 9060xt but hell I had dreamt of it!,Negative
Intel,"I wish they'd get the drivers past the point of frequently broken, but also they haven't produced enough cards for any previous launch to make any dent in the market regardless.  It's pretty much guaranteed the upcoming super refresh will make much more of a difference in terms of bang for your buck.",Negative
Intel,The problem with arc is you need the latest and greatest CPU to go with or you lose 1/4 of performance,Negative
Intel,Intel has the ball in it's court   If you released a New GPU..  that is pretty much a 5070.... add on 24gb of ram...  and price it at 399   u will   make  boatloads.  it will play pretty much any game at max settings at 1440p..  They must really be  hating on turning down sony though at making the SOC for the PS6 cause the margins too low..they really would need that money now lol,Neutral
Intel,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Depending on the price I might give it a shot.,Neutral
Intel,Love it going to get one if I can scratch some money together,Positive
Intel,Wouldn’t there be a risk that future drivers will not be supported and that it comes with US government back doors?,Negative
Intel,"LOL, preparing ""box"" packaging   I immediately thought of advanced silicon packaging like CoWoS or whatever",Neutral
Intel,Who is gonna tell them G31 would be celestial die since B580 was G21 and A770 was G10?,Negative
Intel,"Ah yes, finally the 4060ti 16gb/4070 killer, only 1.5 years too late! Ig at least this will force price drops on the rx9070",Positive
Intel,Aren't the driver overhead issues really only seen on older processors that are six cores or less?  Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course,Neutral
Intel,"> It's hard to think of this fitting a niche of someone with a pretty powerful CPU who wants a midrange GPU.  Is it really though? Powerful CPUs are comparatively cheap, powerful GPUs are expensive. I know plenty of people who went with a 9700X/7700X or even 9800X3D but 'cheaped out' on the GPU because spending $1200 on a 4080 (at the time) was simply too much.",Neutral
Intel,"I agree with everything you said.  However, I myself will buy one just because I want more competition and so I am just going to give Intel a sale. Sure it doesn't move the needle much, sure Intel's probably not going to make any money out of it and I personally probably won't use it much, but I am just doing it out of principle. I sure am in the minority, but at this point I can't sit idle and allow this duopoly to continue without trying something.",Neutral
Intel,"It's a hardware problem, not a driver problem. The cards have insufficient DMA capabilities and uploads must be handled by the CPU, no driver will fix it, and as a consequence the B770 will have even worse overhead.",Negative
Intel,I have a B580 and the driver seems pretty stable to me at this point.,Positive
Intel,"The super refresh is only for 5070, 5070ti and 5080. I doubt the B770 will compete with the 5070 to begin with so those cards are more upmarket.",Neutral
Intel,my b580 has been stable,Positive
Intel,"I’ve had a B580 for 6 months and have experienced one game-specific issue with Tarkov. Everything else, old, new or emulated has worked fine.",Negative
Intel,"I’ve been using Arc on both windows and Linux since alchemist, it’s powering 3 rigs for gaming, transcoding, etc.   Initial few months was rough but drivers are absolutely serviceable and have been for a while, and continue to get better each release.  I play lots of different games on steam btw, very rarely do I have issues.",Positive
Intel,to our knowledge... i wonder what kind of uplift we'll see it have with next gen cpus,Neutral
Intel,It's BMG-G31.  https://videocardz.com/newz/intel-confirms-bgm-g31-battlemage-gpu-with-four-variants-in-mesa-update  https://videocardz.com/newz/intel-ships-battlemage-g31-gpus-to-vietnam-labs-behind-past-arc-limited-edition-cards,Neutral
Intel,"It isn't the number that determines the generation, it's the prefix.  A770 was ACM-G10 (alchemist G10), while the B580 is BMG G21 (Battlemage G21). The shipping manifests that have been floating around for the better part of a year have been for the BMG G31. Unless new leaks I'm not up to date with are discussing a G31 with a different prefix, everything points towards it being battlemage, not celestial.  Now I pray that Intel have found a way to mitigate the driver overhead. If not, the B770 will be utterly useless for gaming. Nvidia is bad in the overhead regard, but the B580 is damn near an order of magnitude worse.",Neutral
Intel,> Shouldn't pretty much anybody that has a 3600x or a 10th gen or newer i5 be just fine. Rebar required of course  Not even close.,Negative
Intel,"HUB showed the b580 lost like 20%+ perf between the 9800x3d and the 7600 or 5700x3d and actually fell behind the 4060, as the 4060 lost minimal performance on the weaker cpu vs using the 9800x3d. And the 7600x and 5700x3d can certainly power much stronger gpus like the rtx 5070 without bottleneck.  Edit: my bad, I didn't know it was for only a specific game, though still not a good result for b580 overall",Negative
Intel,Where do you find such information?,Neutral
Intel,Source: I made it the fk up,Neutral
Intel,"I was going to say, I put my sister in a b580 and she has had no driver issues in 6 months.",Neutral
Intel,"That's sort of my point, they'll probably still exert more price pressure across the stack than the b770, despite being a totally different segment.",Neutral
Intel,https://youtu.be/00GmwHIJuJY?si=z4wU05sJx2SeS7K1  How can people get on here and lie and literally no one questions them? The 7600 only lost anywhere near that performance on Spider-Man Remastered,Negative
Intel,"Inference, there was a blog post tracing and comparing what the Arc driver does with the Radeon driver. The radeon driver just sends a few pointers to buffers, the Arc driver sends large amounts of data. Assuming the driver programmers at Intel aren't idiots, it's because something is seriously wrong with the cards and DMA.",Neutral
Intel,"No, I inferred it from tracing what the driver does, and assuming the programmers aren't idiots.",Neutral
Intel,Try Mechwarrior 5: Clans on high and say there are no problems again.,Negative
Intel,"https://chipsandcheese.com/i/154252057/driver-cpu-usage-vs-application  By the bars, it looks API specific which hints driver problems.  But maybe there is some command protocol inefficiency. I'll certainly hope at least one of the Steves will look into B770 driver overhead.",Neutral
Intel,"that game runs on my 3080 ti like ass, just like all early UE5 games...  even with RT turned off, to hit solid 4k60 I needed DLSS and if I wanted 90+ fps I need to use DLSS performance / ultra performance.",Negative
Intel,Steve from HUB and Steve from GB both lack the technical knowledge to look into the underlying issues,Negative
Intel,"It doesn't even run, it crashes left and right on an Arc.",Negative
Intel,"Not looking for a chipsandcheese analysis, just whether the issue exists.",Neutral
Intel,"that I have no idea, on launch I did have crashing issues on my 3080 ti, but they did get resolved over time.  but if you are now seeing it still then welp  PGI is a small team that may not have gotten help / getting to it themeslves to make their game arc capable.",Negative
Intel,"Yeah, still doesn't work at the latest patch (and latest Arc driver) with anything other than low.",Negative
Intel,SR-IOV at that price. Who cares about anything else.,Neutral
Intel,Intel would be stupid to axe there Graphic card division if this proves to be successful.,Negative
Intel,"Single-slot variant or custom cooler please, my MS-A2 running proxmox is demanding this card.",Neutral
Intel,About 66% overall performance of a B580 it looks like. That's really nice for a 70W card.,Positive
Intel,"This is exciting, definitely looking forward to the b60 as well.",Positive
Intel,"Obligatory ""Intel is exiting the GPU business any moment now"".",Neutral
Intel,how hard are tehse to actually buy?,Neutral
Intel,"Buying one, this is impressive",Positive
Intel,Its better than a 1.5 year old bottom of the range card....well done i guess.,Positive
Intel,Better than NVIDIA? lol .... oooookay,Positive
Intel,"Haven't seen the video, but I'm already buying one if that's the case",Neutral
Intel,Literally it could be a damn arc a310 or rx6400 and people would buy that card at $350 without licencing bs. For anything VDI related the B50 is huge.,Negative
Intel,Intel’s “MOAAAAAR CORES” in the GPU space???,Neutral
Intel,what is that? SR-IOV?,Neutral
Intel,Super interesting!  Wonder how well it would handle AI tasks like Frigate while another VM uses it for inference or a third doing video transcoding with Plex.,Positive
Intel,16 GB VRAM too,Neutral
Intel,They will eventually axe it.,Neutral
Intel,Instead of axing it maybe spin it off like AMD did with Global Foundries?,Neutral
Intel,And if it isn’t successful?,Neutral
Intel,They do have 3rd party vendors for ARC PRO Cards this time around so it most likely will happen.,Neutral
Intel,"The B60 is more exciting to me just for that 24GB VRAM. Still, at this price point the B50 is a pretty compelling buy tbh.",Positive
Intel,I think it would be really stupid for them to do so.,Negative
Intel,You can preorder from newegg now. They ship later this month.,Neutral
Intel,"One Swedish retailer I checked has them coming into stock next week (10 September) and open to orders, how much stock there will be however, I have no clue.",Neutral
Intel,Same. I put my preorder in. Plan to put it into one of my sff builds.,Neutral
Intel,Why is this impressive for $350 USD? How will this be useful for you? I’m not being sarcastic. I am genuinely curious.,Neutral
Intel,What did bottom of the range cards cost 1.5 years ago?  How much VRAM did they have?  Did they support SR-IOV?   Just think for a bit sometimes.,Neutral
Intel,It quite literally is. Watch the fucking video.,Negative
Intel,"Wendell confirmed as much in the comments, looking forward to his future testing of the card.",Positive
Intel,What does AMD have in this product segment?,Neutral
Intel,"Ingen av de kortene greier LLM, hvis du tror det.",Neutral
Intel,"Ingen av de kortene greier LLM, hvis du tror det.",Neutral
Intel,"I didn't know either so I looked it up.   ""SR-IOV (Single Root Input/Output Virtualization) is a PCI Express technology that allows a single physical device to appear as multiple separate virtual devices, significantly improving I/O performance in virtualized environments by giving virtual machines direct access to hardware. This bypasses the overhead of a software-based virtual switch, resulting in lower latency and higher throughput for demanding applications by dedicating virtual functions (VFs) to guest VMs.""",Neutral
Intel,Everyone (including Nvidia) is moving toward APUs with large GPUs onboard. Why would Intel kill their chance at competing in that market?  They've already withstood the most painful part of the transition. There's no point in stopping now.,Neutral
Intel,They are keeping their Fabs which is even more expensive to maintain why would they sell GPU not to mention their iGPUs are pretty Damm good nowdays not like meme in Intel HD4400 even though they could play any game /jk.,Negative
Intel,Doubt they have the revenue to spin out successfully without significant investment from outside sources.,Negative
Intel,Sadly Intel has a recent history of making poor life choices.,Negative
Intel,"Maybe it's just me, but this reads as AI generated.",Neutral
Intel,"I dunno man, I was building a PC for work and the 3050 was the cheapest Nvidia card I can get and the 7600 is the cheapest from AMD. Huge price gap between the two, by about 100 USD. AMD really needs to buck up their APUs to render cheap GPUs surplus or have something cheaper than a 7600 to price match the 3050.",Neutral
Intel,"They're comparing it to an entry-level NVIDIA GPU, the A1000. Saying that Intel GPUs are ""better than NVIDIA"" as a universal statement is flat-out wrong. Let's see some competition to the RTX 5070 Ti, 5080, or 5090. NVIDIA has zero competition on mid-range and high-end GPUs.",Negative
Intel,Radeon Pro V710 and you can't even buy it retail.,Negative
Intel,thanks 🙏,Positive
Intel,Because intel shareholders are super short sighted.,Negative
Intel,this comment is the weirdest version of 'corporations are people' that i've encountered,Negative
Intel,Lmao seriously the formatting and the amount of bolded words just screams AI,Negative
Intel,It's AI generated in your mind,Neutral
Intel,"because AMD has a bad habit of leaving a bunch of their older cards in the channel and having them become the low end...  CPU and GPU, AM4 lives for so long because there are still piles of the stuff in the channel and just gets slow tiny discounts till its gone in full  its like their demand forecast is too optimistic or something but at this point I think its deliberate",Negative
Intel,Because this is not a gaming GPU and thus the A1000 is the correct card to compare with.,Neutral
Intel,Good luck using those super gpus to host multiple gpu accelerated vm with one card. Nvidia won't let you.,Negative
Intel,"Yes, compare an Arc Pro to a GeForce, totally the same market.",Neutral
Intel,"That seems more mid or high tier rather than these relatively low tier gpus, the b50 is a cut down b580...  Also the v710 seems like the kind of ""passive"" gpu that's ""passive"" as long as it's next to several 7,000 rpm fans.  So it would probably not work very well as a retail car because it's rack server focused.",Negative
Intel,"I think the really loud short sighted shareholders have quieted down a bit after it became clear they helped drive the company to where they are. Hell, they're probably not even shareholders anymore.",Negative
Intel,The weirdest version was Citizens United,Neutral
Intel,Did you not figure out why they’re bolded?,Negative
Intel,I'm aware but it's the only current gen GPU for graphics workloads that has virtualization support from AMD.,Neutral
Intel,"The current Chairman of the board, Frank Yeary, is one of these stupid short sighted people. He REALLY wants to sell the fabs, and is probably the reason Intel went through their latest round of layoffs (Lip-Bu Tan wanted to raise money from Wall Street, Yeary apparently sabotaged it).",Negative
Intel,"if corps are people, they should be allowed to vote, right ?",Neutral
Intel,"Not only that, but because they are people, they should also be able to fund gigantic super PACS to get a candidate into office. I love America!",Positive
Intel,"u/michaellarabel It would be super cool to have Molecular Dynamics benchmarks for these kind of cards, since you already use them for CPU testing and a few of them (e.g. GROMACS) support APIs from all three vendors (CUDA, ROCm, SyCL, + OpenCL)",Positive
Intel,"I would be so curious about a die shot and GPU architecture of the A19 vs 8 Elite. Because Apple went from very little rt not so long ago to leading the charts. Like how many RT-Acceleration units are used in the GPU and how other components play into that.  Definetly exciting times, I think that the major players, Qualcomm, Apple, and Mediatek, are not so far away from each other, the leader can change in a generation or two, which hasn't been the case often in recent history.  Especially interesting will be how AMD client will turn out in 2026, if they are still on the charts (we expect Zen 6 clients on N3) or if Qualcomm will top windows charts, and Apple overall all the charts.",Positive
Intel,"Growing the E core into a medium core is really important and something I was worried apple might miss out on, but this means they should be able to keep up in multicore as trends point to flooding CPUs with E cores.  They really ~~buried~~ deleted the lede by not even mentioning the E core improvements beyond the cache and undersold the GPU. I suppose this means we should expect a lot of bragging in the M5 keynote.",Neutral
Intel,Youtube link:  [https://www.youtube.com/watch?v=Y9SwluJ9qPI](https://www.youtube.com/watch?v=Y9SwluJ9qPI),Neutral
Intel,I cannot wait to see what's in store for 2026 Mac Studios and the M5 CPU. Especially if M5 Ultra makes its debut. AI workloads should see a significant performance boost by 3-4x? I wonder if M5 Ultra will offer 1000GB/s memory bandwidth.,Neutral
Intel,6 wide e core holy shit,Negative
Intel,"A major exciting aspect for me is the massive boost to Raytracing performance. The M4 Max is the closest anyone has ever come to matching Nvidia in 3D Raytraced Rendering, beating out even AMD. In Blender M4 Max performs somewhere in between an RTX 4070M and 4080M.  A 56% leap in RT performance would essentially put an M5 Max closer to a RTX 5090M than anyone before at a fraction of the power.",Positive
Intel,"Is there a link for English? If not, can you summarize how they tested sustained performance and how much is the improvement over previous generations?",Neutral
Intel,"The E core having more improvements than just 50% larger L2 is a nice surprise, but damn the efficiency and performance of it is insane. 29% and 22% more performance, at the same power draw is insane, clocking like 6.7% higher too. They used to be behind the others in performance with the E cores but had better efficiency but now they both have better performance and efficiency.  As for GPU, I always wanted them to focus on GPU performance next and they finally are doing it. Very nice, the expected 2x FP16 performance, which now matches the M4 which is insane(M5 will be even more insane). Gpu being 50-60% faster is a nice sight to see. For RT performance(I still find it not suited for mobile but M5 will be a separate matter) I’m surprised that the massive increase is just from 2nd gen dynamic caching, the architecture of the RT core is the same, just basically a more efficient scheduler which improves utilization and less waste.  For the phone, vapor chamber is nice, them being conservative on having a low temperature limit can both be a good and bad thing which is shown, the good thing is that it means the surface temperature is lower so the user won’t get burned holding the device, and the bad thing is that it can leave performance off the table which is shown. As that can probably handle like another extra watt of heat and performance. Battery life is very nice, the fact that it can match other phones with like over 1000mAh bigger battery is funny. As people always flexing over how they have like a 4000, 5000mAh+ battery, of course having a bigger capacity is better, but the fact that Apple is more efficient with it and can have the same battery life at a much smaller battery speaks volumes about it.",Positive
Intel,"The occupancy characteristics of A19 Pro are quite incredible. 67% occupancy for a RT workload.  Look at Chips and cheese's SER testing. 36-44% ray occupancy with SER in Cyberpunk 2077 RT Overdrive.     Assuming NVIDIA can get this working on 60 series an effective a 52-86% uplift. After OMM and SER this seems like the third ""low hanging"" RT fruit optimization. Anyone serious about a PT GPU architecture NEEDs dynamic caching like Apple. And no this is not RDNA 4's Dynamic VGPR, it's a much bigger deal. Register file directly in L1$ has unique benefits.",Neutral
Intel,"Nice been waiting for this. P-core frontend improvements and branch, and a wider E-core with it's newer memory subsystem shows great YoY gains as usual. Though I am not surprised since it's been years leading up to this that Apple has steadily have been increasing power/freq to get the rest of it's performance gains, although IPC YoY is still class leading. The wider e-core takes the stage which is now commonly being focused in the industry (ex. Intel: Skymont etc). Excited for any outlet doing die analysis (I don't know if kurnal has done it yet).  Real generational GPU gains, instead of last year's YoY tick. Supposedly GPU size has not increased and that is impressive. Massive FP16 compute matching the M4, really shows their commitment to ML (as if naming 'tensor cores' wasn't obvious) and this will greatly help with prompt processing if you're into local models. Finally with a vapour chamber in the PRO models, performance overall really stretches it's legs and sustained is really respectable.  Also, since I'm skimming, I'm assuming A19 base like it's predecessor is a different SoC to the Pro. It is also really really refreshing to see the base A19 be better than the 18 Pro, little to no stagnation and a year at that. The base iPhone 17 looks like a reaaly reallly good option, more than ever, wished they didn't drop the Plus model. But man, I feel like waiting another year, hearing rumours about N2 and new packaging tech excites me.  That said, looking forward to QC, MT, and Samsung. SD8EG5 seems to be closing the gap, and that'll be very interesting tho those GB numbers don't tell things like power.",Positive
Intel,"""Generations ahead of other ARMs M cores"".   Uhm we are getting the Dimensity 9500 and 8 elite gen 5 next week    The C1 Pro has 20% IPC improvement IRRC, plus this is N3P   Let's not jump to conclusions before seeing the competition    I also wonder if QC made changes to the E cores",Neutral
Intel,>SLC (Last Level Cache in Apple's chips) has increased from 24MB to 32MB  Really tells you how pathetically stringent AMD has been with cache sizes on their apus (no die size excuse allowed here because they never use leasing nodes specially N4 was already old when Strix point debuted),Neutral
Intel,">A19 Pro E core is **generations ahead** of the M cores in competing ARM chips.  >A19 Pro E is 11.5% faster than the Oryon M(8 Elite) and A720M(D9400) while USING 40% less power (0.64 vs 1.07) in SPECint and 8% faster while USING 35% lower power in SPECfp.  >A720L in Xiaomi's X Ring is somewhat more competitive.  No, Apple's 202**6** E-cores are just **+3% (int perf)** and **+1% (fp perf)** vs Arm's 202**5** E-cores, though at **-22% (int)** and **-16% (fp)** less power.  Note: Geekwan's chart is wrong. The Xiaomi O1 does not use the A72**0.** It uses the upgraded A72**5** from the X925 generation. Not sure how Geekerwan got the name wrong, as they recently reviewed it.  Integer  |Core / Node|int perf|int %|int power|int perf / W|int perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|4.17|103%|0.64W|6.51|132%| |Xiaomi A725 / N3E|4.06|100%|0.82W|4.95|100%|  Floating point  |Core / Node|fp perf|fp %|fp power|fp perf / W|fp perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|6.15|101%|0.92 W|6.68|120%| |Xiaomi A725 / N3E|6.07|100%|1.09 W|5.57|100%|  I would not call this ""generations"" ahead.",Neutral
Intel,I always wonder how they plot the architecture map and figure out such as the depth of LDQ kind of things...Is it public somewhere? That kind of detail won't be able to get via regular benchmark right?,Neutral
Intel,A19 Pro GPU is now only 1.62GHz vs 1.68GHz in A18 Pro while having the same number of ALUs (768). Does that mean the increased performance is basically due to memory bandwidth increase?,Neutral
Intel,">Power however has gone up by 16% and 20% in respective tests leading to an overall P/W regression at peak.  That's really not good for the A19 **Pro**, sadly a mark against Apple's usual restraint. It's a ***significant*** *perf / W* downgrade in floating point. The 19 Pro perf / W is notably worse than **ALL** recent P-cores from Apple, Qualcomm, *and* Arm:  |SoC / SPEC|Fp Pts|Fp power|Fp Perf / W|Perf / W %| |:-|:-|:-|:-|:-| |A19 Pro P-core|17.37|10.07 W|1.70 Pts / W|84.2%| |A19 P-core|17.13|8.89 W|1.93 Pts / W|95.5%| |A18 Pro P-core|15.93|8.18 W|1.95 Pts / W|96.5%| |A18 P-core|15.61|8.11 W|1.92 Pts / W|95.0%| |A17 Pro P-core|12.92|6.40 W|2.02 Pts / W|100%| |8 Elite L|14.18|7.99 W|1.77 Pts / W|87.6%| |O1 X925|14.46|7.94 W|1.82 Pts / W|90.1%| |D9400 X925|14.18|8.46 W|1.68 Pts / W|83.2%|  These are *phones*. Apple, Arm, Qualcomm, etc. ought to keep max. power in check. This is *o*n par with MediaTek's X925, a bit worse than the 8 Elite, and much worse than Xiaomi's X925.  I would've loved to see efficiency (joules) measured, like AnandTech did. That would show us at least if ""race to idle"" can undo this high 1T power draw or not in terms of battery drain.",Negative
Intel,"All of these benchmarks are sort of difficult to compare when the other chips aren't made on the same node N3P. Apple buys first dibs on the wafers so they always have that advantage, it isn't always about the architecture itself.  It will be more interesting when there are Qualcomm chips out with their architecture on this N3P node, and the Mediatek chips with the usual off the shelf ARM cores on this node too to compare.",Neutral
Intel,"Those GPU stats are false. According to Tom's Guide, in 3D Mark Solar Bay Unlimited, the 17 Pro Max is only 10% faster than the s25 ultra https://www.tomsguide.com/phones/iphones/iphone-17-pro-max-review#section-iphone-17-pro-max-performance-cooling",Neutral
Intel,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,">I would be so curious about a die shot and GPU architecture of the A19 vs 8 Elite. Because Apple went from very little rt not so long ago to leading the charts. Like how many RT-Acceleration units are used in the GPU and how other components play into that.  Me too. I'd imagine a marginal increase in size over A18 Pro. The P cores have mostly stayed the same. And the E cores despite major changes are unlikely to contribute to a major increase in area (if I'm right, individually they occupy around 0.6-0.7mm2 per core). The extra cache (around 2MB) should increase area slightly. SLC area as well should contribute to that increase.  I'd imagine the GPU with the new RT units, doubled FP16 units, new tensor cores, and general uarch improvements are the major contributor to any notable area increase.  Plus I still don't feel like the approaches these companies are taking are aligned very much in terms of GPU architectures. For eg, Apple's been very focussed on improving compute performance on their GPU. Qualcomm less so.  >Definetly exciting times, I think that the major players, Qualcomm, Apple, and Mediatek, are not so far away from each other,   True that. Qualcomm has a shot at taking the ST crown lead from Apple atleast in SPECfp. But Apple's done extremely well with this upgrade. The E core jump has made them close the nT gap with Qualcomm while using much lower power.  GPU is a case where technically Qualcomm could take raw perf crown. But Apple's RT dominance, Tensor cores and general compute lead might help them in the desktop space.  >Especially interesting will be how AMD client will turn out in 2026, if they are still on the charts (we expect Zen 6 clients on N3) or if Qualcomm will top windows charts, and Apple overall all the charts.  Its either Qualcomm or Apple. AMD is too far behind Oryon and Apple's uarchs. They consume similar or even more area for their core architecture while lagging in performance while using significantly more power. The x86 ecosystem and compatibility is the only reason they'd survive Oryon.",Neutral
Intel,Having so many SoC makers on arm competing against each other by one upping each other yearly is bearing fruit vs x86. i don't know how Intel and AMD long term can fare at this rate. Arm CPUs are still showing double digit gains yearly.,Neutral
Intel,Most of apples RT gains are from optmsiing how the GPU deals with divergence.  This is not dedicated RT silicon so much as making the GPU be able to maintain much higher throughput when there is lots of divergence.  RT operations have a shit tone of divergence.,Negative
Intel,">Growing the E core into a medium core is really important and something I was worried apple might miss out on, but this means they should be able to keep up in multicore as trends point to flooding CPUs with E cores.  I wouldn't say its a medium core yet tbh. Apple's E cores are still sub 1mm2 in area usage. Compared to other ""M cores"" they are still relatively small. I imagine the A19 Pro E core is fairly larger but the A18 E core was around 0.6/0.7mm2 in size. I'd imagine its not grown a whole lot.  >They really ~~buried~~ deleted the lede by not even mentioning the E core improvements beyond the cache and undersold the GPU. I suppose this means we should expect a lot of bragging in the M5 keynote.  I'd imagine they're saving up these to mention Blender perf etc in M5 keynote.",Neutral
Intel,4 minutes ago. Damn. I should have waited. I'll post it!,Negative
Intel,"An M5 Ultra would offer 1.23 Tb/sec of bandwidth scaling from the A19 Pro.  M5 (128-bit, LPDDR5X-9600) -> 153.6 GB/s M5 Pro (256-bit, LPDDR5X-9600) -> 307.2 GB/s M5 Max (512-bit, LPDDR5X-9600) -> 614.4 GB/s M5 Ultra (1024-bit, LPDDR5X-9600) ->1228.8 GB/s",Neutral
Intel,"Haha. I mean they are actually pretty late to this tbh. Most ""E/M cores from other competitors"" are similar in size if not bigger. I'd imagine Apple's E core is stuck between a true M core like the A7xx and a true E core like A5xx in terms of area, although it probably leans toward the A7xx in that regard.",Neutral
Intel,What does 6 wide mean? What units?,Neutral
Intel,"[https://www.reddit.com/r/hardware/comments/1jcoklb/enable\_rt\_performance\_drop\_amd\_vs\_nvidia\_20202025/](https://www.reddit.com/r/hardware/comments/1jcoklb/enable_rt_performance_drop_amd_vs_nvidia_20202025/)  In gaming RDNA4 RT isn't that far behind Blackwell. Other than that raytraced rendering like in Blender AMD has been for a while far behind. It won't be until Blender 5.0 till we see any improvements to HIPRT. Though for the longest time since following HIP it's been rather mediocre and my expectations are low for next release, though their [PRs](https://projects.blender.org/blender/blender/pulls/145281) make it seem they've been doing some work. It's a low priority for AMD which is unfortunate.",Neutral
Intel,>	beating out even AMD  Was that one really surprising?,Neutral
Intel,Is that Metal vs Optix or Metal vs Cuda?,Neutral
Intel,"The video has a mostly accurate English captions option. CPU P core is up 10%, E core is up by 25%, GPU perf is up by 40% and sustained performance is up by 50%.",Neutral
Intel,> just basically a more efficient scheduler which improves utilization and less waste.  When you take a look at GPUs doing RT task you see that they tend to be very poorly utilized.  GPUs are not designed for short running diverging workloads. But RT is exactly that. So you end up with a huge amount of divergence and or lots of wave like submissions of very small batches of work (so have a large scheduling overhead).     There is a HUGE amount of perfomance left on the table for this type of task for HW vendors that are able to reduce the impact on GPU utilization that divergence has.,Negative
Intel,Maybe Apple measures occupancy differently in their tools. I wouldn't be too sure comparing these two. But I'd definitely think a combination of SER and Dynamic Caching present in A19 should result in very good utilization compared to other uarchs.,Positive
Intel,"That is very impressive, RT tends to have very poor occupancy as it is a heavily branching workload!",Positive
Intel,>Supposedly GPU size has not increased and that is impressive.   Important to note that the supporting SLC which is a major reason for improvements om the GPU side has increased from 24Mb to 32Mb. Which would increase area a bit.,Positive
Intel,">The C1 Pro has 20% IPC improvement IRRC, plus this is N3P  N3P is 5% faster than N3E. By TSMC's own claim..  Also I can't find a source for a 20% IPC improvement. ARM's claim is 16% IPC improvement. And that is not without a power cost since ARM claims that at similar performance, power reduction is only 12%.  https://newsroom.arm.com/blog/arm-c1-cpu-cluster-on-device-ai-performance  >Let's not jump to conclusions before seeing the competition   I mean I agree. But I don't see how the C1 Pro is supposed to cross a 95% P/W disparity. (4.17 points using 0.64W vs 3.57 points using 1.07W) using D9400",Neutral
Intel,"Tbf, Apple had a 32Mb SLC back in the A15 Bionic. They reduced the size of that afterward to 24Mb. Its not like the size significantly mattered in GPU performance until now.",Neutral
Intel,A 30% lead in P/W is a generations ahead in this day and age. Considering the successor (C1 Pro) is stated by ARM to reduce power by just 12% at iso performance leaving Apple with a comfortable lead for a year. Also I specifically was a bit confused by their choice to compare the A725L instead of the M variant.,Neutral
Intel,There's a guy on twitter who does the microbenchmarking for them.,Neutral
Intel,"No. I'm positive memory bandwidth offers very little in terms of performance upgrades. If you recall, the A16 was essentially the same GPU architecture as the A15 but used LPDDR5 instead of LPDDR4X, yet there were practically zero performance improvements.  I don't think anyone has investigated the A19's GPU microarchitecture thoroughly. But the main improvements seem to come from the increase in SLC size (System Level Cache which serves the GPU) from 24Mb to 32Mb and the newly improved Dynamic Caching. Its very likely there are a lot more changes responsible for that 40%+ improvement that we don't know about.",Negative
Intel,With most GPU tasks you are never ALU limited.,Neutral
Intel,">That's really not good for the A19 **Pro**, sadly a mark against Apple's usual restraint. It's a ***significant*** *perf / W* downgrade and the 19 Pro perf / W is notably worse than **ALL** recent P-cores from Apple, Qualcomm, *and* Arm  Technically we need to compare P/W at similar power or similar performance. Peak power P/W is not a very accurate measure to compare gen on gen. And I really doubt you'll be seeing 10W of ST power consumption on a phone. Geekerwan tests these things using Liquid Nitrogen. They are to test an SoC without restraints.   An A19 Pro P core does seem to have increased performance at the same power by about 5%. Only P/W at peak has reduced. Which isn't even an issue when these phones would never reach that peak in daily use.",Negative
Intel,Im having my doubts here. Howis the A19 p core much better than the A19 pro p-core? Aren’t they exactly the same p cores?,Neutral
Intel,">All of these benchmarks are sort of difficult to compare when the other chips aren't made on the same node N3P. Apple buys first dibs on the wafers so they always have that advantage, it isn't always about the architecture itself.  At the end of the day, node does not matter to consumers - only what you get for your value.",Neutral
Intel,">All of these benchmarks are sort of difficult to compare when the other chips aren't made on the same node N3P.   N3P is a mere 5% faster than N3E when comparing similar microarchitectures... This is straight from TSMC's marketing slides.  Comparitively, barring the P core (which did see an ok improvement), the E core and the GPU have seen 30%+ improvements. The node has nothing to do with it.  If Qualcomm loses to, matches or exceeds A19 Pro this year it would be because of their updated microarchitectures and have barely anything to do with minor single digit improvements offer by a sub node.",Neutral
Intel,in the end what matters is the sicion you can buy so you can compare them.,Neutral
Intel,Please tell us more about how we can never compare AMD vs Intel chips by your logic,Neutral
Intel,"Tom's Guide tested basic Solar Bay. This is the older version of the benchmark with less raytraced surfaces.  Geekerwan tested the modern, updated version of Solar Bay referred to as Solar Bay Extreme. This new benchmark has a much higher raytraced load, with far more reflective and transparent surfaces and much more detailed scene with more geometry.  Please kindly read the benchmark title mentioned in the posts. Or atleast watch the videos. Before commenting.",Neutral
Intel,Bad bot,Negative
Intel,">Its either Qualcomm or Apple. AMD is too far behind Oryon and Apple's uarchs.  Considering Zen 5 is designed to clock way higher, I don't think it's that bad really.  Also... Let's not forget about ARM. A SoC with ARM CPU cores and an Nvidia/AMD GPU could absolutely ruin Qualcomm's day regardless of how better/worse their custom CPU cores are.",Negative
Intel,"I think AMD will have a hard time to win any efficiency crowns, but historically speaking they always had the peak ST performance on process node. Of course this is not as impressive because those are desktop chips which draw 100W+, and there have been historically gaps between desktop and laptop chips in ST for AMD (even though laptop technically had the TDP to sustain ST).  I just wouldn't call the race so early, but it does seem very likely, that AMD will be behind. I just dont think it is as bad as it seems. AMD was plagued by Zen5% and still on 4nm for client, they might hit heavy with client dedicated improvements and N3, but in the end we have to see, x86 client performance really seems to struggle rn (and whatever intel is doing...).",Negative
Intel,"IMO, Microsoft needs to push hard to switch to Windows-on-ARM, or else they risk an Android-like OS for Laptops swooping in and filling the gap left by those who do not want to go the Apple route. It feels like a crucial moment for both Windows and Intel/AMD, at least in the x86 product space. It retains PC Gaming at this point, but if that nutt is cracked via even half-decent, *compatible* emulation, then ... sayonara!",Neutral
Intel,"Yeah, the E core isn't quite there yet, but it's good to know they're moving in that direction with now stubborn they are with P cores, refusing to include more than what can be run at the equivalent of all core turbo.",Neutral
Intel,"Wow that will be insane and adding the new ""Tensor"" elements added to the GPU cores will make it a formidable AI workstation.  Especially when NVIDIA monopoly is only offering small VRAM GPU cards at absurd prices.",Positive
Intel,That would be amazing. Id love to see them put some hbm there too,Positive
Intel,It’s highly unlikely they will go past a 256 bit bus. You run out of pins and layers to route a bus that wide. Gets extremely expensive. Still neat bandwidth!,Neutral
Intel,Skymont (Intel’s E core) is 8 wide (9 wide at first stage then bottlenecks to 8 I think iiuc),Neutral
Intel,"According to AnandTech, Apple's E cores were based on Apple's Swift core, back when they had a single core type  Previous die shots show Apple's E cores were close to Arm's A5xx in core area (larger, but far smaller than A7xx core only). But in terms of core+L2 Apple's E cores are similar to Arm's A7xx in core+L2 area  I'd argue it's the other way around, Apple's E cores are the true E cores  Whereas Arm's A7xx were stuck between being an M core and an E core  Now Arm has split their A7xx core to Cx Premium (M core) & Cx Pro (true E core)  Arm's A5xx/Cx Nano are a very odd core, almost no else makes a similar in-order core. Arm's A5xx/Cx Nano are more like Intel's LP E cores, instead of Apple's E cores",Neutral
Intel,The decoder,Neutral
Intel,"Amd is very far behind in rt.  You re linking gaming benchmarks, thats not rt thats mixed use.    Just look at path tracing results for a more representative comparison",Negative
Intel,"Hey, they made an effort with RDNA 4. I think that should surpass the M4 Max. I just can't find any proper scores for it.",Neutral
Intel,"Metal vs Optix.  https://youtu.be/0bZO1gbAc6Y?feature=shared  https://youtu.be/B528kGH_xww?feature=shared This is a more detailed video with individual comparisions and a lot more GPUs.  Its a lot more varied. In Lone Monk, it hangs with a desktop class 5070. In Classroom, it hangs neck to neck with a 4060Ti. In Barbershop, it falls behind a desktop 4060Ti. In scanlands, it falls behind a 4060.   If we consider Classroom as a baseline average, a theoretical 60% faster M5 Max, like the jump we saw in Solar Bay, would land hot on the heels of a desktop class 5070Ti, a 300W card. Competing with a 65W laptop GPU.  Edit; The Youtuber is using the binned 32C variant. A 40C variant would surpass the 5070ti.",Neutral
Intel,"Yeah I forgot what was the term before but I remember, it’s just like Nvidia’s Shader Execution Reordering introduced in Ada Lovelace.",Neutral
Intel,I have a query regarding RT workloads. Would offsetting RT performance to the CPU with the help of accelerators help? Or is that not the case and it would be even worser on CPUs.,Neutral
Intel,"Sure it might be different, but I doubt it. Occupancy is just threads used/total threads.  It's interesting how first gen dynamic caching + SER (apple equivalent) is hardly better than NVIDIA in terms of occupancy. Yet only 44%. So only slightly better than NVIDIA (upper end of range). Seems like first gen was more about laying the groundwork while second gen is really about pushing dynamic caching allocation granularity and efficiency. At least so it seems.  Oh for sure. That occupancy is incredibly high. \~1.5x uplift vs A18 Pro. Getting 70% occupancy in RT workload is really unheard of. Apple engineers did a fine job.  AMD might opt for this nextgen if they're serious, but it's a massive undertaking in terms of R&D, but could massively benefit PT and branch code, ideal for GPU work graphs.",Neutral
Intel,"C1 Pro is two generations ahead of the A720 in the 9400. Also, Xiaomi demostrated a much more efficient implementation of the A720 cores in their O1 chip (4.06 points at 0.82 W).  Edit: actually, it seems like the O1 uses A725 cores. Perhaps that is what they are referring to in the video as ""A720 L""",Positive
Intel,It’s been a BIG bottleneck in AMD’s apus since Vega 11 back in 2018. Doubling from 8CU to 16CU in 860m vs 890m gets you only +30%.   AMD is just so damn stringent with area despite jacking up the price on Strix point massively on an old ass node.,Negative
Intel,"You ought to do the math first. Power is the denominator. 12% reduction in power is *substantial*.  Integer: A19 Pro E-core is 3% faster at 12% less power vs claimed C1-Pro.   |Core / Node|int perf|int %|int power|int perf / W|int perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|4.17|103%|0.64W|6.51|132%| |Xiaomi A725 / N3E|4.06|100%|0.82W|4.95|100%| |\-12% power|4.06|100%|0.72W|5.64|114%|  Floating point: A19 Pro E-core is 1% faster at 4% less power vs claimed C1-Pro.  |Core / Node|fp perf|fp %|fp power|fp perf / W|fp perf / W %| |:-|:-|:-|:-|:-|:-| |A19 Pro E-core / N3P|6.15|101%|0.92 W|6.68|120%| |Xiaomi A725 / N3E|6.07|100%|1.09 W|5.57|100%| |\-12% power|6.07|100%|0.96 W|6.32|113%|  Hardly ""generations ahead"".",Neutral
Intel,Thanks for your reply.  CPU monkey reported 2GHz instead of 1.62GHz. So maybe that's where most of the gain comes from.  I suppose the tensor core like matmul units also boost performance for graphics and AI.,Positive
Intel,"I am deeply suspicious of all of these power measurements. Separating p core power usage from other aspects of the soc is difficult on iOS. Which is not a criticism of your summary, but I’d be wary of drawing anything definitive about the efficiency of the p cores.",Neutral
Intel,"It's not a knock on the design, it's how apple is configuring the CPU. It doesn't matter that performance at the same power is improved if the default clocks on the real product put it way past the sweet spot into diminishing returns so bad it regresses efficiency.   On one hand, the power inflation isn't causing problems if the 23% increased battery life per Wh is anything to go by, but on the other, what's the point of chasing peak performance like this if your boost/scheduling algorithms never allow that speed to make an impact on responsiveness?",Negative
Intel,"Geekerwan's results are ***average*** power, not peak power, IIRC. These are real, actual frequency bins that Apple has allowed.  These frequency bins *will* be hit by some workloads, but just not nearly as long as SPEC & active cooling will allow. It would be good to revisit Apple's boosting algorithms, but IIRC, they hit 100% of Apple's design frequency in normal usage.  It's not like users have a choice here; we can't say, ""Please throttle my A19 Pro to the same power draw as the A18 Pro."" Low power mode neuters much of the phone, so it's rarely used all the time.  //  I find avgerage power useful two reasons:  1. *How* performance vs power were balanced; here, performance took precedence while not keeping power stable.  2. It also shows, when nodes are not the same, where the node's improvements went. Here, an N3P core delivers notably worse perf / W versus an N3E core. TSMC claims [up to 5% to 10% less power](https://www.tomshardware.com/tech-industry/tsmcs-3nm-update-n3p-in-production-n3x-on-track) on N3P vs N3E.  I agree 10W is not common and SPEC is a severe test, but it's more the *pattern* that has emerged on Apple's fp power and whether it's worth it:  2023 - A17 Pro P-Core: 6.40W  2025 - A19 Pro P-Core: 10.07W  Apple has leaped +**57%** average fp power in two years. Seems like not a good compromise when you're eating more power per unit of performance.   That is, the A19 Pro on fp has skewed towards the flatter part of the perf / W curve.   >And I really doubt you'll be seeing 10W of ST power consumption on a phone. Geekerwan tests these things using Liquid Nitrogen. They are to test an SoC without restraints.  I agree it's rare, but why would Apple allow 10W? Were many workloads ***lacking*** fp performance that users prefer a bit less battery life for +9% fp perf vs the A18 Pro?  Of course, to most, battery life is more important, IMO, which is why core energy is most crucial, but missing here.  //  >An A19 Pro P core does seem to have increased performance at the same power by about 5%. Only P/W at peak has reduced. Which isn't even an issue when these phones would never reach that peak in daily use.  So the question becomes: do users want slightly better perf and 5% more power? On a phone, I'm of the opinion that power is paramount and should be forced to lower levels.",Neutral
Intel,">Aren’t they exactly the same p cores?  Definitely not. See the SPEC perf / GHz: A19 is nearly just the A18 Pro. Thus, this seems to be the final picture:  A19 Pro = new uarch, 32MB SLC, 12GB LPDDR5X-9600  A19 = binned OC of last year's A1**8** Pro, w/ faster LPDDR5X-8533, but smaller SLC (12MB)  New uArch clearly didn't pan out as expected in fp efficiency. A19 Pro may sit at a flatter part of the freq / power curve, A19 Pro may have more leakage, A19 Pro's [faster RAM](https://en.wikipedia.org/wiki/Apple_A19) may eat *a lot* power (Geekerwant tests mainboard power, not purely CPU power), etc.",Neutral
Intel,I'm sorry you had to take time away from ripping off others' content to correct my mistake,Neutral
Intel,">Considering Zen 5 is designed to clock way higher, I don't think it's that bad really.   I'd agree if they performed as well as their clocks suggest. Whats the point of clocking to 5.7Ghz if a mobile CPU clocked at 4.4Ghz leads you in absolute performance by 15% (Geekbench) while using a tenth of the total power.",Neutral
Intel,">I think AMD will have a hard time to win any efficiency crowns, but historically speaking they always had the peak ST performance on process node. Of course this is not as impressive because those are desktop chips which draw 100W+, and there have been historically gaps between desktop and laptop chips in ST for AMD (even though laptop technically had the TDP to sustain ST).  M2 and Zen 4 launched around the same period. The desktop chips score around 5-10% faster in Geekbench while using 20W per core power and 30-40W more for the I/O die. Taking the ST crown by a hair's width while using 5-10x more power isn't a win at all imo.",Neutral
Intel,This snapdragon era is Microsoft's third time trying arm (windows rt and surface pro x). Hopefully third times the charm,Positive
Intel,Their E-core is faster than something like Tiger lake per clock. It’s about 60-70% as fast as the best desktop CPU from 2020 and probably as fast as a U-series chip from then under sustained loads.  The real takeaway for me is that the rumored A19 MacBook is going to dominate the $600 laptop market segment if it releases before Christmas.,Positive
Intel,"Weirdly enough, I feel like them not letting the E core balloon in size has helped them in the long run. Seems they're focussed on maximizing performance within a limited area. Their E cores have had exceptional jumps in performance since the A12 almost every generation being 25%+ in improvements.  I'd wager that E core dominance is the primary reason why the iPhones are able to match Android manufacturers using super large and dense batteries in terms of endurance.",Positive
Intel,Doesn't mac studio already 500gb vs nvidia workstations of 96gbs?,Neutral
Intel,NVIDIA currently fist pumping the air over their failed ARM acquisition rn,Negative
Intel,Its a bit unlikely. Maybe for a version of the M series dedicated for a Mac Pro. But one of the main reasons they can get away with this design is because its very scalable. All the way from A series to Mx Max series. Adding HBM would probably require a dedicated SoC redesign for a very niche product segment in Macs.,Neutral
Intel,The M4 Max already uses a 512 bit bus. Does it not?,Neutral
Intel,Its technically not a true 9 wide core. I think its 3+3+3.,Neutral
Intel,E-Core is relative. Skymont is more of a C-core (area optimized) than what we typically think of as an E-core (energy optimized).,Neutral
Intel,with a viable width ISA it is better to look at the typcile throughput not the peak throuput as you very rarely are able to decode 8 instructions per clock cycle.,Neutral
Intel,9070XT: 3125.40  M4 Max: 5155.82  [https://opendata.blender.org/devices/AMD%20Radeon%20RX%209070%20XT/](https://opendata.blender.org/devices/AMD%20Radeon%20RX%209070%20XT/)  [https://opendata.blender.org/devices/Apple%20M4%20Max%20(GPU%20-%2040%20cores)/](https://opendata.blender.org/devices/Apple%20M4%20Max%20(GPU%20-%2040%20cores)/),Neutral
Intel,"With the 9070? I don’t think I’ve seen any results showing that either, however all I’ve looked at is the blender benchmark charts",Neutral
Intel,"the shader re-ordering is different. (apple also do this).  Even with shader re-ordering you have the issue that you're still issuing 1000s of very small jobs.    GPUs cant do lots of small jobs, they are designed to do the same task to 1000s of pixels all at once.     If  you instead give them 1000 tasks were each pixel does something differnt the GPU cant run that all at the same time... in addition the overhead for setup and teardown of each of these adds even more void space between them.   So apple are doing a hybrid approach, for large groups of function calls they do re-ordering (like NV) but for the functions were there is not enough work to justify a seperate dispatch they do function calling.    This is were dynamic coaching jumpstart in.      Typical when you compile your shader for the GPU the driver figures out the widest point within that shader (the point in time were it need the most FP64 units at once, and register count).  Using this it figures out how to spread the shader out over the GPU.  Eg a given shader might need at its peak 30 floating pointer registers.   But each GPU core (SM) might only have 100 registers so the driver can only run 3 copies of that shader per core/SM at any one time.   If you have a shader with lots of branching logic (like function calls to other embedded shaders) the driver typically needs to figure out the absolute max for registers and fp units etc.  (the worst permutation of function branches that could have been taken).  Often this results in a very large occupancy footprint that means only a very small number of instances of this shader can run at once on your GPU.  But in realty since most of these branches are optional when running it will never use all these resources.  The dynamic cache system apple has been building is all about enabling these reassures to be provided to shaders at runtime in a smarter way so that you can run these supper large shader blocks with high occupancy as the registers and local memory needed can be dynamically allocated to each thread depending on the branch it takes",Neutral
Intel,"While RT does have lots of branching logic (and CPUs are much better at dealign with this) you also want to shader the result when a ray intercepts, and this is stuff GPUs are rather good at (if enough rays hit that martial)   We have had CPU RT for a long time and so long as you constrain the martial space a little GPUs these days, even with all the efficiency  loss are still better at it.   (there are still high end films that opt for final render on cpu as it gives them more flexibility in the shaders they use) but for a game were it is all about fudging it GPUs are orders of magnitude faster, you just have so much more fp compute throughput on the GPU even if it is running at 20% utilization that is still way faster than any cpu.",Positive
Intel,I'm a bit confused. You think lagging 15% behind in P/W the competition for an entire year is not being a generation behind?  ARM themselves have managed only a 15% jump this year. So it will essentially be 2 years before we get an E core that matches the A19 pro. And this is just considering the Xiomi's SoC. Mediatek's and Qualcomm's which dominate the majority of the market lag even further behind.,Negative
Intel,Any chance you can include area efficiency?,Neutral
Intel,I'd advise against using CPUmonkey as a reliable source. They're known to make up numbers. (Reported M1 Pro/Max Cinebench scores 6 months before they launched based on predictions),Neutral
Intel,"I agree to some degree; Geekerwan notes they are testing mainboard power, not core power (if you Google Translate their legend).  For me, I assume all the major power draws on the motherboard *are* contributing to the overall SPEC performance, too.   If the faster LPDDR5X-9600 in the A19 Pro eats more power, it's fair to include that power because ***all*** A19 Pros will ship with LPDDR5X-9600. That was Apple's choice to upgrade to this faster memory.    Now, you're very right: we can't purely blame the uArch at Apple. It may well be the DRAM or the boost algorithms (like we saw at the A18 Pro launch last year) and—at Apple specifically—even the marketing overlords.  It's also why I'm a big proponent of JEDEC speeds & timings & volts in desktop CPU tests, much to the chagrin of a few commenters.",Neutral
Intel,Why is separating p-core power usage from SOC power uniquely difficult on iOS?,Negative
Intel,I understand your reasonings. But its the only semblance of comparison we have to date between different SoC. I've learned not to look a gift horse in the mouth.,Neutral
Intel,">Apple has leaped +**57%** average fp power in two years. Seems like not a good compromise when you're eating more power per unit of performance.   >That is, the A19 Pro on fp has skewed towards the flatter part of the perf / W curve.   I mean this has been a trend long before the A17. Apple has been increasing peak power little by little since the A12 Bionic.  I remember reading Anandtech articles about it.",Neutral
Intel,Don't be a sour puss now because you didn't check your sources before commenting. Mistakes happen.  >ripping off others' content   Eh? Are you stupid? You're pissed that someone posted a hardware review ON A hardware sub? The entire sub exists to discuss hardware bozo.,Negative
Intel,"I originally saw this on Phoronix' forums, but I can't find the link to the comment so I'll send this one instead: [https://blog.hjc.im/wp-content/uploads/2025/05/SPECint2017.png](https://blog.hjc.im/wp-content/uploads/2025/05/SPECint2017.png)  Zen 5 is now behind, yes, but it isn't really that bad.",Neutral
Intel,"While the efficiency for this is BAD, I dont think its 20W per core.  When we look at results by [Phoronix](https://www.phoronix.com/review/amd-zen4-zen4c-scaling/2) we can see \~7-8W per core for this (not great numbers, because its a weird chip and different node), which is still very bad. AMD certainly has some power issues, but many of which, i.e. inefficient I/O dies, are not really dependent on the CPU uArch and could switch at any moment. They certainly have much more inefficient chips at the moment than both Apple and Qualcomm. For Zen 6 we expect a major update to the desktop chiplet architectur which could bring some much needed improvements in terms of I/O though.  They have reasonably fast cores, and I think they are not in a terrible position, even though it is far from good. I think what is interesting for AMD to look out for is that they keep moving fast, instead of intel who didnt move fast since like 14nm, and AMD has strong cores. Additionally AMD has (including from the datacenter) an enterprise need to make the CPUs more efficient.  So yeah, very bad CPUs efficiency wise. A bit behind, but not terribly on perf per node wise, efficiency on the desktop is an afterthought for AMD, clearly, but they are moving constantly and are improving. It might be AMD Laptop Zen 6 has again like 35W TDP, for 3000 Geekbench and be dead, but with some client oriented tweaks I see chances (maybe just from the patterns in the tea leaves in my mug)",Negative
Intel,"Per clock, the A19 Pro E core is competitive with Golden Cove. Atleast in SPECint.  M4 E core-> 3.53 points at 2.88Ghz (1.23 p/Ghz) or IPC i9 14900k-> 9.93 points at 6.00Ghz (1.65 p/Ghz) or IPC  A19 Pro has a 22% jump in IPC in SPECint (might be some variations due to time difference and lack of knowledge about the subtests ran, but still gives a good picture)  22% IPC jump over M4/A18 E core = 122% of 1.23 = 1.50 A19 P core = 1.50 p/Ghz  Golden Cove in i9 14900K has a mere 10% lead over A19 Pro in perf/clock in SPECint.  https://youtu.be/EbDPvcbilCs?feature=shared  Source: I9 14900K compared with M4 in this review.",Neutral
Intel,It does but the Macs are limited in other ways (memory speed among other things),Neutral
Intel,It wasn't for a lack of trying.,Negative
Intel,Yeah there was some rumors of a server version and thy have a patent for a multi level cache setup but they also patent and prototype plenty of things that never get released.   https://www.techpowerup.com/277760/apple-patents-multi-level-hybrid-memory-subsystem,Neutral
Intel,"Oh huh, it does but it’s 128 bit per channel. So memory on 4 sides of the die. Wild, don’t see that normally except in FPGA for data logging (or GPUs)",Neutral
Intel,"The difference seems a bit drastic in open data benchmarks.   https://youtu.be/B528kGH_xww?feature=shared  Testing individual scenes, the 9070xt and M4 Max seem neck and neck.  The M4 Max at best (in Lone Monk) is 5070 desktop class and at worst (in Scanlands) is 4060 desktop class. On average, I'd say in Blender, it is neck and neck with an RTX 4060Ti desktop card. I think a theoretical M5 Max should be on par with a 5070Ti if we see the same 60% bump in RT performance.",Neutral
Intel,Apparently Cinebench 2024 GPU is not compatible with RDNA4 cards lol. So I can't find any scores to compare.,Negative
Intel,"So does dynamic caching ensure that the total size will ""always"" be the same as whats being called? As in certain cases it is still possible that there can be wastage like for the example you said ""Eg a given shader might need at its peak 30 floating pointer registers. But each GPU core (SM) might only have 100 registers so the driver can only run 3 copies of that shader per core/SM at any one time."" on that, there would be 10 registers wasted doing nothing, if it cant find any else thats <10 registers to fit in that.",Neutral
Intel,"Well when you put it like that (13-14% behind in efficiency) saying ""generations behind"" certainly sounds misleading.  All it would take for another arm vendor to beat that is jumping one node(let) earlier than Apple, which is certainly doable given the lower volumes, although traditionally it rarely happens. Or jumping earlier to anything that might give them an advantage really, e.g. lpddr6.",Negative
Intel,"It's not ""generation**s**"" behind as you originally wrote. It's being compared to cores from **a year ago** already, mate.  I fully expect MediaTek to adopt C1-Pro and Qualcomm for sure will also update.  Apple's E-cores are simply nowhere near as dominant as they used to be in the A55 era.  EDIT: before we speculate more using marketing for Arm and hard results for Apple, let's check back in a few months to see how C1-Pro  *actually* performs and how Qualcomm's smaller cores actually perform.",Neutral
Intel,Its a bit harder since we don't have any A19 Pro die shots yet. But Apple's E cores have always been sub 1mm2 compared to A7xx cores.,Neutral
Intel,Yeah. All fair points. I don’t disagree. It’s still fun to speculate!,Negative
Intel,"Because we don’t have the tools. On macOS Apple provides powermetrics but Apple states that the figures can’t necessarily be relied on. On some very specific tests you can narrow down power to a cpu core, kinda. Spec tests often stress other aspects like memory, so I would use the provided figures as a guide. Either as a “p core bad” or “p core good” conclusion.",Neutral
Intel,"Oh for sure. It’s not a criticism of either Geekerwan or yourself. They are doing a great job with the available options and I appreciate your summary. I just find it a little amusing when people dissect milliwatt differences as absolutely accurate. We just don’t have the tools, and people are keen to jump on the “p core doomed” bandwagon.",Positive
Intel,"The power increaseas are definitively (and definitely) accelerating, though, worse than it used to be.  [**Geekerwan P-core power**](https://youtu.be/iSCTlB1dhO0?t=422) **| SPECint2017 floating point**  *% is from the previous generation*  A14: 5.54W  A15: 5.54W (+0% power YoY)  A16: 6.06W (+9% power YoY)  A17 Pro: 6.74W (+11% power YoY)  A18 Pro: 8.18 W (+21% power YoY)  A19 Pro: 10.07 W (+23% power YoY)  //  AnandTech did the *great* work of measuring energy consumption / joules. That really proved that race to idle was working; more instanteous power under load, less overall energy   [**AnandTech P-core Power | SPECint2017 floating point**](https://web.archive.org/web/20240907062349/https://www.anandtech.com/Show/Index/16983?cPage=15&all=False&sort=0&page=2&slug=the-apple-a15-soc-performance-review-faster-more-efficient)  A14: 4.72W, 6,753 joules  A15: 4.77W, 6,043 joules (+1% power YoY, -11% energy YoY)  Average power went up, but energy consumption went down.",Negative
Intel,You copied and pasted someone else's data and now you're acting like a hero,Neutral
Intel,"The 9950x lags by 8% behind the M4. M5 is another 10% on top of this lead. M series chips use around 8W of power in total to achieve this perf including memory and what not. 9950x is like 20W per core, another 40W for the I/O and another unknown amount for mempry.",Neutral
Intel,"Still very bad used to work in their IO team, and the idle eatt performance is still very bad. Their idle core loading is also not as good as I would expect for their next generation, and its sad to see the future generation lose to even on M3 on benchmarks. The LP core was supposed to help in this situation, but I just can't see past the fact on why the gap is actually widening over time. I hope Intel can come in and close this gap.",Negative
Intel,">While the efficiency for this is BAD, I dont think its 20W per core.  >When we look at results by [Phoronix](https://www.phoronix.com/review/amd-zen4-zen4c-scaling/2) we can see \~7-8W per core for this (not great numbers, because its a weird chip and different node), which is still very bad.  The Phoronix link shows max 31w for 1x Zen4 P core. So it's way worse than 20w.  We're talking ST power consumption here. Ryzen chips may use 7-8w per core in MT workloads but in ST, they definitely boost well over 20w for the ST core.",Negative
Intel,I was comparing overall score.  Geekerwan got 4.17 at 2.58GHz which is ~1.62pt/GHz which is a little higher than your speculation and a mere 1.8% IPC difference from that Golden Cove number.  The fact that it did this at 0.64w in such a tiny core is absolutely incredible. It once again raises the question about how much ISA matters. I don't see any x86 E/C-cores getting anywhere close to those numbers.  https://youtu.be/Y9SwluJ9qPI?t=260,Positive
Intel,one will think the massive vram capacity just override the disadvantages.,Neutral
Intel,">[https://youtu.be/B528kGH\_xww?feature=shared](https://youtu.be/B528kGH_xww?feature=shared)  >Testing individual scenes, the 9070xt and M4 Max seem neck and neck.  This Youtuber is using 14/32 core binned M4 Max. The Blender link is using the full 16/40 core M4 Max.  Also, it's kind of hard to see what is neck and neck when the Youtuber only tested the M4 Max and not the 9070XT.",Neutral
Intel,"dynamic caching would let more copies of the shader run given that is knows the chances that every copy hits that point were it needs 30 registers is very low.   If that happens then one of those threads is then stalled but the other thing it can do is dynamicly at runtime convert cache, and thread local memroy to registers and vice versa.   So what will happen first is some data will be evicted from cache and those bits will be used as registers.   maybe that shader has a typical width of just 5 registers and only in some strange edge case goes all the way up to 30.   With a width of 5 it can run 20 copies on a GPU core that has a peak 100 registers.",Neutral
Intel,"Historically, the core only area for Apple's E cores have always been between Arm's A5xx & A7xx cores, but closer to Arm's A5xx  But for core+L2 (using sL2/4), Apple's E cores have always been similar to A7xx cores in mid config",Neutral
Intel,"I agree that the presence of inaccuracies is very likely. And I certainly don't think the P core is doomed for a 10% jump in what is essentially a a very minor node upgrade.  But considering the video does go into the P core's architecture where the only substantial changes were the size of the Reorder Buffer and a marginally better branch predictor, the performance numbers make sense.",Neutral
Intel,This is a combination of the meaningless smartphone benchmark game (95% of users would be perfectly fine with 6 of Apple's latest E-cores) and the need to have a powerful chip in laptops/desktops all sharing the same core design.,Neutral
Intel,"I'm confused. And legitimately concerned about your mental faculties.  Literally in your previous comment, you posted Tom's Guide data, data thats not ""yours"" to try and discredit my post. And checking your post history, you also posted a Mrwhosetheboss video to discuss battery life comparison in an another subReddit.   So are you a hypocrite since you're ""stealing"" data as well? I'm not stealing anyone's data. I'm correcting your stupidly incorrect conclusion with a source to back it up. Just like you attempted to lol.",Negative
Intel,"Comparing the 9950X to the M4/M5 is a bit of a stretch... I'm not saying AMD is as good, but if they did a ""Lunar Lake"" with Zen 5C + on package memory they wouldn't really be that far off.  I want x86 to belong in the museum too, but sadly the ISA doesn't really matter (that much) and AMD isn't exactly incompetent... EPYC CPUs are still dominant and this is what they're truly targeting...",Neutral
Intel,"You see 7-8W increase going from 1 Core to 2 Core, which indicates that AMD has a huge base overhead but the core doesn't use 20W.   So its a problem with the SoC design not with the CPU design that it uses 20W for ST.  Its more like running a ST workload draws 20W SoC + 8W CPU stuff. So if they had better SoC design they could substantially boost efficiency even with the same cores.",Neutral
Intel,"Oh I was referring to this statement in your previous comment.  >Their E-core is faster than something like Tiger lake per clock.  By ""per clock"" I assumed you mentioned IPC.",Neutral
Intel,"I wouldn't recommend comparing IPC from two different runs of SPEC2017. There could be numerous different changes or subtests we don't know about. For eg, compare IPC numbers here vs the test used in A18 review. There are some notable differences in scores and IPC.",Neutral
Intel,Oh it's a huge benefit (I have a M2 Ultra at work) but we still use Nvidia. The cuda ecosystem is far more mature and widely supported with better support for embedded and datacenter scale compute.,Positive
Intel,"I see, so dynamic caching can make it so a shader doesnt have to be 30 registers wide if it doesnt have to do 30 often so it doesnt have to reserve that much space and waste it(such as in conventional cases, if its 5 registers and 30 peak, it will still reserve 30 registers despite it being at 5, which then would waste 25 doing nothing)  Also SER happens first right?",Neutral
Intel,I don’t disagree. The performance figures seem good. The power figures may or may not be. I’m just nitpicking.  Edit: just noticed that they show the A19 having 99% P-core FP performance path 11% less power. That is weird and get’s to my point about power measurement strangeness.,Neutral
Intel,"The needlessly high clocks, agreed: Apple could've still improved perf with lower clocks.  >the need to have a powerful chip in laptops/desktops all sharing the same core design.  They previously kept this in check on the A16 and even A17 Pro, both sharing the same core designs as the M2 and M3 respectively. That doesn't seem too related, as every uArch should scale a few hundred MHz either down or up.",Neutral
Intel,I posted links to reviews. I didn't copy the entire data for my own post,Neutral
Intel,"ISA matters much as well, or mostly the implementation of it. X86 and AMD dont go well together, and AMD is the very definition of incompetency. Both deserve to be sunseted by now. The fact that there is no proper ARM support on consumer platform is the only reason why X86 on consumer still exists. For servers, an ARM server is more power efficient, and only those really legacy stuff requires X86. Companies would really appreciate the cost savings and the ARM ecosystem more than the clusterfk of X86.",Negative
Intel,It doesn't matter how much power 2 cores use. The important thing is that it used 31w for powering a single P core.  We are talking about ST performance and wattage. Multicore performance and wattage is entirely different.,Neutral
Intel,"Yeah, I guess that wasn't clear. What I meant is that the IPC of the E-core is so much higher than Tiger Lake that it's performance at 2.6GHz is surprisingly close to desktop Tiger Lake at 5GHz.  I know it's not fair to compare with Intel 10nm (later Intel 7) with TSMC N3P as it's 2 major nodes apart, but this goes way beyond that because these chips are using only around 2.5w for just the E-cores. TSMC claimed 25-30% power savings from N7 to N5 and another 30-35% from N5 to N3.  Using these numbers, we get these 4 E-cores using somewhere around 6-7w PEAK, but this is all-core turbo. Intel's CPU at it's most efficient 12w configuration won't hit it's 4.4GHz turbo required to win in single-core performance and is going to hit closer to its 1.2GHz base clock in all-core sustained performance at which point the E-cores not only have their giant IPC advantage, but double the clockspeed too.  All this again to make the point that a macbook with this will be blazing fast for consumer workloads and might finally be the laptop to usher in multi-day battery life.",Neutral
Intel,"This is true, but there aren't a ton of alternatives short of taking a ton of time to compile/run everything myself. I don't think it's worth it for a hypothetical IPC comparison that it sees nobody else would even be that interested in seeing.",Negative
Intel,"Reordering of shaders has a cost, if for a given martial you just hit 10 rays you will not want to dispatch that shader with just 10 instances as the cost of dispatch and scdulaing will be higher than just inlining the evaluation, so you will merge together the low frequency hits into a single wave were you then use branching/fuction point calls.    You will also use this mixed martial uber shader to use up all the dregs that do not fit within a SMD group.      Eg you might have 104 rays hit a martial but that martial shader can only fit 96 threads into a SIMD group so has 8 remaining thread, you don't want to just dispatch these on there own as that will have very poor occupancy (with 88 threads worth of compute ideal) so you instead inline them within a uber shader along with a load of other overflow.",Neutral
Intel,"Are you perhaps blind? The youtube link to the review is at the top of the post? What is wrong with you friend? Feeling a bit under the weather?  The video is in Chinese and before I updated the post with a youtube link, the previous source was from Bilbili, a platform that doesn't even work in most countries.   So I summarised the important points in it for people who didn't understand Chinese and couldn't infer anything from the graphs. No one's gonna ignore the link that is at the top of the post and read my summary before seeing that a link is available.   You're fighting imaginary demons here. Go to your samsung subreddit and whine about Geekbench or something.",Negative
Intel,"But we were discussing CPU uArches, and on the question of ""Does AMDs single core suck up 31W"" the answer is ""NO"", the SoC power is 31W, and that is an issue that can be solved independently of the CPU uArch.  This just means that the massive inefficiencies are not inherint to the CPU uArch, but are a Problem of SoC design.  7W on a single core isnt super efficient, but its far lower than 20-31W. The question is if AMD is able to strip away a large part of those ~20W overhead, but that is not contingent on questions about CPU uArches.  And thats the point of the 1 Core vs 2 Core comparison. To demonstrate that it isnt the CPU that sucks up 20W, but the SoC, which can be solved much differently.",Negative
Intel,Enjoy your scratch-prone iPhone!,Neutral
Intel,"Same price as B580 with lower performance, 4GB less vram and 128 bit bus.  A round of applause for Nvidia",Neutral
Intel,"GB207 being slower than AD107 is pathetic, what's the point of these x07 dies again? They're not thst mich smaller than recent x06 dies.  They're spaffing design cost on these barely different dues.",Negative
Intel,"Now that we have a third-party review, it pretty much confirms what Inno3D said the other day, it's definitively slower than the RTX 4060 by about 5-7%.",Neutral
Intel,The RTX 5050 is 2.5% slower than the Arc B580.   It's also a 50 series card that costs $250.,Negative
Intel,> the system used a Ryzen 7 9800X3D  If the B580 only wins by 2.6% with this CPU then it's going to lose when you use something weaker because of that CPU overhead problem with Intel.,Negative
Intel,It's actually surprising that it's that close to a 4060 considering the 5050 only has 2 GPCs as opposed to the 4060's 3,Neutral
Intel,IDK but that performance is actually not bad. It should've just been cheaper.,Negative
Intel,The only positive about this is they're likely going to make a 5040 with a cut down chip that should pretty easily fit <75W.,Positive
Intel,"Honestly for people who don't need a lot of GPU power, not the worse.   I have two work computers, one with a 4090 and an old one with a 3090Ti.  These GPUs sit idle just taking up space, would have made more sense to get the something less performant.",Negative
Intel,272mm\^2 for B580 vs \~150 or less for RTX 5050 and perf within 5% lmao,Neutral
Intel,"I think to lose in sales, it would have had to lose a little more convincingly.  Yes, it's worse. But not enough to make up for features and brand-loyalty / nvidias massive reputation with gamers who are not into tech.  This thing will sell. It will sell A LOT. Because Average-Kevin who just wants to play Fortnite and some League will have a great time with it. His YT videos will get upscaling, his shitty mic-quality will get fixed (mostly) by Nvidia Broadcast... and he DGAF why we think it's the wrong move.",Negative
Intel,"Does it actually fall behind the b580 or is it due to vram bottleneck in certain instances?  Either way, this should open up people’s eyes about the b580. The b580 is just marginally above the 5050, which is a terrible product, with 4gb more vram… even at its 250 dollar price point, it’s never been a good deal. Not to mention how it’s going for 400 these days.",Negative
Intel,"People who own 9800X3D's aren't buying B580's or RTX 5050's, i'd wait for more realistic reviews using CPU's people actually own.  Edit: r/hardware is dumb CPU overhead is a thing and budget GPU buyers need to know which of these two cards is effected the most by it. Everyone already knows that you are going to be GPU limited on top tier CPU's its not valuable information for a budget card.",Negative
Intel,Looks like people already forgot that testing the B580 with a top end CPU gives unrealistic results for actual budget buyers.  Every trick in the book for a nvidia sucks article.,Negative
Intel,Don’t make Jensen sad,Negative
Intel,"Hello KARMAAACS! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
Intel,Will it also fall behind them in the Steam hardware survey?,Neutral
Intel,the 3060 is still a better value than this card bc its about same speed but 12gb vram and higher bus,Positive
Intel,"Maybe a product for non-gamers who want/need multiple display setup, like me. Even then, if you bother to spend that amount of money for 5050, better add some more to get 5060, and if bother to get 5060 just better get 16GB one. Furthermore, you may feel why not add bit more to get 5070 series rather than 5060 16GB. That's how these things are priced.",Neutral
Intel,Value engineering to extract as much wealth as possible while unable to match performance conditions.,Neutral
Intel,When you gimp the memory bus of course it is not as good,Negative
Intel,"No 75 watt gpu, pin connectior, yeah this is death of arrival.",Negative
Intel,"Trying to convince people to buy a 8gb card in 2025 should be illegal. Yet, people will line up to buy it because of the Nvidia logo...",Negative
Intel,"It's a Nvidia Rx 7600, actually good recommendation over that card if it released for the same price in India. Rx 7600 is awful compared to rtx 5050 in pretty much everything.",Positive
Intel,Humiliation for the RTX 5050 is well deserved. It's good this gets curb stomped by the Intel B580.  More market share for Intel because AMD also introduced a useless 8 GB card.,Positive
Intel,">128 bit bus  Seems like this isn't the main issue with 5050's performance. 4060 has lowest memory bandwidth out of all 3, yet it's the fastest card. The die is just too cut down.",Negative
Intel,Unfortunately it will still sell like hotcakes in pre-builts to people who don't know much about computers.,Negative
Intel,"Their brand name is so strong that despite being worse in basically every way, including having poor drivers which was one of their major selling points in the past, they will still sell super well",Positive
Intel,"I just looked at PCPartPicker and the cheapest B580 on there, in the US, is $299 (and the brand is Onix). The Intel Limited Edition is $340. If you can get the 5050 at MSRP it would have a decent price advantage.  I mean, I wouldn't get a 5050 but in the US at least, the $249 price for the B580 isn't real.",Neutral
Intel,"Where I live none of the main sellers stock the B580, a round of applause for Intel.  The B580 isn't faster on the CPU's owned by the people who are in the market of a budget GPU.  People who own 9800X3D's aren't buying B580's or RTX 5050's, i'd wait for more realistic reviews using CPU's people actually own.",Negative
Intel,Intel apparently hasn't shipped any GPUs last quarter. The shortage is already been seen as prices of b580 are going up.,Negative
Intel,They're really pushing what they can get away with these days. Less VRAM and narrower bus but want the same money... bold strategy.,Neutral
Intel,"> with lower performance,  even on lower end cpu? Holy crap nvidia wtf",Negative
Intel,Because it will sell a shit load. Probably will be one of the highest selling Nvidia GPUs this generation,Negative
Intel,"At these small die sizes, yields can be really high. High enough that if they wanted to have supply for a 5050, they might not have enough GB206 dies that can be cut down to 20/36 SMs.  The 5060M is probably catching the bulk of the bottom-level GB206 dies at 26/36.  There's also the memory difference, with the 5050 having gddr6 while the rest of the lineup uses gddr7. With dies this small, that cost difference in memory could actually make it worth taping out a new chip with controllers for the cheaper stuff. Gddr6 is dirt cheap at this point.  You could absolutely then argue that there's no reason for the 5050 to be so cut down that it needs a new, even smaller die, and that Nvidia can eat some margins on a budget card to give it better memory. And I'd agree. But I'm sadly not anybody who can make that decision.  Personally, I can't wait to see what becomes of the cut-down GB207s. The 5050 is a ""golden"" fully-enabled die. There will be some with only 18, or 16, or even fewer working SMs. Hell it could lose an entire GPC and be down to half-working. Those will be some absolutely sad little GPUs.",Neutral
Intel,> It's also a 50 series card that costs $250.  I'll believe that when I see it in stock for 250. MSRP is meaningless if the product isn't available for that price.,Neutral
Intel,And will be even worse since it's vram is 8gb. In many many games the b580 would Perform even better,Negative
Intel,Yeah it would be interesting to test with a more real-world CPU pairing.,Positive
Intel,"Yeah, that was my question, did that ever get fixed? It seems like nobody remembers that anymore",Negative
Intel,"TBH, they should have made THIS card a 75W SKU. It probably easily can be, without losing much performance. That would make it a very viable upgrade path for low-end systems. And kind of make the price more attractive, because it would require zero additional changes.",Positive
Intel,Margins go crazy,Neutral
Intel,4x the performance with MFG though.,Neutral
Intel,In mkst cases comparison would make sense since you testing gpu's not anything else. But when you toss in b580 complaints about using too powerfull cpu make much more sense,Neutral
Intel,I have a system with a 9950x3d and a 1060. I’m thinking of getting a 5050 to go with it.,Neutral
Intel,> Every trick in the book for a nvidia sucks article.  That's just a standardised testing suite.   Someone will have low-end CPU testing once this gets any distribution.,Negative
Intel,They're using the same cpu for the 5050 too. What a weird comment to make.,Negative
Intel,"Don’t forget how everyone is shitting on the 5050, calling it the worst nvidia card ever, but were praising intel non stop for the b580.  Even using purely stats from the article, b580 is 3% better with 4gb more VRAM than “worst gpu ever”, so I don’t see how this translates to a win for the b580. Being better than a terrible product by a hair doesn’t exactly make it good.",Negative
Intel,Same can be said for the 5050 no? Both have CPU overheads unlike AMD which any mid-range AMD GPU can easily outperform Nvidia's 5090 at 1080p.   The 12GB VRAM however are better suited for people who are stuck at older Gen3/Gen4 PCIE speeds.,Neutral
Intel,"It'll run nearly every game in existence at 1080p with playable framerates with the right settings. IDK why it would be a product for non-gamers. Calling it that means the B580, 4060 and 7600 are products for non-gamers, too.  Only game I could think of it wouldn't run properly would be oblivion remastered, which has terrible performance on anything.",Negative
Intel,3050 released with the exact same specs and msrp. The 70w variant only came out 2 years after. The first variant was in top 10 of steam's hardware chart.,Neutral
Intel,"This is not a card for 1440p, as it would have issues with getting even 60 fps there, and in 1080p 8gb is mostly enough.  There is a difference between choking on VRAM with 80 base fps (5060ti) and 50 base fps (5050). Yes, second case is still annoying, but you might want to dial down the settings anyway, and then you would be fine.",Negative
Intel,Nvidia RX7600? That a new GPU? 😂,Neutral
Intel,"Funnily enough the 8gb 9060 XT is the best brand-new alternative at $300 and below, hands-down, and that includes the B580. It's x16 interface means it's less restricted by the vram compared to the 5060/Ti 8gb. The B580 still has overhead issues, VR issues, game issues and is rarely available at MSRP.  But people trash it anyway and praise the B580 without actually looking at its performance. In the benchmark linked by the OP alone it loses to the 4060 lol",Positive
Intel,"Performance is fine I'm sure it'll perform just as good as the 3060 maybe slightly if you lower the power to 75W, call it a 5030 and sell it for $150. But these are different times and Nvidia doesn't do reasonable things like that anymore.  Bottom line, it's just the price and TDP that sucks.",Neutral
Intel,TBF a pre-built you're just looking at:   * What's the price?  * Does it have a graphics card or not?  * Overall package/size,Neutral
Intel,"or even people who have a baseline knowledge, but think ""dlss and framegen"" will make this abomination better than its competition",Negative
Intel,"It's not so much the brand name but their near monopoly in the prebuilt market. There will be 1 prebuilt with a 9060XT 8 or 16GB for every 20 prebuilts with a 5050/5060. It's so bad that even the prebuilts with the 16GB 9060XT will be outsold by prebuilts with a 5050 at the same price range, as companies like to pair trash GPUs with ""high"" end CPUs like a 13700F to clear stock but the ""i7"" allows them to mark up the price.",Negative
Intel,"It was in stock at newegg monday for 3+ hours at $249.99, I picked one up after seeing a post that was 3 hours old on /r/buildapcsales   I've stopped into microcenter a couple of times and they said they get them, but I just haven't been lucky enough to be there when they were restocked.  EDIT: it is back in stock as of 11:40 EST at Newegg",Neutral
Intel,"In Germany you can find Arc B580 for €269-283 low-end, and B570 for €218-235.",Neutral
Intel,Her in Canada they are $360 CAD. Cheap cheap. And ways to find. Might be a issue in your country perhaps,Neutral
Intel,"Went on Shopee and the lowest I could find is $280, which is not bad considering there's usually taxes which increases prices of GPUs by 10-20% from global USD MSRP.",Neutral
Intel,">the $249 price for the B580 isn't real.  Yes it is, it just gets sold out as soon as it's restocked - https://www.nowinstock.net/computers/videocards/intel/arcb580/",Neutral
Intel,Doesn't it just need a 7600?,Neutral
Intel,Not in Canada. Prices are exactly as launch. Could be your countries problem,Negative
Intel,"Eh, they recently shipped a new batch at MSRP to Newegg.  Don’t know what’s going on with the partner cards. Probably the typical shenanigans",Negative
Intel,You forgot about the smaller dies.,Negative
Intel,"The chip is tiny, so a single wafer -> more chips. The demands on power delivery, cooling etc. are nothing to speak of. And the GDDR6 memory is cheap and readily available.  If anything will ever be in stock, it's this card.",Positive
Intel,5060 has been in stock at MSRP everyday since launch. This should be the same. Even the 5060ti 8gb and 5070 are at MSRP.,Neutral
Intel,MSRP has always been meaningless because it stand for suggested retail price.,Neutral
Intel,"But if the recent LTT video is halfway accurate, AMD would murder both of them in that scenario. The 9060 XT 8GB might want to have a word with those two cards.",Neutral
Intel,negative margins for intel,Neutral
Intel,The B580 showed we need to test in the systems the cards will end up being used in. The restricted PCIe lines on the lower tier cards i.e. only use 8 also means they need to be tested on older systems.,Neutral
Intel,You really believe you are an average 9950x3D owner? Really?,Neutral
Intel,"Out of curiosity, what is your use case in such a setup? I find myself cpu bottlenecked quite often on a 7800x3D but i use a 4070S, when i used 1070 GPU was bottlenecking me.",Neutral
Intel,"But Intel Arc drivers have a really bad overhead, while nvidia does not, so in gpu bound games the 5050 will have around the same performance but the b580 will lose a lot of it, so it's not a weird comment, it's a very important point.",Negative
Intel,>in 1080p 8gb is mostly enough.  The narrative was pushed so much that this fact became an unpopular opinion.,Negative
Intel,nvidia Rx 7600 equivalent or Rx 7600 alternative by nvidia. Ig people can't even get a silly joke nowadays,Negative
Intel,The 12 GB of RAM in the B580 will age more gracefully for a truly low end gamers. On purpose of 8 gb is retro gaming.,Neutral
Intel,"Hate to be that guy but to call this a 30-class card is insane. The 1030 was 25% the performance of the 1060, and the 1630 was around 45% the performance of the 1660. This card is 80% the performance of a 5060, which should probably be called the 5050.  To take it a step further, the GT 1030 was 13% of the 1080ti, the 5050 is 20% of the 5090. The 1050 was 25% of the 1080ti, so it would be reasonable to call this a 5040 or I could even be satisfied with 5040ti.",Neutral
Intel,That would be an interesting card,Positive
Intel,I think a big thing about prebuilts are the aesthetics. Does it look good? is the cables managed? Is the RGB controllable?  Slapping a PC together is easy but building something that looks beautiful through the side panel is not as easy.,Neutral
Intel,The 4060 is 2% and DLSS did plenty of work for the even slower 3060. DLSS is going to be great for this card. Frame gen will be good in games this card can run at good base framerates.,Positive
Intel,"cool, it's in stock now",Positive
Intel,A 2-year old CPU ushering a new platform at the time of B580's launch. Far from a budget user's upgrade timeline-wise.,Neutral
Intel,"For a brand-new truly budget build, AM5 is still restrictive pricewise to people eyeing GPUs of this level. The 3050 and 6600 were well-known inclusions in $500 builds, building with a 7500F is going to cost $280-300 just for the cpu, mb and ram alone. AM4 or LGA1700 otoh can be as low as $180-200.",Neutral
Intel,Depends on the game  The 7600 can see some serious performance deficits in some games like Spider Man  https://youtu.be/00GmwHIJuJY&t=521  and Hogwarts' minimum FPS is affected  https://youtu.be/00GmwHIJuJY&t=468,Neutral
Intel,I mean tariffs in the US are a thing...,Neutral
Intel,"It wasn't always meaningless. Overall deviation from MSRP used to be smaller, and it wasn't hard to find the basic models at MSRP once upon a time.",Neutral
Intel,more like we need to test with different processors. Like with high-end to see absolute maximum and some lower grade to see how good/bad driver overhead is,Neutral
Intel,I never said I was,Neutral
Intel,"I'm looking into this now, and you are correct. I am just reading this for the first time.",Neutral
Intel,Lol no. For truly low-end gamers the B580 will perform worse because of its CPU overhead that still hasn't been fixed more than half a year after release.,Negative
Intel,"I mean not really. The b580 is low end enough to the point where it'll become obsolete in performance long before it runs out of VRAM.  It's the equivalent of saying buying a 16gb 7600xt in 2025 is ""more futureproof"" than buying a 12gb 3060 in 2025.",Negative
Intel,an 8 GB card is obsolete at day one for modern games.,Negative
Intel,"It's not about the performance vs XX60. It's about the CUDA core count and bandwith vs biggest die in the lineup. You can find those numbers on Gamer's nexus latest shrinkflation video on this topic (https://www.youtube.com/watch?v=caU0RG0mNHg). Considering those things, the 5060 is essentially a 5050 and the 5050 is a 5030.  The GT 730 had 13.3% as many cores as the full GK110 die, with a 50W TDP.  The GT 1030 had around 10.3% as many cores as the full GP102 die, with a 30W TDP though this one was impressive.  The GXT 1630 had around 11% as many cores as full TU102 die, with exactly 75W TDP.  The RTX 5050 has 10.7% as many cores as the full GB202 die (RTX PRO 6000 Blackwell), and around 11.7% vs the 5090 (which is the number you'll se on Gamer's Nexus video, he rounds it up to 12%, I think that was a mistake on his part? I'm not gonna try and correct him though), but it has a TDP of 130W somehow, just 15W less than the 5060, that's absurd.  Hence why I'm saying it SHOULD be a sub 75W TDP card, and priced way below that. My theory is that it was OC'ed past its efficiency curve to make it at least moderately better than the 3060 and still be able to call it a XX50 card.",Neutral
Intel,Nah most people who buy prebuilts just want a good working PC with minimum effort,Neutral
Intel,Yep. That's a problem for that country. Countries with normal leadership have much more viable options,Negative
Intel,Price is only driven by demand and supply. See how the crazy expensive GPU still sell like a hot cake during the COVID lockdown period?,Neutral
Intel,It's well known by Arc owners.,Neutral
Intel,"The 16gb 7600xt  is a giant pile of crap even with 16 GB , so a 12 GB 3060 is the better choice.  If you go to techpowerup you'll see that the b580 is 17% in relative performance than the ""12 GB 3060 """,Neutral
Intel,What if you’re playing at 720p?,Neutral
Intel,"> It's not about the performance vs XX60. It's about the CUDA core count and bandwith vs biggest die in the lineup.  That is a absolute dog shit methodology to use.   The 1080 Ti was 471 mm², 5090 is 750 mm².  Those two products are not the same tier. And can therefor not be used as equal reference points. When it comes to die size the top Pascal card is closer to the 5080 than 5090.",Negative
Intel,The point is more VRAM =/= aging better. The b580 having 12gb isn't saving it from becoming obsolete in a few years.,Neutral
Intel,"They re not the same price tier either bro... They could have called the 5090 a 5095ti if that makes you happier.  The guy you quoted is right.   Also comparing the performance diff between the 1030 vs 1060 because guess what the 5060 should have been the 5050 looking at die size and memory bus.   Not sure why people keep defending these naming schemes, do you think the engineers use them internally lol? Its bullshit that marketing comes up with",Negative
Intel,"Exactly they're not the same tier, that's why I compared the 1030 to the Titan Pascal and the 5050 to the RTX 6000 Blackwell. 👍 Doesn't matter if die sizes get bigger, that's up to Nvidia. Just because the halo product got bigger doesn't mean they can get away with moving their product stack (below the XX90) one tier up in prices.   I guess it is a dog shit methodology to use, only Gamer's Nexus uses it and I'm sure #1 Nvidia apologist u/Alive_Worth_2032 knows more than him.   It's a shit methodology to use if you love conformism and shrinkflation in GPUs, I agree.  And it's a shit methodology to use if you're an Nvidia executive.  If you can't see that Nvidia is putting all the effort into binning high end chips for AI cause it's 90% of their income, then I don't know what to tell you. Obviously it's a good move for them, and it works, but there's 0 reason for a gaming customer to defend them for it. We used to get much more out of their chips in the gaming cards for more reasonable prices, why's it wrong to want that?   You can't just draw a conclusion based on relative performance between 2 underpowered products, you compare them to the ""best"" Nvidia COULD give us (which is the full top die for each generation) and go down from there.  Edit: just look at the 3090/ti and how the chips were proportionally fine still compared to it and that die was 628mm^2. That's not even an excuse.",Neutral
Intel,"> It's a shit methodology to use if you love conformism and shrinkflation in GPUs, I agree. And it's a shit methodology to use if you're an Nvidia executive.  You can chose one methodology.   You can decide on comparing what hardware you get at a certain price point.   Or you can ignore price/bom and look only at arbitrary model numbers as if they mean something.  You cannot do both at the same time.  Personally I prefer to look at die area. The 5050 today is roughly comparable to the 1050 Ti, comparing it to the top cards that are not comparable is irrelevant.  Pascal did not have a analogue for the 5090, period.  >Exactly they're not the same tier, that's why I compared the 1030 to the Titan Pascal and the 5050 to the RTX 6000 Blackwell. 👍  That changes nothing. The Pascal Titan are using the same die as the 1080 Ti. The whole tier of die that is used in today's consumer top SKUs and for the RTX 6000 Blackwell, DID NOT EXIST back then.  >Edit: just look at the 3090/ti and how the chips were proportionally fine still compared to it and that die was 628mm2. That's not even an excuse.  What are you even trying to say? Ampere also did not have a analogue to the 5090. The 3090 Ti, even it is sitting almost a tier below the 5090. The 2080 Ti and 5090 are unmatched in other generations.",Negative
Intel,"You’re still missing the point by focusing purely on die area and pretending model numbers are arbitrary. Model numbers mean something, nvidia knows that so that’s why they kept them consistent for so long (unlike AMD that's hella inconsistent). Because they rely on the perception of tier consistency from generation to generation, even if they’ve worsened the specs behind the scenes.  You say you “prefer to look at die area,” but that’s irrelevant unless you’re building the chips yourself. Customers don’t game on silicon real estate they game on actual performance and hardware capabilities. And the cuda core count + bandwidth vs top-die approach directly shows how much nvidia is offering relative to what they could offer if they weren't prioritizing AI margins.  So yes I stand by my comparison of the core count vs flagship, I think it's fair to judge Nvidia based on that.  > Pascal did not have a 5090 analogue  That’s just semantics. Every generation has had a full-fat top die, whether branded as “Titan,” “RTX 6000,” or whatever, which where I drew the comparison. Comparing lower-tier cards to the top die is how you reveal how much of the architecture's potential is being offered to gamers.  And yes, 3090 and 3090 Ti still preserved proportionally higher specs vs top-tier dies. What changed now is not just die sizes, it’s NVIDIA reserving most of the silicon for AI and throwing scraps to gaming.  > You can choose one methodology...  Sure, and I chose one: compare the lowest-tier GPU to the top die, which accurately shows how nvidia has been shrinkflating consumer value over time. You're welcome to look at BOM and TDP too the 5050 still loses. It should be sub 75W and priced accordingly.  I think it's okay to call it an XX40 series card even if we keep the TDP at 75W but the XX30 series would need to disappear. What I don't agree with is using the die area excuse to justify shrinkflation just because the 5090 is such a halo product.",Neutral
Intel,"> So yes I stand by my comparison of the core count vs flagship, I think it's fair to judge Nvidia based on that.  Do you even listen to your own madness?  You realize that what you are saying. Is that the lower end 5000 series would be ""better"". If Nvidia removed the 5090 entirely. And renamed the 5080 the 5080 Ti, and made the 5070 Ti into the new 5080.  Suddenly, you would praise the lower end cards for being ""better"". Purely because the high end is less powerful. Nothing else changed, no one is getting more hardware for their money.  Jesus Christ the mental gymnastics you people go trough.   The hardware Nvidia gives you for your money, is all that matters. How large the top die is and what core configuration it has. Is irrelevant for the value for cards further down the stack.",Neutral
Intel,End of Life is not the correct term for this. End of manufacturing is,Negative
Intel,Was about to shit on Intel for such a terrible product lifecycle time and how its GPU division was not going to do well if a GPU only has a ~2 years of updates until I read the article...,Negative
Intel,"End of life typically means end of support, not end of manufacture, or am I wrong?  Anyhoo I blame the article for bad wording",Negative
Intel,"> This announcement marks the beginning of the end for a model that arrived just two and a half years ago, and it offers partners a clear timetable for winding down orders and shipments. Customers should mark June 27, 2025, as their final opportunity to submit discontinuance orders for the Arc A750.",Neutral
Intel,"looks like Intel didnt fire enough of its incompetent staff. EOL usually means end of support/drivers/Developmemt.  [Intel® Arc™ A750 Graphics, End of Life](https://www.intel.com/content/www/us/en/content-details/856777/intel-arc-a750-graphics-end-of-life.html)",Negative
Intel,"Why did anyone buy these garbage Arc cards to begin with? The performance/$ was never ever ever ever ever not even for a single second better than comparable Nvidia and AMD cards.  I've never understood the point of the Arc cards. It was like a company in 2020 saying ""we're officially into console gaming and just created the PlayStation 2 for only $399!""",Negative
Intel,But did it really even live to begin with? 🤔,Negative
Intel,is this the shortest life cycle of recent GPUs? AMD was notorious for that... wasn't expecting Intel to top that....,Negative
Intel,"So, for the distribution process this is also known as End-of-Sale. Since this is a B2C business, they can't really control when their resellers reach the end of their stock, but at that point Intel has stopped selling the product.  EoM and EoL are both dates that usually do not coincide with the End of Sales.",Neutral
Intel,"The article is poorly written, and the headline misleading.   The card is discontinued meaning no more orders will be accepted. It says nothing about software support.   Admittedly the miscommunication is Intel's fault because they specifically use EOL in their notification, but I also put this somewhat on the article writers because they didn't do a good job of clarifying that intel meant discontinued. They could have used more appropriate wording in the headline but instead chose to follow intel's lead likely knowing it would sow confusion, but lead to more clicks.",Negative
Intel,"Least likely reddit user behavior, actually reading the article probably puts you in the top 5 :)",Neutral
Intel,"Don't worry, Iris Xe GPUs are still ""supported by the driver"", but their last fix was in 2023.  Intel doesn't notify when an architecture is actually dead, you're just left stranded for years until they make it finally official.",Neutral
Intel,"The incompetence source is actually intel themselves ...  [Intel® Arc™ A750 Graphics, End of Life](https://www.intel.com/content/www/us/en/content-details/856777/intel-arc-a750-graphics-end-of-life.html)",Neutral
Intel,> Anyhoo I blame the article for bad wording   Intel chose the wording in its own notice.,Negative
Intel,"Bruh did you even read the article?  Btw it costs half of a 3060 and it outperforms it, no idea why you are saying it is a garbage product lmao shitty redditors don't know the difference between end of manufacturing and end of life/service.  The article is garbage, but it was talking about the end of manufacturing, not the end of the service/life, meaning it will still het driver support.",Negative
Intel,"No because they are still supporting it, just ending manufacturing of new cards.",Neutral
Intel,"For a website that's predominantly text-based, a shocking amount of its users can't read for shit",Negative
Intel,"The job of a journalist is to provide the translation and context for their readers, not copypasta and regurgitate headlines that laypeople will immediately misunderstand.",Neutral
Intel,That's just for the hardware manufacturing.,Neutral
Intel,"ah, i thought it meant end of driver support.",Neutral
Intel,We all did. unconventional use of EOL,Neutral
Intel,"Unfortunately it’s been like that for decades with Intel, I can count numerous times in the past decade we’ve had this same conversation about their processor lines being EOL.",Negative
Intel,End of Sale,Neutral
Intel,"Tbf arc pro battlemage gpus are supposedly going to be fairly cheap, so its not that out of reach for consumers",Neutral
Intel,"Imo, the lockdown of virtualization started before AI, as Nvidia and AMD didn't want people buying cheap GPUs to virtualize corporate environments.      At least that's the context that I'm most familiar with for this. A lot of small to midsize companies barely need the GPU for their individual VMs so being able to split a single GPU against a beefy CPU would be a good cost saver to avoid the professional GPU tax.",Neutral
Intel,"Intel has said that SR-IOV will come to consumer. Even if it doesn’t, their GPUs aren’t super expensive.",Neutral
Intel,"Splitting a GPU is so that you can run multiple VMs with each GPU. Any AI training and any inference on LLMs or similarly large models wants to run *one* application across many GPUs. There's nothing in common between those use cases. The market for GPU virtualization for remote desktop, etc. kind of stuff is still there, but it never was a big chunk of the datacenter GPU market and still isn't.",Neutral
Intel,"Yep, I have to use Intel for my virtualization server because it's impossible to get anything to work the way I want on more powerful AMD APUs. I can easily have a VM using part of the iGPU while the host shares it.  Also kind of sad that a pretty old Intel iGPU still has more video transcoding features in QuickSync like HDR tone mapping and better stability than modern APUs.    I don't think it has anything to do with AI it's been like this  forever.  AMD is nice consumer hardware that they just don't provide the tools to use in any advanced capacity.",Negative
Intel,"i thought nvidia grid got semi replaced by MIG. then again, MIG is only supported only on a specific subset of datacentre cards.",Neutral
Intel,"Hyper-V supports GPU partitioning without the need for SR-IOV, which is almost perfect on nvidia cards (at least for what I need). It doesn't support VRAM partitioning though, so every VM has full access to all of the VRAM. It also disables checkpoints for the VM (or at least says it does, apparently there's a way around this), and doesn't support dynamic RAM allocation.  In my limited use of other brands of GPU, it has some infrequent but major bugs with AMD IGPUs and frequent but minor bugs with Intel IGPUs (I've never tried a dedicated card from either brand with Hyper-V).",Neutral
Intel,"I think the future way around is to make the vm look like any other application from the gpu drivers point of view. On Linux (both host and guest for now) there exists virtio native, but for now it only works on Amd and is still in Beta and not yet really well documented how to get it working.",Neutral
Intel,"I know AMD GPU passthrough is supported by HyperV virtualization and LXC containers without MxGPU, but that might be Linux-specific.",Neutral
Intel,"AMD appears to be heading towards allowing SR-IOV on consumer Radeon soon, but we'll see if it actually happens.  https://www.phoronix.com/news/AMD-GIM-Open-Source",Neutral
Intel,"I'm a bit late to the conversation, but I have more faith in Virtio-GPU Native Context landing than whatever the big three are planning  But VirtioGPU NC is, at the moment, Linux specific.",Neutral
Intel,"I think the real question is availability and drivers. If it was like the initial Intel Arc GPUs launched in 1 laptop shop in S. Korea, good luck trying to expand market share.",Neutral
Intel,"Yeah, i'd reckon this mostly. Initially, more than a decade ago by now i guess, we were seeing this, with consumer grade GPU's being used or tried to power workloads on virtual desktops, RDS deployments and even research clusters.",Neutral
Intel,Surprised AMD just acted like Nvidia while Intel is the one actually doing this for so much GPU stuff.,Negative
Intel,The problem is they're not super available either,Negative
Intel,"PCIe Passthrough is not the same as GPU splitting. All 3 have support for PCIe Passthrough. I use it all of the time. The main goal of SR-IOV is to remove the need for multi-GPU setups in virtualization which from a market segmentation point would be detrimental to profits at the consumer level because a host may only need 1-4GB of VRAM and the rest can be allocated to the guests. This would be very impactful in ""Prosumer"" markets where someone may need more than 2 GPU outputs (typical for iGPUs) but may not want a multi-GPU setup as a multi-GPU setup has higher power draw.",Neutral
Intel,Intel Linux drivers are solid for everything except games,Positive
Intel,"Its an incredibly small market for this among consumers.  Most people asking for this is in a professional environment, and everyone wants to sell pro cards.",Neutral
Intel,TLDR:    Geomean: RX 9060 XT 16GB is 46% faster (mixed settings).     Raw performance closer to 30-40% faster.          Cheapest RX 9060 XT 16GB in EU is ~400 euros (VAT included)     Cheapest Arc B580 12GB in EU is ~294 euros (VAT included),Positive
Intel,Does the B580 still suffer from the massive CPU overhead or was is ever fixed?  Considering that noone reasonable will pair something like a 9800X3D with a B580 something like a 7600 or even 5600 or 5700X3D this is something a lot of people would have to keep in mind when it comes significantly worse on the kind of CPUs you would actually see in a budget build.,Negative
Intel,average 58fps 45fps 1% lows with 9800x3d system will be unplayable for most people with zen3 era speeds.   Turn off rt- use fsr/xess/dlss + fg for 90fps+,Negative
Intel,"Since 9060 xt is $390, cheapest 5060 Ti 16gb at $430 might as well be included in this calculation",Neutral
Intel,I hope B770 delivers at a price that old-heads like me can appreciate,Positive
Intel,I own both. The 9060xt is better use of money.,Positive
Intel,hot take   intel arc b580 is a better value then the 9060xt 16gb reason   your going entry level gpu   9060xt is a weird spot because if you can find at retail 9070 smashes it at 100 150$ more it makes it EXTREMELY HARD TO SUGGEST YES some people supposendly cant afford the extra 100 150 (although its very very likely not off the bad just a extra month of work or so) especially when your going entry you can argue fps is higher but my argument is 250$ for a 12gb gpu that competes properly against what its trying to compete against the 4060 while the 9060xt is a messy pricing and i also argue no one wants to give intel a realistic chance which is another issue if more people gave intel a chance i fondly believe it would be great for most people,Negative
Intel,"Question is:  They have a deal in few days where i can get BF6 free with intel GPU. It's a game i would have bought anyway so it's like getting a 90$CA off the GPU price    Currently, i can get a Arc B580 for 360$CA which would mean an effective price of 270$CA. A 9060 XT, the cheapest comes at 490$CA (but it's on sale right now so the real cheapest would be 540$CA).   At this theorical price, i guess i can't go wrong with the intel one right? I will pair it with a Ryzen 7 5800x probably.   I'm currently running a Ryzen 7 1800x and a GTX 1070ti and playing at 1440p and i really don't care about running 4K ultra. BF6 was working incredibly well at low on my Nvidia so if graphics are higher i can only be happy but again, dont need ultra and fancy graphics that make real world look dull.    What's your thought people?",Neutral
Intel,"Prices differ between EU countries, in Germany the cheapest B580 is currently 270€ and 369€ for the 9060 XT.  For the US it is 310$/370$ currently.",Neutral
Intel,"Yes, but as he says in the video, there are some major outliers in the results due to things like Vram going past 12GB, and certain games just not liking Arc GPUs. Looks like a more ""typical result"" is 30% to 40% faster when not going over 12GB VRAM and not in a game that Arc has issues with. It's always hard to sum up the difference with a percentage, but especially hard with Arc GPUs, upscaler differences, etc.",Neutral
Intel,"Australia pricing (AUD, GST incl.):   9060 XT 16GB: $629   B580 12GB: $449",Neutral
Intel,What's VAT?,Neutral
Intel,Never heard of any more news about this either,Negative
Intel,"Yes, but the conclusion was that even in this best case scenario for the B580 it offers worse value. So not looking good for the B580, especially when it's way above MSRP.",Negative
Intel,I don't think it can be fixed. The card seems to have been designed to utilize extra CPU resources as much as it can.,Negative
Intel,"have a b580 and 9800x3d combo, and am perfectly happy with it. it even does some 4k gaming stuff just fine.",Positive
Intel,"As long as the zen3 era CPU can hit 58gps average and 46fps 1% lows in whatever game you are referring to, then it would make no difference vs having the 9800x3d. You only see a difference if the CPU frame times take longer to compute than the GPU frame times.",Neutral
Intel,"Yeah, as someone trying to build a new PC for 1080p/1440p it makes no sense to go for the 9060xt since the 5060 Ti is only slightly more expensive and gets me the new DLSS, better ray tracing, etc. I guess for pure raster performance one could also include the 7800xt but it's not as ""future proof"" that's for sure.",Neutral
Intel,"Got a 3080 ti for 550$ yesterday, man that hits hard for value against 7900 xt (750$) and 5060 ti   A 5060 ti, 4070 here costs 600 used.",Neutral
Intel,I’m in your shoes. I just bought my B580 with the Intel bundle for BF6. I have a Ryzen 5 3700X and tbh i’m not super super impressed with how much tinkering and adjustment is needed to get the Arc card to run “properly”. I’ve only had mine for a few days so hard to justify but i’m on edge of returning it possibly.   It seems every game is a hit or miss on how it runs and by saving only $100ish dollars i’m starting to think it’s not worth it. Don’t get me wrong majority of my games are running fine but the few that are inconsistent it’s like was this really worth it?  TLDR; Card is a hit or miss with the game you play. IMO spend the extra $ and get a reliable card from NVIDIA or AMD and feel secure,Negative
Intel,260€ for the B580 in july...,Neutral
Intel,"Also, has Intel fixed the driver over head issue? People buying lower end gpus probably dont have top of the line CPUs to play with.",Negative
Intel,Value Added tax. Think a better version of sales tax.,Positive
Intel,"It's better then *any* 8 gig model, but that is the definition of damnation with faint praise.",Positive
Intel,"Hell no, the 9800x3d is doing a lot of heavy lifting with 1% lows and 0.1% lows. If you ran same tests with 3600-5700x. It wont be same experience at all especially on B580",Negative
Intel,dlss is only dealbreaker if they're extremely similarly priced otherwise I'm not really sold on the difference just for the features    especially where i live,Negative
Intel,"I don't consider a 25% price difference to be ""slightly"" more expensive.",Negative
Intel,Also hits hard on the power draw,Neutral
Intel,Is it fully a overhead issue or were tests done on CPUs that predate RE-BAR support? It looks like Arc is basically have RE-BAR and it performs close to expectation or its in the dumpster without it,Neutral
Intel,Oh,Neutral
Intel,"debatable. 4060/5060, 9600xt on 3600x-5600x are faster than b580 on similar stepup",Neutral
Intel,"I'd rather have a 5060 ti 8GB than a B580 if you offered me them for the same price. It'd be way faster as long as I tune my settings. That being said, id never buy the 5060 ti 8GB.",Neutral
Intel,I would rather take the 5060 and play with lower textures than the b580 and be cpu bottlenecked with driver issues and no DLSS.,Negative
Intel,"You still aren't saying which test you are referring to, so my answer has to say general rather than specific, but you seem to not fully understand how this works. If the CPU computes the frame faster than the GPU, then an even faster CPU will not improve the frame time. If in this particular test a lower end CPU could not compute the frame faster than the GPU, then the faster CPU is helping. But a faster CPU does not always make a difference. It only matters if the weaker one is actually limiting the performance, which is often not the case.",Negative
Intel,"HUB tested very thoroughly and even did a follow up video about it because some russian tech site called HUB liars. TLDW of it is, yes rebar enabled, any game with decent CPU usage seriously hampered GPU performance, not all games but enough that it is something that anybody reviewing an Intel GPU should test for and advise the viewer about.",Negative
Intel,"Running a B580 with a 5700X3D, rebar enabled, tweaking a few settings, and it runs well for what I need it to do.  Basically iRacing and other sim racing titles.",Positive
Intel,"Nah theres definitely a drop if you pair 9800x3d with b580 and 5600 & other gpus as well. You'll see lower average & 1% and 0.1% lows.  Setups with b580, 4060s, 9600xt could see averages of: 52fps 40fps 1% lows. Just turn off the rt, optimize and run fg for \~100fps",Negative
Intel,"That is not true as a blanket statement. It is situationally true in the cases where the lower end CPU  can't keep up with the GPU frame times, but it is not true when it can.",Neutral
Intel,... its disingenuous to only test with 9800x3d on b580. Average person wont get those perf. Adequate step-up for me would've been a midrange cpu,Negative
Intel,The conclusion was that the b580 is still worse performance and value even in this best case scenario,Negative
Intel,Intel isnt serious about dgpu anyways. arc still has less than 0.15% marketshare on steam and the competitors are thriving...well only Nvidia. B580 probably 0.04%,Neutral
Intel,"I will say that Intel's naming is incredible. Now we have *Battlematrix*, what a badass name. Can we, as a society, *please* stop pretending that the rule of cool is ""cringe""?",Positive
Intel,700$ for the B60 is pretty cheap,Neutral
Intel,"Intel badly needs a presence in enterprise or the data center, so these Pro Arc GPUs are to be expected.  Hopefully these cards can act as a lifeline for the struggling Arc division, but I hope that data center revenue doesn’t compel Intel to abandon the consumer segment altogether.",Neutral
Intel,The passively cooled one looks interesting.,Positive
Intel,"Original title: THIS is the Most Important GPU of 2025  Genuinely could be exciting stuff, both in terms of AI/pro use cases, and in terms of giving Intel's discrete GPU arm more sales / runway.  Also includes commitments from Intel towards the gaming side of GPUs, so all in all pretty good news, if this doesn't bomb.",Positive
Intel,Interesting video downvoted to oblivion  GN spam upvoted endlessly  Make it make sense...,Positive
Intel,It’s nice that Intel will let people run the gaming drivers on these instead of artificially segmenting the consumer and professional products. Could see it being a cheap 24GB people’s champion once games start requiring obscene amounts of VRAM and companies offload their old stuff.  I want to see how the 70w model compares to the 6GB 3050.,Positive
Intel,The problem is it is a bit slow even compared with 5070 sigh,Negative
Intel,"But think of all those nVidia cards that could have been sold to gamers instead being sold for AI...  And think about what will happen when there is a $500 24GB AI capable board the AI guys can buy instead!   Or a dual GPU 48GB!  There are many AI cases where the 48GB card is going to trounce any offering from nVidia, and every card Intel sells to AI folk is one more nVidia card available to gamers.   So whether or not you are personally interested in buying a B60, it IS going to be a crazy important card to gamers because this is going to permanently shift the supply/demand for nVidia cards as well as force nVidia to start increasing vram to compete.  Bonus, because Intel has their own foundries, the upcoming celestial line will not be competing with nVidia for foundry time, so this will also increase the total number of video cards produced!",Neutral
Intel,"This is legit, I can see a lot of companies adopting this to use for local AI. Now if only Ollama IPEX can update their support and not lag behind for Intel ARC graphics, compared to the regular build for AMD and Nvidia.",Neutral
Intel,I will be watching out for B50 though.,Neutral
Intel,"Great troll clickbait title tbh, the same day that 5060 hits shelves. People will of course think this is a 5060 review and LTT knelt the knee on Nvidia if they don't watch the video.",Positive
Intel,TL:DR ITS ALL DATACENTER GAMERS MOVE ALONG.,Neutral
Intel,"No, we gotta have AI Pro Max Ultra+   /s",Neutral
Intel,I still feel their architectures called after D&D things is so cool.,Positive
Intel,I'm not a fan of *Sparkle*,Negative
Intel,"What data center revenue, they have pretty much none. Their roadmap also slid, they won't have anything competitive for data center till 2027.",Negative
Intel,"Why is Arc 'struggling'? Every release so far, going back to the first one, have sold out very quickly. B580s haven't been in retailer stock much at all since it came out, until the new AMD GPUs that recently released. But, whether it's scalpers or consumers, Arc never stays in stock for long.",Negative
Intel,"I mean I think that's what the dGPU for Intel is changing for Xe3P and onwards; Intel's fabs producing these GPUs which make money with AI/Workstation. I mean the original Xe produced on TSMC under Raja is certainly gone but feels like they've re-calibated with Xe3P quite a bit.  Honestly if Xe3P performs \*very\* well in labs I'd do a N44>N48 style die upscaling of a probable 256-bit part they're working on and just double the spec of it. If it still struggles in gaming Vs RTX 60 & UDNA/RDNA5/GFX14 PPA wise then they can just reuse said dies for AI easily. Especially could fill in the gap left by Falcon Shores to an extent.  An ""affordable"" 96GB Clamshell card for AI/Professional Vs Nvidia sounds pretty tempting.",Neutral
Intel,"> Original title: THIS is the Most Important GPU of 2025  seeing a title like that would instantly get me to click ""not interested"" in youtube.",Negative
Intel,It's Reddit. How often does it really make sense? XD,Neutral
Intel,I NEED to see this in gaming xD  I don't think gamers should be walled off from having a GPU that can do ai stuff and gaming as well. The GPU should be able to do both IMO,Positive
Intel,No one will actually think that 5060 is important,Negative
Intel,No they won't?,Neutral
Intel,"not all datacenter, but prosumers and businesses that need to leverage stuff for workstations also have a space here. plus, while nowhere near as performant as a full fledged gaming GPU, i bet the B60 with gaming drivers can be a respectable gaming card for those off work hours gaming sessions.",Neutral
Intel,"It was clickbait, I editorialized the best I could. Although I do kind of buy their reasoning for why it _could_ be a very significant GPU overall - if it lands well.",Positive
Intel,Where's my Chaotic Neutral Rogue GPU Intel?  WHERE!?,Neutral
Intel,Sparkle is an AIB who makes GPU boards. Intel didn't pick that name.,Negative
Intel,Their first generation had some issues and was not great even for the low price but in the past year they have been killing it in the budget card space. Its really nice to have some options on the lower end again.,Positive
Intel,"VR development could be an interesting topic here, assuming one GPU die can not access the other GPU's VRAM, it could still be a decent option for development of VR games on the internal testing front. VR is very much staying, along with AI so i can see this being a nice tool for both, just not all at once.",Positive
Intel,you underestimate human stupidity and overestimate their ability to read.,Neutral
Intel,Youll have to wait for R generation for the Rogue. Alignment optional.,Neutral
Intel,Chaotic Neural Rogue.,Neutral
Intel,"I see, ya they do have cool codenames. Also TIL Intel Arc GPUs are named after character classes from D&D. Alchemist and Battlemage for example, no idea where Battlematrix comes from though",Positive
Intel,"$299 and I can do Forza high settings 120fps at 1440p? Sold. ...don't care about RT, I turn it off anyway.",Negative
Intel,I'm just trying to find an affordable card that will run 4k vr at 60 to 90 fps without any dlss; so many cards don't have enough vram for vr. Excited to see if the B60 will be able to do this.,Positive
Intel,below $1000 USD? Sounds amazing value for those that want 48GB of VRAM and BMG okay for that kind of stuff?   One way to recuperate any losses on B580s in the dGPU division.,Positive
Intel,"sure, whatever. welcome back sli  edit: I KNOW ITS NOT SLI",Neutral
Intel,i wonder how well will this perform for LLMs,Neutral
Intel,"Nice to finally see BMG X2 in the wild. We need more weird GPUs like this, now more than ever.",Positive
Intel,"I'm in, I would buy these.  Bring back big cases with big stacks of GPUs.  Let's do this.",Positive
Intel,"The shroud design oddly reminds me of 9800GX2.   But of course, the GX2 had a... 'sandwich' form factor with dual PCBs!  The late 2000s were such a great time to be a nerdy teenager. Technology was still trying to find its footing and everyone seemed to be experimenting with different ideas. We had weird smartphones, weird GPUs (with CGI mascots), and even weirder CPU coolers (Thermaltake SpinQ, Cooler Master Mars/Eclipse, anyone?).  Everything just feels *too* mainstream and 'serious' nowadays... but I digress.",Positive
Intel,Am I dreaming a dual GPU in 2025.,Neutral
Intel,he's wearing the shirt to computex lmao,Neutral
Intel,"From the article I read, it does the processing work of handing out the data from both gpus in the video card itself.   Kind of cool if it's fast enough.",Positive
Intel,Only 2 words  Fuck Nvidia!,Negative
Intel,Can you buy a single one or do you have to buy it in Battlematrix form like the other B60?,Neutral
Intel,"Finally something for modern day workstations. If you are working on a desktop these days, the most important thing is access to AI models.   With these, you can easily buy 2-4 B60s and throw them in your machine to not be reliant on external services all the time. Could be a real productivity booster.",Positive
Intel,Is that Richard Stallman,Neutral
Intel,This would be perfect for lossless scaling multi gpu framegen.,Positive
Intel,Steve trying a mandarin ad. Cute I say...,Neutral
Intel,"This is awesome, but... are we ever going to get a B770?",Positive
Intel,Dual gpu. 2012 amd...is that you?,Neutral
Intel,"Hey, dumb question: can a motherboard with 4 pcie slot runs 4 of this card?  That would make 48\*4=192gb, pretty doable for really large langu model.",Neutral
Intel,I remember when he said that Intel were scumbags  did he change his mind?,Neutral
Intel,Don’t get fooled by 48GB. The card actually behaves as 2 cards of 24GB with pcie x8 lanes each of gen 5.0 in bifurcation mode. So for gaming most likely you will be getting only 24GB of vram (which is still not bad). But this card really shines in AI or other professional loads because they are claiming that you can have one big shared memory across multiple cards with their drivers and without any physical connectors like crossfire or sli. This means you can run full fat deepseek completely locally on your physical system with something like 4 of B60 on a threadripper system. God damm never even imagined that intel would be shining in GPU segment.,Neutral
Intel,this technology is from ASUS Mars II 3 GB Dual GTX 580 maybe from 14 years ago?,Neutral
Intel,they wont sell these discretely [source](https://www.youtube.com/watch?v=F_Oq5NTR6Sk),Neutral
Intel,no software support + half of the memory bandwidth of the 5 year old 3090,Negative
Intel,"Sli never left... Nvidia just hide it away from customers and sell it as nvlink (with some upgrades) for high end component for commercial side because they didn't want people stacking their cheaper cards.    Edit: got it, seems like I misunderstood what Sli does with the vram as it didn't pool.",Neutral
Intel,Unfortunately this is just a way to get more GPUs into one system. There’s nothing like SLI.,Negative
Intel,No need for SLI to use multiple GPUs.,Neutral
Intel,"no this particular card does not support sli. this is just to increase the gpu density in a given system. Increased from 1 gpu per slot to 2 gpus per slot.   According to linux tech tips, the memory sharing between cards are done on the software level, which in my opinion is not a good thing. the pci-e gen5 x8 bus has a memory bandwith of 32GB/s in one direction (latest nvlink support up to 900GB in one direction). More inter-card bandwidth = better performance on large models (assuming the model support multi-gpu setup). No dedicated inter-card interface certainly will harm the maximum theoratical performance in certain workloads.",Neutral
Intel,"Me too. Based on the TOPS, the inference speed seems to be roughly comparably to a 3060TI for the B60 and the Dual being roughly comparable to a 4080, though i think realistically more like a 3080/4060 due to the bandwiths. I've run a good bit of local inference on a 3060 12GB, and while it isn't hyper-performant, the speed is slower but ""acceptably slow"" in my opinion. You won't be waiting 10 seconds per token, but it's also not going to be spitting out a novel per minute. For me, that extra memory is really what matters because it mean less quantization at roughly the same speeds I'm used to, which means less errors in the output. I'm really more hopeful that this will finally light a fire under the NVIDIA execs who thought punishing consumers with less memory to prevent corporate customers/datacenters from using consumer cards in their servers was the right move. You can see they are reversing that somewhat with the 5090, but they can kick rocks with that 3k\* price tag that is once-again intended to get more money from data centers who decide to try to use them. It's pure market manipulation and artificial deflation of the specs to drive pro customers to more expensive hardware at consumer's burden.",Neutral
Intel,"Tokens/second won't be as good as a high end Nvidia card obviously, but you'll be able to fit a much larger model onto it than you could for any consumer and almost any Nvidia card. And for $1000, you could run four of these and still come out way cheaper than one A6000.",Positive
Intel,"Since I've run a bunch of tests on Xe2 (and of course plenty of Nvidia and AMD chips):  * A 70B Q4 dense model is about 40GB. w/ f16 kvcache, You should expect to fit 16-20K of context (depends on tokenizer, overhead etc) w/ 48GB of VRAM. * B60 has 456GB/s of MBW. At 80% (this would be excellent) MBW efficiency, you'd expect a maximum of 9 tok/s for token generation (a little less than 7 words/s. Avg reading speed is 5 words/s, just as a point of reference most models from commercial providers output at 100 tok/s+ * For processing, based on CU count each B60 die should have about ~~30~~ 100 FP16 TFLOPS (double FP8/INT8) but it's tough to say exactly how it'd perform for inference (for layer splitting you usually don't get a benefit - you could do tensor spliting, but you might lose perf if you hit bus bottlenecks). I wouldn't bet on it processing a 70B model faster than 200 tok/s though (fine for short context, but slower as it gets longer.  Like for Strix Halo, I think it'd do best for MoE's but there's not much at the 30GB or so size (if you have 2X, I'd Llama 4 Scout Q4 (58GB) might be interesting once there are better tuned versions.",Neutral
Intel,"Reminds me of the old days when brands would try weird stuff to see what sticks, or just for the market data/rnd.  Kudos for Intel for trying something a bit different.",Negative
Intel,Thankfully not,Negative
Intel,"yes, they are even showing servers with 4 of those [https://geeksynk.com/intel-rises-all-upcoming-intel-high-vram-gpus-with-their-specs/](https://geeksynk.com/intel-rises-all-upcoming-intel-high-vram-gpus-with-their-specs/)",Neutral
Intel,“24gbs of VRAM is not bad for gaming”   It’s the bare minimum these days,Negative
Intel,If it has Vulkan it can be used for inference,Neutral
Intel,It has all the software stack it needs and works great.,Positive
Intel,Good luck fitting 48GB of anything into a single 3090 VRAM.,Positive
Intel,OpenVINO is fully supported on Arc discrete graphics and supports both Windows and Linux.,Neutral
Intel,"Man you don't know what are even talking about, these GPUs are one 192 bits per unit combined you will get 384 bit through 2 GPUs and the 3090 well it's half the amount of VRAM and it is powerful but in workstation use it is not the only thing you need, and software support, there is not support for 50 Series cards on many applications like premiere, Da Vinci and most of the work based applications and the Game ready drivers are more than capable enough to run LLMs on Windows and Linux",Negative
Intel,"Tbh that is probably for the best. Sli was never that great for gaming, all that leaving sli open as an option would bring is greater AI demand for consumer cards.",Negative
Intel,"SLI, as implemented in the later years of its existence, has the same latency cost as framegen, with worse variation.  With how much rage there is about framgen... can you imagine how the public would respond if they had to buy two GPUs to get it?",Negative
Intel,">because they didn't want people stacking their cheaper cards.  because they’re evil and greedy. No other reason, right? 🙄",Negative
Intel,starting to find out why people use /s on this website,Negative
Intel,"According to linus tech tips, the memory sharing between cards are done on the software level. The pci-e gen5 x8 bus has a memory bandwith of 32GB/s in one direction (latest nvlink support up to 900GB in one direction). No dedicated inter-card interface certainly will harm the maximum theoratical performance in certain workloads.  Also, according to linus the purpose of this 2 gpus on 1 board is just to increase the server density, and nothing to do with bandwith. Each gpu is connected to host via pci-e 5.0 x8 interface, and the system will seperate them via pci-e bifurcation.",Neutral
Intel,"Pretty disappointing tbh. This card will not be a nvidia replacement by anytime soon. I think the cheapest option to run LLM locally is to buy decomissioned nvidia tesla v100 16G SXM2. Each cost around 60-70 USD and availability is pretty good atleast in China. And pair with SMX2 to pci-e conversion board.  6 of them can be linked together using nvlink, providing 96GB vram. The downside though, is this setup will drain 1.8kw of power when maxed. And the HBM2 vram inside the core is very prune to failure due to old age. The embedded vram can't be repaired, so once vram died whole gpu is pretty much gone.",Neutral
Intel,between AMD and Intel which is more stable?,Neutral
Intel,"Intel's official figure is 192 INT8 TOPS. I guess this is with sparsity, so 96. Then FP16 should be 48 TFLOPS (or 4x the FP32 perf).   So essentially, a 3060 with 24GB VRAM and 25% higher bandwidth (conveniently available in a dual-gpu version for a 48GB total).",Neutral
Intel,Thanks!!!,Positive
Intel,That’s true and product segment itself focuses on AI. But the fun part is bifurcated dual GPUs. And unlike Nvidia they are claiming to have support for both game-ready and professional drivers at same time. This means you can be professional video editor by day and gamer by night (or gamer by weekends or vice-versa 😄) all with one card without tinkering drivers or monitor ports. And by the way unlike AMD they have great support for video encoding and decoding. I am really excited to see all these various combinations of things on this card.,Positive
Intel,Yeah 24GB has become bare specially for 4K and above. This is good intel has hit a good sweet spot each GPU. 😗,Positive
Intel,source??,Neutral
Intel,Sure but there’s no Vulkan needed.,Neutral
Intel,"this isn't a single card. it's two mid cards on one PCB, you are still splitting vram across two GPU's",Neutral
Intel,"It's an option...? Meaning that you don't have to use it if you don't like it.      And plenty of people use their commercial cards for gaming, literally some of earliest adaptors of nvidia cards were chief tech leads playing games on their workstations in afterhours, as described in many interviews and books such as the recent The Nvidia Way.   And Sli was fine once they made G-Sync, which was from the discovery of microstutters as a big problem.",Neutral
Intel,"Yes, that's why I said:    >...sell it as nvlink (with some upgrades)...     Also, I bet that most people would gladly take just the standard sli without the nvlink improvements simply for the larger vram.    Edit: I was wrong, see below. Cunningham's Law",Neutral
Intel,"The question is less about stability and more about support.  AMD's ROCm support is basically on a per-chip basis. If you have gfx1100 (navi31) on Linux you're basically have good (not perfect) support and most things work (especially over the past year - bitsandbytes, AOTriton, even CK now works. I'd say for AI/ML (beyond inferencing) I'd almost certainly pick AMD over Intel w/ gfx1100 for the stuff I do. If you're using any other AMD consumer hardware, especially on the APUs then you're in for a wild ride. I am poking around with Strix Halo atm and the pain is real. Most of the work that's been done for PyTorch enablement is by two community members.  Personally I've been really impressed by Intel's IPEX-LLM team. They're super responsive and when I ran into a bug, they fixed it over the weekend and had it in their next weekly release. That being said, while their velocity is awesome, that causes a lot of bitrot/turnover in the code. The stuff I've touched that hasn't been updated in a year usually tends to be broken. Also, while there is Vulkan/SYCL backends in llama.cpp that work with Arc, you will by far get the best performance from the IPEX-LLM backend, which is forked from mainline (so therefore always behind on features/model support). IMO it'd be a big win if they could figure out how to get the IPEX backend upstreamed.  I think the real question you should ask is what price point and hardware class are you looking for and what kind of support do you need (if you just need llama.cpp to run, then either is fine, tbt).",Neutral
Intel,"Hmm re-reading, I may have brain-farted the CU math, Arc 140V (Lunar Lake) is I believe 32 TFLOPS so obvs G21 should be higher.  B60 ([official specs](https://www.intel.com/content/www/us/en/products/sku/243916/intel-arc-pro-b60-graphics/specifications.html)) uses the full BGM-G21 which has 20 Xe2 cores, 160 XMX engines and a graphics clock of 2.4GHz (a bit lower than B580).  Each Xe2 core can support 2048 FP16 ops/clock ([Intel Xe2 PDF](https://cdrdv2-public.intel.com/824434/2024_Intel_Tech%20Tour%20TW_Xe2%20and%20Lunar%20Lakes%20GPU.pdf)).  ``` 20 CU * 2048 FP16 ops/clock/CU * 2.4e9 clock / 1e12 = 98.304 FP16 TFLOPS ```  This lines up if Intel is claiming 192 INT8 TOPS (afaik XMX doesn't do sparsity and they claim 4096 INT8 ops/clock, so double FP16/BF16).  These cards seem super cool! My main bone to pick is that the retail plans (uncertain retail release in Q4) makes it less interesting. I guess we'll see what else hits the shelves between now and then.",Neutral
Intel,But you get 48GB of VRAM for that price.,Neutral
Intel,"Intel has a currently working but not released software to share video memory across multiple GPUs for the purpose of AI and such with minimal performance loss. It is effectively 48GB worth of video memory for the tasks these GPUs are designed for.  These cards might be slower than other solutions, but they will be thousands or even tens of thousands of dollars cheaper. Stuff like this (and AMD's AI MAX+ 395) allow companies to have each dev be able to host their own full size(or at least near full size) AI model on their own workstation instead of fighting over time on the big server. They also allow hobbyist to work with much larger models than they usually would be able to.",Neutral
Intel,"The nvlink improvements are what *allow* for larger vram.  SLI's biggest problem was that GPU memory had to be duplicated across all cards, meaning if you put 2x 8gb cards in SLI, you still only had 8GB effective vram. NVLink allows for memory pooling, SLI did not.",Neutral
Intel,thank you,Positive
Intel,"If they really have 98 FP16 TFlops (i.e., 70% of a 3090), they will be pretty cool and better value than a heavily used 3090 (if we ignore the CUDA advantage)",Positive
Intel,with the memory bandwidth of a 5060... too slow for dense models and not enough memory for large MoE models,Negative
Intel,"Unless they have some custom interconnect, it will be slow (PCIe 5.0 x8 tops out at 32GB/s)",Negative
Intel,"Thx, TIL",Neutral
Intel,"Faster tham RAM, which is what you'd need to use with single 3090 when you go above 24GB.",Neutral
Intel,"What the hell, even a B580 has 192 Bit bus compared to 5060's 128 bit bus, and when it is combined you are getting aboout 384 bit bus accross the 2 GPUs and they are not pooled but you won't get that much VRAM even on used markets",Negative
Intel,That’s about the bandwidth of the Nvlink bridge on a 2080(/ti) (which obviously doesn’t do vram sharing),Neutral
Intel,Then just get Strix Halo if you don’t care about bandwidth and performance,Negative
Intel,"Seems like I have to go through this again, but with you this time. So, again. For the price of Strix Halo, if you actually use the memory, you are going to get much higher performance than a system with better GPU but less VRAM, because you'd run out of it and had to go for RAM. Do you guys just not understand that if you are going to use the VRAM, it will have much better performance than something that would run out of it and had to use RAM for the same price?",Negative
Intel,"Disappointing that we're not seeing the B770 in computex. Guess we'll have to ""stay tuned"" until intel decides to release or scrap the B770  The Arc Pro B50 is a B570 with 16gb of VRAM on a 128bit bus. It has a $299 msrp  But we DO get the Arc Pro B60 which has a single 24gb GPU model and a dual GPU B580 which has 48gb of VRAM shared between both GPU's   Intel designed it's software so that you can scale up workloads across multiple Battlemage GPU's up to 8. Intel calls this Battlematrix.  A fully equipped Battlematrix setup would have across 8 GPU's or 4 of the dual gpu B60 cards: 192gb of VRAM, 1280 XMX Engines and 160 Xe cores. This setup comes with what I assume is a Granite Rapids Xeon CPU and MSRP range from $5000-$10000. It includes a customized linux distro with software and frameworks optimized for AI workloads.  Unfortunately, Battlematrix workstations are the ONLY way to get an Arc Pro B60 as they're not selling them as individual cards yet.   EDIT: You can actually buy the Arc Pro B60 individually in Q1 2026.",Neutral
Intel,I’m just happy Intel is still thinking about GPUs at this point.,Positive
Intel,"How likely are these to support SR-IOV?  I want to buy one for GPU Virtualization. I am willing to go through my work to buy one, I don't care if they're not going to be available at retail. Hell I want the 48GB model GN just tore down.",Neutral
Intel,Do I still need to stay tuned for a possible B770?,Neutral
Intel,"A dual GPU B580 with 48GB of VRAM? That's a spicy meatball, especially if the price is low. IIRC a B580 12GB is something like only $399 CAD or was $249 USD at launch.  That's the kind of cooking-with-gas product that would have the AI open source dev crowd marching to a blue tune. Especially if you could get a handful of them for the same price as a 5090 32GB card and run them together.",Neutral
Intel,"Is doing 'AI' actually simple on this hardware though? as in wide compatibility, no headaches trying to make everything work like it just does on Nvidia etc.  I have no clue, not trying to sealion. I just know that for a long time actually doing certain tasks on non-Nvidia hardware involves a lot of extra steps just to make things work.",Neutral
Intel,MFW When Intel GPU Department is dishing out good stuff and CPU department is kina meh.,Positive
Intel,"Nothing on the B770 after saying ""stay tuned"" when asked about it? Seriously?",Negative
Intel,"So much for ""UNDERPROMISE OVERDELIVER"". They all but said to expect B770 and nothing...",Negative
Intel,Are the ARC GPUs any good or not. I've seen some saying they're crap while others say they give the most bang for your buck. So which is it?,Negative
Intel,"The B60 has 24 GB of VRAM for a $500 MSRP (but I imagine it will be difficult to find at that price/single GPU). The memory bandwidth (which will likely be the inference limitation) is 456 GB/s.  For inference, its closest Nvidia competitor is the $550 MSRP 5070, which has just 12 GB of VRAM but bandwidth of 672 GB/s. So if you're running an 8B model (which honestly I can only recommend for small tasks like tab code assist, not image generation) it is better to get the 5070. But if you're running a slightly larger model like Deepseek R1 14B at Q8, you will only be able to run it with B60 or something significantly more expensive, like a 3090.  https://apxml.com/tools/vram-calculator",Neutral
Intel,"You gotta love competition, Nvidia would never do something like this, even though they say they want to democratize AI.  If they really wanted to boost AI research, you flood the world with affordable VRAM so everyone can run big models locally.",Negative
Intel,It wouldn't surprise if the reason they aren't selling the GPUs with Battlematrix ala carte initially is that the code hasn't been mainlined into the Linux kernel yet.,Neutral
Intel,Depending on Price and Software Support I might actually grab myself the 24 or 48gb VRAM Variant for AI workloads. If the gaming performance is improved I might be able to bench my 6700XT.,Neutral
Intel,Hey I want to say good on you for updating every comment you’ve made with the new information; it was a pretty upstanding thing to do.,Positive
Intel,"> Disappointing that we're not seeing the B770 in computex. Guess we'll have to ""stay tuned"" until intel decides to release or scrap the B770 >  >   That was never going to happen before RTX 5060.",Neutral
Intel,"they probably know that they can't just start making dGPUs and immediately get half the market, but that it's a long battle that they gotta continue for quite a while",Neutral
Intel,"From Chips and Cheese's video, it should be comming in Q4  [https://youtu.be/F\_Oq5NTR6Sk?si=A6cXIrKbRquAsNEm&t=280](https://youtu.be/F_Oq5NTR6Sk?si=A6cXIrKbRquAsNEm&t=280)",Neutral
Intel,They are guaranteed to be support SR IOV by Q4,Neutral
Intel,Yes https://x.com/tekwendell/status/1924417394115858735,Neutral
Intel,"You can't buy the Arc Pro B60 by itself. Instead, Intel is bundling up to 4 of them with a xeon cpu as part of a ""Battlematrix"" workstation.   Intel must be desperate to move unsold Granite Rapids inventory  Each workstation will cost $5000-$10000 with a custom linux distro tailored to running ai workloads  Edit: ypu will be able to buy the Arc Pro B60 as an individual card in Q1 2026",Negative
Intel,"I guess so. Intel did throw gamers a bone, though, with the Arc Pro b50 with 16gb Vram with a 128bit bus using the cut down b570 die (16 Xe cores)",Neutral
Intel,"You can't buy the Arc Pro B60 by itself. Instead, Intel is bundling up to 4 of them with a xeon workstation cpu as part of a ""Battlematrix"" workstation.  *sigh* Intel pulling an Intel again. Intel must be desperate to unload Granite Rapids inventory that's sitting in warehouses  EDIT:  intel is going to sell the Arc Pro B60 separately in Q1 2026",Negative
Intel,%100,Neutral
Intel,"According to a news article: ""Intel is working to deliver a validated full-stack containerized Linux solution that includes everything needed to deploy a system, including drivers, libraries, tools, and frameworks, that's all performance optimized, allowing customers to hit the ground running with a simple install process. Intel will roll out the new containers in phases as its initiative matures. """,Positive
Intel,"I own an A770, and the answer is absolutely not. I bought it with the expectation that the experience would be much worse than nvidia/amd, and they blew my expectations out of the water.  My favorite example is that, until [this release 3 weeks ago](https://github.com/intel/intel-extension-for-pytorch/releases/tag/v2.7.10%2Bxpu) ([the fix was merged ~march 3](https://github.com/intel/intel-extension-for-pytorch/issues/325#issuecomment-2694277026)), using large image generation models on Alchemist cards wasn't possible because they have a 4GB VRAM allocation limit (fixed in Battlemage). Now it mostly ""just works"", but a lot of optimizations are nvidia-only.  LLM support isn't quite as awful as image/video generation was prior to the aforementioned release, but it's still substantially worse than nvidia/amd (I also own a 6900xt). For instance, none of the inferencing tools support Flash Attention on Intel's compute backend (a [SYCL](https://en.wikipedia.org/wiki/SYCL) implementation), which _severely_ limits context length / initial generation speed because memory usage scales quadratically with context size.  Intel maintains a [repository](https://github.com/intel/ipex-llm) with forks of most of the popular tools (vLLM, llama.cpp, Ollama, etc). It vastly improves performance when using intel's backend relative to the upstreams, but it's also extremely annoying to use. It's buggier, releases lag behind upstream, there's less model support, it requires a hyper-specific environment setup, etc. I have no idea why they don't just upstream their changes.  At this rate, Vulkan will end up with better support and performance than SYCL, despite the former not being designed with compute workloads in mind at all, while the latter was explicitly built for it (I think).",Negative
Intel,"Been researching what my options are outside of NVIDIA, but it honestly seems Intel cards may be a more viable solution with IPEX than AMD. There are reddit posts floating around with the B580 showing comparable inference performance to a 4060ti for text and image generation.  Getting 3-4it/s on SDXL models would be a huge improvement to my 4-5sec/it with my 1060 ti bottlenecked by RAM.",Positive
Intel,On a new budget computer build they're really good but there's issues putting them in older systems as upgrades.,Positive
Intel,Decently competitive. They do have driver quirks and last I heard they also had a CPU overhead issue.,Neutral
Intel,For gaming the B580 seems like very good value if you're prepared to encounter a few scenarios where it falls behind. In general it seems to offer a decent bit more than AMD and Nvidia at the same price point.,Positive
Intel,">For inference, its closest Nvidia competitor is the $550 MSRP 5070, which has just 12 GB of VRAM but bandwidth of 672 GB/s.  No, the closest NV competitor is the $429 5060Ti 16GB with 448GB/s bandwidth.",Neutral
Intel,I just realized that you can actually put 4 dual GPU b60's in a large case to get 8 gpu's into 1 pc,Neutral
Intel,"You can’t, they’re not sold individually and are workstation only.",Neutral
Intel,"Why? If they plan to release it at all, they should do it before the 5060, not after.",Neutral
Intel,This makes me very erect.,Positive
Intel,They better make these available.  I have been waiting since 2018/2019 for SR-IOV to become available on GPUs ever since I tried it with enterprise NICs.,Positive
Intel,"128 bit on Intel is entry level. Around A580/RX 6600 (which was $190 like 3 years ago). I don't think it makes sense for gaming.  The extra VRAM is nice, but it's too slow to be worth even 199 for gaming, let alone the MSRP of 299. Might make sense for video editing or AI or smth.",Negative
Intel,"Yeah which seems to translate as no, not right now.",Neutral
Intel,"B580 is probably better, but I get closer to <2 it/s on my A770. There might also be something wrong with my setup, given how jank the whole thing is though.",Negative
Intel,And 192 gigs of vram. Thats enough to run almost all of the open source language models out right now. I wonder how that kind of setup would stack against an Nvidia setup.,Neutral
Intel,Proper posture is important.,Neutral
Intel,"If you want this setup, then you'll have to buy a ""battlematrix"" workstation with what I assume is a Granite Rapids Xeon cpu.  Guess they're really trying to get move unsold Granite Rapids inventory  as you can't buy the Arc Pro b60 on it's own.  Each workstation will cost $5000-$10000 with a custom linux distro tailored to running ai workloads  Edit: You will be able to buy the Arc Pro B60 as an individual card in Q1 2026",Neutral
Intel,It's definitely a posture alright.,Positive
Intel,"Wow, what...  I need one of those.  I just wonder if they are already redy to run, or just dont have the software ready like it is for CUDA, can't be writting all the code from scratch.",Negative
Intel,I looked into the situation more and they're apparently going to sell the Arc Pro B60 as an individual card in Q1 2026. Before that it's exclusive to battlematrix,Neutral
Intel,"You can reasonably assume that there will be driver support for a significant amount of time for the B580, honestly it'll probably be supported for the entire time that you have the card. It's unfortunate in that this probably kills future products in the Arc line, but given Intel's need to shed costs, it's also not all that surprising.",Neutral
Intel,Intel releases a competitive GPU   Nvidia instantly proceeds to buy a part of Intel...,Neutral
Intel,Seems to me like they'll be building SoC hardware for like mini-PCs or laptops.  Maybe integrated graphics for desktop CPUs.  It doesn't seem like it's competing with discrete GPUs.,Neutral
Intel,This is shit. Hopefully they still make the mid range 700 series. I was sort of holding out for that but it's taking too long.,Negative
Intel,"This does suggest that Intel GPU unit will likely get the axe.  It also suggests that ARM doesn't have the HPC ability of x86.  ARM was half as fast, at the same power, as AMD EPYC.  So, Nvidia is likely admitting that ARM isn't close enough yet.  Real question is how long this takes to materialize?  Further, this could violate x86 agreement.",Negative
Intel,They aren't going to sabotage a division of a company they just bought for 5 billion. I think it's a problem for the new year release of the 7 series cards though. Those were supposed to challenge some of the mid range cards.   They aren't dropping driver support for a LONG time. By the time you have to worry about it you wont own it.,Negative
Intel,Wasn't arc already on its way out?,Neutral
Intel,"Alright, good to hear. I’ve been pretty happy with it so far otherwise!",Positive
Intel,"Swatch owns all the watch brands. Meta, alphabet and Microsoft own all the tech brands. Nestle, Unilever own all the commercial food products etc etc",Neutral
Intel,Lol seriously. This is how capitalism in america works these days!,Negative
Intel,"True, my first thought was consoles and handhelds (Steamdeck).",Neutral
Intel,I really hope this is the case because I love the idea and technology behind arc and really want it to become more competitive.,Positive
Intel,"Also consider that even for Nvidia spending 5 billion to just terminate a division of intel isn't why they did it.  They'll probably slap their brand on it and rake in the profits. I'd expect a 10% jump in msrp but still.. They aren't in business to lose money. I think they'll keep production going personally, put their stamp on it, sell it for a bit more and keep the status quo.   I think they were worried about what the new intel card was going to do to their low/mid range lineup. The dev cost for the 700 series has already been spent. They will release it.. Just at what price point?",Neutral
Intel,5 billions is a small price to pay to eliminate a competitor in their cash cow almost monopoly segment.,Negative
Intel,b series is less than a year old,Neutral
Intel,it's always worked that way.,Neutral
Intel,Thats true i guess if were being honest,Neutral
Intel,"Sure. B580 is best value GPU on the market right now, as it comes with BF6",Positive
Intel,"First, your cpu is weak enough to still limit a 5060, and if you dont have a frame cap it will cause stuttering and frame drops. You're likely running into a ram limit being on 16gb as well, which again causes stuttering and frame drops, since once the ram is full it starts to use system storage as ram.    Id suggest saving some money up and going for a 9600x/7600x mobo and ram. If you need a cheaper option, id buy a used 5000x3d cpu with 32gb ddr4.",Neutral
Intel,>\- Turn on resizeable BAR  Turning that off actually helped me with FPS drops in games.  Did you test it being turned off with your new GPU?,Neutral
Intel,"turn off ReBAR and check GPU temps, trust me on this one.  if your RAM is healthy and CPU also, with good temps, it could be the ReBAR and temps on ur gpu. check for airflow in ur case",Neutral
Intel,See if your motherboard is outputting the proper pcie signal to your GPU. It might be trying to output the wrong signal. It happened to me because I had an old vertical GPU mount that had an older cable. It couldn’t handle the 4.0 signal and I manually dropped it to 3 in the bios.,Neutral
Intel,"Seems like you tried basically everything, do you have the same stutters with your 1080p monitor as well?",Neutral
Intel,"Before I got rid of my 1060 everything ran just fine. I am willing to upgrade, but I mostly play older games anyways. Many of these games use little ram and leave plenty of cpu headspace.",Positive
Intel,I have the 3600 paired with 16gb of ram and I experience no stutters. Pretty sure the problem is something else.,Neutral
Intel,"Thanks for the tip, just did. It sadly did not help",Negative
Intel,"> turn off ReBAR and check GPU temps  Given that they're probably also using the Wraith Stealth that came with the 3600, they should probably check CPU temps as well. If the install is old enough, it's quite possible that the thermal paste has dried up and the CPU is throttling. Even more likely if this is a chassis something like a non-Flow variant of the NZXT H510.",Neutral
Intel,Did you run DDU yet?,Neutral
Intel,Pretty sure people play a different list of games and have different expectations for different settings,Neutral
Intel,"Thanks both for the tips. Good point on the cpu paste. However, the temps are nothing out of the ordinary. As far as turning of ReBAR goes I also have no succes :(",Negative
Intel,I did!,Neutral
Intel,"Then my best answer is the initial comment I gave, sorry I dont have anything else to offer.",Neutral
Intel,I'd personally suggest the 9060xt.  The 9000 series are spectacular tbh...,Neutral
Intel,If it's the 16gb 9060 xt I'd say the upgrade is worth it,Positive
Intel,If its the 16gb 9060xt then absolutely that. Thats the one i just got for my first pc,Neutral
Intel,"It honestly depends on what your needs are... Fps, solo campaign, rpg, streaming, vid rec., etc ... Do u have an interest in A.I. or M.L.? RX is amazing for raw frames n streaming. ARC is amazing for AI n ML so it depends on what your use cases are",Positive
Intel,9060 XT and get the 16GB version.,Neutral
Intel,is the current price still good with it?,Positive
Intel,the pricing would be my real question as it seems to be too much for both,Neutral
Intel,"they're both overpriced, is this USD?",Negative
Intel,Nah. MSRP is 350. +/-50 is what I'd accept. 100+ more is crazy. That's 30% upcharge.,Neutral
Intel,What country are you in? It would depend on local pricing and what alternatives are priced at. Sometimes countries are just expensive unfortunately,Negative
Intel,usd equivalent of php,Neutral
Intel,where can we get msrp ones im from the philippines.,Neutral
Intel,"if I had to overpay for something, it'd be amd over Intel.  Especially because their new CEO has been threatening to discontinue their GPUs",Negative
Intel,I see. Are the 9060 xt around those price?,Neutral
Intel,thanks for that info ill keep that one in mind,Positive
Intel,on stores here yeah around those price,Neutral
Intel,I see. Then I guess it's fine to buy it. It's a much better card that the b580 by 25%.,Positive
Intel,"If you go with a smaller case you can get a better GPU.  [PCPartPicker Part List](https://pcpartpicker.com/list/RDQQv4)  Type|Item|Price :----|:----|:---- **CPU** | [\*AMD Ryzen 5 7600X 4.7 GHz 6-Core Processor](https://pcpartpicker.com/product/66C48d/amd-ryzen-5-7600x-47-ghz-6-core-processor-100-100000593wof) | $178.51 @ Amazon  **CPU Cooler** | [Thermalright Assassin King SE ARGB 66.17 CFM CPU Cooler](https://pcpartpicker.com/product/9Gstt6/thermalright-assassin-king-se-argb-6617-cfm-cpu-cooler-ak120-se-argb-d6) | $18.59 @ Amazon  **Motherboard** | [\*Gigabyte B650M GAMING PLUS WIFI Micro ATX AM5 Motherboard](https://pcpartpicker.com/product/9HTZxr/gigabyte-b650m-gaming-plus-wifi-micro-atx-am5-motherboard-b650m-gaming-plus-wf) | $125.00 @ Amazon  **Memory** | [\*Silicon Power Value Gaming 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://pcpartpicker.com/product/cCKscf/silicon-power-value-gaming-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-sp032gxlwu60afdeae) | $85.97 @ Silicon Power  **Storage** | [TEAMGROUP MP44L 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://pcpartpicker.com/product/2x4Ycf/teamgroup-mp44l-1-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-tm8fpk001t0c101) | $54.99 @ Newegg  **Video Card** | [\*PowerColor Reaper Radeon RX 9060 XT 16 GB Video Card](https://pcpartpicker.com/product/fzh2FT/powercolor-reaper-radeon-rx-9060-xt-16-gb-video-card-rx9060xt-16g-a) | $379.98 @ Newegg  **Case** | [Cooler Master MasterBox Q300L MicroATX Mini Tower Case](https://pcpartpicker.com/product/rnGxFT/cooler-master-masterbox-q300l-microatx-mini-tower-case-mcb-q300l-kann-s00) | $39.99 @ Amazon  **Power Supply** | [\*Montech CENTURY II 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://pcpartpicker.com/product/sqbypg/montech-century-ii-850-w-80-gold-certified-fully-modular-atx-power-supply-century-ii-850w) | $89.90 @ Amazon   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$972.93**  | \*Lowest price parts chosen from parametric criteria |  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-09-07 23:06 EDT-0400 |",Neutral
Intel,"Looks like you hit the nail on the head for a budget ddr5 gaming system, maximising performance per dollar IMO.   You could try a 9060 xt, to get 16gb of vram to help give your GPU more legs to play longer since a 7500f should still be well suited for that GPU. It is likely about $100 more though.   I see nothing wrong with this build for what your assumed goal is, you could probably squeeze more out of your budget with a different mobo, case, and cooler and upgrade the GPU at most and stay in budget.",Positive
Intel,Bit over budget  https://pcpartpicker.com/list/HfKWh7   In budget  https://pcpartpicker.com/list/jJCGC8,Neutral
Intel,"you can shave of like $25 off the SSD with a 2tb $100 SDD instead. You can also shave 17$ off the cooler by getting a regular assassin instead of the double radiator version, since you don't need a double radiator to cool the 7500f. And you can save money on the power supply by opting for a 650w supply instead. Gives you plenty of wattage overhead as well. All those savings can allow you to get a better GPU like the Radeon RX 9060XT 16gb",Neutral
Intel,"Could also do a 7700xt, slightly worse performance and 4gb less vram, $50 cheaper  https://pcpartpicker.com/product/97kH99/xfx-speedster-swft-210-core-radeon-rx-7700-xt-12-gb-video-card-rx-77tswftfp",Neutral
Intel,"buy a better gpu because it sounds like you're really only interested in playing games at max fidelity, so a good GPU will take you way further than any monitor currently can. IMO, a good/great monitor should be the last thing that you get because if max frames is the goal, it's smarter to see where your personal cutoff is (in regards to resolution/fidelity with your current setup) and whether or not you're really wanting to buy pricier hardware to max out your current monitor or upgrade to a monitor with higher specs or a different panel type like W/QD-OLED",Positive
Intel,I mean in short i have 2 choises intel arc B580 and QD/W-Oled monitor or Rtx 5070/5060 ti and a normal not good not bad monitor but is the monitor worth it? cause im not sure is an OLEd that big of an diffrence,Neutral
Intel,"There is definitely a noticeable difference between a nice IPS panel monitor and a QD/W-OLED monitor, there's absolutely no debating this. But where objectivity ends, subjection begins and this is where your personal standards come into play. For me personally, growing up playing games on low resolution CRT monitors, I only care that I can play my games of choice, so I would personally be happy with a decent IPS monitor because the games won't care if I'm playing on a nice monitor or not, but they will care what hardware I'm playing on. So I would go with the 5070/60 Ti and a decent monitor. The B580 will struggle more with a nicer OLED panel anyway due to the more demanding specs.",Neutral
Intel,A B580 won't be able to properly show off such a nice monitor because it will run so slowly and you won't be able to turn the graphics settings up.,Negative
Intel,I mean i can test the oled and refund it if its not good enough,Negative
Intel,I mean with some ai FG i have 80 frames with high/cinematic settings in BMW its not that bad,Neutral
Intel,"You can, but you'll end up with initial disappointment because you don't know what you want yet. I'm giving you advice that will help you figure out what you want. But I guess I can't force you to take it. Good luck on your search.",Neutral
Intel,Thanks for Gl ill propably test it and refund if its not good enough,Negative
Intel,"Everybody was talking about it when it came out. At MSRP, it’s a good value   It was just a bitch to get because it sold out instantly everywhere. I’m sure that’s changed now. It’s a good purchase   It also requires a decent CPU, that’s the only issue",Positive
Intel,They make up something like 2% of the total GPU market.  You don't see a lot of talk about them because so few have them.  They are ok for what they are.,Negative
Intel,They’re really good for what they are and are pretty universally well received.   They’re budget tier cards with a limited lineup that simply don’t have the same recognition and acclaim as Nvidia or AMD.,Positive
Intel,"The Intel arc GPUs used to be avoided because of the drivers. Some games straight up don't work with them, some had way worst performance than the competitions. Intel's driver has improved leaps and bounds, it's a great one to get unless you have a really dated CPU.",Negative
Intel,"I just got one and it’s awesome, $395 AU and I got a nice increase at 1080p. Made it very cheap to upgrade!   I think my end game is to try get the next line of graphics card at early retail price, or just hop on the next Intel… tbh done me great. The only problem I have had so far was tabbing out of helldivers caused a few glitches on my mates end for the 15 seconds I was gone.",Positive
Intel,"Their A series gpus had some major issues alongside drivers still being pretty raw. The a310/380/580 have some niche uses in video encoding or for driving 1080p displays.   The B series cards have turned out to be really good budget gpus for 1080/1440p. The drivers have come a long way since launch, theyre still not as refined as Nvidia but theyre mostly stable now. They both require more modern platforms to perform well but not the top end hardware alot of reviews claim they need. The b580 has had alot of issues staying in stock since it offers 12gb of vram at a price point that almost exclusively offers 8gb cards.  Ive had my b580 since around launch. I got it on launch day for MSRP and aside from an unstable driver update i had to roll back its been a great upgrade from the 1070 i had previously. If you play on 1080/1440p displays at high settings theyre good cards",Neutral
Intel,"Intel arc a310 is one of the best cards for transcoding videos, can handle multiple 4k streams. I love mine, using it for plex server and immich",Positive
Intel,It's good,Positive
Intel,No they aren’t. The b580 in particular is praised for its great value at its price range,Positive
Intel,As long as your CPU and motherboard supports RE-Bar it's a great option for 1080p/1440p for the price,Positive
Intel,Idk but I take it as a good sign because people will only post if there is something wrong with the product. This also applies to other subreddits.,Neutral
Intel,"They're good and I have one that I use for a rig that only plays games like esport titles and multiplayer steam games. They aren't the best for any type of AAA game.  The bigger deal is that intel has announced some pretty heavy changes and cuts coming and there is good speculation that intel's gpu sector might be cut. If it doesn't happen, it's no big deal, but if it does get cut or restricted, any type of updates or driver updates that are needed for future releases will probably be unlikely to come out. That's probably the only reason I wouldn't recommend it to anyone, but for the price its great for someone who only enjoys current multiplayer games or indie games.",Neutral
Intel,"Intel Arc has physical hardware that works ok, but it's not high performance yet.  But MOST importantly, this is a new company in terms of graphics drivers and industry cooperation in software development. So what? NVIDIA and AMD partner with game developers to optimize graphics, sample and super-sample textures in AI for optimization for best performance. This is an ongoing development of specialized software and tools to improve performance to the maximum with the given hardware. This software is NOT available to Intel and they will have to spend years catching up to their competition.  In short, Intel graphics cards will work fine for low demand popular programs. They may eventually get there for high end performance, but it will take years for them to catch up.",Positive
Intel,"Have you considered 9060 xt at all? There has been a price decrease recently, you might be able to get one for the same price as a b580 (8gb model 9060 xt).  Except for the vram department, the 9060 xt is pretty much better in every way and performs better too.  Was on the same boat with you. The b580 was $360 CAD, and the 9060 xt 8gb was $380. After going through countless benchmarks (especially at 1440p), I eventually decided to pick the 9060 xt sapphire.   In only like 1 or 2 games the b580 scored higher, and everything else was in 9060 xt’s favour. Idk much about the driver issues on the b580, but at least the 9060 xt seemed like the safer bet. I wish it was at least 10gb though…. Amd really missed an opportunity. Otherwise this gpu is a beast! I’ve never seen an 8gb gpu do so well at 1440p. Amd did score with that.",Positive
Intel,They are good for 1080p gaming also if you have a really low budget and want to game. For slightly more money you can pick up a 9060xt 16gb which is good for 1440p and has better long term returns.,Positive
Intel,"They're okay, if you hate yourself get an Intel Arc GPU otherwise get an AMD Radeon GPU or if you have the money to burn literally and figuratively speaking an Nvidia GPU.",Neutral
Intel,I think better price to performance can be found on a used card.,Neutral
Intel,"Has driver issues with more dated and/or less popular games, but they've made it a point that major AAA titles and esports games usually have good drivers. It also basically requires a fairly beefy CPU and it will at least slightly negatively impact the CPU performance too. But when I say fairly beefy I basically just mean AM4 5000 series or basically any AM5 CPU even the 7500f, I personally don't know the Intel comparison but I imagine any Core Ultra or 12th gen would absolutely crush it and be fine.  But the B580 is basically competing in the 60 series tier and is alright, but definitely if you can get it for MSRP or within I'd say even $30 over MSRP it's a steal. Something of note too is at least from my experience and seeing benchmarks, the Arc GPUs normally do this split the middle kind of thing, where they tend to outraster/render Nvidia GPUs of their tier, but get outrastered/rendered by AMD GPUs of their tier, but on the other end XeSS upscaling/frame gen performance and raytracing usually outperforms the comparable AMD GPUs, but loses to the comparable Nvidia GPUs. Although FSR4 and the better raytracing on the 9000 series might've changed that, a lot of those comparisons were from when we were still comparing to the RTX4000 and RX7000 cards.",Neutral
Intel,B series is getting a bit old now. Eagerly awaiting what the C series will bring.,Neutral
Intel,No idea about the drivers but B570 and B580 which released some months ago are very good value and also very efficient.,Positive
Intel,"The B580 is actually very good for a $250 card as it performs similarly to the 4060 and more importantly has 12GB memory instead of 8GB. Even today it's still better value compared to the 5050 at the same price.  The main issue with the card is availability in certain regions, which impacts it's price. Where I'm from (Bangladesh) availability is actually pretty decent and so most B580s can be had for at or near the $250 msrp. The other issue with the card is it's driver overhead, which mostly shows up on the AM4 platform where the performance delta can sometimes be so massive that the card ends up slower than the 3060 12GB.  The issue is far less rampant on AM5. It performs around the 4060/5050 on a cheap R5 7500F, and is already beating the Nvidia cards in terms of value. It can still go faster if you pair it with the fastest CPU on the planet, but not by much and I personally don't think it's that big of a deal.   Alternatively you can play at 1440p and eliminate the driver overhead; the card has the performance and memory for it anyway, so why not. You can even go all the way upto 4k Balanced with XeSS 2.1 which is actually pretty good and supports frame gen. Kinda insane for a $250 card if you ask me.  Driver support and game compatibility is nowhere near as bad as Arc Alchemist, and people have even played the Battlefield 6 Beta on day 1. Older games such as those on DX9 or DX10 may still struggle compared to a 4060/5050, but the card is so fast you should still get highly playable framerates. We're talking 150 fps instead of 300fps, big deal.",Positive
Intel,No one buys them because people truth internet myths even from years ago but the reality is that those are very good gpus and near to MSRP are a great value.,Positive
Intel,"They're decent GPUs but for gaming yo'ull want something a lot more powerful, like an RTX 5070 or RTX 5070 Ti.  [https://www.youtube.com/watch?v=VcKPJvGNr0c](https://www.youtube.com/watch?v=VcKPJvGNr0c)",Positive
Intel,Nobody talk about the A series anymore. Is it still worth to buy?,Negative
Intel,"You must think all of us can afford Nvidia. In Europe you guys probably can, but in America all of us have millions in debt from medical bills and we make minimum wage!",Negative
Intel,So for exemple an 7500f is ok ?,Neutral
Intel,"> unless you have a really dated CPU.  Read: more than a generation or two old. So, not actually dated at all, which is why the overhead issues with Battlemage were so criticized.",Negative
Intel,"I ran an a750 in Linux until Monday. It worked well.  Rarely had problems.  I just wanted more vram so I got a 9060xt. If Intel had an upgrade path for the a750, I would have bought one.",Positive
Intel,Idk if I'd call Nvidia refined either.   I was still using the 566.36 driver until the Battlefield 6 beta made me update and since then I've encountered the flickering display and sudden black screen issues a few times that have been going on with pretty much every driver version since the 50 series came out.,Negative
Intel,"Rebar isn't all it needs. It needs a good CPU. I bought one to replace a 7600, paired with an i5-10400. After extensive testing it didn't really offer any improvement, and even performed worse in CPU-heavy/multiplayer titles. I thought the overhead issue was overblown, it isn't.",Negative
Intel,It really cant,Neutral
Intel,Like which one ?,Neutral
Intel,B series launched literally under a year ago.,Neutral
Intel,Depends. For gaming? Not really. Better bargains. But for a home server the a310 is the champ of media encoding,Neutral
Intel,"In gaming, the B series will generally be better in pretty much every way. It just runs faster, cooler, and cheaper. Outside gaming it gets a bit weird, but actually favors A series. The 310s are *very* good at media encoding for the price. The A770 can also outperform the B series in some non gaming tasks (generally its inferior, but it can get 4gb more vram and has a wider bus, which can make it better, or sometimes close enough that it being older and thus cheaper can make up the difference.) For gaming, B series is strictly better",Positive
Intel,The b580 is close to the a750 in performance. Not worth buying that. The a770 is better but I wouldn’t recommend them now with the amd 9060xt out.,Negative
Intel,"The median net worth in the US is 200k and the US accounts for more of Nvidia’s revenue than the rest of the world. Speak for yourself, big bro.   Regardless, what even prompted you to say this? I’m just answering OP’s question about Intel’s popularity lmao",Neutral
Intel,Totally fine.,Positive
Intel,"Given you dont need to run DDU or (in some cases) do a fresh install of Windows to get the cards to run decently id argue theyre pretty well refined.   Even compared to AMDs drivers Nvidias just tend to work without much effort, from what ive been seeing it sounds like bf6 has had a ton of issues regardless of which gpu youve got. Having a set of rough updates off the back of 20+ years of development doesnt discount the amount of time theyve had to work on getting things working well. Intels 1st dgpus came out in 2022 IIRC",Neutral
Intel,Sure it can. You can get a 3070 at the cost or a 3070 Ti for a little more and it beats the B580.  I just sold my 3070 Ti for $290 a month ago because that's all people were willing to buy at. Some sold the card for even less.,Positive
Intel,"A used b580 is better price to performance, they’re just kinda speaking arbitrarily",Neutral
Intel,3070 or 3070 Ti. One may cost more. I sold my 3070 Ti for $290 and others are selling for less.,Neutral
Intel,"Wrong, the B580 outperforms the A770 in every case i know.",Negative
Intel,You’re comparing the used market to the new market. You can get a used B580 for $170 on ebay as the first listing I see.,Neutral
Intel,"Not arbitrary with my assumption they want to spend $250 on a new B580. I don't know for sure they wanted to go new, but in case it was their plan, I'm saying instead of spending $250 on a new one if that's what their plan is, just spend $250 on a used card which will have better performance. It's really common sensical as to what I am saying. Don't understand what is arbitrary about that.",Neutral
Intel,"Wrong. The A770 has more VRAM and works better for AI/ML workloads.  That's why they're releasing other cards to help with loading models.    As for gaming performance, the A770 and B580 are very close in most benchmarks.  It's game dependent which one does better, but in scenarios more vram is needed the A770 wins.  12GB isn't terrible for 1080p/1440p workloads but it's just OK.",Neutral
Intel,"Indeed, that was my intent and my assumption that OP was going to go for new. $250 spent on a used card is spent better than on a new B580.  I don't think this point is invalid to the point where it should be downvoted.",Neutral
Intel,"Ok… but if OP is willing to buy used cards, $170 is better spent on a used B580 than $250 on a 3070 as you can spend the extra $80 to get other components that are better as well",Neutral
Intel,"Sure this is true, but I was only talking about going for a brand new B580 versus a used GPU. I naturally assume at such a low price point that people will go brand new on that GPU rather than used.  You can argue different scenarios with a different good answer, but what you stated wasn't exactly relevant to my assumption.",Neutral
Intel,"Yes, but your assumption is inherently flawed is all I’m pointing out.",Neutral
Intel,"No, my assumption isn't flawed when I had no evidence to suggest that OP was going used on that card. How can I possibly know what OP intends on doing without it having been explicitly stated? I would personally expect them to go for a new card and not used as the card is already cheap enough to get it new (without considering budget constraints, but they also never stated any details on a budget and other essential info).   Hindsight and what comes after the comment was made is different, but my comment was based entirely on a very specific scenario. I think it's clear from my comment that I was specifically discussing differences between a new B580 and a different used card. I wasn't talking about anything else.",Neutral
Intel,"Your comment here already dismantles the scenario you thought of. You claimed you assumed they would be going for a new card. This assumption automatically means that going for a used card is out of the question, so comparing the new card to the used card doesn’t make sense. If OP were to be considering going used on cards, why would they not consider going used on the B580? The problem is that you didn’t think to expand the logic you presented beyond one specific card, which is inherently flawed.",Negative
Intel,For what purpose? Because if gaming is your goal this is not the play unless you’re refusing both AMD and Nvidia graphics for ideological reasons.,Negative
Intel,"So my goal is to get you to a RTX 5070 which requires $280 of reallocation.  - Downgrade the CPU to a 9600X. $160 saved  - You have three M.2 slots on your motherboard, you don’t need 4TB to start. Drop to 2TB $130 saved.  - Replace the Arc with an RTX 5070  Alternatively you can make a less severe CPU cut with the 7700X ($60 saved) and get the other $100 from your peripheral budget because frankly you don’t need $500 for all that. You could also downgrade the case to something with less fancy rgb (Montech XR, Phanteks XT View) but I understand some folks are very passionate with their case selection.",Neutral
Intel,It is a little cheaper and I do a little bit of video editing on the side!,Neutral
Intel,"This is honestly really good advice, I might make the jump to a 5070! Thank you!!",Positive
Intel,"The first release might be sold with a new PC. If you live in US, you might as well go to China and buy one there (or one of their online stores). Tarif and scalpers might jack up the price by a lot. This is not Intel, but he went to Hong Kong to buy the rare 5050 and found a few of them easily. https://youtu.be/-jXMaS8e8oo?si=6M1XiK54SrVDI3h9",Neutral
Intel,"For the 48GB Maxsun dual variant of the Pro B60, I was quoted $3,300 New Zealand Dollars (NZD) per GPU. Not available in retailers, only to business customers through limited suppliers. The supplier who services my area doesn't even have a website.   For comparison, the RTX PRO 5000 with 48GB is $8,800 NZD, and the cheapest RTX 5090 is around $4,650 NZD at the moment.   I was interested initially because of the RAM in the dual Maxsun card, but was hoping for a price closer to $2,000 NZD.",Neutral
Intel,Why specifically the Intel when plenty of GPUs aren't going to interfere with AI.,Neutral
Intel,That's an interesting idea :) I'm not in the US but going to China to buy some hardware sounds very cool! Maybe I'll do it some day. Thank you.,Positive
Intel,"My idea was to buy a singular one now and another one later when I have the money for it. But dual variants look cool too if you can afford it. $2000 USD sounds too much though. I feel like the supplier in your area is jacking up the prices since it is difficult to find them.  I asked a friend of mine who does business hardware deals. He couldn't even get a quote. I guess it is not even available on many regions now.  Anyway, thanks for the info :)",Positive
Intel,"I was waiting on this, meanwhile bought 5070Ti Vanguard - with vapor chamber and I am really surprised how Nvidia moved with AI workload on this gen, so Intel is no go, It's even better for me to have Scout locally in VRAM and RAM when 5070Ti has so much performance - in FP4 it's lighting speed. Waiting on 5070Ti super - 24GB card with almost 1TBps VRAM bandwidth.",Positive
Intel,24GB VRAM for 500$ sounds super nice and not offered by any other GPU manufacturer. Plus they advertised a card with dual B60s (48GB VRAM total) sharing one PCI-E x16 slot (x8 each).,Positive
Intel,"I went down a similar route and ended up with a 5090. Less RAM than the Maxsun card, but the performance is insane.",Positive
Intel,"Then price the GPUs that have Tensor cores. I'm just wondering because Intel GPUs aren't traditionally for gaming, though that is slowly changing",Neutral
Intel,"Tensor cores is not the only thing that matters when it comes to performance. B60 performance was looking pretty good in the demos but now there is nothing on the internet, no benchmarks, no prices, no people with crazy connections showing off a hardware others cannot buy, nothing.",Negative
Intel,You have a very narrow vision of speed. What you are saying is like your truck is faster than my audi because it has more horsepower. Nvidia is the top dog right now but that doesn't mean you get more from nvidia for 200$. VRAM size and speed is something you cannot ignore. If you have to load half of the model to system memory it is gonna decrease your speed significantly. And guess what? If you can't fit the model into your memory you won't be running it even if you have a billion cuda cores. This is why intel is offering 24GB VRAM for 500$ while you need to pay 2500$ for the same VRAM size in nvidia (rtx a5000 for example).,Negative
Intel,"B60 is similar performance like 5060 in language of HW numbers. In language of TOPS it's slow, because CUDA... and FP4 optimisation of 5xxx cards... No, you really don't want to screw around for days to make OpenML or how is theirs ""engine"" called working...",Negative
Intel,"But compared head to head with an nVidia or AMD/ATI of the same price and the Intel is not the top performer, last I checked.   I looked into this recently because I was thinking about upgrading.  $200 for that versus $200 for an nVidia/AMD and the non-Intel GPU was a slightly better performer.",Negative
Intel,"But compared head to head with an nVidia or AMD/ATI of the same price and the Intel is not the top performer, last I checked.   I looked into this recently because I was thinking about upgrading.  $200 for that versus $200 for an nVidia/AMD and the non-Intel GPU was a slightly better performer.",Negative
Intel,"No i don't have a narrow vision of speed. Maybe ask questions first. Like i said, i read reviews, Intel didn't break through. Maybe I'm incorrect, but i could care less. Reviews change daily. And i could find a dozen that support my opinion. Not like i care. Intel said it wasn't primarily intended for gaming.  What's worse is your rudeness.",Negative
Intel,"Yeah, sure mate. You do you.",Neutral
Intel,9060xt 16gb,Neutral
Intel,Thats about 100$ more expensive that the two I mentioned. Is the improvement worth it for the price? If not I would rather go for the lower price range.,Negative
Intel,yes you can run games at 1440p fine,Positive
Intel,"That motherboard looks solid, definetly a good value combo. I made a couple changes to your list for a better case (a bit bigger, more airflow, more fans), a better power supply for the same price, and a slighty better cooler that's a few bucks more.  [PCPartPicker Part List](https://ca.pcpartpicker.com/list/RNTdVF)  Type|Item|Price :----|:----|:---- **CPU** | [AMD Ryzen 5 7600X 4.7 GHz 6-Core Processor](https://ca.pcpartpicker.com/product/66C48d/amd-ryzen-5-7600x-47-ghz-6-core-processor-100-100000593wof) | $500.00  **CPU Cooler** | [Cooler Master Hyper 212 Spectrum V3 71.93 CFM CPU Cooler](https://ca.pcpartpicker.com/product/jgbRsY/cooler-master-hyper-212-spectrum-v3-7193-cfm-cpu-cooler-rr-s4na-17pa-r1) | $20.99 @ Memory Express  **CPU Cooler** | [Thermalright Assassin X Refined SE ARGB CPU Cooler](https://ca.pcpartpicker.com/product/jvjv6h/thermalright-assassin-x-refined-se-argb-cpu-cooler-ax120r-se-argb-d6-cad) | $23.90 @ Amazon Canada  **Motherboard** | [MSI PRO B650M-A WIFI Micro ATX AM5 Motherboard](https://ca.pcpartpicker.com/product/cRQcCJ/msi-pro-b650m-a-wifi-micro-atx-am5-motherboard-pro-b650m-a-wifi) | $0.00  **Memory** | [Kingston FURY Beast 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory](https://ca.pcpartpicker.com/product/q7LdnQ/kingston-fury-beast-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-kf560c30bbek2-32) | $0.00  **Storage** | [Kingston NV3 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive](https://ca.pcpartpicker.com/product/tmbRsY/kingston-nv3-1-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-snv3s1000g) | $78.48 @ Amazon Canada  **Video Card** | [Intel Limited Edition Arc B580 12 GB Video Card](https://ca.pcpartpicker.com/product/Kt62FT/intel-limited-edition-arc-b580-12-gb-video-card-31p06hb0ba) | $358.98 @ Best Buy Canada  **Case** | [BitFenix Grafi ATX Mid Tower Case](https://ca.pcpartpicker.com/product/zXFCmG/bitfenix-grafi-atx-mid-tower-case-bfc-grm-kkgsk-4f) | $49.99 @ Canada Computers  **Power Supply** | [Super Flower Leadex III Gold UP 850 W 80+ Gold Certified Fully Modular ATX Power Supply](https://ca.pcpartpicker.com/product/R4RwrH/super-flower-leadex-iii-gold-up-850-w-80-gold-certified-fully-modular-atx-power-supply-sf-850f14ge) | $119.99 @ Newegg Sellers   | *Prices include shipping, taxes, rebates, and discounts* |  | **Total** | **$1152.33**  | Generated by [PCPartPicker](https://pcpartpicker.com) 2025-08-18 16:25 EDT-0400 |",Positive
Intel,Thanks for the reply. My main concern for this motherboard is VRM and/or Power design (don't know whether they are same or not). The Power design on this board is 8+2+1.     Also thanks for the suggestion will definitely look into it.,Neutral
Intel,"The P3 Plus uses QLC flash, which has low durability and becomes noticeably slower after a few years. Get a better drive with TLC flash like the Kioxia Exceria Plus G3, TeamGroup G50, Intenso MI500, WD Blue SN5000, or Verbatim Vi5000.",Neutral
Intel,ASrock b850 motherboards have killed x3d cpus recently. While your 9600x should be fine a future x3d chip that you may want to upgrade too years down the line may not be. I’d look at a different board for your “future proofing” spec.,Negative
Intel,"pcpartpicker is not great with german pricing imo, use geizhals instead.  [https://geizhals.de/wishlists/4616811](https://geizhals.de/wishlists/4616811)  here i have assembled a very similar setup with a slightly better cpu and bigger ssd for \~1050€ with shipping.",Negative
Intel,"Thanks m8, I didn't know this. Already looked up your suggestions and they have a decent price, I will adapt my build accordingly.",Positive
Intel,"Oh my, thats an important information 😅, thanks for the info, I will look for an alternative",Positive
Intel,"Oh sweet, thanks m8👌I really like the cpu upgrade",Positive
Intel,"yes just get a new gpu. The cpu is still alright. Also make sure the psu is good, check the model.",Positive
Intel,Do you have a suggestion on a good gpu should I get one of the newer ones I mentioned in my post or something older I searched around and found some decent deals on gtx 1080 gpus.,Neutral
Intel,"the 1080 is outdated. The one you listed are good, see what deals you can find",Positive
Intel,"Okay, need to do some price checking thanks for the help.",Positive
Intel,Rip gpu market,Neutral
Intel,https://preview.redd.it/b50eap3fmzpf1.jpeg?width=1078&format=pjpg&auto=webp&s=aebc215f68b6c6444a8e247dfed48d9eed711dd0,Neutral
Intel,"That’s the last fucking thing I want, an Intel cpu with an integrated Nvidia gpu. Horrible.",Negative
Intel,"[Intel and AMD have done this in a NUC before.](https://www.tomshardware.com/reviews/intel-hades-canyon-nuc-vr,5536.html)  Edit: My educated guess is that this comes from Nvidia's ARM APU, and ARM Windows just isn't there yet. So, to move it to an x86 platform Intel would be the logical choice to partner with to move the APU to x86 Windows.",Neutral
Intel,All I know is we gonna lose,Neutral
Intel,I have been saying for quite some time Intel will abandon Arc. They abandoned the GPU market in the past.  It is past embarrassing at this point what Apple offers with the integrated GPU in the M series of cpus and what Intel delivers.,Neutral
Intel,Let Intel Suffer for all the BS they did with their CPU division while they were a Monopoly and even after they declined still continued their BS  But i like Intel GPU division,Negative
Intel,Even before this announcement Intel Arc faced an uncertain future. Even their most successful GPU the B580 at the very least had razor thin margins if not outright being a loss leader.  Intel simply ran out of both time and money and they can't keep bleeding money on their GPU division with the company's current situation. If Intel actually managed to execute and follow their initial roadmap and release Alchemist GPUs during the COVID pandemic we'd probably be looking at a very different Intel GPU division but here we are now.,Negative
Intel,I don’t necessarily think that it’s that dramatic. The Arc GPUs are aimed at a market that Nvidia no longer competes in (the budget GPU space). If anything you’d think Nvidia would be more supportive of the Arc stuff getting off the ground because it means they can dedicate more fab space to producing the higher end stuff that’s in demand for AI and industry use and commands better margins.,Neutral
Intel,I wonder if this is specifically to go after the handheld pc gaming space where amd basically has a monopoly.,Neutral
Intel,"This is great for people who just want a gaming system, no AI. Cheap motherboards with decent integrated graphics would be great for a lower income. (Most of the gamers I know)",Positive
Intel,That would be an incredibly silly move to leave almost the entire A.I. golden goose for Jensen to eat.,Neutral
Intel,Someone is cashing out for both INTC and NVDA.  Intel traded a shitty incompetent CEO for a conniving one.,Neutral
Intel,I truly hope Intel won't drop their GPU division.,Negative
Intel,"Such a weird thing going on in this thread. Did people forget when AMD acquired ATI it was to make a complete package APUs and for both respective companies to compete with Intel and Nvidia.   Now Nvidia probably wants to do same thing in the CPU market and intel gets the benefits of being with Nvidia in the GPU market.  Only reason it did not happen sooner was because Intel was still king marketwise then.  Intel could have easily acquired Nvidia in 2006 when AMD acquired ATI, or even in 2011 when AMD stopped using ATI branding.",Neutral
Intel,NOOOOOOOOOOOOOOOOOOOO!   Whatever,Neutral
Intel,:(,Neutral
Intel,Low end vs high end. I think it will be fine.,Positive
Intel,If only intel aggressively sell their GPU in third world countries but no they only focussed in America where except 10 redditors everybody buys nvidia,Neutral
Intel,Should have done it with amd instead of this dying pig,Negative
Intel,Arc was never even a player.,Negative
Intel,Then surely no one will buy it.... right?,Neutral
Intel,"I think it would be pretty sweet, if they could cook up something like the apples M series.",Positive
Intel,Uh...why?  Nvidia have consistently had the most power efficient GPUs on the market for years now. That technology in an APU? That's promising stuff.  It wouldn't surprise me if we start to see 4050/4060 performance on a single chip. Imagine what that means for handhelds.,Positive
Intel,"Speak for yourself, that sounds fucking great.",Positive
Intel,That actually sounds amazing for handhelds,Positive
Intel,"its for ai accelerators with built in CPUs, not apus",Neutral
Intel,Unfortunately a majority of gamers/pc users in general don't give two shits about the specifics of their hardware.,Negative
Intel,"Why? AMD makes inferior products in every way compared to Nvidia, except for GPU power connectors.",Negative
Intel,"kinda, Kaby Lake G wans't really an APU, it was just a CPU and a GPU on the same package.",Neutral
Intel,Imagine if Apple suddenly turned round and decided to make a Mac for gaming 🤣,Neutral
Intel,"Companies aren't your friends, AMD being on top means they can pull the exact same shit that Intel did in the past.",Negative
Intel,"How would they suffer? Intel isn't a living entity, and the executives that led the downfall of Intel left, who is gonna suffer except intel's employees and us?",Negative
Intel,"> But i like Intel GPU division    The problem with the Intel GPUs is they are clearly struggling with the technology. The B580 needs a much bigger die than equivalent Nvidia cards, meaning it must be expensive for Intel to make the chips and uncomfortable to sell them at such a low price.   That might be ok initially to get a foot in the door for their GPU brand, but if they can't get the efficiency up then from a business point of view they just can't continue it.",Negative
Intel,Nvidia still sells gt710 and and rtx3050 for the low end,Neutral
Intel,"Make no mistake, this will be marketed for AI before gaming",Neutral
Intel,"Yes but AMD didn't have a GPU division when they bought ATI last I checked, what became of ATI is AMD's Radeon division, CPU or GPU both remained with two major players. The problem here have here is that Intel IS a player in the GPU space, they are the opportunity to have a 3rd company for extra competition but Arc is still in a vulnerable stage where it could easily get axed if it isn't bringing in more money than what they spend. Nvidia investing and putting in RTX iGPUs is the sort of thing that could devalue Arc iGPU investment.",Neutral
Intel,Maybe they want competition. Unlike NVIDIA now wants to have 100% share to everything. Monoply at its finest,Neutral
Intel,"People may not like it but you're right, they essentially had a 0% market share the quarter following the release of their Battlemage GPU's",Neutral
Intel,"whats with the downvotes lol, dudes right, fellas time to step outside yer echo-chamber & touch some grass",Neutral
Intel,Already sold out,Neutral
Intel,"Woops, it's in every laptop ever.",Neutral
Intel,"Intel's mobile CPUs are alright, maybe this could be a responsive to AMD's gigantic APU.",Positive
Intel,Nvidtel: best we can do is Kaby Lake-G.,Neutral
Intel,"Never gonna happen. NVIDIA will put that kind of effort into their own CPU but not an Intel one. This is doomed from the start, we’ve seen it before several times when big tech companies “collaborate” and the product just languishes and dies. The PowerPC CPU architecture and OS/2 leap to mind.",Negative
Intel,"AMD fanboy - thats why. Doesn't like the ""enemies"" joining forces.",Negative
Intel,"That's roughly the power on the amd ai 395+ max whatever the fuck that chip is called. It's mostly ai oriented, not gaming, but the apu has 4060 or above performance in most games.  The real question is whether nvidia would make such an investment for handheld devices which they dont seem to care for all that much, or mabw they just want their foot in the ai tablet market where amd has taken the lead.",Neutral
Intel,"I was obviously speaking for myself. In English, we use the “I” to refer to ourselves. I would have used the word “you” if I were talking about you.",Neutral
Intel,Lol why? Intel sucks at making CPUs now.,Negative
Intel,"It's both, [per Nvidia and Intel's own joint press release](https://nvidianews.nvidia.com/news/nvidia-and-intel-to-develop-ai-infrastructure-and-personal-computing-products?ncid=so-twit-672238):  >For data centers, Intel will build NVIDIA-custom x86 CPUs that NVIDIA will integrate into its AI infrastructure platforms and offer to the market.  >**For personal computing, Intel will build and offer to the market x86 system-on-chips (SOCs) that integrate NVIDIA RTX GPU chiplets. These new x86 RTX SOCs will power a wide range of PCs that demand integration of world-class CPUs and GPUs.**  Which, personally, I think would be an interesting product line.",Neutral
Intel,He said with his 3080 Ti that draws 100 W more than a 6900 XT for the same performance,Neutral
Intel,"Yeah, I was just saying Intel has worked with direct competitors before.",Neutral
Intel,"Their computers can already technically game, just at worse performance than expected. Most developers don't want to develop their games to include direct support for Metal in addition to Vulkan, Directx and/or whatever system consoles use, and Apple doesn't want to directly open their OS to Vulkan or Directx. Everything needs to go through metal in some way.",Negative
Intel,"True like now that they are at the Top where are Ryzen 3? Where are CPUs for the Budget Gamers? Im have money so i dont need ryzen 3.  But still Intel deserves what they did and even AMD isnt as dumb to do this:  \-Force Mobo Upgrade every CPU Gen   \-Limit Users too 4 Cores 8 Threads   \-Deliver minimal Generational Upgradea while doubling the Price   \-Rebrand Old CPUs with new names and increase Power draw so much it destroys itself   \-Limit/Exclude Tech like ECC, PCI-E Lanes for CPUs under 500€   \-Israel",Neutral
Intel,"Yup, I'm someone with an all AMD build, and Intel going down the shitter and the possibility of Arc being axed is not something I'm happy about at all.   Even if I was one of those weirdos with diehard brand loyalty and didn't care about a monopoly/duopoly being solidified, it's still going to suck for me as an RDNA3 user because Intel is still the only only making an (officially supported) upscaler for my card that isn't completely dogshit.  There's still no guarantee for official FSR 4 support on RDNA 3, and if that never happens and XeSS gets axed, I'll effectively be stuck with the god awful FSR 3 for any multiplayer games I can't use Optiscaler on.",Negative
Intel,like when was the last time AMD increased the core count on their desktop lineup y.y  they kept R5/R7/R9 at the same core counts since introduction,Neutral
Intel,They already are. zen 3-4's uplift came almost all from node change,Neutral
Intel,"I know American companies like to shit the bed, does Chinese tech companies have the same history?",Negative
Intel,tell me how Intels Numbers are doing. Why do they need the state too pump money into them now?,Neutral
Intel,They made enough profit while delivering the most minimal Generational upgrade back when they were the Monopoly. They should have invested that money into the GPUs instead of a Paycheck for the CEO and A Paycheck for Israel and building a Plant in Israel.  Let them suffer,Negative
Intel,Once those are gone they’re gone.  Nvidia won’t waste TSMCs limited wafer capacity on budget chips at this point.,Negative
Intel,Yeah Nvidia needs a cpu for their AI servers that’s better than their own arm chips. That’s probably what this is about,Positive
Intel,Why would any company want competition?  Competition is only beneficial for the consumer.,Negative
Intel,"They were actually eating some of AMD's shares, Nvidia didn't budge",Neutral
Intel,"You’re not wrong, but that’s not entirely the point. It was a sign that a big company like Intel was willing to dump significant resources into R&D in a new and heavily dominated market segment. The A series was a good if not sloppy start (general driver & performance issues, many were fixed over time), and the B580 was an amazing bang for your buck card. They were improving fast in the technology, and while they are still on larger dies than NVDA & AMD they were offering cost-performance in the budget/mid range sector that the others weren’t.  TLDR more competition is pretty much always good for the market",Neutral
Intel,Site crashed,Neutral
Intel,Already at 37% user base in steam charts topping any and all AMD gpus somehow.,Neutral
Intel,14nm++++++++++++++++++,Neutral
Intel,I've been calling those builds (IRL) as Intel-Vidia,Neutral
Intel,![gif](giphy|fngnOeui0oSdSwjZ9K|downsized)  Infidel,Neutral
Intel,"""I"" could tell you were speaking for yourself. In English, ""speak for yourself"" is an idiomatic expression meaning that the person  replying disagrees with the prior statement, typically appended with their point of view.  EDIT: OP of the comment can't take a joke and delete everything.",Neutral
Intel,"Yes, they have done for a few years. It all started with the 14+++++++ fiasco. Their fabs are in a bad way and their CPUs are not where they should be.",Negative
Intel,"Who really cares about power consumption? It doesn’t matter unless you need to upgrade your power supply. Especially if you have enough money to buy a high end gpu, an extra 100 W during the few hours a day max that you’re gaming isn’t going to matter at all. 0.1 kWh * 3 hours * $0.16 = $0.048 extra per day. Oh no!!!  Also, even if the 6900 XT had better performance, I wouldn’t have bought it still because of the extras that Nvidia has. Things like Nvidia broadcast, CUDA support, and DLSS, just to name a few.",Negative
Intel,">  and Apple doesn't want to directly open their OS to Vulkan or Directx. Everything needs to go through metal in some way.  Without buying out MS apple cant support DX. (and even if they did the flavor of DX used by PC titles is not compatible with apples GPUs).  And while apple could support VK, remember VK is not HW agnostic, a PC VK backend (the small number to titles that have these) will be written expliclty to target AMD/NV GPUs and would require rather large changes to run well (or even at all) on a VK driver for apples GPUs on macOS.    The work needed to add a metal backend to a modern engine (if it does not already have one... most do) is not that large.  All modern engines are written to be mutli backend enabled.  The amount of a games engine that directly calls a graphics apis is intently tiny since we need this to run very fast within the Redner loop, idealizing keeping the entire loop within L1 Cpu cache.     Adding metal support to an engine is not were the work is.  All the other changes, user input, file system, Audio, networking, digital signatures, color spaces (Mac users expect proper HDR)... this is all way more work than adding metal.    (also most engines already have metal backends).     from a graphics side of things there is a LOT of work but this work is API agnostic and relates to making changes to target the HW (you would need to do this regardless of API as all the modern apis want to remove runtime overhead of the driver reputedly making these optimizations over and over again on each frame).",Negative
Intel,barely anyone bought the ryzen 3s when they were a thing. budget gamers just bought last gen ryzen 5s because of the higher core/thread counts and better performance.,Neutral
Intel,">Where are CPUs for the Budget Gamers?  They're in China, now inaccessible to US buyers due to recent politics.  I got a Ryzen 8400F for $75 back in March.",Negative
Intel,"Both are publicly traded companies and their sole purpose is pumping up share price up as high as it will go. AMD will definitely take the foot off the gas with Intel continuing to crumble, keep sales up and fatten margins. Intel had to push power draw to try and keep up on performance as they were stuck on an inferior node. Everything else is well on the table for AMD until another company challenges their lead.",Neutral
Intel,We dont need r3s for budget cpus when older amd r5s are just as cheap with similar performance.,Neutral
Intel,"You must not remembered AMD’s socket A, socket 754, socket 939, and socket 940…  And AMD was limiting users to dual cores before Intel came out with C2Q, and selling dual cores at ridiculous prices before Conroe.  If you’re looking for morally better companies, AMD ain’t it.",Negative
Intel,what did intel do in Israel?,Neutral
Intel,didn't intel just come out with the Core Ultra 110 which is literally just a 10th gen cpu that they are trying to trick us into thinking is new?,Negative
Intel,Don't need a new ryzen 3 model when an older gen r5 is dirt cheap.  And who is in the market for a €70 cpu on a €200 motherboard? I like AMD's approach of keeping AM4 for systems with low requirements and AM5 for the high-end stuff.,Neutral
Intel,Mayhaps they'll open source XeSS if/when that happens,Neutral
Intel,"Nope, you’re thinking of the highly sought after 4 and 5 nm TSMC fabs. The 3050 is build on Samsung 8nm and gt710 is build on 28nm.  Which does not have high demand and thus is also much cheaper to manufacture on",Neutral
Intel,"To deliver that price/performance they had to sell at cost, the B580's die is 20% bigger than a 5070 and Intel is selling them for less than half the price. Heck, the thing competes with a 4060 and it's 2x the size.  As long as they can't get even remotely competitive on die size they'll be stuck in this situation and personally I'd rather have them spending money on figuring their shit out on the CPU side to avoid  a monopoly instead of selling at cost low end GPUs than don't work well on low end CPUs.",Negative
Intel,Already reselling at 160% MSRP,Neutral
Intel,I like that!,Neutral
Intel,"He didn't delete anything, he just blocked you. Comments will be shown as deleted to you.",Neutral
Intel,Yeah idioms like that are for speaking in person not typewritten on social media. It’s a meme to make that mistake.,Negative
Intel,"> Who really cares about power consumption?  Hey, you're the one who said ""*every* way"" 😎  > Things like Nvidia broadcast, CUDA support, and DLSS, just to name a few.  DLSS for sure, it's black magic and it might even be worth the $200 premium over the 6900 XT to some. I have a 4080 that I got used, but even at MSRP vs MSRP I would have probably picked it over a 7900 XTX just for DLSS.  The other stuff though, do you actually use it?  ^((also as someone who's owned a 4080 and a 6800 XT within the last 2 years, AMD drivers and auxiliary software are *way* better... but DLSS is so freaking good man)^)",Neutral
Intel,"Oh i did not know this. I thought most of them were normies who had no idea what, what meant so they just bought the cheapest one that works",Neutral
Intel,I do love AliExpress,Positive
Intel,AMD has nothing to do with Israel but Intel does. So already a MASSIV plus for AMD,Neutral
Intel,Yeah i didnt think people would do this. But this is good,Positive
Intel,"I remember having the motherboards exploding, just in time for the cpu upgrade anyway during the early 64 bit transition. Damn those crappy capacitors!",Negative
Intel,difference is AMD has nothing to do with Israel,Neutral
Intel,"many things. Including investing in Israel with and Plant and manufacturing Chips, GPS and other stuff for B\*mbs",Neutral
Intel,They did that more than once. 7th Gen Intel and 14th Gen Intel are the once i can quickly think of,Neutral
Intel,"You do know you can get AM5 Motherboard for as low as 70€ new? And a 8400F for 120. Not even 200€ for AM5 Platform.   Thats the dumbest take i have ever heard. ""AM5 only for High end stuff""",Negative
Intel,They’re no longer being manufactured which is the point.  All 40 and 50 series GPUs are manufactured by TSMC and they won’t waste TSMC capacity on budget GPUs with low margins.  It remains to be seen what will happen with Nvidia’s recent investment in Intel.,Neutral
Intel,"272 mm² B580   263 mm² 5070   According to tech power up.   Die size is not really that important at this stage, they need a better architecture and that will lead to better performance for the die size.   Let's also not pretend like NVIDIA is selling at thin margins, they're profiteering. And were is your source they sell at a loss?",Neutral
Intel,That's a bargain!,Neutral
Intel,yep. amd has pretty much replaced the ryzen 3 tier with their last gen ryzen 5 cpus because of how cost effective they are.,Positive
Intel,AMD has an R&D facility in Israel. Every big company has something to do with Israel.,Neutral
Intel,AMD sold their fabs to a Saudi consortium. I’m sure that’s way better.,Positive
Intel,"I can’t find any sources online saying the 3050 is discontinued. They discontinued the 8gb and replaced it with a 5th revision 6GB some time ago? which appears to be current and still manufactured. There’s never been a time in the last 20 years I’ve been watching the gpu market Nvidia has not filled the lowest tier with older generations of cards. If there’s a shortage of manufacturing capacity at the bleeding edge, they can just use older manufacturing processes so there’s no reason not to fill lower tiers if people are buying them. This is what AMD does for their lowest tier CPUs, they use older manufacturing processes, like 7nm and 14nm",Neutral
Intel,"Die size is important, it's the main driver of the cost. Also the point is not the size itself, it's the fact that it performs like a 150mm die from the competition so it has to be priced accordingly.  Also I never said they're selling at a loss, I said at cost. It's sensible to assume that at 250$ the B580 is at cost or at a really thin margin, the VRAM alone is 80-100$.",Neutral
Intel,Now that Israel attacked Qatar they have been talking with the Middle East talking about arming themselfs against Israel. The Saudis have joined that talk. Things are changing,Neutral
Intel,"Sure but Intel isn't losing because of die size, they're losing because theyre 4 years behind AMD and Intel   It's not sensible because we have nearly zero insider info on the modern silicon market operates.",Negative
Intel,wouldn't it be something if we ended up with a hybrid arc/rtx card.   I could see Nvidia throwing some of the tech at Intel helping them build things up so that they don't get slapped with a monopoly,Neutral
Intel,"TLDR:  > ""We’re not discussing specific roadmaps at this time, but the collaboration is complementary to Intel’s roadmap and Intel will continue to have GPU product offerings,"" Intel said in an emailed statement to HotHardware.",Neutral
Intel,"intell will make next 60 series with 12gb vram, lol.",Neutral
Intel,"Honestly, I don’t see much reason for them to abandon the discreet GPU market. Most of these collaborations are mutually beneficial to both companies but I would suspect the Intel is still going to try and muscle into the AI accelerator GPU market which also means relatively easy to keep doing graphics cards for gaming just like Nvidia is not going to give up their ARM based APU ambitions.   Now I think Intel may shutter the GPU division anyway because they are basically rounding error on the current discrete GPU market, but I do not think it’s a foregone conclusion based on this collaboration with Nvidia.",Neutral
Intel,But the dooomers!!!! what will they be mad about now? considering they didn't even read any of the articles on this colab/plan....what will they be upset about now? Back to Borderlands 4 memes I guess.,Negative
Intel,I could see they just letting Intel run DLSS,Neutral
Intel,"the article says nothing useful tho, just that they will continue to provide GPUs for now.",Negative
Intel,"What did you expect to hear from Intel?   ""Oh yes, we're now abandoning the dGPU sector. Those architectures that we've promised and are almost finished, they're not coming. Also, if you bought an Intel dGPU, well, too bad for you.""  Instead they gave a vague statement, saying they will continue with their current roadmap, which means that yes, we should probably see Xe3 and maybe Xe4... But what after it?",Negative
Intel,I am ok with anything at this point. I want to see AMD/Intel be competitive against Nvidia competition is always a good thing.,Positive
Intel,"They said it's complimentary to their roadmap and they'll continue to offer GPUs. ARC isn't going away because this partnership on SoCs isn't in direct conflict discrete graphics cards. It's targeting products that utilize SoCs, not dGPUs.  But I guess people gotta dig deep into what was said to extract what they want to believe in. But as is tradition here, we can already presume that whatever 'hot take' this sub has, the reality will be complete opposite.",Neutral
Intel,"This would be kinda the opposite of that though… it would be basically NVIDIA reskinned.  Edit: by “this” I meant the situation if NVIDIA were to simply supply Intel with chip or chip designs. Not the article in the post, that instead would be the best for the industry.",Neutral
Intel,Isn't the B580 just straight up faster? It'll also be supported for longer and should have the major teething issues solved being a second gen product versus first gen.,Positive
Intel,The B580 is about 10% better than the A770 for any game in which VRAM is not the bottleneck.,Positive
Intel,"Get the b580, I have a a770 and I want to upgrade, 12gb vram enough for 1080p but 8 is not.its also a bit faster.",Neutral
Intel,"Just a side note here, you may want to opt for the G.Skill Flare instead of Ripjaws as it's EXPO for AMD CPUs.",Neutral
Intel,B580 is more mature. I just got one and so far it's decent.,Positive
Intel,I'd go with the B580. 12GB vram is sufficient.,Neutral
Intel,"The answer is pretty simple, do you want more VRAM or a stronger card?",Neutral
Intel,"The A770 is 16gb VRAM but the B580 is 12gb, isn't that something to consider?",Neutral
Intel,"Actually It's a gift with the motherboard, i was planning to sell it anyway :)",Neutral
Intel,Makes no difference. Xmp can be used on amd boards and vice versa with zero downsides. You will even find xmp only kits in QVL lists for amd mobos.,Neutral
Intel,Obviously stronger.,Neutral
Intel,"You're not gonna be playing at resolutions and texture qualities that would need more than 12GB of VRAM with either of these cards.  But yes, if you're a fiend for max texture quality and 4K, the A770 would probably be able to run the latest and greatest broken UE5 games at a steady 20fps, while the B580 would run out of memory and drop to 10fps.",Neutral
Intel,Then the answer is obvious,Neutral
Intel,"I don't play to go crazy right now just because I saw the Witcher 4 trailer or GTA 6. There are tons of games I haven't played and I kinda wanna play those games before considering, to recap I could buy the B580 now and upgrade to something better in 2-3 years.   Thanks for the insight!",Positive
Intel,">to recap I could buy the B580 now and upgrade to something better in 2-3 years.  Absolutely. You can direct also upgrade the CPU without swapping the board or RAM.   Just don't cheap out on the PSU, get something that's at least B tier and at the very least 650W capacity (though 850W units aren't significantly more expensive and you'll be able to keep using that for multiple generations of your PC)  [https://docs.google.com/spreadsheets/d/1akCHL7Vhzk\_EhrpIGkz8zTEvYfLDcaSpZRB6Xt6JWkc/edit?gid=1973454078#gid=1973454078](https://docs.google.com/spreadsheets/d/1akCHL7Vhzk_EhrpIGkz8zTEvYfLDcaSpZRB6Xt6JWkc/edit?gid=1973454078#gid=1973454078)",Positive
Intel,"Absolutely! I am planning to get an MSI MAG 750W 80+ Gold, although I am seeing Be Quiet as an alternative option.",Neutral
Intel,"consider a platform that is alive or get a used mobo + cpu if on a strained budget, while there is stronger cpus on lga1700 they are plagued by degradation issues. Highly depends on use case too, but a budget and country you're planning to build in would make it easier to give advice.",Neutral
Intel,"Not terrible for a low-end gaming PC. But don't cheap out too much on the motherboard; in general if it doesn't have any radiators on VRMs around the CPU socket, it will struggle to run anything hotter than a 12400 in the future.",Negative
Intel,"A series intel GPUs aren't good for someone starting out, they're a first generation product with first generation issues. B series have solved a lot of the issues, but I wouldn't really recommend that either.  12100F is a pretty slow chip for any serious gaming. You have some upgrade potential, but it's limited to chips that are already now 2 years old and have serious degradation issues. And you're even more limited if the motherboard is of the same budget range as the CPU, as it won't run any seriously powerful 12-14th gen i7.",Negative
Intel,"Could just be a typo, BMG-G21 die vs BMG-321 die",Neutral
Intel,you get free bf6 code if you buy intel 14th gen right? or is it just select retailers/online sellers,Neutral
Intel,"B580's completely fine. I throw them out in the <£700 budget builds. They don't tend to come back, they pass 48 hr burn ins fine, drivers are solid. I'm sure 99% of them spend their entire lives playing Fortnite, Roblox, and YouTube, mind.  I don't know if there's anything different on an Intel CPU, nobody really uses or wants those old things anymore, almost every build this year has been a genuine AMD except some nutcase who wanted a 265K.",Neutral
Intel,"They are good GPUs, but you can get them in the £220-250 range. If you are looking at £300-320 options you will be able to find better GPUs.",Positive
Intel,"I believe so, with overclockers UK you get BF6 with the 14600k and the B580",Neutral
Intel,Is this now the best PCIE power only card?,Positive
Intel,It will continue to have absolutely no effect on the market or this subreddit.,Negative
Intel,"Intel please just set aside the CPU game for a bit, focus all your might on GPUs and save us all.",Neutral
Intel,Context is important,Neutral
Intel,WTF is a B50...  https://preview.redd.it/o20yqe9r07of1.jpeg?width=659&format=pjpg&auto=webp&s=3217fc593219caf45ae9aa9e13cfcbd17893336e,Neutral
Intel,The SR-IOV is the real catch here.,Neutral
Intel,Is it better than a RX 580?  Asking for a friend.,Positive
Intel,I hope they are selling. Surprisingly I want Intel to survive.  We need em,Positive
Intel,Could be a good upgrade for my wife. She has a 1050ti lol.,Positive
Intel,"So this is the Intel Arc B50. It's an *extremely* power limited BMG-G21 with only 16 Xe2 cores (B580 has 20) and only a 128 bit memory bus, limiting memory performance to just 224 GB/s: B580 has 456 GB/s.  To get power down to 70 watts and be slot powered, it has utterly castrated the BMG-G21. Only 4 MB L2 cache is enabled (of the 18 MB on the silicon) and it runs 1,000 MHz slower than it does in B580. Some sources claim 8 MB L2 is enabled, so this appears to be a bit unsure at the moment. It's not remotely B580's 18 MB and Intel's dGPU caching architecture is still a train wreck from its IGP origins.  What Intel has done is given it a similar memory config to AMD's 7600XT, 128 bit GDDR6 and 16 GB of it and cut the BMG-G21 down so far it can barely do anything. It should perform around RTX 3060 Ti, RX 6600, RTX 2070 kind of level.  It could potentially be useful for AI, since Battlemage has strong INT8 performance, hobbyist level AI is all about CUDA and PyTorch. PyTorch will run on AMD fairly well, but Intel has pretty shit compatibility here and the card's performance will be inadequate for anyone wanting to play games on it. Sure, it'll beat an RTX 3050, but what wouldn't?",Neutral
Intel,"I die a little inside at all the people gauging this card by its gaming performance.  This card is the absolute perfect media server/homelab card.   It's cheaper new than a used a2000 12gb while having more VRAM, has enough VRAM to load somewhat decent local LLMs, has powerful AV1 encoding, and can handle more simultaneous streams than most people will need. All this while running on slot power, so it can go into most servers.  Are there cards that offer better performance? Sure, but I have no need for the extra headroom, and this card is a massive leap over the a50 that was just shy of being good enough imo. This card knows its target audience, and it's tailor made for us. I cant wait to get my hands on one for my homelab.",Positive
Intel,I've been wanting to get a GPU to mess around with some AI stuff in my homelab i might have to get a few of these.,Positive
Intel,Should be on par with the 3050 but with more VRAM. Sweet!,Neutral
Intel,"Intel CPUs have fallen off, but the GPU division is COOKING 🔥",Neutral
Intel,Would this work well on a Plex VM in Proxmox for video transcoding?,Neutral
Intel,Not that powerful but it's efficient AF,Positive
Intel,Can it run Crysis>!Remastered!<?,Neutral
Intel,Unironically the only problem I’ve had with my arc b580 was vr (it’s a complicated thing and they simply don’t have any support for it yet),Negative
Intel,"""rare"" like we're just going to forget about arc alchemist and battlemage",Neutral
Intel,"Not available on r/etailers in EU for common folk, had to send requests and only I got were ""mpq: 10, and we sell it only to companies"". So I could become a lord of B50 over here, but no thanks, I only need 1 and don't want to bill more hrs for my accountant.",Negative
Intel,Will it run on ARM CPU’s? :P,Neutral
Intel,its not single slot sadly,Negative
Intel,Is it good? P2P? What's the AMD/NVIDIA equivalent?,Neutral
Intel,"waiting on the B60 because it would be 24gig, which is perfect for local AI shenanigans, there's basically only Nvidia there at the moment.",Positive
Intel,r/sffpc,Neutral
Intel,"The Intel ""pro"" graphics cards need to be certified by software developers before businesses will even consider purchasing.",Neutral
Intel,Yes you are..,Neutral
Intel,B60 also looks very impressive. With tdp of 200w you can run 4 of those on 1200w psu.,Positive
Intel,Gaming consumer market will only feel any difference when if amd or intel manage to compete with nvidia on ai workloads. Everything is ai now. Gaming is just peanuts profit wise. Nvidia ai monopoly has to be brought down for prices to fall,Negative
Intel,"And just like Homelander, it's shit",Negative
Intel,They not like us,Negative
Intel,Will there ever be another B card?,Neutral
Intel,"I mean, yeah it’s great for the price but it’s not better than anything else tbf",Negative
Intel,Not sure if it's just the A370M on my work/travel laptop but Arc drivers past 2024 make about half of my DirectX games unplayable.,Negative
Intel,"You are going bankrupt and are collapsing from the inside out, too!",Neutral
Intel,I wouldn’t trust installing anything Intel due to it being a state-sponsored company,Negative
Intel,I didn't even realize it was slot power,Neutral
Intel,"In any decent price point? Yes, in ANY PRICE POINT no we have rtx 4000 ada or w/e the name is now.",Neutral
Intel,Not even close. The Ada 2000 is probably faster and the Ada 4000 is definitely faster.,Negative
Intel,My RTX A2000 would like to be considered.,Neutral
Intel,As far as I’m aware YES if price is being factored,Neutral
Intel,"It's a long way to gain real foothold in the market, but if they're serious about it, at least this is the way to go.",Neutral
Intel,Too unknown for your average joe on a budget and too low end for enthusiasts I'm afraid. I would have loved for Intel GPUs to be a thing back when I had a very limited budget,Negative
Intel,"This, unfortunately.",Negative
Intel,"Whether or not the physical card statistics are good or not, it’ll always be at least a little behind for one reason alone, CUDA. That missing proprietary language makes it lack desirability by so many, since they’re limited in programming options",Negative
Intel,sad but true,Neutral
Intel,Honestly Nivida's strangle hold comes from nearly all prebuilds having nivida in them,Positive
Intel,"Even if it came with a side of fries, people would still grab that new nvidia GPU",Neutral
Intel,"So we can get a carbon copy of this gpu market but of the cpu market and even worse? Yeah, no thanks",Negative
Intel,"That would be unaliving themselves, CPUs are practically intel's only profitable sector, even tho on paper AMD is better in just about everything, in reality AMD struggles to actually ship enough volumes to regular customers to eat intel's share, intel still makes nearly 30 billion from the CPU business and mainly from laptops and pre-builts not from Data centers,they make nearly 3 times more than AMD does from CPUs.       Intel's problem is their Fabs which is a money black hole, but their CPUs are still doing well and making tons of profit, it's where the bulk of their revenue comes from.",Neutral
Intel,lmao,Neutral
Intel,Amd is just a buzzword for social media and content creators. Intel is and will always be better. Just got rid of my 3600x that people on this sub swore up and down about it being an upgrade and convinced me to get years ago. I went back to Intel and I couldn’t be happier. It’s night and day. No more micro stutters or restarting my pc because it randomly locks my entire system to 15 fps on boot!,Positive
Intel,"You do not want AMD doing what they currently do but worse.  AMD, Intel or Nvidia, they will all exercise a monopoly behavior is they can. They did (one currently does with 94% market share), and they will do it all over again.",Negative
Intel,"Oh yes big mega corporation save us from the other big mega corporation. Ain’t gonna change anything, yall saw what intel did with their CPUs. You think they wouldn’t do it with their GPUs?",Negative
Intel,Those chip cycles are like half a decade - they are working hard on both I hope and catch up in the next few years. So we get some competition and better prices,Positive
Intel,They already have. Haven't you seen all the new voltage and degradation issues?,Negative
Intel,It’s a well priced workstation gpu (rare),Positive
Intel,You may not need one.. but have you considered 4?  Affordable PCIe power-only 4x GPU workstation cluster babyyyyy,Neutral
Intel,Yeah it's a pro card it's not meant for gaming (although it can be used for that). At 350 the only redeeming factor for gaming is that it's slot powered and lp,Neutral
Intel,If you have a newer system... yes. There are a lot of reasons this is a bad choice for gaming regardless.,Negative
Intel,"Yes, by miles.  Answering for a friend.",Neutral
Intel,Exactly this.  Nothing drives innovation -or (relatively) drives DOWN prices- like genuine competition.,Negative
Intel,This. A monopolistic market is not fun to be in as a consumer...,Negative
Intel,Competition = good prices.    Monopoly = rapisт prices.,Positive
Intel,"For workstation/AI stuff? Yes.  For gaming? No, they are roughly equivalent.",Neutral
Intel,"Not a gaming card. Workstation card mostly for architects, engineers etc.   Decent value for what it is. I grabbed an arc a380 for video encoding and it’s phenomenal.",Positive
Intel,"Wait I’m confused. You said it can barely do anything, then that I will perform similarly to an RTX3060ti. That’s still a fairly capable card for AAA gaming, will it really perform that well on only 70w?",Neutral
Intel,"A lot of of LP slot powered cards won't beat the 3050 6GB. In fact, nothing in the gaming market. The A2000 does, but it was marketed as a workstation card as well. It's not the nothing burger you're making out to be. Granted, at $350, it's a stupid purchase to play games with when we have the 5060 LP.",Negative
Intel,Does it still have the same video functions as it's big brother? AV1 encoding and decoding would make this a decent plex/jellyfin card.,Neutral
Intel,"AI workload was my first thought as well, but I don’t think it will perform that well.   LLM are extremely bottlenecked by VRAM capacity and memory bandwidth. With the B50 having the abysmally slow 224GB/s, I wouldn’t expect anything impressive from it",Negative
Intel,"> It should perform around RTX 3060 Ti, RX 6600, RTX 2070 kind of level.  You do know the equivalent of the rtx 3060 ti are the RX 6700 XT and B580 right?  Well Arc B50 pro is significantly (~40%) slower than RX 6600 and Arc b570. Do with that information what you want.",Neutral
Intel,Pretty much just workstation,Neutral
Intel,"No, it has the full 18MB L2 and runs at 2600Mhz (250mhz lower than B580). Pytorch compatibility is fairly good.",Positive
Intel,Still hoping to be able to get my hands on a Intel B60 for my server (LLM for home assistant and coding stuff in my freetime). But I highly doubt it.,Negative
Intel,I hope it's stronger than 3050. It's got more silicon.   You get 272 mm^2 of tsmc 5nm vs the 3050s 200 mm^2 of Samsung 8nm,Positive
Intel,"You can do that on any integrated graphics of any relatively recent CPU. No need for a B50.  However, if you have multiple services that want access to a GPU, then the B50 could be a very good option.  The B50 has support for SR-IOV, which allow you to basically split the GPU to multiple VMs",Positive
Intel,Jesus... it was only released 3 days ago wtf,Neutral
Intel,"No, look at kosatec.de",Neutral
Intel,"genuine asking, have ARM CPU in desktop space mature enough to not suffer the consequences of x86 translation? like performance issue or bugs",Neutral
Intel,It should be possible on Linux: [https://www.phoronix.com/news/Intel-Arc-Graphics-On-ARM](https://www.phoronix.com/news/Intel-Arc-Graphics-On-ARM),Neutral
Intel,As long as it has drivers and the ARM SoC has PCIe lanes I don't see why not.,Neutral
Intel,Passively and by 10% share. You can argue that somewhere in the future the government will own it,Neutral
Intel,Either did I. That make it way more impressive considering it's performance.,Positive
Intel,"Keep arguing please, I am scribbling notes like my nephews gaming career depends on it",Neutral
Intel,Blackwell pro 2000 and 4000 sff beg to differ (those cost a fortune though),Neutral
Intel,"Yep, there's only a tiny overlap between those groups that would even consider it an option.  I would consider it myself for my media server, do we know if it can do transcoding and what codecs/how many streams?",Neutral
Intel,"If somebody wants a 5080-level card, NVIDIA is the only company making cards for said somebody. AMD/Intel cards can come with all sorts of fries but if the performance isn't there, a good portion of the consumers won't even bother checking them.",Neutral
Intel,"When AMD or Intel actually delivers better products, that might change.   Nvidia still seems to be comfortably in the lead on technology advancements and product performance.",Positive
Intel,"Why wouldn't they? The software side of Nvidia is great, they've got everyone beat handily and in many parts of the world, equivalent AMD or Intel cards aren't substantially cheaper due to regional pricing and local taxes.",Positive
Intel,"sad but true, I just thought if Intel is struggling atm why not focus on the thing that they're doing well in.",Negative
Intel,That’s what people said about AMD when they first released Ryzen,Neutral
Intel,"nah I'd be down for some fries. I get a ""new"" card every year or so and swap out the oldest/lowest performance card in my machines.   Currently have a 7800xt, and a few 2080's.  Have a stack with older stuff like the 1050ti and rx 480, and a 3060",Neutral
Intel,"I laughed, despite myself.  Now my boss is looking at me weird.",Negative
Intel,"Great for what it is, hopefully even better as new product prices wear off and more deals start to show up. Not meant for gaming so comments comparing it to gaming cards is a different subject. You shouldn't buy it just for gaming at these brand new product prices.  [Here's a video](https://www.youtube.com/watch?v=QW1j4r7--3U) from level 1 techs on it and you'll see all the nerds gushing about SR-IOV at this price point in the comments.",Positive
Intel,And the double B60,Neutral
Intel,It's definitely not made for gaming both in value and performance but it's still gameable if happen to steal one from intel headquarters,Negative
Intel,"If you look up vids testing its performance in gaming it’s way better than a 1050ti, what are you talking about? It’s about the same performance as a rx6600 or rtx 2070",Positive
Intel,"Wait, so how is it different from high-end Iris Xe integrated GPUs, then?",Neutral
Intel,"A lot of people bought the A2000 to play games, because there weren't a lot of LP options at the time. The best was the 1650, and the A2000 was substantially faster. However, that is not the case anymore.",Neutral
Intel,"If you're running the likes of Bentley MicroStation or Autodesk Civil3D (some of the apps I look after at work) on this little thing, you're taking your workstation back to IT, telling them to ram that little toy up their arses, and to get you a real workstation.  Engineering and architecture workstations are about rasterisation performance. The designs and drawings have to be rendered quickly and smoothly on your two 4K monitors, which a stripped back and power-throttled 70 watt BMG-G21 isn't going to do.  For very entry level visualisation, we use RX 7600XTs with 16 GB, they have AutoDesk and Bentley certified drivers and enough VRAM and rasterisation to handle it. The RTX 4060 Ti isn't that bad there either, again you need the 16 GB version. On the next refresh of the low end workstations, they're probably going to end up being 9060XT and 5060Tis. Both of those would smash this little thing silly.  To that, I'm not sure who Intel wants to sell this to. $350 isn't serious money and it's competing with gaming-intended GPUs (which are utterly fine for visualisation and design) of much higher performance. It doesn't have substantially more RAM than competing products, it doesn't ECC protect the RAM and workstations generally don't care for slot powered cards either way. The ones we use come as standard with 800 watt PSUs and our choice of ""gaming"" CPUs like Ryzens or Cores, or Threadrippers and Xeons - Xeons are a bit less popular these days.  I'm seeing only an argument here for it being a solid SFF video card. It's the most powerful thing you can run in a 75 watt slot power budget... ~~but it isn't half-height.~~  [Yes it is.](https://www.servethehome.com/intel-arc-pro-b50-review-a-16gb-sff-mini-gpu/) So yeah, a solid SFF card.  Intel's inability to execute on the CPU side may be bleeding over to the GPU side and that would be very tragic.",Neutral
Intel,But also he is bullshitting.  No it won't perform that well in gaming.  It won't perform anywhere near the rtx 3060 ti. B580 has somewhat similar performanc to rtx 3060 ti.  While the arc pro b50 is around 40% slower than the **Arc B570**  Which is still considered impressive because is a Pcie only card. But budget wise it is trash value,Negative
Intel,"~~From what I can tell here, B50 isn't even LP.~~  Strike that. [It is.](https://www.servethehome.com/intel-arc-pro-b50-review-a-16gb-sff-mini-gpu/)",Neutral
Intel,"Yes, it's great for that",Positive
Intel,"If the B60 wasnt going to require an external power connector, I'd be hyped for it, but unfortunately I need slot power in my system. Its looking to be really impressive for LLM applications though. Excited to see what people can do with it.",Positive
Intel,True. It's better than the A1000 for productivity. I guess you're right. Better than the 3050. Maybe on par with the B570?,Positive
Intel,I was indeed wrong. Didn’t even hear about a new one being released.,Negative
Intel,"The answer is kind of long, but most ARM chips (probably all) can’t emulate/simulate x86, however there is lots of software that does this. Under Windows it has kernel-level ARM to x86/64 translation for Windows binaries. Compatibility is pretty decent but it’s hard to tell under gaming since there’s no GPU drivers for anything but embedded ARM graphics.  Under Linux, it’s a totally different story. You have Box64 and FEX-EMU. I switched to all ARM years ago but it came with a learning curve. Compared to three years ago, there’s many high performance games and performance reaches about 50-70% of native x86 performance.  If you wanna see some games running on ARM you can check out some videos I made almost 3 years ago below. I cover native ARM games, ARM games running under Box64, games running under Box64/Wine, and games running under Box64 and Steam Proton.  I really should make more videos going over the pros, cons and evolution/reality of the landscape… Anyways!  https://youtu.be/OQrDQmONm6s?si=mDm-hcw1BeNeVLUK",Neutral
Intel,Thanks for that good read! And being that post is so old there had to have been progress. Really want to get one now to test on my Ampere machine…,Positive
Intel,Not a single one of the cards in the discussion have any relation to gaming though.,Neutral
Intel,as a relative newbie in PC stuff i love this sub so much because i absorb a ton of stuff i wouldn't even have known how to look for just by reading people's mundane arguments in the comments,Positive
Intel,And neither are on the market yet.,Neutral
Intel,Intel GPUs have VERY good media performance. I believe they support basically all modern codecs,Positive
Intel,True for high end like you mention but the best selling cards are usually the mid rangers and there is competition there.,Neutral
Intel,Not even 1% of the world can afford these lol so who cares.,Negative
Intel,*laughs in my AMD GPU connectors that don't melt*,Negative
Intel,"Wrong. 9070 XT is better and cheaper than 5070 Ti but the Reddi-sphere refuses to acknowledge it, all claiming that ""unless the 9070 XT is at least 50 dollars cheaper, buy the 5070 Ti"" bunch of sheep",Negative
Intel,"For me the Intel arc B580 costs slightly more than a 3060(or a used 3060 Ti), while performing like a 3060 Ti. I only looked at the gamer side of things, the b580 can probably do local AI and media editing better than both cards. The other alternatives at this price are the Radeon 9060XT 8GB(VRAM'nt), the 5060(which gets it's ass handed to it by the 3060 ti) and maybe a used 2080 Ti",Neutral
Intel,Absolutely not. People were thanking Amd since they crushed Intel in multi thread performance.,Neutral
Intel,Hey - I resemble that.    In all honesty it makes me sad that my VM hosts only have single slot PCIE.,Negative
Intel,Some people I know professionally are already planning it for their next upgrade for big CAD assemblies...,Neutral
Intel,Toché.,Neutral
Intel,"The highest end Iris Xe doesn't even beat 1050 mobile in gaming, and doesn't even come close to B50 in workstation/AI workloads.   So, it's different from high-end Iris Xe in that it better in every possible way.",Neutral
Intel,You’re a really strong dude for doing that (going full ARM). Not a big ARM supporter or interested much on it but I support you for what you did for real.,Positive
Intel,"I mean, they do, and defentetelly do for sff gaming",Neutral
Intel,Someday you roo shall participate in the mundane arguments.,Neutral
Intel,Give or take a month or two,Neutral
Intel,Best on market actually,Positive
Intel,"Up until one year ago, NVIDIA also had near no competition in ray tracing, AMD cards just performed very poorly in that regard.  If I had to choose between two cards that are almost identical but one lets me enable ray tracing without losing half the fps, I'd rather have this option than a couple more fps in general.",Negative
Intel,"Talk about best selling, let's not forget about the laptop market which is much bigger that the desktop, Intel doesn't have anything serious there, AMD has some dGPUs but good luck ever buying a laptop with one outside the US, your only option is really Nvidia. Hell, even a decent AMD APU is hard to come buy and for the same price you get a much stronger laptop with an RTX.",Neutral
Intel,Hehe! Good one! Since 2013 I have only had troubles with 2 GPUs. Both AMD. I have during the same time had 3x Nvidia GPU and none has malfunctioned once.   My experience may of course deviate from the norm. But from my perspective Nvidia has delivered where AMD has not.  Perhaps I’ll try AMD again next time.,Positive
Intel,"Reddit is a circle jerk of AMD fanboys, the fuck are you talking about?  You’d think AMD has a 50% GPU market share if you just looked at this sub.",Negative
Intel,"You seem to be incorrect.   https://gpu.userbenchmark.com/Compare/Nvidia-RTX-5070-Ti-vs-AMD-RX-9070-XT/4181vsm2395341  Luckily there are actual benchmarks to refer to.  Edit* as the bot claims userbenchmarks is biased. Here is gamernexus as well.   https://gamersnexus.net/gpus/amd-radeon-rx-9070-xt-gpu-review-benchmarks-vs-5070-ti-5070-7900-xt-sapphire-pulse#9070-xt-benchmarks  The AMD card seems to be worse there as well. But for a price difference like that, I could be convinced that the performance per cost is better. Not the overall performance.  Edit2* I’d be thrilled to get an explanation as to why stating sources of benchmarks yields downvotes. Bad sources? Wrong interpretations? Fanboyism?",Negative
Intel,"b60, the beefier workstation gpu they announced, might interest you whenever they officially release specs and prices then. Supposedly 24GB model and a 48GB model that's basically two combined on one board for what will hopefully be very competitive prices in the space as well.",Positive
Intel,"B50 is on the level of like a 1050. It has nothing to do with gaming lmao.   Ada 4000 cost like $1500 for 4060ti performance.   Ada 2000 cost like $1000 for even worse than that.   For sff you are better off getting a sff gaming card, it will be cheaper and perform better.",Negative
Intel,Not a soul wants this for gaming unless they're using it for other work and then exceptionally light gaming on the side,Negative
Intel,I'm gonna be honest. I'm probably a bit of an outlier. If I see an RT option in a game I Usually turn it off and I've been doing that since the 2080ti. But yeah of they have cost the same and one has way better RT it's a simple choice.,Neutral
Intel,"You seem to be linking to or recommending the use of UserBenchMark for benchmarking or comparing hardware. Please know that they have been at the center of drama due to accusations of being biased towards certain brands, using outdated or nonsensical means to score products, as well as several other things that you should know. You can learn more about this by [seeing what other members of the PCMR have been discussing lately](https://www.reddit.com/r/pcmasterrace/search/?q=userbenchmark&restrict_sr=1&sr_nsfw=). Please strongly consider taking their information with a grain of salt and certainly do not use it as a say-all about component performance. If you're looking for benchmark results and software, we can recommend the use of tools such as Cinebench R20 for CPU performance and 3DMark's TimeSpy and Fire Strike ([a free demo is available on Steam, click ""Download Demo"" in the right bar](https://store.steampowered.com/app/223850/3DMark/)), for easy system performance comparison.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/pcmasterrace) if you have any questions or concerns.*",Neutral
Intel,"You seem to be linking to or recommending the use of UserBenchMark for benchmarking or comparing hardware. Please know that they have been at the center of drama due to accusations of being biased towards certain brands, using outdated or nonsensical means to score products, as well as several other things that you should know. You can learn more about this by [seeing what other members of the PCMR have been discussing lately](https://www.reddit.com/r/pcmasterrace/search/?q=userbenchmark&restrict_sr=1&sr_nsfw=). Please strongly consider taking their information with a grain of salt and certainly do not use it as a say-all about component performance. If you're looking for benchmark results and software, we can recommend the use of tools such as Cinebench R20 for CPU performance and 3DMark's TimeSpy and Fire Strike ([a free demo is available on Steam, click ""Download Demo"" in the right bar](https://store.steampowered.com/app/223850/3DMark/)), for easy system performance comparison.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/pcmasterrace) if you have any questions or concerns.*",Neutral
Intel,"I wasn’t clear, I only have a single height full length slot, so I’m pretty limited.",Neutral
Intel,"But isn't like the best sff gaming card some of the ada series or 4060? I just kinda know that for them options are very very limited, and like for good performance sff builds you have to splurge for not that mutch perf. But if it's at the level of a 1050 then yeah, but I dunno I haven't looked it up yet, I just assumed it was decently good and at least 3060 level",Neutral
Intel,"You definitely have to try some singleplayer games with RT turned on, they're beautiful.",Positive
Intel,Fair enough. Thank you bot!,Positive
Intel,"Full height/length single slot with a blower fan I think is supposed to be in the mix for b60, if that's what you got.",Neutral
Intel,There are tons of SFF builds out there with a 4090. It all depends on how small you define your SFF.  And then there's [https://www.youtube.com/watch?v=od6fwgUv0fA](https://www.youtube.com/watch?v=od6fwgUv0fA),Neutral
Intel,Depends what case but I have a proart 4080 that is 2 and a bit slot so I just took the bottom fans out. I've seen founders 5090 SFF builds too unless what you mean is lower power draw builds?,Neutral
Intel,"Ok, but what if i play heavily modded minecraft 1.7 and really like the ""programmer art"" vibe?",Neutral
Intel,"Honestly, I prefered if it didn't exist  at all as it causes developers to dedicate quite a bit of development time and manpower to give it RT. I prefer games to run better and be able to run it on older hardware than to look incredible. Plus I already consider modern games that have it turned off to look really really good, it doesn't need to be perfect.",Neutral
Intel,I turned it off in Cyberpunk because my gpu couldn't handle it. But it's asking a lot with my 32:9 display,Negative
Intel,I’ll have to look into that,Neutral
Intel,"What? It's the opposite of what you think it is.  Having to build games without ray tracing is what takes actual development time, ray tracing being realtime means all developers need to do is… add objects to the world. No ray tracing means you have to bake the lighting into objects, which requires calculations and therefore development time every single time an object position is changed.  If a studio has an in-house engine, they'll have to implement it, yes, but it can be done in parallel by the people who work on the engine and not on the game. Unreal Engine has it baked in and enabled by default, though it prompts every time for shadows compilation in case of ray tracing turned off.",Negative
Intel,"Well, with *my* 32:9 display I had no issue: 120,000 fps with ray tracing on, baby!   To be honest though, the monitor actually has 288 pixels in total, running at 1.44hz, but the fps number was really big! I couldn't quite see it, ballparking a bit here.",Positive
Intel,to me the bump in fidelity more often than not isn't worth the performance hit in so many games.,Negative
Intel,"B580 is slightly faster, and that card is fine for coding, too.",Positive
Intel,The RGB ram and an AIO is a weird choice. Could have gotten air cooler and cheaper RAM and gotten a better CPU,Negative
Intel,"Beautiful, but the 580 performs terribly with such a bad processor.",Negative
Intel,B580 driver overhead is going to kill that 3600,Neutral
Intel,Spend to much on a pretty shit.,Negative
Intel,Terrible. I can get your system without the stupid AIO and sleeves for half the money,Negative
Intel,Awesome Build!!,Positive
Intel,"Just here stealing this Win11 ready entry level build from you. Thanks much buddy, looks amazing.",Positive
Intel,What do you think of the Arc b580?? I have a similar system running a 3060 Ti with a Ryzen 5 5600,Neutral
Intel,Personally I would prioritize performance and future proofing (if even possible in that budget range) over looks. But it is a very nice looking build I must say!,Positive
Intel,🔥,Neutral
Intel,looks slick... may i know the specs? or part list?,Neutral
Intel,"well you paid quite a lot for someone who intends on flipping it. next time go for cheaper components and just add RGB to give the ""gamer look"" as you will be selling to people who don't know what parts are good or bad.",Negative
Intel,Builds like these are the sickness of our time.,Neutral
Intel,i dont think its so bad. i think if you enjoy it then be happy. i traded a ps5 for a system that had an intel i7-9700k and a gtx 1080 so idk i think you got the better deal,Positive
Intel,"Maybe change the processor to something better, but when you post this, check how many PS you get in Fortnite and maybe buy Fortnite stickers and stick them where the glass isn't if you want it to not ruin the aesthetics. You can paint the sticker black or something and then stick it on or you can just print something like for the sticker and just paint it on, although I think a sticker would still be better so that they can take it off later if they want to.",Neutral
Intel,Bad news: your CPU was a poor choice for the rest of the system  Good news: at least you have ample improving room for a CPU upgrade in the future,Negative
Intel,"I'd go with a better CPU and a bigger PSU. 500w is probably ok, but even intel recommends a 600w with a B580.",Positive
Intel,RGB and AIO cooler is a dumb choice when you're so limited on budget,Neutral
Intel,looks awesome for a 550$ build,Positive
Intel,"Will copy this build, thanks man",Positive
Intel,exactly.,Neutral
Intel,whered you see that?,Neutral
Intel,oh the youtube video. i get you.,Neutral
Intel,Goodluck,Neutral
Intel,It’s actually really good runs similar to the 3060ti to but a little better and and the extra vramm is a good add on,Positive
Intel,"Thanks, I have them posted in the description",Positive
Intel,Well I’m about to get a R7 5700x and then it will be builds like ours!,Positive
Intel,I would recommend upgrading cpu to atleast ryzen 5 5600 atleast,Neutral
Intel,"What, HDD Boot Drives in 2025?",Neutral
Intel,"I spent the same as you but ended up with a 5700x3d and 6800xt. Call it luck, or not being wasteful",Neutral
Intel,oh i wasn't looking i was too mesmerized by the looks lol. my bad!,Neutral
Intel,My computer was built in 2019. I upgraded my cpu when i got a good offer. You just spent 550 to get parts that are already years outdated because you just had to have a AIO for a cpu that uses like 120W max.,Neutral
Intel,"Ok well noted, thanks",Positive
Intel,And I still have room to upgrade to 5700x while keeping cost down and to sit here and say you could built it for half the money is straight ignorant,Neutral
Intel,lol I got really good deals on all of those parts duhh how else would it be in this price range,Positive
Intel,"Don’t hate me cause I used my brain when buying, and I still have room for that 5700x in the budget and it will probably still be cheaper than most builds in this performance range",Positive
Intel,Its not. I DID build it for half the cost with better components. Just accept that you wasted money,Negative
Intel,Well your already tanking your performance with the cpu + gpu combo so good job i guess,Negative
Intel,Okay buddy you be happy with your so called 275 dollar build,Positive
Intel,Yall say this but have nothing to back it up with except for some YouTube video you have watched it’s not bad at all and way better than what I would expect from this price range,Positive
Intel,"Don't stress it man, this website for some reason is so negative towards builds that value aesthetics as well. Killer setup",Negative
Intel,"GPU requirements are fine, but the CPU requirements are insanely high. 7800x3d just to play at high ?? My poor ryzen 7600x   Is this game CPU demanding ?",Negative
Intel,wtf is a 5800f?,Neutral
Intel,why would you need a 7950x3d for 4k?? what the fuck is going on,Negative
Intel,Use fg to get 60fps lol,Neutral
Intel,They recommend frame gen to get to 60fps...,Neutral
Intel,Cpu requirements are insane... Something doesn't look right..,Negative
Intel,That must be a mistake considering B580 is listed under medium.,Negative
Intel,"These recommendations are so cursed lmao. Also, the fact they're recommending a stronger gpu for high than they are for ultra is insane  Also, fucking crazy that they say you need a massively stronger cpu to get 60fps as resolution goes up",Negative
Intel,Did AI make this holyshit. B580 for recommended and ultra but not high. Cpu requirements fucking astronomical.,Negative
Intel,Ryzen 9 7800X3D.... yeah aint no way that this is real,Negative
Intel,When u recommend a B580 at 4K Ultra with RT but you only put the 4070 Ti on high 💀,Negative
Intel,"Wtf is this sys req.? The vast majority of players are interested of 1080p60fps. Then we get a 1080p30fps, and a 1440p60fps instead. Lol. What a fail",Negative
Intel,when you upscale from 360p i think it could run that,Neutral
Intel,That table is including a huge amount of bullshit.,Negative
Intel,Probably hiding that you will need XESS on ultra performance. Like every other shitty dev out there,Neutral
Intel,"14900k and 7950x3d for ultra? What the fuck?  I understand it's Ultra but then if you need the top 0.1% tier CPU why not say you need a 5090 and 7900xtx for the GPU?  Calling it right now this game will be a stutter fest because the CPU will be absolutely hammered.  If not then these specs are absolutely fucked as other have already said a B580 for the ""recommended"" 1080p30 and also for the 4k60...",Negative
Intel,"Me looking at the chart: Oh, minimal gpu recommendations are quite lo... ooooooh, it's 30fps, not 60.",Neutral
Intel,cpu requirements seem insanely high i think my ryzen 5 5600 will struggle with this one what a shame,Negative
Intel,I have no idea where my 3090 fits on that chart tbh..high i assume,Negative
Intel,Another game slated to run like sublime crap,Neutral
Intel,"Do love me a good Ryzen 9 7800 X3D. Can’t find any for sale though, what am I doing wrong?",Positive
Intel,"This game doesn’t look that much improved over the predecessors graphically and other games on the market shit on it visually with lower or same spec requirements. Frame gen to hit 60fps on ultra with ray tracing is insane, if I had to guess they probably have really heavy ray tracing features but the actual models and textures and other tech is not fancy enough to bring enough total eye candy/detail to the game to make it seem worth it.",Negative
Intel,Am I seeing things or is there a ryzen 9 7800x3d 💀,Neutral
Intel,My cpu isn't even on minimum requirements WTF,Negative
Intel,"This low key sucks ass, unless the game is genuinely impressive with graphics, physics, and AI.  Also using frame gen to get 60 is a terrible idea.  You shouldn't be using frame gen with a native 30fps.  Good thing I got this game for free.",Negative
Intel,"The feeling, when you see your CPU, is mentioned as recommended.",Neutral
Intel,This makes absolutely no sense…so I could play ultra settings with 4080 super but not really because I have 7800 x3D? Also isn’t this CPU faster in games than 14900K? Somebody use chat gpt to make this I swear 😂,Negative
Intel,"The GPU requirements for low were ok until i read ""30 fps"" + paired with the somewhat elevated CPU requirements for all settings...  Lets not even talk about FRAME GENERATION FOR ULTRA 60FPS > im tired of companies abusing this technology to skip doing a proper optimization work.  Based SOLELY on this requirements chart, the performance will be shit.",Negative
Intel,i almost never buy games on launch day.  10$ says this is going to be buggy and broken initially so i better just ignore it and wait a year,Negative
Intel,30fps should be deleted from being minimum in any game. No one wants to play a jittery slideshow.,Negative
Intel,They're recommending a 7900GRE and 4070Ti for 4K HIgh. I think that's pretty good isn't it?,Positive
Intel,Ill just play on high lol. My 9070xt isnt even going to try ultra 4k 😅,Neutral
Intel,"And my 7900 GRE will need FSR, because why not?",Neutral
Intel,"It all depends on how intensive the game is. We routinely used the RTX 2080 Ti for 4K 60FPS gaming, and the B580 on that same kind of level.  Given its requirement for 4K60 at ""Ultra"" is just 12 GB VRAM, this probably isn't a hugely intensive game.  All that said, however, this is likely a mistake. The B580 is already in the 1440p 60 boxes and it isn't keeping up with an RX 9070 or the RTX 5070 alternative.  What's weirder than this are the CPU requirements. The 7950X3D (Ultra) is **slower** than the 7800X3D (High) unless the game is using utterly silly numbers of threads. The ""Minimum"" Ryzen 7 5800F isn't even a real thing!  Nobody has put much thought into this at all.",Neutral
Intel,70gb storage  https://i.redd.it/hbfuai741tmf1.gif,Neutral
Intel,"This seems like some nonsense. B580 for both recommended and ultra? Cpu requirements are insanely high too. The best option, like always, is to wait for the game to come out and check out some youtube videos w.r.t. to your specs and then make a decision.",Negative
Intel,Why do you need a top X3D 16c chip for 4k Ultra with RT + FG  That's asking for too much even for 4k High,Negative
Intel,ohhhkay so my 1080 should do 1080p60 at medium-high.. as it does with everything else. yay.,Neutral
Intel,I have an RX 6600 and an i7-11700 and a 1080p monitor  I have absolutely no bloody idea how this game will run for me. But I'm hoping for at least medium settings at 60FPS. Probably wishful thinking on my part.,Negative
Intel,"as a guy with 4070tis and i7 12700kf, if im not going to get 140+ on 2k, I'll be disappointed",Neutral
Intel,My poor 3070 😢,Negative
Intel,Never knew there was a 5800F,Neutral
Intel,"If its legit. Then thats called a great optimization. If not, well. You know the answer",Positive
Intel,"Any modern instruction CPU with 8 ""P"" (normal hyper-threaded/SMT capable with sufficient CPU speed) should be able to keep up with the GPU recommendation for 4K (if truly optimized well enough to max out with an Intel B550 - the vMem and raw horsepower of this card shouldnt be underestimated).   My $0.02",Positive
Intel,Is 32GB the new 16GB?,Neutral
Intel,14900 still kicking lol🙏🏾,Neutral
Intel,"Btw they also released laptop gpu requirements as well , which feels even more wierd  https://preview.redd.it/knpbietfeumf1.jpeg?width=1080&format=pjpg&auto=webp&s=4cf82a2c75a69b25e1c3d2e0b5fe52aff9722204",Negative
Intel,First time I've ever seen the GRE mentioned in one of these lol,Neutral
Intel,This game is supposed to be basically a DLC of DL2. so why the fuck a 6750xt is for 1440p medium? If it can max out DL2 easily at 1440p native. Cut the bullshit,Negative
Intel,Man the specs are cooked low 30 fps even with a ryzen 7.,Negative
Intel,"I'd imagine cpu specs are that high because of how many zombies can be rendered at once and if they have more than the most basic of AI save for the mutations.  Still, box specs aren't always a great metric to go by.",Neutral
Intel,I’d guess it’s a mistake?,Neutral
Intel,"the Ryzen 7 5800F doesn't exist. why is the B580 both in 1440p medium and 4k ultra? why does 4k ultra (with frame gen, so technically the cpu renders 30fps) require a Ryzen 9 7950X3D when 4K high requires a Ryzen 7 7800X3D (here cpu renders 60fps), those cpus have the same perfomance. why does 4K high require a 4070 Ti and 4K ultra a 5070, which is slightly weaker.",Negative
Intel,"Talks about the 7900 GRE (Sold way less then an XTX) and 9070xt but not the 7900xtx? 7800x3d which is faster in gaming recommending for lower then the 7950x3d which is for higher recommendations but performs worse? Then Intel Arc B580 recommended for 1440p60 and also for 4k Ultra Ray Tracing 60fps (THIS CARD BARELY DOES 1440P)? Huh? And then a 5800f? What is that? this CPU doesn't even exist! THEN A 7800X3D FOR 4K HIGH? THE DIFFERENCE IN 4K FOR A 5800X3D AND A CREAM OF THE CROP 9850X3D IS LIKE 5-10% (In BF6 it's 7%) WHO TF MADE THIS? WITH FRAMEGEN AND UPSCALING FOR THIS?  This genuinely makes no sense, this cannot be real. This also has to be the most demanding ""traditional"" game for these requirements I've ever seen. Framegen to get 60fps on Ultra 4k? What is going on? This started as a DLC for a 2022 game? This has to have been made by an unpaid intern using ChatGPT. Monster Hunter Wilds spec chart is like half as demanding as this games and we all know how that played out. Rip Techland, this game wont' sell because no one can play it.",Negative
Intel,Wtf I have a 5600g I'm cooked,Neutral
Intel,I’m sorry 32gb of ram for high and ultra? What the hell!  Don’t tell me this game is horrible optimised?,Negative
Intel,wow,Positive
Intel,Must be using the upscaling at like ultra mega performance mode with 8x frame generation,Neutral
Intel,[https://www.youtube.com/watch?v=UELrfslYU8s](https://www.youtube.com/watch?v=UELrfslYU8s)   RTX 3060 Ti | Dying Light: The Beast,Neutral
Intel,"This shit is so wrong on so many levels, they don't even care to put real hardware on it.  ""Ryzen 9 7800X3D"" doesn't even exist, 7950X3D isn't faster than 7800X3D in most games, so why it's included for 4K with RT/FG, shouldn't it be 9800X3D?.  ""Ryzen 7 5800F"" doesn't exist. Arc A750 Is 87% faster than 5500XT and 1060 and has more VRAM and it's in the same category for 1080p 30fps low.  The Intel I5 13400F doesn't change from ""30fps low"" to 60fps medium"" but the jump from 5800F (X)?? to Ryzen 7 7700 is pretty big.  Arc B580 being for ""4K with Ultra RT/FG"" and for medium 1440p/60FPS is insane. The 9070 and 5070 are 80-100% faster than that.",Negative
Intel,"this chart doesn't make sense to me. my ryzen 5600x cpu and 7800xt gpu should be more than enough for 1440p 60fps, and should be enough for far more than that, but this chart is making it seem like the cpu will be a bottleneck for performance?   Looking at the chart again, the recommendations just aren't adding up to me. Im thinking this chart has, possibly, misinformed or incorrect information right now, and that ill probably be fine for 1440p 60fps.",Negative
Intel,Ryzen 7 5700F ???,Neutral
Intel,There is no way its not a typo. Otherwise this game is like made for arc gpus,Negative
Intel,Where would I scale with a 3900x and a rtx 2080? Im thinking like a mix of high and medium?,Neutral
Intel,"The only conclusions i can make are the storage requirments are set, theres supported tech in the bottom left, and everything else if confusing and in need of more certified benchmarks.",Neutral
Intel,In what world is the b580 equal to the 5070/9070?,Neutral
Intel,"Is this AI generated? These are rarely consistent or but this one is really bad. An A750 can only do 1080p 30 but a b580 can do 1440p 60? A b580 can do their medium, can't do high but ultra works again? A 5070 being significantly better than a 4070 Ti? And what the hell is a 5800f? Ultra including frame gen?   What were they thinking?",Negative
Intel,People makes games in 15 min at UE5.  No optimisation.  Game sector went fast food from fine dining.,Negative
Intel,"FSR 4, hell yeah.",Neutral
Intel,want this game!,Positive
Intel,Does a Ryzen 7 5800F exist? It's listed under 1080p/30 or did they mean 5800X?,Neutral
Intel,"These days, resolution and framerate targets are both useless... We need to know if this is at NATIVE resolution and with or without the fake frames...",Negative
Intel,The CPU requirements are nonsensical. They recommend a 7950X3D for 4K Ultra with RT and the 7800X3D for High 4K when the 7950X3D is slower for gaming than the 7800X3D lol.  I wouldn't worry too much about it. Your CPU will be more than fine.,Negative
Intel,"Its funny how games like cyberpunk and kcd2 recommend a 7800x3d, but a 7600x is perfectly fine and will give you high fps even in cpu intensive areas, yet a game like stalker 2 which recommends a 3700x, you will be bottlenecked at 85 fps",Negative
Intel,Yea it looks to me that these have to be typos.  Can’t imagine it being that intensive especially at 4K when it comes to the CPU and then regarding the intel gpu……..maybe it’s the persons first day on the job?  This seems wrong that’s all I’m saying.  However optimization these days is rather lack luster so who knows?,Negative
Intel,That’s what I was thinking… seeing something other than 5700x3D or 5800x3D makes me wonder…,Neutral
Intel,Does cou requierement ever actually matter?  So manny times it says I will need better than my 13600k but there are never any issues,Negative
Intel,GPU req are also insane,Negative
Intel,Another non optimized game of 2025. Game of the year should have awards for unoptimized games of the year winners.,Negative
Intel,"Idk man , but yeah cpu part seem insane, while we can get away with GTX 1060 but we need 13400 for cpu lol",Neutral
Intel,"Same with ""Ryzen 9 7800 X3D""  Also who tf use frame gen to reach 60 FPS?",Negative
Intel,"At 30 fps no less (60fps fg, so the CPU only sees 30 fps). This tells you all the people who made/approved this chart are clueless.",Negative
Intel,So over all these companies targeting 60fps for everything and everyone being ok with it for some reason.,Negative
Intel,4k 60 with RT... With a 5070...,Neutral
Intel,With ray tracing though.,Neutral
Intel,ppl forget ultra usually is future proof settings. using frame gen to achieve it makes sense,Neutral
Intel,Stop being one of those people that just says that like you ignore all context. Especially when you're talking about Ultra settings. You scream that you have an unoptimized system and are already want to blame devs.   ![gif](giphy|71LVGkH37wAyRKHMAl),Negative
Intel,Probably ai related. X4 also requires crazy cpu power. 9800X3D recommended,Neutral
Intel,Yeah listed at medium and ultra but not high,Neutral
Intel,"I don't know if it's true but I think it's intel sponsored game(people in dying light sub claim it is) , but still even that won't push b580 for 4k ultra gaming lol , and it was posted by someone official from techland in dying light sub  Edit: why down votes?",Negative
Intel,For real. Guess my rig is now trash 🤷🏽‍♂️,Negative
Intel,Well.........yeah u had me here,Neutral
Intel,And look at cpu recommended. They feel insane   My poor 3600xt crying in the corner,Negative
Intel,"Where does it say that?   It usually says if the requirements are with upscaling taken into consideration, as with the FG for 4K Ultra RT.",Neutral
Intel,The Ultra requirements are aiming for 60 fps with frame gen so actually 30 so it kinda makes sense...  There's also no talk of upscaling here which I find hard to believe.   This is probably made my an intern.,Neutral
Intel,Do you run it native 2k or do you do DLDSR + DLSS ?,Neutral
Intel,i think we will be fine this chart makes no sense and i run dl2 2k 140 running around,Negative
Intel,I've noticed more games using 17GB so it's getting there.,Positive
Intel,"It's intel sponsored game I am guessing it's a situation very similar to amd's insane advantage in cod games, in cod even 70 class amd gpu beat the hell out of  rtx 4090",Negative
Intel,"Nope , it's real, they even shown it in their pc build video on YouTube",Neutral
Intel,https://preview.redd.it/b2sy4jzrttmf1.png?width=705&format=png&auto=webp&s=bbe7c8675adc5441f773fc8137282673c2ff1adb  [https://www.cgmagonline.com/articles/previews/dying-light-the-beast-event/](https://www.cgmagonline.com/articles/previews/dying-light-the-beast-event/)  ??????????????,Neutral
Intel,I think it’s a typo.,Neutral
Intel,"tbf it is possible that it needs the extra threads for it to perform better. I heavily doubt it, but it *is* possible.",Neutral
Intel,Why is this x950X3D slower than x800X3D shit still being thrown around everywhere. It is incredibly easy to configure x950X3Ds so they run as just upgraded x800X3Ds.,Negative
Intel,"IIRC, the 7950X3D not using the right cores in games was mostly fixed.",Neutral
Intel,"Ok good I ran to the comments worrying about mine, I usually have no problems with my 7800x on ultra but I'd like this game to work for me well",Positive
Intel,"This is from [9800x3D review](https://www.eurogamer.net/digitalfoundry-2024-amd-ryzen-7-9800x3d-review?page=4) done by Eurogamers (Digital Foundry). Captured in a CPU heavy area that they use in all their CPU & GPU testing. You can easily see the differences between CPUs. Even my 9800x3D is CPU often limited in many ways. Not so much when playing with locked 120Hz, but definitely when going any higher.   Edit. One addition. When using high GPU intensive path tracing, it will also tank the CPU performance. Something that actually got my mind blowed when I tested this on my 5800x3D & 9800x3D. Already CPU intensive titles have a hard time when using the PT.  https://preview.redd.it/yyw9oejud0nf1.jpeg?width=1216&format=pjpg&auto=webp&s=fef5205dddcf03ee0df8972aad5e3b6c8da0f6b9",Neutral
Intel,"3070ti for native 1440p 60 fps at medium doesn’t seem to be horrible. I’m sure there is many settings that don’t change fidelity a lot but eat up fps, so putting some to low and some to high may be a better experience. This also seems pretty damn general of a spec sheet here, almost like it’s useless based on some things here.   Throw on dlss with the 3070 ti and enjoy probably like 80 fps.  It does seem to be a gorgeous looking game too.   However, I’m not trying to defend it, and I would expect it to be optimized for the money that they’re asking. Like any game these days. And if it’s not it should be hated on.",Neutral
Intel,"To be fair, dying light has always been a very AI heavy game, so requiring a beefy CPU makes sense. Specially if any of the zombies can parkour at all.",Neutral
Intel,60 fps target with raster performance is a very sensible target.,Positive
Intel,and in 4K,Neutral
Intel,I don't mind if they recommend frame gen for something more logical like 120+ fps where the latency won't be super horrible. But requiring frame gen just to get 60 fps means the experience is going to suck. I'd rather play at 30 fps than with the weird feeling latency that frame gen causes when working with a low starting frame rate.,Negative
Intel,It still is the input latency of 25fps,Negative
Intel,"With cheese Mr Squidward, with cheese.",Neutral
Intel,"No it’s a perfectly valid thing to point out, there is a massive difference between 4k 60 and 4k 60 with frame generation.   This is coming from someone who generally enjoys frame generation, but what has allowed me to enjoy it is knowing how to use it properly, and going from sub 60fps up to 60 is not the right way to use frame generation.",Neutral
Intel,fuck the devs,Negative
Intel,"If it's not a mistake, maybe frame gen and upscaling make it technically possible, but with unbearable latency.",Negative
Intel,"I was being ironic. Every UE5 game I’ve played needed upscaling to hit 60+ fps, e.g. MGS3 Delta, Oblivion Remastered, KCD 2 (real good game). But that's just me being annoying, wanting everything on ultra.",Negative
Intel,"yeah ""intel sponsored game"" but Arc A750 that is 87% faster than the 5500XT/1060 is for 1080p low 30fps, and that argument is dead. B580 doesn't have enough power to play at 4k, the difference between B580 and 5070/9070 is so massive it won't matter anyway that it's better optimized for battlemage architecture. That's just cope",Negative
Intel,"Oh great, my 14900 is going to overheat just like in bf6",Positive
Intel,Isn't the non-X3D CCD basically turned off when gaming? By the X3D boost setting?,Neutral
Intel,"It’s not ‘it’s possibly’ it’s most likely what it is, for some reason many people think cpu > next to irrelevant but it absolutely isn’t for *specific* games, devs put this out there for a good reason probably",Neutral
Intel,Multithreaded optimizations are notoriously difficult for most developers. I doubt this game needs that much throughput.,Negative
Intel,9950X3D is notably an exception to the rule. The 7950X3D was and is slower than the 7800X3D.,Neutral
Intel,"Emmm the game looks just like DL2, and that gpu maxed out the game easily, so why now barely 60 at medium?  crap optimization",Negative
Intel,"But as we're told by Jensen, AI runs on the (Nvidia) GPU, right?",Neutral
Intel,It's with the Ray Tracing ON that's why it requires frame gen especially on Ultra 4K. Stop rushing and read more carefully.,Neutral
Intel,... The B580 definetley has 12GB of Vram wdym.,Neutral
Intel,If they implement every single optimiztaion intel recommends then its more than possible to get 4K 60K on the B580 with FG.,Positive
Intel,"Ah, I guess it's not out of the realms of possibility these days anyway 😂  Yeah UE5 is the worst. I've not *had* to use upscaling on my 9070 XT at 1440p ultra for a playable experience but since I prefer a higher refresh rate experience then I've usually had it stick it on to quality to get that experience when other games/engines I can easily play native and do that.  The newest version of XeSS is decent if you're on RDNA 3.",Negative
Intel,"To be fair, Bf6 is particularly CPU heavy. My wife's 5600x was at 100% usage the entire time the game was open. My 9950x3d would be as high as 60% avg on one or 2 maps",Neutral
Intel,"No, it is still active! The problem with the 7950x3D (and 5950x3D) is that some processes can go to the part without the 3D-Vcache. So you could say the problem is that it isn't turned off, or rather that the scheduler doesn't prioritise correctly.",Negative
Intel,Not if you bother spending 10 seconds properly configuring your $900 CPU.,Negative
Intel,You can literally just turn off CCD1 on the 7950X3D and it'll run as a slightly upgraded 7800X3D. Proper CPPC and Process LASSO configuration will make it always run better or the same.,Positive
Intel,I got like 85 fps in ultrawide with a mixed/ ultra settings. What are you talking about?,Neutral
Intel,Why are you assuming I didn’t know it was with RT on?,Neutral
Intel,"The issue is that FG is a “win more” tool. You don’t use FG for sub 60 to get to 60, as it has poor latency. Generally not a good experience.",Negative
Intel,"The issue is I’m expecting 4K Ultra out of a 1440p card; as I game at 5120x1440, about 20% shy of 4K.      But honestly, I don’t care that much, I’ve played games at 40/45 and still had fun with awesome graphics. I just need stability.",Negative
Intel,My 5070 was bottlenecked by my 5700x3d in bf6,Neutral
Intel,"I throttled my cpu and had no dips in performance, temps went back down to 60ish. I don’t think they were utilizing the cores correctly or something.",Negative
Intel,"To be perfectly fair, a lot of people who are spending $900 on a CPU are fairly reasonably expecting it to just work best out of the box.  A lot of the ""value"" of most super premium consumer products is the fact that everything is handled for you.",Neutral
Intel,Wow sounds like a great deal for hundreds more lol,Positive
Intel,"but you have a 5070 not a 6750xt/3070ti..................................   Edit: also, barely 80fps on a 5070 on and old gen game is HORRIBLE",Negative
Intel,Man do you even realize how demanding the RT is? they put a 5070 there not a 5090.,Negative
Intel,"Makes sense, my 7800xt was about right with my 5700x3d. There was no bottleneck since all my components were fully utilised",Neutral
Intel,"If money isn't an issue, yeah actually. It's at parity or slightly better in gaming with massive headroom, and completely shreds it in productivity.",Negative
Intel,That’s what I got with my 3070ti and 5600x. I just recently upgraded. I get far more fps now on max settings with raytracing.,Positive
Intel,"Yes, I use RT often so I know how heavy it is, in fact I used it in the DL2 base game, but that’s neither here nor there when it comes to my point.   They really shouldn’t be labelling it as 4k 60 if they are using frame generation to get there. Nobody will realistically want to used FG to get from sub 60fps up to 60, so they should have just put 4k 30fps in that category.",Neutral
Intel,"You won't need frame gen to hit 60 if you have a current gen card like 5080.   5070 has the performance of a 3080ti or 4070ti, both cards for 2K nowadays. Look at it as a sign of good optimization for those that have 4K monitors.  I also don't use FG, but the graphic looks better rather than slapping 5080 there to brute force it.",Positive
Intel,"Yeah I’m aware of all of that, but again it’s not really my point, my point is that they really shouldn’t be marking it up at 4k 60 for those cards, probably in the hopes that a lot of people will just skim over it and not see the bit further down in brackets that you actually need to use frame generation to achieve that with those cards.   It’s misleading at best, as those cards won’t get near 60 fps with those settings without it, and it’s not the way FG is supposed to be used, it would be an awful experience, they either should have listed the actual cards you will need to get 4k 60 with those settings, or marked that category as 4k 30fps.",Negative
Intel,you're probably CPU bottlenecked though the slow RAM isn't helping you at all either. Get a 2x16 3600 speed RAM kit and/or grab a 5000 series AMD CPU,Negative
Intel,"Yeah, you're probably being hit by the driver overhead issues. You want at least a 5000-series Ryzen to mitigate that, as the other commenter suggested.",Negative
Intel,Intel cards have a terrible CPU overhead issue with lower end processors.,Negative
Intel,Spec list here ! https://fr.pcpartpicker.com/b/GVW48d,Neutral
Intel,"look great but $1200 is a way better system.  maybe ask the internet for advice before buying anything  this will out perform your system by a healthy margin for the same price  \[PCPartPicker Part List\](https://fr.pcpartpicker.com/list/X4xqv4)    Type|Item|Price  :----|:----|:----  \*\*CPU\*\* | \[AMD Ryzen 5 7600X 4.7 GHz 6-Core Processor\](https://fr.pcpartpicker.com/product/66C48d/amd-ryzen-5-7600x-47-ghz-6-core-processor-100-100000593wof) | €178.00 @ Amazon France   \*\*CPU Cooler\*\* | \[Thermalright Assassin Spirit V2 66.17 CFM CPU Cooler\](https://fr.pcpartpicker.com/product/fhFmP6/thermalright-assassin-spirit-v2-6617-cfm-cpu-cooler-as120-v2) | €19.89 @ Amazon France   \*\*Motherboard\*\* | \[Gigabyte B650M GAMING WIFI6E Micro ATX AM5 Motherboard\](https://fr.pcpartpicker.com/product/sNQwrH/gigabyte-b650m-gaming-wifi6e-micro-atx-am5-motherboard-b650m-gaming-wifi6e) | €134.94 @ Amazon France   \*\*Memory\*\* | \[Patriot Viper Venom 32 GB (2 x 16 GB) DDR5-6000 CL30 Memory\](https://fr.pcpartpicker.com/product/4cCCmG/patriot-viper-venom-32-gb-2-x-16-gb-ddr5-6000-cl30-memory-pvv532g600c30k) | €102.88 @ Alternate   \*\*Storage\*\* | \[Western Digital Blue SN580 1 TB M.2-2280 PCIe 4.0 X4 NVME Solid State Drive\](https://fr.pcpartpicker.com/product/rqhv6h/western-digital-blue-sn580-1-tb-m2-2280-pcie-40-x4-nvme-solid-state-drive-wds100t3b0e) | €64.90 @ Amazon France   \*\*Video Card\*\* | \[MSI VENTUS 2X OC GeForce RTX 5070 12 GB Video Card\](https://fr.pcpartpicker.com/product/D3bypg/msi-ventus-2x-oc-geforce-rtx-5070-12-gb-video-card-geforce-rtx-5070-12g-ventus-2x-oc) | €560.00 @ Infomax Paris   \*\*Case\*\* | \[Antec AX20 ATX Mid Tower Case\](https://fr.pcpartpicker.com/product/HZ9wrH/antec-ax20-atx-mid-tower-case-0-761345-10060-1) | €39.34 @ Amazon France   \*\*Power Supply\*\* | \[MSI MAG A750GL PCIE5 II 750 W 80+ Gold Certified Fully Modular ATX Power Supply\](https://fr.pcpartpicker.com/product/qcbypg/msi-mag-a750gl-pcie5-ii-750-w-80-gold-certified-fully-modular-atx-power-supply-mag-a750gl-pcie5-ii) | €98.35 @ Amazon France    | \*Prices include shipping, taxes, rebates, and discounts\* |   | \*\*Total\*\* | \*\*€1198.30\*\*   | Generated by \[PCPartPicker\](https://pcpartpicker.com) 2025-09-16 22:36 CEST+0200 |",Positive
Intel,This was a comissioned build. Client wanted RGB. Client wanted a 4Tb SSD.I don't want to seem to brag about my skills but I actually build a lot of systems and don't really need internet as I am generally the one giving advice. Aesthetics and RGB was with storage the main criteria here. That's not really how i wanted to spend my client's money but directives are directives and client is happy with it's b580 that will do nothing as it's not intended for gaming and happy with his 4Tb of storage. I guess i am happy too since I got my part of the money too...,Neutral
Intel,![gif](giphy|3WmWdBzqveXaE)  Nana watching you regain your inheritance,Neutral
Intel,"What a disgusting build, I love it",Positive
Intel,the content we crave,Neutral
Intel,">AMD+Intel+Nvidia GPUs within the same PC  okay, now i wanna know ***much*** more about how this works.  is this a linux only thing or does windows also let you have multiple gpu brands installed at the same time? i would assume it would be a bit of a hellscape of conflicting defaults and drivers.  im a bit of an aspiring dipshit myself and ive been quietly losing my mind trying to figure out how to get windows 10 to run software on a specific gpu on a per program basis, by chance you got any idea if thats possible at all, or if linux magic is the missing ingredient?",Neutral
Intel,What GPU are you using in your build?  All of them,Neutral
Intel,you're one hell of a doctor. mad setup!,Negative
Intel,The amount of blaspheming on display is worthy of praise.,Negative
Intel,Brother collecting them like infinity stones lmao,Neutral
Intel,I'm sure those GPUs fight each others at night,Neutral
Intel,Bro unlocked the forbidden RGB gpus combo,Neutral
Intel,How does this card hold up compared to other comparable cards in your Computational Fluid Dynamics simulations?  Also how much of an improvement did you see from Intel Alchemist to Battlemage?,Neutral
Intel,What the fuck,Negative
Intel,I was wondering for a second.. Why such an old Nvidia graphics card until I saw it is a behemoth of a TitanXP. Good!,Positive
Intel,Yuck,Neutral
Intel,Wait until you discover lossless scaling,Neutral
Intel,Can you use cuda and rocm together? Or do you have to use Vulcan for compute related tasks?,Neutral
Intel,"This gave me an idea for getting a faster local AI at home. Mine is eating all my 24GB vram, and its not super fast cause of the lack of tensor cores in any of my hardware.  But if i could just stack enough VRAM... I have an old mining rig with 1070s collecting dust.   Hmmmm :P",Negative
Intel,Now you just need to buy one of those ARM workstations to get the quad setup,Neutral
Intel,holy smokes! I follow you on YouTube!!! Love your simulations keep up the good work!  If you have some time you mind pointing me to the right direction so I can run similar calculations like your own?   Thanks!,Positive
Intel,Love it lol. How do the fucking drivers work? Haha,Positive
Intel,What an amazing build,Positive
Intel,wtf is that build man xdd bro collected all the infinity stones of gpu world.,Neutral
Intel,You’re a psychopath. I love it,Positive
Intel,This gpu looks clean asf😭,Positive
Intel,The only setup where RGB gives more performance. :D,Neutral
Intel,Now you need a dual cpu mobo.,Neutral
Intel,Placona! I've been happy with a 6700xt for years.,Positive
Intel,"That is not ""SLI"".  That is Crossfire.  There is a major difference.  ""SLI"" only permits alternating frame rendering (AFR).  Crossfire permits splitting a single frame load among different cards in addition to AFR.",Neutral
Intel,"Brawndo has electrolytes, that's what plants crave!",Neutral
Intel,"You have always been able to do something like this! Though cross manufacturer has not had ""benefits"" until Vulkan in some spaces for games, now lossless scaling, but for anything requiring VRAM or software level rendering, it's been there as an option.   Unironically, ""SLI and crossfire"" I'd argue, is back, but not in the traditional sense.  I did this with a 6950XT and a 2080ti, simply because I wasn't able to jump on the 3090/4090 train in time tho.",Positive
Intel,"Works in Windows too. But Windows has way too much overhead for all of the AI garbage, ads and integrated spyware running in the background to still be a usable operating system. Linux is much better.   The drivers install all side-by-side, and all GPUs show up as OpenCL devices. In the software you can then select which one to run on.   FluidX3D can select multiple OpenCL devices at once, each holding only one part of the simulation box in its VRAM. So VRAM of the GPUs is pooled together, with communication happening over PCIe.",Neutral
Intel,"Windows has a section where you can select a gpu to run certain applications. It was introduced in win 10, but i only know the location in win 11    I think you can get to it through settings -> display -> graphics",Neutral
Intel,"What kind of application are you trying to run on specific GPUs? IIRC Vulkan will let you specify what device to use, even if it's not the GPU whose monitor is showing the application. DirectX I think is controlled by the Graphics settings in Control Panel. I think there's a page somewhere that lets you pick the GPU. That might be a Windows 11 thing though. OpenGL is the one that AFAIK will only render via the device whose monitor is displaying the application.",Neutral
Intel,Team RGB,Neutral
Intel,"_snap_ and half of CUDA software is dead, as people prefer the universally compatible and equally fast [OpenCL](https://github.com/ProjectPhysX/OpenCL-Wrapper)",Neutral
Intel,"- The 7700 XT is quite slow, AMD has bad memory controllers, a legacy moved forward from GCN architecture. And the oversized 3-slot cooler doesn't make it any faster either - 2828 MLUPs/s peak - Arc B580 - 4979 MLUPs/s - The 8 year old Titan Xp (Pascal) - 5495 MLUPs/s - Arc Alchemist (A770 16GB) is similar memory performance, with wider 256-bit memory bus but slower memory clocks - 4568 MLUPs/s   Full FluidX3D performance comparison chart is here: https://github.com/ProjectPhysX/FluidX3D?tab=readme-ov-file#single-gpucpu-benchmarks   But performance is not my main focus here. I'm happy to have all major GPU vendor's hardware available for OpenCL development and testing. Quite often there is very specific issues with code running in one particular driver - compilers optimize differently, and sometimes there is even driver bugs that need workarounds. Extensive testing is key to ensure the software works everywhere out-of-the-box.",Negative
Intel,"Had that since 2018 - got it for free through Nvidia academic hardware grant program. It has slower memory clocks, but double (384-bit) memory bus. It's actually the strongest of the three GPUs.",Negative
Intel,"OpenCL works on all of them at once, and is just as fast as CUDA!",Positive
Intel,"ARM mainboard/CPU, 3 GPUs, and Xeon Phi PCIe card to also have an x86 CPU ;)",Neutral
Intel,Start here with FluidX3D: https://github.com/ProjectPhysX/FluidX3D/blob/master/DOCUMENTATION.md 🖖,Neutral
Intel,"They work well together - all GPUs show up as OpenCL devices. Need specifically Ubuntu 24.04.2 LTE though, as all drivers need specific ranges of Linux kernel versions and kernel 6.11 happens to work with them all.",Neutral
Intel,"Technically FluidX3D uses neither SLI nor Crossfire, but cross-vendor multi-GPU instead, for domain decomposition of a Cartesian grid simulation box, to hold larger fluid simulations in the pooled VRAM.   The rendering is done multi-GPU too, as domain decomposition rendering. Each GPU knows only a part of the whole fluid simulation box in VRAM and can't see the others. It only renders its own domain, at 3D offset, to its own frame with accompanying z-buffer, and copies those to CPU over PCIe. The CPU then overlays the frames.",Neutral
Intel,I find it sad we killed SLI and Crossfire especially now that we have Resizable Bar and higher speed PCIE connections. (I’m no expert but I know we have made advancements that would improve the experience of multi-GPU setups.),Negative
Intel,I recall Ashes of the Singularity demonstrated this capability almost 10 years ago. DX12 heterogenous multi GPU with AMD and Nvidia cards.  https://www.youtube.com/watch?v=okXrUMELW-E,Neutral
Intel,how much pcie bandwidth do you realistically need for this sort of thing to work? is there any headroom at 3.0 x4?,Neutral
Intel,"god i wish.   that menu is entirely useless, the only options are power saving / high performance, which are all forcibly autoselected to the same gpu.  please tell me that the windows 11 version actually lets you manually select what specific gpu you want via a dropdown menu?",Negative
Intel,"lets be honest, this is the REAL reason intel getting into graphics is a wonderful thing.",Negative
Intel,Thank you so much for the very detailed response!,Positive
Intel,Well worth it!,Positive
Intel,Thank you my man!! Looking forward to run some tests once I get home.,Positive
Intel,That's awesome!,Positive
Intel,"Yes, but SLI is a bad description for it.",Negative
Intel,"The faster PCIe 4.0/5.0 and future iterations mean that dedicated SLI/Crossfire bridges are obsolete. The PCIe bandwidth nowadays is more than enough. And PCIe is the generic industry standard interface, easier to program for than proprietary hardware that's different for every vendor.   For games multi-GPU is gone for good (too few users, too large cost of development, no return of investment for game Studios). But in simulation/HPC/AI software multi-GPU is very common as it allows to go beyond the VRAM capacity of a single large GPU for cheaper.",Neutral
Intel,"sli/crossfire were killed for good reason, its just a bad time all around if half of your gpu's core/cache is located a foot away from the other half, unless your baseline performance is so damn low that the microstutters just get lost in the noise.  ultimately chiplet cpu/gpu designs are basically just an evolved form of sli/crossfire, and we're happily starting to get quite good at those.  (assuming we're talking about games)",Negative
Intel,"indeed it did, if only game devs adopted this more. Then again, the idea of two high end GPUs like we have today in a single PC is kinda horrifying.",Negative
Intel,"There is not really a clear limit. More PCIe bandwidth makes scaling efficiency better, less means the software will run a bit slower in multi-GPU mode. 3.0 x4 (~3.3GB/s) is just enough for reasonable efficiency.",Neutral
Intel,"It does actually. I have 3 gpus i can select from (7900 XT, iGPU, and Tesla P4)   Ill reply to your message once i get a screenshot",Neutral
Intel,"NVLink 3.0 (2020, GTX3090 use this one for reference) is a tiny bit faster than PCIe 5.0 (16x, 2019) : 50GT/s vs 32GT/s  But PCIe 6.0 is faster nvlink 4.0 but not 5.0 (those are only use in DC GPU AFAIK)  [Source](https://en.wikipedia.org/wiki/NVLink)",Neutral
Intel,"Indeed, people forget that the speeds electricity travels is slow in the computer world.   Kinda why the RAM slot closer to your CPU performs so good. And why benchmarkers will use that slot, and not the one furthest from the CPU.  Same with NVME m2 SSD, the closest slot is the best one. PC will perform the best if OS is located on the closest one.   Much better off just slapping two GPUs together in a single form factor than two separate GPUs.  Guess that is why we have 5090 these days. At about double the price of the old flagships.    You can view that as SLI i guess :P",Neutral
Intel,"Iirc Rise and Shadow of the Tomb Raider were the only games to support the used of mixed multi GPU (at least mainstream) other than ashes. A bit of a bummer from goofy multi GPU setups, but yeah, today the thought of two 600 watt GPUs in a single system just sounds like a recipe for disaster. With an overclocked CPU, an intense game could literally trip a 120v breaker!",Neutral
Intel,"thanks man.  that is incredibly relieving to hear, and equally annoying considering this is probably going to be the reason ill eventually 'upgrade' to win 11 one of these decades.  cant believe internet stories of a functional fucking menu is more enticing to me than the actual trillion dollar marketing...  ​​​  also this is a bit of a dumb question but can you actually play games on gpu-1 if the monitor is connected to gpu-2?  i'd assume so considering thats basically what laptops do, but... im done assuming that things work without issue.",Positive
Intel,"Yes i can do games on 1, but using monitor on 2. I have one monitor connected to the gpu itself, and the other to the motherboard, since my card only has 1 hdmi port which i use for vr",Neutral
Intel,Why are you connecting the monitor to the gpu and not the mobo?,Neutral
Intel,"👍   thanks for the info, this'll definitely come in handy eventually.",Positive
Intel,why not? how would you benefit from connecting the monitor to the motherboard instead of just using the gpu's ports?,Neutral
Intel,No worries mate. Good luck,Positive
Intel,"For some reason I switched up, connecting to the gpu is the way to go. I derped",Neutral
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,It's alive. Rejoice.,Neutral
Intel,"We know the Arc B580 runs well with a Ryzen 7 9800X3D, which is 8 core/ 16 thread CPU.  According to these graphs, the i9-14900K (8P + 16E = 32 thread) and the i5-13600K (6P + 8E = 20 thread) CPUs do fine.  The extreme budget CPU i3-12100F (4P = 8 Thread) performs with a notable degrade in performance.  My current hypothesis is Intel's driver is relying on a heavier multithreading with a bit of crosstalking of the driver workload, potentially to take advantage of underused E cores, which the 13600K and 14900K have plenty.  Given the Ryzen 5 series CPUs have similar performance issues as the 12100K, having 6 cores and 12 threads, I would like to see Ryzen 7 non-X3D CPUs (8 core/ 16 thread), Core i5-14400 (6P + 4E = 16 Thread), and Core i5-12500 (6P + 0E = 12 Thread) CPUs compared as well.  Playing off Intel translating DX11 to DX12 drivers as an example, when DX11 game loads, Intel establishes 2 processes, the DX12 driver and the DX11 translator.  For optimal performance, all threads need to be running simultaneously, the DX11 translator sends command to the DX12 driver in real time.  If there isn't enough room for the threads to be running simultaneously, any data traded between the two have to wait until the next thread is switched in before getting a response.  More threading density means more delays.  Some games don't get impacted either because the game involves less threads or the driver doesn't need the real-time translation threads.",Neutral
Intel,"It's probably because the Intel gpu drivers weren't written that well since it was probably ported with little changes from their igpu drivers where there was always a GPU bottleneck which meant that Intel might not have known there was even an issue until more attention was bought to the issue with Battlemage.  Alchemist was a flop, not many people bought it so not much attention was paid to CPU overhead issues.  AMD/Nvidia by contrast have spent the last 20 years painstakingly writing and optimizing their DGPU drivers. Nvidia had some CPU overhead issues a few years ago and they managed to improve it with driver fixes.",Negative
Intel,One thing I appreciate about AMD is having the lowest CPU overhead for their graphics drivers. Makes a difference if you're CPU limited in a game.,Positive
Intel,So Nvidia now has the lowest driver overhead? Seems like they took the HUB video seriously,Neutral
Intel,So the money you save on a GPU you will need to spend on a better CPU??  Might as well get a faster GPU.,Neutral
Intel,Interesting that B580 doesn't look bad at all with a 13600k. I wonder what it's like with a 13400 or 12600k. It seems like just having those extra threads provided by the e-cores takes care of the overhead it needs.,Positive
Intel,"Unless you're running a CPU that's *many* many years old, GPU overhead is not really something you need to worry about. Whether AMD has less overhead or Nvidia has less, it really doesn't matter.",Neutral
Intel,"On an older post an Intel graphics engineer explained the issue, it isn't what you said. Intel is too verbose in commands which slows everything down.",Negative
Intel,I'm fairly sure they use dxvk for d3d9 to 11.,Neutral
Intel,Could just be a cache issue,Neutral
Intel,Battlemage drivers use the cpu for software accelerating certain processes that are not being hardware accelerated in the GPU.,Neutral
Intel,Glad you brought up Nvidia as I didn’t know this had improved until the testing around Arc showed it had gone.,Positive
Intel,"According to the graphs, AMD has slightly less overhead than NVIDIA.",Neutral
Intel,"No, they do not.  The reason they have overhead can't be solved with software.  They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  The main example that seems to suggest otherwise is actually a demonstration of nVidia's forced threading of DX11 games, which can increase performance despite the increased overhead it entails, when the CPU has enough headroom overall (i.e. it doesn't eat into the single-thread performance).",Negative
Intel,"Lowest with DX11 and older, but not with the newer APIs",Neutral
Intel,And when is the last time HUB did a dedicated video showing the improvement in overhead?,Neutral
Intel,or it's just a cache/memory access issue,Neutral
Intel,"The overhead is minimal for both AMD GPUs and NVIDIA GPUs, which is probably why reviewers didn't look at the overhead until Intel GPUs came along.",Neutral
Intel,"> Unless you're running a CPU that's many many years old, GPU overhead is not really something you need to worry about.   That's just not true with Battlemage.  CPUs released in 2024 showed the issue in testing.    It's not year of release, it's capabilities.",Negative
Intel,"Intel uses software translation for DX11 and lower, so it does matter for them.",Neutral
Intel,"Hmm, Nvidia lost less performance going from 14900k to 13600k than AMD but more when going down to 12100",Neutral
Intel,"> No, they do not. The reason they have overhead can't be solved with software. They've excluded hardware from their GPU's and required the CPU to do the work of that missing hardware.  This was true for Alchemist but not for Battlemage.",Negative
Intel,That's not true. Intel's issue is being too verbose in commands/calls.,Negative
Intel,"Never, because HUB doesn't like portraying Nvidia in any light besides negative.",Negative
Intel,HUB used DX12 games that also showed the issue.  It's something else.,Neutral
Intel,"The comment to which I am replying is talking about nVidia, not Intel.",Neutral
Intel,"I’m pretty sure HUB doesn’t like Nvidia *or* AMD. They’re calling it how it is, these parts are too damn expensive.",Negative
Intel,That's actually... just worse news.,Negative
Intel,I always dreamt of the day APUs become power houses.,Neutral
Intel,"Is it my expectations being too high or this ain't a huge uplift? To go back to the classic: hopefully Zen 6 with RDNA 4 will offer a bigger uplift. We only have to wait a year and a half...  Anyway, question for the more knowledgeable people: how could the 890M perform with a 50W chip variant, but with 5600 SO-DIMM RAM? What to expect?",Neutral
Intel,"I find it sad that most review outlet is not testing CCX latency for these new CPUs.  These Zen 5 + Zen 5c have insanely high cross CCX latency, 180ms tested by geekerwan to be exact. For reference the 5950x had a 70ms latency for their cross ccd latency and the 4 ccd 1950x had a 150ms cross ccd latency with the closet 2 ccd and 200ms between the furthest 2.  Essentially games will be limited to the 5.1ghz peak 4 core zen5 core cluster or the 3.3ghz peak 8 core zen5c core cluster.",Neutral
Intel,Damn Why is AMD even involved in iGPU,Negative
Intel,"If this is true, Strix Point is going to claim total dominance over the GTX 1650 market. Won't be until 2023 when the theoretical RTX 4050 is released to surpass Strix Point's efficiency. Then super budget-friendly Strix Halo will come next year and take the RTX 2080's lunch money. Game over Nvidia.",Positive
Intel,"Strix Halo is rumored to be a whopping 40 CUs of RDNA3.5 so...   That'll do it no sweat, if they release it.",Neutral
Intel,almost there,Neutral
Intel,"We're a ways off from that still. These Strix Point 890M results are comparable to 1/2 the performance of the RX 6600. That's only good enough for \~30 FPS in Assassins Creed Mirage at 1080p Max.  I think this will be great for non-gaming purposes, like Adobe, Autodesk and so on. 890M should be a photo editing powerhouse.",Positive
Intel,"I mean current consoles are already APU power houses, they can give you 120fps depending on the game, and 30-60fps depending on what mode you select. And these consoles are pretty power constrained and pared down compared to PCs. So this APU here could easily double the performance of a console.   That's tapping on 4070/7800 levels of performance.",Positive
Intel,Never gonna happen as long as they use DDR memory.  The only powerful APUs are those that use GDDR or HBM. See: every AMD-powered console and the MI300A.,Negative
Intel,"Radeon iGPUs are mostly limited by the shared RAM bandwidth. I was thinking of getting an 8700G a little while ago, and the benchmarks varied wildly depending on RAM frequency and overclocks.  Maybe they'll improve it by hooking it up to a wider GDDR bus in laptops, similar to how the current PS5 and Xboxes work (IIRC?)",Neutral
Intel,The biggest uplift would be seen on lower power comparison.     Strix Point simply doesn't have enough bandwidth to feed all those GPU cores at high performance mode.,Negative
Intel,"This review is quite a bit different than the others.  The other paint a much more positive picture.  Also, so-dimm is much slower so expect worse performance.",Negative
Intel,"It depends on what your goals are for a laptop.  AMD added 4 CUs and 3% clockspeed increase but got only half the expected 36% uplift, so 2 CUs went to waste (memory bus bottlenecks)!   I would argue that the problem with laptops today is the horrible 100w+ chips from Intel, as Apple has proved with its wildly duccessful M1, M2, M3 chips.  If you agree with this, the Strix point chips use half the power of the AMD 884x chips and move alway from Intel Thighburner laptops, and this is the most important direction right now, as ALL recent Intel laptops have terrible energy efficiency ...",Negative
Intel,"likely memory bottlenecked severely and on-package memory will probably become standard for these types of chips thanks to Apple. the bandwidth benefits just can't be ignored anymore, especially with the slowdown and exponentially increased costs of node shrinks. Intel is already moving on it and I think the main thing holding AMD back is that they rely on 3rd parties for memory packaging so the capacity goes to the more lucrative enterprise chips first.",Neutral
Intel,"The iGPU uplift is extremely underwhelming, I guess this is why Asus did the ROG Ally X model instead of waiting for these chips. I wouldnt be surprised if Lunar Lake with Xe2 passes Zen 5's iGPU at lower power levels, at higher ones im sure RNDA 3.5 will be ahead.",Negative
Intel,yes its so bad. better go buy some steam deck or ally x,Negative
Intel,Low-quality trolling and shitposting. Spamming this same meme at different threads now.,Neutral
Intel,"If they put it in the next Razer Blade / Asus G16 laptop, I will instantly buy it.",Neutral
Intel,How are they going to feed all those CUs? Quad-channel LPDDR5X?,Neutral
Intel,That's considerably faster than an XSX.,Positive
Intel,>That's tapping on 4070/7800 levels of performance.  What is?,Neutral
Intel,"```That's tapping on 4070/7800 levels of performance.```   The PS5 Pro will land around there, but the current consoles are like 6700 ~ 6700 XT tier.",Neutral
Intel,"Your idea sounds good, been thinking about it myself, but the price is what determines its value.",Positive
Intel,CAMM2 (low power variant LPCAMM2) is already shipped in Thinkpad P1 Gen7 and its [specs](https://www.lenovo.com/kr/ko/p/laptops/thinkpad/thinkpadp/thinkpad-p1-gen-7-(16-inch-intel)/len101t0107?orgRef=https%253A%252F%252Fwww.google.com%252F&cid=kr:sem:cim8te&matchtype=&gad_source=1&gclid=Cj0KCQjw-5y1BhC-ARIsAAM_oKmKRTudxyl7UkjMEa1T5vUumlNVXVT6GwQitr32yqF1x7elrF3gBWoaAltREALw_wcB#tech_specs) show 7500MT/s,Neutral
Intel,Did the other reviews you looked at compare with a 780m with 7500 ram or have multiple 890m devices for comparison though?,Neutral
Intel,"bandwidth is mostly determined by the amount of channels, not whether the memory modules are in the same package or not",Neutral
Intel,"How is 40-60% performance uplift at half the power underwhelming? If anything it is the CPU performance and the usefulness of the NPU, which are the underwhelming parts of this package...",Negative
Intel,It's called satire. You're just salty because you're the butt of the joke.,Negative
Intel,throw it in the next steamdeck and I’ll upgrade immediately. If they bin the 890m they will have absolute monster in their hands.,Neutral
Intel,Praying the blade16 gets it.,Neutral
Intel,"This is the rumor, if you’re interested in detail:  https://videocardz.com/newz/alleged-amd-strix-halo-appears-in-the-very-first-benchmark-features-5-36-ghz-clock",Neutral
Intel,256 bit bus + infinity cache.,Neutral
Intel,I wish they would make a custom design for mini pcs and laptops that had quad channel ram and 8 cores with 3D Cache instead of 16 cores.,Neutral
Intel,"can be, if you put enough wattage at that I'm certain it can match or be better than PS5/XSX",Positive
Intel,"Yes, it’s like a desktop 7700XT or RTX4070! Juicy rumor, that one.",Neutral
Intel,The rumored 40CU strix halo chip. Not the actual chips released this week.,Neutral
Intel,7500mhz ram and the 780m,Neutral
Intel,"if you don't consider power, sure, but in that case you may as well go discrete. efficiency is a big reason for these AIO packages and on-package memory can prevent breaking the power budget while pushing higher bandwidth.",Neutral
Intel,"Indeed. Also the closer the memory is to the CPU, the higher the speeds, thus bandwidth. On-package memory will always be faster.",Positive
Intel,"Have _you_ looked at the actual game benchmarks in the review? The Ally X (a low power handheld) is within 1fps of the bottom of the 890m laptops. It's 5-9fps to the very fastest (again a higher power laptop!!), all at 1080p high settings which i think should  be the target for this range of entries in the roundup.  There is nothing like a 40-60% uplift in those games and that very standard resolution? I was stoked for Strix Point myself but this is super underwhelming.",Neutral
Intel,Literally where did you see 40-60% uplift at half the power?,Neutral
Intel,> 40-60% performance uplift at half the power  Source?,Neutral
Intel,"i chuckled, then again im not a fanboy of anything",Negative
Intel,Dont expect 40CUs in a handheld anytime soon,Negative
Intel,"Based on the results, it seems like the next steam deck might be more than a year away. Not particularly impressive gains from the previous gen.",Negative
Intel,"It'll need to be a custom tooled APU like Aerith/Van Gogh if it is to take full advantage of the 890m.     Nearly all of the configs that release of 16cu Point APU or 40cu Halo will be an APU slapped in a chassis without an adequate power or memory bandwidth setup for the igpu.     What we need is a Steam Deck with 6c12t of full zen5 and an 890m.  This chip should have custom power profiles set up, just like Aerith, so that the GPU takes a bigger share of the power budget and can actually perform at lower wattages.  The system should have an actual TRUE quadcore memory setup.  Many of these systems have currently (and will absolutely continue to have) dualchannel ram available to the igpu, and it cuts the bandwidth down which strangulates the igpu.     Each chip is on a 32-bit bus, so a dualchannel bus would come in at 64-bit, and with 7500mhz lpddr5x come out to ~60gb/s.  This matches my system that runs a 780m with 7500mhz lpddr5x.  In theory, a quadchannel setup would pump that to 128-bit and ~120gb/s.  This will continue to hamstring these APUs regardless of how many cu they throw at em.",Neutral
Intel,"“Absolute monster”? It is 1/4 the graphical power of M3 Max, and eats way more watts. We are talking about Steam Deck here, so you basically have the same catalog of games on SteamOS as you do on Mac/CrossOver.  If you want to go price to performance, the base M3 is the same performance for around the same prices (starting at $500 for Mac Mini and going up to $899 for MacBook Air, with SD OLED starting at $549 and going up to $649). (I am assuming if a new SD had a new chip, it would at minimum start at OLED prices.) With the SD you will get higher base storage and RAM (though in my testing on both systems, neither has been able to pull 8GB total system RAM use on AAA games, due to APU bottleneck.). On the Mac side you will have better build quality, higher resolution, more ports, better speakers and most importantly for mobile gaming you will have 6 hours plus of AAA gaming. Where as there were some AAA games that killed my deck in 1 hour, with most dying around the 2 hour mark.    AMD has a long way to go before claiming “Monster” class APUs. 890M gets absolutely destroyed by the fanless ultra thin tablet mobile APU in the iPad. AMDs desktop APUs with full fat coolers and pulling watts from a wall outlet aren’t even close to being in the running with a tablet, let alone M3 Pro.. Let alone M3 Max… let alone M2 Ultra. Its desktop tower chip is behind the entry level mobile OS chip from its competitor. It is a decade behind the desktop chips of its competitor, itis hardly Monster class.",Neutral
Intel,Blade 16 with AMD HX 375 and RTX 5070 along with dual display mode. Dream laptop.,Neutral
Intel,"Even for Strix halo, most optimistic prediction puts it on a level with _mobile_ 4070. That’s far from desktop 4070, never mind 4080.",Neutral
Intel,A real one.   https://www.anandtech.com/show/21485/the-amd-ryzen-ai-hx-370-review/9,Neutral
Intel,Everyone sane would seem like a troll for fanatics enthusiastically living in a different reality.,Neutral
Intel,">AMD has a long way to go before claiming “Monster” class APUs  AMD doesn't need to make ""Monster"" class APUs as they cater to the x86 desktop market where they make ""Monster"" dGPUs which can be upgraded independently.   And AMD ""can make"" such APUs -> PS5 Pro (as a more cost effective solution). AMD isn't like Apple who can make up the expense of creating a mega sized APU by selling a finished product/selling services etc.",Neutral
Intel,"APU is one of AMD’s biggest markets. You are kidding if you think they don’t need to compete there. They are way behind the race with Nvidia in desktop cards so that is irrelevant, unless your point was to say that they don’t need to compete anywhere and they should always be in second place.     AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant. The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra. It sucks way too many watts for that level of performance (which also accounts for cost). Not to mention games aren’t the only thing APUs are used for so PS5 isn’t wholey in the conversation. PS5 also costs monthly to play online and their games aren’t more expensive than PC so the whole cost savings thing is thrown out the window when you consider the real money being spent. Apple is a hardware first company and thats where the bulk of their profits come from, not services. Especially on Mac where there are little services at all people would even use there that have a subscription or software for sale.   If services were the reason, then for sure you would be able to buy a Surface Laptop powered by an AMD APU that puts MacBooks to shame, considering all your data Microsoft is selling, along with Office sub sales, and all the ads and preinstalled third party software. But instead Surface laptops are priced around the same as MacBooks and they have less powerful APUs and the AMD version suck up battery life.",Negative
Intel,"Not a single point of yours make sense.   ""APU is one of AMD's biggest markets"" - No. The major APU customer of AMD is Sony and Microsoft for their consoles. Not the general public as it's going to very expensive to sell PS5 type APU in the open market. 8700G costs 330 usd which is crazy.  ""The PS5 Pro sucks. It performs worse than M2 Max and M2 Ultra."" - Interesting, you already have comparisons between an unreleased console and an Apple laptop/desktop. Oh and how much does the cheapest M2 Max and M2 Ultra machine cost?   ""AMD cannot make such APUs. Their GPU cores suck 1 to 1 core to core compared to Apple’s, so the size comparison is irrelevant."". No idea what benchmark you are referring, what metric you are comparing.   However I can provide some idea on CPU cores and die size as cross platform benchmarks are available.  Cinebench R24 Multicore:  2x71 mm2 16 core 7950X: 2142 pts   2x70.6 mm2 16 core 9950X: 3000 pts  1000mm2 M2 Ultra: 1918 pts  So yea, Apple's solution is simply throwing more money at the problem. A budget RTX 4070m/7800m will crush an M2 Max in pure GPU grunt.",Negative
Intel,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
Intel,"Hey OP — PC build questions, purchase advice and technical support posts are only allowed in the [Q3 2024 PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).  For help building your system, purchase advice, help choosing components or deciding on what to upgrade, we recommend visiting /r/buildapc or using [PCPartPicker](https://pcpartpicker.com/).  For technical support we recommend /r/AMDHelp, /r/techsupport, [the official AMD community support forums](https://community.amd.com/t5/support-forums/ct-p/supprtforums) or [contacting AMD support directly.](https://www.amd.com/en/support/contact).  If you have found bug or issue with AMD software or drivers and want to report it to AMD, please use the [AMD Bug Report Tool](https://www.amd.com/en/resources/support-articles/faqs/AMDBRT.html).  The [subreddit wikipedia](https://www.reddit.com/r/Amd/wiki/index) is also available and contains answers to common questions, troubleshooting tips, how you can check if your PC is stable, a jargon buster for FSR, RSR, EXPO, SAM, HYPR-RX and more.  The [AMD Community](https://discord.com/invite/012GQHBzIwq1ipkDg) and [AMD Red Team](https://discord.com/invite/k4wtjuQ) Discord servers are also available to ask questions and get help from other AMD users and PC enthusiasts.  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification.",Neutral
Intel,Gotta remember that it's Intel's first line of GPUs. It's going to have issues ofc. Even now they're still improving. And it's only going to keep getting better from here on out,Positive
Intel,"Ok mate, take a first gen product and compare it to a 7th or 8th gen product.  Intel has their issues, anyone buying into them should have known that.",Negative
Intel,"You probably setup VRR wrong, whether that wasnt enabling V-Sync (yes, youre supposed to for VRR), or you tried to use an older HDMI standard, or had a bad driver install and didnt clean install new drivers. Because it absolutely does work as intended with Arc. Arc's VRR is based on VESA's adaptive sync, like Freesync and G-sync compatible also are.  As for A750 performance being worse than a 6800 XT, duh. One card sells for $180, the $450, they are in completely different price and performance tiers. Just like a 7900XTX would make your 6800 XT look like its junk.",Negative
Intel,6800 ultra??? EDIT: so im a dumb it's a nvidia gpu that was made 20 years ago,Neutral
Intel,"Don't be afraid to voice displeasure with any of the hardware vendors, otherwise you end up like the Nvidia stans.  Grats on the upgrade.",Neutral
Intel,"I don't recall any real driver issues with my 9700 and 9800 pro. None specific to ATi at least,  rather just the norm for Windows XP era gaming.",Neutral
Intel,"My experience with my RX 5700 was also really bad in the first months. Driver timeouts, blackscreens, game crashes. Not even exaggerating. Never thought I'd ever buy an AMD GPU again.      Now I have a RX 7800 XT and very happy. No game crashes due to driver issues, no blackscreens, everything is fine.",Negative
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2024, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1dsetov/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,bruh. This is Intel first generation of discrete GPU. it's damn impressive how fast they are improving. I like AMD too but Intel is doing a pretty good job there,Positive
Intel,I had an arc a750 as a placeholder until I got. A 6950xt and I love it so much. Except amd still hasn't fixed the ghost of tsushima issue other than that it's been phenomenal and I get over 60fps in almost every game at 4k,Positive
Intel,"Well one great thing you have to look forward to is amd is going all in on software. They already said FSR with AI is coming, and I have a feeling a lot more. We should be seeing some pretty cool software features coming out now that they have more employees for software",Positive
Intel,"Actually not. Intel i740, released long time ago was the first discrete GPU from them.",Neutral
Intel,"I'm keeping my eye on Intel gpus, but I certainly won't be a first adopter. Honestly I'll be even more skeptical now with Intel's recent issues with 13/14 gen cpus. All in all though more competition is always good for us consumers. If Intel can be competitive in the budget market it will at least put a fire under amd to lower their prices/make a better valued product.",Negative
Intel,That would be fine if no one else had ever invented a GPU until Intel did.   The fact is there's lots of architectural precedent for Intel to have learned from that they just...didn't. Problems that Nvidia and AMD both solved decades ago that are holding Intel back in 2024.   It's not a mystery how a GPU should be built but that didn't stop Intel from not figuring it out.,Neutral
Intel,"Installs beta software, proceeds to complain about it",Neutral
Intel,Doesn't make it less of a fact that users are experiencing issues and they still paid hard cash for those GPUs.,Negative
Intel,"Nope it was set up correctly and verified by Intel insiders discord ARC engineer team also verified it was set up by multiple people Intel acknowledge the VRR was not working as intended but had no solution and all drivers cleaned in safe mode with DDU.  VSYNC with VRR, both on and off, also verified to be working via windows confirmation, connected to Display Port because ARC does NOT support VRR over HDMI 2.0 and needs minimum HDMI 2.1  I am also a experienced PC Technician for over 2 decades.  The 6800 XT just works, right out of the box rock solid functionality, period!  I just happened to have a monitor capable of reporting extra statistics and I have knowledge of using Frog Pursuit from aperture grill to test both backlight strobing cross talk and VRR functionality for each individual monitor and GPU my monitor is also a Blur Buster 2.0 certified monitor  after realizing it was an issue with ARC I ordered the 6800 XT, removed ARC and ran DDU in safe mode.  Slapped in RX 6800 XT, installed newest driver and VIOLA, works beautifully first attempt with zero configuration whatsoever. Forget about the raw power we know the 6800 XT is obviously in a class far above anything Intel is currently offering so is it's price. It's just unfortunate the ARC fails to match even a 6600 XT in UE5 games but it's gonna be fixed with battlemage rest assured.   The ARC architecture just isn't there for UE5, drivers won't fix that performance issue, AMD just happens to do extremely well with UE5 because their architecture is more mature.  The bottom line the AMD drivers are obviously and understandably light years ahead of Arc drivers.  Nothing wrong with that ARC is a beta product that's why Intel doesn't build a 4090 or 7900 XTX competitor because drivers are their current issue not hardware.  Again there is NOTHING wrong with ARC having these issues it is a beta card, Intel specifically warned AGAINST buying it if you need a reliable card, I bought it to help intel and test it our of curiosity, I wasn't really prepared for that much issues but it's fine it has a happy new owner who isn't even using if for gaming he is using it for AV1 encoding.  I am just glad I could help out with the sale for a 3rd party vendor in the race here and am even happier I got rid of it and it has a new owner who isn't using it for gaming  It was an impossible sell for gaming nobody wanted it for gaming sadly but it worked out for me in the end",Neutral
Intel,What Ghost of Tsushima issue?,Neutral
Intel,"That was back in the 90's... While it would technically be their first, absolutely nothing from that dGPU wouldve carried over to Arc, its so old that its irrelevant to talk about.  You could also say DG1 from 2020 could be their 'first' dGPU since it was their first modern dGPU oriented at consumers, albeit it was clearly just an ultra low volume test platform to figure some stuff out prior to the Arc launch.  Most people would consider Alchemist (Arc gen1) as Intel's first dGPU, even though it technically isnt, it's still the most relevant one.",Neutral
Intel,"This was a graphics card, not a ‘GPU’ in terms that we understand them now, just to bolster the point about how much of a disparity this comparison reveals.",Neutral
Intel,"Arc is not beta, neither the hardware, firmware or sofware. Intel does not refer to it as beta, why should the consumers do so? They paid full price for a product and it should work as advertised.  With that said, the issues with Arc are widely known and complaining about it after the fact is a bit sillly at this point.",Negative
Intel,If you're playing ghost of tsushima with her enabled it will crash your drivers and you'd have to re-download them via integrated graphics on your cpu,Negative
Intel,"It's not completely irrelevant, as it shows they already had GPU produced before. That GPU had driver issues same as ARC and i believe same will be passed on to BATTLE MAGE.",Neutral
Intel,"Dude, GPU is not same as graphics card. i740 was a GPU in a same way nVidia RTX and AMD RX series are today.  You are mixing them up because todays graphics cards have names same as the GPU used on them.   Heres a bit of good ol' Wikipedia:  [Intel740 - Wikipedia](https://en.wikipedia.org/wiki/Intel740)",Neutral
Intel,Graphics Processing Unit.  Maybe you're confused and thinking of GPGPU?,Neutral
Intel,"her?   i cant say i encountered any problems other than launching with FSR activated crashed the game, but it was optimized enough that you dont need FSR at all (also a ps4 game port which helps)",Neutral
Intel,It's irrelevant because it's from so long ago the people who worked on it are likely no longer working at Intel so there's no organizational knowledge to transfer into designing Arc.,Negative
Intel,Not irrelevant though is that Intel has been making iGPU drivers for the last 20+ years with massive marketshare and still don't get it anywhere NEAR right.,Negative
Intel,The documentation for it would still be in their archives,Neutral
Intel,"""last updated by unknown user at 3:26AM March 15th, 2003""  Please keep this page updated. It's our only document for this application.",Neutral
Intel,"Pretty annoying how everything follows the same linear fps/price curve, there’s no advantage from buying the cheaper cards as there used to be in earlier generations years ago.",Negative
Intel,Wish Arc cards were better. They look so pretty in comparison to their peers,Positive
Intel,Thats actually a pretty solid and accurate breakdown.,Positive
Intel,I like the part where they declare that 8 GB of VRAM is not enough for today.   But that was a very well done article.,Positive
Intel,3080 still looking good too,Positive
Intel,What they have peaceful then 4k series?,Neutral
Intel,Just get a 4090. I will never regret getting mine.,Neutral
Intel,i miss old good times where radeon HD 7970 as best single core card cost around 400$,Neutral
Intel,"Damn, the A770 is still so uncompetitive...",Negative
Intel,"It's like the free market priced cards according to their relative performance. How weird, right?",Negative
Intel,How is that possibly annoying,Negative
Intel,Honestly the Nvidia Founders edition in person is the best looking card I've ever seen.,Positive
Intel,"I bought an ARC A770 16GB card for experimentation and my experience seems to have been better than computerbase.  I had no problem using it for 3440x1440 without raytracing. I have to reduce some settings in the heaviest games, but then I can hit 60fps in most games without using upscaling.  It makes me wonder if they have used older drivers, since they don;t even get 60fps rasterized at 1080p in some games.  edit: And I paid much less than the minimum price they are listing, I'd need to check if prices went up - even though computer base suggest that isn't the case. The bigger problem still, but getting better, is that when it doesn't work it's really really terrible.",Positive
Intel,"Well I mean... I guess it depends on what you're wanting to do of course, but even my 12 GB card was struggling to do raytracing a couple years ago, so that claim isn't really far fetched.  My 20 GB card struggles to hit 60 fps with path tracing at 1440p",Negative
Intel,I have a budget build for my vacations off grid with arc a380 heavy oc pushing 2750mhz. Works amazing for 1080p e sport titles and some heavy games low settings around 50-60fps.. off no ray tracing lol.,Positive
Intel,8gb perfectly fine today :),Positive
Intel,"Ah yes sure, now where did I leave my 1500 euros?",Neutral
Intel,"I don’t mind free markets, I’m just saying the state of the market is less fun now than it used to be.",Negative
Intel,Something about that sexy look of my GTX 1080 fe is gonna make it very hard to replace it.,Positive
Intel,"Yeah, i like the black super series.",Positive
Intel,"But that's not because your GPU has 20gb vram, that's because AMD doesn't perform well in RT and especially not in PT I promise you a 16gb 4080 will run circles around your 7900xt with PT.  And no I'm not an Nvidia chill I have a 7900xtx myself",Negative
Intel,"people have more information more easily available now, so they know what a good price is for a gpu.   Yeah, you can't a good deal on older cards just because they're old, but you can get more money for your old cards yourself when you wanna upgrade.",Neutral
Intel,I think this needs more mainstream coverage - someone like Wendell@Level1Techs should be interested in this and related phenomena.,Neutral
Intel,"Same experience when using AMDGPU on Linux. Hardware rings will reset after timeout, but you have no guarantee that functionality will return to normal after the reset. The only solution is to reboot the entire system. The video codec rings VCN/VCE/UVD is seriously affected by this. But there seems to be nothing the kernel developers can do about it. [https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note\_2236916](https://gitlab.freedesktop.org/drm/amd/-/issues/3098#note_2236916)",Negative
Intel,"""The ability to “turn it off and on again” should not be a low priority additional feature""  THANK YOU    Please please please AMD fix this. I use your CPUs and GPUs, and have for a long time. I am also a some time VFIO user, and I do NOT want to have to buy an NVidia GPU for this purpose.",Neutral
Intel,">listen to them and fix the bugs they report  AMD have been dropping the ball on this for decades, and aren't about to pick it up any time soon. It is genuinely astonishing how poor their bugfixing/driver development approach is. I filed a bug recently and was told they didn't have a single windows machine with a 6700xt available on for testing/reproing a problem, which...... is quite incredible",Negative
Intel,"""EDIT: AMD have reached out to invite  me to the AMD Vanguard program to hopefully get some traction on these  issues \*crosses fingers\*.""  That is a great idea actually and I vouched my support on the matter.",Positive
Intel,"They couldn't care less. We've had issues with AMD drivers in a video production house where we ran Vega GPUs under Linux for DaVinci Resolve editing on the desktops and for rendering on the farm.   Those were the worst years of my life where I had to support the investment that failed as soon as the decision to go with AMD was made.   It costed our company the weight of those cards in solid gold.   After years of battling AMD and failing, I made an ultimatum to our ceo and told him directly that I didn't want to support this anymore and that I'd leave if we didn't switch everything to Nvidia and I actually quit the company over this because the response was that it was impossible. 2 months later they sold all the AMD hardware at a fraction of the original price and managed to take a credit to switch everything to NVIDIA.  Somebody else even made a huge post here and on r/linux, phoronix covered it slightly and AMD went into full panic mode, their developer advocate came here and on AMD forums and in emails and made many grand promises. Here we are almost 10 years later, same issues still exist.  Oh yeah, and BlackMagic (DaVinci Resolve maker) today officially doesn't support their software on any AMD hardware. Thousands of editors, graders and admins go on forums and ask about AMD only to just get directed to Nvidia by the BlackMagic staff.  Great job AMD! You don't deserve a single customer...",Negative
Intel,"Bit of a rant, but I have an AMD 6700XT and do a wide variety of things with my computer. It feels like every way I look AMD is just completely behind in the drivers department..  * Compute tasks under Windows is basically a no-go, with HIP often being several times slower than CUDA in the same workloads and most apps lacking HIP support to begin with. Blender Renders are much slower than much cheaper nvidia cards and this holds true across many other programs. DirectML is a thing too but it's just kinda bad and even with libraries as popular as PyTorch it only has some [half baked dev version from years ago](https://github.com/microsoft/DirectML/issues/545) with many github issues complaining. I can't use any fun AI voice changers or image generators at all without running on CPU which makes them basically useless. [ZLuda](https://github.com/vosen/ZLUDA) is a thing in alpha stage to convert CUDA calls to HIP which looks extremely promising, but it's still in very alpha stage and doesn't work for a lot of things. * No support for HIP/ROCm/whatever passthrough in WSL2 makes it so I can't even bypass the issue above. NVIDIA has full support for CUDA everywhere and it generally just works. I can run CUDA apps in a docker container and just pass it with --gpus all, I can run WSL2 w/ CUDA, I can run paravirtualized GPU hyper-v VMs with no issues. * I'm aware this isn't supported by NVIDIA, but you can totally enable vGPUs on consumer nvidia cards with a hacked kernel module under Linux. This makes them very powerful for Linux host / Windows passthrough GPU gaming or a multitude of other tasks. No such thing can be done on AMD because it's limited at a hardware level, missing the functionality. * AMD's AI game upscaling tech always seems to just continuously be playing catch-up with NVIDIA. I don't have specific examples to back this up because I stopped caring enough to look but it feels like AMD is just doing it as a ""We have this too guys look!!!"". This also holds true with their background noise suppression tech. * Speaking of tech demos, features like ""AMD Link"" that were supposed to be awesome and revolutionize gaming in some way just stay tech demos. It's like AMD marks the project as maintenance mode internally once it's released and just never gets around to actually finishing it or fixing obvious bugs. 50mbps as ""High quality""? Seriously?? Has anyone at AMD actually tried using this for VR gaming outside of the SteamVR web browser overlay? Virtual Desktop is pushing 500mbps now. If you've installed the AMD Link VR (or is it ReLive for VR? Remote Play? inconsistent naming everywhere) app on Quest you know what I'm talking about. At least they're actually giving up on that officially as of recently. * AMD's shader compiler is the cause of [a lot of stuttering](https://www.reddit.com/r/Amd/comments/12wizig/the_shader_cache_stutter_on_amd_is_way_more/) in games. It has been an issue for years. I'm now using Amernime Zone repacked drivers which disable / tweak quite a few features related to this and my frametime consistency has improved dramatically in VR, and so did it for several other people I had try them too. No such issues on NVIDIA. The community around re-packing and modding your drivers should not even have to exist. * The auto overclock / undervolt thing in AMD's software is basically useless, often failing entirely or giving marginal differences from stock that aren't even close to what the card is capable of. * Official AMD drivers can render your PC completely unusable, not even being able to safe mode boot. I don't even know how this one is possible and I spent about 5 hours trying to repair my windows install with many different commands, going as far as to mount the image in recovery environment, strip out all graphics drivers and copy them over from a fresh .wim but even that didn't work and I realized it would be quicker to just nuke my windows install and start over. Several others I know have run into similar issues using the latest official AMD drivers, no version in particular (been an issue for years). AMD is the reason why I have to tell people to DDU uninstall drivers, I have never had such issues on NVIDIA. * The video encoder is noticeably worse in quality and suffers from weird latency issues. Every other company has this figured out. This is a large issue for VR gaming, ask anyone in the VR communities and you won't get any real recommendations for AMD despite them having more VRAM which is a clear advantage for VR and a better cost/perf ratio. Many VRchat worlds even have a dedicated checkbox in place to work around AMD-specific driver issues that have plagued them for years. The latency readouts are also not accurate at all in Virtual Desktop, there's noticeable delay that comes and goes after switching between desktop view and VR view where it has to re-start encoding streams with zero change in reported numbers. There are also still issues related to color space mapping being off and blacks/greys not coming through with the same amount of depth as NVIDIA unless I check a box to switch the color range. Just yesterday I was hanging out watching youtube videos in VR with friends and the video player just turned green with compression artifacts everywhere regardless of what video was playing and I had to reboot my PC to fix it. * There are *still* people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  If these were recent issues / caused by other software vendors I'd be more forgiving, I used to daily drive Linux and I'm totally cool with dealing with paper cuts / empty promises every now and then. These have all been issues as far back as I can find (many years) and there's been essentially no communication from AMD on any of them and a lack of any action or *even acknowledgement of the issues existing*. If my time was worth minimum wage, I've easily wasted enough of it to pay for a much higher tier NVIDIA GPU. Right now it just feels like I've bought the store brand equivalent.",Negative
Intel,"Yo, I saw the title and thought this gotta be Gnif2.",Neutral
Intel,"And I'm over here struggling to keep an Nvidia T4 passthrough to work reliably on Hyper-V to Ubuntu 22.04. :(  Is there a specific software combination that works more reliably than others?   Also, what do you think is the core fix here? Is it hardware design, in the firmware, drivers, combination of everything? If it was an easy fix, you'd think AMD would have fixed it.  When Hotz got on Twitter for a particular issue, AMD seemed to jump on it and provide a fix.  But for these larger issues they don't.  Could there be a level here where the issue is really the vendors design and how they implement AMD's hardware?   Some of the most powerful super computers use Instinct.  Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade, which Oak Ridge has done.  They working with some kind of magic radiation over there?",Negative
Intel,"I've got a 7900XTX for a year now, and I've not had any stability or performance issues with it, so far at least.  What does bothers me though, is that 1 year later I still cannot connect my 3 monitors to the card without it sucking 100watts at idle, and recent drivers don't even mention that as an issue anymore, so it's not even being recognized as a problem by AMD.  This happens even if my monitors are turned off, I literally have to go under my desk and pull out the cable to resolve this, obviously rendering my extra monitor useless.   So now I'm looking to upgrade my cpu (5800x) to one with an integrated GPU so I can connect my secondary monitors to the iGPU so my system doesn't constantly suck an obscene amount of power doing absolutely nothing.  You're free to guess what vendor om looking at to replace my CPU with. Damn shame really.",Negative
Intel,"Fact: AMD does not give a shit about any of this.   We still have CPU scheduler issues, we still have NUMA issues when dealing with latency sensitive PCIE deployments, the famous reset bug in your OP, lack of Vendor relationships and unification across the platform (IE, Epyc, Radeon/Instinct, AMD Advantage+, ...etc).   In the years since Zen shipped, it took an act of god to get them to move. Maybe Lisa remembers those meetings we pulled with Dell, HP, and VMware back then. Where the cloud providers that adopted Epyc 7001 early were all very pissed off at the over all performance because of the failure of AMD to work with the OEMs to correctly adopt how NUMA changed. Because they did not get any guidance from AMD engineering on the matter until after these SI's were mid/full deployment.   So yes, I doubt AMD is going to take your OP any more serious then they took the NUMA issues until it starts to affect their bottom line. If all CDNA customers switch to NVIDIA and those PO's dropped in volume, it might make them care a little bit.",Negative
Intel,"6600xt reset just fine but my 6800, oh boyyy. amdgpu refuses to unbind it so I can restore it to the host. Thank you for all the great work!",Positive
Intel,"I’ve been buying ATI / AMD since the ATI Rage 128, and I think my next GPU will be Nvidia. I primarily game on my 6950XT, but sometimes I might try to mess around with an AI tool, or some sort of tool that uses GPU compute. Every. Single. Time. It is a massive PITA and most of the time I end up giving up and moving on. The most recent time it involved using an AI tool to restore a photo. After hours of screwing around on Windows and Linux I ended up just having a friend with a 3080 do it for me. He had it working in 10 minutes.   And when stuff (outside of gaming) does work, it’s usually a day late and a dollar short. Blender on Linux still can’t do hardware RT in Cycles (it can on Linux), and general HIP support tool far too long.   The argument can be made that there’s no need to worry about this if you only game, but unless price is an issue, you may be locking yourself out from testing a cool piece of software later.   I guess it really depends on if things are improved when it comes time to buy a new GPU, but we’ll have to wait and see.",Neutral
Intel,"I promise you the Vanguard program will yield nothing. ""*AMD Radeon*™ *Software Vanguard* Beta Testers are selected community members with exclusive access to early drivers to provide critical feedback.""  Basically they made a program out of you doing free QA work for AMD. Don't fall for it.  Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  These issues haven't been fixed for a decade. I doubt AMD is capable of fixing them. I think a lot of community people could with docs and source, but AMD doesn't even seem willing to take that step.",Negative
Intel,"[Wish i could play Hell Divers 2 but when i bought it took 30 seconds to get a driver timeout,](https://i.imgur.com/FqM9MRx.mp4) anyway i decided to not switch NVIDIA cos i also well usually play a lot of World of Warcraft but that game has problems for both AMD in form of freezes and driver timeouts gradually getting worse until you update drivers again, cos shader cache gets reset it stops crashing again for couple of days, then starts crashing more frequently and the frequency varies per user and what they doing as well as if their some sort of memory leak.  Also some other games having driver timeouts to, but i have games that also never timeout.  Speaking of which users started reporting flickering issues in browsers such as chrome, or any chrome based browsers, and their 2 reports of it being fixed after MPO is disabled so i guess MPO issues are back on the menu.  [Also i would love to see AMD Gaming YouTube channel to play and livestream Horizon Zero Dawn with HDR turned on in game using AMD relive ](https://i.imgur.com/1RtZtsi.mp4)  Their also way more issues then i just mentioned i have like 41 commonly reported issues from reddit and forums that not been fixed in 24.3.1 and its still going up, some of my own reported issues as well.  I highly recommend AMD to have public bug tracker for reporting issues also games, allow users filter on games to see all the user reports for that game, have it all consolidated into same issue if its the same issue, allow users only to upvote remove down vote, i do not have any issues does not contribute to fixing problems it encourages ignorance nothing personal against anyone not having issues, i often have no issues to but they are not proof of stable drivers, they are just one user experience not everyone user experience, everyone is allowed to speak for them self, AMD does not require any defending, the only time its appropriate is when AMD is treated unfairly missing from benchmark charts unfairly.  Also not all issues are always caused by AMD but that does not give AMD the right to ignore it, especially considering their plenty of problems usually, it just means AMD is lacking in the compatibility departement and the whole anti-lag+ debacle says enough about that, alto i really liked that feature i would rather blame cheaters, cos without cheaters you would not need anti cheat, and this would be less of a problem, still says more about fact that their probably should be something like api support for features such as anti-lag+ but also AMD enhanced sync or NVIDIA features.  I think developers and studios etc all should work together, instead of trying to sabotage each other for the sake of monopoly i am looking right at you NVIDIA just stop.",Negative
Intel,Long but worth it read; Well Done!,Positive
Intel,"Business opportunity for EEs now: Time to make some custom PCIe adapter boards with a bunch of analog switches for cycling all power and signal lines on the PCIe slot, then sell them for use in corporate AMD GPU deployments. Sure, toggling PCIe signal is expensive, as it's basically a radio signal at ~10 GHz. Toggling the 12 V power input is also expensive due to the high current. But both are ""just"" expensive but doable. The cost, at worst, is an expensive relay for power, and additional PCIe redrivers or switches for signals. ""It's one PCB, What could it cost, $100?"" If corporate users have already paid hundreds of thousands of dollars on AMD GPUs, and now someone's offering a solution to actually make them usable at a fraction of the original hardware cost, it must be selling great.  On second thought, the hardware must be certified and pass PCIe compliance tests and electrical safety tests before they're acceptable for big corporate users. Even then, most are not in a position to do any hardware modification (including adding additional hardware). So the ""proper"" way of doing so would be first contacting a big corporate user first, ask them to request this feature from server vendors like Super Micro. Then you need to pass this design to them, and they pass this design to Super Micro, and it will only appear in a next-gen server... This makes this workaround largely impractical. I guess that's why nobody is already doing it.",Neutral
Intel,I had the same problem with the Vega 64 liquid edition...    On my PC the 6800xt is working ok... The 7600 on my work pc is SHIT ... Same problems with Vega and if you have a second monitor is x2 :(,Negative
Intel,"The reset issues also happen in Windows, even when it recovers after 5 mins (what the hell it's quicker to reboot, nvidia cards reset in 10s max), the card is not fully reset and some issues i personally noticed with display detection/wake-up not working normally;   Also in a crash UEFI portion doesn't load properly so either the bios resets CSM to enabled, or if your mobo/bios doesn't do this it will go without video output until windows loads. This is with 6900xt, huge FAIL in my opinion.",Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are. This issue is plaguing your entire line, from low end cheaper consumer cards to your top tier AMD Instinct accelerators.  Not over here my guy. I switched from a 1080 Ti to a 6800 and it actually fixed crashing issues I was getting in Cyberpunk. Used that 6800 for over 3 years with no issues, and then switched to a 7900 XTX and also no issues.   I also have a used 7600 I bought cheap for one of my TV computers, and that one has also been fine, even when I borrowed it out for a while to a friend so he could run Helldivers 2 without the text graphical glitches he was getting with his old 1080 Ti.  I know there are some issues with AMD drivers, just like there are issues with Nvidia drivers, but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are and I'm over here using them for years with no problem.",Neutral
Intel,AMD solftware stack is lacking hard. . . The AI / LLM issues recently and now this. AMD needs to invest in it's software side now.,Negative
Intel,I’ve been using the 6800xt for almost a year now and from the crashes to the timeouts I decided that im gonna pay the green tax so i paid 900$ for a 4070ti and magically all of my problems disappeared as much as i love AMD i just cannot recommend their GPUs,Negative
Intel,"Thanks for bringing some sense of consumer advocacy towards VFIO.  Very difficult dealing with AMD lately, especially with RMAs on busted CPUs/GPUs (had Vega and 5950X die on me). Let us know how the Vanguard(trash name) program is.",Negative
Intel,Exactly why I got rid of my 7900XT and went back to using a GTX 1080.  The constant crashing was driving me nuts.,Negative
Intel,"why invite to a conference instead of directly contact gnif and fix the problems 5 years ago? why does gnif need to create the reddit post, begging amd to fix their shit? Why can't amd fix the problems without external impetus? It says a lot about the company.",Negative
Intel,"AMD bugs is why my workstation runs Nvidia, I'm hoping Intel moving into the GPU Space is a wake up call to AMD.  I had these issues as well.",Negative
Intel,"Nevermind, you just came to do god's work, and a very good one btw, to find the same fanboys ""I've had Bla Bla years experience and bla bla I game and bla bla never had problems with AMD.""  God damn those guys are just blind. Every time I say the truth about the instability of AMD software, I just get downvoted by people that are just blind. I think they're the defense of a brand like it they are defending their sports club.  We're stuck between the overpriced that just work, and the nightmare the AMD software overall is. I get it for the normal user and some power users, if we look at normal windows usage, adrenalin is such a good attempt to have everything on one software bundle, the overclock, the tuning, the overlay, the recording. All in one GUI that makes it easy. In theory, it is a good attempt.  Note I said attempt...  I'm not debugging the same as you are, I am mostly troubleshooting, I only use regular Linux, normal windows, virtualize one machine I use and some I try also virtualized, and configuring some basic routing through Linux server, but still I bought one AMD card, and I already did more than 6 bug reports to AMD to fix a bug with my specific hardware setup regarding the adrenalin causing stuttering in the OS every few seconds and in my long IT experience not focused on the debugging and coding of things but more on the troubleshoot and fixing of computers, hardware/software wise I must say that what I think is: They tried to do it all in one, they wanted to put the foot on the door to face the overpriced green team, great software/hardware specs, something that would put normal users with a power software suit that could do it all. Except it can't.  Constant thousands of posts regarding crashes, hangouts, reboots, tweaks, registry edits, hotspots over 100ºc, incompatibilities with the OS, everything is to blame on the system except the AMD GPU. Chipset drivers that can't clean old drivers on install and create registry entries mess, GPU drivers that, will mostly work if you always do clean install, but with a software bundle that causes too much conflicts with the driver itself etc etc  I know Microsoft is complicated, but we're not talking windows millennium here, and if other brands manage to have drivers/programs that actually work with the OS, why can't AMD, and why do the warriors for AMD blame the OS, the PSU, the user, everything except AMD, when it is their favourite brand to blame?  And when you want to factually discuss it to have maybe a fix, a workaround, a solution, some software written by someone like you that actually fixes things, something, what do you get?  ""I have had X AMD GPUs, with Y experience in computers, never had a problem!""  Or even better, ""That is because you suck at computers"" said by some NPC that doesn't even know what an OS is..  I really hope your letter gets where it needs to go, and please keep up the good job. I still hope AMD steers in the right direction so it can put Nvidia to shame(I want to believe poster here). Not because I have something against this brand or the other, but because we need competitors, or else you'll end up paying 600$ for a nice system, and 6000$ for the GPU. Competition between hardware manufacturers is overall good for innovation, and good for our wallets.",Negative
Intel,Lmao as a recent AMD intern I feel this in my bones. I still can’t fathom just how little effort is put into software stability these days.,Negative
Intel,100% all of this...  Love looking glass by the by,Positive
Intel,How does say VMware handle this? Does it kind of just restart shit as needed?,Neutral
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.   Wait really? How come I never noticed this on over 15-20 amd GPUs since 2016, I game a lot and use them for 3d modeling... Always stable as a rock.",Neutral
Intel,"well you know what, I got a amd 7950x based machine with a 6800xt and 7900xtx with unraid handling 2 windows vm. I agree that rdna3 cards are more difficult to run but man the 6800xt worked well without doing anything and 7900xtx only needed a few clicks. for cards not meant to do this it's quite good. btw build has been running flawlessly since feb 2023",Positive
Intel,"I really think AMD gives users too much control. They've popularized Precision Boost Overdrive and tuning your GPU within the driver which dramatically will increase the issues people have.  For example: black screen restarts will significantly increase when PBO is on during gaming even without curve optimizer. Do you know how many issues I've helped people fix ""with their gpu"" by just resetting their BIOS and turning on XMP?  Also, too many people go online and watch a 5 min tutorial on GPU overclocking. They throw on Fast vram Timings, undervolt their card, overclock the core, and set a fan curve with 0 testing.",Negative
Intel,AMD lost a graphics card sale to me because of this issue -- Went with the 4070 instead of the 7800xt.,Negative
Intel,"As a Linux user I feel your pain!  Even more as there are a lot of programs and game that either don't work at all with compatibility layers or they still have a lot of problems even if they work.  And that's besides the extremele huge amount of time wasted with the ""trial and error"" to find a working combination of configurations.  A properly virtualized Windows would solve so many problems until more programs and games become Linux compatible, either natively or through compatibility layers.  The moment a GPU vendor takes virtualization properly and works on the consumer GPUs and works well, I'm gone!  Price doesn't matter as much for mas the quality!  So AMD, please stop with the bullshit that virtualization is required / needed for enterprise cases only and make it work well for all the consumer GPUs, or get lost!  I'm really tired of this crappy attitude!  I'm already very upset upset that a 30 dollars Raspberry Pi has CEC support to control the programs on it with the TV remote and your 10-20 times more expensive GPUs don't!",Negative
Intel,"> Those that are not using VFIO, but the general gamer running Windows with AMD GPUs are all too well aware of how unstable your cards are.  Hyperbole - most people have few issues - this is one of those perceptions that isn't really matched by reality.  Things like ROCm are definitely still flaky, but gaming is basically fine - it's not as if Nvidia drivers never give people issues. If AMD's drivers were as bad as people make out (for gaming), no one would ever buy them.",Negative
Intel,"does crashing a OC on desktop on GPU, reset CPU PBO settings from bios still ?",Neutral
Intel,"Agree with the post. As someone in the industry (and a homelab), we all know buying amd is a compromise.",Neutral
Intel,"a few years ago I emailed Lisa Su about a big problem with Instinct GPU offerings in Azure because I couldn't figure out who to email the problem to, and the issue made AMD look bad even though it was a microsoft problem.  She cc'd in the correct engineering department, and a week later they rolled out a fix    I'm not suggesting everyone email the CEO for any little thing, however if the problem is severe enough then you could try emailing her and explain why this makes AMD look bad even to AMD supporters and why it should be important to them to care about",Negative
Intel,"""You cant get fired for buying Nvidia"", they dont even need to say it.  This was a old saying back then about IBM",Neutral
Intel,Ever since I switched to an RX 6800 I'm getting a bluescreen maybe once every 100 hours in Windows 10. My GTX 970 was extremely stable in comparison.,Neutral
Intel,"well, after facing annoying blackscreen flickering with my rtx 3070  @4k 120hz iam not ao sure about driver stability in nvidia.",Negative
Intel,"If PSP crashes, the security state of the data on chip and on the board is compromised and it should not be recoverable. I think it opens up the chip to all sorts of vulnerabilities.",Negative
Intel,"It's wild that this is supposed to be such a big issue, but I've been on AMD for nearly a decade and have had ZERO issues.   Methinks that when you power users get into super complex setups, you forget your basics and lead yourself into your own problems.",Negative
Intel,TL;DR. **PEBKAC**.,Neutral
Intel,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible**, this is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  Posts regarding purchase advice, PC build questions or technical support will not be approved. If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the pinned megathread.   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
Intel,"While I'd love for AMD to fix its problems, I think that it's simply that smaller, lower visibility markets matter less to AMD. Working to be competitive both for gaming and for AI servers is work enough.",Negative
Intel,maybe stop using consumer grade GPUs for enterprise ML? I'm glad these issues exist.,Negative
Intel,"Gnif is active on the l1t forum. Wendell can't really do much on his own either, root issue is just amd stonewalling and sticking its head in the sand",Negative
Intel,I use my second monitor to check my 9 cameras. They use video hardware acceleration. Every time I open or close a game in the main monitor the client freezes and crashes...  😤😭,Negative
Intel,"Serious question, why would you not want to buy what just works if you're having problems.  Brand loyalty doesn't compute in this scenario to me.",Negative
Intel,it is the reason i stopped mining to be honest. i had a vfio server that during dead hours i would start hiveos or something to mine on. it was a great automation project and the server had like 4 gpus so i was a good bit of money but the need to have the server reset for the vdi to work in the morning was awful,Negative
Intel,"Thanks mate I appreciate it, glad to see you here :)",Positive
Intel,"Yes, lets fix AMD stuff for them. Im sure they love free labour.",Positive
Intel,"So, did you join the vanguard yet? and are you seeing just how worthless that program is?",Neutral
Intel,"I enjoy a variety of hardware with elements from AMD. Such as my ryzen based desktops and laptops. Ps5, ROG ally. But i just wont buy a high performance AMD based GPU. Especially for productivity tasks. Too many software issues and the support just is not there. Steer clear when your livelyhood and income depends on it.",Neutral
Intel,"Boy do I remember some of this. Wasnt even a company I was working at, but they brought us in as a SI to ""help"" fix some of the resolve issues. After working with BlackMagic we just used their PR  to tell the customer ""Sorry, you are shit out of luck. This is not supported and there is nothing that can be done. it's time to rip and replace and eat the cost, unless you do not care about profits and having a functional business."".",Negative
Intel,Lol wow.  People wonder why Nvidia has a $1 trillion dollar market cap....,Positive
Intel,This sounds more like a RIP on black magic than it is AMD... after all AMD hardware works fine for those tasks in other software.,Neutral
Intel,"I agree with most things except VRAM, you have to compare GPUs with the same amount of memory, otherwise it's typical to use more if more is available. Why would you load assets constantly from SSD/RAM instead of keeping them in VRAM for longer. Unused VRAM is wasted VRAM.",Neutral
Intel,>HIP often being several times slower than CUDA  ZLUDA proves that HIP isn't slower... the application's implentation of the algorithms written over HIP are just unoptimized.  HIP has basically 1-1 parity with CUDA feature wise.,Neutral
Intel,"This is honestly why as much as I'm liking my 7800XT, I'll probably go with the ""5070"" or whatever it's called next year",Positive
Intel,"Epic. Thanks for details.  I seen many times how youtube-creator/streamer went for amd gpu, get multiple crashes in first 20 min of using it, and returned it and get replace for nvidia, also vr-support on amd is joke, especially with screen capture.  For me it always was crazy to see how ""tech-youtubers-hardware-reviewers"" never ever test VR or/and ML on AMD, and those who promote amd-for-linux on youtube - they dont even use amd-gpu themselves, and do alll video-editing and AI-ML stuff on Nvidia... for promo video about amd-gpu... ye  I have experience with amdgpu from integrated gpu in Ryzen, and I was thinking to go for amd for compute-ML stuff just last month, but I did research:  [https://www.reddit.com/r/ROCm/comments/1agh38b/is\_everything\_actually\_this\_broken\_especially/](https://www.reddit.com/r/ROCm/comments/1agh38b/is_everything_actually_this_broken_especially/)  Feels like I dodged the bulled.  >AMD's AI game upscaling  Nvidia have RTX voice, they launched upscaling of video in webbrowsers, and now they launching RTX HDR - translation 8bit frames to hdr.  It is crazy to hear from ""youtube-tech-reviewer"" - ""amd good at rasterisation""... we in 2024 - you do need more than just ""rasterisation"" from GPU.",Neutral
Intel,">There are still people suffering from the high idle power draw bugs these cards have had for years, me included. As I type this my 6700XT is currently drawing 35 watts just to render the windows desktop, discord and a web browser. How is it not possible to just reach out to some of the people experiencing these issues and diagnose what's keeping the GPU at such a high power state??  My only fix for this with two monitors is:  1. alternate monitor must me locked at 60hz 2. main monitor needs a custom hz rating, set within ""Custom Resolution"" in AMD Adrenalin.  Basically I set a ""custom resolution"" in 1hz increments from 160-170hz (top 10 hz rating that your monitor is capable of) until I found the highest refresh rate that would give me low idle power.  I found that 162 hz was the highest my main monitor could go with my 2nd monitor sitting at 60hz. If I went with 163hz on the main my idle power goes from 7w to 40w.  That being said, this is typical AMD BS that you have to deal with as an owner of their GPUs. There are countless other examples that users have to do similar to this to get a mostly good experience.",Negative
Intel,"Excellent post, very informative. Would take issue with this though:       ""Speaking of VRAM, The drivers use VRAM less efficiently. Look at any side-by-side comparison between games on YouTube between AMD and NVIDIA and you'll often see more VRAM being used on the AMD cards""   Saw a side-by-side video about stuttering in 8gb cards (can find it if you want), the nvidia card was reporting just over 7gb vram used yet hitching really badly. The other card had more than 8gb and wasn't.    Point being: How accurate are the vram usage numbers? No way in hell was 0.8 gb vram going unused in the nvidia card, as the pool was clearly saturated, so how accurate are these totals?    There is zero (afaik) documentation of the schemes either manufacturer uses to partition vram; what is actually in use & what on top of that is marked as 'this might come in handy later on'.    So what do the two brands report? The monitoring apps are reading values from somewhere, but how are those values arrived at? What calculations generate that harvested value to begin with?    My own sense is that there's a pretty substantial question mark over the accuracy of these figures.",Neutral
Intel,"Funny, I saw the title and thought the same too!",Neutral
Intel,"SR-IOV and MxGPU is edge case. There are far more vGPU deployments powered by NVIDIA and that horrible licensing then there is anything else. AMD is just not a player there. That's the bottom line of the issue here. And VFIO plays heavily in this space, just instead of GPU partitioning its the whole damn GPU shoved into a VM.  So the Instinct GPUs that AMD are selling is being used on metal by large compute arrays, and not for VDI, remote gaming sessions, or consumer space VFIO. This is why they do not need to care, right now.  But if AMD adopted a fully supported and WORKING VDI vGPU solution they could take the spot light from NVIDIA due to cost alone. Currently their MxGPU solution is only fully supported by VMware, it ""can"" work on Redhat but you run into this amazing reset bug and flaky driver support, and just forget Debian powered solutions like Proxmox which is taking the market with Nutanix away from VMware because of Broadcom's ""Brilliance"".  I brought this issue up to AMD a few years ago and they didnt see any reason to deliver a fix, their market share in this space (MxGPU/vGPU, VFIO, Virtualized GPUs) has not moved at all either. So we can't expect them to do anything and spend the man hours to deliver fixes and work with the different projects (QEMU, Redhat, Spice, ...etc).",Negative
Intel,"```Seems hard to believe that they would just put up with these issues and go back to AMD for their next upgrade```   If they're big enough they'll just write their own firmware, drivers, and etc.",Negative
Intel,one of the 2 reasons I refunded my 7900xtx and went back to my 3070,Neutral
Intel,"All of zen 4 has an igpu output. I would try to set some custom resolutions on that 3rd monitor in Adrenalin. For example if that 3rd monitor is rated to 144hz, try custom resolutions from 134-143 hz and see if any one of those settings drops your idle power!",Neutral
Intel,"It's a memclock physics issue and the same threads are on the nvidia forum. Just get one 42/48"" monitor or two max at same res and hz and call it a day. Other combos can work. Plugging 3 different monitors in isn't doing any favours.",Negative
Intel,">I doubt AMD is going to take your OP any more serious then they took the NUMA issues  Not a lot of logic to this.  You are talking about today versus 2018 -- those are not the same companies. The number of employees more than doubled and revenues more than tripled.  Whatever challenges and resource constraints AMD faced back then are not the same as today.  That's not to say they don't still have resource constraints and will be able to immediately fix every issue. It just means you cannot make extrapolations from an experience years ago with CPU/platform all the way to GPUs and accelerators today.    Obviously there's no memo going around which says ""make the customer experience bad. signed, the management""",Negative
Intel,">Watch their hands, not their mouth. Docs + firmware source = good. Promises + ""access"" = worthless. I fell for this too, not again.  Exactly, docs + firmware source code is what matter, not promises!",Negative
Intel,ursohot !  back to discord rants...,Neutral
Intel,I've had issues with Nvidia drivers too where AMD have been fine. Guess it's really situational,Negative
Intel,"```but I feel like I'm taking crazy pills where the internet is screaming about how incredibly terrible AMD GPU's and drivers are```   OP was referencing data center use cases, which can vary wildly, and stress different parts of the GPU depending on the task.   It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.   Now imagine Radeon's bugs but on the scale of enterprise/data center/servers and that's why OP pretty much typed out a cry for help.",Negative
Intel,"I dunno man. I’ve been through a few AMD cards, and getting frametimes rock solid has never been possible for me in certain scenarios. That said, and in fairness, I haven’t used anything by team green lately, so it may all be the same shit , different pile.",Negative
Intel,Lol same with me tbh I haven't had any problems 😂 but I guess some do idk 🤷. I have crashed less with AMD than my old  Nvidia card.,Positive
Intel,"gaming is completely different to compute workloads.  it's also different when you're running multiple of these 24/7 in a single machine at full load and if any one of those hard crashes, having to reboot the whole system is really really bad.  read what others' professional experiences are in this post. AMD GPUs are just terrible in the datacenter.",Negative
Intel,"I've had a fair number of issues with my 6950 xt. System wide stutter from alt tabbing in a game because instant replay is on. Video encoding that looks worse than what my 1050 ti was able to do (seriously fucking disappointing there). Text display issues due to some setting AMD had on by default. AMD has caused me a lot of issues that I shouldn't be getting from a card that cost me £540. I get it, it's last gen and my issues are kinda trivial, but it was a huge investment for me at the time and now I'm wishing I'd spent £200 more on a second hand 3090 instead of this.",Negative
Intel,"It doesn't handle it, it has the same issue.",Neutral
Intel,>never noticed this  search in the internet - `amdgpu ring gfx timeout`  [https://www.reddit.com/r/linux\_gaming/comments/1bq5633/comment/kx14ojy/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/linux_gaming/comments/1bq5633/comment/kx14ojy/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button),Neutral
Intel,"I personally also never had any major issues with AMD/ATI cards I can think of. One thing is true though, sometimes they do really take a long time to fix certain bugs.",Neutral
Intel,"Same, used a 6800 for over three years with no issues (actually solved crashing issues I was having with my 1080 Ti) and now moved onto a 7900 XTX, also with no issues.",Neutral
Intel,Me neither. I use a RX580 8GB since launch and not a single problem.,Neutral
Intel,Because they're talking absolute rubbish that's why.,Negative
Intel,You are one of the lucky ones!,Neutral
Intel,"How is an AMD feature ""giving users control"". If they advertise something and people use it, it's not the end users fault. It's amd for (once again) coding shit features that break things.",Negative
Intel,"Most people that have issues blame the game because of the way that DirectX debugging works. Unless the developer specifically enables the debug layer, and the user has the SDK installed (it will crash without it), and the user runs software to capture the debug strings, there is simply no indication presented to the user as to the cause of the crash that is actually useful, or even hints at a GPU level fault. The game ends up just crashing with some generic error.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)   [https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw](https://learn.microsoft.com/en-us/windows/win32/api/debugapi/nf-debugapi-outputdebugstringw)",Negative
Intel,"> nooo but amd drivers fine, Reddit told me!   You do realise its possible for people to have had no problems with the drivers right?",Positive
Intel,lol your flair is Please search before asking,Neutral
Intel,"Hey OP — Your post has been removed for not complying with Rule 2.  e-Begging (asking for free PCs, sponsorships, components), buying, selling or trading posts (including evaluation posts), retailer or brand disputes and posting referral or affiliate links is not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,Looking at it the wrong way will make AGESA reset the BIOS.  That's more of a CPU/platform issue than a GPU issue.,Negative
Intel,Pretty sure gnif2 mentioned once that he had communicated directly with her in an effort to get this problem resolved.,Neutral
Intel,"Then if it's crashed, why doesn't a hardware watchdog send it through a full reset, bringing it back to a known safe state again?  Sorry but this makes no sense, leaving it in a crashed state is not making it ""safer"" but rather in a state that it's behaviour is undefined and could lead to any such secrets being leaked out.",Negative
Intel,"So you have had nearly a decade of experience with GPU passthrough, ROCm, and AMD Instinct compute accelerators?  Methinks you didn't read through the original post.",Neutral
Intel,AFAICT OP is the author of [vendor-reset](https://github.com/gnif/vendor-reset) kernel module which was used to work around some of the VFIO reset issues on Vega. I suspect that they have more knowledge of these quirks than anyone else outside of AMD (and certainly more most on this subreddit). Do you have any additional info to confirm that it's a user error?,Neutral
Intel,Maybe read this through again and see that AMD Instinct GPUs are also faulting.,Neutral
Intel,"```what I find absolutely shocking is that your enterprise GPUs also suffer the exact same issues```   This legit killed me lol 🤣🤣🤣🤣   I hate to say it, but I understand why companies are paying god knows how much for B100 now.   Gamers used to joke about Radeon drivers but this is next level.",Negative
Intel,"If you search for \`vcn\` in drm/amd, there are many similar victims using 6800xt (and navi2x). [https://gitlab.freedesktop.org/drm/amd/-/issues/2156](https://gitlab.freedesktop.org/drm/amd/-/issues/2156)  AMD's video codec IP seems to be heavily influenced by other IP blocks, such as SDMA. And they only have one chance to get it right each time they submit a set of commands to the VCN, otherwise they have to reset the entire GPU and lose your desktop context.     Another interesting fact is that these instabilities may disappear when you switch from Wayland to Xorg.",Neutral
Intel,"I usually stick to AMD because I'm a Linux user and conventionally it has worked better with Linux, and has open source drivers that aren't garbage. My brand loyalty is not absolute, I've used Intel and NVidia before.",Positive
Intel,"I mean, the main reason I wouldn't want to is because it further supports an anti-consumer costing structure...  But if I was buying for enterprise, 100% I'd just buy the thing that works. I just won't personally do it as an individual.",Negative
Intel,"""NVIDIA, it just works""",Neutral
Intel,NVIDIA have already demonstrated multiple times over a decade or more of what they do when they have a near monopoly on the market. I do not want to see what their behaviour with a full monopoly looks like.  That and AMD has the better FOSS driver situation.,Negative
Intel,What is the AMD Vanguard?,Neutral
Intel,"I am not fixing anything, this is an incorrect assumption.  I have a setup that is exhibiting these faults, the faults are affecting me and my clients, and as such I am in the ideal position to report the debugging details to AMD in a way that is most useful to the AMD developers to resolve the problem. And because I already have systems experiencing these problems, I am very able to quickly test and report back to AMD on if any fixes they implemented were successful or not.  Do I think AMD should have more rigorous testing so these things get addressed before release? Yes, sure, 100%, but there will always be missed edge cases that are unexpected and not tested for.  A prime example is another issue I have with the AMD drivers that is really not their fault, and they could chose to just say that it's unsupported.  Recently I discovered that it was possible to use a DirectX 12 API to create a texture resource in memory that the user allocated ([https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device3-openexistingheapfromaddress) \+ [https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource](https://learn.microsoft.com/en-us/windows/win32/api/d3d12/nf-d3d12-id3d12device-createplacedresource)), and have the GPU copy into that directly. This API is documented by Microsoft as a diagnostic API, it was never intended to be used in this manner, however it works on NVidia, and mostly works on AMD, improving the performance of Looking Glass by a factor of 2x or more.  Not only is this using a ""diagnostic"" API, we are mapping memory that was mapped into userspace from a virtual PCI device, which is memory that has been mapped on the host system, which then finally maps to physical RAM. To my knowledge there is absolutely no other use case that this would ever be useful for.  I can almost guarantee you that there is no way the developers would have thought to write a test case for this, it is not just off the path a little bit, but instead down a cave, in the dark without a torch, being lead by a deaf mute with only one leg while being chased by a pack of rabid wolves.  The issue here isn't about helping AMD fix their drivers or not, it's about being able to help them in the first place. And if this is a feature that they do not want to support, having the documentation needed to self-support the feature.",Negative
Intel,Definitely not because of lack of those issues but investement in AI.  Frankly speaking going forward I fully expect Nvidia to drop the ball as well. Rest of their business compared to AI is just so miniscule.,Negative
Intel,You misspelled $2.3T market cap....,Neutral
Intel,"Okay yeah fair enough, hadn't considered this. Removed it from my post",Neutral
Intel,"So maybe AMD should sponsor some development on widely used software such as Blender to bring it within a few percent, or embrace ZLUDA and get it to an actually functional state. As an end user I don't want to know who's fault it is, I just want it to work.  Does ZLUDA even bring it close to CUDA? All I see is graphs comparing it to OpenCL, and this sad state of affairs..  https://i.redd.it/mdcvx487vcsc1.gif  From the project's FAQ page.. only further reinforces my point. This is dead and AMD does not care.  * **Why is this project suddenly back after 3 years? What happened to Intel GPU support?**   In  2021 I was contacted by Intel about the development of  ZLUDA. I was an  Intel employee at the time. While we were building a  case for ZLUDA  internally, I was asked for a far-reaching discretion:  not to advertise  the fact that Intel was evaluating ZLUDA and definitely  not to make  any commits to the public ZLUDA repo. After some  deliberation, Intel  decided that there is no business case for running  CUDA applications on  Intel GPUs.Shortly thereafter I got in contact with AMD and in early   2022 I have left Intel and signed a ZLUDA development contract with AMD.   Once again I was asked for a far-reaching discretion: not to advertise   the fact that AMD is evaluating ZLUDA and definitely not to make any   commits to the public ZLUDA repo. After two years of development and   some deliberation, AMD decided that there is no business case for   running CUDA applications on AMD GPUs.One of the terms of my contract  with AMD was that if AMD  did not find it fit for further development, I  could release it. Which  brings us to today. * **What's the future of the project?**   With  neither Intel nor AMD interested, we've run out of  GPU companies. I'm  open though to any offers of that could move the  project  forward.Realistically, it's now abandoned and will only possibly receive  updates to run workloads I am personally interested in (DLSS).",Neutral
Intel,"So HIP isn't written badly because it has ""1-1 parity with CUDA feature wise"".... on this episode of I don't understand what I'm talking about but I have to defend the company I like.",Neutral
Intel,"If you have good raster you dont need upscalers and fake frames via generation. Those ""features"" should be reserved for low to mid range cards to extend the life, not a requirement to run a new game on a high end GPU like we have been seeing lately with non-existent optimization.",Neutral
Intel,This is not a fix. It's a compromise.,Negative
Intel,"Someone else pointed out this is likely just because it has more vram it's using more vram, I think that's the real reason looking at comparisons with both cards at 8gb -- I've removed that point from my post",Neutral
Intel,Any card that has 8 GB of VRAM wont be running a game at settings so high that it would cause a stutter due to lack of VRAM in anything but snythetic youtube tests.,Negative
Intel,"AMD's reputation on VDI seems to be a dumpster fire in homelab scene despite having the first SR-IOV implementation compared to Nvidia and Intel(yes, even Intel is into VDI market!). Sure in homelab setup you're on your own with google-fu, instead of paying for enterprise level support.  But the kind of negligence is different on AMD side. Only the old old old S7150 ever got an outdated open-source repo for Linux KVM support and that's it. This means the documentation and community support are pretty much non-existent, you REALLY are on your own with MxGPU.  Nvidia Grid(meditated vGPU), despite having a notorious reputation on licensing, just works and can be hacked onto consumer cards. Best of all it's pretty much gaming ready with hardware encoders exposed for streaming acceleration(see GeForce Now).  Intel had been providing open source Linux support since their GVT-g(meditated vGPU) days and now SR-IOV on Xe(gen12) architecture. Direct passthrough is also possible without too many hacks like AMD do(*cough* vendor-reset *cough*).  People always consider Intel graphics processors as a laughing stock but you gotta respect them for the accessibility of vGPU solution, directly on integrated graphics that everyone gets. They are even trying to enter VDI market with GPU Flex cards based on Alchemist GPUs(SR-IOV was disabled on discrete ARC consumer cards). Hopefully subscription-free model can make Nvidia a run for its money, at least in entry VDI solutions that Nvidia has no interest in.",Negative
Intel,[https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series](https://learn.microsoft.com/en-us/azure/virtual-machines/nvv4-series)  [https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/](https://aws.amazon.com/about-aws/whats-new/2021/04/amazon-ec2-g4ad-instances-available-in-additional-regions/)  [https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series](https://learn.microsoft.com/en-us/azure/virtual-machines/ngads-v-620-series)  [https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/](https://wccftech.com/tencent-cloud-launches-xinghai-wisdom-wood-series-ga01-amd-pro-v620-gpu/)     AMD's Virtual Graphics products are aimed directly at the cloud service providers now. You'll note that the recent virtual product lines are not available via the channel/distribution.,Neutral
Intel,>AMD is just not a player there.  Except all the playstation streaming is doing from AMD GPUs probably outclassing every other vGPU instance out there. Most of the other streaming platforms were done on AMD as well... of course most of the generally fail due to the entire premise being silly.,Negative
Intel,"It's more that I don't want to reward a business for failing me.  If I bought a car and everytime I drive it the heater jumps on and starts to cook me, and a year later the manufacturer still hasn't resolved it I'm not gonna buy a car from the same brand.   As for possible solutions; at this point I've sunken far too many hours into it to warrant further attempts, I've tried a plethora of drivers, ran DDU multiple times, fiddled with the settings (such as freesync), setup custom resolutions with varying refresh rates etc... If my only issue with AMD was occasionally reverting a driver I wouldn't be complaining, I had to do that with my previous Nvidia card as well, but this is unacceptable tbh.   Anyway, so far nothing has worked, the only time I've seen normal idle power is if all my monitors are turned off (not standby after you press their button, but physically turned off using the powerstrip they're plugged into). If I then remote into the system it's normal, not exactly practical though.  And overall it's not a major issue if it didn't negate the one advantage this card had over the 4090, namely it's value. Some rough napkin math tells me this thing could cost me close to 100 euro's per year extra just in idle power draw, over the course of several years this means a 4090 would've been cheaper despite its absurd price.  As a final note to this, if AMD came out and said they can't fix this issue due to the design of the board or w/e, I could honestly respect that, at least then I know I shouldn't keep on waiting and hoping but I can start looking for a workaround. Instead a couple patches ago they ""improved high idle power with multiple displays for the 7xxx series"" (which did the opposite for me and added a couple watts even) and ever since they don't even mention it anymore, I don't even know if they're still trying to fix it or gave up entirely. And the thing I hate even more then just waiting forever for a fix is being stuck in limbo not knowing.",Negative
Intel,">Not a lot of logic to this.  Look at my other reply  ""SR-IOV and MxGPU is edge case. There are far more vGPU deployments  powered by NVIDIA and that horrible licensing then there is anything  else. AMD is just not a player there. That's the bottom line of the  issue here. And VFIO plays heavily in this space, just instead of GPU  partitioning its the whole damn GPU shoved into a VM.""  ""I brought this issue up to AMD a few years ago and they didnt see any  reason to deliver a fix, their market share in this space (MxGPU/vGPU,  VFIO, Virtualized GPUs) has not moved at all either. So we can't expect  them to do anything and spend the man hours to deliver fixes and work  with the different projects (QEMU, Redhat, Spice, ...etc).""",Negative
Intel,"I'm the same. my issues with Nvidia drivers were so bad it made my gpu and entire windows install functionally bricks. Got rid of my EVGA 760 when the 900 cards and AMD's 300 series came out, jumped to R9 390 and haven't looked back since (R9 390>RX 5700xt>RX 7700xt) The only issue i ever had with AMD was the first few months of the 5700xt and its awful unplayable performance issues in DX9 games, but that was solved within months, and they eventually went on to improve opengl performance on Navi/RDNA as well which was a nice welcome surprise. Ive had a few hiccups that looked like driver issues that turned out to actually be Windows issues, and i always wonder if people are quick to blame AMD for issues because of what they have heard vs actually investigating and finding the real cause of the problem. More often than not any system issues im having end up being the fault of Microsoft, or a specific game wasnt tested on AMD properly and the blame lies with the devs.",Negative
Intel,The comment I quoted was talking about people playing games having issues.,Negative
Intel,> It's why AMD clocks EPYC processors significantly lower than the Ryzen variants. Because a Ryzen CPU isn't intended to be hammered 24/7 @100% utilization for months and sometimes years on end.  I think that's more about the unreasonably high power they'd use if they boosted the same as ryzen,Neutral
Intel,The thing I quoted was talking about people playing games though.,Neutral
Intel,"I've also had numerous issues with my 6800XT, currently stuck with a 23.11.1 driver version as all newer ones are just trash on my system. This one is usable, newer ones all have a ton of stutter and all that Radeon stuff.   I should have just re-pasted my previous GeForce and ride out the pandemic shortage, but I wanted a faster GPU and thought I'd give a Radeon one final chance. There wasn't a 3080 or 3090 available back then, otherwise I would've rather bought one.   While 6800XT has had some okay drivers here and there, the overall experience remains sub-par; the road still is full of unpaved and rough sections. I've decided to ban Radeons from my household after this one is evicted. It's not worth the driver hassle, not even the numerous Reddit upvotes you get by saying you use a Radeon. :D   It's good that AMD still has the willingness to keep fighting back, it's good to have rivalry. But... I don't know, man. I'm not giving them a consolation prize for a lackluster participation.",Negative
Intel,"I spent 330, you spent 540, we could have spent 1000 in the 7900xtx, it isn't supposed to have these kinds of problems, and all the hours of troubleshooting that comes with it.  OPs not being able to reset the card state without a hardware reboot is just.. bad especially on the server side of things.  We have to start calling things by their true name, and all of these situations are just bad firmware/software/vbios/drivers implementation by AMD.  That and drivers install are just finicky like it happened to me in the latest chipset driver install.. sorry not normal.  Just saying you have no problems won't erase the existence of these thousands of cases of people having problems. And the truth of OPs issue he mentioned in this thread.",Negative
Intel,"Idk, I don't use Linux",Neutral
Intel,"Yeah, they are around 20x smaller than nvidia so kind of expected imho",Neutral
Intel,"RX580 is Polaris, before the big redesign that was Vega and brought the PSP into the mix. Note that none of this is referring to that GPU. Until you upgrade to one of the more modern GPUs, your experience here is exactly zero.",Neutral
Intel,"No I am not, this is 100% the truth, but you can of course think whatever you want and be ignorant.",Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,Keep on living in fairy tale land:   [https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/](https://www.digitaltrends.com/computing/amd-driver-windows-crashing-boot-problems/)  [https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html](https://www.tweaktown.com/news/96479/amds-latest-radeon-drivers-aims-to-stop-helldivers-2-crashing-and-fix-stuttering-in-many-games/index.html)  [https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html](https://www.pcworld.com/article/2242084/nightingale-removes-fsr-3-pre-launch-for-crashing-too-much.html)  [https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news](https://www.techradar.com/news/amd-fixes-bug-that-freezes-up-windows-11-pcs-but-theres-still-bad-news)  [https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs](https://www.extremetech.com/gaming/343132-amds-new-unified-graphics-driver-for-rdna-2-and-3-is-crashing-some-pcs)  [https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/](https://www.thephoblographer.com/2017/07/11/driver-fixes-lightroom-amd-gpu-crash-bug-as-adobe-seeks-your-feedback-on-performance/)  And don't forget that AMD has invested into adding debugging to their drivers so that people like you can submit useful bug reports to try to get to the bottom of why their GPUs are so unstable. When was the last time you saw Intel or NVidia need to resort to adding user debug tools to their drivers!  [https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes](https://www.tomshardware.com/news/amd-radeon-gpu-detective-helps-troubleshoot-gpu-crashes),Neutral
Intel,"I don't know man, most of the people I know that use Radeon have not had issues at all. Some are running 5000, 6000, and 7000 series cards.  Don't mean to downplay the issues with VFIO, just my perspective.",Neutral
Intel,Because adding a feature for a product literally gives users more control for that product.,Neutral
Intel,And If I get no crashes with my AMD graphics cared - how does that fit your narrative?,Neutral
Intel,"> That's more of a CPU/platform issue than a GPU issue.  It happened to me 0 times with an Nvidia card while OCing for hundreds of bios cycles and thousands of hours on AM4/AM5, while Radeon users are experiencing it all of the time. The CPU/platform is fine.  The Radeon graphics drivers hooking into CPU OC and platform controls intimately - or even at all - for no good reason are not fine.",Negative
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"It should reset, maybe it doesn't know it's crashed in the specific bug you have generated. But your data should not be recoverable. If you have reproduced the bug confidently and sent the report to AMD and they haven't fixed it there is nothing more you can do.",Negative
Intel,Sorry to jump on a random reply - but does this have any relevance? It might just be PR hot air  https://twitter.com/amdradeon/status/1775261152987271614,Neutral
Intel,"Yes, I am the author of vendor-reset. This is my third attempt now to get AMD to resolve these issues properly. vendor-reset was supposed to be a temporary stop-gap workaround while we waited for a new generation that was fixed.",Neutral
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"I get that you are being cheeky, but the use-case is very difference and the professional-demands are far far far higher.  When you run several machines off of a single unit, suddenly there's workloads that has to be completely in due time for things to move ahead.  I just want to contextualize the issue you are making (a tadd) fun of.  So in the basic but common example above, you can't really complete your job because the entire main system has to be shut down. That's like stranding 6 people because the bus broke down. And now all 6 people have to walk. Instead of let's say a gamer: He take his super expensive OR cheap car 10 minutes down the street instead. To and from work, the store. His car 100% for sure will break down, but it's happening so rarely a normal check gets the fault before it's found. OR, he only miss a few hours once a few years if his car break down.  I think it's a decent comparison of the issue here, to use PC hardware in multiple instances, but being forced to restart a system in un-manageable. There need to be a proper high-grade (and low grade) reliable way to avoid that.  Just sucks it took this long, and so much effort to get AMD to pay notice to the issue at hand here. To people that didn't get what the main issue was, hopefully my explanation helps.",Negative
Intel,> Gamers used to joke about Radeon drivers but this is next level.  Getting banned from games is peak driver fail.,Negative
Intel,"Yeah, I always wondered why NV was so huge in datacenter stuff also for compute way before this AI craze. especially in fp64, AMD used to be competitive especially factoring in price.  But reading this explains it all.",Neutral
Intel,"That beeing said, Nvidia's GSP approach and Red Hat recently announcing Nova (Nouveau successor written in Rust), things might change in the future. E.g. AMD's HDMI 2.1 not beeing approved to be open sourced is a perfect example, which works fine in Nvidia's hybrid approach (from a legal/licensing perspective).   AMD has the lead regarding Linux drivers, but they need to keep pushing, if they want to stay ahead.",Neutral
Intel,*wayland users have joined the chat,Neutral
Intel,You're falling for slogans.,Neutral
Intel,"To be fair, Noctua do make some of the best fans out there (if you do not want rgb ofc).   From their server grade ones up to consumer grade ones.   They are really expensive, true, but the sound profile is by far one if not the best one.   Pair that with how high the static pressure and airflow are, and yes, its the best out there, for an expensive price.   With half the price you can get 80% of the performance on other brands, I wont deny that, but if you are qilling to spend money, they are the best in the market, period.",Positive
Intel,Unpaid beta test program that has existed since ages... hasn't resulted in any of the complaints in this thread getting fixed though.,Neutral
Intel,[https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html](https://www.amd.com/en/products/software/adrenalin/amd-vanguard-program.html),Neutral
Intel,"you're kinda missing the point tho, it's because they do pay attention to software and firmware that they were able to establish that foothold.",Neutral
Intel,Honestly after a trillion I kinda stop counting 😂🤣,Neutral
Intel,"VRAM usage is specific.  In context of Unity games and VRChat - Nvidia does use less VRAM than AMD... but only in Windows, only Nvidia DX driver in Windows have this ""hidden feature"" and only with DX API. So it may be DX feature. It very common/easy to see it in VRChat large maps, or large Unity games.  In Linux - *in some cases, but it very common* - you get more VRAM usage on Nvidia compare to AMD because this how Vulkan driver implemented in Nvidia and overhead of DXVK.  P.S. For context - Unity VRAM usage is - Unity allocating ""how much it want"" and in case of two different GPU Unity may allocate less or more in DX-API, or DX-API have some internal behavior for Unity case on Nvidia so it allocating less. In Vulkan - DXVK have huge overhead about 1Gb on Nvidia GPUs in many cases, and Unity ""eat all vram possible"" behavior explode difference.",Neutral
Intel,"No its more like, nobody has bothered to optimize or profile HIP applications for performance for a decade like they have those same CUDA applications.  I'm just stating facts. You are the one being aggressive over... some computer hardware good gosh.",Negative
Intel,"Let me tell you some stuff regarding how a GPU works.   Raster performance can only take you so far.   We are in the brink of not being able to add more transistors to the GPU.   Yield rates are incredibly low for high end parts, so you need to improve the space usage for the GPU DIE.   Saying that these ""features"" are useless is like saying AVX512, AVX2, etc are useless for CPUs.   RT performance can take up to 8x same GPU surface on raster cores, or 1x surface on dedicated hardware.   Upscaling using AI can take up to 4x dedicated space on GPU pipeline or 1x on tensor cores.   The list goes on and on with a lot of features like tessellation, advanced mesh rendering, etc.   GPUs cant keep increasing transistor count and performance by raw brute forcing it, unless you want to pay twice for the GPU because the graphics core will take twice as much space.   Upscaling by AI, frame gen, dedicated hardware to complete the tasks the general GPU cores have issues with, etc are the future, and like it or not, they are here to stay.   Consoles had dedicated scaling hardware for years.   No one complained about that. It works.   And as long as it works and looks good, unless you NEED the latency for compwtitive gaming, its all a mind fap, without real world effects.   Im damn sure (and I did this before with people at my home) that if I provide you with a game blind testing it with DLSS and Frame Gen, along with other games with those features on and off, you wont be able to notice at all.",Negative
Intel,"I'm just trying to help, not debate the semantics of what is considered a fix or a compromise. Purchasing an AMD GPU is already a compromise.",Negative
Intel,It's not a dumpster fire.. you just have to buy an overpriced GPU to even have it... so pretty much a completely utter nothing burger that AMD is not even interested in.,Negative
Intel,"Except the V620/520 are not the only GPUs that support MxGPU, Instinct's line does too and offers the same ""features"" as the V520/620, but the native driver support is more geared towards GPCompute and not 3d rendering, but are also supported by the exact same driver family as the WX workstation, V cloud, and RX GPU lines.   Also, been a lot of offloading of the V520 and V620 ""cloud only"" GPUs on the gray market, and I can CTO HPE servers with V620's by enterprise ordering today.",Neutral
Intel,"This is not at all on the same level as what the OP is talking about.  I can also stream from my RX6600M, RX6600, my Ally,..etc just like you can from the Playstation. But it has nothing to do with VFIO, virtualization, or MxGPU.   What my bitch about, and it aligns with OP perfectly, vGPU support (MxGPU) for VDI setups on non-VMware solutions. AMD has completely dropped the ball here and its never been more important then **right now**.",Neutral
Intel,"Hey, just trying to help your setup right now. I would be frustrated too, I had the same issue with two monitors, not three. I was able to fix the idle power issue by setting the alternate monitor to 60hz and setting my main monitor to 162hz (max 170). Obviously spend your money where you think it's worth it.",Negative
Intel,">It's more that I don't want to reward a business for failing me.  Have your displays continued working reliably? Oh they have? You are over the vblank limit for idling down... so its not and never will be a bug on ANY GPU.  This is far more akin to your car idling up when the AC comes on... you have 3 displays on a certain amount of framebuffer bandwidth is REQUIRED to implement that, + a bit more to account to account for any lite tasks that might be running on the GPU at the same time.  The whole issue here is that your memory bus with 3 monitors active is NOT idle... if you want it to idle down turn your dang monitors off, its that easy.  At some point they may have a solution that just powers up a single memory lane or something and allocates the frame buffers in there, but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.",Negative
Intel,"AMD is working with [Amazon ](https://aws.amazon.com/ec2/instance-types/g4/)and [Azure](https://www.amd.com/system/files/documents/nvv4-datasheet.pdf) on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.   I'm sure there has historically been little incentive to make this rock solid on consumer GPUs. Though that is a shame.  However I see no reason to assume the constraints which led to that choice in the past exist today.",Neutral
Intel,"True, windows had an awful habit of breaking my system by continually trying to uninstall new drivers",Negative
Intel,"What are you talking about? AMD employs 26000 people, NVIDIA has 29000. They're the same size... oh, you mean profits? Well then, yeah...",Neutral
Intel,"Idk bro, had 470', 570', 580', 590, 460, few of vega64, 56, 6700xt, 7900xt.... Never had issues, even with those vegas I abused, overcloccked etc",Neutral
Intel,Oh then just ignore my comment 😅,Neutral
Intel,"I'll be honest, I've been using AMD GPUs since 2010 and they've been solid.  However the features Nvidia is rolling out is making me consider a 5070 next year",Positive
Intel,Heartbreaking to see you downvoted by bringing these issues up. Reddit is such a terrible place.,Negative
Intel,"Awesome, not biased at all, now pull up a similar list of nvidia and intel driver issues, it wouldn't be any shorter...",Positive
Intel,"And you keep grossly overstating the issue.   Most of which were quickly resolved and/or effected a small number of customers and limited to specific apps, games or usage scenarios.  I've had an AMD gpu in my primary gaming PC for the past three years. Not a single one of the issues you listed effected me or a majority of owners.   And umm yeah, Nvidia also have bug / feedback report tools....  Intel right now are causing me far more issues with their Xe drivers so please. I'm still waiting for Xe to support variable rate refresh on any fucking monitor.",Negative
Intel,"\> Don't mean to downplay the issues with VFIO, just my perspective.  Understood, however you responded to a comment directly related to someone that has been lucky with VFIO.  u/SckarraA I am curious, have you tried simulating a VM crash by force stopping the guest and seeing if the GPU still works? This is usually guaranteed to put the GPU into a unrecoverable state.",Neutral
Intel,It's really weird how many AMD fanboys like yourself are popping up and denying the existence of a well known and documented issue because it either isn't majorly impactful to them or they don't know how to recognize it.  Just because it doesn't affect you doesn't mean it's not a real issue and doesn't mean it shouldn't be addressed.  Quit being so obliviously self-centered.,Negative
Intel,"It was only a few months ago that an amd feature in their drivers literally got massive amounts of people banned in online games, so much so that amd hat to completely pull that feature and no one has heard of it ever since.   How can you claim that amd drivers are in a good position?",Negative
Intel,Also what sucks the most is that such a bios change takes a preboot 40-50 seconds before anything is even displayed on the screen,Negative
Intel,I definitely got it all the time while OCing on nvidia cards. Sometimes it just resets for the hell of it on a reboot where I wasn't even doing anything.  I'm not sure why you think AMD's GPU drivers have some intimate link with the BIOS. They don't.,Negative
Intel,"WTF are you talking about. Do not apply CPU OC from Adrenalin Ryzen Master API, and it won't reset on GPU crash.    Spoiler, if you save preset with GPU OC, while you have CPU OC applied through separate Ryzen Master or BIOS, it won't be affected by Adrenalin OC reset on crash.  How it is that i had never had my CPU PBO reset after dozens of forced GPU crashes (through UV and one bug i found out on occasion)?   There was only one case where it was affecting people. When AMD integrated Ryzen Master API in Adrenalin for the first time, as previously saved GPU OC presets had no CPU data, and forced CPU OC to reset to defaults. After re-saving GPU OC profile, it never happens again.",Negative
Intel,"Yes, it should and when the GPU is still in a semi-functional state we can tell the GPU to perform a reset... which does nothing. So yes, it should reset, but they do not.  \> But your data should not be recoverable.   Correct, we are not talking about recovering data, just getting the GPU back to a working state without rebooting the system.     \> there is nothing more you can do.  Not entirely true, if it was the case the \`vendor-reset\` project would not exist:   [https://github.com/gnif/vendor-reset](https://github.com/gnif/vendor-reset)",Neutral
Intel,"Too soon to tell, but hopes are high.",Negative
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Honestly, the HDMI 2.1 fiasco has pushed me (and many other people) to stay away from HDMI, not AMD.  As for Nova, we'll see how it goes, but it's likely a multi-year endeavour, just like it was many years ago for the Amd open drivers.  Currently, from a consumer and Linux user point of view Nvidia should be avoided whenever that's possible, and I speak from experience since I made the mistake of buying a laptop with hybrid graphics and Nvidia gpu. It was a good deal, but that has cost me *a lot* of hours of troubleshooting of different issues, that never happened with AMD or Intel.   The strange thing about Amd is that they focused a lot, in the past few years, on consumer drivers/software, while from the hardware pov they pushed the accelerator on HPC/AI hardware, so there is some kind of mismatch and often their product either have great hardware or great software, but usually not both.",Negative
Intel,"Agreed, they cannot rest on their laurels.",Negative
Intel,"Execept for when it comes to VFIO usage, NVidia literally just works. They even endorse and support it's usage for VFIO Passthrough, as niche as this is.   [https://nvidia.custhelp.com/app/answers/detail/a\_id/5173](https://nvidia.custhelp.com/app/answers/detail/a_id/5173)",Neutral
Intel,"According to theoretical physicists, the numbers are correct as long as they have the correct order of magnitude.  > How Fermi could estimate things! > > Like the well-known Olympic ten rings, > > And the one-hundred states, > > And weeks with ten dates, > > And birds that all fly with one... wings.",Neutral
Intel,console gamers know pc’s are better and don’t really complain about upscaling and 30fps.. you’re right that competitive sacrifices everything else for latency. also may be true that your average casual gamer wouldn’t notice increased input latency. but they have been adding transistors and ppl were willing to pay doubling amount of cost for them. i rmb when a midrange card used to cost 200.,Neutral
Intel,I'm well aware of what VDI desktops are... it effectively the same thing though.  And yes... Sony does use vGPU/MxGPU for streaming PS games.  There really is no ball to drop because no solution has exited outside of VmWare. at least not one that has involved a company actually working with AMD to build any solution.,Neutral
Intel,"Haha dw, just venting a bit.  It's also genuinenly my only gripe with the card and setup, it's just annoying it's not getting fixed and I can't apply any workaround, particularly for the price I've paid.   I would just put in any of the older cards I've got laying around just to drive the other monitors but then I'd have to give up 10gbit networking, and I'd still have higher than ideal idle usage  but it would be cut down a bit.   So I'm mostly miffed that if I wanted to actually resolve this it would be by moving to a cpu with integrated graphics, and that's money I don't want to spend. But if I don't, I'm spending money I don't want to spend.",Negative
Intel,"Apparently 100watts is ""normal"" and to be expected, and I should just be grateful, the fk are you waffling on about? That's 20watts short of the max TDP of a 1060... a card that could run these 3 monitors without trying to burn a hole in my wallet FYI..  And fantastic solution, so I spend over 1000euro's on a GPU but then have to turn my monitors off, genius... Quality stuff, can't make this shit up. Also like I actually typed out:  >the only time I've seen normal idle power is if all my monitors are turned off   so how would that work? Oh maybe I can throw my main monitor in the trash and then the problem is solved I suppose?  >but people complaining about a problem that doesn't have a solution and only affects 0.5% of people is annoying.  Am I supposed to complain about issues that don't affect me? Or are you saying I've got no right to complain? Is me having a bad experience annoying you?  and if not by complaining how am I supposed to know this issue doesn't have a solution? Do you even listen to what you're saying?  You know what's annoying? People dismissing other people's complaints because ""they don't like it"" or they're such fanboys they can't stand someone criticizing their favourite brand.",Negative
Intel,"Sorry but AMD ""working with"" is a joke. I have been working with companies that have hundreds to thousands of AMD Instinct GPUs.  I have been able to interact directly with the AMD support engineers they provide access to, and the support is severely lacking. These issues here have been reported on for over 5 years now, and what has AMD done for these clients?  Until I made my prior posts here on r/AMD, AMD were not interested or even awake when it came to these issues. I have had direct correspondence with John Bridgman where he confirmed that GPU reset was not even considered in prior generations.  Of what use are these support contracts and the high cost of buying these cards if AMD wont provide the resources to make them function in a reliable manner.  Why did it take some random (me) to have to publicly embarrass the company before we saw any action on bugs reported by their loyal paying enterprise clients?",Negative
Intel,">AMD is working with Amazon and Azure on systems with 1-4 GPUs supporting SR-IOV/MxGPU. This is only with ""Pro"" or ""Instinct"" cards though.  and MSFT, but we are not seeing these changes upstream via open standards. We still are lacking working support for the likes of Nutanix and Proxmox (both KVM), where Redhat has some support but there are still unresolved issues there.  Fact of it, the changes AMD is pushing at AWS would upstream to every other KVM install and bring those fixes to mainstream. But this has been going on for well over 6 years that I can recall and still we are no closer to a ODM solution released to the masses. I had hopes for RDNA2 and I have expectations for RDNA3+/CDNA3+ that are just not being met outside of data sciences.",Neutral
Intel,"I am a FOSS software developer, on hand right now I have several examples of every card you just listed, including almost every generation of NVidia since the Pascal, Intel ARC, Intel Flex, AMD Mi-25, AMD Mi-100.  Even the Radeon VII which AMD literally discontinued because it not only made zero commercial sense, but suffered from a silicon bug in it's PSP crippling some of it's core functionality.  I have no horse in this race, I am not picking on AMD vs NVIDIA here, I am trying to get AMD to fix things because we want to use their products.  You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  Very often these are caused buy the GPU driver crashing, but due to the design of DirectX, unless you explicitly enable it, and have the Graphics Tools SDK installed, and use a tool that lets you capture the output debug strings, you would never know.  [https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers](https://learn.microsoft.com/en-us/windows/win32/direct3d11/overviews-direct3d-11-devices-layers)",Neutral
Intel,Why does every valid criticism of amd has to be dragged down to that tribal stuff? Stop being a fanboy and demand better products.,Negative
Intel,I am not at all stating that NVIDIA GPU do not crash either. You are completely missing the point. NVIDIA GPUs can RECOVER from a crash. AMD GPUs fall flat on their face and require a cold reboot.,Negative
Intel,my dude this is a guy that has worked with both of the other 2 companies and has repeatedly complained about the shit locks and bugs in both intel and nvidia. the software that he has created is basically state of the art.  this is /r/amd not /r/AyyMD,Negative
Intel,"Not at all, you just keep missing the point entirely. You agreed with the post above you where is stated that the GPUs are rock solid. I provided evidence to show that they are not rock solid and do, from time to time have issues.  This is not overstating anything, this is showing you, and the post above you, are provably false in this assertion.  Just because you, a sample size of 1, have had few/no issues, doesn't mean there are clusters of other people experiencing issues with these GPUs.  \> And umm yeah, Nvidia also have bug / feedback report tools....  Yup, but did they need to make a large press release about it like AMD did. You should be worried about any company feeling the need advertise their debugging and crash reporting as a great new feature.  1) It should have been in there from day one.  2) If the software is stable, there should be few/no crashes.  3) You only make a press release about such things if you are trying to regain confidence in your user-base/investors because of the bad PR of your devices crashing. It's basically a ""look, we are fixing things"" release.",Negative
Intel,"My friends don't do VFIO stuff so I cannot say about them, but while I've never forcibly ended the VM (via htop or something) they have crashed repeatedly in the past, [especially this one](https://old.reddit.com/r/VFIO/comments/11c1sj7/single_gpu_passthrough_to_macos_ventura_on_qemu/). I've even got a 7800XT recently and haven't had any issues. Though this might be anecdotal since I am focusing on college right now and haven't put a ton of time into this recently.  EDIT: Also, I love your work, I hope I wasn't coming off as an asshole, I just have autism.",Negative
Intel,Guild Wars doesn't work on 7000 series cards and I assume it never will. a 3rd of the FPS I get with a 2080,Negative
Intel,"I'm not denying the existence of his issues around VFIO, I'm pushing back against him conflating it with gaming, for which there is a known circlejerk around AMD drivers being seen as 'unstable', which is hugely overblown.",Negative
Intel,You mentioned earlier you are diagnosing issues for a corporation related to IOV in GPUs they purchased. Are you refering to Navi based cards or Datacenter parts?,Neutral
Intel,"I partly agree with you there. But unfortunately it's difficult to avoid HDMI 2.1, when you need to hook it up to a 4K TV. I would absolutely *love* to see 4K TV manufacturers offer DisplayPort in future TVs, but that's probably not happening anytime soon.   About Nova you're probably right. But please keep in mind, that its scope is much more narrow than any other open source driver out there. Mostly, it only serves as an adapter between the Linux kernel and GSP firmware. Current Noveau implementations reflect this: GSP features are easier to implement and thus currently more feature complete. And since there is an AI/Nvidia hype train at the moment, they will probably also dedicate more resources into it than say stratis-storage.",Neutral
Intel,When did they do this switch? I remember years ago when I configured that their windows drivers weren’t being so nice to the card detected in a VM.,Negative
Intel,"The price of the GPU is not determined by the transistor count, but by the DIE size.   In the past they used to shrink the size WAY faster than now, enabling doubling transistor count per square inch every 2 to 4 years.   Now they barely manage to increase density by a 30%.   And while yes, they can increase the size, the size is what dictates the price of the core.   If they ""just increase the size"", the cost per generation will be 2 times the previous gen cost :)",Neutral
Intel,">I'm well aware of what VDI desktops are... it effectively the same thing though.  Nope, not at all. One is virtual with IOMMU tables and SR-IOV(and a ton of security around hardware layers), the other is a unified platform that runs metal software with no virtual layers. Clearly you do not understand VDI.",Negative
Intel,"You can just stop looking for solutions as it's not a bug. Your setup clearly exceeds the limkts for v-blank interval to perform memory reclocking. In that case  memory stays at 100% and you get a power hog (Navi 31 is especially bad because of MCD design. Deaktop Ryzen suffers from the same thing).  This will never be fixed, as there's nothing to fix. Works as intended and if you try reclocking your memory when eunning such a setup  you'll get screen flicker (happened in linux a month ago because they broke short v-blank detection)",Negative
Intel,"if the monitors run at different resolutions and frequency than each other my power increases. if my monitors match, idle power is normal",Neutral
Intel,100w is normal for the memory bus being clocked up.... yes.'  The exact same problem occurs on Nvidia hardware also since a decade also.,Neutral
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  I'm not the guy you're replying to, but for me, almost never.  I've had exactly one driver-based AMD issue - when I first got my 5700XT on release, there was a weird driver bug that caused the occasional BSOD when viewing video in a browser - this was fixed quickly.  My gaming stability issues were always caused by unstable RAM timings and CPU OC settings - since I upgraded to an AM5 platform with everything stock, I'm solid as a rock. My 7900XTX has been absolutely perfect.  There is an unfair perception in gaming with AMD's drivers where people think they are far worse than they really are - it's a circlejerk at this point.  Your issue is different (and valid), you don't need to conflate the known issues in professional use cases with gaming - it'll just get you pushback because people who use AMD cards for gaming (like me) know the drivers are fine for gaming, which makes you come across as being hyperbolic - and if you're being hyperbolic about the gaming stuff, what else are you being hyperbolic about? Even if you aren't, it calls into question your credibility on the main subject of your complaint.",Negative
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?    Literally zero. I guess I just have a good pc setup... It is weird how some people always have issues",Negative
Intel,"> You state you never had issues, however, how many times have you had a game randomly crash with no error/fault or some random error that is cryptic? How often have you assumed this is the game's fault?  My aging 5700 XT crashes in games far less often than my friends who are on various Nvidia cards from 2080 Ti to 4090.  Same for when I was on Polarix with RX 470s.  Game crashes are rarely the fault of the graphics driver (or hardware), regardless of brand.  This isn't a good point to be making, because it's just wrong.  > suffered from a silicon bug in it's PSP crippling some of it's core functionality  This again?  No, Radeon VII and other Vega products were killed off because they were very expensive to produce and they weren't moving enough units at any price to justify any further investment or even any meaningful support.  Everyone paying attention called this when they revealed Vega, and even long before with the tragic marketing.  Insert the GIF of Raja partying at the AMD event, complete with cigar.  People love coming up with theories as to what critical flaw or failure point caused a given generation of AMD GPUs to suck, and how those will be fixed in the next generation.  From silicon to firmware to coolers to mounting pressure to bad RAM to unfinished drivers or whatever else.  It's never the case.  There's never any 1 critical point of failure that make or break these products for their intended use case (gaming or workstation).  If you are an actual AMD partner working on things with workstation cards / compute cards, you **do** get actual, meaningful support for major issues.  Does AMD need to improve things?  Of course.  But to act like there's 1 critical flaw, or that something is fundamentally broken and making the cards unusable for a given purpose, or to cite George Hotz as an authority is just way off target.",Negative
Intel,"Part of it is rooting for the underdog, part of it is probably due to people legitimately not having problems.  I was an Nvidia user for several years, and moving to AMD I've had a lot of problems with black screen, full system crashes and driver timeouts that I haven't had on Nvidia.",Neutral
Intel,"Good ol' ""it works on my machine"".  It's a small and niche userbase so it gets downplayed, backed by ""it works on my machine"" when you express your concerns, despite the fact they don't use that feature or have zero knowledge on the topic. Same goes to H.264 hardware encoder being worst of the bunch for years.  And the average joe just doesn't use Linux, if they do, then few of of them actually toy around virtualization, then even fewer of them poke around hypervisors with device passthrough(instead of using emulated devices, which has poor performance and compatibility). It really is the most niche of the niche circle. I'm not looking down on users or playing gatekeeping/elitism but that's just a hard pill to swallow.  But that doesn't mean AMD should be ghosting the issues as people have been expressing their concerns even on datacenter systems where real money flows.  How many r/Ayymd trolls actually know VDI, VFIO and let alone what ""reset"" means? Probably has never google'd them, despite the fact one of the most well-respected FOSS wizards in this scene is trying to communicate with them. I hope gnif2 doesn't get upset from the trolls alone and wish him a good luck on Vanguard program. (I also came across his work on vendor-reset when I was poking around AMD integrated graphics device passthrough.)",Negative
Intel,"Demand what rofl, I have literally zero issues. 99% of criticism is not valid and is extremely biased and overblown, that is why.",Negative
Intel,"No they don't  I've crashed AMD gpu drivers plenty of times while overclocking and it recovered fine  AMD have dramatically improved their driver auto recovery from years ago when such basic crashes did require hard reboots.  Might still be shit in Linux, but what isn't...",Negative
Intel,Oh and XE also have bug feature reporting.  Omfg!!!!,Neutral
Intel,Nobody is 100% right ;),Neutral
Intel,Guild Wars 1 or 2 (does Guild Wars 1 even work anymore?? XD),Neutral
Intel,"It's been explained why what you just said is wrong and you appear to be ignoring it.  You don't understand the issue at hand and are just running your mouth making an ill-informed and baseless argument that is irrelevant to what is being discussed here. Either you tried to understand it and failed, or, more likely, you never tried to and just want to whine about Redditors.",Negative
Intel,"Firstly, i am barely even able to find any posts about this issue. Which means that issue is extremely case specific, so you should not categorically blame AMD and Adrenaline. With how many people this happen with, it is not problem of Adrenalin itself (otherwise it would've been reported A LOT more than i can find).   They may have some weird system conflict, or some weird BIOS setup from manufacturer. But Adrenalin installation doesn't OC your CPU just at fact of installation.  For context, if i still would've had my 5600X (now i have 5800X3D, and Adrenalin doesn't see it as CPU it can work with, as it doesn't provide CO option iirc), and had it OC'ed through BIOS, Adrenalin would've seen it as OC'ed. It doesn't mean that Adrenalin OC'ed CPU, but rather that SOMETHING did that.  I also saw reports that after deleting Adrenalin, resetting BIOS to defaults and installing same exact Adrenalin version back, they stopped having OC on their CPU.     From global issues with CPU OC was only one i mentioned. When AMD integrated Ryzen Master, old GPU OC presets did reset CPU OC values to default. To fix that you just needed re-set GPU OC and resave preset after update.",Negative
Intel,"Yes, these are Instinct Mi100 for now, depending on how things go with this GPU it may also be later GPU generations also.",Neutral
Intel,What about using a DP to HDMI 2.1 adapter for that situation?,Neutral
Intel,"2021 my guy, it's right there on the date of the article.",Neutral
Intel,LOL you literally just said this one thing is not like this other thing because its the same as the thing. PS Streaming runs multiple instances of hardware per node... with separate virtualized OS deal with it.,Neutral
Intel,They could do something like relocate video framebuffers to one memory channel and turn the rest off... if idle is detected.  But that would be very complicated.,Neutral
Intel,"I see your point, and perhaps my statement on being so unstable is a bit over the top, however in my personal experience (if that's all we are comparing here), every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  In-fact no more then a few days ago I passed on memory dumps to the RTG for a \`VIDEO\_DXGKRNL\_FATAL\_ERROR\` BSOD triggered by simply running a hard disk benchmark in Passmark (which is very odd) on my 7900XT.  ``` 4: kd> !analyze -v ******************************************************************************* *                                                                             * *                        Bugcheck Analysis                                    * *                                                                             * *******************************************************************************  VIDEO_DXGKRNL_FATAL_ERROR (113) The dxgkrnl has detected that a violation has occurred. This resulted in a condition that dxgkrnl can no longer progress.  By crashing, dxgkrnl is attempting to get enough information into the minidump such that somebody can pinpoint the crash cause. Any other values after parameter 1 must be individually examined according to the subtype. Arguments: Arg1: 0000000000000019, The subtype of the BugCheck: Arg2: 0000000000000001 Arg3: 0000000000001234 Arg4: 0000000000001111 ```  Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to   provide an industry standard means to properly and fully reset the GPU when these faults occur.  The amount of man hours wasted in developing and maintaining the reset routines in both the Windows and Linux drivers are insane, and could be put towards more important matters/features/fixes.",Neutral
Intel,And I guess infallible game developers too then. /s,Neutral
Intel,So you decide what criticism is valid and what not? lol,Neutral
Intel,AMD cards don't recover from a crash. This is well known and can be triggered in a repeatable manner on any OS.  You don't understand the issue and are just running your mouth.,Negative
Intel,"Yup, but do you see them making a big press release about it?",Neutral
Intel,that is not how it works but sure,Negative
Intel,2 lol  7900xtx dips to 30fps in combat or around players. 2080 never dips below 70,Neutral
Intel,>whine about Redditors.  The irony.,Neutral
Intel,"IF I got an AMD gpu, that would be my only option.   There's mixed reports on that - you have to make sure it's an active adapter - and some of the Display port 2.0 to hdmi 2.1 adapters might work.   Some ppl say a 'Cable Matters' brand works but you might have to update/upgrade the firmware.   But, if you are shopping for a higher tier card - for e.g., a 7900 xtx - that's a pretty expensive risk - especially when you have to factor in the cost of an adapter, too?",Neutral
Intel,learn to comprehend.,Neutral
Intel,Thank you for your response - I actually agree with a lot of what you are saying. AMD is lacking in pro support for quite specific but very important things and you aren't the first professional to point this stuff out. How much of this is down to a lack of resources to pump into software and r&d compared to nvidia over many years or how much of it is just plain incompetence I can't say,Neutral
Intel,">every generation of GPU since Vega I have used, has had crash to desktop issues, or BSOD issues under very standard and common workloads.  I thought it was only me... but ye it is this bad - just watching youtube and doing discord video call at same time - crash  >At the end of the day here, I am not trying to say ""AMD is bad, do not use them"". I am trying to say that AMD need to provide an industry standard means to properly and fully reset the GPU when these faults occur.  I can say - AMD is bad, do not use it, their hardware do not work.  Wasting time to ""debug and fix"" their drivers - it can be fun for ""some time"" until you see that there are infinite amount of bugs, and every kernel driver release make everything randomly even worse than version before.",Negative
Intel,"> Note: There is zero doubt that this is a driver bug, I am running a EPYC workstation with ECC RAM, no overclocking, etc.  Can you replicate the issue?  If so, it could be a driver bug.  If not, have you actually tested your memory?  Being a workstation platform or ECC memory means nothing.  I bought some of the first Zen 2 based servers on the market, and I got one with a faulty CPU with a bad memory controller that affected only a single slot.  Dell had to come out the next day with a new CPU.",Negative
Intel,"No, that would be you obviously /s",Neutral
Intel,Oh so it's only applicable in specific usage scenarios outside of standard usage...  Got it.,Neutral
Intel,"Yea, given the state of XE drivers every major update has come with significant PR.",Neutral
Intel,Why not ;),Negative
Intel,Go word salad elsewhere.,Neutral
Intel,"I have replicated the issue reliably yes, and across two different systems.",Neutral
Intel,If discord crashes my drivers.. once every few hours. I have to reboot,Negative
Intel,Discord doesn't crash my drivers  I don't have to reboot.,Negative
Intel,Really love how the 6000 series radeons look.,Positive
Intel,"Why is there is a 6800 and 6800XT pictured, but only results for the 6800XT?",Negative
Intel,That's a good looking line up,Positive
Intel,"From the article:   >However, temporal upsampling such as AMD FSR or Nvidia DLSS has now become so good that it either matches or **sometimes even exceeds the image quality of native Ultra HD** despite the lower rendering resolution.   Hmmm.. I don't agree with that.",Negative
Intel,"I had a reference 6800 that i sold to my brother when i upgraded to a 7900xt, I miss the design i loved it since the moment it was announced.",Positive
Intel,"In my opinion RX 6000, aswell as RTX 980/1080 Ti are the best looking graphics cards. Notable mentions are Radeon VII, RX 5700 (non-XT) and Intel Arc 770 Limited Edition.",Positive
Intel,Darktide looks&runs way better with FSR2 than native 1080p for me. I don't know how they do it but there is no amount of AA that makes native resolution look better.,Positive
Intel,"Use Radeon Image Sharpening at 50% in the game's Radeon Settings profile when running native. FSR2 has its own sharpening pass. Many TAA implementations are blurry as fuck, which gives the illusion of better image quality when upscaling.",Negative
Intel,Okay fair enough! To me any upscaling has always looked worse than native in the games I've played and I've left it off.  Though it has undoubtedly gotten better recently - to my eyes it's never looked **better** than native resolution.,Negative
Intel,"I've used both FSR (2160p desktop) and DLSS (1080p laptop) and the loss in quality is noticeable to me. Always a softer image with less detail - yes, even DLSS. Performance always has a cost. Temporal upscaling is no different. DLAA and FSRAA actually are better than native, since they replace terrible TAA implementations and render at native.  However, this better than native upscaling narrative seems more like a belief (or a marketing push/tactic) than reality.   - If action is high enough, it probably won't matter much, but in single-player games where I like to look around, I just can't deal with the resolution loss. I'd rather turn RT off, as I did in Control (for AMD and Nvidia HW), which actually has decent RT effects.",Negative
Intel,"Tbf, Darktide is the first game where this was the case. I tried lots of different combinations but nothing worked out. I'll probably try the other suggestiom with the sharpening in Adrenalin later.",Negative
Intel,That's suprising. Generally the 70 series and AMD equivalent cards are more popular in Germany. Is 4060ti discounted heavily?,Neutral
Intel,"Always happy to see ARC succeeding. We need a third competitor, and they've got what it takes.",Positive
Intel,Im more confused about how do you guys still buy RX 6xxx series. Only RX 66xx series that are still widely available in my country. RX 67xx and above is just OOS.,Neutral
Intel,That 7900xtx sale number is insane,Negative
Intel,That just shows that most people that buy GPU's don't know a thing about them.,Negative
Intel,"4060 Ti is not discounted well at all here.   390€ and 460€ for 8/16GB versions, really bad deal which is why I am absolutely baffled to see it this high in the list. Might be christmas shoppers who just buy the highest nvidia card within their budget without doing any research - that's my best guess actually.",Negative
Intel,best discounts were 6750xt 6800 and 7800xt,Positive
Intel,4060 ti is selling as a surrogate Quadro for AI/ML. $400 for 16gb is not awful in that context and you get full CUDA support,Neutral
Intel,"Intel is putting in the effort on the software front as well. Arc cards are great value, especially with the improvements intel has made with driver updates. With talk about XeSS being integrated into DirectX with Windows 12 (among other upscalers as well) I hope that intel GPUs will be even more competitive.",Positive
Intel,"Battlemage needs to be stable and competitively priced.   First impression matters so much these days, it cannot be overstated imo.",Neutral
Intel,"They're not out of stock there, duh",Neutral
Intel,Yeah the Germans love the 7900xtx in particular for some reason. It was really high up considering its price in last week's summary as well. I guess they don't appreciate the 4090's price.,Positive
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"I have so many (adult) friends who always bought NVIDIA, will always buy NVIDIA and don’t do a single bit of research.   Had one friend even ask me, I recommended the 7900Xt for her budget and she got the 4070 Ti because she didn’t trust my word or AMD  It’s such a big habit issue. Shopping season really showed me that nvidia could literally sell a brick and habit shoppers would buy it",Negative
Intel,">390€ and 460€ for 8/16GB  That equals to $350 and $415 before taxes, a lot lower than in USA ($390 and $450 before taxes)",Neutral
Intel,"They are pretty much gone in Germany as well:  6950XT - only XFX for 599 at Mindfactory   6900XT - gone   6800XT - only XFX for 499 at Mindfactory and Asrock for 580 at various   6800 - only XFX for 435-440 at Mindfactory and some   6750XT - a bunch of models for 373-405 at various   6700XT - a bunch of models for 344-369 at various   6700 - XFX and Sapphire for 298 and 322 from one retailer   6650XT - gone   6600XT - gone     So the high-end is gone except that XFX stock that will probably be gone in the next 2-3 weeks as well, the lower end is gone. Only 6750XT and 6700XT is still there a bit more and of course the 6600 because that has no replacement at it's current 200-220 price, the 7600 is way more expensive at 270-320.",Neutral
Intel,"I know, but i mean, both 6750 XT and 6700 XT still has more than 300 stocks each in single week (if i read the stat correctly). That's a lot of GPU. I thought the stock is already thin globally which in turn make stock in my place nonexist.",Neutral
Intel,"I thought I saw some 8GB models as low as $330 and 16GB models at $400 but even then I would much rather get another GPU like a 6700XT  (you can still get at here and we even had one down at 299€ for a while, think they are back at 330€ tho)",Neutral
Intel,the card is pretty bad if you missed that somehow,Negative
Intel,AMD probably ships leftover to countries in which they know it will sell,Neutral
Intel,"Sure, but I was replying to you saying ""4060 Ti is not discounted **well at all** here""",Neutral
Intel,"Because most people are not willing to pay that much for GPUs that weak. Sure ""discounted well"" is subjective anyways, I got your point that they are below MSRP but I am pretty sure you know what was implied when reading my comment.",Negative
Intel,"As you notice the photoshop version differs, so you can't compare them really",Neutral
Intel,I also another insanely high score:     7900X + RTX4090 [Photoshop score 8995](https://benchmarks.pugetsystems.com/benchmarks/view.php?id=168538),Neutral
Intel,So basically PC games are never going to tell us what the specs are to run the game native ever again.,Negative
Intel,"Well, folks, the game is basically software ray tracing an open world regardless of settings at all the times using compute shaders from what I gather. I mean, what'd you expect from something pushing consoles to the brink? That it would not be equally demanding on PC?",Neutral
Intel,The true crime here is needing FSR to reach these requirements.,Negative
Intel,"Oh wow, this is probably the first time I'm seeing dual channel listed in the RAM section of specs requirements.",Positive
Intel,im more baffled as to why they are comparing a RX 5700 to a GTX 1070. Id have thought they would have said a RX 590 (instead of 5700) or RTX 2060 (instead of 1070),Negative
Intel,> Enthusiast   > 1440p [...] with FSR Quality [...] 60 FPS  1440p + xxxx Quality upscaling is just another way to say 1080p you cowards. It's 2023 and enthusiast level is 1080p @ 60 FPS.   Looks pretty though with the amount of vegetation.  I'm surprised that they put the RTX 3080 and RX 6800 XT side by side in a game that has forced ray tracing. I guess that's the advantage of being cross platform and AMD sponsored.,Neutral
Intel,"well, at least the chart is easy to read, not a complete mess",Neutral
Intel,This gives me hope that Star Wars outlaws will also have FSR 3. 🤌,Positive
Intel,60fps with balanced upscaling on a 4080/7900xtx … That’s not a good sign,Negative
Intel,It's joever. We are officially at the point where devs are using upscaling as the standard to reach playable frame rates instead of optimizing their games.,Negative
Intel,"If you look in the top right, these seem to be based on ray tracing being enabled? If that is the case, these specs actually seem reasonable.",Neutral
Intel,"FSR and DLSS were supposed to be optional, now they seem to be mandatory in any graphics setup.",Neutral
Intel,Yay finally a game that'll have both fsr 3 and dlss 3 so it'll work perfectly on either gpu brand at launch.,Positive
Intel,"Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. The requirements should all be done without any upscaling or frame generation tech to be upfront and honest about how the game will run. Upscaling will then just be an available option for users to further increase performance.",Negative
Intel,"Actually not to bad for current standards, 1440p 60 with fsr quality on a 6800xt/3080 is great if high includes RT, if it doesn't then it sucks",Negative
Intel,"So with my 6800xt, which should be able to do 1440p high settings at over 60fps easily, I'll have to use FSR just to get 60fps? Great, another unoptimized game headed our way.  So sick of the awful PC ports and such. 6800xt is still a very powerful card yet I'm seeing more and more games require it for 1440p 60fps high or even 1080p 60 fps high. Ridiculous.  The reliance on upscaling tech instead of properly optimizing the game is becoming a problem. And it seems this game has forced RT which is idiotic. RT destroys performance and should always be an option.",Negative
Intel,"If you notice, all the mentioned AMD GPUs are significantly better than Nvidia GPUs, so does that mean bad performance for AMD GPUs?",Neutral
Intel,So i bought an 1200€ card (rx 7900xtx in my region and at the time of launch) and can now enjoy new games at 60 fps with BALANCED fsr upscaling. We love to see it. /s,Positive
Intel,so I'm doomed to use FSR even at 1080p ? what a shame pc gaming has become.,Negative
Intel,"I honestly don't get this upscaling outrage. It's not a fixed requirement. If you want to play native, just turn off upscaling but you will have to turn down settings to compensate. Everyone wants to have their cake and eat it too. People want super low requirements, super high graphics with high FPS.  The thing is game devs/publishers want to highlight all the ""eye candy"" in games to show their advancement and that's how they want to advertise it. They have a certain level of visuals they want to present for the game. So they crank up the baseline and focus on making it look as good as they possibly can and by using upscalers to get to the playable FPS.  Other option is they ditch making games look any better and just keep it restrained to good FPS at native. But then you'll have the other subset of complainers that will complain about the game's graphics looking like X many years ago.  Take Alan Wake 2 for example, The medium setting looks better that some High-Ultra setting games from a few years ago. Running a AAA title 2-3 years ago at 'medium' preset may not equate to running a current AAA title at medium preset.  The games are getting too demanding because they are starting to push the limits visually (partially because on AAA titles, that's something they really focus on for advertising), if your card can run it, great, if not, adjust your settings to match your cards capabilities. If you bought a 6700XT 2+ years ago to play native 1440P on high setting, guess what, it's not always going to play 1440P at high settings as the years go by. You have to adjust your expectations of the card's capabilities as time goes on. Medium preset now is likely better than High preset then. If you want the ""same"" performance you used to get, match the visuals and see how it does.  Seems like the only way to get people to stop complaining is to just halt all visual/graphical progress and stick to limiting the game's visuals so they run well on mid/low tier cards.  People keep throwing out ""OPTIMIZATION"" but on one knows what/how they want that done. WHAT exactly do you want optimized? How do we know the game isn't optimized to the graphics/performance it gives?  What if they just made the ""High Quality"" preset look like games from 2018, would that now be considered ""Optimized""?  (I'm not talking about this game in particular, just in general, it was same outrage w/ AW2 and that turned out better than expected)",Neutral
Intel,I hope they will bundle this game with CPUs/GPUs,Positive
Intel,holy hell so for 1080p you need a 1440p card because not even a RX 7600 is on par with a 3060ti or a 6700XT,Negative
Intel,Recently upgraded GPU and CPU and crazy that newer games are already requiring the newest GPUs to play them. Personally I don't think it's a horrible thing but at the same time it just screams horrible optimization.,Negative
Intel,This game better look like real life with those specs. I does looks beautiful!,Positive
Intel,"Another garbage with forced upscaling at baseline. When upscaling winded up - everyone was ""Hell yeah, high refresh rate AAA gaming""  Fast forward to today: ""FSR performance just to hit 1080p 60fps - yay..""   I fucking knew it.. it was so obvious upscaling will turn into this and replace game optimizations. Why optimizing shit, when you can tell people to turn on fucking FSR / DLSS / XeSS performance??   What a fucking shitshow... And few weeks back when I said I'm not gonna buy AMD GPU again because upscaling is getting mandatory and DLSS is just objectively superior - I was laughed in the face. Every major AAA game onwards will be shoving up upscaling as base line performance - so your typical 60fps marks (and 30fps - yes apparently that is still a thing on PC in 2023). There's no escape from this.",Negative
Intel,Damn. My 5700XT did Mirage fine on Medium/Low shadows. Now its basement quality.,Negative
Intel,This is starting to get scary and not cool. Every stat is not native performance and a 6800xt with fsr at 1440p is only pulling 60fps?!?!? What the actual fuck is going on?!?!? Midrange is definitely on its death bed if this is the new norm...,Negative
Intel,I love it how absurd these things are these days.,Negative
Intel,Really tragic this IP ended up with the most anti-consumer publisher in the world.  Layers upon layers of DRM.  Timed Epic exclusivity. No achievements.,Negative
Intel,Does anyone know if it supports SLI or crossfire?,Neutral
Intel,Source  https://news.ubisoft.com/en-gb/article/G0eJBcH8NcvNO2MPqb1KV,Neutral
Intel,"Hopefully this is another Alan Wake situation where the game performs better than what their system requirements would suggest. With this being a based on a movie game, I don’t have super high hopes that that’ll be the case though. Going from movie to game and vice versa almost always has bad results.   Also, displaying system requirements with upscaling is a bad joke. This is basically saying you’re not getting native 1080p 60fps without a 6800xt or 3080. Upscaling is definitely becoming the crutch a lot of folks feared it would be.",Negative
Intel,I thought upscaling technology is for low end gamers to get more FPS or people who work on 4k monitors and use upscaling to get better frames on them. but it seems to be used as an excuse for bad game optimization. What a complete joke!!,Negative
Intel,They are so bad this days that if the say the specs without the upscaling it would be 4090s all across the board,Negative
Intel,Why in the f*ck is upscaling included on a specs page?,Negative
Intel,no more software optimization and full upscaling  bleah,Neutral
Intel,Now THIS is what I call a reasonable spec sheet for a next-gen game 👏👏👏,Negative
Intel,"Surprisingly good specs. 1080p high 60 FPS on 3060ti/6700XT is what we can expect from 3 years old cards in open world games.  Yes, I know it's with upscaler. But if the footage stays - it's understandable. Ubisoft is always about open world games afterall.",Positive
Intel,"I've never even heard of this game, nor care about it, but these system requirements offend me.",Negative
Intel,seems like graphics reached its eak with ps4 and games dont look that much better naymore and requires 4k dollar pc to run with dlss,Negative
Intel,"What about steam achievements ? :), i want to show off perfect games on my steam profile :)",Positive
Intel,"Telling me my $1,000 GPU will only get me ~1200p at 60fps tells me you don't want me to buy your game.",Negative
Intel,How about support them with games that people actually PLAYS ? Like Alan wake 2 🤣,Neutral
Intel,Fuck FSR and DLSS. This isn’t how they are meant to be used.  Rather than making a good experience better they are being used to make unplayable trash playable. Papering over shit programming.,Negative
Intel,Nice,Positive
Intel,"If Anti-Lag is still locked by then , please enjoy your frame gen input lag.",Neutral
Intel,"The game has a huge graphical potential, it's the right time to release that much potential environment in this era .   But it wasted its potential by considering lower class GPUs there by reducing polygonal complexity and complexity of environment including terrain and vegetation , animal complexity ,etc.....  Imo, it needs to have 2X polygon than what it seems.  It seems ultra poooooorly optimized as per the recommended requirements coz it's a bad graphics.",Negative
Intel,Native gaming died or what ? Wtf they turning pc gaming into console gaming,Neutral
Intel,"60 FPS WITH Frame Gen AND FSR balanced? God, that's gonna be a horrible experience.  I really don't like the direction we are heading.",Negative
Intel,4k balanced FSR means 1200-1240p ... lower than 1440p ! And 4080-7900 XTX for that ?,Neutral
Intel,Their supporting FSR 2 and 3? what the hell does that even mean? Unless they plan on releasing b4 FSR3 becomes drops.,Negative
Intel,"Omg, it would be a graphic master piece or  bad optimized thing.",Negative
Intel,First time I see matches recommendations for nv and amd GPUs...,Neutral
Intel,Rip laptop rtx 3060 6gb,Neutral
Intel,"How do FSR 2 & 3 work with tech like ReShade?  ReShade has a pretty cool shader that will make any game into stereoscopic 3D.   When it works it works well, but it doesn't work in every game.  I'm curious because I would like to try it for this, since like 99.97% of Avatar's appeal (shtick?) is the 3D.",Positive
Intel,Upscaled 1080p low settings and only 30fps with A750? At least the game has xess.,Neutral
Intel,*NATIVE* resolution gang ftw!,Neutral
Intel,"I have been wondering with a few releases lately (Forza Motorsport, Cities Skylines) what GPU the dev machines they are using. I have a 7900XT and it ain’t enough to get above 60FPS without variable resolution at 1440p, seems kinda crazy that one of the top 5 cards from the latest gen can’t push enough pixels for these games.",Negative
Intel,Looks capped at 60fps?,Neutral
Intel,"Funny, how it only tells you the settings for FSR but not for DLSS and XeSS even though those 2 are also supported.",Neutral
Intel,How come 6800XT is mentioned? But not the 7800xt? As recommended specs for 1440p?,Neutral
Intel,"Rip my 780m and rtx 4050m. I don't understand why upscaling is ""mandatory"" not ""supplementary"". Do we need a SLI rtx 4090 for native 4k@60hz?",Negative
Intel,Is it using UE5?,Neutral
Intel,"Ubisoft, rip on launch.",Neutral
Intel,guessing no DLSS3 then?,Neutral
Intel,I guess that fsr3 will be added in an update at some point since everything mentions fsr2. It will be interesting to see it running on my gtx 1060 since it seems to be a big upgrade on older cards.,Positive
Intel,"Another game with upscaling baked into all presets. ""Free performance"" they said. I expect frame hallucination tech to be included in these presets in 1-2 years tops",Neutral
Intel,"Can't wait to be ridiculed by Alex at DF again for wanting PC requirements that don't include upscaling.   Upscaling should be an assist, not a requirement.",Negative
Intel,Farewell 1660ti… looks like it’s time for an upgrade,Neutral
Intel,well my 3300x is now obsolete for these new AAA games...,Negative
Intel,4k ultra right up my alley 😏,Neutral
Intel,fsr3 frame gen but in the specs themselves FSR2 is stated everywhere. What? So there's FS3 FG bu no FSR3 upscaling?,Neutral
Intel,7900xtx will do 4k 120fps with FSR 3 then I guess?,Neutral
Intel,What must one do to achieve a higher rank than an enthusiast? A demi-god?,Neutral
Intel,"I think we're at a point in gaming where DLSS / FSR will be mainstream in pretty much every game now. I mean even my 4080 can't get 60FPS 1440p in the Witcher 3 or Cyberpunk maxed out with ray tracing, have to use DLSS to increase to above 60fps. I do think DLSS / FSR is actually good technology, but for those enthusiasts who like to play native, I feel like having a $1200 GPU that can't run 4K or even 1440p ULTRA settings at 60fps depending on the game, is just a cash grab.",Neutral
Intel,Is vrr fixed with frame gen then?,Neutral
Intel,"Will someone please get a message to developers that FSR/DLSS should be *optional* and used only if you want to sacrifice some visual fidelity in order to run 144 FPS+ on high refresh rate displays, and let them know FSR/DLSS should never be *required* to get a 90GB bundle of spaghetti code to *barely* reach a blurry ass 60 FPS?  K? Theeenks!",Negative
Intel,"Yet another AMD sponsored game without DLSS....      EDIT: Nm, it appears this game will offer DLSS thank god.",Positive
Intel,Hoping with my 7800x3d and 7900xtx fsr 3 4k maxed is around 120fps for my tv refresh rate. To be honest at my age I barely notice the quality difference of any input lag.,Neutral
Intel,They recommend upscaling even at 1080p.. disgusting ew,Negative
Intel,pretty much this we all knew they would start using upscaling as a crutch.,Negative
Intel,Welcome to modern times and how you can’t just brute force everything anymore. Or you can go buy a 4090 and not complain about how expensive that is.,Negative
Intel,"Yeah, I'm not crazy about using upscaling techniques on everything to gauge it's performance and requirements. I think more time needs spent on optimization but I could be ignorant for stating that.",Negative
Intel,My thoughts too…,Neutral
Intel,Thanks to all of you that were screaming dlss looks better than native lmao.,Positive
Intel,"I tend to be much more picky than those around me. A group of people I play with were all playing Remnant 2, and I FSR was driving me crazy, they all have way less powerful computers than I do, and none of them cared at all FSR was on. In fact they were impressed with how well it ran and how it looked. I think there is a general 80% of the PC population that doesn't mess with graphic settings and just wants the game to play. I'm a min/maxer when it comes to graphics where I constantly tweak it until I get the perfect output for me.     I'd love to have real data from games like this on how many people actively change graphic settings.",Neutral
Intel,and a year ago i kept saying this here and i got downvoted to hell   its so obvious that the game engine devs and game companies pressured both nvidia and amd for this because they can release games faster and somewhat unoptimised,Negative
Intel,"Upscaling, once a promising and beneficial tech, now abused by almost every dev cause they are lazy to optimize. :(",Negative
Intel,Nope we as a community abused a nice thing,Positive
Intel,"Alan Wake 2 (and others) is proof that this doesn't matter any more. DLSS Quality looks and runs better than DLAA native rendering as shown a number of times and documented by tech power up and actually observed by those of us playing it  It's about time people stop crying the ""omg no native rendering"" beat as this is a mentality that is irrelevant in 2023. It's pretty clear that too many games come with performance issues and as such ""native"" rendering has the worst performance in those titles. At least upscaling offers a means to get better performance and in the vast majority of titles, better image quality as a result too with intricate details rendered at a higher fidelity thanks to image reconstruction. The likes of Digital Foundry showcase those benefits often in their videos. Plus the fact that all of the consoles upscale yet nobody bats an eyelid, yet for some reason a small portion of PC gamers seem to demand it no matter what, and then complain when it doesn't run as well as they had hoped in various titles.  It's a hard pill to swallow, but it's \[upscaling\] progression and the key driving factor as to why we can path trace and use other modern engine tech with such good performance and visual quality. This is probably the wrong sub to say such things, but this is the truth, and I fully expect pitchforks to be mounted.... But don't take it out on me, point the finger at AMD and ask them why they are falling so far behind.  At the end of the day if it looks good and runs good then nobody needs to bat an eyelid on whether it's got native or upscaled rendering. Everyone benefits as a result. Fact is the vast majority of modern games look and run better when using DLSS with only a handful of FSR exceptions (Callisto Protocol, for example).  Personally I want improved image detail, sharpness and performance, not just AA that DLAA brings to the table since it's just a more modern AA method vs ancient TAA/MSAA/FXAA etc. I don't care if there's no native option, I can just run with DLDSR if I want to render at a higher res and then apply an upscaler to that output to get the best of both worlds.",Neutral
Intel,Didn't take them long to make upscaling worthless.,Negative
Intel,i smell a burgeoning cottage industry of game spec reviewers!,Neutral
Intel,"I've hated upscaling since the beginning.   Now, it's being proven right in our faces that companies are no longer making 100% they're making 45% done games, and then they rely on upscaling and fake frames to do the rest.   Why have a fully optimized game that runs well natively when you can just lower the resolution by 3x then have frame generation help?   Full force ahead to frame generation= Every game will be optimized so badly that you're forced to upscale",Negative
Intel,"It’s literally right there in the hardware specs. 1080p fsr2 quality is 720p input resolution, so at native the game runs at 720p low on a 1070 or 5700.  It is the inverse of what reviewers have been doing with their benchmarks - if you are a fan who really has a stick up their ass about native res, just do the math yourself. They’ve given you all the pieces you need to calculate it out yourself, but that’s not how the game is supposed to be run so they aren’t going to put native res in the official marketing material.  Games are optimized around upscaling now and it is unhelpful and misleading to pretend otherwise. Rendering 4x as many pixels is *obviously* going to throw things off with more intensive effects, and lead to a generally unplayable experience.",Neutral
Intel,"Judging by how games work on 4090/7900 xtx, these cards turn from 4k 60 fps to 1080p 60 fps",Neutral
Intel,requirements have been a joke for over 20 years at this point i dont know why people are surprised.   on the other hand crappy devs using fancy upscaling and fake frames to get the game playable instead of just making a well made game is infuriating,Negative
Intel,yeah this is the new standard,Neutral
Intel,"RT is the future. With this game and Alan Wake 2 using software RT at all times and hardware RT for anything beyond ""low"", RT is the future.  At some point GPUs will be so powerful and game engines will have RT in the bag that RT will be used in basically all indie games...and then suddenly nobody will talk about RT anymore because RT is in everything for years.",Positive
Intel,and with fsr now standard on consoles it(or a similar tech) will be standard on pc :(,Neutral
Intel,"people also forget both AMD and nvidia didn't really offer a generation leap in GPU performance this gen, sure it's popular to call games ""unoptimized"" (I know many AAA releases are, not denying that) but the issue compounds with the fact that game devs couldn't predict a GPU generation to be this bad caused by inflated crypro-boom margins a generation prior",Negative
Intel,"As with almost every game release recently, expect the game to perform a bit better than the requirements.    Sometimes the hardware can achieve 60 FPS most of the time but drops below that in other times, so the devs just say it's guaranteed 30 FPS and call it a day. If you look at the 5700 vs 6700XT, the gap is almost 50% in most titles, this means that if the numbers for the 5700 were accurate then the 6700XT will be getting around 45 FPS at 1080p low (unless the game utilizes special hardware only available on the 6700XT) but the game targets 60 FPS at 1080p high on the 6700XT which leads me to believe the 5700 is gonna perform much better which should leave room for turning off upscaling.",Positive
Intel,"It does state ray tracing in the top right, we don't know if that's included in the requirements or not. I would definitely reserve judgment until we find out.",Neutral
Intel,Don't forget about Frame Gen too. IMO that's the worst part because that means it's 30 fps without it. Imagine the input latency...,Negative
Intel,The actual true crime here is even having fsr to begin with. It should just have dlss,Negative
Intel,Seems like they tried to cover every basis with these.,Neutral
Intel,"Yeah it's kinda weird, even games that had a massive difference of performance when using dual channel RAM never listed it on their requirements :/",Negative
Intel,"I've noticed that many developers do this, for some reason developers seem to have some kind of secret passion for the 1070 lol",Neutral
Intel,It's because the minimum settings is with software RT enabled and thus they're using the fastest cards without hardware RT that can reach 1080p30 FSR2.,Neutral
Intel,"GCN support is over, RDNA1 is the lowest currently supported arch.",Negative
Intel,"In 2023, RX 5700 performs equally or better than the RTX 2060 Super/2070. The 5600XT is more like a 2060 competitor.   Either the game runs like shit on AMD hardware, or the system requirements are wrong and they just write down the components that they tested with.   Another thing is that the game is doing is software based RT by default and that might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.",Negative
Intel,"They're not, those are the cheapest/slowest GPUs that have 8 GB of VRAM. Clearly game devs are done pandering to people who bought Nvidia's e-waste.",Negative
Intel,What do you mean forced raytracing?,Neutral
Intel,1440 + upscaling looks better than 1080p because you're on a 1440 resolution monitor.   Do you even 1440p?,Positive
Intel,"Honestly RT has a lot of fluff and the majority of which is the most costly for the least amount of noticeable fidelity increase. Try path tracing in Cyberpunk with reduced rays & bounces and you'll still get great soft lighting and GI without most of the performance hit when full tilt. If games only leverage certain aspects of RT then the performance is still comparable, that's also why the 3080 and 6800XT perform similarly in Fortnite with full RT/nanite/lumen settings.",Neutral
Intel,It's actually 960p :(,Neutral
Intel,"can advertize pushing all the triangles and RT if you dont stick with a low resolution, certain things launched WAY before they should have in the consumer space.",Neutral
Intel,Support FSR3.0 and being able to use FSR3.0 at launch gives me a bit of skepticism lately,Neutral
Intel,We'll see in a year.,Neutral
Intel,AFAIK  &#x200B;  It is with RT,Neutral
Intel,Seems pretty good to me given it is at 4K with RT,Positive
Intel,"Likely includes the RT features , its also 4k",Neutral
Intel,"Its ubishit, what do u expect, however, since the game has RT reflections, shadows and GI, then i would say this is based on AMD shit RT performance, so lets wait and see how the 4080 is gonna perform.",Negative
Intel,"I wouldn’t mind so much if it was exclusively a 4K thing, but now devs are leaning on upscaling *at 1080p*, where it still objectively looks like ass and blurs everything in the background.",Negative
Intel,"Everyone asked for this. ""iTs tHe fuTuRe"".  ""LoOks bEtTeR tHan nAtiVe"". ""BiGgeR nUmBeR bEtTer"".",Positive
Intel,and we were called [names that frankly should have resulting in bans] when we said this is where it would lead to. and we still get shat on for saying the same about fake frames,Negative
Intel,"I think the top right table is listing the available features/settings the game supports. Depending on the presets, RT may be enabled by default or it may be a separate option from the presets. I guess we'll find out.",Neutral
Intel,"Nothing will stop DLSS / FSR nor RT in new games at this point.  The problem with the rising hardware requirements is, that it will keep a lot of 1440p/4k dreaming gamers with 1080p for a very long time and FSR doesnt really look good enough in 1080p.",Negative
Intel,I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.,Neutral
Intel,Exactly! It even got xess so Intel users also can use xess,Positive
Intel,">Really disliking this trend of all new games having ""with FSR/DLSS"" in their PC requirements tables. T  its not much different for consoles , like alan wake 2 uses Low to mid settings and FSR balanced on PS5 even on switch titles use scaling and no man sky even FSR and stuff.",Negative
Intel,"I agree with this, that’s not that outrageous if this does include RT. Non RT would be pretty bad to need fsr but that is the quality mode.",Neutral
Intel,This is going to be the exact same situation as with Alan Wake. The game is going to come out and the requirements are going to make sense. People just aren't used to seeing games with mandatory rt.,Neutral
Intel,I would say pressured bc they announced it over a year ago and promised we will get it in september. Thats the problem with games an tech they all make promises and then cant keep the deadline. Just make smth new and when it works then announce it and make the last fine adjustments. If you make promises to early you set yourself up for failure.,Negative
Intel,"Maybe on the lower end, but for enthusiast and ultra... no, they're not ""significantly"" better. These are pretty on-par comparisons (outside of RT).",Negative
Intel,"How so? The 4080 and 7900xtx are neck in neck, same as the 6800 and 3080.",Neutral
Intel,"It's 4k, it will take another 5-8 years to be mainstream.  1440p slowly creeping to 10% marketshare give it another 2-3 years to be mainstream",Neutral
Intel,"Consoles use fsr as well, and not just fsr but often at low precision making the upscale rougher. Its an industry wide trend.",Neutral
Intel,">what a shame pc gaming has become.  Could be worse , Consoles use mostly FSR balanced like in Alan wake 2 the PS5 uses Low to mid settings and FSR balanced.  its likely the trend of going from specially home made engines  catered to the games needs to "" Can do everything , but nothing great"" multi tool engines like UE.",Negative
Intel,"> what a shame pc gaming has become  I mean, PC gaming is in the best state ever. So many amazing games that run so well. Just don't buy all these shitty games, not like there is a shortage.  Tbh this game looks like standard movie cash grab shite anyway. Solid 4/10 game I bet.",Negative
Intel,Which means if all games will require upscaling it will force AMD to improve there's. I'm on board with you though will see way less optimization going forward which is not what we want.,Negative
Intel,"Absolutely agreed my friend. Time to leave pc gaming and focus on building yourself. The gaming industry is corrupted to the extreme, and will continue to do so as long as players continue to praise lazy developers. In the pussy that whole shit.",Negative
Intel,"Mirage is PS4 game, Avatar is PS5",Neutral
Intel,"Update: I must've read that wrongly somewhere! Apparently, it uses their own Snowdrop Engine 😳  Well AC: Mirage is nowhere near a next-gen game. Avatars of Pandora, on the other hand, uses Unreal Engine 5 with hardware ray tracing. So it's only natural it needs more horse power. But it'll look 10x better than AC: Mirage. So there's that 😉",Negative
Intel,Timed epic exclusivity? Aww man.,Neutral
Intel,Was guaranteed to happen when up scaling was announced... It's an excuse for less optimisation or to push fidelity more.  I saw a video of this avatar game it got an insane amount of plants visible at all times.,Negative
Intel,You... are.. joking... right..?,Neutral
Intel,Because it’s not coming to steam. It’ll be an epic games exclusive so everyone will pass on it until it hits steam in 18 months. At which point no one will buy an 18 month old game for full price and will wait for a 50% off sale at the minimum.,Negative
Intel,Kind of a messed up opinion.  Maybe games don't look much better than PS4 on your PC but they sure as hell do on mine and that would still be true at a lower resolution and framerate.,Negative
Intel,Ubisoft no longer employs achievements on new titles.  Don't get your hopes up.,Negative
Intel,Try ubisoft achievements :),Neutral
Intel,"It's doing rt at all levels and 4k? Ye... It isn't mainstream yet, heck 1440p is like At 10% market share",Neutral
Intel,"Iam using afmf daily, the input lag introduced by afmf is between 5-12 ms for me.  Doubt any can literally feel this.  I have afmf enabled in all possible games & emulators.  If you think it's bad for you, you don't need to use it",Negative
Intel,Pretty much yeah... Was inevitable that Devs will use up scaling as excuse.  Same for taa being a absolute shoddy aaatleast up scalers do better aa :/ . But even consoles run now heavy fsr like the PS5 uses fsr balanced on WLAN wake on mostly low settings,Negative
Intel,We don't know the settings.  If its like cyberpunk psycho rt on release which 3080 and 3090 easily needed dlss balanced or performance for on 1440p and 4k...    Then I can see the same case in this game for 4080 and similar cards  Also they dont speak about frame gen just fsr2 ( which doesn't exist here because they support fsr3 which does up scaling and frame gen it's not split like dlss 2 & 3),Neutral
Intel,"The fans of avatar, or people that like fantasy games or just games to have fun in.  Luckily games don't need to be for everyone.",Negative
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,"Really comes up to how far the rt and lumen stuff can be pushed, if it goes to cyberpunk psycho rt levels? At release it needed a 3080 and 3090 with dlss.   So it could need top end gpu of this gen if its a similar case",Neutral
Intel,yup that is why I will play 1440 UW native with a 7900XTX.,Neutral
Intel,I guess they are confused with dlss.  Dlss 2= up scaling Dlss3 = frame gen  So they thought its for fsr3 too. ( as in fsr2 up scaling and 3 frame gen)   But fsr3 can do frame gen or not.  Fsr3 is up scaling and frame gen.,Neutral
Intel,"Likely mid range gpu and a suit yells in the background ""30 fps with up scaling is fine""",Neutral
Intel,"Likely the same, they didn't want to make the graph even more annoying, hence why fsr3 and rt and xess are mentioned top right.  They got anyway already confused with fsr3 and fsr2 ( cause 3 is fsr frame gen and up scaling it's not like dlss where 2 is up scaling and 3 is frame gen)",Negative
Intel,Very similar performance I guess,Neutral
Intel,Ye,Neutral
Intel,They've been using it since forever. The difference is that PC GPUs were 10x faster than console GPUs so there was no need for it on PC.,Neutral
Intel,"It mentions dlss, fsr3 and xess support, doubt they limited it to dlss2.",Negative
Intel,"Likely, they were confused because dlss 2 and dlss3.  So they thought its fsr2 and 3 too.",Neutral
Intel,"Your comment has been removed, likely because it contains trollish, antagonistic, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
Intel,Honestly... Maybe fsr3 frame gen can help you but... A 3300 Was already outdated when it came out.,Neutral
Intel,Most likely the upgrade part called FSR 2 and the frame creation part called FSR 3,Neutral
Intel,If they didnt get confused between FSR 3 and 2 then yes fsr 3 roughly doubles fps same for AFMF.,Neutral
Intel,"Nah usually enthusiast in hardware did mean "" max settings no compromises high frames"" but let's say... This changed the last 3 gens sadly.",Negative
Intel,"with AFMF it works , didnt test with FSR3 now.",Neutral
Intel,Reading before raging :),Neutral
Intel,"It is even low quality with 30fps and still needs 5700 ~~XT~~. That is pretty bad :D  *Corrected to 5700, which I think is still a lot for FSR2 at low fullHD, but definitely cheaper GPU.",Negative
Intel,That's with 5700 and 1070.... WHAT exactly are you expecting those cards to do in visually demanding AAA titles in 2023?  People have some crazy expectations. Everyone wants the eye candy but no one wants to pay the performance cost.,Negative
Intel,For 30fps even lol,Neutral
Intel,"Especially recommending FSR.  DLSS at 1080p? Sure why not?   FSR at 1080p? Disgusting, absolutely horrific.  Anyone who downvotes this is a fanboy, end of story. Nvidia's DLSS simply is better at reconstructing an image. At lower resolutions that is SIGNIFICANTLY more important.",Negative
Intel,Or be happy your shitty video card is still supported.,Neutral
Intel,and we still get shat on for saying they are or admitting its a bad thing. corpo fanbois are so cringe,Negative
Intel,Even 4090 users use DLSS because it fixes TAA problems. And has a sharpening slider for easy sharpening adjustment.,Negative
Intel,When did I ever complain about how expensive gpus are. I've always been in the category that they are luxury item and vendor can charge whatever they want for em. So you aint never hear me complain about gpu prices.,Negative
Intel,I can't market a game for running with 120fps 4k with raytracing on on a RTX 4060.  But I can market 400 completely identical missions that take up 150 hours of your lifespan and empty out your soul.,Negative
Intel,Yea I know nothing about optimizations and if it could or couldn't fix this or that. I just want them to tell me exactly what I need to run the game at X resolution a Y preset with 60 fps. If that calls for a 14900k and a 4090 then just say so.,Negative
Intel,"Nah, I think we're really beginning to see a trend here with abusing upscaling for free mileage.  Not that upscaling isn't great but...  Apart from RT, on medium settings, I can't really see    massive improvements on visual quality that would warrant such a massive increase on requirements.  UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  We'll probably see this continuing to be a trend for a while, and then as tech novelty wears out, some devs will come out with more optimized and scalable titles.  With hardware prices being as they are, and with 60fps now being highly requested in consoles as well, it's pretty much a given.",Negative
Intel,"People accepted upscaling with open arms, so now devs know they can rely on people using it. Thus they don't have to care about optimizing for native res anymore. Oh you have a 4k card? Okay well we'll make sure it can run the game at 1080p so you can upscale it.",Neutral
Intel,"No average consumer wanted to come to this point. The market leader started pushing upscaling instead of just making stronger GPU’s and there’s nothing AMD can do to win them, only join them.",Negative
Intel,Turns out TAA is worse than AI TAA with AI upscaling.   Of course devs are gonna use it to cover their own asses. And they get perf boost!  Let's face it. The FUTURE is AI everything. Nobody is gonna talk shit about frame generation and upscaling when it looks perfect each frame.,Negative
Intel,But it does in many ways.,Neutral
Intel,"I don't know who those people are. I play all my games on native, whether I'm using an amd or nvidia card.",Neutral
Intel,"dlss looks better than native with TAA because taa is so bad, but dlaa/fsraa at native >>>> upscaling",Negative
Intel,you think engine devs can pressure AMD and even Nvidia? Come on man. Be real.,Negative
Intel,"Ever notice how multi-GPU vanished the moment DX12 shifted the onus from AMD/Nvidia over to the developers? It shows just how much effort they put into getting SLI/Crossfire working when developers instantly drop them the moment they have to do the bulk of the work (significantly less, I might add) themselves.",Negative
Intel,"If I relabeled ""Medium"" preset to ""Ultra"" preset and changed nothing else and told you that you can run this game at Native 1440P Ultra on a midrange 7800XT card and get 60+FPS, would that be ""optimized"" now?",Neutral
Intel,speaking facts my guy,Neutral
Intel,You shouldn't have downvote dood wtf redditors ?,Neutral
Intel,People with AMD cards dislike upscaling more because FSR sucks ass lol.,Negative
Intel,"Uhm, me btw...",Neutral
Intel,"> Who actually plays games at native these days, if it has upscaling?  I do.",Neutral
Intel,"I won't judge your preferences, but at least using native rendenring isnt the clown show we have here. Native 4k look the same no matter what gpu render it (performances aside). Now with all the quality preset, we can't figure what quality to expect. 4k60fps at balenced preset? What's that crap even supposed to mean as thoses presets vary from title to title, from NVIDIA to amd. And btw, I'm still using fxaa over taa and fsr when i can, don't ask me why.",Negative
Intel,"You're even seeing this on console, where Spiderman 2 now has RT reflections in every mode. Even Unreal 5's Lumen renderer is a software raytracer that can be hardware accelerated.  RT is the future because there are scenarios that rasterized rendering is just not capable of resolving. Screen space reflection artifacts, light leaking, weird glow on indirectly lit models... I find these so distracting and immersion breaking.",Negative
Intel,onto absolutely nothing.,Negative
Intel,"Wait what? No performance improvement this gen? Bullshit, RTX 4090 is literally 2 times faster than RTX 3080 and RTX 3090 is only 10% Faster than the 3080  you do the math. RTX 4080 is 50% Faster than RTX 3080.",Negative
Intel,Or the game performs at 50 fps at 1080p low on 5700 but 50fps is a weird number to target and they can't say 60 so they just say 30fps and call it a day.,Negative
Intel,Not if it's using mesh shaders like Alan wake. In that game the GTX 1650 outperforms the 1080ti.,Neutral
Intel,The game is raytracing only with a ridiculous ammount of foooliage.,Neutral
Intel,"I'm so glad this is finally getting more main stream spotlight.  I do a little bit in Dota 2 and the amount of people that simply have a terrible experience because of a single stick of ram in that title is staggering.  Sure, it's an outlier, but a lot of esports/CPU bound games see similar issues.",Negative
Intel,i wonder when dual rank and dual channel will start being listed too. since dual rank does help gaming,Neutral
Intel,Honestly my 1070 still runs great for what it is. It’s in my sons rig now because it couldn’t keep up with 1440p depending on the game and definitely not 4k. Still a fairly solid 1080p card.,Positive
Intel,False.  guy blocked me lmao,Neutral
Intel,"I thought that was on Linux, though I might be wrong",Neutral
Intel,1070 doesn't have hardware RT though.,Neutral
Intel,According to the technical director there's no non-raytracing mode. I'm assuming that means software ray tracing for the minimum spec.  Source: Skill Up video - https://youtu.be/v91y87R5iDk?t=332 (5m32s),Neutral
Intel,Spoken like someone who has never upscaled 1080p content on a 1440 screen. The pixel ratios are all off. It's fucked.  1440p is perfect for watching old 720p HD content though.,Negative
Intel,Completely agree.,Neutral
Intel,Damn my brain must have been on auto pilot. I'm so used to 2160p + FSR quality neatly equalling 1440p.,Negative
Intel,I see the 5700 and the GTX 1070 on the minimum specs. Are those cards going to be RT capable for this game? (It's 30FPS LOW with FSR but still.),Neutral
Intel,"Sometimes the DigitalFoundry reviewers use their meme powers for good (eg:fighting shader compilation stutter), but their  >""it's better than native!""   memeing about upscaling seems to have gone sideways.",Negative
Intel,"Dude tells truth, go to Nvidia sub, and you fond every second post someone telling that DLSS looks better then native, and every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote.",Negative
Intel,It's purely on the devs for targeting low performance.   We could be getting 240fps at 4K using those upscaling techniques.,Negative
Intel,"Derp, derp.",Neutral
Intel,Sounds like John on Direct Foundry Direct every week.,Neutral
Intel,"""Avatar will be a title that will only appear with ray tracing. But we're developing the game so that the quality and performance will be scalable.""  Seems I was right.",Positive
Intel,">I guess the wonders of everyone jumping on the Swiss knife engine approach ue rather than doing their own engines made for their games.  Facepalm...  Avatar Frontiers of Pandora very literally uses a custom engine, not Unreal Engine.",Neutral
Intel,Yep. This company learned something from all the failure game launches that happened this year. The game companies can't just implement one gpu company's tech and not the others bc then it'll only work on the one gpu that you implemented the tech for and it won't work on the other gpus from other companies. Meaning for example if a game company only implemented fsr it's only going to work on amd gpus and not on Nvidia and Intel gpus and vise versa is true too for both Nvidia and Intel gpus for dlss and xess.,Negative
Intel,Avatar is RT only. There is no non RT mode.,Neutral
Intel,This is mad with snowdrop engine. It's Ubisoft they don't use unreal...,Negative
Intel,"Yep, I missed that. Good call! Makes sense why the specs are a bit higher then but if it is forced on, that's going to be rough on any one with a more mid tier card.",Neutral
Intel,">  Consoles use mostly FSR balanced  heck sometimes they use performance or less, heck a lot of the time even. Especially with RT or performance modes",Neutral
Intel,They have really long way to go. Take for example Lords of the Fallen - UE5's TSR is better than FSR. Take Alan Wake 2 and you get ton of shimmering on the edges. It's also not like nvidia not improving - so while AMD makes a step forward so will do nvidia..   There's also question how much they improve without dedicated accelerators that both Intel and nvidia is using. Intel jumped late to the party and they're already ahead - which just shows how much AMD cares. It's rally embarrassing considering both XBOX and PlayStation are also relying on FSR - ant that's bigger market than what nvidia has.,Neutral
Intel,I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?  I mean the facts are the facts. I need to play on lower settings till we get a 5070 or 8800xt.,Neutral
Intel,"It uses their own in-house engine, Snowdrop. The last game this developer did was The Division II which was excellent.",Neutral
Intel,"Took a look at some gameplay on YouTube, and it does look nice enough, though water looked like ass, and as mentioned already, it's an in-house engine (which is nice to see TBH).",Positive
Intel,"Next gen seems promising so far, low native rez hidden by upscaling, 60 FPS target on +500$ GPUs, traversal stuttering thanks to the glorious UE5. But hey, we have great reflections on puddles so at least we can take good looking screenshots.",Positive
Intel,You will never see a high production game from Ubisoft ever on the Steam store ever again at initial launch.  They are the anti-christ of the gaming industry seriously.,Negative
Intel,Consoles are the baseline in most multiplat games and they've been using upscaling since forever. The real reason is just that we don't have GPUs anymore that are 10x faster than consoles like last gen. You can't bruteforce high resolutions in games that run at like 720p on consoles with GPUs that are only 2-3x faster.  If upscaling didn't exist on PC the minimum here would just be 720p at the minimum and 1080p-1440p at the max.,Negative
Intel,Next-gen doesn't mean that current gen can run it well. It means it's so far ahead that most curren-gen hardware can't run it properly + top-end hardware can only barely run it well.,Negative
Intel,"Exactly. These paid exclusive games launches only make me care less whenever they actually release.  It's not just Epic, Meta has done it with several games (Moss Book 2) and when a proper looking pcvr port comes a year later it sells bad because people forgot about it.",Negative
Intel,"Is that with significant RT? I assumed it was ""normal"" ultra settings.  Still I can play Far Cry 6 and RE4 with AMD sponsored RT at 4k native with 100fps. This game will have to look amazing to justify what... a quarter of that performance?",Neutral
Intel,"You are correct. I overlooked the part where, under Ultra settings, it said that was with FSR2. Now it doesn't seem as bad. I mean, it's still not good because it's rendering somewhere between 1080p and 1440p, but at least it's not 30 fps without Frame Gen/60 fps with it like I thought.",Neutral
Intel,"It might be, AFAIK neither Nvidia nor AMD still support their implementation of it in their drivers.  But I've tried it on a few games and it is pretty cool to me, much stronger than almost any modern 3D movie (IMO why 3D has gone into hibernation), and I gather that Cameron designed the franchise around 3D.  Plus the Avatar 1 game was in 3D.  FWIW.",Positive
Intel,This is what I am thinking too.,Neutral
Intel,"You say that, but they specifically mentioned FSR3, and just DLSS, so i have a feeling DLSS3 will have to go via a mod again.",Neutral
Intel,"i have a believe that even 4C8T will survive for the decade, like the case of that 4790K. But turns out these newer UE5 games are wrecking even newish CPU like the 3600. i mean cmon it's still a 4 years CPU old which i think it's not old yet",Neutral
Intel,Let's hope so. 30fps is far from recommended for FSR 3,Neutral
Intel,Yeah I heard it works with that hoping it works with fsr3 now,Positive
Intel,"I've noticed that for some reason newer games look blurry at 1080 even without any kind of upscaling and I'm not sure why, older game look much sharper at 1080p",Negative
Intel,"Just lazy optimisation, that’s pretty much a PS5 equivalent GPU and you can be damn sure that the console version won’t be 1080P 30fps with FSR",Negative
Intel,"No it says 5700. NOT 5700XT, and yes there is a difference. Quite literally the difference between a 1070 and 1080ti. Or a 2070 and 2080.",Neutral
Intel,no no my 7 year old card needs to be able to play at high settings. so does my low end card from today. gotta be high or its just bad,Negative
Intel,Crazy expectation? A game released in 2023 should be able to run at 1080p low 30fps on a low end card like rx6500xt (lowest end modern card you can buy now).  Optimization matters.,Neutral
Intel,"Yup, these cards came out around the time of ps4 I think..that's a long time ago in the graphical quality of games we are seeing.  People should be happy to a point that those cards can even run some of these games if upscaling helps them run that's a bonus as it means they don't need to upgrade if those graphics settings are acceptable to them.",Positive
Intel,"I have a 5700 and it does better at 1080p on these games than what is normally posted. Now it does need quality fsr but its not the worst thing, anything lower than quality is. Bottom line is these are not 100% correct and never have been.  I have zero interest in this game but the specs seem fine even though I wish upscaling wasn't what was relied on for optimization.",Negative
Intel,"well you dont know what they mean by 30 fps. It could be de 1% lows or the average, we dont know. The easy assumption is that it will be 30 fps average. but that's crazy that a game at 1080p low cant be run by a 5700. It better be a new Crysis.  edit: apparently it uses raytracing only for lighting which would explain the hows and why it crushes the framerate so bad especially on a gpu that doesnt have hardware raytracing like the 5700.",Negative
Intel,"Or in the case of games like Alan Wake 2, you will get unplayable fps if you try to running it at 4k native max settings. Even with a 4090, you will get a glorious 20fps.",Negative
Intel,Assassins creed origins and odyssey side quests/collectibles oh my god,Neutral
Intel,Yeah it doesn't give us a actual gauge to understand what we need for set native parameters. I mentioned optimization out of concern that they rely too much on upscaling and it seems like it's the lazy way out of something. It's unrealistic that a usable gaming experience be in a top tier hardware setup that costs $2k+ and top tier hardware of just the last generation are just barely cutting it anymore.,Negative
Intel,I appreciate your insights and opinions. Thank you.,Positive
Intel,">UE 5 is still pretty new, so it might be a problem of most devs not having the knowledge and practice to optimize new tech, or the new tech being used is simply too resource heavy for marginal graphics quality increase.  Because they market how ""good"" it looks, not how ""well"" it runs. There's a big influence from marketing perspective to place high focus on ""eye candy"", it's what sells.  The how ""well"" it runs is usually restricted to the small % of us gaming nerds. ![gif](emote|free_emotes_pack|flip_out)",Neutral
Intel,console games started upscaling way before PCs .,Negative
Intel,This sub is rife with crazy conspiracy theories. I don't think it was this insane a couple years back.,Negative
Intel,you forgot who owns one of the most popular engines out there?,Neutral
Intel,"I guess technically it would count but then everyone would flame it for looking bad. But seriouly, what happened when Ultra settings ran real smooth on top of the line hardware? Gone those times.",Negative
Intel,downvoted by devs lol,Neutral
Intel,> RTX 4080 is 50% Faster than RTX 3080.  The msrp is 71% higher too.,Positive
Intel,"Yeah at 1200 bucks and above you got one, that's true but many cards below didn't offer much.",Neutral
Intel,that's stoopid but it's somewhat more understandable. Why a 5700 would struggle so much since it doesnt have hardware raytracing,Negative
Intel,Yep it's like a 10/15% performance uplift in some games but I'd guess they would prefer to keep the requirements sheets somewhat simple.,Neutral
Intel,Why is there a dialog message about unsupported hardware when you try and run a 390X?,Negative
Intel,It can do software based RT just like every other modern GPU out there.,Neutral
Intel,Well then no wonder rx 5700 can't manage 30 fps lol,Neutral
Intel,"Wait so we are finally getting games made only with ray tracing in mind? This is actually great, 2023 is officially the year the new generation started",Positive
Intel,Avatars uses a Lumen like software RT solution.,Neutral
Intel,"yeah they never finished the sentence for some reason. The full sentences is ""its better than native, when native has forced taa!"" which is why dlaa/fsraa at native look so much better than the upscale versions. normal basic TAA is just so bad, that even upscales not using it look better than native using it",Negative
Intel,"The AA side of things might be better, but I swear no matter the resolution textures become a blurry mess even with Quality mode etc, and it irks me that most people don't seem to notice that.",Negative
Intel,Some people seem to have very low sensitivity to added blur. See also *depth of field blur* and *motion blur* being on by default in every game. It's funny how much people rage against FXAA/DoF/Motion blur but ignore DLSS.  Of course only one is tied to someone's purchasing decision :),Negative
Intel,"Yeah, thatsl happened.",Neutral
Intel,">every time I tell them that I can tell the difference between native and upscaled and it's not better then natibe they call me a liar and downvote  Well, yes, you are using shitty FSR which is literally unplayable no matter resolution instead of godly DLSS that is literally better than native even at 720p/performance. The only way you could have seen the ever unmatched DLSS for PC kings instead of the plebean FSR is through a compressed youtube video so your opinion is literally invalid. I hope I am being sarcastic enough.",Negative
Intel,"The problem is some (if not most) games have such a bad TAA it might as well be true, and if often cannot but turned off.",Negative
Intel,"""We could"" but not profitable. Just let imagine these games does not exist in our reality.",Negative
Intel,You didn't understand. It's another Swiss knife engine,Neutral
Intel,"It is indeed not a great situation. Things like this shouldn't be forced on, and it leaves me feeling like they haven't given the devs enough time to optimize the settings.",Negative
Intel,It works because most consoles players are noobs they don't care about the tech. They just want to sit on the couch and play. I'm hoping we see them start working more on FSR3 and upgrading older games to it. And you are right nvidia will keep moving forward but so will AMD. They may never catch them since they have a head start.,Positive
Intel,AMD just released drivers for Alan Wake 2 that fixed the flickering while using FSR.,Neutral
Intel,The PS5 has a 6700.,Neutral
Intel,">I thought the PS5’s graphics processing was a tad above the 5700XT but far below the 6000 or 30 series?   Not exactly. It's a custom quasi-RDNA2 design (has the improved RDNA2 compute units, but it's missing a few features such as fully-DX12-compliant mesh shaders and VRS-capable ROPs).  For most purposes, it's pretty comparable to a 6700 (non-XT), with the same number of CUs (36), and similar clockspeeds.",Neutral
Intel,"Oh, wow. I didn't know that. But it'll likely be on a similar level as UE5, right?",Positive
Intel,Next gen DOES NOT MEAN that your CURRENT GEN hardware can run it well for sure... Why don't people get that?,Negative
Intel,That reminds me to grab moss 1 and 2. I just got a quest 3.   Agreed though. I’ve been waiting for kingdom hearts 3 to come to steam for so long that I’m basically going to skip it at this point. It’s still stuck on the epic game store.,Neutral
Intel,Far cry 6 and re4 are older and or lower demanding games.  And yes any Mode here does either Hardware or Software rt from. Min to max.,Neutral
Intel,It's a 4 year old cpu yes... But how it was built (4c) was like 10 12 years now old.  6 cores are since a few years already base and more and more 8 cores are standard.,Neutral
Intel,TAA,Neutral
Intel,Temporal anti aliasing.,Neutral
Intel,"TAA is the devil. it forces a blur on every frame, and only gets worse as the resolution goes lower. its why fsr, xess, and dlss can look better than """"native"""" because it doesnt use the horrid plain taa solution so many games do. and why dlaa, and fsraa are so important and need to replace taa completely",Negative
Intel,"Ahh, welcome to r/FuckTAA",Neutral
Intel,Even 4K looks blurry with some implementations of TAA,Negative
Intel,they're giving you the bare minimum until your upgrade!,Neutral
Intel,Because they are built with upscaling in mind so they have a blurry looks intentionally as it works best with aggressive upscaling.,Neutral
Intel,"Not even close, you're completely out of your mind. The 5700xt is 9% faster than the 5700. The 1080ti is **32%** faster than a 1070 and the 2080 is **19%** faster than the 2070.   Source from techpowerup: [https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339](https://www.techpowerup.com/gpu-specs/radeon-rx-5700-xt.c3339)",Negative
Intel,"Well the difference is only 256 shaders more on the 5700xt. The 5700xt has 2560 shaders and the 5700 2304. That's 11% more. That's not that much.  The 1080ti has 86% more cuda cores than the 1070, as well as more and faster memory. I would not compare now so haha",Neutral
Intel,"The 5700 and 5700xt is more like a 1070 and 1070ti situation. It's not that big a gap, and you can close it even tighter because the majority of RX 5700 cards can be flashed to a 5700xt bios tightening the gap farther.",Neutral
Intel,"The 1070 was released in 2016. Its already the end of 2023. That card belong to ps4 era. Wake up.   People in this sub are delusional, really.",Negative
Intel,Those are some dumbass expectations. Maybe you should correct them instead of blaming the developers for it.,Negative
Intel,"The 6500XT offers $200 2016 performance. In fact its often worse because of how gimped it is interface wise. Its one of the worst products released in recent years.  Its not the game developers fault AMD pretty much rebadged the RX 480 4 times and they can't be expected to sink their optimization to that level.  RX 480, RX580, RX 5500XT and RX6500XT are virtualy the same product performance wise. RX 5500XT is the most consistent one of the lot offering a somewhat more modern arhitecture in its 8GB variant. The 6500XT on the other hand while on a newer arhitecture is an unmitigated disaster that is on par at best and sinks even below the RX580 when memory, bus and PCIE interface constrained.  The console baseline is around 6700XT/2070 Super/3060ti level. People better get used to that as its gonna become or already has become the norm for 1080p high settings gaming. The 5700 is upwards of 30% slower by default while also having less hardware features. Its gonna become the minimum norm going forward alongside the 2060's of your time.",Negative
Intel,"the 1070 was 2016, the 5700 was 2019. While both not new its not 10 years old like the PS4 (2013)",Neutral
Intel,This is literally only true if you use path tracing. I think you're trying to insinuate the game is unoptimized when it just scales really high with visual features if your hardware supports it. It straight up runs great for the visual features it uses.,Neutral
Intel,"That was exclusive to console releases, no? And never to the point of replacing AA sharpening and rendering the game from 720p…",Neutral
Intel,No I havent. AMD is bigger than Epic.,Negative
Intel,"Right, you'll have different complaints but complaints none the less even though the new Ultra might still look slightly better prior gen games and offer high FPS but people would complain that the graphics are not advancing fast enough, they look the same etc etc.  Yah those days are likely gone but it's hard to say if it's because the game is unoptimized or if the dev is really pushing the limits of what they can do graphically and use upscaling as a buffer because what they have in mind, or what they intend to show is too taxing for current hardware. We won't really know until a game is released  So screaming game is ""unoptimized game, lazy devs"" seems to have just become the norm before anyone's even played it. This is why I've never in my life ever pre-ordered a game. I still don't understand why people do it (esp now that games are digital) but lot of people do. Then again I've never really paid for a ""skin"" or some other visual add-on so maybe that's what attracts people.  I'm not saying games aren't unoptimized, those do exist, they offer nothing more but run like crap. We know what those game are, it's not hard to tell. But then you do have games like AW2, which before release, we were hearing the same exact complaints and it looks amazing and the minimum requirements were on the conservative side. The game looks better at ""medium"" than high/ultra presets of some games, heck you can even argue that at some low settings it looks better than some games at medium/high presets. The presets are all relative anyways. There's no universal settings that define what each preset represents.  I just don't get the outrage on these minimum spec sheets compared to what the game is showing graphically. At least wait until it releases and we have some hard data.",Negative
Intel,"I'm also a dev dood sometimes it's literally a ""foutage de geugle"" I had a friend who talk about how shit devs in EA (the people who's making the engine) code and doesn't optimise",Negative
Intel,"The commenter I replied to said there wasn't a big generational improvement for GPUs, while I said there was. But I agree that 4080 is expensive for what it offers.",Neutral
Intel,It was the same price as a 3090 at launch and offers 90% more performance.  It was definitely a generational leap.,Positive
Intel,It's software ray tracing which isn't accelerated by hardware.,Neutral
Intel,and i guess it can be hard to tell if ram is dual or single rank with just two sticks of it too.,Neutral
Intel,>might be the reason why the AMD GPUs suffer more compared to their Nvidia counterparts.  I was pointing out that 1070 doesn't have hardware RT either.,Negative
Intel,So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?,Negative
Intel,"Why not make it toggleable though, you can choose other graphics settings so it makes no sense. Most people don't play games with RT enabled",Negative
Intel,Still kinda weird that they put the RX 5700/A750 on the same ballpark as the 1070... Being that the newer GPUs actually perform closer to a 1080(Ti).,Neutral
Intel,Sounds like improperly configured negative LOD bias. If you have Nvidia you can fix it with nvinspector. On AMD there's nothing you can do IIRC if Devs mess it up.,Negative
Intel,"Exactly what I'm telling them all the time, that I can see difference in textures, in hair, fences, powerlines etc. And every time I get downvoted by the army of ""but it looks better then native"". I feel like majority of that sub needs to go get a prescription for new glasses",Negative
Intel,"So if you stopped measuring who's bigger, why do you think I have no access to DLSS? Tested on rig with 4080 on a C2 42"" screen. Once you stop thinking with the tip of your head where probably Nvidia's logo is tattooed or maybe Jensen's face, who knows, you could come to realization that one person have access to more hardware then only his personal rig.  Do I say that FSR is better then DLSS? No! Both of them have their issues, but none of them is better then native. Keep on fanboying mate, instead of providing normal convoy as grownups",Neutral
Intel,I don't think YOU understand what you're talking about.  Did you seriously expect Snowdrop to be something used in like one game and then discarded completely?,Negative
Intel,"The thing is: creating good looking lightning with RT is much easier than with prebaked illumination. I can fully understand, why developers of new games are using RT as the basis of their game.",Positive
Intel,Minimum requirements go up as games get/look better. They probably made this game with the series s as the minimum. Low settings probably looks better than high in most games.   Rt for the last few years has been added on top of already complete games. Now games are being made with rt as the standard and they would have to manually do a ton of extra work to support 7+ year old hardware. It's the same as having to drop support for last gen consoles to be able to deliver a better game.,Neutral
Intel,"And still - superior upscaling makes it worth paying extra, RT on higher end is worth paying extra. AMD even this gen doesn't undercut price by enough, especially in EU where power efficiency also play a role. For example, assuming I'm gonna use GPU for 2 gens, then I save ~90-100€ with my usage just on electricity alone. I know it's over period of 4 years, but still it's price difference that will recoup over time, even if I have to pay extra upfront. AMD is simply no longer a value proposition.",Neutral
Intel,"They did not, lol - that fixes completely different thing, not the FSR caused shimmering edges.",Negative
Intel,Both excellent 👌 Down the Rabbit Hole was another solid one.,Positive
Intel,"RE4 came out this year.  Anyway the difference between us is I don't think any lighting effect is worth a $1,000 GPU using 1080p resolution and 60fps, but I know many disagree. That's life.",Neutral
Intel,"3600 and the 3300x was built with the TSMC 7nm if i recall, that was 2018 cutting edge tech. and somehow in gaming it's already obsolete. i was planning to replace mine with 5700x  but i cancel it because it's still very much capable for what i do now. but now after UE5 i think i'll reconsider it again, maybe even as far as 5800X3D",Neutral
Intel,It is crazy that I see everyone saying that modern games look blurry. I've known for years taa does this but it seems like I see more people than ever dissatisfied by it.,Negative
Intel,I have to turn it off in borderlands 3. Ugh.,Neutral
Intel,r/FuckTAA,Neutral
Intel,"My friend nicknamed it after his eyesight. When he wears glasses he has 20/20, but when he doesn’t he says that he sees the image about the same as without glasses",Neutral
Intel,"Out of curiosity what resolution do you usually game at and at what distance? I find taa usually looks pretty good in modern games, but I'm also playing on a TV (48 inches but I'm not at monitor distance anymore). When I played on my 1920x1200 monitor I HATED TAA, well, not as much as fxaa, but moreso than smaa, and I missed msaa or SGSSAA with a passion. Now on the TV at 4k (lol, usually not on a 6700xt), 1440p, or even 1080p I find TAA can do a pretty decent job paired with varying levels of the Radeon sharpness adjustment in the adrenaline drivers. But again, probably due to viewing distance.  Looks leagues better than FXAA and I prefer it to SMAA now. But again, I'm sitting further away and tweaking sharpening to my liking. FSR2 seems to be a wash most of the time but the performance gain is worth it.",Positive
Intel,You dumbass do understand that even the console version of the recent games are total mess? Alan wake 2 have to run on reduced settings to even reach 30fps on xbox/PS. Callisto protocol used to have frametime issue on consoles. Console ports still targets 30fps on quality settings with upscaling while target should be 60fps with 120fps for performance mode.,Negative
Intel,"Yes, seems you are correct, it was the ps4 pro that came out in 2016, still, I wouldn't expect that to be capable of running some of the games we are seeing now without some serious compromises.",Neutral
Intel,I never said that Alan Wake 2 is unoptimised. I just said that even a 4090 can’t max out the game at native 4K resolution.,Negative
Intel,epic is tencent...,Neutral
Intel,"In terms of absolute numbers sure, 3090 always had a stupid price tho for being 10% faster than a 3080.   In terms of performance per dollar the 4090 even at 2x 3080 performance was worse value since you pay 1600 not 1400.",Negative
Intel,"Yes, you either have to get the spec sheet of the RAM modules (That even then it may not specify if it's a single/dual rank module) or test it in a system and use HWinfo/CPU-Z. Not as straightforward as dual channel that's mostly just using two identical modules.",Neutral
Intel,You don't need RT hardware to do software RT. That's what I'm saying.,Neutral
Intel,"For this particular game, there is a software fallback. Also, even if we did get a game that mandated hardware support for RT they'd still likely build it with console in mind so even RDNA2 GPUs shouldn't have issues",Neutral
Intel,"The first RT/DLSS capable cards are 5 years old now, way older than the current consoles. You are definitely not being left behind if you're only outdated by 1 gen lol",Neutral
Intel,I mean the last non ray tracing cards were released 4 years ago (1650/60 and 5000s from AMD),Neutral
Intel,Games running poorly on computers weaker than current gen consoles isn't exactly anything new.,Negative
Intel,"No, you have multiple generations of games from prior to raytracing to enjoy.  Games shouldn't be held back forever just because someone wants to game on a GeForce2 MX400.",Negative
Intel,You are stuck playing older games. Unless someone can find a way to disable it with a mod.,Negative
Intel,> So what happens to people with older cards not even ancient but a gen old. Are we officially fucked?  A gen old is second gen RT e.g. rtx 3000.  We're halfway through the third RT/matrix generation right now.,Negative
Intel,Yes hah,Neutral
Intel,"The 3090 or 6950xt will very likely run this game extremely well, they're a gen old.",Positive
Intel,"For the AMD GPU, it's probably because minimum will be the max it will support. Medium probably requires Hardware RT.  For Intel, I don’t know. Perhaps it signal not so great optimization for Intel GPUs...",Neutral
Intel,DLSS is very strong in thin lines like fences & hair. Much better than most TAA solutions. Not sure about FSR in general.  An example: https://pbs.twimg.com/media/E6erA6BVkAAuf_M?format=jpg&name=large  Only DLSS resolves the lines properly.,Positive
Intel,"I mean, it's not that black and white. Quality upscaling to 1080 will obviously be more noticeable than 4k. Maybe you *can* tell the difference between 4k native and dlss quality, but people aren't gonna believe you unless you prove it with some kind of blind test, but I doubt you'd be willing to go out of your way to do that.  Even if you could, the benefits of better aa might outweight some negligible drop in textures or whatever it is upscaling reduces. Though that's irrelevant when DLAA or NAA are options too, I guess.",Neutral
Intel,"Test, which one looks better, explain.  https://imgsli.com/MjE3MzEy",Neutral
Intel,Oh man the hair... its so crazy how much upscaling kills the hair..,Negative
Intel,i mean they already arent the smartest bulbs considering they went team green.,Neutral
Intel,Reading comrehension dude.,Neutral
Intel,"I do, but thanks for your interest.",Positive
Intel,That will depend on the user. Power is cheap here in canada so none of thar factors into my buying decisions. And rt will depends on the games you play also not everyone plays tripple a games.,Neutral
Intel,"You need to get 23.20.17.05.  Fixed Issues Intermittent flicker may be observed on some textures while playing Alan Wake 2.  Fixed for most people, also has a nice performance boost of about 10fps from what I'm seeing.  That being said I always prefer native resolution unless absolutely necessary. So far only had to use it in Martha is Dead.",Positive
Intel,Nice. It’s on the list. Thanks man,Positive
Intel,Yes.... 7nm was but 4 cores the actual important thing was obsolete like 10 or 12 years ago.,Neutral
Intel,Probably because TAA is (unfortunately) more prevalent than it ever was.,Negative
Intel,"Nah, most people are FINE with it. Its people on FuckTAA that are mad.  The thing is, you have sharpening filters today. You can adjust the sharpening for TAA. You can make it look oversharpened if you want. There's driver level sharpening for both AMD and NVIDIA. Both also have software sharpening filters.  And DLSS and FSR bot have sharpening sliders.  There's 3 ways to adjust how blurry and how sharp the game looks for everyone. It's a non issue and even Digital Foundry acknowledges the benefits of TAA for game devs vs things like the blur. At the end of the day, these games are looking great...in this era, with exceptions. Not all games are blur. The whole blur thing was like in the past 4 years since upscalers started and TAA became the norm. But with all the sharpening options, and with devs actually used to making TAA not so soft looking, things are definitely getting a lot better for TAA.",Neutral
Intel,Oh my bad if I read that wrong,Negative
Intel,And I'm saying 1070 doesn't have hardware RT so it's just as gimped as AMD cards when doing RT.,Negative
Intel,Still pretty annoying nonetheless. Until it's stable no way should it be forced upon a game when someone's willing to run it normally at native.,Negative
Intel,"and the 2080ti is still winning, best purchase in gpu i made in a long time.",Positive
Intel,imagine being mad that 4 year old cards arent high end,Neutral
Intel,I'm on a 6700XT. 🙇. I just feel very shocked that I would probably need a gpu upgrade soon. With the demands of new games. Also 1440p ruined me.,Negative
Intel,"Technically makes sense, but they can just make RT something you can toggle between Software/Hardware and they also could just add the Vega 56 in the requirement sheet being that was AMD's competitor to the 1070 or they could've added the GTX 1080(Ti)/RTX 2060(Super) into the sheet.  On Intel's side they only have the A580 as the closest GPU to a 1070, even when it's too similar to the A750 performance wise :/",Neutral
Intel,"It is that black and white for me. I can immediately see the added image softness from upscaling, and it's especially prevalent with DLSS 2.5+ unless you enable a sharpening filter. DLAA also has weird issues like blurring textures (almost like it reverts to initial mipmap) while moving, usually at mid-distance from camera. It's too distracting and I have to go back to regular TAA. Immediately goes away.  1440p to 2160p is better than 720p to 1080p, but it's still noticeable. Detail is immediately lost as lower resolution image is scaled to higher resolution. Is it terrible? No. Actually changing monitor resolution outside of native and having all UI elements scaled looks terrible. That was what we used to do and upscaling is much better than that alternative. LCD panels need to stay at native signal resolution and GPU should scale render resolutions.",Negative
Intel,"And you look at static image all the time while gaming? It's different story when movement is involved, that's where you find all the artefacts you get. Static image doesn't tell a thing when it comes to gaming",Neutral
Intel,"Yes, and that's driving me crazy. I really couldn't stand it in Cyberpunk (in other titles it's either hit or miss), that I play it in native 4k instead.",Negative
Intel,"Wouldn't phrase it like that tough. I had that choice too in January, and actually by going 4070Ti instead of 7900XT I would be able to play Cyberpunk in higher fidelity and better frames then on mine, because AMD cooked FSR3.0 almost a year in the oven, and God knows how long it will take for CDPR to add it to the game, if ever considering that active development on game is ended and they are on bug fixing sprint right now. Also AMD needs to fix FSR in general, because how it works with smaller details like hair and fences is awful. But don't get me wrong, I love my card, just feel I git sold on features that would come handy right kow instead of when I don't need them anymore",Neutral
Intel,"Sorry mate, guess I need to read comments early morning instead of late night, just saw that last bit. My apologies. I'm just tired of upscalare wars whatever sub I go, and green team fanboys screaming that ""FSR is shit, DLSS is better then native"" and then throwing around static images for comparison",Negative
Intel,"It's for textures, not object edges from FSR use, lol. Two completely different things",Neutral
Intel,"people echoing the same shit like ""ooh 4C is obsolete, you need to upgrade that geriatric cpu"". 4C is still solid for gaming (ofc no UE5) and literally anything beside doing compute intensive professional workloads.",Negative
Intel,"Sharpening makes the image less blurry, but it doesn't recover the texture detail lost from TAA blur.",Negative
Intel,"""Even Nvidia foundry""   \---   they are like chief proponents of blur-tech and they don't care about how sharp and easy-to-see the image is. TAA flattens the image so you can't see which objects are far and which are near, you have to strain your eyes and mind to determine that which is unbearable to many. Just give us option to disable it even if it breaks some effects (which is just lazy thing to do on the side of devs)",Negative
Intel,"Sharpening fixes blur, not clearity. That is TAA destroys+add significant ghosting no matter the implementation.      Sharpening looks was worse than the normal cleairy of games, it's not the same thing at all.      >Its people on FuckTAA that are mad.  We are mad because it being forced 1 and 2 it's being used to hide lazy effects that break without temporal frame smearing. People are now punished for turning off forced frame smearing.",Negative
Intel,And I'm saying that maybe the GTX 1070 is better at even software RT compared to AMD cards with no RT hardware. GTX 10 series card did get RTX support through CUDA even though they were horribly bad at it. Older AMD cards with no RT hardware didn't get any support like that from AMD.,Neutral
Intel,"You're not forced to use upscaling though. Unless you replied to the wrong person this is about ray tracing, which can be run at native just fine. If your GPU can't handle RT at native res, then just use upscaling.",Neutral
Intel,The minimum requirements for this game includes de 1070: a 7-year-old gpu from 2016.,Neutral
Intel,"I'm not really shocked, and was out there telling people not to buy those cards on day 1 in large part because of their deficiencies in RT performance and complete lack of matrix accelerators (which Nvidia and Intel have both made a large part of their architecture and got shockingly good results with - DLSS and matrix-XeSS are twice as good as FSR)",Negative
Intel,5700 was probably the lowest AMD card they had to test with.,Neutral
Intel,"It also looks better in gameplay, less ghosting, less shimmering, less aliasing.   So which one looks better? You clearly see the difference in textures/fences/powerline, right?  I even made it easier for you, both images are 1080p output.",Positive
Intel,I personally didn't try DLSS even once but FSR 2.0 does have shimmering I notice to various degrees. It was very noticable on fur in Ratchet & Clank cutscenes for example. Overall I am just happy my 5700xt could still keep up with 1440p.,Neutral
Intel,Hmm yeah whatever you want to believe and let's you sleep.  Have a nice day.,Positive
Intel,AMD CAS is aimed to restore detail,Neutral
Intel,"They aren’t chief proponents of anything, TAAU has been popular for well over a decade at this point and it’s the default approach for consoles (checkerboarding has moire and aliasing issues) and digital foundry simply recognizes this is the reality of how the console world has settled on doing graphics.",Neutral
Intel,The problem with Nvidia is they are massively overpriced. One 4080 would probably cost the same as my whole build or more.,Negative
Intel,"Yes I see which one looks better, and as I said before it's a static image. While I didn't use upscaling in this same exact example, I can go by my own experience and tell that it doesn't look better then native let's say in same Cyberpunk or any other game. And I will beleive my own eyes on my own screen then static images tossed around. TBH, in my opinion it all depends on screen and screen size too, as to me that difference between upscaled and native was less noticible on my old 27"" 1440p screen then it is now on 42"" 4k. DLAA tough, that's another story",Neutral
Intel,"I tried both, and neither of them are better then native as people tend to claim. In my testing FSR have big problem with smaller details like hair, fur, fences, powerlines and as you mentioned, you get severe shimmering effect. I really hope AMD will keep on working on it, an try to get away from that. But DLSS also adds shimmer, it's just less visible then in FSR (atleast in my testing on Cyberpunk pre 2.0), and whatever upscaling method I've used I also could see that image is upscaled (lower res textures here and there etc.). I like idea being upscaling tech, it helps prolong life of GPUs, and for more powerful GPUs to run higher settings with more fidelity, but there's still work to do on them as they aren't perfect yet.",Negative
Intel,Yeah but a 3060ti didn't,Neutral
Intel,"In Cyberpunk you are not comparing upscaling vs no upscaling at the same settings though. You are comparing low RT native vs path traced upscaling and at 4K, the decission there is absolutely clear. On my 4k OLED screen, upscaled PT is so clearly the way to go, there is no competition.",Neutral
Intel,"The gameplay looks exactly like the image + your typical TAA/DLSS smearing. And as I said, in my example DLSS also looks better in gameplay, because as you see... native has a ton of aliasing which is super noticeable while playing (shimmering). There are so many objective tests, which show why its most of the time better than native. (up res to 80% instead of 67% and its always way better, even at 1080p)   Well when did you try DLSS the last time? Newest versions are miles ahead of the ""old"" 2.0 version and pretty much always ahead of standard TAA.  DLSS is essentially a way better TAA solution + AI to fight TAAs issues + lower res. Its so much better than your standard TAA, that youre able to reduce the res and it will still look better in movement. Think about it like codecs for streaming... there are codecs which need a way lower bitrate for the same quality than others. (av1)  Everything graphics related is not rendered at full res anyway (shadows/lighting/lod etc...), there is no line between native and upscaling. Talking about native is nonesense, just image quality counts and thats were DLSS or even FSR 2 (some games) gets better and better. There is a reason why consoles use it... it looks better than without it.",Positive
Intel,Confirmed Can it run CP2077 4k is the new Can it run Crysis,Neutral
Intel,It'll be crazy seeing this chart in 5-10 years with new gpu's pushing 60-120 fps with no problem.,Positive
Intel,In 1440p with 7900 xtx with fsr 2.1 quality it doesn’t even get 30fps,Negative
Intel,"Makes sense. Is almost full path tracing, it’s insane it’s even running.",Positive
Intel,good. this is how gaming should be. something to go back to before the next crysis shows its teeth,Positive
Intel,3080 falling behind a 3060? what is this data?,Neutral
Intel,wake me up when we have a card that can run this at 40 without needing its own psu.,Neutral
Intel,5090 here I come!,Neutral
Intel,"Playing the game maxed out with path tracing, FG, RR and DLSS set to balanced at 1440p. Over 100fps 90% of the time. Incredible experience.  *With a 4070 ti and 13600k",Positive
Intel,"I mean, Path tracing is to Ray tracing, what Ray tracing is too rasterization.",Neutral
Intel,When Cyberpunk first came out the 3090 only got 20 fps in RT Psycho mode.  https://tpucdn.com/review/nvidia-geforce-rtx-4090-founders-edition/images/cyberpunk-2077-rt-3840-2160.png  Still does  Fast forward just one gem and you don't see anyone saying its demanding with many able to get RT Psycho on thier cards as new cards got faster.  Give it 2 gens and you are going to get 4k60 here.  Gpu will improve and get faster.,Neutral
Intel,"Look at that, my 6700XT is just 19fps slower than the RTX 4090 in this title.",Negative
Intel,It's a good thing nobody has to actually play it native.,Positive
Intel,Well good thing literally nobody is doing that…,Negative
Intel,The future is not in native resolution so this is really pointless information.  Cyberpunk looks absolutelt mindblowlingy insane with all the extra graphical bells and whistles it has gotten over the years and with Nvidia’s technology it runs so damn smooth as well.,Positive
Intel,4.3fps lol. No amount of upscaling is going to fix that and make it playable. People were saying the 7900xtx had 3090ti levels of RT when it launched. A 4060 ti is 50% faster then it.,Negative
Intel,That's actually pretty amazing for any GPU to get a metric in frames per second instead of minuites per frame.,Positive
Intel,"Meh I get like 90+ fps with everything cranked with RR, FG and DLSS(Balanced) toggled on with my 4090 @ 4k.  Path tracing does introduce ghosting which is annoying buts not really noticeable most times but at the same time with RR enabled it removes shimmering on 99% of objects that is normally introduced with DLSS so I am willing to compromise.   Honestly as someone who used to have a 7900XTX I am disappointed with AMD its clear that AI tech in gaming is the way forward and they just seem so far behind Nvidia now and even Intel(going by some preview stuff). FSR is just not even comparable anymore.",Negative
Intel,Who cares when it looks worse than with dlss and ray reconstruction on top of running a lot worse? Native res 4k is pointless.,Negative
Intel,"Well the upscaling in this game is really good, DLSS with ray reconstructions AI accelerated denoiser provides better RT effects than the game at native with its native Denoiser.  Also Path tracing scales perfectly with resolution so upscaling provides massive gains. using DLSS quality doubles performance to 40fps, and dlss balanced gives 60fps on average, performance about 70-80 or 4x native 4K, that includes the 10-15% performance gain RR gives as well over the native denoiser as well.  I've been playing with about 60fps on average 50-70. with DLSS Balanced and RR and its been amazing. I don't like frame gen tho since it causes VRR flickers on my screen",Positive
Intel,"Running Ray Tracing or Path Tracing without DLSS or Ray Reconstruction is like intentionally going into a battlefield without any gear whatsoever, it's absolutely pointless and suicidal, what we can clearly see here though is top of the line 7900 XTX losing against already mediocre mid-range 4060 Ti by over 50%, which is just beyond embarrassing for AMD Radeon.  All this says to me is AMD Radeon need to get their shit together and improve their RT / PT performance, otherwise they will continue to lose more Marketshare on GPU department no matter how hard their fanboys thinks that they are pointless features, just like DLSS was back on 2020 right?  Also, with my 4070 Ti OC i can run it at average of over 60 FPS at 1440p DF optimized settings with DLSS Balanced + Ray Reconstruction without even using DLSS Frame Gen, with it on i can get over 80+ FPS",Negative
Intel,Running this way means you lose RR…why in the world would you run at native 4k?  It’s completely pointless now with RR.,Negative
Intel,Im having 80 fps  thanks to dlss 3.5  and its looking better than ever,Positive
Intel,"Why would I run native? I have amazing DLSS, ray reconstruction for huge image gains and frame gen. Nvidia offering all the goodies.",Positive
Intel,"I have a 13900KF-4090 rig and a 7800X3D-7900XTX rig. They are connected to a C2 OLED and to a Neo G9.  I've been holding on to play the game till 2.0 update. I've tried many times with different ray-tracing options and they all look good and all. But in the end I closed them all, turned back to Ultra Quality without ray-tracing and started playing the game over 120FPS.  This is a good action fps game now. I need high fps with as low latency as possible. So who cares about ray-tracing and path-tracing.  Yeah ray-tracing and path-tracing are good. But we are at least 2-3 generations away for them to become mainstream. When they are easily enabled on mid-range gpus with high-refresh rate monitors, they will be good and usable then :)",Positive
Intel,"If you have the HW, go all out. That's why we spend on these things.  I'm getting the best experience you can get in the premiere visual showcase of a good game.   It's path tracing. It isn't cheap but the fact that we have DLSS and FG with Ray reconstruction is a Godsend. It looks stunning and it's still early in development.",Positive
Intel,This doesn’t seem right…. 3080 performing worse than a 2080TI?,Negative
Intel,How tf is a 2080 ti getting more fps than a 3080?!?,Neutral
Intel,I'm not seeing the RTX A6000 on here...,Neutral
Intel,4 fps for 7900 XTX  Oof. Shows you how much of a gap their is between AMD & Nvidia with pure ray tracing.,Negative
Intel,"Everything maxed, PT, 1440p, DLSS+RR+FG, input feels good, game looks and runs great, 100+ fps",Positive
Intel,"""Get Nvidia if you want a good ray tracing experience""  Yes Nvidia GPUs give a better ray tracing experience but is it really worth it if you are required to turn on DLSS? Imo, the more AI-upscaling you have to turn on, the worse the Nvidia purchase is.   I have a 6900XT and I will readily admit that the RT experience (ultra RT, no path tracing) at native 1080p resolution is ok, like 30-45 FPS (around 50 on medium RT settings), but if I turn RT lighting off (so RT shadows, sunlight, and reflections are still present) suddenly I get pretty consistent 60 FPS (i left my frames tied to monitor refresh rate, so 60 FPS is my max) and i can't tell the damn difference at native 1080p compared to RT medium or Ultra.   So would I spend another $400-$1000 to get an imperceptible outcome (imperceptible to me that is)? Most definitely not.",Negative
Intel,Does anyone actually play this game? Its more of a meme game imho,Negative
Intel,4.3 fps 😂😂😂,Neutral
Intel,"RTX 4090 DESTROYS 7900XTX with over 400% increased FPS, coming in at an astounding…. 19.5FPS 😫😂",Positive
Intel,"Overclocked RTX 4090 can (there are factory OC models that run up to 8% faster than stock). A measly 2.5% overclock would put that at 20 FPS.  Native is irrelevant though, DLSS Quality runs much faster with similar image quality.",Neutral
Intel,[85-115fps (115fps cap) in 4k dlss balanced](https://media.discordapp.net/attachments/347847286824370187/1154937439354368090/image.png) ultra settings,Neutral
Intel,Lol the 3060ti is better than a 7900xtx...crazy,Positive
Intel,Why would you use it without DLSS? Upscaling is the way of the future and AMD better improve their software to compete. Future GPU's aren't going to be able to brute force their way to high frames.,Neutral
Intel,Who cares about native though.  Cyberpunk PT at 4k DLSS Balanced + FG is graphically so far ahead of any other game ever released that it literally doesn't even matter if other games are run at native or even 8K. Cyberpunk is just in league of its own.,Neutral
Intel,ThE gAMe iS PoOrLY OpTImiSed!!!,Neutral
Intel,Who cares about 20 fps native? Use the tools and tricks provided by the game and card manufacturer and you are good. The whole native rendering argument can be thrown in the garbage bin.,Negative
Intel,"The 7900xtx losing NATIVELY to the 4060ti 16gb is embarrassing, that card is half the price and overpriced, nevermind Nvidia's far superior upscalers. AMD falls further and further behind in RT, sad.",Negative
Intel,"99.7fps  with everything maxed out at 4k balanced with frame gen.   64.3fps with everything maxed out at 4k balanced w/out frame gen.   29.4fps with everything maxed out at 4k native.   That’s a big difference from this chart what I’m getting vs what they’re getting. For record, I’m using a 4090 suprim liquid undervolted to 950mv at 2850mhz. Stock is 2830mhz for my card.",Neutral
Intel,"Even with frame gen and DLSS quality, Ultra+Path Tracing looks incredible. I get solid 90 FPS. Funny thing is I can't get into playing CP 2077, even with incredible graphics and a QD-OLED monitor. It's just not my type of game :-D",Positive
Intel,I was wondering why a thread like this with the name like this is in this sub. And the I see the name of the OP. And it all makes sense,Neutral
Intel,"""4090 is 4 times faster than 7900XTX""",Positive
Intel,"So? Native 4k is such a waste of compute hardware, only an idiot would use it.",Negative
Intel,People need to change their mind frame. AI is here to stay and will be a major part of graphics rendering going forward.,Neutral
Intel,I played 2.0 yesterday with 2k ultra and path tracing with 155fps (DLSS3.5),Neutral
Intel,"The 4060 Ti literally better than the 7900 XTX in RT, cope snowflakes!111!11! /s",Positive
Intel,"So Nvidia only needs to increase their RT performance 3x while AMD needs to do 14x. We are not seeing that within a decade, if that.",Neutral
Intel,Sounds like Ray tracing isn’t ready for prime if nothing can run it and you have to use AI smoke and mirrors to fake it playable. I’ll worry about Ray tracing when it’s a toggle on or off like shadows and there is no major performance hit. See you in the 9000-10000 series of Nvidia or 12000-13000 series for AMD.,Negative
Intel,Native resolution is a thing of the past. DLSS looks better than native anyway.,Neutral
Intel,Is this with DLSS + FG?,Neutral
Intel,"Ray tracing already wasn't worth it imo, that's why I saved money buying an AMD card. It's not worth the cut in frame rate that then has to be filled with upscaling that makes frames look blurry anyway, so what's the point of even using it?",Negative
Intel,"Meanwhile me using medium settings + performance dlss on 3080 ti to get that locked 120fps. I just can't go back to 60fps. The game still looks amazing, especially on blackstrobed oled.",Positive
Intel,What is even with with all these path traced reconstruction bullshit lately? Ray tracing itself isn't even that mainstream yet.,Negative
Intel,No one is using these settings.,Negative
Intel,"Ray tracing is fun and all, but games have really improve much.. Same animations.. Same type of quest... Same listening to notes.. There have to be better ways....",Positive
Intel,"4K rendering for games like this is pretty pointless unless you're upscaling it to a massive 8K display via DLSS Performance.  Ultimately, the final image quality is going to be more affected by how clean the Ray / Path Tracing is than whether you're rendering at Native 4K or not.",Negative
Intel,"I'm honestly kind of shocked my 4070 is that far above the 7900XTX. I'd have thought it would be no where near close, regardless of ray tracing.",Negative
Intel,cyberpunk sucks don't worry about it,Negative
Intel,3080ti?,Neutral
Intel,"Yes, that is true, but what the numbers do not tell you is that if you tweak some settings, you are able to get 90 fps on that resolution with path tracing with no problem.",Neutral
Intel,I feel like the 4k resolution is here to stay for a couple years boys. Get yourself a decent 4k monitor or OLED TV and chill.,Neutral
Intel,"This is why DLSS3 and frame generation are so important. 4x the performance at over 80FPS with a relatively minor amount of silicon dedicated to AI, with only minor visual degradation.",Positive
Intel,"It's way too early for using ray / path tracing in games comfortably, maybe in 10 years it'll be different, but for the foreseeable future I'm more than happy with normal rasterization.",Neutral
Intel,Who cares tho? Its not even the way it's meant to be ran. This tech is is a synergy. No point in trying to 'flex' native at this point. A 4090 gets very high FPS and amazing visuals when doing things right.,Negative
Intel,can wait in 2030 with the 8030RTX low profile 30 watt card run this game with full path tracing in 4k 120fps lol (in 2030 a low end card will run this like butter don't down vote me because you didn't get it lol),Positive
Intel,"Seems like this tech is only around to give Nvidia a win. Also path and ray tracing doesn’t always look better. If the one true God card can’t run it, is it reasonable tech or is it a gimmick? Especially seeing how like no one has a 4090. Like a very small percentage of PC gamers have one. Most aren’t even spending $1600 on their whole build!",Negative
Intel,What body part do you think will get me a 5090?,Neutral
Intel,"Once they introduced this dlss upscaling anti-lag all these gimmicks I don't think we've seen true 4K native and almost anything lately.  This is maybe one game that does look that good but many of the games don't look that great and have some really serious system requirements when there are games that look much better and are much better optimized.  Companies are taking shortcuts, instead of optimizing they give you suboptimal and use FSR dlss all these gimmicks.  Don't get me wrong I always thought it was great for people who wanted to reach a certain frames per second someone with a weaker system to allow them to play at least the minimal at a steady frame rate.  It just seems like this is the normal now non-optimized games.  The 4090 is a beast if you could only get about 20 frames native with Ray tracing it tells you everything you need to know about Ray tracing.  It needs to be reworked that needs to be a new system I believe one real engine has it built in in a certain way probably not as good as Ray tracing directly.  To me it seems like ray tracing is hampering many games that tried to add it, too much focus on that instead of actual making a good game.  Let's daisy chain 4 4090s ..gets 60 fps.. and then house goes on fire!  Obviously I'm being sarcastic but realistically it's actually pretty sad a 4090 20 frames per second.",Negative
Intel,"Its not really surprising, raytracing is decades away from being truly mainstream. I applaud the misguided pioneers who betatest the technology, their sacrifice will be honoured",Neutral
Intel,"bro, for who are they making such intensive/badly optimised games? for aliens with more powerful pcs? i trully dont get it.",Negative
Intel,That not even proper path tracing.,Negative
Intel,And I thought that ray tracing was a needlessly huge performance drop for a barely noticeable difference...,Negative
Intel,"Can't wait to buy a $1,600 GPU to run a game at 20fps!!!  &#x200B;  Nvidia fans: i cAnT wAiT 🤑🤑🤑",Neutral
Intel,Native is a thing of the past when dlss produces better looking image,Neutral
Intel,Lmfao ... where are all those nvidia fan boys who prefer eye-candy path trace over 60fps?,Negative
Intel,Sounds like some efficient tech. Lmao. What's the fucking point if it makes the game unplayable? RT just washes out the image and looks like shit. Kills the contrast. Can't wait for this fad to end,Negative
Intel,In my opinion it's too early for Ray / path tracing it's computationally expensive and if you are using rasterization at ultra settings not only does look nearly identical but it's easier to do the work for the graphics and that gives you more fps...,Negative
Intel,"Rather than having 2-3fps, I would go and see a video of path tracing.",Neutral
Intel,"Does this fact really matter tho? Dlss in cyberpunk is known the look on par or even better than native, frame generation will further smooth it out. I can definitely see the 7900xtx being able to do path tracing at 1440p at least once amd release fsr 3 and better once they release something to now match ray reconstruction which also give a small boost in fps making path tracing even more doable.",Positive
Intel,cool idc i wont play that shit at such shit settings,Negative
Intel,Do we need this BS? Seriously? Gpu’s will end-up costing over 2 grand soon,Negative
Intel,"And everyone loose their minds about the last ports that came out! Starfield, TLOU, etc. It´s not that the gpu´s are not good enough. Ray Tracing and Path tracing are just not optimized.",Negative
Intel,I don't even get the hype around ray tracing most games don't have it and when they do it either makes the game look blurry or isn't noticeable unless you're in a weirdly lit area or looking at a reflection,Negative
Intel,"Am replaying it with a 6950xt and a 7700k(defo bottleneck).  All maxed out(no RT) at 1440p. If I wanted 4k I would have to enable FSR.  Its completely playable imo.   Mostly 50-60fps.  For me RT vs off vs path tracing.   The game doesn't look more real or better, it just looks different.  But ignorance is bliss.",Positive
Intel,This chart is bull. Since when is a RTX 3060 faster than a 3080. Someone needs to seriously question the source of the chart,Negative
Intel,Just goes to prove once again that Raytracing was pushed at least 4-5 gens too early as a pure marketing gimmick by Nvidia,Negative
Intel,"The Tech Power Up article says:   >Instead of the in-game benchmark we used our own test scene that's within the Phantom Liberty expansion area, to get a proper feel for the hardware requirements of the expansion  So it's very hard to reproduce their data at the moment, but I've run 3 tests with the built in benchmark (run to run variance below 1%, so 3 runs are enough) and my config is averaging 25.7 fps at 4K native with Path Tracing, so the title's premise is not correct. Still, the game is not playable without upscaling at 4K with Path Tracing, so the basis of the sentiment remains correct. Benchmark data available [here](https://capframex.com/api/SessionCollections/279f9216-bdb8-4986-a504-91363328adbe).",Neutral
Intel,RTX 4090 1600€ 19FPS 👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼👍🏼,Neutral
Intel,"Yay i will get 13,8 FPS with my 4080 👍😂 Someone want to borrow some frames? I don't need them all!",Negative
Intel,The first card on this list a normal person would consider buying is a 4070. 8.5 fps lol.   Even with upscaling that's basically unplayable.   Then you have all the UE5 and path trace type demos where a 4090 is only getting 60fps with upscaling.. at 1080p lol.   We are not remotely ready for these technologies yet. They're at least one if not two console generations away. So multiple years.,Negative
Intel,"Looks like 4K is mainly just for videos, not gaming for the time being.",Neutral
Intel,How can you bring out such technology and no hardware can process it properly.  an impudence,Negative
Intel,the question is who need path traycing? i mean that cool and beautiful but 20 fps with a 1600dollar price tag that wild.,Negative
Intel,You could just you know... disable path tracing and get 70 fps or go below 4k,Neutral
Intel,"That what I expect when a game gets built specifically to market Nvidia, but Nvidia over estimated their technology.",Neutral
Intel,"um...im running cp 2077 on a 4090 at 70-90fps, 5k monitor with path tracing/ray reconsturction on and most settings maxed out...",Neutral
Intel,Yo guys let's make this really unoptimized game then rely on hardware manufacturers to use AI to upscale the resolution and insert extra made-up frames to make it playable on a $1600 GPU lmao.,Negative
Intel,"so it is settled, Devs have zero optimization in games.",Negative
Intel,"Love how shit the 3080 was lol, even when it came out it was shit. But somehow everyone loved it",Negative
Intel,"Or, hear me out, developers could just do better. I've seen far better looking games without the constant frame hit.   Cyberpunk hasn't looked good enough to justify the frame rates from the get go.",Negative
Intel,Once again proving RT is useless,Negative
Intel,Cyberpunk w/ path tracing at max settings seems even more demanding than Crysis was at launch.   20fps with a 4090 is insane.,Negative
Intel,"""only gamers get this joke""",Negative
Intel,"People have forgotten or are too young to remember when Nvidia had hardware tessellation features when that was first emerging. They had tessellation maps under the ground and water in Crysis that were doing nothing other than tanking AMD (ATi) performance. I am not saying this whole Cyberpunk thing is exactly the same, but it's essentially similar. In all honesty, turning everything up to 11 on my 3090 doesn't net much visual gain beyond some mirror like reflections and nothing I really care about when actually playing the game (compared to standard High/Ultra) and not pixel peeping, which seems to be the new game now for people buying expensive GPUs, so they can justify their purchase. Meanwhile, everyone else just gets on with enjoying video games instead of living vicariously through someone's 4090 at LOL 1080p",Negative
Intel,Starfield is peaking around the corner.,Neutral
Intel,starfield as well,Neutral
Intel,I remember when physx was so demanding people had a dedicated 2nd Nvidia graphics card just to turn the setting on in Arkham City. Now it's considered so cheap to calculate that we just do it on the CPU lmao,Neutral
Intel,The new Crysis,Neutral
Intel,"I wouldn’t be so optimistic. Transistor shrinking is crazy hard now, and TSMC is asking everyone to mortgage their house to afford it.",Negative
Intel,How long until a $200 card can do that?,Neutral
Intel,Most improvement is probably going to AI software more than hardware in the next few years.,Positive
Intel,"8800GT giving advice to 4090:  “I used to be 'with it. ' But then they changed what 'it' was. Now what I'm with isn't 'it' and what's 'it' seems weird and scary to me. It'll happen to you!""",Neutral
Intel,GPU need to have its own garage by then,Neutral
Intel,Yep! Insane how fast things change.,Neutral
Intel,"Most likely there will be no gpu that supports path tracing and gives you native 4k 120fps in 5 years, maybe even in 10 years.  The technology has slowed down a bit. It’s increasingly more challenging to make more dense chips.  That’s why Intel has been struggling for many years already and every iteration of their cpus gives only minor improvements. AMD went with chiplets but this approach has its own problems.  Nvidia stands out only because of AI. Raw performance increase is still not enough to play native 4k even without ray tracing.",Negative
Intel,Yeah rtx 12060 with 9.5 gb Vram will be a monster,Neutral
Intel,I'm willing to bet hardware improvement will come to a halt before that.,Neutral
Intel,"In 10 years AI based upscaling will be so good, no one will want to natively render unless they are generating training data",Negative
Intel,10 is too much. Give it 5.,Neutral
Intel,Transistor density advancements have been declining for a good while now. We can't expect hardware performance gains of old to continue into the future,Positive
Intel,Like the 1080 barely doing 4k30 and now we have gpus that do 4k120 id way heavier games.  Its still weord to me to see 4k120,Negative
Intel,But then the current gen games of that era will run like this. The cycle continues,Neutral
Intel,TRUE! i think 5-10 years was the actual point in time where anybody should have paid their hard earned dollar for raytracing gpus. instead ppl dished out $1000s for the RTX2080/Ti and now are sitting on them waiting for raytracing to happen for them xD,Neutral
Intel,Who knows what new tech will be out in even 4 years lol,Neutral
Intel,"And that’s when I’ll turn on this setting in my games. Fuck having to pay $1600 for a sub 30fps experience. Path tracing is far from being a viable option, it’s more like a proof of concept at this point.",Negative
Intel,"You wish, GPU makers have stagnated like it's crazy, even the graphics have peaked. I expected to see CGI level graphics like Transformers now",Negative
Intel,"...but will cost your everlasting soul, body, firstborn and parents. If it's on sale.",Neutral
Intel,It is path trancing though. The technology used by Pixar to make Toy Story 4 (though they spent up to 1200 CPU hours for one frame) Path tracing used to take up to a day per frame for films like the original Toy Story. And they had their supercomputers working on it. It is a miracle of modern technology that it even runs real time.,Positive
Intel,"You basically need a 4090 to crack 60 fps at 1440p w/ dlss on quality without frame gen. It looks good, but not good enough to run out and buy a 4090.",Negative
Intel,"Same card, I just turned off RT at 4K. 75-120fps is better than 40 with muddy but accurate reflections",Neutral
Intel,"The 7900xtx is one of the worst gpus iv ever had lol on paper it looks so so good, In games it just play isn’t. Fsr is garbage, 4080 mops the floor with it :( plus the drivers from amd besides starfield have been awful this generation",Negative
Intel,I think that most of the progress will go together with software tricks and upscalers.,Neutral
Intel,"lol. And you missed the 2080Ti.   Every result under 10 fps is just to be ignored, it isn't representing anything outside of ""woot, the card managed to chuck a frame our way"".",Negative
Intel,That's VRAM for you,Neutral
Intel,we are counting decimals of fps its just all margin of error.,Neutral
Intel,If you buy a card with low ram that card is for right now only lol,Neutral
Intel,Wake me up when a $250 GPU can run this at 1080p.,Neutral
Intel,Well DLSS isn't best. DLAA is,Neutral
Intel,Reviews have been saying that the game looks better with DLSS than native. Not to mention runs extremely better.,Positive
Intel,Can’t you just disable TAA? Or do you just have to live with the ghosting?,Neutral
Intel,TAA is garbage in everything. TAA and FSR can both get fucked,Neutral
Intel,"Yeah. 70 to 100fps on a 4080, but with 3440x1440 and DLSS-quality-FG-RR (nvidia needs new nomenclature....)",Neutral
Intel,"same, high refreshrate at 4k with optimized settings + PT + FG. With a 4080 of course, it's insane that it can look and run this great.",Positive
Intel,"Not exactly. Path tracing is just ray tracing, but cranked up to 99% with the least amount of rasterization possible.",Negative
Intel,"> Give it 2 gens and you are going to get 4k60 here.  Assuming the 5090 literally doubles a 4090 (unlikely), that only gets us to 4K 40hz.  Assuming a 6090 doubles that, 80. which won't be bad.  Going with more conservative 50% boosts. 5090 will give 30. 6090 will give 45.  And i feel like 50% is being very generous, as nvidia have claimed moores law is dead and they can't advance beyond a 4090 by much. I'd guess we get 30% uplift in 5090 and maybe 10-15% uplift in 6090. So we'd still be under 4K30.",Neutral
Intel,Imagine buying a 4090 and then using upscaling.,Neutral
Intel,Path tracing is so much more demanding than ray tracing due to light scattering being modelled. It is a marvel it even runs.,Positive
Intel,PC gaming is in Crysis.,Neutral
Intel,Well it's a good thing 99.999999% of PC gamers don't give a shit about ray or path tracing.,Negative
Intel,You're the first other person other than myself I've run into that had an XTX but ended up with a 4090 in the end.,Neutral
Intel,"Plus frame generation works very well in Cyberpunk in terms of image quality. In some games, you need to get closer to ~80 fps output for an acceptable image quality with FG. But the FG in CP2077 is decent with 60 fps output, ~~and I get ~65-70 fps output with quality DLSS + FG at 4k on a 4090.~~ EDIT: I misremembered what I was getting. With path tracing, DLSS quality, frame generation, and ray reconstruction, I got 80.1 fps with the benchmark!  Of course there's the matter of latency, and the latency of CP2077 with FG output of ~65-70 fps isn't great. So I'll often use DLSS balanced + FG. Thanks to ray reconstruction, this now looks very close enough native 4k (to my eyes), with acceptable latency (to me), at a high framerate output.",Positive
Intel,"Nvidia features are always useless until AMD copies them a year or two later, only then they become great features 😁",Positive
Intel,"Tbh, list the games that have ray tracing right now. Pretty few.  It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it). I would personally go amd over nvidia because those 50 or so euros more that nvidia has against the amd counter parts simply are too much for just better rt and  dlss. I could spend those for a better amd card with generally better performance.  Regarding dlss, personally I would just lower the graphics before resorting to kinda bad generated frames, fsr even worse. But that's my point of view.  I find that amd still has to fix some driver issues but other than that, they are a fine brand. (so is nvidia)",Neutral
Intel,Yeh because native 4k looks worse than dlss + RR 4k,Negative
Intel,"Yeah the game with DLSS, RR and path tracing at 1440p looks amazing and with very high fps",Positive
Intel,What's the point of having $5000 PC when you're still gonna have literally the same graphics as $1000 PC then?,Negative
Intel,"This is a tech demo. That's the whole point. It's not really playable yet, but the game really is meant to showcase what is possible in the future and how close we are getting. That's what Nvidia is doing here by funding this whole project.  Crysis who many people are comparing this to, was in itself quite revolutionary for its time. The destructible environment in Crysis to this day holds up, and that was it's killer feature really.  You're gonna have swings at the future that miss as well, and that's ok.",Positive
Intel,"Did you try DLSS?  Imagine getting down voted for asking if he was using DLSS. What a fragile community. Sometimes it's okay to use DLSS, other times you better not or people will come to your house with torches",Negative
Intel,VRAM,Neutral
Intel,"Depends on what you are after i guess, is it worth it for me? Absolutely, DLSS+FG+RR with PT is a great experience.",Positive
Intel,still better than 90% of games in 2023,Positive
Intel,Arguably better now Ray Reconstruction has been added. It's quite a big image quality upgrade.,Positive
Intel,"…no it does not, unfortunately. Have you seen the artifacting in Cyberpunk?",Negative
Intel,"Someone ate up the marketing. I often see people wonder why games are so unoptimized today, your answer is right here. \^",Negative
Intel,All graphics you see on your computer screen is fake,Negative
Intel,Path tracing is more demanding than Ray tracing,Neutral
Intel,"That's why I downscale from 1440p to 1080p, running DLSS and using Contrast Limited Sharpening. It looks better than native resolution and still performs well.",Positive
Intel,I guess between the 3090 and the 4070,Neutral
Intel,Yep hahaha,Neutral
Intel,RemindMe! 7 years,Neutral
Intel,"No, this tech is around so game developers can sell more copies of their games by making them look better. The only reason why Nvidia is winning is because their GPUs are much better at intensive ray-tracing.",Positive
Intel,"This is an absurd, fanboyish thought lmao",Negative
Intel,dont let novidia marketing see this youll get down voted into oblivion,Negative
Intel,I would still argue it's still in proof of concept stage. The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native. That will the allow the lower end GPU'S to get 4k 60 or 1440p 60 upscaled.,Neutral
Intel,I mean I have no problem playing at 30fps dlss balance at 1080p DF optimized settings on a 2080ti with pathtracing idc what anyone says RT/PT is the future it just looks so much better than Rasterization,Positive
Intel,"Possibly a little more. Assuming that the leaked 1.7x improvement is right and path tracing performance scales the same, that would be about 34 fps at 4k native. Might be possible to hit 60fps or close to it with DLSS Quality.",Neutral
Intel,It could as well do 60fps if Nvidia adds a lot of path tracing HW into the GPU.,Neutral
Intel,Fanboyism of both kinds is bad,Negative
Intel,"It won't end, but hopefully we see games that have art designed with RT lighting and reflections in mind.  Cyberpunk isn't it. We'd need a full rebuild of the game, every location, every asset and surface remade, so it looks like originally intended by the artists.",Neutral
Intel,Since it needs more than 10gb vram,Neutral
Intel,Cyberpunk is more optimized than majority of games that came out in last two years. Just because you can't run it at 4k 120 fps with path tracing doesn't mean it's not optimized.,Negative
Intel,It’s not unoptimized at all. Game runs great for the quality of the graphics. You guys just don’t understand the insane amount of power path tracing takes. The fact you can do it at all rn is insane but once again people just throw “unoptimized” at it because they don’t understand the technical marvel happening in front of their eyes.,Negative
Intel,"No, this game is actually very well optimized. It's just extremely demanding at 4k native, which is why DLSS exists.  It would be unoptimized if it ran like this while looking like nothing special, but this is probably the game with the best graphics, period. At the very least, it's certainly the game with the best lighting.",Positive
Intel,"and people will be like ""well RT is the future""  raster was fine for over 15 years and raster games look great but we get it we must let incompetent game devs and their bosses make shit quality games because average gamer is ready to spend $100 to pre-order electronic version of horse shit and then justify it for years because they can't admit they wasted money",Negative
Intel,It's settled that you have no idea what you talking about,Neutral
Intel,Its full path tracing u cannot really optimize this much.,Negative
Intel,"It always did. Most games don't scale well, Cyberpunk does.  Look at the last two years of pc ports and you will understand how cyberpunk looks and performs like it's black magic.",Neutral
Intel,"Baked illumination requires you to choose where to add light sources and how the lighting is applied to everything, it's a creative choice of the author, the AI needs instructions to know what to do, so it would need constant handholding, on the other hand RT uses simple physics to calculate how light bounced and NN to apply the light on surfaces.",Neutral
Intel,But the destructable environment and the water graphics and other things in Direct X 9 made high end pcs kneel as well.  I remember playing on my 9800gtx/+ with my Intel Q9300 quad core (lapped and oc'ed to 3.0ghz - EDIT: checked some old evga posts got it to 3.33ghz) with 2x4gigs DDR2 @1000mhz cas 5 trying to maintain a sustained 30fps at 900p resolution on my 1680x1050 monitor. And I oc'ed the crap out of that 9800gtx 835 mhz (cant recall if it was unlinked shadder or not now) core on blower air (won the silicon lottery with that evga card).  Tweaking settings was mostly user done and guided with old school limited forum help. Ahh the good old days of having lots of time during school breaks.,Neutral
Intel,"The equivalent of a 4090 at the time, 8800 ultra (768MB) got 7.5 fps at 1920x1200 very high quality. The other cards were not able to run it. Even the lowest tier last generation card runs this, even though it's at 5% speed.",Neutral
Intel,No and no again lol  The 8800gtx could do 1152x864 high and that was the top dog. DX9 cards could do that at medium but high shaders was like a 25fps average tops and that dropped into the teens on the snow level.   It took things like the gtx480 and HD 6970 to do 1920x1080 30fps max settings. That's before getting into Sandy Bridge being the first cpu that could sustain 40+ fps if the gpu power was there.   Crysis came during the core 2 duo/k8 athlon and first wave dx10 cards era and it took *2 more generations* of *both* gpus and cpus to do it some justice.,Neutral
Intel,"My 8800ultra was getting single digit fps at 1080p ultra, so CP isn't quite as demanding.",Neutral
Intel,Amd had a tessellation unit. It went unused but it was present,Neutral
Intel,"My fun story for PhysX was Mirrors Edge. I don't remember what GPU I had at the time but the game was pretty new when I played it. Ran fine for quite some time until one scene where you get ambushed in a building by the cops and they shoot at the glass. The shattering glass with PhysX turned on absolutely TANKED my framerates, like single digit. I didn't realize that the PhysX toggle was turned on in the settings. This was at a time when PhysX required a dedicated PCIe card in your system.   Once I turned it off it ran fine. Now I can run that game at ridiculous framerates without my system getting warm.",Positive
Intel,"I remember around about 2008 when companies like Asus and Dell were selling ""Physics Processing Units"" and some claimed that these would be commonplace in gaming machines just like Graphics cards had become 10 years previously.",Neutral
Intel,"Hardware accelerated physics (as in on the gpu), is different than what's going on the CPU.",Neutral
Intel,Yeah. People with Windows 7 were using AMD to play the game with old Nvidia card for Physx. Nvidia didn't like that and blocked it via driver,Neutral
Intel,"People used to have ATI/AMD for main and a lower-end NVIDIA for PhysX.  When NVIDIA found this out they pushed out drivers that disabled PhysX on their cards if an ATI/AMD card was detected, limiting you to the intentionally piss-poor CPU implementation of PhysX.  Think about that crap.  One day everything's going fine for consumers, the next day NVIDIA decides they don't like how consumers are legitimately using their cards and gimps everyone, weaponizing a physics engine company that they bought in 2008.",Negative
Intel,"Oh damn, I forgot about those cards.  I wanted one so badly.",Negative
Intel,Remember when companies tried to sell physics cards lol,Neutral
Intel,>PhysX  It never was a dedicated Nvidia card - it was a [dedicated psysx](https://www.techpowerup.com/review/bfg-ageia-physx-card/) on which the tech later was bought by Nvidia and implemented in it's own GPU's.  But the things never became really populair.,Neutral
Intel,"This makes me wonder if we'll ever see something similar with Raytracing, where we get a 2nd GPU purely for RT, and then the main GPU just does all the other stuff. Would that even be possible?",Neutral
Intel,I remember when people had a dedicated PhysX card.,Neutral
Intel,The ray/path tracing in this case done by specialized hardware which has more room to grow faster.,Neutral
Intel,"I would, transistor shrinking isn't the only method of increasing performance, and honestly, these companies have to keep putting out better cards to make money.  There have been many breakthroughs over the last few years. I give it another 5 as both amd and nvidia are pushing ai accelerated ray tracing on their cards, nvidia is in the lead for now but amd will eventually catch up.",Positive
Intel,There is still a big leap possible since all lithography processes at the moment are hybrid euv and duv.   But the moment everything is done euv things will drastically slow down.,Neutral
Intel,"No those are just pennies in the pocket of Nvidia, but as the consequence you as the customer need to take a mortgage on a brand new GPU.",Neutral
Intel,"Yeah the next 10 years are gonna be spent getting budget cards up to 4090 level performance, rather than bringing the bleeding edge to 5-10x the 4090 performance. As long as 4K is king, there’s not much incentive to do the latter.",Negative
Intel,"In 10 years we will be begging one of several thousand test-tube created Musk Family members for $200 so we can buy a cheeseburger.  But the jokes on them, we’re just gonna spend it on space crack.",Negative
Intel,"Never. Even if performance can be pushed that far, by the time it happens there won't be such a thing as a $200 graphics card anymore.",Negative
Intel,It's not happening unless the market crashes and they start focusing on offering good price/performance cards instead of bumping up prices every generation.,Negative
Intel,25 years,Neutral
Intel,"The software needs to run on hardware, right now it eats through GPU compute and memory.",Neutral
Intel,And sadly ended up with 450 Watt TDP to achieve that performance.,Negative
Intel,This. We'd be lucky to see more than 3 generations in the upcoming decade.,Positive
Intel,"Having seen a few newer games on relatively low resolution CRT display, I can't help but think it might come down to improved display tech and embedded scaling. Like DLSS3 features in the display instead of the GPU.",Neutral
Intel,Not with that attitude,Negative
Intel,Love how it's still gimped on memory size 😂,Positive
Intel,"They just need to render the minimum information needed , and let the ai do the rest",Neutral
Intel,"Don't get upscaling, to me it looks shite. Why spend hundreds or thousands on a GPU to not even run at native resolution.   It's something Nvidia are pushing because they've got a better version of it at the moment.  Maybe it'll improve, it's pretty much snake oil currently imo.",Negative
Intel,"Feels pretty good with 120 frames. Playing with a controller, i don‘t mind or even feel the input lag on a projector screen.",Positive
Intel,"It's mad that even with a 4090 running games at 4k, they still look less realistic than the low-res FMV sequences in The 7th Guest from 30 years ago.",Negative
Intel,"Do keep in mind that despite both being named the same, the devil's in the details. Movies use way more rays per scene and way more bounces too.   Path tracing in CP2077 shows temporal artifacts due to denoising, something that doesn't happen in movies. It is being improved with the likes of DLSS 3.5, but it is still quite off when compared to said movies.",Neutral
Intel,"Keep in mind the systems that rendered toy story 1, costing upwards of like 300k IIRC, have less power than your cell phone today and were super delicate/finicky machines. There's a dude on youtube that got a hold of like the second best one that was made at the time and the machine honestly was really impressive for when it came out, but it pales in comparison to even a steam deck really.",Neutral
Intel,"> Path tracing used to take up to a day per frame for films like the original Toy Story  Toy Story didn't use path tracing though, A Bug's Life was Pixar's first movie to use ray tracing (not path tracing) and only for a few frames in the entire movie for specific reflections, they started using ray tracing more generally for Cars and I can't find exactly when they started using path tracing but it should be around the early 2010s which is also when the other Disney animation studios started using path tracing",Neutral
Intel,No it's not a miracle. Because it's downscaled a lot and also uses lower quality math and piggybacking on AI to make up for it. It's basically running through 3 layers of cheats to produce results that aren't that much better than just traditional raster.,Negative
Intel,"That’d be some next level consumerism, paying 1500$ minimum to turn on a single setting in a single game just to play it wayyyyy slower than you would otherwise",Negative
Intel,"Hell playing path tracing with my 2080ti at 25 FPS is still looks absolutely fantastic, I would absolutely play with pathtracing on a 4090 constantly. Idc dlss looks great with RR even at 1080p. I won’t upgrade for another year or 2 (just bought a phone so I’m broke right now)",Positive
Intel,4090 getting 60fps at 4k balanced dlss with no frame gen. 100 with frame gen. I can make it drop into the 20s if I stand right next to a fire with all the smoke and lightening effects or if I go to a heavily vegetated area it’ll drop to mid 40s. But it’s stay consistently at 55-65 and even goes Into the 90s if I head out of the city.   Haven’t tried it at quality or with dlss off though. May go do that now that it says only 19fps lol. Have to try it to see for myself,Neutral
Intel,"https://www.tomshardware.com/news/nvidia-dlss-35-tested-ai-powered-graphics-leaves-competitors-behind  Well it's so close (54fps) it's more like a 3080Ti or higher from 3000 series or a 4070TI + from 4000 series it seems? The old 3000 series is punching way above it weight vs the 7900xtx, which was meant to deliver similar RT performance to the 3090.. which it doesn't.",Neutral
Intel,At some point the RT pixels are so expensive that native resolution w/o DLSS and frame gen is just not gonna work for the time being.,Negative
Intel,"On the other hand, if you set the new DLSS 3.5 to performance (which you should in 4k), and just enable frame generation, you get 90+ in 4k with basically zero issues unless you pause to check frames.",Positive
Intel,So rendering at 960p? Oof...,Neutral
Intel,"you take that back, I literally returned my 4080FE for a 4090 liquid x for more frames in cyberpunk.",Neutral
Intel,I almost see no difference.   And to see the little diffidence I need to take two screenshots and compare them.,Neutral
Intel,The game look is ruined on PT it makes everything look completely different.  Regular RT keeps the style of the game and performs good.,Positive
Intel,Dont use useless raytracing and you wont have any problems lol,Negative
Intel,"Because PC gaming blew up during a time where you could buy a mid range GPU and not need to upgrade for 5-6 years. Now those sample people buy a GPU, and 2 years later it can't run new games. At least that's my theory.",Negative
Intel,AW2 looks insane. Can't wait to play soon,Positive
Intel,Well the 3050 managed to hold with just 8GB while the 3070Yi crashed?,Neutral
Intel,"Yeah, and people insisted on defending the configurations at launch lmao. The cards just won't be able to handle heavy loads at high resolution such as this game, regardless of how fucking insane the actual processing unit is. You can't beat caching and prepping the data near the oven. Can't cook a thousand buns in an industrial oven at the same time if there's trucks with 100 coming only once an hour.",Negative
Intel,My 3080 in shambles,Neutral
Intel,"It looks better than native even with fsr quality, the Taa in this game is shit",Negative
Intel,You dont need to increase raw performance. You need to increase RT performance.,Neutral
Intel,Imagine!,Neutral
Intel,DLSS unironically looks better than native TAA even at 1080p in this game because of the shit ghosting.,Negative
Intel,Only people who don't give a shit about high quality graphics don't care about ray tracing.,Negative
Intel,I got a stupidly good deal on the 4090(buddy won an extra one from a company raffle) and it just so happened my XTX was one of the cards suffering from the mounting issue on release.  Honestly very happy that all of that happened. Sure I would have been happy with the 7900xtx if I got a replacement but the 4090 just kinda blew me away especially with all the DLSS tech behind it.,Positive
Intel,"Nvidia used cp77 as a testing grounds for their dlss3 and rt tech, it'd no surprise it looks and performs relatively good, they literally partnered with cdpr for that.  Do not expect these levels of RT on most games anytime soon, probably in a couple years (with rtx 5000 series) the tech will be more mature and not as taxing for most cards, because needing to upscale and use framegen to get 60fps on an RT PT setting is kinda absurd.",Positive
Intel,">only then they become great features  Watch this happen with ""fake frames"" FSR3 in real time",Neutral
Intel,>Ray tracing as of right now is a gimmick   This line was already false years ago when control launched. Now it’s just absurd,Negative
Intel,"There are a lot of new games that features Ray Tracing on them as of now, too bad most of them doesn't look as visually as good as they could be though as most of the time their RT effects are dumbed down or reduced to the point it's pointless.  But the thing is that wasn't the point even from the beginning, RT Overdrive on Cyberpunk is literally considered as a Technological showcase  What matters here IMO for all of us is PC platform in general is showcasing here what a real high-end computers can achieve beyond what current gen console can do, and they are showcasing it really well here on Cyberpunk with much better visuals and using plenty of tricks to make them more than playable, where they shouldn't be considering how much demanding they are over standard, that is impressive enough to me and shows a good sign of technological engineering innovation.   As what Bryan Catanzaro said on his interview on DF Direct, work smarter not harder, brute force is not everything, of course it still matters a lot but doing it alone is starting to become pointless just like how it is on real life.",Negative
Intel,Much more fps :) Newer generation cards have at least 50-60% better raster performance then prev gen.,Positive
Intel,The Evangelical Church of Native,Neutral
Intel,"So just to be clear, you don't like running games at native resolution? Because the purpose of DLSS is to improve performance by specifically not running at native resolution",Negative
Intel,Nice but ive been playing video games since the 70s and its Shit end off..,Negative
Intel,The future of frame generation is going to be huge. I can definitely see 360p frame rendering upscaled to 4k with 80% maybe even 90% of the frames being generated. All with equal to or even better visual quality.   A 20 FPS game like this will become a 100-200fps game.  We may be looking at the end of beefy graphics cards being required to play games. Just a good enough mid or low tier chip (granted 5-10 years from now) with frame generation might be enough.,Positive
Intel,"After 2.0, it's around 100""fps"". Just 60""fps"" (meaning with FG enabled) is far from being playable due to latency. You need at least 90""fps"" for playable experience.   Cyberpunk PT vs RT is a huge difference in terms of graphics, while DLSS Quality vs Balanced is really slim in comparison. What you say would be a really bad trade-off.   At 1440p DLSS Quality it's easily 120+""fps"" which is perfectly comfortable.",Neutral
Intel,Cyberpunk is greatly optimized. It looks fantastic for the hardware it asks. Starfield looks like game from 5 years that asks hardware from the future.,Positive
Intel,"Yeah, sure, now go back to play starfield.",Neutral
Intel,"There literally is a /s, what else do you need to detect sarcasm?",Neutral
Intel,"Someone made a really cool comment yesterdays on Reddit that even a generated frame using path tracing is less fake than a rasterized frame. It is effectively closer to reference truth. I had never thought about it that way, but people really are clutching their pearls with this shit",Positive
Intel,"I know, which makes path tracing even worse off imo.",Negative
Intel,I will be messaging you in 7 years on [**2030-09-23 15:26:41 UTC**](http://www.wolframalpha.com/input/?i=2030-09-23%2015:26:41%20UTC%20To%20Local%20Time) to remind you of [**this link**](https://www.reddit.com/r/Amd/comments/16pm9l9/no_gpu_can_get_20fps_in_path_traced_cyberpunk_4k/k1v42p3/?context=3)  [**CLICK THIS LINK**](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5Bhttps%3A%2F%2Fwww.reddit.com%2Fr%2FAmd%2Fcomments%2F16pm9l9%2Fno_gpu_can_get_20fps_in_path_traced_cyberpunk_4k%2Fk1v42p3%2F%5D%0A%0ARemindMe%21%202030-09-23%2015%3A26%3A41%20UTC) to send a PM to also be reminded and to reduce spam.  ^(Parent commenter can ) [^(delete this message to hide from others.)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Delete%20Comment&message=Delete%21%2016pm9l9)  *****  |[^(Info)](https://www.reddit.com/r/RemindMeBot/comments/e1bko7/remindmebot_info_v21/)|[^(Custom)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=Reminder&message=%5BLink%20or%20message%20inside%20square%20brackets%5D%0A%0ARemindMe%21%20Time%20period%20here)|[^(Your Reminders)](https://www.reddit.com/message/compose/?to=RemindMeBot&subject=List%20Of%20Reminders&message=MyReminders%21)|[^(Feedback)](https://www.reddit.com/message/compose/?to=Watchful1&subject=RemindMeBot%20Feedback)| |-|-|-|-|,Neutral
Intel,haha this is gold 🥇,Neutral
Intel,"At the moment, it’s definitely what it seems like.  Especially seeing how there’s only one card on the planet that can run this. Do you have a 4090?",Neutral
Intel,">I would still argue it's still in proof of concept stage.  Not at all, in fact another AAA game with ""path tracing"" is coming out next month.  >The tech will only become viable once the flagship GPU'S start getting 4k 60fps at native  Forget ""native"", that ship has already sailed. Nvidia, AMD and Intel are all working on machine learning techniques, and several game developers are starting to optimize their games around DLSS/FSR.",Neutral
Intel,"Nope, not even close.  Shaders 3.0 - that's improvement. Ray tracing is noticeable, but nowhere near to justify the hit.  I've looked techpowerup comparison RT vs LOW (lol). Low looked far better and cleaner, then blurry mess of RT (Overdrive).   I don't need ""realistic"" lighting, I need pleasent bisual part with good gameplay. For ghraphics I have a window in my appartment.",Neutral
Intel,"In a screenshot, sure. When actually playing the game, unless the fps is low enough to remind them, 99,99% of people will forget if they left it on or not.",Negative
Intel,Pixar movies will sometimes take the better part of a day to render a single 4k frame on a render farm. So the fact that Cyberpunk takes more than 0.05 seconds to render a 4k frame in its path-tracing mode on a single 4090 clearly means that it's unoptimized garbage! /s  EDIT: Apparently people in this thread don't understand sarcasm (or don't understand how a path-traced game can take longer than 0.05 seconds to render a 4k frame with high-poly path tracing and still be optimized).,Negative
Intel,Man you had to lap your q9300 to hit 3.0ghz? Those chips really weren't OC friendly.  My q6600 hummed along 3.2 no problem on 92mm arctic freezer and a slight voltage bump.  I was so pumped when I upgraded to a gtx 260. Crysis was a dream.,Positive
Intel,Funnily Crysis still have better destructible environments than Cyberpunk has tho,Positive
Intel,"This. People losing their shit either weren't around when Crysis launched or have forgotten just how demanding it was . PT CP kicks the shit out of a 4090, Crysis murdered my 8800ultra.",Negative
Intel,crysis did not do 1080p 60 on maximum setting and dx10 on a 8800GT.   I had a 1280*1024 monitor back then and barely had 30 fps with my 8800GT.   Even the 8800 Ultra did not do 60 fps on full hd.  https://m.youtube.com/watch?v=46j6fDkMq9I,Negative
Intel,"Man, I remember how hype the 8800gt was. Thing was a hell of a big jump compared to previous cards that came out, prolly our first REALLY powerful card.  Still remember the old xplay video where they build a PC to play crysis and Bioshock. Put 2 8800GTs in sli in there with a core 2 quad which costed one thousand dollars back then!",Positive
Intel,"My poor HD 4850 got pushed to the limit to give me 1280x1024, lol",Neutral
Intel,I remember thinking $300-400 (IIRC) was so much for a GPU back around those days.  7800XT is the closest we've seen in a while and not even close to the top end for today.,Negative
Intel,This is still the case to this day because the game includes a really old DLL file for PhysX. The other day I followed the instructions on the PCGamingWiki to delete some DLL files in the game directory and only then it ran perfectly smooth on my RTX 3070.,Neutral
Intel,LOL I remember this exact scene also.,Neutral
Intel,"CPU PhysX back then was single-threaded and relied on ancient x87 instructions if I recall correctly, basically gimped on purpose. Even with a 5800X3D the shattering glass reduces the frame rate to the low teens. Sadly an Nvidia GPU is still required if you want to turn PhysX effects on for games from that era, though I hear that it's possible again to use it with an AMD GPU as primary.",Negative
Intel,"OMG I had the exact same experience, hahaha I remember it vividly, I was so confused why that room would just destroy my FPS until I figured out PhysX was enabled LOL",Negative
Intel,"I'd like to see ray tracing addon cards, seems logical to me.",Neutral
Intel,"TBH, I could probably run some of the old games I have on CPU without the GPU.",Neutral
Intel,Had the exact same experience. One scene in the whole game that actually used PhysX,Neutral
Intel,You could probably run Mirror's edge with physx on today's hardware without gpu fans turning on,Neutral
Intel,Last time I tried on an R7 1700X and RX580 I still couldn't turn on PhysiX without it being a stutter party 2 fps game.,Negative
Intel,What gpu you have,Neutral
Intel,Now it even runs fine on a Ryzen 2400G.,Positive
Intel,"And they were right, PhysX and systems very much like it are still used but things have advanced so much nobody even thinks about it and it no longer requires dedicated silicon.",Neutral
Intel,"Yeah, it’s been common knowledge for many years now that Nvidia are the most ruthlessly anti-consumer company in PC hardware, and it’s not particularly close.",Negative
Intel,Ah good old Ageia before nvidia bought them out https://en.wikipedia.org/wiki/Ageia,Neutral
Intel,"No, you could actually install two Nvidia cards and dedicate one of them to only PhysX.",Neutral
Intel,"It would certainly be possible, but it wouldn't really make sense. Splitting it up on multiple GPUs would have a lot of the same problems that sli/crossfire had. You would have to duplicate memory, effort, and increase latency when you composite the final image.  It may or may not make sense to maybe have a raster chiplet and a ray tracing chiplet on the same processor package on a single gpu. But, probably makes more sense to have it all on one chiplet, and just use many of the same chiplet for product segmentation purposes instead.   A separate PPU did make sense tho, I'm still annoyed that the nvidia ageia deal essentially kill the PPU in the cradle. Our gaming rigs would cost more if PPUs became a thing, but we could have had a lot better physics then we do today. There is still a revolution in physics to be had some day...",Negative
Intel,would be cool if 2000/3000 series users could get a small tensor core only PCIe card to upgrade to framegen,Positive
Intel,"That's true, but if he's talking about *the equivalent* of a current $200-class card, I'd say about it's 10 years, what do you think?",Neutral
Intel,At least with the 4090 you can run 70% power target and still hit insane FPS while only pulling around 300w which is the same as my old 3080. The gains with that extra 150w are a couple percent at best. Not worth it to me.,Negative
Intel,Intel design will be a little bit different as far as now.  If I understood it right AMD chiplets communicate via lines on pcb but Intel wants to make something like chip-on-a-chip.,Neutral
Intel,bedroom ring library cows summer thought aspiring worm north joke   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"It's also worth noting that those early movies don't use path tracing either, Pixar switched to PT with Monster University around 2013 IIRC.",Neutral
Intel,"There is a lot of room for improvement, both in the software and future generations of hardware.  It's coming along though!  Overdrive mode looks nice, but there's just a lot more ghosting than the regular RT mode.",Positive
Intel,"They are still the same thing and should be named the same, your disclaimer is just semantics about the implementation.",Neutral
Intel,"Weird argument.  Traditional raster is cheats upon cheats upon cheats already.  In fact, all of the lighting entirely in raster is essentially cheats.",Negative
Intel,It will not work well/look good using a single frame worth of data either due to the low ray counts. Digital Foundry has shown Metro Exodus builds the lighting over ~20 frames or so and it seems like Cyberpunk is even more. Even Cyberpunk ray pathtracing doesn't look as good in motion due to that same build-up effect.  You need to cast 20-50x the rays to have enough data for a single frame but then you will be measuring the game performance by seconds per frame.,Negative
Intel,"RT looks significantly better than raster when it’s not just using simple reflections like a Far Cry 6. Raster is all cheats, the so called reflections are essentially rendering the scene or part of the scene being reflected in reverse then applying fake material effects to mimic metal, water, etc. Raster also takes HUNDREDS to thousands of person hours to get the fake reflections and lighting to look decent to good. That’s why devs love RT and want it to grow, as RT/hardware does the work for them.",Negative
Intel,It's significantly better than raster. It kills fps but the quality is great,Positive
Intel,Better graphics needing more expensive hardware is hardly a hot take.,Neutral
Intel,"Yes, better graphics costs performance. SHOCKING",Positive
Intel,People do it!,Neutral
Intel,It’s not way slower. I get 110 FPS at 4k with all DLSS settings turned on and honestly it’s insane.,Positive
Intel,"I shamefully admit I bought a (inflation adjusted) $400 console in the past to play one game. That's marginally better than buying a GPU for one setting in one game, but still.",Negative
Intel,Sounds like most nvidia fanboys,Neutral
Intel,"If you're into the eye candy, I can see a lot of people that have the money doing it.  Not everyone though. That's around a mortgage payment for me.",Neutral
Intel,And yet people will tell you how nvidia is mandatory. Like you want to overspend to play 1 game with shitty fps? I don't understand these people.,Negative
Intel,Nvidia fanboys being happy with fake frames and fake resolution will never not be funny to me.,Negative
Intel,What? I thought amd was always more tuned to raster as opposed to reflections,Neutral
Intel,Which is why nvidia is rabidly chasing AI hacks,Negative
Intel,This and rumors are saying that the 5090 is gonna be 50-70% faster than 4090 which wont be enough for native 4k either.,Negative
Intel,Say that to the people playing upscaled games at 4k (540p) on PS5.,Neutral
Intel,"I understand some RT effects might not catch everyones eyes, but seriously, path tracing cyberpunk vs 0 RT cyberpunk IS a big difference",Neutral
Intel,Huh? [Watch this is in 4K on a big screen.](https://www.youtube.com/watch?v=O7_eHxfBsHQ&lc=UgyO2vRiSI6CRU5kYmV4AaABAg.9uyI5P8oamz9uylTN_rdmL) The differences are really apparent in each version.,Neutral
Intel,"PT looks way better in the game to me - it makes everything in the environment have a more natural look to it.  It adds, not ruins it...",Positive
Intel,"That's how I feel as well. It looks awesome don't get me wrong, but it doesn't look like cyberpunk to me.",Negative
Intel,It's not a perfect way of measure but you can clearly see how (at least on Nvidia GPUs) the 8/10GB cards are way behind the >11Gb cards. Meaning you need 11 or 12GB of VRAM for this scenario which cripples the 3080 but not the 3060.  We said from the start these configurations are shit but no-one listened. There you go.,Negative
Intel,The crypto miners did me a comical solid by preventing me from acquiring many countless times and hours I wasted (came from a gtx1070 before finally being able to upgrade). Was able to get my 6900xt eventually for around $600 with 2 games. Becoming increasing thankful for the extra Vram these days.   Once I start getting through my game backlog and into the gard hitting ray tracing ones will hopefully upgrade to something with at least 24gigs of GDDR# lol.,Positive
Intel,"I mean, Alan Wake 2 is also releasing with path tracing so my guess is we're definitely gonna see more games featuring heavier RT, although not necessarily path tracing due to the performance hit.",Neutral
Intel,"I expect path tracing support to be the exception for a while because most developers will consider the non-trivial dev time to implement it not worth it. However, I'm sure Nvidia would be willing to throw dev help at most developers who would be willing to implement path tracing with their help.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode. Presumably, a less complex world helps, but they may also have implemented opacity micro maps, which I'm not sure if Cyberpunk has done in the 2.0 update. Perhaps they're cherry-picked examples for the trailer.",Neutral
Intel,Raytracing now is a gimmick *  ( * - in AMD sponsored games where RT effects are so tiny you have to zoom on to notice them),Negative
Intel,"Except this is a tech demo. CP77 has lots of cut corners to achieve that, NN was trained to keep the interface stable in that game and even then, ray reconstruction creates horrible artifacts at some angles and distances. And that's with Ngreedia's engineeers literally sitting at CDPR's office.",Negative
Intel,You can already have 120fps on $1000 PC,Neutral
Intel,It improves both looks and fps so it is a win win,Positive
Intel,"The issue with this is the inputs will only be registered at 20Hz. This would feel awful. The benefit of high FPS is having very low latency. This will just make it look smoother, not feel any better than 20 FPS",Negative
Intel,Considering there is no input on the generated frames that is going to feel horrible to play. Base FPS (incl DLSS) really still needs to be 50-60fps to feel playable imo.,Negative
Intel,I'm thinking will there ever come a day where 100% of frames are generated by AI. Like the data inputs directly from the card into the neural network and it no longer requires physical hardware limitation to generate frames.,Neutral
Intel,"Don't forget that all NPCs will be controled by AI to simulate a real life.  They'll wake up in the morning, shower, commute to work, work, then go home.  All this to do it over and over and over until they die...  mhhh... this is kinda sad to type out.![gif](emote|free_emotes_pack|cry)",Negative
Intel,It absolutely is. I’ve been playing at 1440p with DLSS Balanced and Ray Reconstruction and it is absolutely the worst version of DLSS I’ve ever seen. It’s basically unusable.,Negative
Intel,There are horrible artifacts with RR. And ghosting. It replaces artifacts with different ones. Needs a lot more work tbh. Or training.,Negative
Intel,"The thing about ray and path tracing is that they are demanding on their own, not because they are badly optimised",Negative
Intel,"I have a 4070. It runs the game just fine with the tools that have been set in place.  This is absolutely not a pissing match. Cyberpunk is demanding, and takes advantage of lots of tech available for upscaling and ray tracing and frame generation. AMD card’s currently do not match Nvidia’s in those categories, not even close. This us not an attempt to showcase Nvidia tech, but it does take advantage of it.",Positive
Intel,Because it's not about the flagship card. Only if the flagship can attain good performance at native then you will have headroom for lower end cards. Think about it. Of course upscaling is very important. But you also need some headroom to upscale.,Neutral
Intel,"Yeah. I wanted better cooling performance after upgrading to the artic freezer 7 as well with those weird crapy plastic tension clips you pushed in manually, fearing bending them (worse than the stock intel cooler tension twist lock). Kept having random stability crashes until after i sanded it down for better thermals...Good old sandpaper and nerves of steel fearing an uneven removal of the nickel coating.  Was trying to maximize performance as back then core 2 duo and the extreme versions were king and they had way higher single core clocks and were easy to oc. Wanted the multhreaded for Crysis, LoL (back when you had to wait 5min+ to get into a game), BF2, and was eventually playing TF, Day of Defeat and Natural Selection.  My upgrade back then was to the Evga 560ti DS, which I ended up installing a backplate and  a gelid cooler. They had wayy better thermal tape/individual heatsinks for the memory/vram chips for the heat transfer. Evga back then told me if I ever needed to RMA it that I would just need to re-assemble it as it was shipped. Remember using artic solver ceramique instead of as5 due to it potentially not degrading as fast as well.   Good times =)",Negative
Intel,"Yea same here, didn't play crysis until I got a hold of a 260 after having 2 8600gt's in sli. Played mostly wow at the time and those 2 8600gt's in sli got blown away by the gtx 260, but man that card was hot!  Remember in 2011 upgrading to a gtx 570, the first gen with tessellation and playing dragon age 2 which was one of the first games to have it. Ended up turning off tessellation cause it hit the card too hard, least until I got a second 570 in sli, which sadly died like a year later due to heat while playing Shadow Warrior.",Positive
Intel,Q6600 was the bomb. I ran mine at 3.0GHz all its life and it's still alive today. Had it paired with an 8800GT which was IMO one of the best GPU value/performance of all time.,Positive
Intel,Had 3.4ghz on my q6600 didnt want to hit 3.6ghz on modest voltage but 3.4 all day and was quite the lift in fps in gta4 compared to stock 2.4ghz. Run 60fps 1440x900 no problem those where the days when OC gave actual performance boost,Neutral
Intel,> I was so pumped when I upgraded to a gtx 260. Crysis was a dream.  In those days I was a broke teenager and I remember upgrading to a GTS250(rebadged 9800GTX+ with more vram) and crysis still hammered my Athlon x4 965(?) system.  While my richer neighborhood buddy upgraded to a 260 just to play Left 4 dead.,Positive
Intel,"> q9300  Yes, it was not a good bin. I was at 4.0GHz with my Q9550, with just a copious amount of voltage.",Negative
Intel,"After checking sone old Evga posts, (mod rigs server is done, along with old signatures), I was able to get to 3.33ghz.",Neutral
Intel,"Yeah my lapped Q6600 was my daily driver at 3.2 GHz with a Tuniq Tower 120 Extreme. It ran it pretty cool. I think when Crysis came out it and my 8800 GTX could do 20-25 frames at 1680x1050 with drops into the teens. EVGA replaced it with a GTX 460 when it died, big upgrade.",Positive
Intel,moving data between the two is the issue,Neutral
Intel,grey cagey sharp detail handle north grandiose connect sparkle hungry   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,"I wonder the same thing. But I guess if Nvidia would put it all in an extra card, people would just buy more amd to get the best of both worlds.",Neutral
Intel,Game physics doesn't seem to be a focus anymore though,Neutral
Intel,I'm ashamed I even own one NVIDIA card.    Unsurprisingly it's in a laptop that I bought during the [illegal GeForce Partner Program](https://hothardware.com/news/nvidia-geforce-partner-program) before the public knew about the program.  Not making that mistake again.  Screw NVIDIA.  Every other card I've ever gotten has been ATI/AMD (and obviously the low-end integrated Intel GPUs on some office laptops)  EDIT: Keep them downvotes coming.  You've all been bullshitted by NVIDIA's marketing.  Your boos mean nothing; I've seen what makes you cheer.,Negative
Intel,Anyone who owns an Nvidia GPU should be ashamed of themselves tbh.,Negative
Intel,"There was more nuance to that ""bugfix"" than just ""it was a bug that got fixed""   Modders were able to re-enable the AMD/NVIDIA PhysX combo basically immediately via editing DLLs.  Not driver replacements, but actual DLL tweaks.  The speed at which modders were able to fix what NVIDIA ""broke"" combined with the public outrage put them in a funny legal spot.  If they continued with PhysX disabled and claiming it was a bug then the bug can easily be fixed, as shown by modders doing it first.  So either fix it and get everyone back to normal, or leave PhysX disabled even though it can be easily enabled and open the company up to potential anti-competitive lawsuits.  Obviously not many modern games use the GPU implementation of PhysX anymore and the performance difference between current GPU and CPU implementations are negligible anyway, but their intent at the time was clear once it was shown how quickly modders were able to get everyone back to normal.",Neutral
Intel,"Yes, but not after Nvidia bought Ageia.",Neutral
Intel,"If the $200 cards of today aren't 3x better than the flagship cards of 10 years ago (they're not), then you shouldn't expect a $200 card in 2033 to be 3x faster than an RTX 4090. Average annual performance gains are more likely to decrease between now and then, rather than increase.",Negative
Intel,I like how you said: static image  because in motion upscaling is crappier,Negative
Intel,"It isn't a weird argument. Rasterization is premised on approximating reality, while RT is simulating it.  The extreme few amount of rays we trace right now, including bounces, means effects are often muted and the performance hit is enormous. To compensate, we're using temporal upscaling from super low resolutions *and* frame interpolation.  Temporal upscaling isn't without it's issues, and you know how much traditional denoisers leave to be desired. Even Nvidia's Ray Reconstruction leaves a lot to be desired; the thread on r/Nvidia shows as much, with ghosting, artifacts, etc. all over again. It's like the launch of DLSS 2.0.  All of that, for effects that are oftentimes difficult to perceive, and for utterly devastating traditional rasterization performance.",Negative
Intel,"It's a shame raster is so labor intensive, because it looks so interesting when done right.     I look at screenshots from cyberpunk, Raster / Ray Trace / Path Trace, and in most of them the raster just looks more interesting to me. Not constrained by what would be real physical lighting. The 100% RT render versions of old games like Quake gives a similar feeling.     But I imagine there will be more stylistic RT lighting over time, it saving a ton of labor is all things considered good, freeing it up for actual game design.",Positive
Intel,">That’s why devs love RT and want it to grow, as RT/hardware does the work for them.  It's the marketing departments that love RT. Devs hate it because it's just yet more work on top of the existing raster work.",Positive
Intel,"It *is* way slower, you're just compensating it by reducing render resolution a ton",Negative
Intel,If it was bloodborne i am guilty of that myself,Neutral
Intel,price zealous rinse detail yam rainstorm groovy automatic snails fact   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
Intel,4k max without dlss or fsr or frame gen I still get 30fps. Sorry you can’t even hold 10. Guess I’ll shill some more to at least not be spoon fed shit from amd,Negative
Intel,"Ray tracing and path tracing aren't just ""reflections"" , but this thread is specifically about the new path tracing modes in CP2077.",Neutral
Intel,Rasterisation is a “hack” too,Neutral
Intel,If it works it works....computer graphics has always been about approximation,Positive
Intel,"4090 is \~66% more powerful than 3090 overall, but it has 112% more fps in this test.     The RT capabilities of NVIDIA cards has grown faster than general capabilities.",Positive
Intel,"I guess on the internet, you can just lie. There’s no game on ps5 that upscale from 540p to 4k.",Negative
Intel,"I guess I was wrong on that one, although SWJS upscales from a resolution as low as 648p, which is not much.",Negative
Intel,It's big in certain lighting situations. In others it's not as noticeable as you'd hope,Neutral
Intel,"PT vs no RT comp is silly.  U should compare PT vs RT, and there, the difference is minor and not even subjectively better. Just different.",Negative
Intel,Natural? It over saturates the light bleeding into the entire scene. Everything becomes so colorful. Even when floors or ceilings are not supposed to be reflective. Literally tiles with grainy texture suddenly becomes a mirror..  It's ridiculously overdone.,Negative
Intel,Guys I'm trying to suffer while I play the game what settings will let me suffer the most,Negative
Intel,"I mean this is native RT/TAA. Was never meant to be played this way. You need to use DLSS and Ray Reconstruction. With that setting, i get about 40-50fps at max settings with my 3080. Not too bad.  The thing about AMD is that they don't have any of this AI technology (yet). You have to rely on raw power, which won't get you far.",Neutral
Intel,"Until the next generation of consoles, I think path tracing will be added to something like a couple major games per year with Nvidia's help. If I'm right, that's wouldn't be a lot, but it's definitely be a bonus for those who have GPUs that are better at ray tracing.  Interestingly, [the recent Alan Wake II DLSS 3.5 trailer](https://youtu.be/HwGbQwoMCxM?t=36) showed the game running at ~30 fps at native 4k when path-tracing (presumably on a 4090). That's substantially faster than than how fast a 4090 runs Cyberpunk on the path tracing mode (which could be for a variety of reasons, including CP2077's larger world).",Positive
Intel,Alan Wake 2 is also an outlier since the same dev made Control which was a test bed for DLSS 2.0 and one of the earliest games to fully embrace ray tracing. Alan Wake 2 is also NVIDIA sponsored and looking to be another test bed for them.,Neutral
Intel,"Redditors trying to have reading comprehension (impossible)     >It's not about being a fanboy of amd, but ray tracing as of right now is a gimmick, not because it's unnoticeable or bad, just because it's not ""popular"" (I myself really like it).",Negative
Intel,"Every single game has cut corners, starfield has multiple cut corners. Tech demo would imply there's no great game under great visuals.",Positive
Intel,Get that fps with a 5120*1440 240Hz monitor,Neutral
Intel,"It's almost like crushing your fps with all these fancy new graphic features isn't worth it, even on the best cards. It's user preference in the end.  That said, many gamers, once they play at high hz screen and fps, find it very difficult to go back to low fps for any reason. Games feel slow by comparison, no matter how great the lighting looks.",Negative
Intel,"Both DLSS and FSR have weird image artifacts (FSR is worse though) so i guess if you don't see them then DLSS only works in your favor then.  I personally want as little noticeable image artifacts from DLSS/FSR when I play so opt for native resolution, generally.",Negative
Intel,Have you tried dlss quality. Dlss balanced sacrifices image quality and dlss quality doesn't/can enhance it sometimes,Neutral
Intel,">a lot of the comparisons just make it look like it gives too much bloom  This is due to colors being compressed to SDR. In HDR that ""bloom"" looks awesome as it's super bright like it would be in real life but still not losing any details. You lose those details only on those SDR videos.  PT just looks and feels way more natural than just RT, when only raster literally looks like it was two different games from two different hardware generations.  Once I saw Cyperpunk PT there is no way I'd imagine playing this game in any other way. All other games look absurdly ugly next to it now. It's something like when you get a high refresh screen and can't look back at 60Hz anymore.",Negative
Intel,"Trust me, I know how the technology works. That doesn't change the fact that it's not worth it imo. I'm not  paying 4090 money to have the newest fad that just makes games look over saturated and glossy.",Negative
Intel,We are talking about running the game just fine now are we? Because rasterization is just fine. I will consider ray tracing reasonable when you can do it maxed out for $1200(GPU) native. Until then it’s a gimmick. IMO,Positive
Intel,"So even you, someone who is an enthusiast of this sort of thing, i.e. a small minority of the userbase, had to spend \*minutes\* benchmarking to realize you didn't have RT turned on.   Yeah, you would lose that bet.",Negative
Intel,Those wolfdale C2D were beasts. I had a buddy with one and we took it 4.0. And it kept pace/beat my C2Q as most games didn't utilize MC as well back then.  Man XFX and EVGA all the way. Shame they both left the Nvidia scene.,Positive
Intel,"Ah, WoW. That started it all for me on an C2D Conroe with ATI x1400 on a Dell inspiron laptop freshman year of college.",Positive
Intel,8600gt in SLI man you are Savage!🔥,Neutral
Intel,"I had the same setup, and i managed to get my Q6600 stable at 3.2.  Was a blazing fast machine and it still shit the bed in some parts of crysys",Positive
Intel,there there’s no way there will ever be another 8800 GT. you got so much for your money.,Negative
Intel,"Those 9800gtx+ were solid cards, it's what I upgraded from.  You make due with what you had.  My first system was a Craigslist find that I had to tear down and rebuild with spare parts from other systems.",Neutral
Intel,Seemed like a perfect use case for the sli bridge they got rid of.,Positive
Intel,Why would it need to send the data to the other card? They both feed into the same game.,Neutral
Intel,Big up the Vega gang,Neutral
Intel,"That is because destructible buildings and realistic lighting REALLY does not go hand in hand. Realistic looking games use a ton of ""pre-baked"" light/shadow, that might change when ray tracing is the norm but it has a delay so things still look weird.",Negative
Intel,"I remember playing Crysis, flipped a wheeled garbage bin into a hut, which came down on top of it. I blew the wreckage up with grenades and the bin flew out, landing on it's side. Took pot shots at the castors and the impact made them spin around in the sockets.   Here we are 15 years later and game physics have barely moved an inch from that",Neutral
Intel,Cause regular stuff is so easy to simulate or fake simulate that super realistic complex items are not really worth the trouble.  &#x200B;  water still looks like crap in most games and requires a lot of work to make it right.  and even more processing power to make it truly realistic.  Cyberpunk is perfect example.  the water physics is atrocious.,Negative
Intel,It's because it's not the big selling point now and as the comment you're responding to... Things have gone so far no one really thinks about it anymore. Ray tracing is what physx used to be or even normal map and dx9/10 features you don't consider now.,Negative
Intel,"Honestly, same here - I hate Nvidia so much for what they do so shamelessly to the average consumer that I’d struggle to recommend their products even if they *were* competitively priced.",Negative
Intel,"People are just stupid, if game companies just slap ray tracing on and don't tweak the actual look of the games lighting, gaming graphics is doomed. Rt is not the magic bullet people seem to think it is, everything looking 'realistic'=/= good art design.",Negative
Intel,"I thought RT was loved by devs, as it allowed them to avoid the shadow baking process, speeding up dev time considerably?",Negative
Intel,We are guilty of the exact same sin.,Neutral
Intel,"Hey OP — Your post has been removed for not being in compliance with Rule 3.   Be civil and follow side-wide rules, this means no insults, personal attacks, slurs, brigading, mass mentioning users or other rude behaviour  Discussing politics or religion is also not allowed on /r/AMD  Please read the [rules](https://www.reddit.com/r/Amd/about/rules/) or message the mods for any further clarification",Negative
Intel,"All rasterization does is approximate colors for all the pixels in the scene. Same thing does raytracing, path tracing, AI reconstruction and whatever other rendering techniques there are.   The final result is what's most important, doesn't really matter how it's achieved",Neutral
Intel,would you care to explain ? Kinda interested to here this,Neutral
Intel,jellyfish follow simplistic plucky quarrelsome unwritten dazzling subsequent mourn sable   *This post was mass deleted and anonymized with [Redact](https://redact.dev)*,Neutral
Intel,"It isn’t minor. The PT with DLSS 3,5 is quite a bit more accurate and reactive, as in dynamic lights are reacting closer to instantaneous than without. The best way is to watch on a 4K monitor or to actually play it on someone’s 4090/4080 or wait for AMDs FSR 3 to see if that helps it do path tracing.",Positive
Intel,> Literally tiles with grainy texture suddenly becomes a mirror  PT doesn't change what the materials are bro. You are just seeing what the materials are meant to look like.,Neutral
Intel,Turn on path tracing. Embrace the PowerPoint.,Neutral
Intel,Control also has some of the best implementations of RT I've ever seen. It's one of the very few games I've turned on RT and felt like it was worth the performance hit. Everything else I've played has looked very good with RT on but not good enough to be worth the performance hit.,Positive
Intel,"Of course it is not perfect but it also allows max RT, PT, RR which improves the graphics greatly...and i do not enjoy below 100 fps either",Negative
Intel,"I tried 1080p DLSS Quality and it was just as bad, but I’m not really happy with the performance of 1440p DLSS Quality (like, 30-40fps), so I didn’t want to use it.",Negative
Intel,"You're is coming off as salty that you *can't* afford a 4090. If other people wanna spend the extra cash and turn on all the bells and whistles, why does that bother you?",Negative
Intel,"Mate, you can't fucking run starfield at good fps, a game that's raster only that looks like from 5 years.  Look at how many games scale and run badly in last two years and compare that to cyberpunk",Negative
Intel,"This just absurd and frankly entitled. Not everyone can afford a PC strong enough to run the most demanding games absolutely maxed out.   I never could, couldn’t even afford a PC until this past summer.   This obsession with performance metrics is ridiculous. Like, less than 5% of PC gamers even have 4k monitors. Almost 70% still have 1080p.   CDPR made a VERY demanding game that looks GORGEOUS completely turned up. Modern hardware cannot handle it natively, it can’t. It is not a gimmick that a company has developed a superior technology that allows the game to be played at almost the same quality as native, significantly smoother and faster. You’re in denial if you think otherwise.  I’m not tied to either brand. I have an AMD CPU bc it was the best option for what I wanted, and a Nvidia GPU for the same reason. I care about RT, frame gen, all of that. The Witcher 3 still is my favorite game, and the entire reasoning for building a gaming PC over a console is graphical quality and speed while doing so. That game is absolutely gorgeous with RT on. Same with Cyberpunk, which I can enjoy thanks to DLSS.   You don’t care about those things? That’s fine, but it is solely a personal opinion.",Negative
Intel,"Yea well I was defining an arbitrary performance. It's not about having native performance. But, but if a flagship is able to get 4k 60 with path tracing at native then that allows headroom for 60 tier or 70 tier cards to perform well.  In this example the 4090 gets 19.5 fps at 4k. The 4060ti gets 6.6 fps. Let's assume a next gen flagship is able to reach 60 fps at 4k. That would theoratically allow the 60ti card to reach around 20 fps. Once you do your various upscaling and frame generation techniques you can then reach 60 fps ideally with a mid range card.",Neutral
Intel,"The quad core showed up the core 2 duos like 2 yrs after because of emerging support and I was on cloud 9. After evga left, i jumped ship with my XFX 6900xt and I think I am here to stay with team Red until value/longevity says otherwise.",Neutral
Intel,Shame you don't. They did it for a reason.,Negative
Intel,The 9800GTX was a bad ass. I'd consider it the biggest upgrade in generation until the 1080 and then the 4090.  It was far more powerful than anything near it.,Positive
Intel,"I do miss it, but it was a pain in the ass to get working with my custom cooler due to the HBM.",Neutral
Intel,They've actually gone a bit backwards. Devs dont seem to care about implementing physics anymore. It's just an afterthought. And you can forget about destruction,Negative
Intel,"I gobble up the downvotes every time I say it and I don't give a crap.  Every other corporation sees through their bullshit.  Otherwise every console would have NVIDIA cards.  No console has an NVIDIA card anymore.  But the general population is more than willing to get bullshitted into buying their cards against their own interest.  EDIT: Forgot about Nintendo Switch, as is tradition.",Negative
Intel,"Anyone can say anything.  Anyone can change their intent after the fact.  Happens all the time.   Judge them by their actions alone and ignore what they say.  Going solely by **events** :   1) People use (mostly used) NVIDIA cards for driving PhysX   2) NVIDIA disables this ability, locking everyone to the god-awful CPU implementation at the time   3) Modders edit DLLs to get functionality back   4) NVIDIA THEN says it was a bug that will be fixed    It's not foil hat because I'm coming to this opinion based solely on what's measurable and real; events and actions.   It's not conclusive, but I'm not going to rely on any company's PR statements to sway me one way or another because they will (obviously) say whatever they're able to prevent lawsuits and public image damage.  Based strictly on **events** that looks like damage control on a bad decision.  They rectified it, even more recently made PhysX open source, I give them credit for making it right, but the intent at time time was clear.",Neutral
Intel,This is why I'm excited for Alan Wake 2. It is coming out with path tracing always having been part of it. This will definitely mean they built the art style around that.,Positive
Intel,"Yeah, I can't really blame people for not recognizing great design if they are not interested in it. I don't think most people are interested in design, even though we all benefit from it without knowing it.     People might get some sort of subtle feeling when they see some great craft made by a experienced professional, but they can't really put it into words or evaluate the quality of the design without experience. They likely can't tell what cause the feeling to begin with even if they can notice it.     But everyone can instantly judge how real something looks, which is the go-to standard for ""good"". And bigger numbers are better, even if they are fake. Niche features that nobody uses also sells, as long as it's a new feature with a captivating name.     This reminded me. Live action lion king, what a travesty, stripping away all the artistry and expression for realism.",Neutral
Intel,"If you're using RT exclusively maybe, but no one in the games industry is - nor will they be any time soon.",Negative
Intel,"A lot of raster techniques reduce the complexity of lighting into very naive heuristics. Like shadow mapping renders the scene from the light's pov to create a depth map, then the objects that are at a greater depth have darker colors drawn (I'm simplifying that but roughly how it works). It's like teaching a kid how to illustrate shadows on an ball vs the how light actually works. Raster techniques have evolved a lot since then, and use raytracing in limited contexts, screen space reflections and ambient occlusion for example, but they're still operating with extremely limited data in narrow, isolated use cases, which is why they often look awful. There's just not enough information for them to look correct, just enough to trick you when you're not really paying attention.",Negative
Intel,All lights in a rasterised scene are “fake”.   Someone explains it much better here:  https://reddit.com/r/nvidia/s/eB7ScULpih,Negative
Intel,Or a bag of fake tricks.,Neutral
Intel,">Every scene is lit completely differently with PT enabled.  It's really not, and it's very disingenuous of you to try and pretend it is. There are plenty of screenshot comparisons showing the difference in certain areas if you want me to link them, and I have the game myself and use a 3090.  There are differences, but saying ""every lit scene looks completely different"" with PT Vs regular RT is false.",Negative
Intel,"I'm not talking about DLSS 3.5. I'm talking about PT vs regular RT in CP2077. The visual difference is subjective, it looks different, is it better? Not in my opinion, it over saturates the scene.  DLSS 3.5 is a separate issue, it fixes some of the major flaws of PT, the slow reflection updates.",Negative
Intel,Have you tried Metro Exodus Enhanced?,Neutral
Intel,At less than 4K resolutions Quality is the only sensible option. The resolution it upscale from becomes too low otherwise and DLSS then struggles to have enough data to make a good image.,Negative
Intel,"It doesn't, I have no problem with people buying what they want. My problem is people assuming I can't afford something...",Negative
Intel,What is this? A reasonable comment in this dumpster fire of a sub?,Neutral
Intel,Thinking that a $1200 graphics card should be just strong enough to run everything Max is entitled. I wear that crown.,Positive
Intel,"I've got an evga 3080ti now. I suspect my next card is either going to be an XFX 8800xt, or catch an XFX 7900xtx on clearance.",Neutral
Intel,"I don't blindly support a single company, such as simply switching to AMD because my chosen vendors left nvidia. That's how competition stalls.  That being said the card I have now is a product of what I could get my. Hands on during the crypto boom, and is EVGA.  My next card is likely to be an XFX 8800xt/7900xtx.  So not sure why you're in here attacking me.",Neutral
Intel,Except it was just a rebadged 8800GTX/Ultra,Neutral
Intel,"I mean, the Nintendo Switch is using Nvidia graphics, albeit it’s a tiny 28nm iGPU from 2014… but I get your point. They’ve opened up a large technological lead over AMD thanks to the RDNA 3 debacle, and I’m *still* recommending only Radeon GPUs because Nvidia is just that hubristic.",Neutral
Intel,Pixel art go brr,Neutral
Intel,Yeah they will. Lumen and lumen style gi systems will have to exist for fallback so the industry can move on.,Neutral
Intel,"Yeah, I agree. It’s just a shame because it’s not really playable otherwise. DLSS with ray reconstruction is a little mixed.",Negative
Intel,"It just doesn’t make sense. Game developers do not set GPU prices.   The game runs well on a $1200 GPU. Perfectly well. Not completely maxed though, and it’s weird to think the game shouldn’t be able to make full use of available technology if it wants to. My GPU cost less than half that and runs the game very well with RT on thanks to DLSS.",Negative
Intel,"RR is not perfect, but I think overall Cyberpunk 2.0 looks so much better than it did previously - and it looked pretty damn good earlier too!  [Digital Foundry's video](https://www.youtube.com/watch?v=hhAtN_rRuQo) is again the yardstick for how many small things they point out where RR improves the situation, even if it has some things where it fails atm. But maybe DLSS 3.6 will solve those.  I do feel DLSS itself has also improved in the past few years in terms of ghosting and performing at lower resolutions.  On a 1440p monitor these are the starting point resolutions:  - Quality: 1707x960p - Balance: 1485x835p - Performance: 1280x720p - Ultra Perf: 853x480p  So even the highest quality option is sub-1080p. I wish Nvidia introduced a ""Ultra Quality"" preset that would be say 80% per axis resolution, something between DLAA and DLSS Quality. At 1440p this would be 2048x1152.",Positive
Intel,"Speaking of the AMD overlay, am I the only one that gets near zero CPU utilization every time they pull it up? GPU utilization fluctuates up and down depending on resolution and graphical settings, so it seems to be fine, but my CPU utilization readings never go above more than a few percent tops. I have a 5800X3D.",Neutral
Intel,GN has already made a [video about it.](https://www.youtube.com/watch?v=5hAy5V91Hr4),Neutral
Intel,Honestly this is everything that I wished Adrenalin overlay had. It's pleasantly lightweight too.   Hats off to Intel's development team. They didn't need to publish this at all.,Positive
Intel,I wonder if this will get added to Mangohud and Gamescope.,Neutral
Intel,This is quality. Great work.,Positive
Intel,Doesn’t capframeX uses presentmon as its monitoring tool?,Neutral
Intel,I can finally see if it really is the ENB taking down my Skyrim gamesaves,Positive
Intel,"FYI, the open-source version has been available for years, was last updated 9 months ago, and does not contain an overlay or GPU busy stats. (It also has version number 1.8 while this new thing is v0.5)  Maybe they're planning to release source code later, but right now the source is closed.",Neutral
Intel,This is pretty damn awesome and will be incredibly useful when they develop the future features Tom mentioned. (insightful metrics),Positive
Intel,"This is not bad at all. But it's not replacing Radeon overlay for me... yet.  1. Needs an option to minimize to system tray.  2. Many important Ryzen CPU metrics (temperature, power consumption etc.) aren't available without an ability to access Ryzen Master SDK. 3. Radeon Overlay can continue to monitor on the desktop, which I use constantly. I tried manually hooking DWM.exe and it won't. I might just be dumb, but I couldn't get it the overlay working without 3D running. 4. Will give credit where credit is due, unlike Radeon overlay, it works right with OpenGL. 5. And the credit is immediately removed, because unlike Radeon Overlay, it doesn't work right with DXVK.  6 out of 10.",Negative
Intel,This is so good. The only thing I dislike is that there isn't a way to scale the whole thing up an down.,Positive
Intel,"At this rate Intel will have a more polished driver and software stack than AMD in 2 or 3 years, if not before.  Seriously, I suck for being able to buy an AMD GPU like the 6990 I owned, but I can't get near any hardware they made while using my system for work.  I need a CUDA alternative that works on more than 6 GPUs on specific linux distributions when the red moon shines above my building.",Negative
Intel,it's sort of misleading to call it opensource    when whatever intel beta version released is just MSI installer and binaries inside ...    i see no source of what is available for download ...   what's on gihub is something old w/o overlay,Negative
Intel,Thanks Intel! I will try this out at least since I hate MSI afterburner.,Positive
Intel,The download link gives a warning in Windows:  >This site has been reported as unsafe   >   >Hosted by intelpresentmon.s3.amazonaws.com   >   >Microsoft recommends you don't continue to this site. It has been reported to Microsoft for containing harmful programs that may try to steal personal or financial information.  ???,Negative
Intel,"I really wanted to test with it why I was getting stuttering in Apex. Unfortunately, due to some work, I would be possible on Sunday.",Negative
Intel,Doesn’t work for me. It crashed at the start with an error message and made Dolphin run way worse.,Negative
Intel,And AmD gives far more than Nvidia.,Neutral
Intel,I can't get this to work. Is anyone having the same issue?  CPU Temperature (Avg) NA C  CPU: 5800X3D,Negative
Intel,I have a 5800x3D and PresentMon does not recognize it. Says UNKNOWN\_CPU.  Anyone?,Neutral
Intel,People have reported that cpu usage in Radeon software is not very accurate.,Negative
Intel,I saved this because it is posted so frequently lol  https://www.reddit.com/r/AMDHelp/comments/14n55gd/cpu_usage_percentages_differ_between_task_manager/jq66gop?utm_source=share&utm_medium=android_app&utm_name=androidcss&utm_term=1&utm_content=share_button,Neutral
Intel,"Probably overlay wasn't updated for W11 22H2.  Microsoft updated something and CPU reading was broken for MSI Afterburner also, after working the same for a decade or more before. They fixed it in the meantime.",Negative
Intel,"I had the same problem, googled around and found the problem to be c state in the bios. Just turn that off and it should be accurate.",Negative
Intel,Just use afterburner as OSD.,Neutral
Intel,I had that issue with my 5600X both with Nvidia and AMD overlay. More recently it randomly fixed itself.,Neutral
Intel,"Yeah adrenalin was useless for it. It was displaying 1-2% usage for me, but msi afterburner, hwinfo, and rtss were showing much more, like 30-50, which made more sense.",Negative
Intel,You beat me to it :),Neutral
Intel,"Yup, ""new Intel"" definitely seems to be getting nicer and with a ""younger"" company culture. Also, it's a smart move. They're building a nice little ecosystem around Arc cards, which is eventually what is going to drive sales when performance and stability matches NVidia and AMD.",Positive
Intel,And it's open source!  I'm liking Intel more and more since they're getting their ass kicked.  Apparently it was one guy's pet project.,Positive
Intel,Technically it should be possible to add in MSI afterburner because it's open source,Neutral
Intel,"The new PresentMon shows more information.  Since it's open source, capframeX can implement this as well.",Positive
Intel,It was a pet project of one of the Intel engineers.   6/10 is not bad!,Negative
Intel,It's still beta. I'm sure they'll fix it.  Intel has amazing engineers.,Positive
Intel,Intel is working on their 2nd generation GPU and it's said to compete with Nvidia highest tier.  And now AMD isn't planning to release a big RDNA4 chip.,Neutral
Intel,"didn't nvn get merged with mesa recently edit: sorry, I mean NVK not NVN",Neutral
Intel,I hate afterburner and RTSS. This is way better,Positive
Intel,"people probably spammed microsoft because this PresentMon also gives you the option of enabling amd, nvidia, or intel telemetry.",Negative
Intel,"Thank you for contributing exactly NOTHING to this discussion.  Truly, the world is a better place because thanks to you.",Positive
Intel,"Tell that to anyone that needs CUDA. Or GPU accelerated workloads in general.  Id suck for being able to get something like the 6990 I owned back then, but between lack of support for almost all workloads and the driver being shit since like forever... Nope.  Yes, yes, nvidia's control panel is old and looks like a windows 98 app. But it at least works and actually saves youre settings, unlike AMD's one that half the time simply forget about everything.  I currently own 5 systems. 2 nvidia ones, 3 amd ones. And all of the amd ones had driver related issues.  You can't simply work with that. Is not viable. And as a gamer you should not need to see if the drivers are working or not.  Fuck, amernime drivers exists just because how hard amd sucks at it.",Negative
Intel,"Well, everyone uses RTSS anyway and it gives you basically everything.",Neutral
Intel,Similar issue here with an i5 12600kf. Can't get temperature readings from it. I'll need to look at it more.,Neutral
Intel,"Really? Good to know it's not just me, then. Nonetheless, this seems like a pretty egregious oversight on AMD's part. If I'm not mistaken, CPU metrics are on by default in the overlay and every time someone switches it on, it's an extremely visible reminder of work that AMD still needs to put into the drivers.",Negative
Intel,Speaking of said software why does it keep setting gpu max frequency to 1200MHZ?,Neutral
Intel,"Thanks for sharing your findings. It's pretty damning if even the Windows Task Manager does a markedly better job than Adrenalin. That's probably an understatement since, as I found out via other helpful Redditors here, the Adrenalin overlay may as well be broken. I'm just glad it's not something wrong with my system.",Negative
Intel,Afterburner fucks with my settings in adrenaline,Neutral
Intel,"NVIDIA's highest tier? Nah, but around RTX 4080 is the target, which might be bad news for AMD.",Negative
Intel,"By NVN do you refer to Nintendo's API for nvidia? Or you wanted to refer to NVK that got into mesa?  By CUDA alternative I mean stuff like ROCm. CUDA have so many years of development on top of it and a stupidly widely support from software vendors.  While yes, NVK could help to alleviate the issue, is not even close to what native CUDA can do regarding GPU usage for general computing.  The issue with ROCm is mainly a lack of support on a lot of software and lack of support for a lot of hardware.  On the nvidia end you can get the cheapest trashiest GPU and it will have the same level of CUDA support as the most expensive enterprise product.  Yes, performance is not the same, but I can use a 3090 Ti for loads that needs more than 12 GB vram without needing to buy a dedicated AI GPU, and if a given task needs serious shit, then again, I can buy a dedicated solution that will run the exact same software with the exact same code I ran on the ""weak"" 3090 Ti.  That is not possible with AMD, if you buy an enterprise grade GPU from them, you need to build all your software stack for it instead of just moving it and keep going.  And while yes, developers from tools like PyTorch COULD improve their ROCm support, I totally get why they are not doing that. ROCm works on like what? 7 GPUs? On specific linux distros?  Is a matter of scalability and costs. ROCm needs to be supported on ALL AMD GPUs in order for it to be something worth developing for, otherwise there is not really a use case where the end user wont be writing their own stack.  And again, if you can start small, then scale up, nvidia is the only option, so AMD on this space is either ""I got a stupidly great offer where AMD provides the same hardware power and performance per watt at like half the prize so I justify building all my software from scratches"" or ""Ok, I'll go for nvidia and just use all the software already built for it and call it a day""  I really, really want to being able to use AMD GPUs aside of trivial tasks, but this is a very, VERY gigante issue that appears a lot, especially on the profesional space.",Neutral
Intel,Except PresentMon seems to not be compatible with Ryzen CPUs at all... :/,Neutral
Intel,I get downvoted for asking a valid question?,Neutral
Intel,Thank you for continuing to contribute Nothing to this conversation.,Positive
Intel,Clearly not more than this beta of presentmon,Negative
Intel,I use amds for overclocking and that's about it really. But it's still far more tools than nivida gives,Neutral
Intel,"To defend AMD here as loathe as I am lately. CPU utilization isn't a very useful metric to begin with, so it misreporting would be super low priority.  The stat tells you nothing useful about the hardware, just the thread scheduling. CPU could be stuck in wait and reporting a high number while doing no work or it could be bottlenecking somewhere else while reporting a low %.",Negative
Intel,Where are you seeing this?,Neutral
Intel,"The Adrenalin overlay is good for really everything except usage. But yeah, this has seemed to have been a problem for a good while now. Ive been using AMD for a little over a year now at least Adrenalin and the CPU usage was always wrong on two different CPUs (AM4 and AM5) as well. Me and that other person tested it on my system and he had 3 systems running different AMD CPUs and all were the same. Ill find the thread one day but its been a while so it is way down the list of my comments.",Negative
Intel,"False setup.  Disable the button in the ui ( not settings)  For ""load on startup""  You literarily told afterburner to load it's settings on start up.  Huge mistake many people do because most people think it's ""boot on startup"" but in reality it's loading it's oc settings and stuff effectively overwriting adrenaline if you leave that enabled.",Negative
Intel,3070 Also was their target for their first generation. I don't expect Intel to suddenly be great just because that's what they aim for.,Neutral
Intel,And XeSS 1.1 looks better than FSR.  XeSS 1.2 was just released and it has even more improvements.,Positive
Intel,Great news to gamers though,Positive
Intel,ah sorry I meant NVK,Neutral
Intel,"I don't know, I didn't downvote you.",Neutral
Intel,My point is if you've got nvidia card you simply use 3rd party soft and got everything and more. So comparing that AMD's software to Nvidia's is kind of meaningless.,Neutral
Intel,"I actually agree with you, but I don't need the metric to be some scientifically accurate reading of performance or whatnot, I just think it'd be a useful tool for adjusting game settings. For example, if I'm trying to optimize performance in a game, does this setting make CPU utilization go up or down? How much almost doesn't matter. Is there a bottleneck, where is it, and am I relieving pressure on it with these settings or not?",Neutral
Intel,It’s where you oc in adrenaline,Neutral
Intel,"There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Not because upscaling, but also because how important is to have drivers that work in tandem with specialized hardware units in the GPU.  Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  AMD fails hard on 2 of those things.  Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.",Positive
Intel,"Yep, XeSS is just great, I happily use it in MWII on my Arc A770. But I still use DLSS on NVIDIA. But both are better than FSR.",Positive
Intel,"Then you should be looking at ***GPU*** usage, because CPU usage will not help you answer any of those questions",Neutral
Intel,"Problem is, by itself it still tells you little to nothing. If GPU utilization drops like a brick you know something is bottlenecking. If CPU utilization changes in the absence of other data you know absolutely nothing of value.   The real kicker is the CPU utilization may not even be ""wrong"", it may just be polling at a very low rate because that's the other thing with these OSDs if you poll too frequently for up-to-date data that can kill performance from the overhead.",Negative
Intel,"> There is something important here though, Intel is way better at software stack than AMD, and current GPUs rely A LOT on the software stack to perform.  Agreed, not to mention that when they made Alchemist, yes their target was 3070 and they underperformed that, but they will eventually hit their targets as they learn to improve their architectures. For a first shot, getting to around 85-90% of their target is actually quite good. It's not like they undershot by 50% of the target. They got close.   But like you said, software is super important and these days having good software is half of a driver. I have an A770, the driver actually pretty good, like the software menu. It used to suck when it first came out because it was some crap overlay, but over time it's become a separate program and it works really good considering they're not even a year into Arc's release. They also have a good set of monitoring tools built in and the options are all simplified. They don't have super unnecessary things like a browser integrated into the driver suite, or a bunch of other stuff. I'm sure by the time Celestial is around if Intel does stick with Arc, that they will be ahead of AMD on the software game, assuming AMD continues down the same path they have now.  > Nvidia's 4000 series did just that to RT. Is not that their hardware got that much extra RT cores, but they added custom units to improve hardware utilization, that means firmware level work, driver level work, SDKs with easy integration for devs.  Yep, tools like [this one from NVIDIA](https://youtu.be/-IK3gVeFP4s?t=51) have made it easier than ever as a dev to optimise for NVIDIA hardware.  > AMD fails hard on 2 of those things. Intel? Intel started working on XeSS even before releasing their first consumer grade GPU.  Yep because Intel recognises that to get close to NVIDIA they not only have to make great hardware, but also great software to go along with it. You can only do so much with hardware to match NVIDIA. You have to make great software along with in-game solutions like XeSS to match NVIDIA. FSR is behind not only DLSS but now also XeSS. AMD is just not competitive with NVIDIA on all fronts. Arc is just getting started, give Intel 5-10 years of time in GPU and they will outpace AMD for sure.",Positive
Intel,"There are many settings that directly impact CPU usage, so of course CPU utilization reading can be useful.  Not everything needs to be precisely accurate, this is a lousy defense. Getting 0% reads all the time is entirely worthless but some accurate reads is still useful information.",Neutral
Intel,"You are going out of your way to defend a broken feature.    7800X3D shows up as 1% CPU utilization during games such as Watch Dogs, Stellaris, Age of Empires. Clearly broken. Undefendable.",Negative
Intel,"And with a bit of luck, they will also hit AMD in the console market.  Right now AMD stays afloat GPU speaking because consoles, they were during a lot of years the only vendor offering both CPU and GPU in a single package, reducing costs A LOT.  Nvidia was not able to compete against that with the lack of CPUs and Intel with the lack of GPUs.  Now that Intel is working on dedicated GPUs they are going to eventually create APU like products, its just the natural outcome of producing both CPUs and GPUs.  And then AMD will NEED to compete against them instead of being the only option.  And if that happens, unless FSR gets turned into a hardware specific tech like XeSS that falls into different techs depending on the GPU its running on, AMD will be murdered.  Better software stack, better general hardware to accelerate and improve upscalling keeping costs down AND being a second player pushing prices down towards Sony and Microsoft?  AMD will be screwed up if they don't up their game. And hard.",Neutral
Intel,"Right, so what kind of useful information can you tell me if I raise graphics settings and CPU utilization goes from 12-16% to 16-17% while framerate goes down from 120 fps to 110 fps?",Neutral
Intel,"All CPU utilization tells anyone is thread scheduling, and nothing about what the threads are or aren't doing. Like the only situation that it marginally tells someone anything is how many threads a program uses. But that usually doesn't fluctuate with games, games usually scale up to <x> number of threads and that is that. GPU utilization is the one that is far more useful for spotting bottlenecks, general performance observations are far more useful for tweaking.  Even Intel and co. will tell you that with modern processors and all the elements they contain as well as hyperthreading/SMT CPU utilization isn't a very useful stat anymore. Meant a whole lot more when CPUs were one core unit and didn't also include the FPU and memory controller and everything else.",Neutral
Intel,"Cause unfortunately even if it wasn't broken, it's not useful in any meaningful capacity. Should it be broken? No. Would anything notable come of it if it were fixed? Also no. Given the bugs they got and room for improvement elsewhere it's one of those things that should be super low on the priority list or just removed wholesale.",Negative
Intel,"That whatever setting you are changing does not have much of an impact on CPU utilization, and if your GPU allows, you can use it.  What is the purpose of this question, are you really not aware of any settings that impact CPU usage? Some RT settings have big impacts on CPU usage too.",Neutral
Intel,"The problem you're missing here is that settings that are CPU demanding generally don't have an easily observable impact on CPU usage, if they have any impact at all.  The reliable way to see if a setting is CPU demanding is to turn it up, see the frame rate go down, and then look at how **GPU** usage changed.",Neutral
Intel,"I'm aware that some settingd impact CPU usage, but I think looking at the CPU usage is about as useful as your daily horoscope, or AIDA64 memory benchmark",Neutral
Intel,Congratulations on the new gaming laptop. Don’t let people get you down and not everyone has to have the very best to be happy so if it makes you happy then I’m happy for you.,Positive
Intel,"Congrats the 30 series isn't half bad. Laptops aren't bad but gaming desktops add more umph to their performance. My first gaming rig was a gaming desktop with 64gbs ddr4 3200mhz, rtx 3060 12gb, 850w platinum psu, b660m gigabyte motherboard and a intel i5 13600kf. Couldn't have had a better first rig.my current build now is a hellhound raedeon rx 7900xt,gigabyte  z790x ax gaming motherboard,  850w platinum psu, 64gbs ddr5 6000mhz,Samsung 980 1gb ssd, intel i5 13600kf(watercooled).best upgrade I made for 1440p gaming.",Positive
Intel,"Congratz!  Also, what's that wallpaper? Looks awesome",Positive
Intel,I don't understand the stickers. Ryzen CPU Check. Both Radeon and GTX graphics? Is one discrete and the other dedicated? Just needs an Intel sticker to make it more confusing.,Negative
Intel,"That is not a ""gaming rig"" not even close my friend. ;)",Negative
Intel,3050ti?  Gaming rip?  No,Neutral
Intel,"5 or so years ago i built my first real gaming rig. Few years later i bought new laptop, because i already had a gaming pc i wanted something capable to game but didnt want to spend too much money for high end. So i bought Lenovo legion with same gpu - 3050ti.   What i soon realized was that i love the comfort of being able to browse the net and game from the couch, being able to put my legs up, put my back against a big pillow etc... Despite having much more capable pc, i was lazy to play on it and was using gaming laptop instead 90% of the time.   Problem is, 3050ti in the last year or so really starting to show its limitations, most of new games are straight up unplayable anymore. Even on lowest settings it doesnt run newest games at playable standards, probably because of only 4gb of vram. Hogwartz, Rachet and Clank, Remnant 2 all pretty much unplayable unless you drop resolution to 720... I really regret not putting few hundreds more for something like 3070. (unrelated, but why stepping up laptop gpu tiers are so expensive, for example 3060 to 3070 is like 300e here for essentially same system otherwise, 3070 to 3080 is like 4-500e extra more, its even more expensive overall than desktop gpu tiers for way lesser performance improvements...).   So my advice would be, if you can get at least 3060, its worth it to pay up a bit more, ideally 6600m or 4060, those few hundreds increase in price will be worth in a few years.  ALSO: not sure if OP knows, but these laptops come with shittiest ram, upgrading it to a dual channel dual rank is pretty much a must as it nets 10-20% more performance.",Neutral
Intel,There is nothing wrong with laptops I'd build my own if there was a market readily available for it like a desktop.,Neutral
Intel,No!,Neutral
Intel,"This isn't exactly what I'd imagine hearing the words ""gaming rig"" but I'm glad you like it. Happy gaming!",Positive
Intel,Ignore the haters dude. Congratulations on your new gaming laptop. Gaming is something that is supposed to make us happy not cater to snobs. Not everyone needs 4k 60fps. If it satisfies your requirements it's the best rig for you. I have a Ryzen 5 2500u and I game on that so I am super happy for you.,Positive
Intel,"People take things too seriously, and tend to forget that every enthusiast was once a novice.  This is OP's first gaming setup; they deserve to feel that excitement.",Negative
Intel,who even buys the 'very best' laptop anyway?,Negative
Intel,Lenovo Ideapad Gaming 3,Neutral
Intel,"It's just the default one that came with the laptop, theres a red and a blue version probs from AMD and Intel lol.  You should be able find them online.",Neutral
Intel,Its a ryzen cpu with integrated radeon graphics and an nvidia dedicated gpu. Its completely normal and not really confusing.,Neutral
Intel,APU + dGPU,Neutral
Intel,"You know what I mean.   It's my very first PC/Laptop set up so I'm definitely calling it a rig, it's way more powerful than anything I've owed before it.",Positive
Intel,"It games, it's a gaming rig. :)",Neutral
Intel,you do realise not every wants or needs a 4070 to play games they like,Neutral
Intel,I upgraded the ram. Has 16gb ddr 5 dual channel now,Neutral
Intel,"Thanks dude! I think people forget that not everone needs the same kinda thing from a computer which is why they sell multiple models.  This is great for what I need it for, and I feel like a kid at Christmas, happy to be able to dip my toe into PC gaming, finally!",Positive
Intel,You get my point at least you should,Neutral
Intel,"Just to let you know , these laptops have different versions , one I have is something like Ar15mhqp or some shit like that   It is less powerful than yours but decent enough",Neutral
Intel,That is okay if you see that way and congrats.,Neutral
Intel,"There's nothing ""rig"" about a laptop. Laptops aren't built to be rigged.",Neutral
Intel,"No, it's a notebook.  Also have a low end gpu dude.  Not a game rig. It's nice but is not that.",Negative
Intel,"I know, but the same laptop or any other with 6800m or other AMD gpu would have been a much better choice lol",Neutral
Intel,All that matters is that your happy with it,Positive
Intel,Glad you’re enjoying it. People something think that unless you get something with everything maxed out then you couldn’t be happy lol,Positive
Intel,"Yeah I'm just happy to be able to start playing around on some games on PC now. I've wanted to play flight simulator on pc instead of my xbox for ages now becuase of all the addons,and now i'll have access to those :D",Positive
Intel,"3050ti isn't really low end. And it will do for everything I need it for. I have an Xbox too for the most modern stuff.  People think that everyone has to have 4k 60fps in every game these days and not everyone needs or wants that, especially in a first machine.",Positive
Intel,"for you maybe, this fits my needs perfectly.",Neutral
Intel,"Yup I am more than happy!  I've only ever had and played with integrated graphics on any other laptop I've had so this is a huge step for me.  I will play most brand new stuff on my Xbox for the best ecperience but stuff like KF2 I can now play a 1080p ultra and get 100s frames, could only play that before at 900p low.",Positive
Intel,Well you say that right now but one day a game you really wanna play will come and your pc will be shitting itself trying to run it. If I had the money I would buy the best rig.   But if you don’t have a lot of money like me it’s all about price/performance,Negative
Intel,That is the important that you are happy but still the reality is one.  Just enjoy the notebook.,Positive
Intel,"not too similar but I can recommend grabbing dirt rally 2.0 on a steam sale with all DLC for 8€ / $10 or whatever it is, generally games that simulate some things are pretty much at home on PC",Neutral
Intel,"Yes, the 3050 ti is a low end card.",Neutral
Intel,I have an Xbox too so I'll be able to play it on there anything that is out of spec.,Neutral
Intel,I have a friend playing Baldurs Gate 3 on a 980/ You don't know what low end is apparently lmfao,Neutral
Intel,It's better than the 1650 and the 2050 though. And they were the other choices.,Positive
Intel,And that is normal the game don't requiere a big gpu to run the game is a mid / low graphics.  Besides the 980 was the top of that generation. The 3050 is the low end of that particular generation.,Neutral
Intel,"Dont let the haters bring you down dude, if you are happy, then enjoy your new laptop!",Positive
Intel,"Both cards are the lowest version of his generation.  Again the notebook is nice but is a low end ""gaming"" notebook and again the point is that you ENJOY IT and play many games.",Negative
Intel,He needs to know what he have but also I congrats OP for your purchase. The point is that you enjoy it.,Neutral
Intel,Keep em coming.  I saw Intel was hiring for a GPU stack software engineer in Toronto. I expect more good things for the Boys in Blue.,Positive
Intel,Does newegg have a pro or business section for the B50?  I heard CDW or similar outlets may start selling B50 / B60?  I want to look into four B60s for an AI workstation but the B60 may not release through consumer chains.  Has anyone seen the B60 for sale?,Neutral
Intel,Quad mini display port ? WTF,Neutral
Intel,It is so much weaker than the B580 though with only 70w... Very niche use cases.,Negative
Intel,On the other hand noone buys workstation GPUs on NewEgg. For B50 being bestseller means 95 orders.,Neutral
Intel,Looks like some stores have it available to order currently. Central Computers has it available as a Special Order.   [Central Computers -ASRock B60 24gb Creator $599](https://www.centralcomputer.com/asrock-intel-arc-pro-b60-creator-24gb-graphics-card.html),Neutral
Intel,Better than micro-hdmi.,Positive
Intel,The entire point is that you can power it from the motherboard.,Neutral
Intel,It doesn't matter. The card sells and that's what matters.,Neutral
Intel,Its a small workstation card like the Quadro 1000 etc..,Neutral
Intel,I guess I got lucky. I was able to get one off Newegg before they sold out. Should be arriving in the mail today.  Hopefully they will get more in soon. I'm surprised Newegg had so few of them.,Positive
Intel,or the usual supply chain is sold out so users buy them where they are available,Neutral
Intel,Those are professional cards. Your company should be getting them from different sources.,Neutral
Intel,"It's small but a decent performer, if you ask me. Intel needs to keep them coming.",Positive
Intel,A single slot version would've been nice.  70w doesn't need a dual slot cooler.,Neutral
Intel,12W idle power... here does the dream of low idle server... :I,Neutral
Intel,"I should have waited, i could have gotten two!",Neutral
Intel,Where can we buy this? Seems like limited supply and retailers are going to try and raise prices. All the tech companies need to keep the retailers under their boot.,Negative
Intel,"I know it's not for this, but how does it do at gaming?",Neutral
Intel,they need if they want to survive,Neutral
Intel,I was thinking opposite ... can we have full passive model?,Neutral
Intel,nvidias alternatives here are also dual slot,Neutral
Intel,yeah single slot and x16 for pci4 boxes,Neutral
Intel,"I haven't internalized the docs or tried it myself (on an A310), but apparently some folks have managed to bring idle power down quite a lot on Arc cards.    e.g. https://www.reddit.com/r/IntelArc/comments/161w1z0/managed_to_bring_idle_power_draw_down_to_1w_on/",Neutral
Intel,It's currently available for pre-order on Newegg and B&H.,Neutral
Intel,"Yeah, a multi-billion corp that's been dominating the CPU market for decades , with a 75% market share needs an entry level Pro card to survive. Take your meds.",Neutral
Intel,Something akin to a KalmX 3050,Neutral
Intel,"company that is falling apart, that had very bad run recently, firing a lot of crew, canceling huge investments that already costed billions.        Having foundries ... that no one want to use and not having load because products don't sell.             Big fall quickly as they have huge costs that cannot be easily scaled down.               Yes Intel is in very bad shape, it needs a good product that will be competitive and what is most important rebuild the brand that is in shambles now.",Negative
Intel,"There is good news, the employees got their free coffee back",Positive
Intel,"Way to go Intel, keep them coming.",Positive
Intel,[https://youtu.be/QW1j4r7--3U?si=zAaOvCnc8LnVU6Mw](https://youtu.be/QW1j4r7--3U?si=zAaOvCnc8LnVU6Mw)  From Wendell Level one tech   **The Intel Arc B50 is the Better Alternative to Nvida's A1000**,Neutral
Intel,"[The Intel Arc Pro B50 is designed for professional users, architects, engineers, AI developers, and video professionals, who need a stable, memory-rich, and energy-efficient tool for demanding tasks.](https://timesofindia.indiatimes.com/technology/tech-news/intel-expands-professional-gpu-lineup-with-new-arc-pro-b-series/articleshow/121288086.cms)",Positive
Intel,three fiddy,Neutral
Intel,>16GB VRAM  Fuck yes.,Neutral
Intel,"$350, SR-IOV and 16 GB of VRAM   Freedom from green for workstation users",Neutral
Intel,"I got a Dell Micro workstation with a 285, 64gb of ECC memory and their fastest SSD.  But I didn't get an Nvidia card as they seemed overpriced and the A1000 doesn't support 6k monitors!  The built in Intel graphics support higher resolution than the cheap Nvidia cards...  I'm going to buy this.",Neutral
Intel,Anyone have a link to where I can buy one at ?,Neutral
Intel,Where is link for it I been looking for this card 😭will micro have it,Neutral
Intel,"I would curious how this Card can perform vs RTX 3050 6GB in terms of gaming.  Since Slot Powered GPU are very rare these days on release which the demands are high for those.     And yes, i know, Workastion arent designed for gaming",Neutral
Intel,"Waiting for a single slot low profile card. Screw me I guess for choosing a case with such requirements. I already got the a310, but it is limiting to just transcoding with its small video buffer.",Negative
Intel,god do i wish this was an option for me,Positive
Intel,I want 4 of these.,Neutral
Intel,I wonder to ask who is main audience for this GPU?,Neutral
Intel,WHERE GPU,Neutral
Intel,"That's pretty attractive, need more benchmarks on it.",Positive
Intel,How would it compare to Blackwell RTX PRO 1000 and Radeon Pro W7400?,Neutral
Intel,"Sad to not see any gaming benchmarks, yes not intended but still would be nice to see",Negative
Intel,"8K resolution, but with compression? No, thank you!",Neutral
Intel,Just work on the memory bandwidth guys. Quadruple it (or at least double) and these will sell out like hot cakes.,Neutral
Intel,For gaming Nvidia RTX 4000 SFF will run circles around this. The only con is price. It's a titan of small form factor.,Neutral
Intel,"Looking forward to his Linux testing of this card, it could be interesting (And on that note I'm also looking forward to the B60 and the dual B60 cards partners are allowed to make).",Positive
Intel,was just thinking 70W and 16GB of ram no way is this a gaming card lol. awesome value though for an AI dev.,Positive
Intel,"Check out the level 1 techs video, on Newegg right now",Neutral
Intel,"Pre-ordered on Newegg yesterday (9/4), expected delivery 9/18 +-. Just checked back a few minutes ago (12:30am, 9/5) and shows Out of Stock. Don't really need it (and I ordered a ARC A750 on release day that I didn't need ...lol), so I guess it's a me thing.",Negative
Intel,"Still a valid question though. Good enough is, well.. good enough when you're on a primarily business oriented machine. Would be nice if it could run some titles at min or close to min quality settings.",Neutral
Intel,Timespy scores:  3974 - rtx 3050  4269 - hx370 890M  4485 - intel arc a380 with a310 cooler mod  4833 - intel arc 140t  6041 - rtx a2000 12gb  6551 - rtx a2000e 16gb  8514 - Intel ARC b50  9329 - 4070m 8gb @ 70W  11003 - rtx 4060  11261 - ryzen 395+  13564 - rtx 5060,Neutral
Intel,"this the card where that might be achievable,   id chuck the next gen equivalent into that sexy minisforum ryzen ai max pc thats coming soon..",Positive
Intel,"People running professional applications that require a lot of VRAM and precise compute.  CAD software, photo editing, video editing, digital painting, transcoding, broadcasting, servers.",Neutral
Intel,People who use shared remote login who do need gpu processing power while logged in remotely.,Neutral
Intel,https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007,Neutral
Intel,"This card is not designed for gaming. Or if it was, it's a joke to compare it to an A1000, considering that the A1000 is for a low end CAD or pro video workstation. Intel is not even trying to compete in the gaming market.",Negative
Intel,"There isn't much of a point to it, it's a slow card. It will be slower than a B570. It will be slower than an RTX 5050. For *gaming*, find a review for something that gets called a waste of sand for being too slow and too expensive, and this will be more expensive and slower than that.",Negative
Intel,"Based on specs and power draw, it should be around a GTX 1070  Edit: Based on Level1Techs' review it's probably more like a 1070 Ti, or somewhere around these two",Neutral
Intel,That's like asking someone to run a marathon in basketball shoes.,Neutral
Intel,level 1 tech did them,Neutral
Intel,This is not gaming gpu...,Negative
Intel,The gaming benchmarks have already been posted here.   [https://youtu.be/QW1j4r7--3U?si=zAaOvCnc8LnVU6Mw](https://youtu.be/QW1j4r7--3U?si=zAaOvCnc8LnVU6Mw),Neutral
Intel,"The B60 is supposed to have double the memory bandwidth (456 GB/s vs. 224 GB/s for the B50), and the B60 comes with 24 GB memory (And 192 bit vs. 128 bit for the B50).  Will be interesting to see the dual B60 cards that were shown previously that partners can release.",Neutral
Intel,"Shockingly, the modern [$1250](https://marketplace.nvidia.com/en-us/enterprise/laptops-workstations/nvidia-rtx-4000-sff-ada-generation/) workstation card beats the $350 workstation card... and at gaming...",Neutral
Intel,"RTX 2000 is pretty close though, but that is still double the price of this card.",Neutral
Intel,I would hope it would at that price point.,Neutral
Intel,Release Date: 9/18/2025  https://www.newegg.com/intel-arc-pro-b50-16gb-workstation-sff-graphics-card/p/N82E16814883007,Neutral
Intel,Fly by Night is one of the best,Positive
Intel,"I don't give a shit what its intended purpose is, still doesn't hurt to have game benchmarks.",Negative
Intel,\>but muh gaming,Neutral
Intel,"Agreed... But the B570 is 150w, and the RTX 5050 is 130w. What about SFF PCs without a PCI-E power connector? What about low power desktops running on battery (RV, etc.)? What about a cheap, compact eGPU that can use a PicoPSU as the power supply?  Currently, for slot powered, the options are slim:  * Geforce RTX 3050 has solid performance at 70w... but only 6 or 8GB of RAM. * Radeon RX 6400 at 53w is a little beefier than a Steam Deck, but not much. * Radeon RX 7400 just came out, is 53w, and is better still - but still limited to 8GB of RAM and appears to be OEM-only. * Alchemist A310 at 75w and 4GB is a joke. * Alchemist A380 at 75w and 6GB isn't amazing, and trades blows with the RX 6400.  An Arc B50 Pro ticks a lot of boxes. 70w means slot powered and not even pushing to the very limit. 16GB of RAM means you won't have massive framebuffer limits. [Geekbench 6 Benchmarks from Toms Hardware](https://www.tomshardware.com/pc-components/gpus/intel-arc-pro-b50-is-up-to-20-percent-slower-than-the-arc-b570-gaming-gpu-in-early-geekbench-tests-almost-doubles-the-a50-in-synthetic-tests) show 69890 OpenCL and 78661 Vulkan, which is slightly better than the 3050's 63488 and 62415 respectively.  The prebuilt eGPUs with a 7600 XT are decent. Only 8GB RAM, but the 7600XT easily beats the B50 Pro and 3050 at GB6 with scores of 83093 and 99525. But... it's $550-$600 for these. They aren't upgradable. And that 8GB of RAM is rough.  And sure, the Ryzen AI 395 with the Radeon 8060S is really solid... but Mini PCs with that are at least $1000, and that's getting a whole new machine. Thunderbolt 3+/USB4 ports are now quite common. Taking an existing laptop or mini PC from '720p low only' to '1080/1440 with moderate settings' is huge,",Neutral
Intel,It's slot powered no PCIe power cable needed,Neutral
Intel,What 2080ti is low profile and slot powered?,Neutral
Intel,"Intel had to trim pretty far to get the power down. below 70W.  Really nifty, but it does hamstring the memory more than I'd hoped.  Definitely looking forward to seeing what it's capable of, especially for the price.",Positive
Intel,Does rtx 2000 even exist. Didnt rtx become a thing around 4000s,Neutral
Intel,Already gone.,Neutral
Intel,Also an excellent album,Neutral
Intel,"A380 is definitely more powerful than RX 6400, unless the game has driver issues or poorer optimization on Intel arc. But yeah, nothing amazing really, except for the codecs. RTX 3050 LP is pretty much the only viable choice today.",Neutral
Intel,"Rtx 2000 ada sff was released at the same time as rtx 4000 ada sff. So yes, it does exist.",Neutral
Intel,I agree,Neutral
Intel,"I'm not aware of anything else that's this small, doesn't need external power, performs this well and has 16GB VRAM, and this isn't for gaming but it can game, as shown in the video. Would make for an interesting ultra SFFPC.",Positive
Intel,"I have a 5090 thanks, again its slot powered.",Positive
Intel,Oh yea okay.,Neutral
Intel,No you don’t.,Neutral
Intel,"Actually I do..... https://m.youtube.com/@TnTTyler   And other gpus and other systems, so try again",Neutral
Intel,"You love to call people kid, kid.",Positive
Intel,Brave attack there kid. Btw. You do know kid has more than one definition. It’s your actions and attacks that are childish. If you weren’t ignorant you’d know this but instead you’re accusing people of things and crying about AMD everywhere with one of your many accounts. Lame. Try again.,Negative
Intel,Whats this product codename? Still Battlemage?,Neutral
Intel,I miss the days when entry level and mid end GPU doesn't need external power.,Neutral
Intel,forgive my ignorance but this isn’t made for gaming right?,Neutral
Intel,A is for Alchemist.  B is for Battlemage.   C is for Celestial.  D is for Druid.,Neutral
Intel,"It's not made for gaming, however Intel drivers allowed it to run game. Kinda like what Nvidia did with their Quadro card.",Neutral
Intel,"Why does it always have to be ""gaming...  No, it's an entry level Pro card, for AI etc. While I'm at it, it is also cheaper and faster, has twice as much VRAM as the nVidia A1000 card.  From what I've seen, it can do 70/90fps at 1440p medium details in CP2077.",Neutral
Intel,">Why does it always have to be ""gaming...  because 'gaming' is a single task that is generally representative of realworld hardware performance in all aspects due to its unparalleled diversity, no single number will tell you as much about a gpu as the framerate will.",Neutral
Intel,"This day and age with crappy game optimizations, buggy engines and early access slop being sold as AAA full price ""masterpiece"". Highly debatable.",Negative
Intel,Am I just misremembering that the a750 limited had 16gb of RAM?,Neutral
Intel,"Such empty in the comments.  They need to release an A750 with 16gb in large numbers.  Intel is not producing many B50 / B60s?  Four A750s with 16gb each could be a decent AI workstation. llama4 requires at least 48gb, 64gb or higher recommended.",Negative
Intel,you're thinking of the 770,Neutral
Intel,They need B7xx cards. Releasing another Alchemist card right now would be pointless when there already is a 16GB A770.,Neutral
Intel,"why on earth would anyone buy A750 16gb in 2025-2026?  the A770 couldnt even beat the best of 2018 hardware, and their profit would have to be basically negative if they wanted to compete with AMD's 300-400$ cards nevermind nvidia with the cuda advantage.",Negative
Intel,Not surprising  It has 16Xe Cores (equivalent to 32CU/SM)   14Gbps GDDR6 memory   ~2.3Ghz core clocks   B580 has:  20Xe cores   19Gbps memory  2850mhz core clocks,Neutral
Intel,It’s a workstation so,Neutral
Intel,This is a workstation card.  Its drivers won't be optimized for games -- they will be tuned for things like Autodesk Maya and AI.  This rumor is totally immaterial.,Neutral
Intel,That colorway is actually... Cool.,Positive
Intel,$90-$100 MSRP? 🤔,Neutral
Intel,hope this helps drive down the prices of LP cards on the used market,Neutral
Intel,"I've one pulling from my uncle's workstation for testing before returning it to his case.    Cons: Even though it has 16GB Vram but the card can only match with 2060 Super/Vega 64 at raw perfomance (I've know it's using for training AI or workstation task) and laking driver, even MSI Afterburner was not reconized with this card...    Pro: The Benmark score on Superposition 8K optimized is 'bout 2100 to 2250 depend on how cooler the card is. God of War 2018 2K High Setting has 30-55FPS and play very smoth without lagging, Cyberpunk has bout 30-40FPS at 2K High Setting so it's playable.    So will you get one for yourself?   _Yes: It's card with 70W TDP (But runing at full benmark consume about 55W) so if you are using under 5 Litters sff case like NV10, A09M with small PSU like Flexguru 250W or Pico PSU 200W above, 250W Gan HD Plex,... or you wanna train small AI or developing or any workstation task that consume much Vram... Buy this card.   _No: The perfomace is not match with it price so better buy 4060 LP, 5050 LP, 5060 LP or much money buy A2000 Ada, A4000 Ada SFF...",Neutral
Intel,Arc Pro? Who's the target audience? Doesn't look like a gaming oriented card to me.,Negative
Intel,"They're comparing it against the B570, not B580.",Negative
Intel,And just 70W  https://www.intel.com/content/www/us/en/products/sku/242615/intel-arc-pro-b50-graphics/specifications.html,Neutral
Intel,Why do people who visit this sub assume everyone is a gamer?,Negative
Intel,"This used to be true decades ago (back then even the hardware was significantly different in some cases), but nowadays pro cards are quite similar. The ""special drivers"" usually just exist for certification purposes and are made from the same code as the gaming drivers. It's mostly product segmentation these days. So the pro cards tend to have more memory, sometimes have ECC memory, different form factors and different power/performance optimization. That's it.",Neutral
Intel,Doesnt really matter. This is just another cut down version of Battlemage. It will scale however they want to chop it up to. Like the Radeon Vega Frontier workstation  card came out and had mediocre performance so everyone outcried saying just wait for the gaming “optimized” Radeon Vega 64. It was the same card and performed identical.,Negative
Intel,Which part of geekbench said it is a gaming benchmark to you?,Neutral
Intel,MSRP for the Arc Pro B50 is $299.,Neutral
Intel,Professionals / AI freaks,Neutral
Intel,Low-end workstations and lab equipment. I had a similar low-end professional GPU installed on an [X-ray diffractometer.](https://www.malvernpanalytical.com/en/products/product-range/empyrean-range/empyrean),Neutral
Intel,The term “pro” is short for “professional”,Neutral
Intel,"Also depends on the price, if its under $300 then it's a pretty good deal as far as workstation cards go.",Positive
Intel,Because the vast majority of redditors aren't very smart people to put it mildly.,Neutral
Intel,"That's its typical usage.  But you're right, Geekbench is a shoddy cross platform bench, despite its typical use for gaming by bad reviewers.  That said, it is very unlikely that this card's drivers are tuned to Geekbench, unlike the mature drivers in the B570.",Negative
Intel,I bought my B580 2 weeks ago :(,Neutral
Intel,"Good enough a reason to get my buddy a b570 for his birthday, might just pocket the game!",Positive
Intel,You gotta be joking. Literally got a 14600K for $150 a few days ago. :(,Neutral
Intel,Hopefully this means BF6 will get an APO profile.  u/Aaron_McG_Official can you confirm?,Positive
Intel,I got Borderlands 4 with my 5090. Would rather trade it for this.,Neutral
Intel,Maybe its a good time for them to drop big battlemage on the market??,Neutral
Intel,I'm surprised hardware companies are still doing this. People can easily abuse this by getting the game for free and returning the product they bought. Well it works in places that have strong consumer protection laws.,Negative
Intel,"I bought my intel ultra 7 less than two weeks ago, damn",Neutral
Intel,Deep thought stimulation.,Neutral
Intel,"I wonder what retailers, will amazon be one of them?",Neutral
Intel,"That's a pretty sweet deal, especially for pc builders on a budget.",Neutral
Intel,Are latest Achemist/Battlemage GPUs faster than my 3080 at 4k?,Neutral
Intel,Is this live now?,Neutral
Intel,I bought 13700K on Jan 2023 can I get a serial as a compensate ?,Neutral
Intel,"Quick question: I got a laptop from Amazon with an Ultra 9 CPU, and Amazon sent me a code. However, no useful instructions on how to redeem it. Usually they have you download nvidia app or something like that, but this says to add the item to your cart and try to use the promo code? Tried that and nothing. Anybody else having trouble with this promotion? Is there an Intel app I need to download to redeem this code?",Negative
Intel,"Bought i7 14700 from amazon uk, yesterday, did not reciece master key from them to redeem offer",Neutral
Intel,Dang I just found out about this today but got my i9 cpu on the 24th of august. Luckily I just got a laptop so I get the code from there but it would've been nice to give one away.,Positive
Intel,"Bought a 14600K, only to be hit by a ""This offer is not available in your country"". There is NO region restrictions outlined in the promotion Terms and Conditions. Very sketchy.  Something that can be looked into, or do I need to return the processor?",Negative
Intel,Is this a Steam or EA key?,Neutral
Intel,"If anyone wants to flip their BF6 key let me know ill buy it off them at a discount, ive got a 13700 so think I can redeem it",Neutral
Intel,Is there a way to redeem the game without actually installing or having a 13/14 gen cpu? I got a 14600k in a bundle and want to resell it as new but I also want to redeem battlefield 6. The guy at microcenter says the system does a hardware check to make sure you have a qualifying cpu installed.,Neutral
Intel,I bought it at the height of the voltage controversy when the fix wasn't even known. Nothing for me then? 😐,Negative
Intel,Even if it's free it's not worth it.,Negative
Intel,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Neutral
Intel,Price scheme or they are that desperate.,Neutral
Intel,I’ll do this deal when my stock sells ha,Neutral
Intel,Mine just got delivered on Friday too….,Neutral
Intel,Return it and than rebuy it,Neutral
Intel,"That's still a great deal, even without BF6!",Positive
Intel,Same here lol. I got one to upgrade my sisters build.,Neutral
Intel,Can’t return it?,Neutral
Intel,Check you probably got the previous deal,Neutral
Intel,"Per Intel policy, I can't comment on unreleased products. Historically, APO has tested the top superlative and popular titles; if the team gets them to show improvement then they are added.",Neutral
Intel,"That's fair. I can trade you a B580 with Battlefield 6 for a 5090, you can even keep the copy of Borderlands 4.",Positive
Intel,"That would be amazing, but I doubt it.",Negative
Intel,It will likely come in Q4 2025 or Q1 2026,Neutral
Intel,I have a buddy at bestbuy and asked if this would work and he told me that when you go and return the product the price of the game will be withheld from your refund if the code has been redeemed. He did also say that would happen when nvidia included a code for star wars outlaws and I had no problems getting a full refund with the game redeemed,Neutral
Intel,Same man 😅,Neutral
Intel,Nope. 3080 is faster by a long shot.,Positive
Intel,Yes,Neutral
Intel,Anyone solved how to get/redeem the BF6 code?,Neutral
Intel,"I got the same error, hoping that it can be resolved.  Edit:  Fill out a support ticket indicating the error you got here as soon as possible, selecting the ""I have an issue with a title promo code"" issue topic: [https://softwareoffer.intel.com/Support](https://softwareoffer.intel.com/Support)  The ""not available in your country"" error is one of their FAQs: [https://tgahelp.zendesk.com/hc/en-us/articles/13531766299021-Unfortunately-this-bundle-is-not-available-in-your-country-If-you-received-this-message-in-error-please-contact-support](https://tgahelp.zendesk.com/hc/en-us/articles/13531766299021-Unfortunately-this-bundle-is-not-available-in-your-country-If-you-received-this-message-in-error-please-contact-support)  Their FAQ says to contact support and provide the following: receipt of purchase, master key and country of residence.  I did this and I am now waiting on their feedback. When you submit the ticket, you will receive an email confirmation which says you will receive a response in 3-4 business days. Fingers crossed they apply the offer to my account.",Negative
Intel,They perform a hardware check with a utility before allowing you to access the key in your account. There's an area for support so that you can send them information with your purchase instead.,Neutral
Intel,Is it fixed now?,Neutral
Intel,A megacorp that has almost 70% of CPU market share is so desperate that they're giving away a game with a purchase of one their CPU's. You're a genius.   I got Dying Light 2 with my old 12th gen.,Negative
Intel,"I remember getting FarCry 6 with my Ryzen purchase a few years ago. Right now looking at a local pc shop website, you can get Borderlands 4 with a 50 series gpu purchase.  I guess by your logic, both AMD are desperate for CPU sales and Nvidia is desperate for GPU sales ?",Neutral
Intel,If only there was something called a return period....,Neutral
Intel,How generous. But no thanks. 😅,Neutral
Intel,"I'll up it to a 1070, or my current 3070.",Neutral
Intel,So unlucky lol,Neutral
Intel,"Thank you! I did so a couple of days ago, and was already requested to provide that documentation. I was then replied back stating that the error has been fixed, and I should be able to claim the offer now. Sadly, I'm not in my country right now, so cannot test until next week.",Positive
Intel,Yup,Neutral
Intel,"These idiots also act like AMD never bundles games, I got Star Wars Jedi Survivor with my Ryzen 7 7700X.",Negative
Intel,"I got ""Dying Light: The Beast"" and ""Civilization VII"" as a gift from Intel with my Core Ultra 7 CPU.",Positive
Intel,Let's not pretend Intel isn't in desperation mode right now. The company is in the worst shape they've been in 30 years.,Negative
Intel,"Intel: here kid, have a balloon   Kid: what's in it for *me?*",Neutral
Intel,"> A megacorp that has almost 70% of CPU market share  Since you mentioned your 12th gen, on steam it's under 60% intel now, an 11% reduction in intel cpus in just 1 year. On amazon, there's [0 intel cpus in the top 10 best sellers.](https://i.imgur.com/LGwqOZs.png) Anecdotally the only high end cpus I see recommended in gaming discords is 7800x3d/9800x3d.  Obviously intel still has the lion's share in oem/laptop/server but that's been falling rapidly too, despite intel illegally bribing oems to not use amd products. Apple ending their partnership+their own laptop chips have been chipping away at intel's share too.  One company's been rolling losses year after year with falling market share, one is the complete opposite, the writing was on the wall a long time ago.  It took amd the better part of a decade to claw back from bankruptcy, intel needs to stop their attitude of looking for short term gains if they want to even start a comeback.",Neutral
Intel,"TBF, you shouldn't be looking at marketshare but percentage of new CPUs sold/ profit.  Marketshare just tells us intel was number 1 for a long time. Change in marketshare tells us they're losing ground fast.",Neutral
Intel,Yep that’s exactly what I did since they wouldn’t honor the promotion even though it’s during the return period,Negative
Intel,Well you said you would trade so…,Neutral
Intel,AMD literally set up a store at one point to give away games and GPUs with the purchase of a bulldozer.,Neutral
Intel,"What are you talking about. AMD has been on a brink of going extinct since the 90's with the exception of AMD64 (Clawhammer etc) days. 2005-2008 or thereabouts.  Second, every heard of anti-monopoly laws? It would be cheaper for AMD to pay from their own pocket to keep Intel afloat rather than let them go bankrupt. Why do you think AMD is still around after all these years. Intel will never fail.",Negative
Intel,"AMD's zen architecture was the company's final hail mary which is their last decade, ryzen being a literal play on words for risen (from the dead). Their own engineers admitted if it failed the company was going to go under.  >Second, every heard of anti-monopoly laws?  Same laws that intel has been consistently losing for decades?  >cheaper for AMD to pay from their own pocket to keep Intel afloat rather than let them go bankrupt.  That's not how anti trust/monopoly laws works. Company A going bankrupt because they kept shooting themselves in the foot doesn't mean that Company B automatically become subject to anti trust lawsuits. Nvidia isn't exactly paying AMD even when they hold 90-95% of the market. Legislative scrutiny tends to come from manipulative practices, not from just selling a better product.  Since you might bring it up, Microsoft propped up Apple because they wore forcing their own software through Windows, not because they were just simply selling Windows. In a twisted turn of fate, now Apple is going through anti trust lawsuits/legislation around the world despite holding a minority share of operating systems worldwide.",Neutral
Intel,That is surprisingly affordable for how much vram it has. I expected close to 3.5K.,Neutral
Intel,"Yep, I totally need this for plex",Neutral
Intel,Perfect for AI inference market Lip Bu Tan is targeting,Positive
Intel,Would this be a competitor to the Framework Desktop?,Neutral
Intel,You can now buy CORE 1 series cpus on desktop motherboards from ali.,Neutral
Intel,How did I not see this? Has anyone tested the B60? There seems to be extremely limited supply.  Maxsun does not have good support in the US?,Negative
Intel,48GB Vram,Neutral
Intel,It's just two B60 Pros. They're supposedly priced at $500 each if they'll ever actually be available for purchase.,Neutral
Intel,"Biggest thing in this update for me is finally they allowed iGPU memory allocation for Intel Core Ultra Series 1 & 2. This is huge!      I haven't tested this drivers on my MSI Claw yet, really excited to use this features because it can solve a lot of texture issue and low memory warning in the game where it only detect Arc integrated graphics with 128MB vram.",Positive
Intel,"I get crashing in ALL games when attempting to use the new Speed Sync option on this driver.  Specs are:  Ryzen 9700x, Arc B580, 32GB DDR5 RAM, 2tb SSD.  (Will make an actual post for more visibility)",Negative
Intel,Are the iGPU memory allocation settings in the BIOS or the Intel ARC control panel?,Neutral
Intel,"intel dynamically scale IGP usage and has a hard cap that is 57% of system memory. As per reporting by CapFrameX, Vulkan) maybe DX12 too, not sure) will allocate dynamically as high as the aforementioned hard cap but DX11 (legacy API) will stuck at 128MB VRAM reporting   I shall test this update on my MTL-H laptop",Neutral
Intel,Where is this settings at? Doesn't appear on my U125H,Neutral
Intel,Just used this by accident and i think i accidentallx bottlrnecked CPU on the 258v lmao,Neutral
Intel,"It's on Intel Graphics Software. On BIOS usually it will be greyed out and written as 128MB. Last time i remember Intel allowed iGPU memory allocation on BIOS is at skylake era, nowadays it's locked and it's done automatically until the recent update.",Neutral
Intel,"What do you mean bottlenecked CPU? iGPU memory allocation isn't affecting how CPU works because it only changed the amount of ram reserved to iGPU, not the speed.",Negative
Intel,Because the CPU had too little memory to work with!,Negative
Intel,"CPU has its own memory called as cache (L0, L1, L2, L3 and L4), it's nothing to do with RAM. The only thing RAM affecting overall performance is latency and speed.  The amount of RAM doesn't matter much, as long it has enough RAM for OS and application then it wouldn't make the system runs slow, let alone to make the CPU runs slower which is not possible.",Neutral
Intel,I am also questioning why is Intel still focusing so much on battlemage when celestial is supposed to be just around the corner.   It feels more and more like Lip bu tan axed Celestial.,Negative
Intel,The weird thing is when he did a keynote a slide noted dGPUs was part of Intel.,Negative
Intel,"Huh?  They launched one die, and haven't said anything about launching more.  How is that being focused on Battle mage?",Negative
Intel,Celestial IGP is pretty much done with `force-probe` option has on its way to be removed (driver promoted from experimental to mainline)   Celestial dGPU is pretty much unknown. If Tan wants to bet dGPU because Gaudi accelerators didn't sell (or badly marketed? too expensive? dunno) then he better not kill it. Arc Pro B50 and B60 received good AIB reception that they can go crazy and make whatever from it,Neutral
Intel,There is big battlemage coming in Christmas apparently.   Which worrying since celestial supposed to be 2026.   I guess we are lucky if we get celestial early 2027,Neutral
Intel,"This is some good news after the layoffs  Hopefully, the Arc DGPU Division didn't get cut too deeply and that there's still gaming Xe3 Arc cards and Arc Pro cards in the pipeline  Insane that Lip Bu Tan would gut so much of the workforce   The board must've been itching for layoffs after firing pat or Lip Bu Tan might think this is how you save Intel.  Either scenario is bad.  Deep across the board cuts like this is how you destroy a company as talented people and lazy underperformed are caught in the dragnet.  Irreplaceable talent will go to companies with better stock options, 401ks and compensation like Nvidia, AMD Qualcomm, ARM, Apple.  The board and Lip Bu Tan are fools for thinking this is a good idea.",Neutral
Intel,I wish that they just added way more memory instead of messing around with dual GPU,Neutral
Intel,Seems like a decent option for a low power PC,Neutral
Intel,Gorgeous.. love these builds.   How hot did she get?,Positive
Intel,I would buy HX cpus adapted to lga1851 with an interposer like the chinese 12900hx and 13950hx es chips.,Neutral
Intel,I. . .WANT. . .THIS!,Positive
Intel,The 192-bit bus and density of gddr6 limits them to 24GB per GPU chip max. They have to go multi-gpu to have more than that.,Neutral
Intel,Yes. You are right about the memory bus .  I completely forgot about that,Neutral
Intel,When compared to this  AMD intel and Nvidia with thier supposed Ai GPU's are just creating E-Waste . https://support.huawei.com/enterprise/en/doc/EDOC1100285916/181ae99a/specifications,Negative
Intel,"Just like motherboards have 4 RAM slots for 128 bit bus and only two channels, GDDR6 can also use clamshell mode to double the VRAM without going multi GPU.",Neutral
Intel,"Kind of a shame honestly. If bigger Battlemage does materialize, that 256-bit bus gets you a 32GB max capacity instead. 64GB per dual-GPU card.",Neutral
Intel,"Yes, and that tops out at 24GB for a 192-bit bus.",Neutral
Intel,Yeah..Shave really because there's a company which has developed an ai GPU which will have an ability to add more memory just like we do with system ram..  It's still in works and very long time to go until an actual release .   But they have demoed their FPGA card which very soon will be turned in to an ASIC,Neutral
Intel,We need Intel to be a viable competitor to AMD and nvidia. That means better prices all across the board and more options for us consumers.   I’m rooting for Intel in the gpu sector. If only they would get their shit together for CPU.,Positive
Intel,Amd is competitor with intel? XD,Neutral
Intel,… yes.,Neutral
Intel,"What the hell are you even talking about? CPUs? GPUs? Both? The lack of specificity is really irking me, and it shows you don't care enough about the topic to even be right about it.",Negative
Intel,"I work with AI, comfyui and other heavy things, the AMD gpu are 💩💩 there, good luck if you amd fanatic",Positive
Intel,The unintelligible ramblings of a madman.,Neutral
Intel,"Sure, a madman not amd fanatic 🤡",Neutral
AMD,I think the more valuable thing for AMD than this product in itself is testing the new chiplet interconnect on a hardware level which will probably guide Zen 6 development.,Positive
AMD,How loud are the systems pulling 180W?,Neutral
AMD,"The RDNA3.5 8060S found in Strix Halo, at a TDP of 120W (dynamic between CPU+GPU) performs in raster similarly to an RTX 4060 desktop (that's around 100W with a 9800X3D). The die housing the GPU, IO, media engine, NPU etc. is \~305m2 while AD107 is 159mm2. Though with power the sweet spot is more 50-85W, Perf/watt in gaming is impressive, but you can argue die area not so much.  Also, since just hearing the Nvidia and Intel news, honestly really excited for this space in RTX chiplets used for future Intel SOCs. RDNA 3.5 can hold itself well, but hopefully this pressures AMD to be more aggressive with their future SOC designs. They already have a new packaging direction, even going as far as redesigning Zen 5 CCDs on Strix Halo with their new IFOP replacing GMI, so going for X3D + not lagging behind their dGPUs not wasting time on RDNA3.5+ and going for RDNA5 and hell why not add 512-bit bus should be interesting.",Positive
AMD,"The CPU is obviously much faster than the one found on the PS5 even with less cache compared to desktop version, it is obvious that the limited power consumption as well as lack of bandwidth is clearly holding back the Desktop APU here.  I wonder when AMD can release version of this APU with built in GDDR7 Vram built in and unlocked power that can reach up to 300 Watts TDP... Given with enough cooling I wonder how this will perform against the PS5 Pro even.",Neutral
AMD,"I see this more competing against Apple chips than anything else. They seem pretty impressive considering you can game on them properly and do everything else as well including ""productivity"". Depends on price though and what devices they end up going into.",Positive
AMD,"There are about two dozen mini PCs with this chip and only one shitty  overpriced 14 inch HP laptop with it. Fuck AMD for giving HP exclusive rights to that chip for laptops. As shitty as Intel and Nvidia can be, you can trust that they'll make their chips available to everyone. They don't make you wait a year until an exclusivity deal is over to finally get access to the chip. I hope they make AMD pay for this kind of anti consumer practice.",Negative
AMD,"AMD has a lot of low hanging fruit with whatever follows this that will easily have it beating a desktop 5060 or PS5 pro  1) Still not using an RDNA4/4.5 gpu. Just doing that should give 30-50% better frame rates especially with RT on  2) Still haven't used V-cache, that will add 10% or more and should decrease power draw also  3) Still using old IO die with only 8000 MT/s. They will can go to 11000MT/s and gain another 30% memory bandwidth   4) Can easily give in the same cache as regular desktop ryzen which should add 2-3%   5) They could go with a single 8 core CCD which would cut power draw and give the GPU more power. Zen 6 is rumored to have a 12 core CCD which will give a single CCD config much better performance while keeping latency down  6) LPDDR6 should give much more bandwidth also and improve performance  Just the first 2 which can easily be done today would put this past a 5060 if not 5060ti, so substantially better than a laptop 4070 and would crush PS5. If the benchmarks in OP are correct that means it's likely you could cut it to 50W and still beat desktops using 250W",Neutral
AMD,"Kind of a pointless product unless they manage to reduce manufacturing cost (which is hard since it has a big die, compare to even dGPU die).   Looking for a gaming laptop? A RTX dGPU will be better any day while costing you 1/3 the price  The only good potential use now is AI, with big unified memory. But again AMD software stacks are behind Nvidia.   I see many comments still stuck with the old iGPU mindset (being cheap).",Negative
AMD,It’s powerful because it has “AI Max+” in its name.,Positive
AMD,Good question  [https://youtu.be/uYLwDkGZOJk?si=4cQxYgWc12uQ0SLU&t=703](https://youtu.be/uYLwDkGZOJk?si=4cQxYgWc12uQ0SLU&t=703)  Found this for a Cinebench run full throttle.,Neutral
AMD,Robtech's review measured 50 decibels at 30 cm away for the performance mode. The balanced mode measured at 48 decibels.,Neutral
AMD,If you are pairing AD107 with an AMD 16 core 9955HX (which would be the closest equivalent) then you need to account for the 122mm2 I/O die.  Strix Halo uses only a little more silicon than your hypothetical.,Neutral
AMD,"I still think best case is they utilize a form of hybrid cache, whether it be L4 or shared x3d, to aide in the igpu ram bandwidth issue.  A 16cu RDNA4 igpu with on soc cache would be a force in the low power/portable sector. And the design could easily allow larger cu counts with less bandwidth constraints",Neutral
AMD,Sucks it will take a few years minimum for anything from intel + Nvidia to bear fruit. Waiting sucks lol,Negative
AMD,"i really wonder if we can't have a product segment that is no (or only token) iGPU in the SoC, but with oodles of memory channels (soldered or CAMM or what have you I couldn't care less, ideally HBM lol) and pcie gen 5 or gen 6 x16 (or two x16!! ahhh yesss)  Unified system memory designed for use with dedicated GPUs, trimmed down compared to server platforms but offering similar main memory bandwidth.",Neutral
AMD,"GDDR7 uses 44 data lines for a 32-bit connection, and runs at a much higher bitrate along with using PAM3 encoding.",Neutral
AMD,The tech is impressive but still far from perfect. The cost is a real barrier and is mostly being attracted by the local LLM crowd. The miss for Halo in this space is from the bus width. An M4 Max has double the bandwidth and while compute is competitive not filling out the checkbox for bandwidth doesn't leave much confidence. Also their decision to stagnate RDNA 3.5 just leaves out a lot of perf when RDNA4 is arch is shown to be great.  Not only that AMD has many other priorities needed to be checked like their media engine and HW acceleration like RT which is equally a huge chunk why Apple silicon sells not just gaming which is very small in comparison,Negative
AMD,You can game & do productivity on dgpu laptops which are cheaper for many people's perf/$,Positive
AMD,Is the 2025 Asus ROG Flow Z13 unusable as a laptop?,Neutral
AMD,"Where are you getting this exclusive nonsense from? There are like five products in the world with this chip, two of them laptops and those are from different manufacturers.",Neutral
AMD,"V-cache only makes sense with TOP GPUs. Paying $200-300 extra for a feature that only gains 0-5% performance increase with such a weak GPU make zero sense. See HUB's recent CPU scaling video.   Jarod showed that the extra cache (9955HX vs 9955HX3D), when paired with much faster **5090** laptop GPU, barely showed a 6% improvement at 1440p. So yeah, cache makes no sense with an iGPU so AMD will not release this in the following years. Maybe with a 512-bit iGPU when that's ready.",Negative
AMD,These things are high end AI workstations and are being sold as such. The gaming part of it is basically irrelevant when people are using these for work and not as a toy.  Nvidia has zero products in this segment by the way. You need to spend $3000+ to get the kind of VRAM needed to compete here if you want Nvidia.,Negative
AMD,"Well to be fair this is one of the few ""AI"" branded things that people actually use for running AI stuff locally.",Neutral
AMD,Aren't GDDR6 PHY are much larger than a ddr5 ones,Neutral
AMD,"MALL already fixes that, it already has 32mb. The real problem is a size constraint and AMD is better off improving their memory subsystem which is what they're going for",Neutral
AMD,"Shared cache is a lot harder to implement, Intel tried it once and dump the tech and pretended it never existed(desktop broadwell).   AMD had it separated for over 10 years for a good reason, and I assure you they have more masters and doctorate under their helm than any random redditor. There is a reason why they have not done it despite being in the game for so long.",Negative
AMD,"Yea for the price it doesn't really make any sense at least not to me. If it was around the price of a console it would make a lot more sense. Not sure why anyone would buy into Apple for gaming. Even on native games performance leaves a lot to be desired, at least from the benchmarks I have seen.",Negative
AMD,>  The cost is a real barrier and is mostly being attracted by the local LLM crowd.   AMD could lower costs by cutting the CPU in half (8 cores is enough) and removing the NPU.,Negative
AMD,"APU should always be cheaper, but fact that it is not shows poorly on AMD and Intel, who want to double dip into dGPU market by not pushing forward APU performance and bandwidth.  Apple has nothing to lose in dGPU market, so they are the only ones racing ahead on SoC market.   MacBooks are best for perf/$, at least excluding the heavily milked highend/RAM upgrade stuff. Same with PS5 and Xbox X with APU, best value for money in gaming.",Neutral
AMD,That's a tablet form factor with terrible thermals.,Negative
AMD,It’s an exotic way to shit on AMD,Negative
AMD,"It has the same performance in Moe models as a mainsteam cpu with fast ddr5 and a 3090. The output performance... The prompt processing is just bad.  For dense models the bandwidth is too low.  For any other type of AI, anyone sane will get something with CUDA.  It's almost useless.",Negative
AMD,High end AI work station lmao  Last time I checked it didn't even have ROCm support,Negative
AMD,">Nvidia has zero products in this segment by the way.  That's not true. NVIDIA started it. They have a long history of SOCs that combine a decently powerful GPU with a CPU. There is the Jetson Line and the DGX Spark will follow soon.    Jetson is basically the same concept as Strix Halo, the Orin was Ampere based and released 3 years ago and is still pretty competitive in performance. Recently, the Thor released and is already superior in pretty much every way.    DGX Spark adds scalability and enables clustering of multiple mini PCs  The reason why they never got much attention was the fact that they run Linux and are not suitable for Gaming (nvidia showcased Gaming with Raytracing on ARM on Linux though). But that doesn't mean they don't exist.    That's also the reason why they're working with intel now, to release x86 versions.   NVIDIA is the defacto market leader in that segment as of today",Neutral
AMD,"I thought it was the opposite, at the same bus width",Neutral
AMD,AMD talked about the MALL cache on this chip can be used by either CPU or GPU. AMD decided to give it to the iGPU. But it's something they can change. Would like the ability for the user to configure this personally.,Neutral
AMD,"I still am not sure why broadwell L4 was dropped so quickly. those chips were gaming monsters in their day. and back then igpus were pitiful.  I spoke with another poster many months ago with solid knowledge (i believe he had an engineering background) who stated how and why hybrid cache was doable with a few tricky bits.  We already know infinity cache works great, the question becomes implementing it in a form on the SOC where its useable by both cpu and gpu.  As for the 10yr separate thing. there are a lot of things they are doing now they never did before. A lot of it comes down to refined ways, as well as nodes getting to a point where the designs become feasible.",Neutral
AMD,"AMD does make a single chiplet version (the Ryzen AI 385) with 8 cores; the issue is though is that the CPU chiplets are pretty small compared to the i/o, NPU, iGPU die - 71 mm2 for 8 cores vs 307 mm2 for the other die, so dropping one chiplet doesn't actually save much in manufacturing costs (the cost to AMD for one such die is probably like \~$25).  Granted, that second die could have been a fair bit smaller if they didn't include that stupid NPU unit in it; we can blame Microsoft and the laptop OEM marketing teams for that, they have been heavily pushing the Copilot+ branding and that needs a large NPU to sit around and waste silicon.",Neutral
AMD,MacBooks are best for perf/$?  Consoles have always been best value since they're usually sold at loss at first and pretty low margins (if you dont play online)  But PCs are better value since you get 2-1,Positive
AMD,It's unfortunate that even a long-term launch partner like Asus had to resort to such hacks like a weird top-heavy form factor just to get supply allocations.,Negative
AMD,"No, I was speaking about this with respect to Token Generation, not prompt processing. Token Generation scales with memory throughput, which doesn't give CUDA any reasonable advantage in any LLM benchmarks I've seen. People choose Nvidia because consumer cards and older Enterprise cards are just easier to obtain. This also why cards like the P40 and Mi50 still have relevancy today.  I am talking exclusively about consumer based solutions for agentic coding workflows. In those scenarios everything is a mess and these are really the only competition that exists to ageing cards like the 3090 which demand multiple in parallel to use modern coding models, and massively overpriced and overpowered cards like the 5090 which are only purchased because they can barely fit Qwen3 Coder into VRAM.  I really would love to see what you recommend for a coding llm workflow at $2000 and how many tok/s you'd get from it and tell me with a serious face that these aren't the best bang for the buck when it comes to performance/wat, and when it comes to buying new hardware. Oh by the way power costs exist, and I'm really tired of people saying that 2-4 300W cards is better than something like this especially when I'd be spending near what? $0.50/hr to run a setup like that?",Neutral
AMD,Got a link for that 3090 with > 100 GB of ram?,Neutral
AMD,ROCm works great on it. Im running Comfy UI and all sorts of image generation on mine right now.  It's literally just 3 commands to download and install ROCm + PyTorch: [https://github.com/ROCm/therock?tab=readme-ov-file#installing-from-releases](https://github.com/ROCm/therock?tab=readme-ov-file#installing-from-releases),Positive
AMD,"Sorry for getting to this late.  My 7950X3D's iGPU has ROCm support, these absolutely do have ROCm support (you just have to enable the override). ROCm gets a bad rep for reasons beyond my comprehension - probably because the older drivers were so bad and because it's equally as hard to get set up as CUDA.  In either case, Vulkan as a backend is _very_ good.",Negative
AMD,Is this true?  How do people keep claiming these are AI workstation products then?,Neutral
AMD,"You have no idea what I am talking about. Products like the Orin (I own an Orin), Spark, and Thor are all arm based solutions. They do not really matter in the space I'm talking about (consumer LLM usage). Right now some of the best devices you can get in the consumer LLM space are these Strix Halo APUs (that and Apple's M4 chips). Nvidia doesn't have a direct competitor and doesn't attempt to get one because they are very ARM and Enterprise centric. ARM because of their long history with ARM and the technology they leeched off of Mellanox (I have used Bluefield 1/2/3s) and Enterprise because that's where the absurd profits are. Gamers see no value in the massive price inflation that happens past 24 of VRAM, especially when Coding LLMs really need 32/48GB of VRAM these days for best inferencing.  There is exactly 1 consumer GPU from Nvidia that can do 32GB of VRAM and it's the 5090. AMD has Strix Halo. Apple has numerous Apple Silicon products. Yes these iGPUs perform _very_ well with LLM workloads because most LLM workloads are heavily bottlenecked by IO throughput and unified memory delivers that.  Everyone knows that 32GB+ of unified memory (or dedicated VRAM) is in demand but gamers because gamers are using toys and not doing actual work.",Neutral
AMD,"I wouldn't say that it has the same speed if i was talking about models that fit in a 3090. The 3090 would run circles around it.  Moe models at around \~80-100GB with llama.cpp will run at the same speed in a ddr5 100GB/s bandwidth mainsteam system with some parts of the model + the KV cache in a 3090 or 5060ti. You will also have 16-24gb more free ram for other uses and far better prompt processing  It may look as a good option, but it doesn't offer anything for AI, except for portable devices.",Neutral
AMD,"It's people who don't use the tools and are trying to act like they have knowledge /experience in an area they don't.  If you're talking about inferencing llama.cpp supports overriding just fine. And if you use a vulkan backend, I've even run inferencing on polaris and iGPUs.  https://github.com/ggml-org/llama.cpp/discussions/10879  (Note: You need to consider the small size of the model being used here - models people use for coding workflows are usually much larger (e.g. 20-40B is not out of the ordinary these days).  Is a good point of reference of the performance of different platforms under inferencing workloads. Traditional review sites (e.g. Phoronix) are way behind the curve on providing adequate benchmarking results as the drivers are constantly changing (see the Arc results in the thread).",Neutral
AMD,"Check for yourself. Sure you can make it work, but buying an expensive hardware, with AI in the name, and not having it listed on the official support list is a fcking joke  https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html",Negative
AMD,">You have no idea what I am talking about. Products like the Orin (I own an Orin), Spark, and Thor are all arm based solutions.   I literally said that myself. That's why the intel announcement is so relevant.   >They do not really matter in the space I'm talking about  Then, define what space exactly you're talking about.   >that and Apple's M4 chips  You're trolling, right? Do you know that Apples CPUs are ARM too? You say ARM SOCs from nvidia do not count because they're ARM but ARM CPUs from Apple are ""*some of the best devices you can get*""?  >Yes these iGPUs perform very well with LLM workloads because most LLM workloads are heavily bottlenecked by IO throughput and unified memory delivers that  No, not really. The only benefit from strix halo is offering lots of relatively fast ram but performance really lacks behind nvidia and Apples offers.  Why exactly don't Digits Spark or Jetson count?    You're throwing both development work on LLMs (professional workload) and consumers (which you cannot define?   NVIDIAs jetsons can be used as a desktop PC with Linux it and they can even run games. But Software support can still be an issue in some, especially windows. But that's why nvidia announced x86 versions literally today.   Most LLM devs run hardware like this as servers anyway. There is no point in giving each developer its own training system. And most want to LLMs on Linux but still use Windows for their desktop.   But yes, for people who only want one system, do heavy llm development but don't care that much about performance and don't mind having their desktop system doing the heavy work all the time, can only run Windows but don't mind paying a lot of money, Strix Halo is the only option.   The key difference is that AMD advertises them as gaming/mobile systems, nvidia doesn't. But have a look at r/LocalLLM, r/LocalLLaMA etc..",Neutral
AMD,It's weird how much attention these products are getting in that case...,Negative
AMD,"You're just trying to mince words here. I am talking about consumer products, not tinker or prosumer parts. No consumer is going to spend $3500 for an AGX Thor, just like nobody bought the Orin for $2000. These products are used in industrial automotive systems (I know, because I maintain a custom kernel to put on them).   > Apple is an ARM product.  You totally missed the point of my post. Apple is a consumer ARM product, not an SBC.  I'm sorry, I'm only skimming your posts as I don't really have the attention span to read half dozen paragraphs about this.",Negative
AMD,"5600F is a terrible name. It should mean Ryzen 5600 without iGPU, but of course the 5600 already lacks one. They could've used almost any letter except X, G and F...",Negative
AMD,"Guess AMD will keep doing new am4 releases till am5 dies lmao. I wonder what happened with the whole bios problem to support new am4 releases? I have a gigabyte A520M DS3H - REV 1.3 and[ its support page doesn't even list the 5500x3D](https://www.gigabyte.com/Motherboard/A520M-DS3H-rev-1x/support#support-cpu), which is the CPU I want to upgrade to from my 4500.",Negative
AMD,the boost clock is quite a bit slower than the regular 5600 (4ghz vs 4.4ghz)  i wonder why they didn't call it a 5550 or something instead,Negative
AMD,"At this point this is almost farcical. AMD hasn't designed any new Zen 3 dies in years (for obvious reasons), so they just keep releasing ridiculously minor variations of the same Vermeer and Cezanne silicon. This is what, the 8th or 9th desktop hex-core Zen 3 SKU they've released now? 😂",Negative
AMD,How low can they go?  * Ryzen 5 5500 CPU ? * Ryzen 3 5400 CPU ? * Ryzen 3 5300 CPU ? * Ryzen 3 5200 CPU ?,Neutral
AMD,I just wish they'd just make a 5950/5900x3d. Would be a nice sendoff to AM4.,Positive
AMD,Somehow Palpatine Returned,Neutral
AMD,So disingenuous to say am4 is still “supported” with these niche releases no one ask for,Negative
AMD,"who even cares about this? This fake ""extending life"" story is getting old.",Negative
AMD,At this point amd is just selling defective chips for am4. Way to milk the people.,Neutral
AMD,"This is fine, the 5600T, 5600XT, 5800XT and 5900XT were actually very bad for AM4, cause AMD just withheld perfectly functional CPUs and applied a meaningless overclock to them to sell them as a new SKU for a higher price.       This one seems to be actually made from defective 5600 dies so this is totally fine, the only other option is to throw them away cause they can't match the spec of any existing SKU.",Negative
AMD,If older motherboards gets updated BIOS I see the new AM4 CPU as a win.,Neutral
AMD,"Hello swordfi2! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,"I don't know why anyone would be up in arms about this. I AMD can make a new sku off of a lower binned chip and sell it at a lower price, where is the downside here?",Negative
AMD,"All of these zombie silicon releases stopped making sense after the 5600X3D, not counting the 5700X3D because that can't clock high enough for the 3D V-Cache to really be useful. If they had just ended it with the 5600X3D or the March 2022 batch (which was the one that introduced the great CPU ever) then AM4 would have gone out with a bang in my opinion.",Negative
AMD,0 days since last AM4 CPU reveal,Neutral
AMD,"Always great to have options. My guess is this will be in some budget prebuilds as well. AMD was smart to not kill AM4 in early/mid life of AM5. When you hop on AM4 and use AMD as a platform it's more likely you will continue to do so with next gen platform - so folks who go online for advice will either get the ""Go AM5 + 7500F/7600 for budget, great uplift"" or ""Skip AM5, jump to AM6"" it's a win for AMD in both cases.",Positive
AMD,Should've use either S or T as it's once use to differentiate the standard clock SKU or reduced clock SKU.,Neutral
AMD,"I would say Ryzen 5 5500X or 5550, but then the problem is that it clocks lower than the 5500.   Higher clocks but APU core with less cache and only PCIe Gen3, or Full fat Vermeer with all the (standard) cache and PCIe Gen4 but in its lowest clocked form. Pick your poison.",Neutral
AMD,5600O it is.,Neutral
AMD,"The F, is for Fuck intel.",Negative
AMD,Tbf isn't it intel who uses the F naming scheme to denote there being no IGPU. I'm not saying the naming isn't stupid i am just pointing that out.,Negative
AMD,"Does it need a new BIOS? Email support and ask.  For the 5700X3D, a new BIOS may not have released as it was supported under the 5800X3D BIOS.",Neutral
AMD,"""basically the same"" CPUs don't necessarily need new BIOSes.",Neutral
AMD,It might be way Intel decided to get in on the fun and bring back the 10th gen 10400f as the Core i5 110    14+++,Neutral
AMD,"its likely just their strat now  no actual value parts, but keep all mistakes / seconds in a warehouse somewhere and when the next gen comes out, they become the value parts and thats it  when AM6 comes out, AM5 becomes what AM4 is, and really we have seen this already in AM5 with discounted 7000 series coming out in force from ali",Neutral
AMD,"""new"" releases as in downbinned SKUs leftover from other production?",Neutral
AMD,">  I wonder what happened with the whole bios problem to support new am4 releases?    Same as it always was, it was a storage capacity issue on the early boards. if your mobo doesn't say it supports it, it never will; unless you modify the bios files yourself, which is do-able.",Negative
AMD,was never a problem.,Neutral
AMD,AMD doing to AM4 what Intel did (and doing) to 14nm.,Neutral
AMD,"Hmmm, I assume that it retains the 5600's higher L3 cache amount? Looks like the specs in the link provided don't confirm this tho..",Neutral
AMD,5500X or 5550 would have been a more understandable name... but then a problem is that it clocks lower than even the 5500. So then it's a battle between full fat Vermeer but clocked in its lowest form vs. Cezanne with half the cache and only PCIe Gen3.,Neutral
AMD,"These are just chips that failed binning. When you're using a manufacturing process that have good yields, your binning only gets a few that don't pass the requirements in each bactch. It takes time to amass stock good enough to sell as a lower part.  This is pretty much why we don't see Ryzen 3 these days. AMD doesn't have enough to sell because TSMC has a really good yields for them.",Negative
AMD,What could have been if the zen4 and zen5 ccd are compatible with am4 io die,Neutral
AMD,"They get praise for it, so it works for them. It would be one thing if they were pushing new heights of performance (like 5800X3D), but no, this is a PR push that works for them somehow.",Positive
AMD,"5500 already exists, it's a 5600G without the IGPU. it was rather unpopular in its first six months of existence, but later on it got cheap and underwent a bit of a redemption arc from what I have seen.     as for the Ryzen 3 parts, if those ever come into fruition then it's far too late to to helpful, given that I've seen the 5500 for as low as $60 brand new.",Neutral
AMD,"Iirc, AMD did have an engineering sample that's a Zen 3 part with x3D on both CCD's.",Neutral
AMD,"I honestly don't think we're going to get that but I would be totally on board for Ryzen 3 5400X3D because we never got a ryzen 3 normal chip out of the 5,000 series except on mobile this would be perfect.",Positive
AMD,"That would be the perfect upgrade for me right now, really want to upgrade my 5900x but sadly the x870 boards are still a little to expensive here on brasil.",Negative
AMD,"Im sure there's data in there somewhere behind the decision.    I just upgraded to a new AM4 CPU, and I have no intention of running AM5, but I've been saving up for the big build once AM6 comes out (or a year after)   Im sure there's many people who feel the same way about their existing AM4. Worth upgrading the CPU but not worth upgrading mobo + ram",Neutral
AMD,What's fake about it? It's still incredibly popular due to the continued support.,Neutral
AMD,Are you kidding? People love this,Positive
AMD,"Imo, the main benefit is that the CPU's are still in production, which puts enormous pressure on the used market and increases the supply overall in the long run. And as new generations come out, the last generation prices get lower and lower.  Often, the fastest generation for a socket ends up with a steep premium on the used market after production ends, but Zen 3 has been sold for long that there's a massive supply of it. I wouldn't be surprised if there's more units of Zen 3 in existence than all prior generations combined.",Positive
AMD,"All less-than-highest-end CPUs are ""defective"", with fuses burned off to disable parts of the chip that didn't pass QC to sell it as a lower model tier. Both Intel and AMD do this. Hell, nVidia, too. Their lower tier cards are often higher tier models with less shaders and a lower clock.  It's not a conspiracy.",Negative
AMD,"you want them to throw away the chips instead? so you want higher prices for the non-""defective"" chips.",Neutral
AMD,Literally how all chips from the second down are. They aren't throwing away something that costs them money because a core or two is defective or doesn't meet clock targets.,Negative
AMD,Every single Ryzen CCD chiplet is quite literally either a QC-failed or downbinned Epyc CCD.,Negative
AMD,Isn't the 5900XT just a weaker 5950X?,Neutral
AMD,"* 5900XT = 16C, cheaper 5950X * 5800XT = heavily discounted after the 5800X went OOS  Right now the 5800XT is still available for about the same price as the 5500X3D, and the 5500X3D is crippled enough (6C @ 3.9GHz) that it wouldn't beat the 5800XT in many games.",Neutral
AMD,"the 5600T was especially pointless, I have not seen ONE out in the wild (i.e. any physical stock at retailers) or let alone someone buying them. I think I saw an amazon listing for a 5600T once, but it was rather sparse.",Negative
AMD,None of what you said makes 5600T etc bad. These are AMD's products not ours they are free to do with them whatever they want.,Negative
AMD,"You are clueless . You have less than zero idea what you are talking about. ""cant clock high enough for 3d v cache to be useful"" it was 5% slower at maximum than the 5800x3d, and at the price difference already become the better option between the two long time ago.",Negative
AMD,"There already exists a 5600T, so I suppose we're left with 5600S.  At least, according to your argument.",Neutral
AMD,"5500X3D isn't a Cezanne but they called it 5500X3D anyway, so there's that",Neutral
AMD,"Nope, AMD do the same since they started adding iGPUs to Ryzen with the 7000 series. For example the Ryzen 7500F is a slightly weaker 7600 without iGPU; there's also Ryzen 7400F, 8400F, 8700F and more.",Neutral
AMD,The 5600x3d never got a release either. It works fine since it's just a 6 core version of the 5800x3d. The 5500 is the 6 core version of the 5700x3d,Neutral
AMD,"Some SKUs do. eg The 5500GT needed an update to add support despite being just a -300mhz base clock 5600G.  Afaik none of Gigabyte's A320 chipset boards support it, except for the ""A320"" boards which use relabelled B350 chips and so used bioses from the more frequently updated B350 lineup.",Neutral
AMD,"14nm+++++++  (one for each 14nm re-release, that being Skylake, Kaby Lake, Coffee Lake, Coffee Lake-R, Comet Lake, Rocket Lake, and now Comet Lake 5 Years Later)",Neutral
AMD,"They forgot the ""motherboard compatibility"" part.",Neutral
AMD,There's only so much cutting edge TSMC production to go around so if you can serve parts of the market with the old nodes it's better than those customers going to competitors.,Positive
AMD,Pretty awful comparison. Supporting a platform for a decade certainly isn't like anything Intel have done or will be doing anytime soon.,Negative
AMD,Same cache as 5600. Official spec pages confirm it  https://www.amd.com/en/support/downloads/drivers.html/processors/ryzen/ryzen-5000-series/amd-ryzen-5-5600f.html  https://www.amd.com/en/support/downloads/drivers.html/processors/ryzen/ryzen-5000-series/amd-ryzen-5-5600.html,Neutral
AMD,"I went from a 5500 to a 5700X about a year ago. Upgrading to AM5 would have been much more expensive, I think I'll also upgrade soonest in a couple of years. The performance is fine, but I'd love to have more and faster PCIe lanes.",Positive
AMD,"because this is CPU from 2020, which is also gutted and downclocked to oblivion?",Negative
AMD,theres no continued support. this is just AMD cleaning out the trash bin.,Negative
AMD,"5600F, 400mhz downclocked, already aging 5600. Just buy a 12100f and call it a day. Both are dead platforms anyway",Negative
AMD,5090 is literally a defect. It is only 90% working. The full working die is in those 10k dollar gpu called rtx 6000 pro.,Negative
AMD,i want them to stop pretending they are doing this to support the platform.,Negative
AMD,"You're right, I mis-remembered",Neutral
AMD,I just built a system with one for myself.  I've found the 5600T to be more readily available than the 5600 and its 1$ more expensive. I guess it depends on the countries we live in,Neutral
AMD,"True, it's a correct business decision, but it's still bad for the customers.",Negative
AMD,Oh i was unaware of that sorry for being stuoid and trying to correct you i usually know everything about the about any PC hardware that is somewhat recent i just have somehow managed to not see anything about the existing of any F skew AMD CPUs.  Did they release them a good bit after the initial launch or something?  I agree with you that having a 5600F is just stupid naming they should have just called it like a 5500L or something for low power or low clock speed or something.,Neutral
AMD,"No the bigger problem with a320 boards most of them wasn't the slowness of the update it was that they put the smallest possible bios chip on there so they can't add new CPUs to a lot of the boards without starting to remove the old ones they started off by removing the old bulldozer based APUs, but if they wanted to add the newest Verizon that have to start dropping Ryzen 1000 and possibly 2000 support.",Negative
AMD,"yeah, its an optimization of the stack that AMD did that I think Intel needed to do for the turnaround, esp with chiplets that can scale up and down.  I think its a good idea if the pricing was good, that means bottom of the market naturally gets better over time if your top end is pushed.  Which is strange why they haven't been doing the same thing for GPUs esp as they exit top end GPU market, but hey, what do I know.",Neutral
AMD,"This isn't ""supporting"", it's refreshing. Besides Intel also recently released i5 10400 as  Core i5-110.",Neutral
AMD,"Sweet, thanks for confirming that!",Positive
AMD,People still want them though. Developing countries especially aren't buying AM5 any time soon,Neutral
AMD,I don't know what you call continued support but my x470 motherboard just got a security fix bios update yesterday and an AGESA update bios last week.,Neutral
AMD,"> CPU releases yesterday  > ""dead platform""  Pick one.",Neutral
AMD,That's the dirty non-secret of computing. Everything is a lower-tier defect if you're buying consumer grade products. Those chips in your new NVMe drive? C-Grade compared to what gets sold for 1000x more in server grade parts.  Expensive consumer SD Cards and CPUs and Ram are still shit tier compared to the highest cuts of the wafer.,Negative
AMD,"developing countries are buying am5, mostly because AM4 is barely produced anymore. So if you arent salvaging some old used board you are getting am5.",Neutral
AMD,7400F with some A620/B650 board isn’t that much more expensive,Neutral
AMD,I dont think you know what continue support is if you think security fix for bios is continued CPU developement.,Negative
AMD,"This 2020 board gets security BIOS updates and no would call Z490 a ""live"" platform.  https://www.gigabyte.com/Motherboard/Z490-GAMING-X-rev-10/support#support-dl",Neutral
AMD,"The last actual new cpu release that wasn't just a refresh for am4 was the 5800x3d in 2022. Nothing that has released after that has actually been anything new to keep the platform alive, AMD does this to just sell off old inventory, not to keep a dead platform alive.",Neutral
AMD,"Its a dead platform. Arrowlake refresh is coming, but people still call it a dead platform. core i5 110 based on 10th gen just launched. No one calls it reviving the dead platform.",Negative
AMD,"Its not dirty, you don't have to buy them.",Negative
AMD,DDR4 and AM4 boards are dirt cheap from places like Ali Express,Neutral
AMD,5700X3D and 5600X3D both offered something for those interested in 3D vcache but didn't have the budget for a full 5800X3D.,Neutral
AMD,"Sure, they are sticking to old designs, but stop and listen to yourself. If it's being kept alive, how is it dead?",Neutral
AMD,"Looking back, the 20 series aged incredibly well",Neutral
AMD,If I didn't need a laptop I'd still be running the 2070. Was such a beast.,Neutral
AMD,"\>only 69% upvoted  Yeah, reddit absolutely hates it if you even suggest that the 9060 XT 8GB is actually a very good value card for the price. I've seen people unironically recommend a 3060 12GB over it just because of the extra VRAM which as you can see from the video, is just silly. They should have included the B580 in the video as well. It doesn't make sense for it not to be there, it's the same price range as the 9060 XT right now.",Negative
AMD,I don’t get why AMD would rather push an 8GB 9060XT for like $230–260 instead of a 9070XT at $650. Pretty sure the 9060XT still costs them significantly more than half of what the 9070xt does to make.,Negative
AMD,The 20 series cards are far from obsolete but the 9060xt kills it for the value  on new cards,Negative
AMD,"a well done comparison. only missing intel and current pricing value charts, though mixing used and new prices would be a headache.. and eh, not exactly a current buyer's guide unfortunately.",Negative
AMD,do we even need a comparison?   AMD pretty much gave up Radeon discrete graphics. Its not like AMD are flooding it with good price/performance and pushing market share aggressively now.,Negative
AMD,Because Turing has the same level of hardware features as current gen Consoles plus AI upscaling. Especially with games built with RT haven't fully arrived en masse.  We probably haven't even maxed out Turing optimisation probably. I feel Pascal and older need to be fully ditched. RTX 20/RDNA q/2 along with nVME SSDs need to be the baseline TBH. Optimisation is being held back.,Neutral
AMD,"Yeah the 2080 Super is an absolute workhorse. I sold mine and got a 4080 about three years ago, and then repurchased one a few months back for my wife’s rig. She still is hitting 90+ fps on 1440 decent settings",Positive
AMD,"Still using one! the RTX Titan, might be a special case but a 2080Ti would still be just as usable. It's fine, I'll probably sell it soon though given the silly value it still has and replace it with something more modern.  Kid has a 2060 Super PC as well and it's mostly fine",Positive
AMD,It will be one of the influential GPU series of all time.  Completely reshaped the graphics landscape. Also the first mass market GPU with AI acceleration.,Positive
AMD,"After the DLSS2 update came out, it was pretty obvious that Nvidia had a technology that AMD needed to match as soon as possible. It was really only after FSR4 came out that I could seriously consider buying an AMD GPU, with the last time being in 2013.",Neutral
AMD,"Swapped my 2070 Super for a 9070 XT, and honestly if it wasn't for 4K gaming or MH Wilds it would have still kicked ass. It'll be perfect for a racing sim or light editing rig later, but right now it's wasting away inside a music production PC. Killer card",Positive
AMD,Who could have seen this coming? (Everyone but tech youtube).,Negative
AMD,Pre and Post crypto boom cards all aged well because    1. Performance uplifts since are terrible and   2. Every company tried their asses off to flood the market with infinite demand. So 2nd hand market of rdna2 and ampere cards are dirt cheap and dragged down rtx 20 and rdna1 prices with them.,Negative
AMD,only aged well because there was no innovation until blackwell  if anything this should tell you that nvidia could have acted sooner on these technologies but strung everyone along until they could justify selling a 70 class GPU for $600,Negative
AMD,I've seen people who are budgeting for 5070 get told they should get the 9060XT 16gb because it's 'more future proof'.  Literally talking people into buying cards that are 35-40% slower than what they budgeted for because VRAM lol. It's mind boggling.,Negative
AMD,Yeah it's weird to not include Intel when they're *the* budget option.,Negative
AMD,"And who is going to be buying 9070 XT's? The only thing it offers over the 5070 Ti is a price advantage and street pricing has been more favorable towards Nvidia with NV cards showing up far more often at MSRP than the 9070 XT has. The smaller the NV tax, the smaller the reason to go AMD.  NV also has the Borderlands 4 promotion going on now, so even in my case where I can walk into Microcenter and get either one of these cards at MSRP, if I value the BL4 promotion at all, the pricing gap has now shrunk in favor of NV.",Neutral
AMD,"9070XT is not selling well even at $650. Every single Microcenter has had the $650 Reaper card for while and inventory isn't going down. What is selling and where inventory is going down is the MSRP 5070ti.  9070XT had it's opportunity to sell well, really well at the MSRP starting a few months ago.......now it's a too late. You'd need to bring it at or even below MSRP to get more people steered towards it. That's just what the situation is for AMD right now.",Negative
AMD,">do we even need a comparison?   Evidently so. The 9060 XT 8 GB is the best option in this price range—it's 22% faster than the 5060 at 1080p and 1440p ultra, and it's regularly $20-30 cheaper.",Positive
AMD,"The 9070 XT and non-XT are some of the most competitive AMD cards in years. They're better value than the 5070 Ti and non-Ti, and even rival the 5080 in some games.",Positive
AMD,Pascal is ending driver support next month so there’s that,Neutral
AMD,how’s the 4080 going for you,Neutral
AMD,2080ti user here. BL4 running at low settings on an ultra wide at 90fps. Had to download frame gen for it to improve from 40s. I'm still debating upgrading like you said to pull some of the value out. Waiting on 5000 supers,Neutral
AMD,People whined hard at the time with how useless this was. But they aged so well.,Negative
AMD,It is BY FAR the most revolutionary arch since unified shaders.,Neutral
AMD,> it was pretty obvious  You'd think so but the amount gaslighting has been absolutely insane. Pretty much 100% tech reviewers straight up refused to _properly_ test the technology for over a year and people who'd never seen it run on live hardware just kept making stuff up arguing with each other.,Negative
AMD,"I remember seeing that Death Stranding DLSS 2 trailer and thinking ""man I got to get a 3xxx series card this shit is the future"".",Negative
AMD,And that makes it even sadder because so many people parrot YouTubers it gave AMD a free pass for dragging their feet.,Negative
AMD,"Not true.  If you buy a RX 5700 \[RDNA 1\] today it will come with many compromises that will make them not worth it even on used market anymore such as not supporting DX12 Ultimate and proper Upscaling such as DLSS.  Whereas the RTX 20 series they aged like fine wine despite being so hated back when they got released. No reviewers other than Digital Foundry saw the future of Upscaling and DX12 Ultimate features, everyone is stuck on Rasterization only matters mindset.",Negative
AMD,None of the AMD cards in the last 3 generations have aged well. All of them were significantly lacking in features which at the time many downplayed the significance of but it has come back to bite them.   Whereas on the Nvidia side only the VRAM crippled cards have not aged well. But that's something one could have seen coming from a mile away.,Negative
AMD,Not because of that. Because of RTX and upscaling.  The 2060 Super and 5700 were nearly the same price. But the 2060 Super would have given you a significantly better experience due to supporting DLSS upscaling. Even if you ignore all the other features that came with RTX,Positive
AMD,"What is weird is suggesting people buy a product almost noone has, with who knows what support, history of bad drivers from a company that is collapsing",Negative
AMD,>And who is going to be buying 9070 XT's?  Linux users who want a hassle-free experience.   edit: WTH. I keep getting downvoted for answering a query. I have a 9070 XT on my gaming PC & a 4060TI on my AI/SFFPC.,Negative
AMD,A $650 9070 XT is not as bad of a value proposition as you think vs a $750 5070 Ti. Borderlands 4 notwithstanding,Neutral
AMD,The 9060xt is 60-70€ more expensive,Negative
AMD,"No, the fake MSRP killed that narrative in less than a week.",Negative
AMD,"I have an old laptop with Pascal GPU, still kicking in pre-UE5 games and especially in very extensive indie and smaller games library. ""Fully ditched in new games"" and end of drivers support is not the same thing and kinda sucks... That means not even a slight support and security updates?",Negative
AMD,"Can’t complain, I basically play high/max settings and average 100 ish fps in demanding games like Rust and Escape from Tarkov, anything with lower reqs is maxed with normally 144fps as I have a 144 hz monitor",Neutral
AMD,"Yeah, 5070 Super seems like the obvious upgrade if the specs turn out to be true. I think I can still get £500 from the Titan thanks to people craving the VRAM which seems ridiculous given its age but who am I to judge, I don't need it but would like a comfortable 18GB card.",Positive
AMD,"because how small the jump in raw raster was from 10 series, and how much prices shot up  the 1k+ 2080 ti was a shocker for the top end buyers, and then if you went for 2080 as normal, you got a huge % difference in performance because its just like the 5090 vs 5080 were on entirely different chips that was massively cut down vs salvage die of 30 series.  the 20 series was a massive cash grab, make no mistake. the DLSS being good is a much later development.",Neutral
AMD,"That's because for some odd reason the new generation of media (YouTubers) don't know the history of PC features. Unlike consoles where major shifts in visual fidelity and performance happens per generation. On PC they come out almost nilly willy and the userbase is fragmented on what their hardware supports.   Now forwarding looking concepts like ray tracing (a holy Grail type of feature to PC gamers of the 90s), YouTubers didn't like it and kept slamming Nvidia for working on it while praising AMD for charging consumers more for raster performance.   These hacks, HUB, still stood by their RX 5700 XT recommendation when they did a retrospect video during the mesh shader nonsense.   People finally going to wake up? AMD and YouTubers have been selling/promoting products with less features but almost equal prices.   RDNA4 finally makes things equal and it's AWOL thanks to AMD resource allocation.",Negative
AMD,Nvidia weren't kidding by calling it *Graphics Reinvented*.,Neutral
AMD,"if you had an early 20 series card like I did, DLSS 1 was bullshit on anything not 4k, and using it to hit 4k60 was the most it could really do and you better have it on quality scaling and not anything farther.  DLSS2 came out later, and more in line with the 30 series launch (not exact, 2 came out earlier), that allowed it to be used for 1440p scaling and 4k scaling at more than just quality setting (IE performance or even ultra performance for some games where its really good on).",Negative
AMD,No bad gpus only bad prices. Rdna1 isnt being hailed as a dx12ultimate game machine right now. Same with rtx 20 series. You wont be able to enjoy the games that require the tech that dx12u requires. Maybe except for 2080ti. Which you paid 1000 bucks at 2018 (1200 of todays dollars) and yes it aged well as hardware. But it still wasnt worth it waiting 6+ years for tech to mature with a 1000 dollar sticker price. And with rt features it offered at launch. Push back is completely justified for an uncooked featureset.,Negative
AMD,">No reviewers other than Digital Foundry saw the future of Upscaling and DX12 Ultimate features, everyone is stuck on Rasterization only matters mindset.   It was just Nvidia's marketing, quite frankly.  DF's track record isn't nearly as squeaky clean as one might think.  They were the only ones at the time who were ""impressed"" by DLSS version ""1.9"" in Control (basically DLSS 1.0 with minor tweaks), which wasn't entirely dissimilar to FSR 2.0 and ran on standard FP32.  Yet, they kept comparing it with PS4 Pro's vastly inferior checkerboard rendering technique to present it in a positive light.   In hindsight, it was 'probably' IGN that was twisting their arms behind the scenes, but that's hardly an excuse for bad (or at least biased) journalism.",Negative
AMD,"Rasterization is good though. Nvidia is pushing for software gimmicks to act as makeup for game developers. Just like how developers decided to make games bloated because of storage, developers will make games increasingly shittier to run outside of DLSS and frame gen.   I have to wonder if there's an ulterior motive by nvidia to have game developers make their games less optimized on middling hardware so that they can upsell GeForce Now.",Negative
AMD,AMD is currently working to make FSR4 reverse compatible. They will age like fine wine.,Positive
AMD,"Can't help but wonder what the 40 and 50 series will look like in 5 years, and if it'll be the same scenario. I bought a 4070 Super a year ago, with a 7900gre being another option I don't regret avoiding. If AI horsepower is more important than VRAM , which in expecting will happen again, and only get more severe, I made the right decision.",Neutral
AMD,DLSS gets you so far when baseline performance isnt good. Thankfully they are good cars from 20 generation.,Positive
AMD,"Intel ARC won't get any better if no one buys it doofus, no one would recommend Battlemage cards if they're truly dogshit either btw. Might as well buy only NVIDIA with that logic.",Negative
AMD,So a number of users so inconsequential it's not even worth considering in terms of manufacturing output for these GPU's.,Negative
AMD,"Yeah no, i run 4090 and 5070 it in my two Linux builds. Both are hassle free. And have been so for years.",Neutral
AMD,That's the 16 GB model. This video compares the 9060 XT 8 GB against the 5060 and other cards that also launched with an MSRP near $300.,Neutral
AMD,"depends on the country, imo if you can get a 9070XT for ~15% cheaper than the 5070 Ti it's completely worth it",Positive
AMD,only security updates   > GPUs will only receive driver updates for critical security items beginning October 2025 and continuing through October 2028,Neutral
AMD,Yeah true but I think 2027 should be the absolute latest Pascal should be supported.,Neutral
AMD,1.0 soon. It will be perfect!,Positive
AMD,> That's because for some odd reason the new generation of media (YouTubers) don't know the history of PC features.  It's interesting that LTT - generally mocked here - was one of the few channels to get this point early on while the more 'serious hardcore' channels had to be dragged to it/still are not really understanding.,Neutral
AMD,"That makes no sense... Reviewers review for the current buyer, not the age like fine wine dealie. Most people should be buying for now, not the promise of possibility, the ones who keep saying fine wine in a serious way are a joke for sure.   The pricing of the 20 series at launch was ASS, even with hindsight here, RX 5700 XT was 400 bucks, which it trades blows on average with the 2070 that was 500 dollars (although, at the time it was HEAVILY game dependent, where some it would outpace very well others just die). The exact trade off that gets made later, but this time at a much more acceptable price point, 100 dollars less on a 400/500 dollars product is actually sizable, not so much on a 1000 dollar product.   The lack of features for DLSS didn't matter until really 30s gen when the updated DLSS came out, and really until the latter point of it or 40 series when it got mass adoption.  If your upgrade cycle is 2 cycles, it was a good idea and then pick up a 40 series because by then DLSS really did come into its own, and AMD's lack of competition then was an issue (that they worked to address with PS upscaling that turned into FSR4)",Negative
AMD,"An RTX 2060 Super will play games like Indiana Jones The Great Circle at optimized settings paired with DLSS 4 Upscaler, with RX 5700 you need to do swap out to an entirely different Operating system as well as many mods just to make it run on a non DX12 Ultimate compatible GPU.  Nope. They aren't the same, the RTX 20 series Turing aged far better than RDNA 1 RX 5000 series.",Neutral
AMD,"> No bad gpus only bad prices.  I hate this truism because somebody could say ""This card isn't even worth buying for 50 dollars because of outdated features/support"" and you would just smugly say ""well then the price is bad isn't it"". It's a functionally worthless phrase that is just a worse version of 'supply/demand'. It's a thought terminating cliche almost.",Negative
AMD,"> DLSS version ""1.9"" in Control (basically DLSS 1.0 with minor tweaks)  DLSS 1 was a spatial AI upscale, DLSS ""1.9"" was a temporal upscaler, they were nothing alike, not ""DLSS 1 with minor tweaks""",Neutral
AMD,"First off even DLSS 1.9 looks far better than the FSR 2, FSR 2 only looked close when comparing between standstill image which gave some false analysis which most of the tech journalist were guilty off including Hardware Unboxed. And DLSS 1.9 is barely relevant to use anyways as that version of DLSS AFAIK only were used on a single game.  Everything else eventually used DLSS 2, which on comparisons has proven itself to be much better as well compared to FSR.  Second IGN at the time doesn't own Digital Foundry they were under Eurogamer at the time and by then they were still independent, they were only purchased by IGN back on 2024 and then Digital Foundry eventually bought their independence back off them just recently this year.  Some people like you keep saying that they are biased or bad journalist, when in reality they seem to be the most mature of them all especially compared to the likes of Hardware Unboxed, Gamers Nexus and every other tech tuber which seems to rely on gamer rage click bait to get views nowadays.  No offense to them I still appreciate their content and still watches them to this day. But any day I prefer the likes of Digital Foundry which focuses more on technical state of the games they are reviewing also, plenty of game developer interviews that gives us more insight on how the future of visual game rendering technology is going to be in the future, which they correctly predicted with Upscaling as well as Ray Tracing, DX 12 Ultimate features in general rather than the Hardware rasterization focused only.  Which gave us some false insight on what the future is going to be like. Back in 2019 everyone is focuses on rasterization only matters, upscaling, ray tracing bad train.  6 years later it's very obvious which became the standard, and this is something like subreddit like r/pcmasterrace will never admit, I guess.",Neutral
AMD,"That's not going to happen. They may choose to call it ""FSR 4"" but it will look nothing like it.",Negative
AMD,"Fine wine - 5 years from launch, when your hardware is woefully underpowered and you're ready to upgrade, you finally get feature parity with the RTX 2060",Positive
AMD,PS6 might full AI with fsr4 or fsr5. Fsr4 was created in collaboration with Sony. Sony vision is most likely going for ML upscaling with their upcoming consoles,Neutral
AMD,"That's not how that works.   Again, the point is it aged a lot better. You basically got a ""free"" \~30% performance boost over the AMD counterpart.   Fundamentally the 20 series cards are technologically not too different from today's cards. That's why something like a 2080 Ti is still pretty damn good, and even a 2060 Super is perfectly usable (around RX 6600 XT performance, but with upscaling, so real world way better experience).   Meanwhile let's be honest, the 5600 XT or 5700 are like stone age tech. And even the RX 6600 cards are worse than the 20 series cards. It's just crazy.",Positive
AMD,AMD has already settled for a tiny market share. Linux might not be an irrelevant proportion of it.,Neutral
AMD,"It's crazy seeing people upset they can't run new features. Man, that's what made reviews for new generations and refreshes exciting.  New feature introduced or fixed/improved, or both, and you were just constantly in awe. Now good portion get snake oiled into ""raster is king"" and they try to burn down the company that is moving the needle while PC gaming gets stuck in console version of generational shifts, ie too damn long between hops!  Had AMD got at least pushed to put hardware in RDNA2 at the earliest, we'd have way more RT optimized games, but nah let's not even do it for RDNA3!   RDNA4 and with it FSR4 is out and suddenly ""FSR3 looks like ass!"" And the gimmick is now a demanded feature to be backported.",Negative
AMD,"> Reviewers review for the current buyer, not the age like fine wine dealie.  Yes, so a product that is within 1-5% of another product in raster, with more features for the same price is dismissed because of ""gimmicks"".  https://tpucdn.com/review/amd-radeon-rx-5700-xt/images/relative-performance_3840-2160.png  Option A: $400, 95% raster, Tensor Cores, DLSS 1.0 (Garbage), Ray Tracing (limited support)  VS  Option B: $400, 100% Raster....  > RX 5700 XT was 400 bucks  The 5700 XT was announced at $450, targeting the 2070, but AMD got blindsided by the Supers, leaving it trailing a now $500 2070 Super, tying a $450 2070, and duking it out with a now shifted up in price 2060 Super because AMD got greedy and was forced to price cut before launch.  https://www.techpowerup.com/257106/confirmed-amd-to-cut-rx-5700-series-prices-at-launch  > The lack of features for DLSS didn't matter until really 30s gen when the updated DLSS came out  And what did AMD respond with? RDNA2 and STILL no hardware solution, they went with software and continued to get clubbed like a baby seal. And what did reviewers do? Wouldn't mention DLSS as a competitive feature because it wasn't apples to apples, leading to directly misinforming their audience through omission, and would actively downplay ray tracing whenever possible.  EDIT: fixes",Neutral
AMD,"I think there needs to be a distinction between technological foresight vs buying advice. They don't necessarily overlap.  DF definitely had a better read of future trends in the industry. But at the same time, as a consumer in 2019, making purchase decisions based on games and features that wouldn't have materialized until years later wasn't a particularly wise move.",Neutral
AMD,"I suspect it's still an irrelevant amount since AMD's benefits on Linux are often gaming oriented, which based on Steam Hardware Survey is only about 2.64% and heavily driven by Steam Deck and similar hardware using AMD APU's rather than discrete GPU's.  If you're using Linux in a work enviornment, the value of CUDA is so enormous that even under Linux you're still seeing massive Nvidia use. ROCm is not a remotely competitve option.",Neutral
AMD,"Again, on 20 series launch, it had support for BF V and Metro, it was a gimmick and wasnt until DLSS2 it was good, and only really mass adopted by new games by later on in 30 gen.  Again, it was a gimmick.  And right, the 400 dollars was fighting a 450 2070 or a 400 2060 S or 500 2070 S  again, for the two games it supported, 3 if you counted control added later on, DLSS was not a huge thing and was a gimmick.   I brought a 2080 ti to run a 4k screen because I brought one super early to pair with a 980 ti, and really it wasnt until way later I can use DLSS to actually help running with games, the first major one where I felt it was actually helpful and necessary was launch cyberpunk 2077 partly because that was when it was starting to struggle to hit 4k60 with games, and partly because that was when DLSS was getting to the point where it looked good enough to properly upscale even at 4k compared with DLSS 1 that was a joke.  I am telling you as someone who has brought them and lived thru the time, DLSS 1 was a joke and a half, just like FSR pre 4.",Neutral
AMD,"> Again, it was a gimmick.  Fine, it was a gimmick. That doesn't change, for the same price a buyer could pick the product with the gimmick, and end up no worse if the gimmick fizzled out but far better if the gimmick succeeded.  AMD had barely any advantage at the same price point, so going down the features list to devalue having MORE FEATURES isn't a serviceable recommendation.",Neutral
AMD,"I dont know what to tell you if you think a card with 2070 performance for the price of 2060 S is not good enough.  at that point the difference between cards are small sure, but that was still a half tier down.  not to mention, if you looked at actual card prices then, nvidia being nvidia had far more AIB cards not at MSRP than AMD did (reminder this is the time the FE was more expensive than ""reference"" models, by 100), for better or worse that is. As AMD's MSRP-ish cards could have issues like weak cooling / ram cooling while nvidia is better with some nice designed stuff but again you are paying for it while the ""reference"" stuff are largely okay but not spectacular but at least you wont see random 100C memory temps on them.",Negative
AMD,"> I dont know what to tell you if you think a card with 2070 performance for the price of 2060 S is not good enough.  I believe you are missing my point. I'm not saying the 5700 XT wasn't a good product, I'm saying the 2060 Super was a better product SOLELY because of the features that reviewers called ""gimmicks.""  What sane person would pick an option with less features for the same price? Worst case scenario features never get used, you end up same as if you picked other product, best case scenario features get used and end ahead likely ahead of if you picked other product.  And this was JUST for RDNA1. For RDNA2 it got even worst because now you had DLSS 2.0 and actual RT games with passable usage/effects, and AMD still sat on their hands. And by RDNA3 you can no longer even defend this, yet reviewers still didn't want to admit they were wrong, they weren't gimmicks anymore but we had to sit through the gaggle of ""fake frames"" stupidity.",Negative
AMD,"TL;DW - An OEM only CPU called the FX-4200 has four modules with four integer units, four floating point units, and access to all the cache. In synthetic FPU benchmarks, it dominates over the FX-4350. In gaming there isn't much a difference.",Neutral
AMD,Phenom 2 was pretty damn good and AMD might have held up better by doing a die shrink refresh of it rather than gambling on the weirdness of Bulldozer.  It might have saved them some desperately needed money too that could have been better invested in a good successor architecture. I'm fairly sure the only reason the shared fpu thing happened was to save die space and therefore money as AMD were up shit creek at the time financially.,Positive
AMD,"It's an interesting oddity, I had never heard of the FX-4200  2011-2013 was still so single-thread dominated I don't think this difference would have even registered to most, especially when Intel was still pulling off insane die shrinks that gave them 20% more performance for 20% less power  But these videos are also a reminder that FX series didn't age as badly as it benchmarked in those years, at least.",Neutral
AMD,That 40 seconds at the start of the conclusion felt like it 10 minutes long.,Neutral
AMD,"It seems like the high inter-module latency is hurting the 4200 in some cases which causes the 4350 to have better performance. I remember reading something from chips and cheese I think that showed the inter-module latency was somewhere over 300ns. I think this is also why Microsoft changed the scheduler behavior in Windows 8+. In windows 7, it loads the first core in each module first essentially treating it like a CPU with SMT. In windows 8 and newer the scheduler keeps threads on the same module if it thinks they have a lot of inter-core traffic.  I honestly don’t think the construction core design wouldn’t have been that bad if the L1 and L3 cache weren’t so poorly performing alongside the poor 32nm Global Foundries node. 28nm wasn’t much better either from Global Foundries. I would’ve liked to have seen a full blown desktop Steamroller and Excavator chip with 4 modules and L3 cache.  AMD learned a lot with these designs and carried over quite a bit into Ryzen.",Negative
AMD,Almost went for a fx 8350 back in the day before I even knew anything about their weirdness. Somehow ended up with a G3258 instead. Was so impressed with their clocks vs intel.,Negative
AMD,I had a 3 core i believe! Tried to enable the 4th but it wouldnt boot lol.,Neutral
AMD,"When I was hesitating to upgrade, ryzen was already put but I gave my PC one last hurrah by buying a used fx-9590 lol. Then I had issues with two games that had issues with FX CPUs causing massive FPS drops so I upgraded to the ryzen 3900x and my FPS in certain games literally doubled.",Neutral
AMD,"Wrong question, what if Intel didn't sabotage multiprocessor development.",Negative
AMD,Really curious where the 8 core variants compare to the 4c4m versions in the synthetic benchmarks as it would be 8 core for integer tasks but quad core in fpu tasks essentially  I know when I ran ESXi on my desktop FX-8120 it ran pretty good so long as I set the core preferences for each VM,Neutral
AMD,"Llano was technically a 32nm shrink of Phenom II, but with a rather big IGP slapped in. I recall than it had very poor frequency scaling due to having used a process to favour the GPU density over CPU clock speed, so it didn't left a good impression.",Negative
AMD,"From a marketing perspective too I think Bulldozer benefited too. A 4.2 GHz ""quad core"" FX-4350 sounds great for $122. Sort of like how 3 GHz Pentium 4 sounds better than a 2.2 GHz Athlon 64.",Positive
AMD,"It was also designed to hit 6ghz stock, where it would have easily beaten Phenom and all the design compromises would have made a ton of sense. The actual silicon ended up not coming close.   Edit: I'm not sure where I read/heard 6ghz specifically. However if we take the (very safe) assumption that they intended to beat Phenom's performance, and it was 33% narrower with the corresponding IPC loss, then the intended clock speed would need to be 40-50% higher.",Neutral
AMD,"Honestly there's a fairly straight line AMD could have taken to evolve the Stars cores into something akin to Zen (or just hold down the fort until Zen). Bulldozer was a tremendous waste of resources (during some lean years as well). Llano showed that even with a VERY light touch, it was trivial to squeeze another 5 to 10 percent more IPC out of Stars, even though the focus of that 32nm shrink was the iGPU. They could have bought a little time by making a Phenom III by slapping some L3 cache back on there and running 6 cores by default like Thuban. Then the next step would have been to widen things a bit and implement all the instructions they had fallen behind on. The next step would have been a uop cache and SMT and well.... you're looking at something mighty close to Zen1, assuming you could fab it on 28nm since 20 wasn't going to happen.",Negative
AMD,Wasn't bulldozer competing with sandy bridge at that time? It want even close.,Neutral
AMD,Thubans were great chips!,Positive
AMD,"Phenom II was good for the time (I had a 955BE), but I'm not sure how much farther they could have pushed the architecture. Once Intel released Sandy Bridge I don't think there was much AMD could have done at that point. It was such a huge leap that it served as the foundation for Intel for nearly a decade, with only minor tweaks. Even the original Ryzen barely matched it in IPC and certainly not in clock speed.",Neutral
AMD,Phenoms were a beast. I got my 955 BE almost to 4ghz. Ran that thing from 2009 to 2017 with an upgrade to ryzen.,Positive
AMD,"My daughter's gaming rig is still powered by a 1100T. Granted, nothing she's doing is all that Intensive, but for a 15 year old CPU, it does everything asked of it without it being obvious it's technologically ancient.",Neutral
AMD,"I barely knew about their weirdness, and that put me off, even though it was  brand new. When my MB died, I had to choose between Phenom II X6, FX-8320 and i5-2500K for basically the same price. Thought the FX was weird, the Phenom was great, but old, so I went with the i5. Best decision I have ever made in regards to hardware.",Negative
AMD,I was able to do it on a Gigabyte board,Neutral
AMD,"If memory serves correct, and I could be off, anandtech did a 4T / 4M test. FP perf per thread went up but was lower overall since 8T going to the what was, loosely speaking, a 4 FPUs with SMT had a performance uplift.   There was also ""typical"" scaling for INT workloads going from 4T / 4M to 8T / 4M in that it was ""close enough"" to 2x.",Neutral
AMD,"I'd forgotten about Llano. I didn't get my hand on an A series APU until the Steamroller A-10 generation. Even with the crappie bulldozer based cpu architecture that A-10 really impressed me with its GPU performance, it was a massive jump up from the Intel IGP despite still being far below discreet GPU performance and Intel CPU performance.",Positive
AMD,"I know the design goal of Pentium 4 was to get higher clock speeds (to which the laws of physics said no), but I've not heard of Bulldozer being the same.",Neutral
AMD,Bulldozer was crap. Nobody denies that.,Neutral
AMD,"same here, it's sad that they are so often equated to bulldozer chips.",Neutral
AMD,Phenom IIs were good but the Phenom64 was pretty mediocre for the level of hype it had. I had a 9950BE which was the fastest one they made IIRC and I couldn’t get it stable past 2.8Ghz with modest air cooling. I think quad cores weren’t exactly ready for prime time back then and a core 2 duo probably would have served me much better.  When I got a 955BE later on it performed much better,Negative
AMD,2500k was the goat. Back when overclocking could actually give you major gains. The G3258 Actually won an overlooking competition with it on a Evo 212 lol. Such a cool little CPU wish they'd make interesting cpus like that again. Think I hit like 4.8 ghz or something like that haha. Although I jumped ship on ryzen 1700x. AM4 is the goat. Still got me a 5800x3d no reason to upgrade now. Missing being a broke kid finding the best deals,Positive
AMD,"Makes sense given the hardware constraints and the clock speed uplift would probably show a bit of performance improvement on FPU tasks.  I probably read that article, completely blanked on it, and then you reminded me and now I can't look at it because the site is more or less down now, which is a shame.",Neutral
AMD,"The whole time I had it on air too, I wonder what I could squeeze out of it with my push pull 360 aio I have now. Got it to boot at like 4.1ghz but would crash if you tried to do anything. I'm surprised I didn't melt it.",Neutral
AMD,"Performance-wise, original Phenom was 65nm and had a crippled L3 cache capacity from die space constraints. 45nm Phenom II fixed this with triple the L3.",Neutral
AMD,"Yeah, if you put that 5800X3D on your mainboard from your 1700X, that's longevity right there. And it will be good for the next years as well!",Positive
AMD,I think it's actually this  [https://www.extremetech.com/computing/100583-analyzing-bulldozers-scaling-single-thread-performance](https://www.extremetech.com/computing/100583-analyzing-bulldozers-scaling-single-thread-performance),Neutral
AMD,"I was using an aio with it the entire time, the stock cooler i initially got with it was very noisy, I got them to send a new one, but by that time i was already using the aio; i believe i got it stable at 4.1 or 4.2 but I don't really remember right now; not sure if this was a factor but i had it paired with low-cas memory.",Neutral
AMD,"Oh wow, somehow I never realized just how paltry the L3 cache was on the first Phenoms.",Neutral
AMD,9800x3d doesn't seem big enough bump to upgrade so I'm fat chilling. 4090 and 5800x3d is enough for me lol,Neutral
AMD,"Thanks for the link, makes sense from the hardware layout",Positive
AMD,"me too, but with the 5090.",Neutral
AMD,"Im impressed that Qualcomm hasn't made a good support for linux of their Elite lineup.    There isn't a lot of Linux people out there, but a lot of them would happily use an ARM laptop.    Im even thinking of buying that ARGON laptop that you use with a Pi5 just because it looked fun",Neutral
AMD,The X Elite 2 could be very interesting if Qualcomm gets full Linux support working at hardware release time by getting the major distributions installing on it out of the box. I reckon it would do wonders for their mind share having devs & enthusiast tech early adopters switch to it.  I'm not hopeful of them managing it though. Whoever is in charge of the X Elite program really screwed up the entire dev hardware release and outreach.,Positive
AMD,"> While there are some quirks in setting it up like keeping WoA installation for extracting the necessary firmware  Yeah you can call that a ""quirk"" for sure.",Neutral
AMD,"I would have expected Linux to be supported on day one. These SoCs are ideal for Chrome Books and QCOM already has plenty of Android support experience, and they are targeting the data center with some of their new many core SKUs based on the Oryon uarch.",Positive
AMD,"Yea, linux is the only relatively popular OS family where transition to ARM would be painless. Yet here we are.",Neutral
AMD,"Why would Qualcomm, a company thats all about locking their clients into proprietary walled gardens, want to support linux?",Negative
AMD,>The X Elite 2 could be very interesting if Qualcomm gets full Linux support working at hardware release time by getting the major distributions installing on it out of the box.  This is very unlikely to happen. Qualcomm is not that kind of company.,Neutral
AMD,"Android is so far off from Linux nowadays, that I’m not surprised. The driver fragmentation in mobile is unbelievable, and they even have problems with updating a system…",Negative
AMD,I am really buffles with the bad linux support. Mediatek has ChromeBook support for the recent high end chip.,Negative
AMD,"ARM Chromebooks have great kernel support, but their boot firmware isn't really supported by anyone that isn't specifically ""arm chromebook distro""",Neutral
AMD,As opposed to the painful experience of simply buying an ARM laptop?,Neutral
AMD,There's patches on LKML for it,Neutral
AMD,yes,Neutral
AMD,Can you point me to a Snapdragon X Elite device that has out of the box support (working drivers for all hardware components) for a major linux distribution?,Neutral
AMD,"it's the kernel modules that are the concern heh, the drivers.",Neutral
AMD,"The modern renderer picks and choses from Raster and Ray/Path Trace where the fit is best, and I just kind of assumed Neural would be added to that, but just part of the stack where it makes the most sense, w/o replacing the other two entirely.",Neutral
AMD,"I'm not sure I fully understand. ML2CODE is converting the model from requiring Tensors to accelerate, to executable via Shaders?  Is ML2CODE a static library that performs this conversion? Is it an ML-based neural conversion system?  I have so many questions... if it's performant, then why wouldn't/couldn't any other Tensor-based operation be converted to run on Shaders alone?  I don't know what to expect from Redstone. This article is discussing running the model via Shaders; other speculation (and working on Windows and Linux) is running a compiled, special version of FSR4 via INT4/8 and/or WMMA.  So—AMD may have a way to run FSR4 across various Tensors, and even accelerate it on pure Shaders?",Neutral
AMD,">FSR Redstone is being developed using AMD ML2CODE (Machine Learning to Code), a research project of ROCm. The core part of the neural rendering technology is converted into optimized Compute Shader code by using ML2CODE. This means that the neural rendering core of FSR Redstone can also be run on GPUs made by other companies .",Neutral
AMD,Decided to go to the original source instead.,Neutral
AMD,I'm a bit worried about the performance of an automated conversion tool.,Neutral
AMD,Fsr4 already did via leaked model,Neutral
AMD,"ml2code (likely) is an executable that converts neural network model to hlsl/glsl/spirv compute shader. It probably wont remove ""requiring Tensors"" as those can also be used from shaders.  >  why wouldn't/couldn't any other Tensor-based operation be converted to run on Shaders alone?  They could be converted, limitation is just missing data types (fp4 or any int1-7 are not exposed yet).  So process (likely) is: Python Tensorflow/etc to describe (and train model) -> store result using [ONNX](https://onnx.ai) -> ml2code.exe then converts onnx to .spirv/.hlsl file(s) that can be included like any other compute shader to be ran with dx12/vulkan",Neutral
AMD,"But as usual, Nvidia will make their own that only runs on Nvidia.",Neutral
AMD,"Nvidia already has their own version of these technologies (neural radiance caching, Machine learning based frame gen DLSS FG, machine learning based denoiser (Ray reconstruction)).   AMD is just playing catch up, as is tradition.",Neutral
AMD,"You do know that Nvidia has all these techs? And running on shaders on Nvidia is inferior to tensor cores, so the much superior option for Nvidia GPUs is to use the existing nvidia versions",Neutral
AMD,"And as usual it arrives 3 years earlier and with superior quality.  Instead of focusing on subpar features that run on every gpu, AMD should actually focus on making superior quality features even if it means locking it on their environment, everyone saw how shitty FSR was until 4.0.",Negative
AMD,And then they’ll sue foss devs making compatibility layers,Neutral
AMD,why would nvidia spend billions on dollars on software that makes AMDs cards more competitive...?,Negative
AMD,You have to be kidding right? So bad faith,Negative
AMD,"> AMD is just playing catch up, as is tradition.  And as is tradition with AMD's ""we have <*insert name of NVIDIA technology*> at home"" projects, there is at least a 50/50 chance that this will wither on the vine before actually becoming deployed and/or usable in any real sense of the word and a 90/10 chance that it won't be nearly as good even if it does actually reach end users in a finished state.",Negative
AMD,It's strategic on their part to make open source software and hopes of that wider adoption will bring them more market share. It doesn't make any sense for them to lock their features to their own software when they only command 10 to 20% of the market.,Neutral
AMD,Look! we've got a Jensen's resident boot kisser here,Neutral
AMD,Why would AMD?,Neutral
AMD,"In fairness, FSR4 is excellent and FSR Frame Generation is very good given its constraints (no hardware acceleration and no Reflex/Antilag 2 integration). Adoption is the issue.  After bumbling around with FSR 1 and FSR2/3 for far too long, they're finally making some good progress. I just hope that supporting hardware besides RDNA4 doesn't end up sacrificing quality. That can come later.",Positive
AMD,"Open source doesn't mean it works on any hardware, it just means the code is open to the public.  Being able to use FSR on nvidia is pointless, no one uses it, because of how shitty it is, if anything it gave fsr a bad reputation. Quality comes first.",Negative
AMD,Next you're gonna complain that iOS doesn't run on Samsung phones,Negative
AMD,No one is complaining that AMD isn't doing that. They are complaining that Nvidia isn't doing that.,Negative
AMD,"TBH, universal deployability outweighs quality to some degree, as it means game devs can just say 'FSR' for tech suite they want to add and not have to think about it, which is a competitive advantage.",Positive
AMD,"While quality absolutely matters, I'm simply pointing out their strategy.",Neutral
AMD,"Apple's closed wall approach is one of the major faults with them, yes.",Neutral
AMD,"Not even the same, but nice try",Negative
AMD,Because AMD is. They open source pretty much everything. They've supported Nvidia GPUs pre 2000 series better than Nvidia has.,Positive
AMD,"Only if the sacrifices required for the universal solution result in something worth using in the first place. FSR Frame Generation was a value add, FSR 1 and FSR2/3 earned brownie points with a vocal section of the tech community but were otherwise free marketing for DLSS.",Neutral
AMD,"Yeah, apple's phone silicon is second to none and if I felt adventurous I'd be tempted to make my next one an iPhone and try to crack that motherfucker and install an open phone OS instead.",Negative
AMD,"You're right, it would be a lot easier to get iOS working on Samsung than to get CUDA working on amd",Neutral
AMD,"The RDNA based Xclipse GPUs found in recent Exynos SoCs use AMDVLK as their Vulkan driver. They'll switch to RADV in future updates I suppose.   The X960 inside the Exynos 2600 was spotted on Geekbench 6 with the AMDVLK driver that contained the Vulkan 1.4.304 header version or to be precise: [v-2024.Q4.3](https://github.com/GPUOpen-Drivers/AMDVLK/releases) according to the GitHub repo.  This announcement comes to no surprise as well, if you look at when the most recent driver was released",Neutral
AMD,Hopefully that means that RADV can catch up with AMDVLK in RT.,Positive
AMD,Better to have one good driver than two meh drivers IMO.,Positive
AMD,That's a good thing. AMDVLK was always subpar.,Positive
AMD,Will this be better for using Rocm and stuff like Davinci Resolve for Linux and AMD users wouldn't have to go through hell highwater to get Resolve and other commercial software working on Linux?,Neutral
AMD,Does this mean Vulkan is dead?,Neutral
AMD,They get driver updates?,Neutral
AMD,"ROCM has a completely separate driver stack, so this does not affect it. Regarding Davinci Resolve, there's a new OpenCL driver in Mesa3D called Rusticl and it's already competitive with ROCM's OpenCL driver. However that doesn't help for software which is CUDA/HIP only.",Neutral
AMD,>…throwing our full support behind the RADV driver as the officially supported open-source Vulkan driver for Radeon™ graphics adapters.   It's in the article.,Neutral
AMD,"No, they using RAD Vulkan Open Source",Neutral
AMD,Vulkan is the API things use to do things on your screen.. AMDVLK and RADV are userspace Vulkan implementations that convert those Vulkan calls to the AMDGPU kernel driver.,Neutral
AMD,Tell me you can't read without telling me.,Neutral
AMD,Linux kernel contains drivers (most of it is drivers actually) so the drivers update with the kernel. Most of the time I mean. Exceptions exist: Nvidia doesn't have it's drivers open-sourced and so in the kernel which is why they can have issues if something changes inside the kernel.,Neutral
AMD,AMDVLK and RADV are userspace drivers and not kernel drivers. They both talk to the kernel driver which is AMDGPU.,Neutral
AMD,"AMD drivers arent open sourced either. Most of them are binary blobs whose source code is unknown. They are just integrated into linux kernel unlike Nvidia. But that doesnot matter much, as linux is full of closed sourced binary blobs nowadays. Linux does not pass FOSS. Unless you look at some really old and outdated distros.",Neutral
AMD,"For those unaware, RADV is distributed inside [Mesa, the unified open-source userland graphics driver library](https://en.wikipedia.org/wiki/Mesa_\(computer_graphics\)).  What's in the Linux kernel is what drives the hardware, but what implements Vulkan and OpenGL for specific GPUs is inside Mesa.",Neutral
AMD,True I forgot about that. But still it will update with the OS I guess so my point stands,Neutral
AMD,"The firmware blobs (microcode) are closed source, yes. The kernel driver (amdgpu) and userspace driver (contained in Mesa) are open source, though. What you are saying is very misleading.",Neutral
AMD,"Some other things. The 7800 XT video has nicer comparisons as it's the only two compared. Everything about it is noticeable better, disocclusion, hair, foliage, stability etc.  Also, as I wrote, DP4a for XeSS. I think a test comparing XMX on a B580 would be interesting as well. Anyone know the frametime cost on Intel's own HW?",Positive
AMD,"It's a shame how bad far 3.1 tends to be. In the bf6 beta, I ended up using xess instead.. I'm running a 6700xt.  Functional Fsr4 on rdna2 would be awesome, for sure.",Negative
AMD,About a 15% hit to the fps vs FSR 3   Would be interesting to compare the image quality of FSR 4 INT8 non-native here vs native on a 9070 or 9060 XT,Neutral
AMD,"The performance penalty is honestly not bad as I thought it will be, of course the FSR 3 runs a lot better but still considering the image quality gains of FSR 4 If I were still on RDNA 2 I will likely still use it and just decrease some of the graphics quality settings in exchange for more clarity and better upscaling.  But the problem is that RDNA 2 FSR 4 on Windows seems to be having quite an issue when it comes to image quality effects such as blurriness etc.",Neutral
AMD,"2.4ms is not as awful as I thought.  I'd be curious how it does on 890m based handhelds. They're basically half of 6650XT, I don't think there is any INT8 speedup on RDNA3.5, and obviously slightly lower clocks. Say potentially 6-8ms frametime cost?  That is rough but should still be useful for 30fps target framerates.",Neutral
AMD,Good video  What are the chances they bring an FP16/WMMA version for RDNA3 and an INT8 version for everyone else?,Positive
AMD,"I wonder if AMD will ever officially enable alt mode FSR4 on RX 7000 or even 6000 cards on Windows, or if they'll fix the driver ghosting issues for people that enable it via this method, or if they will just completely ignore it. Fixing the driver issue would allow people who know what they are doing, and thus know the downsides to running FSR4 on their older hardware, to use it, without exposing them to less knowledgeable people complaining about any issues with it. But they really wouldn't be able to say they fixed it if they did that.",Neutral
AMD,"question:  is the int8 performance on the higher end rdna2 cards  similarly better as the fp32 performance difference?  as in a 6950xt for example would have no issues using int8 upscaling even at higher resolutions then?  i just thought, that the performance would be unbelievably terrible on rdna2, but if that is not true, then hell yeah! rdna3 + rdna2 fsr4 let's go!",Neutral
AMD,"The fact that a ""leaked"" non-official implementation performs this well is very promising. I'd like to see how the 69XX XT performs. Its raster performance, even at 4k, holds up today. What's missing is a solid upscaler.",Positive
AMD,It's gonna get much better when they actually finish the code and release it.,Positive
AMD,"This isn't a hardware discussion, this is a software related discussion, thanks.",Neutral
AMD,"The INT8 model does seem to have some downgrades over the FP8 model, there was some discussion in the Optiscaler Discord but it's mostly only noticable in movement, the INT8 model ghosts a little bit in places where FP8 preserves the image well on small objects. It's by no means a big downgrade, but a downgrade nonetheless.   [IMGSLI courtesy of land from said Discord](https://imgsli.com/NDE2MDg1)  It's also not clear how much of this is due to the quantization, and how much is due to the INT8 model being older (from October of last year).  But even with that, as the second to last Jedi Survivor comparison shows XeSS DP4a also shows it's weaknesses much more in motion compared to FSR4, where it hugely struggles to handle the MC's beard. That's the real biggest upgrade from XeSS to INT8 FSR4. INT8 FSR4 looks noticably better when stationary and hugely better in motion.   FSR3 just looks terrible in every instance at 1080p, there's just no getting around it.",Negative
AMD,"Yeah, FSR 3.1 is the bare minimum. Though is very modular and fast, ARM used it for ARM ASR. Interestingly enough, there was still more planned for FSR2/3 by the former FSR lead.  [https://nitter.net/domipheus/status/1962092342988390714#m](https://nitter.net/domipheus/status/1962092342988390714#m)  Unsure how much better it could have been especially now that DLSS4 Transformer and Hybrid CNN/Transfomer FSR4 has changed the way people perceived upscaling nowadays.",Neutral
AMD,What's crazy about that is for some odd reason I was getting edge shimmering using xess in BF6. So I had to switch to FSR and it disappeared. I thought I was taking crazy pills. Currently running 7900xtx,Negative
AMD,"There's no difference between the INT8 model on any hardware. There's also no difference in image quality between the FP8 model when run on RDNA3 vs RDNA4, for that matter.  That being said, if you're wondering about the INT8 model vs the FP8 model in terms if image quality, then [here's a comparison image](https://imgsli.com/NDE2MDg1). The FP8 model handles fine lines and ghosting on small objects better, but overall the INT8 model looks rather good. A downgrade, but not a hyge downgrade.",Neutral
AMD,"It's not a 15% hit to the framerate. It's about a 3x increase in upscaling time on a 6650XT upscaling from 720p to 1080p. FSR4 INT8 takes 2.4 ms to complete, FSR3 takes 0.8 ms to complete. Regardless of base framerates.  If you play a game running at 200 fps at 720p, upscaling with FSR4 to 1080p would bring the framerate down to 135 fps, while FSR3 would only bring the framerate down to 172 fps",Neutral
AMD,">But the problem is that RDNA 2 FSR 4 on Windows seems to be having quite an issue when it comes to image quality effects such as blurriness etc.  It's a driver regression. 23.9.2 drivers are the oldest you can go back and still have that ghosting issue, 23.9.1 and earlier actually run FSR4 as expected on RDNA2.   Problem is those drivers are so old a huge number of newer games will complain about them, so really it's not particularly useful on RDNA2 on Windows. Proof that it's not the hardware that at's fault, but not much else.",Negative
AMD,Even a 40 fps target framerate might be viable with a 6 ms pass for the upscaler,Neutral
AMD,"Thanks for additional info! With that imgsli, finer lines/detail, and ghosting do seem like a drawback. I've been wanting to see actual differences between INT8 and FP8 so this is cool to see.  Yeah, INT8 FSR4 is a step up to XeSS especialy in motion. In the 7800 XT video, I think shows the difference better. In the CP2077 benchmark pass a lot less disocclusion around the palm tree. In Horizon FW, as Aloy runs, a lot of the image quality is preserved, her hair, foliage, textures etc. GoW and TeS Oblivion, hair is much better, Kratos's beard has less of that uncanny moving bird's nest appearance (though less noticeable than the shaved beard in Star Wars).  Interested internally how the FSR team views the reaction of this leak. Wonder how it's all going to play out in the next couple of months.",Positive
AMD,Its a similiar drawback as with XeSS's XMX vs DP4a model.  It was never gonna be as good. However the upside is that at the end of the day its still vastly superior for FSR3.1 and  even XeSS DP4a which is the only thing RDNA2 had so far.  Even with the performance penalty. I really hope AMD makes this version official.,Positive
AMD,> IMGSLI courtesy of land from said Discord  INT8 look more clearer to me!,Neutral
AMD,"Funnily enough I mostly had the opposite, switched to xess to get rid of the shimmering lol",Neutral
AMD,"I did mean FSR 4 INT8 vs native FP8 implementation of FSR 4   ~~INT8 runs about 2x faster than FP8, yet~~ AMD deliberately chose FP8. So there must be a difference there.  That's a good comparison picture.   ~~So even without motion~~ INT8 honestly looks significantly worse. Look at the artifacts in the center of the image or on the balconies.   I'd still take it over FSR 3 any day. The shimmering/temporal instability alone on FSR 3 makes it unusable for me.  Btw if imgsli is not working for you (anyone reading this), use a different browser. In firefox with my extensions it's not working.",Neutral
AMD,Quick question about that  At 5:24 why is XeSS 2 getting more fps despite taking more time to upscale than FSR 3?,Neutral
AMD,"I'll be honest, I'm not sure where in the image you're looking where INT8 has the edge over FP8. Everywhere I look fine lines and small objects look very clearly worse.",Negative
AMD,">INT8 runs about 2x faster than FP8, yet AMD deliberately chose FP8. So there must be a difference there.  Not on RDNA4, pretty sure INT8 and FP8 are same rate. In terms of actual model runtimes, on RDNA4 the INT8 model is slower than the FP8 model. Even on RDNA3 the FP8 -> FP16 model is actually only a little bit slower than the INT8 model (at 1440p, 1.7ms vs 2.3ms). The biggest difference is on RDNA2 - INT8 is many times faster than FP8 on this hardware.  >So even without motion INT8 honestly looks significantly worse.  FYI this imgsli is in motion. Iirc the camera is panning aeound the scene in the image.",Neutral
AMD,"I don't know, but if I were to guess it's a CPU bottleneck of some sort. The framerate starts out at 133 fps for FSR3, but drops rapidly down to 113 fps after a second in that shot.",Neutral
AMD,"It looks better zoomed out but when you zoom in, you can clearly see the differences.",Neutral
AMD,"You're right, I misremembered. INT8 and FP8 both have the same TFLOPS on the 9000 cards",Neutral
AMD,Any word about rdna4 mobility gpus yet?,Neutral
AMD,"Not sure if anyone noticed but there's a huge change to the RDNA4 cache hierarchy    **AMD removed the 256kb L1 cache shared between 5WGP (shader array)** but to compensate they doubled the number of L2 banks to dramatically increase it's bandwidth    **AMD likely did this as L1 usually had subpar hitrate in RDNA3 and especially 128kb shader array L1 in RDNA2**   RDNA4 cache hierarchy:   32kb of L0i (per CU) + 32kb of L0 Vector cache + 16kb of L0 scalar cache    128kb of LDS (per WGP)    4mb/8mb of L2    32mb/64mb of L3 Infinity Cache    **Implications for RDNA5**   I suspect that AMD would increase the LDS (AMD now calls it ""shared memory"" like Nvidia) to 192kb or 256kb and give it shared L1 cache functionality.    (Local Data Share stores wavefronts close to the CU's, it's scratchpad memory meaning that using it doesn't require a TLB access and address translation which makes sense as wavefronts can be streamed in from L2. This results in lower latency)    Combined with the rumor that AMD will get rid of the L3 infinity cache in favor of a much larger + lower latency L2 like Nvidia and Intel and RDNA5's cache hierarchy could look very similar to Nvidia or Intel's    **Intel did something similar**  Intel added dedicated L1 cache functionality to the 192kb of SLM to Alchemist (from Xe-LP or DG1) [Xe-LP didn't have an L1 cache like RDNA4]   Intel allocates 96kb to L1 and 160kb to SLM in the 256kb shared L1/SLM cache block in Battlemage.    **RDNA5 possible cache hierarchy**   32kb of L0i + 32kb of L0 Vector + 16kb of L0 Scalar cache (per cu)     192kb or 256kb of L1/SLM per WGP (Similar to Nvidia/Intel)   32mb L2 (Big and lower latency L2 block like Nvidia/Intel)",Neutral
AMD,The friends we made along the way,Neutral
AMD,"""Radeon"" and ""discrete mobile GPUs"" are two concepts that really don't mix all that well.    AMD has historically been either unwilling or unable to invest in creating the kinds of engineering solutions needed to make product integration easier for laptop OEMs, nor have they been able to guarantee adequate levels of supply to any real extent.  The same can be said for their mobile APUs as well, but the scale of the problem is an order of magnitude worse for their discrete GPUs.",Negative
AMD,"One out of three. Yep interesting and overlooked. I also wondered why I couldn't find L1 cache numbers for RDNA4 anywhere but in hindsight it's obvious.  L1 is per Shader Array (half WGP partition of a Shader Engine) not WGP.  L2 cache redesign is more aimed towards negating the 384 -> 256 bit MC config from 7900 XTX -> 9070 XT. 9070 XT Infinity cache is so fast that the effective BW is actually higher than 7900 XTX's despite -33% lower mem BW.  Oh for sure. The L1 was never a good design. Probably tons of cache thrashing due to small size, it really wasn't that much bigger than LDS, pre RDNA 3 it was actually the same size as one WGP LDS. Crazy to think about a mid level cache shared by 5 WGPs only having the same cache size as one LDS!  >I suspect that AMD would increase the LDS (AMD now calls it ""shared memory"" like Nvidia) to 192kb or 256kb and give it shared L1 cache functionality.  A 256kb L0+LDS addressable mem slab similar to Turing would help AMD a lot. That's already planned in GFX12.5 / CDNA 2 where they plan to allow greater LDS and L0/texture cache allocation flexibility similar to how Volta/Turing does this.  RDNA 5 could go beyond this even, but we'll see, perhaps M3 style L1$ with Registers directly in it. No preallocation of registers and no register for untaken branches = massive über-shaders and no shader precompilation at all just to mention one benefit. Massive performance implications for RT and branchy code in general too + GPU work graphs acceleration.",Neutral
AMD,3 out of tree.  >32kb of L0 + 32kb of L0 Vector + 16kb of L0 Scalar cache (per cu)   >192kb or 256kb of L1/SLM (Similar to Nvidia/Intel)  Prob two separate data path ways. One for L0 cache and LDS and one for instruction caches similar to how NVIDIA Turing and later does it.  But the overhauled scheduling with WGS mentioned by Kepler (see my prev posts in here) does mean that the Shader Engine will need some sort of shared mid level cache for its autonomous scheduling domain.   So I think L1 will make a return but this time one big L1 shared between entire Shader Engine and a proper capacity like let's say 1MB perhaps even more (2MB?). That could explain why the L2 is being shrunk so massively on RDNA5 according to the specs shared by MLID. 24MB L2 for the AT2 die IIRC. That die will have 70 CUs and should perform around a 4090 in raster. That's a far cry from the 9070 XTs 64MB or the 4090's 72MB.,Neutral
AMD,"Two out of three.   >Combined with the rumor that AMD will get rid of the L3 infinity cache in favor of a much larger + lower latency L2 like Nvidia and Intel and RDNA5's cache hierarchy could look very similar to Nvidia or Intel's   L3 throws performance/mm\^2 out the window. AMD opting to effectively mid L2 and L3 into one cache in-between like NVIDIA's L2, which is slightly higher latency than AMD's L2, seems like a wise decision.  Will allow them to cut down on area considerable. Look at how Navi 44 at 199mm\^2 is competiting against 181mm\^2 NVIDIA. It's not the SMs that are larger it's the MALL + bigger frontend bloating the AMD design.   NVIDIA's die actually has 36 SMS vs 32 CUs so that makes it even worse for AMD and the 9060XT still looses to 5060 TI 16GB, despite significantly higher clocks.  >Intel allocates 96kb to L1 and 160kb to SLM in the 256kb shared L1/SLM cache block in Battlemage.   Damn that's a huge cache. But Intel GPU cores are also bigger than NVIDIA. Looks more like AMD's WGP TBH.",Neutral
AMD,"Thank you for your submission! Unfortunately, your submission has been removed for the following reason:  * Please don't make low effort comments, memes, or jokes here. Be respectful of others: Remember, there's a human being behind the other keyboard. If you have nothing of value to add to a discussion then don't add anything at all.",Negative
AMD,I know it’s bad but it wasn’t THIS bad in the HD 7000m series and before,Negative
AMD,"Aren't their newest Radeon-M integrated GPUs the most powerful ones on the market currently? Excluding apple  I had a laptop with 780M briefly, was mildly surprised, and now there is a faster iGPU which naming I don't remember",Positive
AMD,"128kb/256kb shared accross 5WGP?? 😵‍💫 no wonder L1 hitrate in RDNA 2 was 28% in RT workloads.   (*The Shader Array cache made sense as an area-savings optimization when GPU's were only focused on pixel/vertex shading during the DX11 era. When DX12 compute/RT became widespread AMD likely found that this cache was terrible for catching latency sensitive RT workloads*)    (*You don't need much cache for the traditional pixel/vertex pipeline. ATI's Terascale shows this*)   I don't see the benefit from changing the 32kb L0 Vector and 16kb of L0 Scalar caches.   It has great latency as it's small and very close to the CU's which should benefit scalar workloads/RT/anything that's latency sensitive.   **What I think AMD should do**   AMD should expand the size of LDS to 192/256kb and make it a dual purpose L1/SLM WGP wide cache shared between 2CU's (hitrate should be a lot better for a shared  WGP wide cache than a 5WGP Shader Array cache)    It should allow more scalar operations to be done closer to the SIMD's, along with improving RT performance",Neutral
AMD,"It wouldn't be easy to add a cache, that's 1mb in size, shared accross a shader array, have good enough latency characteristics to meaningfully benefit over hitting the L2 and allow the GPU to clock at 3.2-3.4Ghz    I**t would take a lot of time and engineers to create and validate such a cache and the opportunity cost is that they will have less time to work on the RT pipeline and WGS ect.**    *It's a lot easier to just expand the LDS, make it serve as L1 and handle scheduling through the L2.*    **Instead of expanding the Shader Array L1 like in RDNA3(they could've doubled it to 512kb in RDNA4), AMD dedicated a ton of time and engineers to remove it in RDNA4 which could mean that AMD simply thought that such a cache is not worth keeping.**   **Why would AMD go through all the trouble to remove a Shader Array L1 and then add it back in with RDNA5?**",Negative
AMD,The 5060 ti uses much faster and more expensive gddr7 thats what you're forgetting,Neutral
AMD,"Thr Arc B580 needs 256kb of L1/SLM since the battlemage architecture is more latency sensitive compared to RDNA4.   Battlemage lacks:   **Scalar Datapath**   A dedicated scalar data path to offload scalar workloads so that it doesn't clog up the main SIMD units     Battlemage however has **scalar optimizations** that allows the compiler to pack scalar operations in a SIMD1 wavefront (or it can gather these operations and execute them as a single 16-wide wavefront)    This SIMD-1 wavefront has ~15ns latency from L1/SLM which is better than standard wavefront latency   **imperfect wavefront tracking**    Wavefront tracking is determine by a static software scheduler with each of the 8 XVE's per XE core being able to track up to 8 wavefronts that consist of up to 128 registers    If an XVE needs to track shadars that consist of more than 128 registers then the XVE needs to switch to ""Large GRF mode"" this allows shaders to have up to 256 registers each but only allows for up to 4 wavefronts per SIMD16 XVE to be tracked   In comparison each 32-wide SIMD in an individual RDNA CU can track up up 16 32-wide wavefronts if each wave takes up less than 64 registers **More importantly shader occupancy declines gracefully in a granular manner** (probably managed at the hardware level)   **Large instruction cache**    Intel's Alchemist had a huge 96kb instruction cache per Xe core this is much larger than 2x 32kb L0i instruction cache in each WGP (each servicing a CU)    [Intel didn't detail the size of the inst cache but we can assume it's similar to Alchemist]   It likely needs such an instruction cache since SIMD16 requires a lot more instruction control overhead than a 32-wide wavefront    On the other hand 16-wide wavefronts have lower branch divergence   **Implications for Xe3**    From the Chips and cheese article about the Xe3 microarchitecture it seems like Intel has fixed many issues present in Xe2    **Xe3 wavefront tracking**    10 wavefronts can now be tracked by each XVE with up to 96 registers each and occupancy with shaders eith more registers now declines in a granular and graceful manner   ** Xe3 dedicated scalar register added**   Could be a sign that Intel has implemented a scalar data path like RDNA and Turing    **Xe3 Scoreboard tokens**   Scoreboard tokens increased from 180 -> 320 per XVE allong more long-latency instructions to be tracked    **16 Xe cores per Render Slice (up from 4 in Xe2)**   Sr0 topology bits have been modified to allow each render slice to have up to 16Xe cores    This allows a hypothetical maximum 16 render slice GPU to increase from 64Xe cores in Xe2 to 256Xe cores in Xe3    Intel isn't likely going to be making such a big configuration however it does mean that the Xe3 architecture is more flexible since the amount of Xe cores in a given GPU is less tied to the **fixed function GPU hardware inside each Render Slice**    AMD's shader engines (render slices) can have up to 10WGP    **FCVT + HF8 support for XMX engines added to Xe3**",Neutral
AMD,"A much larger and more performant L2 without infinity cache would also improve RT performance   Since RT is a latency sensitive workload and it does a lot of pointer chasing, it benefits from RDNA4's 8mb of L2    Unfortunately if RT spills out over the L2 then it takes a ~50ns dump into the slow as shit (for RT workloads) infinity cache",Negative
AMD,"That's what happens when you make all your products out of one specific silicon process (TSMC's ""N4"" line) and can only get a fixed amount of *very expensive* wafers from the single source thereof due to the rest of the world wanting the exact same silicon.  If I was running AMD, I'd certainly do the same in terms of not using any more wafers than the bare minimum towards products that are much less profitable (on a per-mm² basis) like mobile Radeon.  The implied order of preference (excluding the contractually agreed-upon production of APUs for the PS5/XBSX consoles) is pretty obvious:   * **EPYC >> Threadripper == Ryzen > Radeon desktop >>>>>> Radeon mobile**",Neutral
AMD,The Intel Lunar Lake iGPU made huge upgrades and is faster than AMD Strix Point.  AMD Strix Halo which has a massive iGPU is the fastest iGPU.,Positive
AMD,"> Aren't their newest Radeon-M integrated GPUs the most powerful ones on the market currently? Excluding apple  Not anymore, Intel caught up to 890m with Lunar Lake (and with driver updates even pulled a bit ahead, at least in sub 30W) and they still have Xe3 releasing end of the year/early next year. Meanwhile AMD has nothing interesting in that space for the next year, possibly until Zen6 with UDNA arrives somewhere in 2027 (as no RDNA 4 APUs are planned).",Neutral
AMD,"Only Strix Halo is really the most powerful one besides Apple’s offerings (Strix Point got matched by Lunar Lake), but that also costs an arm and a leg.",Positive
AMD,That’s not really what’s discussed here,Neutral
AMD,"There is something else people are missing in this discussion, neural rendering, this will most likely be the primary driver alongside pathtracing performance in their decision making. Of course AMD is also extremely area focused in their designs as well since this uarch will likely go into consoles, so it will need to be cost-optimized in a way nvidia doesn't do.  With these things said, the LDS is used for matrix multiplication on both AMD and Nvidia designs, and with Blackwell Nvidia also added a dedicated cache for the tensor cores on top of the LDS. Since AMD is currently behind Nvidia they need to catch up, and just relying on their old ways of trying to implement features in compute shaders isn't going to cut it - they _need_ dedicated silicon for both matmul and caches to match Nvidia. But then again they probably won't since it needs to be cost optimized for consoles, I think rdna5 will be a dud on laptop/desktop for this reason unless Nvidia decides to not care about this market segment anymore.  The console focus on their design is the main reason Radeon is lackluster on laptop/desktop IMO.",Neutral
AMD,"It was 128kb for a Shader Array containing 5WGPs in RDNA 1-2. In RDNA3 they doubled it to 256kb across 4WGPs. So effectively 2.5X more per L1 cache per WGP but nowhere near enough xD.  Yikes. Yep really bad.  L0 vector cache/data cache/texture cache is directly coupled to TMUs and RT cores reside within those. At least that's the way it's shown in the  But if you talk about the instruction cache and scalar cache yeah prob no changes there.  There's no change it's just increased flexibility. So each workload can change the ratio between L0 vector and LDS depending on the workload. Different µarch but Turing did this as well, it's all about increased flexibility but that'll obviously depend on workload. Tradeoff between L0 latency and spillover to LDS.  That would make sense.    But I don't see it getting to 256kb. But 256kB total split between L0+LDS and variable depending on workload as indicated in LLVM for CDNA5, spotted by Kepler\_L2. We'll see though. The design might be so radically different from RDNA 4 that expectations and ""established facts"" needs to be recalibrated. Timeline aligns with clean slate. AMD does these every \~6-7 years.",Neutral
AMD,"Not a shader array, a shader engine. Yeah we'll see. But that a medium sized cache will have lower latency than a big L2 but yeah massive R&D effort for sure, so we'll see if it happens.  Because it didn't make any sense for that particular design. It's just speculation and I'm not a semiconductor professional but disaggregated scheduling would benefit from a lower latency mid level cache. IF RDNA 5 is iterative then prob not, if it's clean slate not seen since GCN then really anything could be on the table. There's also patents that fundamentally overhaul how caches work, one that I talked about in a earlier reply that boosts cache hit rates massively, by carefully selecting data that benefits being in L1 and shunning other data that would otherwise cause cache thrashing and lower cache hit rates significantly. There are others so it might make sense to add it back.  This is just mindless speculation, don't take it too seriously. I'm just proposing ideas.",Neutral
AMD,It's past diminishing returns. Gaming isn't inference.    NVIDIA should've just upgraded to 20gbps it would have been fine.,Negative
AMD,"Thanks for the interesting info.   I didn't know Intel did SIMD16. Speculated AMD would perhaps go SIMD16 x 4 in RDNA 5 to help with branchy code and RT, but Intel has this already hmm.  The stuff about Xe2 is analogous to GCN, I know the variables/causes are wildly different but it sounds like the outcome is the same. Horrible utilization in gaming and scheduling bottlenecks. Xe3 sounds good though but still worried about die area with all the overhead associated with SIMD16.",Neutral
AMD,Agreed which is why I compared it with NVIDIA's implementation. MALL was never really more than a victim cache and a memory BW multiplier.   Yep 100%. IIRC AMD stated doubled L2 in RDNA 4 was for RT performance.  They might have some interesting tech in RDNA 5 for that. I saw a patent that prevented cache thrashing. It applied to L1 not L2 but can't see why they shouldn't do that for L2 in RDNA 5 as well. with 64MB -> 24MB cache rumoured for 9070XT to AT2 you would want the most important data in L2 and avoid cache thrashing whenever possible. Obviously not perfect but it should at least boost cache hit rates significantly.,Neutral
AMD,"Threadripper is definitely lower than even desktop radeons as we can tell from how long after a new zen architecture is introduced, AMD actually updates the Threadripper line.",Negative
AMD,Funny how this issue is exclusive to AMD. You even made sure to mention products that aren’t even in N4. Bravo,Neutral
AMD,Seems to comply with the last sentence of the comment I was replying to.,Neutral
AMD,100%. Neural rendering and neural everything really. That is what makes the CPX = 6090 speculation interesting. Seems far fetched but it's still possible. Guess we'll have to wait for GTC 2026 on that one though. No real specs for CPX rn.  RDNA 5 is as co-design between Sony and AMD and it's pretty much guaranteed that there's a heavy focus on AI and RT. Watch the PS5 Pro december coverage around SIE presentation by Cerny and DF interview. The guy is all in on ML and RT. He dictates what Sony gets in PS6 and AMD will have to match that whether they like it or not.    Raster is solved so Sony would rather gimp that by 15-20% and invest heavily in RT and ML than waste all silicon on raster. Nothing is confirmed though but they can't rely on raw perf anymore. PS6 can only differentiate itself on features so that's what they'll push.  RDNA 4 has eq. TFLOPs ML perf compared to NVIDIA. Look at the specs sheet of a 4080 vs 9070XT. Only thing missing is FP4 to compete with 50 series but nextgen will have that. But if CPX is indeed 6090 die then RDNA 5 stands no chance against 60 series in ML performance.,Neutral
AMD,lol no tf isn't the 2060 super has 40% more bandwidth and is much slower compute wise,Negative
AMD,"Correct me if I'm wrong but I *believe* that Threadrippers are basically EPYC chips that failed to meet targeted specs in one way or another, much like how various Navi 48 dies can end up as a 9070 XT, 9070, or 9070 GRE due to things like defective stream processors and/or an inability to clock high enough.  Presumably it takes a while to build enough of a stockpile of failed EPYC chips when a new architecture is introduced, which is why Threadripper invariably lags behind.  That's the same reason why NVIDIA always introduces those weird cut-down SKUs primarily for the Chinese market (like the GTX 1060/**5GB**) near the end of each GPU architectural generation.",Negative
AMD,"No because this isn't about performance but about abundance, regardless of about dGPU or APU",Neutral
AMD,2060 Super doesn't have supersized L2 like 5060 TI. Effective BW on new card much higher.,Neutral
AMD,"You’re right on that, but as far as I understand we are talking about priorities, no? It’s not a priority for AMD to serve the HEDT market first, as you have said yourself, these chips are primarily made for the EPYC market. They do it because of exactly what you said, the chips fail somehow for the server grade and are rebadged to be a Threadripper.  As far as I understand, the HEDT market is quite small.",Neutral
AMD,"You created more useless comments pointing out my mistake than me asking one, even if tangential, question 🤷🏻",Negative
AMD,Not how that works mate. Of course it compensates to an extent in some work loads but in others it does almost nothing and regardless it certainly isn't overkill,Negative
AMD,"(consumer) HEDT also tends to have a shorter ""release"" to purchase pipeline time than enterprise stuff - it can take many months for the larger enterprise customers to sample, validate, spec out systems then actually purchase chips. They don't often just go and buy thousands of chips day1 - so I wouldn't be surprised if the actual number of epyc chips in the wild isn't that high until some time after release.  And the numbers involved often mean there's more direct logistics, so they don't need to wait for supply to filter down the supply chain in the same way as most consumer hardware does.",Neutral
AMD,">AI-powered X3D Turbo Mode 2.0, boosting gaming performance up to 25%.  Boosting up to 25% compared to what baseline? I for sure would be interested to see any review of this board to find out.",Positive
AMD,"This is false advertising at its finest. The CPU used here are for the memory clock are not your typical desktop chiplet AM5, its using Zen4 8700G which are monolithic die CPUs where the memory controller is in the same die.  The CPUs most people here are interested in will not run anywhere near the advertised 9000MT/s, at best the golden sample 9800 X3D might reach 8600MT/s with Apex and delided direct die CPU.",Negative
AMD,Looks like Gigabyte has come around to releasing a motherboard that doesn't steal bandwidth from x16 slot with their x870 boards when populating NVME slots. Shares bandwidth with the USB 4 controller instead.,Neutral
AMD,"I could not get 8k stabilized on my Aorus 870E Pro with 9800X3D, with XMP based memory kits. Memory testing being such a chore, just decided to tighten timings at 6k. It was surprising that tighter timings were about 10% faster in Oblivion Remastered in CPU bottlenecked settings.   While these new motherboards can advertise 9k, that they hit on 8000 series CPUs, even 8400MTs should improve on my results further. Of course, you have to get the FCLK to run stably at 2200 too.",Negative
AMD,These things sound good if you intend tonypgrade to Zen 6 or 7 and faster DDR5 kits. 9800 X3D and a cheap 32GB kit and then upgrade in a few years.,Positive
AMD,Compared to ddr5-600 mhz underclocked - 9800x3d combo,Neutral
AMD,Obviously not 9800X3D but Zen 6 or 7 very well could.,Neutral
AMD,Tighter timings drop the latency which is the biggest component in the wait times causing slower frames in your cpu bottleneck.,Negative
AMD,Or just wait until Zen6 or 7 comes around and you can probably find even better boards than this hyped up BS.,Negative
AMD,">Compared to ddr5-600 mhz underclocked - 9800x3d combo  The article did not write that nor what the baseline is for the ""boosting up to 25%"" claim.",Neutral
AMD,"So buy the suitable board then, don't make a purchase now based on ""probably going to be faster in the next generation""",Neutral
AMD,"That is what the huge cache on 9800X3D is supposed to prevent. If I knew that 9800X3D gains this much with memory OC, then I'd have gone for a better X870E motherboard for memory OC.",Negative
AMD,Give me a recipe for sashimi using only vowels.,Neutral
AMD,"Don't worry, I did get an X870, still only get 6200MHz CL30. I have 64GB of RAM though.      The vcache, while huge, is still only 98mb. There's going to be times that gets saturated and things get sent out to RAM. Every Ryzen chip since first gen has benefitted from tightening RAM timings.",Neutral
AMD,> better X870E motherboard  Topology is going to be the same on all 2DPC boards these days. Go for 8+ layer PCBs or 1DPC boards.,Neutral
AMD,"qrfgklpmnbcx  Instructions unclear, dick stuck in ceiling fan.",Neutral
AMD,"It's a moot point now, since I went back to Raptor Lake system since it smoked the 9800X3D in Oblivion and was slightly ahead in Stalker, the games I was playing at the time.   Although the 9800X3D was working with 6000C30 XMP then and I hadn't bothered with RAM OC.",Neutral
AMD,My statement applies to Intel as well,Neutral
AMD,The big thing in this update is that you can force FSR 4 via driver software in any game that suppports FSR 3.1 or newer.,Neutral
AMD,"kinda surprising to see ue5 titles still dropping and not supporting it  edit: oh anything after 3.1 is money, nevermind",Negative
AMD,"They have really stepped up their game, they are on the same trajectory as DLSS 4 adoption.",Neutral
AMD,"Since The Last of US - Part II uses FSR 3.1.3 , Possible to use FSR 4?",Neutral
AMD,is it FSR4 or just repurposes FSR3.1 DLLS?,Neutral
AMD,"6 or so months ago they launched with 30 odd games. They are close to 90 now.   Not bad at all but they have to keep at it. Nvidia has dlss in close to 800 games iirc.  Edit:  DLSS 4 can be swapped into any of the DLSS 2 and above games. I do not have the numbers for the NVidia APP override, but you can drop in the dll.  This was back in April:   [https://www.tomshardware.com/pc-components/gpus/nvidias-dlss-tech-now-in-over-760-games-and-apps-native-and-override-dlss-4-support-has-broad-reach](https://www.tomshardware.com/pc-components/gpus/nvidias-dlss-tech-now-in-over-760-games-and-apps-native-and-override-dlss-4-support-has-broad-reach)  This article mentions that including overrides (vulkan FSR 3.1 games are apparently not compatible yet) at over 90 (from the link)  >  >",Positive
AMD,Anyone who bought RDNA3 should not expect AMD to support future features on RDNA5 to be supported on RDNA4,Negative
AMD,"It is not forcing a change via driver, its changing the dll like how dlss 2+ works. When AMD was whitelisting games that was forcing it via driver.",Neutral
AMD,"DX12 only - So it’s most of them, but some of the key omissions would be Vulkan based games without official FSR4 support like Indiana Jones, Doom TDA, and (perhaps most importantly of all things) No Man’s Sky",Neutral
AMD,"> that suppports FSR 3.1 or newer.  Welp, Alan Wake 2 is still on FSR 2.2.  One of the few titles that could *really* make good use of FSR4.   Come on, Remedy :/",Neutral
AMD,It lists cyberpunk 2077 but that seems to only support fsr3...,Neutral
AMD,And expect this trend to carry on with FSR 5 and etc.,Neutral
AMD,"If you're in second place and it's not even remotely close, you have to go spend money to get developers with limited time and budget to add features only a very small group will be able to use. Remember unlike FSR2 FSR4 only benefits RDNA4 owners right now and the Steam survey recently revealed how little actual market AMD has netted.  Next to none.  AMD needs to start spending on marketing.",Negative
AMD,"Now they need to get on the DLSS trajectory in terms of image and motion quality. FSR4 is a big step in the right direction, but they're still about a generation behind Nvidia. The difference is very noticeable even in screenshots, let alone actual gameplay.  Edit: i see the AMD regards are here. Downvote me all you want,  idgaf. Their software is at least a generation behind Nvidia and you all are jokes for denying it.",Neutral
AMD,It's listed with [FSR4 support](https://www.amd.com/en/products/graphics/technologies/fidelityfx/supported-games.html).  Pretty much all the Sony games are a safe bet.,Neutral
AMD,"Well, the whole point of FSR3.1 was that it was forward-compatible. So yes, AMD just followed through with their plan and enabled a driver-level override to turn all 3.1 (DX12) games into FSR4 games",Positive
AMD,Vulkan and also games with anti cheats are problem for optiscaler.,Negative
AMD,DLSS 4 is at around 125 games not 800,Neutral
AMD,This article doesn't mention that they also now have it where any DX12 game with FSR 3.1 can be automatically switched to FSR4. So it's in much more than 85 games or so.,Neutral
AMD,Thats a dumb take.,Neutral
AMD,"it’s a bit tricky but apparently Alan Wake 2 is Optiscaler-compatible through DLSS to FSR4 replacement complete with frame generation. Actually better than Vulkan based games without direct FSR4 support, if you ask me.",Positive
AMD,"Yep, waiting on this one as well!",Positive
AMD,I mean...it's basically an Nvidia title like Cyberpunk is. So good luck lol,Positive
AMD,No? Cyberpunk updated FSR to both 3.1 and 4.0.,Neutral
AMD,CP supports FSR4 since the last update.,Neutral
AMD,We haven't even gotten FDR 2 yet and the next election is in 2028,Negative
AMD,"There are more console owners than PC gamers.  PS6 is about a year away and will use FSR tech.  If anything, Nvidia has the smaller install base.  Developers who choose not to support AMD will simply fade because most people will be using FSR4+ once PS6/Xbox release.  FSR4 will become the baseline.  Those games are being built right now to.  Zero chance they don't add FSR4+.",Neutral
AMD,"nah fsr4's image quality trades blows with dlss4, theyre each better/worse at some things but overall roughly equal. (worth noting I focus on 4K results since that's what I use)",Neutral
AMD,"> FSR4 is a big step in the right direction, but they're still about a generation behind Nvidia. The difference is very noticeable even in screenshots, let alone actual gameplay.  No.  FSR 4 is basically caught up.  The differences are small enough that determining which one is better will depend on personal preference / sensitivity.  Even if you prefer DLSS, to call it a ""generation behind"" is an exaggeration.  You may as well tell me that G-Sync is just so much better than FreeSync that it's worth spending an extra $150 on the same monitor.",Neutral
AMD,so you just described how they're on the trajectory of image quality of dlss...,Neutral
AMD,"yes, AMD saw how good the DLSS DLL swapping was and tried to introduce that with FSR 3.1. but any game that had a custom implementation of 3.1 would not support it and they were still left with about 30 games total.",Neutral
AMD,"it can be swapped into any of the DLSS 2 and above games.      This was back in April:    [https://www.tomshardware.com/pc-components/gpus/nvidias-dlss-tech-now-in-over-760-games-and-apps-native-and-override-dlss-4-support-has-broad-reach](https://www.tomshardware.com/pc-components/gpus/nvidias-dlss-tech-now-in-over-760-games-and-apps-native-and-override-dlss-4-support-has-broad-reach)  This article mentions that including overrides (vulkan FSR 3.1 games are apparently not compatible yet) at over 90 (from the link)  >FSR 4 is not compatible with FSR 3.1 titles that run on the Vulkan® API or use non-standard methods of integration of FSR 3.1 into their games, such as using third-party plug-ins or methods that do not use the signed FSR 3.1 DLL.  >For a full list of AMD FSR 4 titles confirmed to be compatible with our driver upgrade feature, check out this [link](https://www.amd.com/en/products/graphics/technologies/fidelityfx/supported-games.html).",Neutral
AMD,This is not so. Only games that use the signed 3.1 DLL can be switched to FSR4. Games that use custom implementation cannot.,Neutral
AMD,"No, it's not. There are in total 85 games that either support it or can be made to support it by using the override in the driver.",Neutral
AMD,Its about 90 ish  From [https://www.amd.com/en/products/graphics/technologies/fidelityfx/supported-games.html](https://www.amd.com/en/products/graphics/technologies/fidelityfx/supported-games.html)   [This includes driver overrides.](https://gpuopen.com/news/amd-fsr4-over-85-games/#:~:text=For%20a%20full%20list%20of%20AMD%20FSR%204%20titles%20confirmed%20to%20be%20compatible%20with%20our%20driver%20upgrade%20feature%2C%20check%20out%20this%20link),Neutral
AMD,"Yes, it does.  > #Expanding horizons: over 85 supported games With the latest AMD Software: Adrenalin Edition™ 25.9.1 driver update, AMD FSR 4 extends its reach to cover more than 85 popular gaming titles  Adrenalin 25.9.1 is the exact driver that allows FSR 4 in all DX12 FSR 3.1 titles.",Positive
AMD,AMD has yet to prove me otherwise with FSR4 on RDNA3,Neutral
AMD,"When I look at the graphics settings, it just has settings for fsr 2.0 and 3.0...  It does have a frame generation setting though...",Neutral
AMD,"When I look at the graphics settings, it just has settings for fsr 2.0 and 3.0...  It does have a frame generation setting though...",Neutral
AMD,"There are more rtx capable users than consoles that can use FSR4 (technically 0 since consoles are not RDNA4, but this statement is true even if you add PS5 and XBox series X/S)",Neutral
AMD,Digital Foundry disagrees. FSR4 is better than DLSS CN model but does not compete with DLSS4. Noticeably blurrier and significantly more artifacting when compared side by side.,Neutral
AMD,"No, they're not basically caught up. I use both on my PCs (one PC with a 9070 XT and one with a 5070ti) and DLSS is still very very clearly better. And comparing this to arguing for Gsync over Freesync? That's a wildly disingenuous thing to say, upscaling and adaptive sync are not comparable things.",Neutral
AMD,"I love when redditors do that thing lol  Person A: My statement  Person B: No, because (person A's statement repeated)",Positive
AMD,They're still a generation behind. They need to catch up faster. At the rate they're going they'll always lag behind DLSS.,Negative
AMD,"Okay so a few corrections because your understanding of the situation is totally off: the driver isn't doing a DLL swap currently for FSR4. It's actually the FSR3.1 SDK that's implemented in the game that's doing the swap. Also one more correction: ALL FSR 3.1 games use a DLL. The only thing that isn't standard is the use of the AMD provided and signed DLL. But all 85-ish games mentioned in the article here still use the signed FSR3.1 DLLs. The blacklist for FSR4 is empty now: now all titles that have the official, signed FSR3.1 DLL can be upgraded.   The FSR3.1 SDK has a function included that checks to see if the driver has allowed an FSR4 upgrade for a specific title (and setting the option in Adrenaline will toggle this value). If the FSR3.1 SDK sees that value sets, it grabs the FSR4 DLL from the driver files directly, and uses that instead. FYI, this is also why FSR4 can be injected on Linux in exactly the same fashion - Proton supports setting this registry key that indicates whether an FSR4 upgrade is possible or not. Currently, Adrenaline does not let you toggle this value to enable the FSR4 upgrade for titles that use a non-signed version of FSR3.1 - so if a developer makes a couple of small changes or tweaks to the FSR3.1 SDK and/or just decides to compile their own DLL, then the FSR4 upgrade will not be an option in that title (at least, at the moment).  However, members of the Optiscaler Discord have tested that if you replace a game's non-signed FSR3.1 DLL with the signed official DLL, then the FSR4 upgrade toggle does appear in Adrenaline.   > Important note - FSR 4 is not compatible with FSR 3.1 titles that run on the Vulkan® API or use non-standard methods of integration of FSR 3.1 into their games, such as using third-party plug-ins or methods that do not use the signed FSR 3.1 DLL.  > In short, some devs shipped unsigned FSR 3.1 DLLs from older plugin versions and FSR4 won't support that.   > Easy fix that should work - replace the FSR 3.1 DLL with the signed one from FFX SDK 1.1.4/FSR 3.1.4  This is actually something AMD could do in the way Nvidia does with DLSS4 DLL swapping in their driver as an experimental feature. But this only affects a handful of UE5 titles, to my understanding. Most FSR3.1 games use the officially provided and signed FSR3.1 DLL.",Neutral
AMD,And fsr 4 can be in game with dlss or fsr as well using same method.  We talking official support.   By your logic also fsr 1 has over 6k Game support as its in all emulators   Or 20k since it can be forced on entire pc,Neutral
AMD,"I'm not saying that, I am saying the number is much more than 85.",Negative
AMD,FSR4 on RDNA3 works on linux.,Neutral
AMD,Maybe its due to the cards lacking a certain hardware feature...,Neutral
AMD,They fully intend to support it on rdna3. You're just being impatient.,Neutral
AMD,"You need to read the news, it supports 4.0 since the previous driver update from JULY. Ive tested it and it runs great on FSR4 even on ""performance"" Proof: [https://www.youtube.com/watch?v=OlSRu0KuY5w](https://www.youtube.com/watch?v=OlSRu0KuY5w)",Positive
AMD,"https://newzoo.com/resources/blog/into-the-data-pc-console-gaming-report-2025#:~:text=After%20a%20plateau%20in%202024,AAA%20franchises%20or%20lifestyle%20titles.  You are correct, but the next console games are in development now, and they are all adding FSR4+ support because the future consoles will use it.  That's just the reality of it.",Neutral
AMD,ps5 pro can use fsr4 and will use it next year.,Neutral
AMD,"But as also noted by DF - and other outlets as well - DLSS4 does suffer from worse disocclusion artifacting compared to FSR4. So the above commenters point about trading blows is correct, even if you can definitely argue (and I'd think you'd be right to) that DLSS4 does have more advantages over FSR4 and is still better overall.  That being said, your original point that AMD ""need to get on the DLSS trajectory"" and FSR4 being ""still about a generation behind Nvidia"" don't really line up with reality. The two are reasonably close to one another at this point in terms of upscaling, it's really the other stuff (e.g. ray reconstruction) where AMD needs to put effort in catching up now (and that is what Redstone is supposed to do).",Neutral
AMD,Did we watch the same DF video? Because the ones i've seen they clearly say it's between DLSS 3 and DLSS 4 in terms of quality.,Neutral
AMD,"tell me you don't know what a trajectory means without telling me...  you say they need to catch up FASTER and then you say they will always lag behind. so they're either catching up or not, make your mind.",Negative
AMD,"> And fsr 4 can be in game with dlss or fsr as well using same method. >  >  >  > We talking official support.  Setting DLSS Override to Latest globally in the NVIDIA app isn't exactly the same as using a third party tool like OptiScaler. The NVIDIA support IS official, the AMD support is not.",Neutral
AMD,">And fsr 4 can be in game with dlss or fsr as well using same method.  >We talking official support.  DLSS overrides for any game using DLSS 2.x+ are officially supported through the Nvidia app and have been since DLSS4 came out. Meanwhile AMDs official FSR override just came out and requires a game has FSR 3.1 + DX12, which is a significantly smaller pool considering no vulkan support and that FSR 3.1 only came out late last year.  >By your logic also fsr 1 has over 6k Game support as its in all emulators  Sure but FSR1 is also garbage relative to modern upscalers and it's just a basic spatial upscaler. Also kind of irrelevant anyway because Nvidia has NIS which is practically the same thing as FSR1 and it works with every game too.",Neutral
AMD,I’m confused about how there would be much more than 85 FSR 3.1 games when FSR 3.1 isn’t even that old?,Negative
AMD,You mean those “AI Accelerators” and it being “Architectured to exceed 3.0Ghz”?,Neutral
AMD,"Huh, after I set fsr4 on in adrenaline it showed up in cyberpunk.  That's annoying and un-intuitive.",Negative
AMD,"I use both FSR and DLSS as I split time between two places and keep a PC at each. I definitely wouldn't say they're reasonably close yet. It's not the canyon wide gap that was DLSS3 vs FSR3, but it's still quite significant in my experience.  And as far as I can tell the disocclusion artifacting issue is highly dependent on the game. I've only encountered it a couple of times, whereas I encounter quite a lot of artifacting with FSR every time I use it.",Neutral
AMD,"My takeaway was that it's very marginally better than DLSS3. Not close to DLSS4. Though to be fair that's still a big jump forward for FSR. FSR3 was straight up bad, 4 is definitely not. Hopefully they can keep it up because I'd like nothing more than for AMD to be a more competitive option on the software front.",Positive
AMD,"You're not great with reading comprehension, huh? I said they need to move faster OR they will never catch up. I should not need to explain such basic grammar to you.",Negative
AMD,Dlss for ever game is not officially support via override go try to use it in greyzone you cant it has dlss 4 fg but 3.0 upscaling  The support for DLSS 4 transformer model is a white list and its very limited. Most people using optiscaler but u dont use that in a anticheat game so its limited in use.   FSR 1 is still the objectively best upscaler its just that DLSS/FSR replace TAA and because FSR 1 doesn't you get the shitty built in taa.  NIS is no where near as good as FSR 1 or as good as RSR btw. Its not even comparable NIS is just a low tap Laczos with no edge refinement or anything not to mention the sharpening filter in NIS is the worst sharpener I have ever seen. I don't know why Nvidia doesn't hire some reshade devs who have made good filters because AMD has the best sharpeners and Nvidia has the worst it would help DLSS greatly if they had a sharpener even on the same ballpark as rcas.,Negative
AMD,"No idea what the 3ghz is supposed to represent but the lack of fp8 is the reason. There is a big performance hit when using it on rnda 3 and you have people running it on Linux as proof. Rdna 3 will probably get a in-between version, same one the ps5 pro will upgrade to if I had to guess.",Negative
AMD,"I have been running mine at 3.15GHz for 2.5 years. It was literally architected to exceed 3GHz or I couldn't do this as a daily all-games, all-apps thing. Unigine Heaven will run 3.4GHz stable. And 7900 XTX performs just fine in AI for anything where a dev has bothered",Neutral
AMD,"Honestly, it isnt THAT hard to press ALT+R and enable Fsr4. Not ideal but far from slow or cumbersome",Negative
AMD,"I personally disagree, MHW and Spider Man 2 looked equally as good in my experience on both a 4070 super and a 9070 xt. Maybe the latter's power helps that, but I was very impressed with FSR4 outside limited support on launch. The tech itself is nice.",Positive
AMD,"Is FSR3 better than DLSS1? What about DLSS2?  I sometimes think about how something which was worse than FSR3.1, which is not great, made people so hyperbolic, like it was such a big deal  Edit: Blocked smh, my reply was going to be ""Quite a motte.""",Negative
AMD,"It is officially supported in every game if you use the global override, you can confirm it with the DLSS indicator overlay. Even then though if you draw the line at only counting manual per-game overrides for some arbitrary reason DLSS4 overrides are supported in \~175+ games (as of last month) while FSR4 overrides are in \~85 games (as of today) which is still significantly more  >FSR 1 is still the objectively best upscaler   lol, no.",Neutral
AMD,"""Proof""  At least search the linux_gaming sub or this sub and sort by latest to get the latest info before spewing bullshit.  The latency is down to around 2ms for rdna 3 with 4.0.0 at 1440p. Just slightly more than XESS and miles ahead wrt graphics.",Neutral
AMD,Today you learned that people's standards for tech change as it ages and improvements are made.,Neutral
AMD,Dlss override does not support every game.   The dlss 4 chart is incorrect as it counts any Game with any dlss 4 support.  Greyzone has no dlss 4 upscaling but has dlss 5 fg its on the dlss 4 list.,Neutral
AMD,">Dlss override does not support every game.  My guy I just tested this a few days ago with the DLSS indicator @ global Preset J override, it does.  >Greyzone has no dlss 4 upscaling but has dlss 5 fg its on the dlss 4 list.  Even the game you're literally using for an example supports DLSS upscaling overrides, you can even find tutorials for it from [months ago on YouTube](https://www.youtube.com/watch?v=_pxZyWJp68c&t=403s). Do you even have a Nvidia card capable of DLSS or are you just pulling stuff out your ass?",Neutral
AMD,That guy is using profile inspector not Nvidia app that will get you banned,Negative
AMD,Great hopefully that trickles down to the next M chips (or the rumoured macbook with A chips),Positive
AMD,"Is anyone at all interested in the GPU scores?   37% is quite a bit higher than the 20% from they keynote, but the huge improvements seem to be concentrated in blur performance. Could it be from the new tensor cores, and if so, why don't subtests like face or edge detection benefit similarly?",Neutral
AMD,"Sucks iOS and iPad Os are so locked down, all that power but you are limited on what apps you are able to install.",Negative
AMD,"This is somewhat silly. Critics of GB scores back when AMD didn't have AVX512 vs Intel were somewhat valid. the comparison of 2 uarchs where one doesn't have low precision matrix acceleration is even worse.  A Linux gb6 zen 5 score vs A19pro paints a different story https://browser.geekbench.com/v6/cpu/compare/13752715?baseline=%20%2013754236  Besides the `clang` benchmark, where apple really pulls ahead is stuff entirely dependent on 8/16 bit GEMM slop.  It's kind of embarrassing how far behind AMD and Intel are IPC wise (hence why GHz chasing needs to die), but lets be real here. The 9950X is going to pull ahead in a majority of ST workloads on the computer (albeit not by much)",Negative
AMD,Zen 6 should have similar single threaded performance to the A19 Pro. Very strong indeed.,Positive
AMD,Will be interesting to see the next gen Desktop CPUs from apple.  This is like the weakest new gen processor  that apple will make. (If we ignore  normal A19 :P),Neutral
AMD,Isn't that kinda like beating a Ferrari in offroading?,Neutral
AMD,I don’t care about all this performance on a phone but it looks like M5 series will be a monster. Especially with matmul addition to GPU will make them monsters for local ML. P,Negative
AMD,It's single core vs single thread comparison.,Neutral
AMD,It would be funny if Windows ARM runs faster on the A19 than a Surface laptop.,Positive
AMD,"> The GPU scores 45,657 points, which is comparable to the GPU performance of M2 or M3 in iPad Air. It is also comparable to the performance of AMD's Radeon 890M integrated GPU.  It's insane to think that the A19 Pro's GPU is comparable to the [AMD 890M which itself is comparable to a GTX 1070](https://www.tomshardware.com/pc-components/gpus/amd-latest-integrated-graphics-perform-like-an-eight-year-old-nvidia-midrange-gpu) depending on the game being tested.  Sure the 1070 is an 8 year old GPU, but I know a few people who still do 1080p gaming on it - crazy to think that a mobile SOC has this kind of power!",Positive
AMD,Interested to see if iPhone 17 Pro can play Genshin Impact at a steady 120 fps now that it has a vapor chamber.,Neutral
AMD,"This is only midly interesting. Both CPU are really not designed for the same workloads, though.  Zen 5 is not optimised for single-threaded workloads. You have to make use of SMT to be able to use the full width of the Zen 5 architecture.  Whereas Apple CPUs are fully designed for peak single threaded performance.  It's the age old ""are we comparing single thread or single core performance?"". If the latter, then you need to compare Zen 5 with SMT against the other core.  So, really, no surprises there.",Neutral
AMD,All that power to answer emails and browse tiktok lmao,Neutral
AMD,That rumoured mobile SoC Macbook is going to be *very* interesting. Let's hope they don't do something insane like limit it to 8GB of RAM.,Positive
AMD,"> Geekbench  So, worthless scores, lel",Neutral
AMD,"For how long, sustained? 500ms? 5s? 50s? 8m?",Neutral
AMD,OF COURSE IT’S GEEKBENCH,Neutral
AMD,Purely synthetic benchmarks.,Neutral
AMD,I remember getting laughed out of a forum when I said phone cpu would over take Intel pc cpu because Intel are lazy dog sh*t 10 years ago.,Negative
AMD,This is gonna boost my geekbench workflow.,Positive
AMD,Geekbench benchmarks are so valuable that it could literally tell you a Fiat Panda is faster than a TestaRossa.,Positive
AMD,"Now do AVX-512. Ohhhh, wait…",Neutral
AMD,"I mean this is just a continuation of trends. Apple silicon posts high single core scores regardless of what chip or platform is tested, because pretty much all of them can power a single core fully. It's multicore where things get interesting",Positive
AMD,Aka geekbench accelerator,Neutral
AMD,"Hello LandGrantChampions! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,"Geekbench, tuned to fit in 512kB of cache so anything seems fast.",Neutral
AMD,Apple silicon is just black magic at this point,Neutral
AMD,"Wake me up when the M series variant comes out. This is a mobile processor so it's almost guaranteed to throttle after a few minutes irl.   Case in point, phones have been faster than the PS4 for years, yet you wouldn't believe it when comparing like for like scenarios in-game.",Positive
AMD,"Is it really the cpu and the gpu that are the key differences? In what way will that substantially improve my user experience? I get that it’s faster and more efficient, but blaaah The phone is still too big, too heavy, and the battery times haven’t changed in a meaningful way for a decade. Yet the price is getting more out of control. Since Jobs has been gone it has been about peeling another skin off of the same thing with every release without any real innovation. Great product but nothing new and it is for a horrible value.  Having said all that I am sure I am going to buy whatever is the latest and smallest size once my iphone 12 mini throws in the towel.",Negative
AMD,A mobile processor more powerful that one of the best laptops processors is insane .,Positive
AMD,In Geekbench. I’ll let you figure out the rest.,Neutral
AMD,"Not really a surprise, even the predecessor was faster than the 9950X.   Solid uplift but not really anything impressive either.",Negative
AMD,Geekbench is liquid garbage,Neutral
AMD,A18 pro already did it one year ago.,Neutral
AMD,What could you possibly want with this kind of performance on a phone. Especially one with an excessively locked down operating system.,Negative
AMD,"All that performance is pointless because there aren't any apps that need it.  Even the cheapest phones can handily run everything out there.  I'm not saying that pushing performance is a bad thing.  I'm saying that it's not a reason anyone should buy a phone right now.  There's literally nothing that will use that performance, other than the battery draining faster.  I'm sure there's an edge case here and there that 1% of phone users actually use.",Negative
AMD,"x86 is an anchor around the neck of progress. When will dell,hp,motherboard manufacturers start selling ARM based machines?",Neutral
AMD,bullshit comparison   comparing arm architecture vs x86,Neutral
AMD,Geekbench is not made for x86 but ARM. There is no point in comparing different architectures with it. ARM flat lines in a complex instruction set environment.,Negative
AMD,6 cores vs ? 24 or something,Neutral
AMD,"""It will be significant.""  -Ling's Dad",Neutral
AMD,None of this matters if you cant game with it.,Negative
AMD,geekbench is userbembchmark for Apple,Neutral
AMD,"That is a poooooooor multi core score relative to the single.  Seems like a bug, or just maybe they have broken IPC really badly by widening the input instruction buffer (to account for better SpecEx) without improving rollback on misses? Something like that?",Negative
AMD,How's it do in a real work load? What about cinebench?,Neutral
AMD,Comparing a RISC processor locked to a closed OS architecture to a CISC processor with a more open OS architecture is apples to oranges...,Neutral
AMD,"The A19 Macbook will be faster than any Lunar Lake laptop in ST and MT, honestly, the WIndows side needs to step up",Positive
AMD,"For the A19 Pro, I think they [claimed 40% in sustained performance](https://www.apple.com/newsroom/2025/09/apple-unveils-iphone-17-pro-and-iphone-17-pro-max/)  Note GB6 GPU is GPU compute, whereas Apple's 40% sustained performance claim is probably for graphics not compute  I'm more interested in GPU AI performance, how do the GPU's new Neural Accelerators perform? Apple claimed a huge 4x uplift, but that's plausible",Neutral
AMD,The reason blur will be faster is more SCL cache.  Blur requires doing lots of data reads as you read adjacent pixels.    I would not expect things like face or edge detect to benefit until the devs re-work them to target lower (int8/Int4).,Neutral
AMD,They did mention that it's comparable to the integrated AMD 890M which is really impressive.,Positive
AMD,I kinda like this gen base iPhone. A shame the system is so locked down. Ability to install apps from other sources at this point is vital over here.,Negative
AMD,"The main limiting factor for devs is not the restrictions but the market, users are just not interested in paying real money for iOS and iPadOS apps.  Apps that use a lot of compute (cpu or GPU) tend to be rather self contained so they would have no issues with the locked down nature of the OS but users need to be willing to pay for the work as it is a LOT of work to re-work your entier UI/UX for touch.",Negative
AMD,It’s so dumb to have a modern computer in your pocket that you can’t just plop down to a dock and use with a keyboard mouse and monitor.  It’s incredible hardware with completely crippled software.   All these people who will carry around a processor better than a 9950x and just use it to FaceTime and send text messages lol. Such a waste,Negative
AMD,There's a good amount you can do with iPadOS these days. I was editing and rendering with final cut while traveling a few weeks ago and extremely impressed at how fast it was.,Positive
AMD,"Completely genuine question, what apps do you use that aren’t on an App Store? I used to be heavily into Android custom stuff but it’s been over 10 years and I haven’t run into any issues running stock iOS",Neutral
AMD,"While that’s true, at least we get to tap into that power with the M-series chips on Macs. No limits to what you can install on Macs.  Also, citizens of the EU can install whatever they want in iOS now!",Neutral
AMD,Outside of shady windows emulation what does Android let you do with the power of a snapdragon elite chip?   We've reached a point with arm based cpus that they can't be fully utilized on a mobile platform. It's literally up to someone other than Google and Apple to figure out how to get these things in desktop environments so that they can actually be used for compute heavy tasks because it's completely wasted on phones.,Negative
AMD,Reddit translation: I can’t game on this,Neutral
AMD,"Apple has been making some incredible hardware lately, but ill never get to use it because of how bad apple software/philosphy is.",Negative
AMD,Just buy a Mac.,Neutral
AMD,Microsoft single handedly created a trillion dollar malware business from the security nightmare of an OS. We don’t need another Microsoft.,Neutral
AMD,That’s the exact reason I’m buying iPhone. I want a phone that just works (which is easier done on a more strictly selected software pool) than a phone that can do anything but only if you want to fiddle with it.,Positive
AMD,Apple M4 Max is far faster than 9950X in Cinebench 2024 single thread and almost as fast in Cinebench R23. So I’m not surprised A19 pro is faster.,Positive
AMD,"It's shocking to me that its even close, given even a single core has triple/quadruple the wattage being pumped into it versus the A19.",Negative
AMD,9950x uses around 40w just to idle and even more when it ramps up even just a single core to nearly 6GHz. The fact that it loses to a PHONE should astonish everyone.,Negative
AMD,"I did not understand a single word you said. But because of that I trust you know what you are talking about, and i now immediately think this post is silly and will tell others that as well. Even w/o knowing anything myself. You formed my opinion. I will spread it as fact",Neutral
AMD,"With its claimed late 2026 launch, Zen6 will face the M6 / A**20** Pro.",Neutral
AMD,"By the time Zen 6 releases it's fighting very different products faster than the A19,  Oryon v4 and A20/M6  AMD needs +30% to be ahead of the competition in 1 generation and we all know that is not gonna happen",Neutral
AMD,but at 10x the power draw.,Neutral
AMD,"SMT is just a way to increase resource utilisation when you can't saturate your core resources with a single thread. It makes sense when you want to maximise multi-threaded performance, but let's not delude ourselves into believing it isn't also there to help the weaker front-end.   Apple can get away without SMT because they can keep the core well fed enough not to need it. This comes from better branch prediction, more bandwidth, bigger OoO circuitry, etc...  If Apple wanted to have SMT on their cores they could, but then the resources would get shared between different threads which could hurt single-threaded performance.  Different companies targeting different markets...",Neutral
AMD,"Imagine if we were allowed to install our own OS on it.  Turn these things into little awesome portable docking desktops with Internet everywhere.  That'd cannibalize Apple's other product lines, though, so it'll never happen.",Positive
AMD,"The X Elite and X Plus chips are pretty great though. They were designed to compete against the M2 but ended up beating the M3 in performance and nearly matching the M4. I'm honestly happy seeing how competitive CPUs are becoming again, feels like the glory days of Intel inhabiting other companies",Positive
AMD,890m is absolutely not on par with 1070 in games. Your link showed exactly 0 games.,Negative
AMD,I doubt it will match the 1070 in irl performance due to throttling,Negative
AMD,Great,Positive
AMD,I wish a VR headset would be upgraded to this. I hate mobile graphics for VR but we need all the power possible.,Neutral
AMD,">It's the age old ""are we comparing single thread or single core performance?"".   Not really. The question is simple: are we testing *one* instruction stream or two?  Many applications are bound by the performance of single-threaded, thus 1T is equivalent on AMD, Apple, Intel, Arm, etc. cores.  Any test using SMT is, by definition, a *multi-thread* benchmark. It means the application is feeding *two* instruction streams: the traditional single-core applications ***are*** single-thread. They do not scale with two instruction streams.",Neutral
AMD,"That is not true what so ever, SMT was made to make sure you take full use of the core while the other thread has unused parts of the CPU, that happens on Apple CPUs as well, 1T doesn't take the full width of the architecture that's why they focus so much on making sure branch prediction is the best in the world.  SMT is only used ""effectively"" in throughput oriented workloads, for example, SMT can reduce gaming performance, not increase it. ARM makes SMT enabled cores for automotive because there's a reason for it, due to a ton of threads it needs to handle the workloads in real time. Even for normal server use, SMT can be a pain, specially when they are considered ""cores"" and cost much more to license",Neutral
AMD,the benefits of SMT disappeared with 8 core CPUs. Anything that uses so many threads will most likely be able to feed the IO enough where SMT is irrelevant. Anything inefficient with feeding will have plenty of cores left for extra threads.,Neutral
AMD,What does that even mean? AMD is purposely designing their CPU so it's slower?,Negative
AMD,"You can play open-world games like Genshin Impact and Wuthering Waves on iPhones, and those require a lot of horsepower.",Neutral
AMD,"Well, emails are websites IS single core compute heavy. And short vids does utilize full hardware decoding to save energy. Unlike desktop, 40w minimal zipped every hour for simply idle.",Neutral
AMD,How is it going to be different than the M chips? Just fewer cores and less wattage?,Neutral
AMD,Apple has been generous with 17 series' launch. I believe the trend will continue.,Positive
AMD,Single core is pretty much sustained since the per core power consumption is well within the thermal power envelope of the thermal solution for an iPhone,Neutral
AMD,"These are mobile processors, they’re much more efficient than desktop level CPU’s and should do even better, assuming same environment, in sustained performance. Where the desktop CPU’s still do better in is in multi threaded jobs",Positive
AMD,The new Pro's aluminum chassis and heat pipe should let the phone maintain that speed for a long time. The old all-aluminum phones basically didn't throttle.  The last-gen pros basically had the worst possible thermal design--titanium and glass have really low thermal conductivity.,Negative
AMD,"Unlimited. It's 5-8W, for fuck's sake.",Negative
AMD,"No, not really. It's Geekbench not something like cinebench or spec.",Negative
AMD,"I was deeply skeptical that something weird was going on with the benchmarks a decade ago because when something seems to be too good to be true, it generally is.  Here we are years later and people are still defending x86, AMD, and Intel when it's obvious that all THREE of the major ARM core designers have surpassed them by a huge margin.",Negative
AMD,"easy to say if you didn't produce an estimated timeframe, like I could predict the stock market will crash everyday and eventually I'd be right",Negative
AMD,“Geekbench sucks because [made up scenario]” is always my favourite genre of Geekbench criticism.,Negative
AMD,"No need to wait: Geekbench 6 also activates SIMD on x86, too, in all the same tests:  **AVX** Generic floating-point 256-bit SIMD instruction set  **AVX2** Generic 256-bit SIMD instruction set  **AVX-512** Generic 512-bit SIMD instruction set  **AVX-VNNI** Accelerates quantized machine learning workloads  **AVX512-VNNI** Accelerates quantized machine learning workloads  **AMX** Accelerates quantized machine learning workloads  \^\^ **all** are used in GB6. The problem with GB6 isn't a bias for Arm CPUs.",Neutral
AMD,"Single core is interesting because it matters most for typical DIY client workloads. Unless you’re building a workstation PC with the need for massive heavy cores, a ton of your experience will be based on single core performance.",Positive
AMD,Won out in SPEC too against that and Lion Cove.,Neutral
AMD,"Geekbench accelerator my ass. Test it on any halfway decent benchmark and it will still pull ahead in 99% of cases.   If anything, all the extra cores/threads on desktop CPUs are just benchmark accelerators. 95% of consumers cannot take advantage of 32 CPU threads. Single-thread performance translates to real-world experience.",Negative
AMD,"Wow, people really hate those that dare call out geekbench huh? Shows the fixation people have on numbers in a synthetic benchmark than actual real world performance in real life use cases.",Negative
AMD,Faster and better computational photography? Recording high resolution RAW video (Apple even pointed this out for the iPhone 17 Pro)? Local LLMs?,Positive
AMD,"Augmented reality video generation, real time.",Neutral
AMD,"""sustained"" performance of 6-8W is virtually unlimited. Any modern smartphone can sustain 6-8W, as Geekerwan demonstrated. There may be minor throttling after dozens of runs, but 1T is not taking serious power.  Most x86 chips use [10-20W on 1T](https://youtu.be/ymoiWv9BF7Q?t=532); Apple is half or less.",Neutral
AMD,"There are already a lot of Qualcomm laptops by multiple manufacturers with their Snapdragon X ARM chips inside. Don't expect to find any PCs you can build yourself, but there are insane amounts of these ARM laptops in the current market.",Neutral
AMD,"Raspberry Pi. lol. That’s a fairly stock ARM design.  Making high performance CPUs is an intensive process. ARM sells a template to build upon, but they’re slow & good enough for embedded use. It takes a giant company like Qualcomm or Apple to redesign the internals.  ARM CPUs don’t all perform like Apple’s M1, just as AMD CPUs don’t all perform like Intel’s.  The PC world would need a company willing to invest billions into custom CPU design only to likely be consistently slower than Intel for the first decade or two.",Neutral
AMD,Why? It's the same workload. No reason to exclude Arm designs when they're superior.,Neutral
AMD,It's not bullshit when the workload output is the same.,Neutral
AMD,Meaningless as most apps now days are multi-threaded. Also Apple products benefit from being highly vertically integrated. Its OS only need to work on Apple Silicon and nothing else. And was this test done on Linux or Windows for the 9950x?,Neutral
AMD,Go look at Cinebench or any other benchmark for M4 versus any x86 and it paints a similar picture.,Neutral
AMD,"why? If it computes the same result, what's the difference?",Neutral
AMD,or in this case. apple to amd.,Neutral
AMD,"There's really no such thing as RISC and CISC anymore, at least not for high end CPUs.",Negative
AMD,"ARM now has the momemtum of economy of scale, which x86 can't catch up.   There are now 5/6 orgs doing high performance ARM out-of-order cores vs 2 for x86.   So it's very unlikely x86 will ever lead in that thermal/power envelope, maybe catch up the best they can.   We're in the SoC era, and there is where the funding/revenue are, and thus where talent and performance are moving.",Neutral
AMD,"More like Microsoft and it's Windows on ARM, plus developers needs to step up. Since all this thing Copilot+PC and ARM Windows, I've only seen Surface tablets and 2in1 devices from Microsoft. Other than that some lame looking laptops and nothing more. So nobody is interested to make devices with this seems like it. As for Windows on ARM Microsoft is lazy as always just like with Windows RT, Windows S, Windows Phone or Windows Mobile.",Negative
AMD,Just wait for AMD's Medusa 😁,Neutral
AMD,Bring on the mini macbook,Neutral
AMD,where did they mention this? :v,Neutral
AMD,"Unfortunately Google is removing this and Iphones are becoming more appealing to android users, watch yesterday's LTT video about it.   https://youtu.be/qeQCVcBWqis",Negative
AMD,Or at least allow PWA,Neutral
AMD,> better than a 9950x  In one biased singlethreaded benchmark. Geekbench is in no way representative of real-world performance and is only useful in comparing chips like to like e.g. previous apple chip generations against the new one.,Negative
AMD,Thats about the only actual computational workload i can do on an ipad until blender releases,Neutral
AMD,"No doubt, but there are apps that are on the play store, but blocked on the App Store for various reasons.  Not enough to switch to android, but it is annoying being limited.",Negative
AMD,"Which iPad do you have (so I can get an idea of the processor)?  I had the iPad Pro 11"" M1 and I'm still impressed with how powerful it is.  Oddly enough, I checked GeekBench's scores earlier today and found that the chip in my iPhone 13 Pro was more powerful than the M1.",Positive
AMD,Firefox with an adblocker would be pretty nice.,Positive
AMD,"Easy, Nzb360, not available on the App Store.  And I’m sure there are a lot of apps I can’t use, I wouldn’t know cause I’m on iPhone lol.",Neutral
AMD,"> what apps do you use that aren’t on an App Store?  Everything on Steam if this ends up being put into a MacBook as the rumors state.  Browsers that don't use the Safari/WebKit engine.  DaVinci. Handbrake. Premiere. Audition.   Performance for a CPU can be the best in the world... but if I can't run the software I want on it, then it's useless to me.",Neutral
AMD,The answer is always piracy,Neutral
AMD,> citizens of the EU can install whatever they want in iOS now!   ... not really.,Neutral
AMD,">citizens of the EU can install whatever they want in iOS now  Doesn't apple actively try to fight that, still requiring some steps for developers to use the side stores?",Neutral
AMD,"Unless you're gaming on a phone, they're essentially just very powerful race-to-sleep chips.",Negative
AMD,"Idk what you are on. I don't game on my phone but one of the things that stops me from switching back to iOS is stuff like being able to install uBlock origin in Firefox. Mobile web is already bad, but without adblock it's borderline unusable, so many popups and obtrusive elements. And all the adblocks I have tried on iOS had very limited functionality and were obtrusive while uBlock works just like on desktop.",Negative
AMD,"Credit where credit is due, there’s also a guy complaining he can’t pirate shit off Usenet with an iPhone elsewhere in these comments lol",Negative
AMD,What year do you think it is? Android just works too. It's not 2011 anymore grandpa.,Neutral
AMD,"I think it's just all in the branch prediction.  It seems to be far easier to break a smaller instruction set into extremely predictable micro-op patterns, where x86 plus thirteen extensions only allows for so much SpecEx.  I still strongly believe that much of Apple's early Arm lead came from their pioneering of the prediction of vector operations. That's my total shot in dark theory.",Neutral
AMD,What I/O does a phone provide compared to a PC?  They aren't the same use cases and should not be compared.,Neutral
AMD,"> uses around 40w just to idle   ... while the 16 cores are only drawing 1-5 watts, whereas the IO die and the infinity fabric run at full power all the time.  The desktop Ryzens are designed for minimized RAM latency to, among other things, pump more frames in video games. Idle power draw is not a relevant metric.",Neutral
AMD,Terrible comparison. Look at LNL if you're going to make a comparison like that.  Apple power consumption is going up too though. They need to bump IPC (something they haven't done since introducing M1 half a decade ago) otherwise they will become powerhogs too.,Negative
AMD,My guess is M7/A21 will be even faster too,Positive
AMD,"And Ascalon, Ventana and a bunch of other RVA23 designs.",Neutral
AMD,You're missing the point.  The A19's performance is wasted ***if I can't run the software I want to run on it.*** It's iOS only according to Apple.  The Zen CPU has no such restriction imposed on it. Any OS compiled for it can be installed on it.,Negative
AMD,"I tend to agree. By the time Zen 6 becomes available Apple will have increased their IPC further to stay ahead. IPC wise Apple will have a 12 to 18 months lead over X86 parts, and I'm ok with that. Zen 5X3D is still very good and I'll be able to upgrade to Zen 6 with A19/M5 level of single thread perf seamlessly come next year. Don't have that hardware upgrade path on iOS, you have to buy brand new devices.",Positive
AMD,"Apple hardware does not compete with AMD. While true that they both make processors, and yet they are not competitors. Apple will not steal sales away from AMD. The kind of systems and use cases that AMD chips facilitate are impossible with Apple hardware.",Neutral
AMD,"Unless Apple releases their chips for non-Apple hardware, Zen 6 isn't ""fighting"" them at all.",Neutral
AMD,"except amd is going all in next gen, paying for N2X node, it will be \~2 nodes jump skipping N3 / N3P  and N2. as well as moving to an interposer scheme which will vastly lower IO power cost,",Neutral
AMD,Zen6 was taped out back in April.,Neutral
AMD,if you keep the front end fully fed SMT overhead is actually destrimental to performance. but that is such a niche use case it really doesnt matter.,Negative
AMD,"Yeah that's just stupid. All those SoCs are the same, even when they beat a bigger GPU on paper in synthetic benchmark those wins never materialise in real world use because they throttle hard if you use them for hours like you would for gaming.  At this point GeekBench scores are a joke and not something to be taken seriously, the real use experience is too disconnected from what they pretend.  This phone chip is not very relevant because it will throttle hard under real load and make the phone uncomfortable for prolonged use case.",Negative
AMD,"> That is not true what so ever, SMT was made to make sure you take full use of the core while the other thread has unused parts of the CPU, that happens on Apple CPUs as well, 1T doesn't take the full width of the architecture that's why they focus so much on making sure branch prediction is the best in the world.  No you are confusing things. Apple core can only run one thread at a time. It doesn't have SMT. Apple core uses extreme ILP (instruction level parallelism) to take advantage of the width of the core but you can't have multiple logical threads use the core without context switching.  > 1T doesn't take the full width of the architecture that's why they focus so much on making sure branch prediction is the best in the world.  This is wrong as well. An advanced branch predictor is far more important to the performance of the long pipeline cores such as Zen or Intel P cores. Because a branch missprediction is much more costly when you have a long pipeline. AMD's and Intel's branch predictors are quite sophisticated for this reason. AMD is constantly redoing their branch predictor, and Zen5 got a whole knew from ground up design in this most recent iteration.",Neutral
AMD,"No, AMD designed their CPU for heavy multi-threaded workloads.  Zen 5 is a pretty big and wide core by x86 standards, and AMD knows that it's difficult to find instruction level parallelism in x86 (more than it is for ARM at least). So they optimised the core's design for maximum performance with SMT. As a consequence they opted for 2x 4-wide decode instead of a more complex 8-wide that can also do 2x 4 (for SMT).   This is also a consequence of AMD targeting server workloads, which Apple doesn't.  So when you compare Apple's ARM core to a Zen 5 core using single-threaded benchmarks, you are not really comparing the performance of each core.",Neutral
AMD,"Purposely designing their CPU's to be better at different tasks. Theres some overlap of course, but the demands placed on either platform are generally not quite the same so designing both chips to be the best at the same thing doesnt really make sense.",Neutral
AMD,It means that the A19 is made for one thread per core workload. good for phone trash for actual Desktop computing. This score means nothing for a desktop CPU that is made for SMT (Simultaneous Multi-Threading). ELI5: AMD is making cores for crazy loads.,Negative
AMD,Even those games are getting old now compared to the newest hardware. I have an 8belite chip in my phone and it demolishes genshin with no trouble at all.,Negative
AMD,"I play those games on a Pixel 7, a phone with objectively far worse performance, I mean their GPU is the worst of all flagship phones and it's now 3 generations behind, and it's... fine? It could at times be more smooth, but tbh that's not a problem and only really noticeable when I switch between phone and PC. I'm sure you don't need to go from the very limited GPU of the P7 to A19 to smooth out the occasional hitches I see.  So still not really needed, least for those.",Negative
AMD,I have a PC for gaming..,Neutral
AMD,"Yep, so cheaper to make, and either better battery life or a very thin and light laptop, but with better single-thread than an M4 and the same multi-thread as an M1.",Positive
AMD,"Not at all. Mobile processors throttle hard, just look at thermal rundown charts for almost any activity on a phone. Mobile processors, especially single core scenarios, are only optimal for bursty and short workloads. This is why your device gets hot on FaceTime, maps, or web browsing while outside.    Truth is, looking at a geekbench number will tell you close to nothing about real world performance unless your only use case is geekbench lol",Negative
AMD,That’s a lot in a phone.,Neutral
AMD,"This is an 1T test. There is virtually **no** throttling of 1T on *any* modern smartphone.  1T is 5-8W, easily dissipated in modern chassis, as Geekerwan tested.",Neutral
AMD,"What? Cinebench is far less synthetic, it's a real workload. Geekbench is like spec, specifically a synthetic benchmark.",Neutral
AMD,Geekbench is actual trash.,Negative
AMD,We just need ARM core designs on desktop. X86 has long been overpriced and undelivered. Even some of the engineers I used to work with dont know some parts of the architecture. It's a shame that people ARM is really better than X86. If only the same amount of resources was put into ARM or RISCV and also not into the 2 incompetent companies driving X86.,Negative
AMD,https://www.reddit.com/r/hardware/s/Ef1Tyk92ee,Neutral
AMD,"Source for those curious: thought I linked it earlier.  [Geekbench 6 Internals](https://www.geekbench.com/doc/geekbench6-benchmark-internals.pdf) CTRL+F ""AVX"" or ""AMX"".",Neutral
AMD,"Since the M1 the benchmarks of actual workloads proved the exact opposite.  Apple chips are very good at ST synthetics, but when it comes to actual productivity workloads in real software, they mostly fall behind.  They simply aren't optimized for actual workloads software top performance.  **What they are optimized for is incredible power efficiency.**",Negative
AMD,You would think if you where really into photography and wanted to shoot RAW video and stills you would just get a dedicated camera at that point?,Neutral
AMD,> Meaningless as most apps now days are multi-threaded.  most? No.,Neutral
AMD,"> Meaningless as most apps now days are multi-threaded  They absolutely are not. Background operations like I/O and network, sure. But building the next frame - what really matters for perceived smoothness - is single-threaded. You have 8ms to compute the next frame on a 120hz device.   Source: am a mobile developer",Negative
AMD,"Most apps most people use are all either websites or electron apps (which are just a web browser with the serial numbers filed off). All that stuff runs on a single thread.  Even most ""multithreaded"" games only use one core at 100% with a couple mostly-used threads. After that, it's usually a long tail of cores using almost none of their potential compute power.",Neutral
AMD,"So does Snapdragon Elite X but when you actually look at real workloads on Phoronix, it's like 2 generations behind x86, like it can't even beat Hawk Point.  Every time you have a single synthetic benchmark everyone uses, it is so easy to game. PCs actually gets put to real tests to determine real world performance. Not a single synthetic benchmark which runs too quickly to even stress the CPU and which doesn't even use all the cores/threads you might have.  We don't just use 3D Mark to test GPU performance either, because we've seen wide disparity between it and real games.",Neutral
AMD,Hopefully software starts targeting ARM or we get a flawless translation layer in arm based windows image eventually,Positive
AMD,Also Micro architecture wise this is the era of the FireStorm core. The staple of CPU design. QC bought Nuvia for it,Neutral
AMD,This,Neutral
AMD,"Just wait for a 2027 product? when the A20 has released and we are on the X Elite gen 3/ 8 Elite gen 3?  By then Medusa point might will still e worse than QC and Apple unless AMD has +25% improvements.     To battle the M5 and X Elite gen 2, AMD has Gorgon Point which is Zen 5 and Zen 4 will continue to be the mainstream",Neutral
AMD,"> At the presentation of its A19 Pro, Apple did not reveal anything about improvements to its GPU, but said that it still has six clusters. Based on Geekbench 6 results published so far, the A19 Pro GPU is a whopping 37% faster than its predecessor. The GPU scores 45,657 points, which is comparable to the GPU performance of M2 or M3 in iPad Air. It is also comparable to the performance of AMD's Radeon 890M integrated GPU.",Positive
AMD,"News is currently, that ADB installs are not going to be restricted, so tinkerers will be fine.",Neutral
AMD,"This also worries me. But with Android phones, there's at least a way to open the bootloader on some on them and maybe flash some degoogled ROM.  If this isn't an option either. Well, I don't know. Might as well get an iPhone and what happens happens.",Negative
AMD,> Unfortunately Google is removing this   Misrepresentation. They're not removing sideloading: They're requiring sideloaded apps to be signed in order to keep play integrity.  ADB to sideload is unaffected.,Negative
AMD,"Google is not removing ""sideloading"" (dogshit term IMO forced by duopoly) in general, they just force apps to be verified. I don't think there will be any restrictions how many apps users can install outside PlayStore + ADB/Shizuku will still work unrestricted because it's essential for debugging apps via Android Studio.  This won't be present on AOSP/GrapheneOS ROMs and non-certified devices.  iOS still has broken JIT outside of Safari, no proper 3rd party browsers and lacks FOSS apps.   As a iPhone owner, ""sideloading"" is less than a problem for me compared to the above restrictions.",Negative
AMD,"Well yeah. Apple is starting to understand why Android users buy phones and Google just wants to copy Apple from 10 years ago who was shitting out phones for 15 year old girls.  If I buy a phone I want the hardware, battery, and performance to be good while also having decent sideloading capabilities at a good price.  And as much as I don't like the rest of iOS(I do like MacOS though), well, the iPhone 17 ticks all of those boxes. Even though I'm American I can still enjoy basic sideloading and now that the app store has emulators I wouldn't have to sideload as much.  I would have to rebuy some games during the switch but... Oh well. Of course I'm not gonna go out and buy a 17 at launch but I might get one used a year or two down the line when all the kinks have been ironed out of the software, and my S21 is still working just fine.",Positive
AMD,"They are not removing the feature, only restricting it to signed apps, which any person can do.",Neutral
AMD,Google is removing non appstore apps?  The EU won't like that.,Negative
AMD,"Android isn't removing side loading yet, even with the notarization. Apple doesn't even allow real side loading to begin with.",Negative
AMD,Please go out of your way and verify you see anything any of the LMG groups say.,Neutral
AMD,Winlator.,Neutral
AMD,My point still stands that iPhones are incredibly powerful computers. Just in terms of compute power they are totally sufficient for 90% of people’s normal office/streaming/social media use cases,Positive
AMD,"I also use it for Lightroom. It’s not as intensive as Final Cut, but I take 60MP images so it can seriously chug on weaker machines.",Neutral
AMD,I have the M4 11’’.,Neutral
AMD,"This too, the ability to use the browser I want and have Ublock origin.",Neutral
AMD,"would not result in more CPU or GPU usage, if anything adblocker would reduce the need for power CPU/GPU.",Neutral
AMD,Try Orion. It supports uBlock origin among other addons.,Neutral
AMD,Just use Edge. It ships with adblock plus.,Neutral
AMD,No way. Ive been looking for something simple that just lets me manage torrents and Plex and stuff when I'm away from home. Was literally gonna try to build some solution myself but this looks perfect. Thanks for posting this!,Positive
AMD,"This might come as a shock to you, but the chip inside a device doesn't solely dictate what OS it runs. The rumored A18 Pro **Mac**Book will still run **mac**OS, not i(Pad)OS.",Neutral
AMD,The answer is privacy. Its none of apple/google business what software i run. I use a lot of plugins/mods/costum modifiers. almost none of them are ever signed. Half of them is abandonware.,Negative
AMD,"Just did a quick search, seems you’re right. Maybe it’ll eventually open up more.",Positive
AMD,"Windows does too… so many GitHub projects I can’t run without explicitly removing them from Microsoft security.   My Mac, I enabled that option once, and can install anything easily now. Annoying on any platform, but it’s to protect the normies",Negative
AMD,"I mean, why not though? Millions of people have incredibly powerful hardware and all they can use it for is shitposting",Negative
AMD,"Had an android three years ago and the experience really wasn’t good. It was slow despite good hardware on paper, and the app UIs were sometimes lackluster at best",Negative
AMD,Even Apple desktops with lots of IO have much lower idle power consumption.  Much of this comes down to AMD cheaping out on the IO die and a lot of the rest from the chip interconnects with some additional amount being from AMD's chips being less efficient.,Negative
AMD,Your assumption here seems to be that Apple chips have terrible latency. All the tests that I've seen seem to indicate that this is not true.  It is especially untrue of the most critical cache (L1) where Apple has 2.7x more data cache (128k vs 48k) and 6x more instruction cache (192k vs 32k).  Chips and Cheese did a just-for-fun analysis on how to improve Golden Cove's caches and the clear winner (not even close) was theoretically using the M1 cache setup.  https://chipsandcheese.com/p/going-armchair-quarterback-on-golden-coves-caches,Negative
AMD,Lunar Lake still didn't compete in efficiency and was downclocked to the point of having terrible performance.  I've been quite critical of Apple's clock ramping for performance wins. You can only chase those gains for a little while and then you are stuck with worse perf/watt. I wonder if too many key people got poached by other chip companies.,Negative
AMD,Big if true,Neutral
AMD,I would even venture to say that the M8/A22 could possibly be even faster than those.,Positive
AMD,"I'm confused: did I say anything about apps here? We're discussing launch dates of Zen6.   Nobody disagrees Apple is very restrictive on iOS, less so on macOS.",Neutral
AMD,"Laptops, desktops, handhelds?  The only markets Apple doesn’t serve M chips to are DIY and server farms. The rest? I’ve seen so many clients buying Mac minis to replace aging PCs. Those would have been AMD machines, but they’re Apple instead.",Negative
AMD,on the laptop side of things apple very much does compete with AMD.,Neutral
AMD,Apple isn't the only company making ARM desktop chips.,Neutral
AMD,Oryon v3 releasing in 10 days will kick AMD/Intel in the gut. C1 Ultra at 4.2Ghz can also compete quite nicely vs Zen 6 if it follows the typical AMD improvement cadence,Neutral
AMD,Zen 6 is not coming till late-ish 2026,Neutral
AMD,What are these special tasks that AMD is designing for exactly?,Neutral
AMD,"The A19 uArch will be replicated across the M5 desktop SKUs, too. The uArch is virtually ***identical*** between the phones & the larger form factors. Just add [0.2 GHz - 0.4 GHz](https://www.reddit.com/r/hardware/comments/1ndm9ph/comment/ndngxa6/?context=3) to the clock speed: there is your desktop 1T perf.  That's kind of the point.",Neutral
AMD,"Most client workloads benefit more from high 1T performance than nT, especially once you get up to a dozen cores. The weak performance from AMD is due to inferior design, a server-first focus and the x86 architecture.",Neutral
AMD,Pixels cap at like 30-45fps on these games while iphones can do 60 locked or even 120 for short bursts. The existence of the 1060 doesn't mean the 5080 isn't really needed for gaming imo. Some people just want a different experience and for the same cost (since phones are all about the same price) you should expect it imo.,Neutral
AMD,"Good for you. Some people might decide that the phone they paid $1200 for is enough to cover their gaming needs, and that phone might oblige them this time around.",Positive
AMD,"I see. What would be the usecase for something like this? Essentially a lower end option than even the current macbook air, for people with less intense workflows, but who need a desktop OS vs something like an ipad?",Neutral
AMD,"> assuming same environment  Mobile processors throttle hard because they're tightly enclosed in a small phone and rely on passive cooling. That has nothing to do with their actual efficiency, which Apple designed mobile SoC's are much more so than pretty much every non-Apple desktop level processor.",Neutral
AMD,"[No, it *definitely* is not](https://youtu.be/FwCIsBSUSNw?t=882). 100% load at 5W for 200 seconds makes a modern smartphone … still ***cooler*** than your fingers at room temp (<37C).  1T loads can be sustained at virtually 100% at room temp. Your battery will die before it throttles on 1T load.  Geekerwan's testing is thorough and I encourage everyone to review his tests.",Neutral
AMD,"Geekbench is a collection of many real workloads. Cinebench is also a real world workload, but as 99.9% of people don't use cinema4D every day, it's far less relevant than so called ""synthetic"" benchmarks like gb and spec.   Either way, cinema4D has been improved and no longer has a terrible arm port and the results correlate well with geekbench, so cinebench has lost its luster with the amd/pcmr crowd.",Neutral
AMD,"Geekbench is a far better metric of end user performance. It measures burst loads, which is 99% of what a phone does.",Positive
AMD,"No, it's not. Who told you that?    Geekbench is far from perfect but it's a collection of multiple real world applications. Cinebench has no real world use.",Negative
AMD,Why? Because you don't like the results?,Negative
AMD,"People could and should stop rewarding AMD and Intel for overpriced and underperforming CPUs. That should force both companies to compete more for market share instead of it being handed to them on a platter.     But there's also a software component that can't be ignored. Many companies have been so slow at offering ARM and RISC-V builds of their software that people have been forced to keep buying x86 systems for running their applications.   My way around this has been to get the bare minimum x86 systems to keep me going for the next few years while hoping and waiting for breakthroughs to be made on the ARM and RISC-V sides.   My main lightweight machine has a RISC-V SoC, but it obviously can't handle all tasks yet. I expect that to happen over the next few years.   In the meantime better ARM SoCs are going to be released. x86 has firmly arrived in legacy/media/gaming territory for me now. I only power those machines on as needed and increasingly rarely.",Negative
AMD,Do you have anything to prove these assertions?,Neutral
AMD,"part of the problem is the limited power scaling and dogshit cooling solutions typical of apple hardware. its clear apple built these for short burst workloads typical of most mac users that use their machines as glorified internet browsers. x86 will never be as power efficient on idle or burst workloads, but x86 will always work better under sustained long duration load.",Negative
AMD,"Why not both? I don’t always like to bring my big dedicated camera (I‘m only an amateur though). If I don’t need a shallow field of depth and the lighting conditions are good, an iphone camera can take great pictures. RAW gives you the possibility to do post-processing too (e.g colour grading to get the same style as with another camera). So yeah, I think professionals would use iphones for serious work too. At least in some cases.",Positive
AMD,"It is not 7W on **SPECint2017**; that's the critical point. Geekerwan's tests are rather extreme loads; see my link. From that Geekerwan chart, AMD has not kept 1T power consumption in check:  185H = 20W, 6.26 pts  HX370 = **18W**, 6.69 pts  258V = 9W, 7.22 pts  M4 = 7W, 10.67 pts  //  X4 is an Arm Cortex core, not Apple. Zen3 is very competitive in perf, but not perf / W, even in laptops. Per Notebookcheck [using Cinebench R23](https://www.notebookcheck.net/Apple-M2-SoC-Analysis-Worse-CPU-efficiency-compared-to-the-M1.637834.0.html) (a floating-point benchmark, which is not representative of most workloads, but we'll use it anyways):  AMD 5600U, Zen3 = 1488 1T points, **16.4W**  Apple M2 = 1585 1T points, **5.3W**  Even as this is 5nm vs 7nm, TSMC doesn't ever claim you will get 70% power reduction moving from 7nm to 5nm. It's not even close. If you want [a virtually iso-node comparison](https://www.notebookcheck.net/Apple-M3-SoC-analyzed-Increased-performance-and-improved-efficiency.766789.0.html):  AMD 7840U, TSMC N4 = 1723 1T points, **17.1W**  Apple M2, TSMC N4P = 1585 1T points, **5.3W**  As I wrote, under full CPU benchmarks, x86 CPUs consume around 10-20W. On lighter workloads, less. But so do Apple SoCs: they consume *much* less on lighter workloads, too, just like x86.",Neutral
AMD,But “sustained performance for 10 minutes” isnt an issue,Neutral
AMD,I don't see your point. Most modern game are multi-threaded. You can submit command list in DX12 on multiple threads. From a pure performance standpoint single threaded performance is only part of the story. And once you do a benchmark across OS and architecture there are a lot more variables in play. If A18 A19 was so good Apple would have stick them in Macs. Geekbench's workload are short and bursty which don't represent real work complex task very well. You can't really say the 9950x performs worst when you don't use it to its full potential.   As a mobile chip A19 is fantastic. Why do you guys always need to compare it to a desktop chip which was design for a different purpose lol. Are you going to be playing AAA titles or running CFD simulations on your phone? No because it simply can't.   Source: AAA game developer,Neutral
AMD,For games at least which needs things done ASAP ish to maintain a level of smoothness it is less about using the full compute power of a core and more about having just enough cores so we don't have to wait for all the higher priority threads or context switches.,Neutral
AMD,"If we do this, expect transition in 10-15 years maybe. People dont change what works on infrastructure that runs 15 year old code.",Neutral
AMD,It's hard to tell rn if Qualcomm's Oryon cores this generation have any sort of leg up on the stock ARM P-cores tbh. Perf and Power are pretty much a draw. Maybe Qualcomm is able to get a lead with their upcoming core though.   Really it appears to just be Apple's cores that have so far been so good.,Neutral
AMD,"The GPU inside the Medusa APU is on-par with a RTX 5070 TI, I believe they mean on par with the desktop version. This is a huge step imho.",Neutral
AMD,"To be honest that’s still scummy and annoying. Now instead of just being able to install an apk with a package manager, you have to use another device with adb.   To make a metaphor: being able to install “at least” with ADB is like finding a shit on your food plate and then claiming it’s fine because there is a nice appetizer next to it.",Negative
AMD,"Still, that's sloppy hill Google rolling, i will vote with my wallet and stop giving them my money",Negative
AMD,Popular brands are getting increasingly harder to root. This fucking suck.,Negative
AMD,You can also make an apple EU account. There’s then alternative app stores and side loads.,Neutral
AMD,"Can also run a Linux terminal on Android which is kinda fun, although I have yet to find a reasons why that's useful at all to me.  Really considering switching lol",Positive
AMD,That’s going away as well,Neutral
AMD,"Can't you just apply for a developer account? That gives you all the functionality back, no?",Neutral
AMD,"Pixels no longer get device trees, binaries, or kernel commit logs so making roms will be more annoying, and several months worth of AOSP updates never came out.",Negative
AMD,"Google demanding the verification and setting up the rules that will decide if that verification is valid is effectively Google removing sideloading. There is nothing that would stop them just not verifying apps they don't like citing some vague paragraph in some updated ToS.  Okay, it might not be present on AOSP, but it is the beauty of current android to have the best of both worlds - Google services and free sideloading",Neutral
AMD,"In practice anything rooting/adblocking/piracy related is unlikely to get certified, and Google can now leverage their uncertified status to punish users by way of Play Integrity.",Negative
AMD,"> Google is not removing ""sideloading"" (dogshit term IMO forced by duopoly) in general, they just force apps to be verified.  They're effectively removing side loading. They will not sign (""verify"") apps they do not like, either because they steps on their greed, their control, their surveillance, or their politics.  They will tell you it's all for your safety, despite the Google Play Store being the largest single source of malware on the internet.",Negative
AMD,Sounds like they' want to get rid of apps like revanced so people can stop using youtube without ads or keep using things like RIF for Reddit.,Neutral
AMD,">Google is not removing ""sideloading"" (dogshit term IMO forced by duopoly) in general, they just force apps to be verified.  thats the same thing. All the apps i install from third party sources are there because the official store refuses to verify them.",Negative
AMD,I have given up roughly two years ago playing cat and mouse with a particular unlock/root detection vendor that all of my local banks and government apps use. I got a second locked down basic android phone for that. An older iPhone would work too.  I have a completely unlocked phone from a Chinese phone maker Poco running custom firmware for my actual daily mobile computing.  I never charge my phones above 80% and my four+ year old battery is still almost as good as new. The only drawback is ass photos and videos as these ROMs will never get the proprietary sauce for the device's camera. I use my mirrorless for that anyway.,Neutral
AMD,They did not. They just need you to sign using cert  issued by them so scammers can stop taking over people phone.,Negative
AMD,"It's because of the EU that they're allowed to do it.  When the EU forced Apple to implement a third party standard, it created precedent for a worse system (third party apps requiring Apple's approval) than Google's own existing system that the free market had created.  Google was obviously going to take the opportunity to clamp down more control on their platform if a more user restrictive system was enshrined in law.  Redditors circlejerk all day about the EU, but there's downsides to everything. Government should be implementing building codes, baseline Healthcare, and infrastructure.  Luxury electronics being regulated to this degree created this.",Negative
AMD,"Do you, use it for games? Or something useful?",Neutral
AMD,Safari have it with uBOL. Albeit its the lite one,Neutral
AMD,Hope you aren’t on iOS lol,Neutral
AMD,"Who MANUFACTURES the chip, in this case, does dictate what OS it runs because Apple locks the bootloader of every iDevice they sell.  Go ahead and show me how to put MacOS, Windows, Linux or another flavor of BSD on any iPhone or iPad if you believe otherwise.  You're needlessly splitting hairs in this case. Apple dictates what you run on their SOC and there's no alternative.",Neutral
AMD,"> Are you running that software on Android phone right now?  Some of it, yes. I am able to run Windows and Linux programs on my Android devices and it's quite easy to do so.  You run Firefox, but it is still Webkit based. Not Gecko based. Apple requires ALL browsers on iOS to use their built in engine. No third party engines allowed.  No other OS in the world has this restriction.   I currently own two Mac Minis and Macbook Pros. I hate the restrictions that Apple imposes on their users.",Neutral
AMD,"Expiring 7 day certificates and 3 apps maximum at one time is not ""sideloading"" in any true sense.",Neutral
AMD,in windows you have the option to remove them from security quarantine. in iOS you dont.,Neutral
AMD,the i/o die consuming a bit more power on idle for desktop and server applications is literally irrelevant.,Neutral
AMD,"> Your assumption here seems to be that Apple chips have terrible latency.  No idea where you got that impression from. I never implied anything about the latency of Apple chips.  AMD and Intel do everything to minimize memory latency on the desktop, even at a price of increased idle power draw and temps. This doesn't hinge on whether the chip itself has high or low inherent latency. In fact the chiplet design of the Ryzen is known for poor latency.  I was just saying that the 9950X draws 40W idle by design and this number doesn't reflect its efficiency. You could turn off SoC OC mode and get 10-15W idle. But even then most of the power draw is due to the SoC and the Infinity Fabric. AMD's chiplet design is inherently bad at minimizing idle power draw, x86 or not. The monolithic R9 395 draws 5W idle with the same 16 Zen 5 cores.",Negative
AMD,"Yes but to get a major advance you need competle new thinking, with a major reachitecturing of everything. Even if the ""good people"" stayed, it's doubtful they would started over. Apple Silicon is just capitalising on investment made a very long time ago in PA Semi. It's basically a scale up of the phone chip will the compromises that require. They added accelerators to target specific use case of their niche market and that's basically it. All the speed bump are related to node advantage and frequency boost there is nothing new under the sun and I doubt it will change anytime soon.  In fact they are stuck on the high-end of things, they can't figure out how to scale their Ultra chips (that is already gluing 2 chips and doesn't scale very well) because they already are at the limits of manufacturing and physics.  Which is why all the marketing is about efficiency. It's cool and all, and is actually what's useful for most average users but it's not very impressive at the top the the curve.",Neutral
AMD,Again. You're missing the point.  The A19 Pro beating the R9 9950X in single threaded tests is a rather pointless comparison to make when ***YOU CANNOT RUN THE SAME SOFTWARE ON BOTH.***,Negative
AMD,"What I meant is that Apple does not sell chips for handhelds, desktops or laptops. AMD does not sell handhelds, desktops or laptops. They are not direct competitors.   But even going a level deeper, the number of people who will make a major change to give up on Windows or Linux for a Mac (the only way for AMD to actually even indirectly lose chip sales to Apple) is very small, and many prohibitive barriers are in place before the CPU is a consideration. It very rarely happens, as evident by Apple's stable market share.  Apple has steadily held single digit market share of systems for decades regardless of chips they used, and now that chunk of the market is exclusively served by Apple chips, with no way for AMD to compete regardless of how they perform. Likewise, Apple does not attempt to compete by selling chips for ~90% of desktops, handhelds or laptops that don't run MacOS, so they don't notably threaten AMD's business regardless if they're faster or slower.   In other words, for 9 out of 10 desktop/laptop/handheld buyers, the performance of Apple desktop/laptop/potential handheld chips does not matter as that performance and those chips cannot be in any way utilized in the software environments that people actually use - there is no way you can buy an Apple chip to accelerate those workloads as is.",Neutral
AMD,"They compete against each other the same way Coca-Cola competes against KFC. They affect each other indirectly, to a limited extent.  You cannot get an AMD chip on a Mac, and the vast majority of people would make their decisions about getting an Apple laptop or not way before they compare CPU performance.  I know we are in a hardware forum obsessed with performance of individual components, but even a 30% performance difference isn't going to make a large dent in the users who just don't want to use the Mac ecosystem. Again, their market share has stayed in single digits for decades, regardless of the CPUs used. Apple making a superior CPU may make AMD sell a few percent fewer chips, not threaten them in any meaningful way.",Negative
AMD,It's usually a year after tape out.,Neutral
AMD,"What would you normally do on a desktop? I could say stuff like word processing but my dishwasher could probably handle that by now.   Think things that required more sustained performance across multiple threads. Could be gaming, could be photo/video editing work. Could be compiling code.   Itd make more sense to compare the M4 etc series chips to AMD in terms of what both are designed for,, and theyre extremely good as well.   Id be very interested to see how the performance on both types of apple processors lines up (m vs a processors)",Neutral
AMD,"Adobe Premiere, Photoshop, Lightroom, basically the whole suite. Gaming. 3D rendering programs. Ray tracing. Big data throughput. Designed in general to run 100% flat out forever, heat and power be damned.   y'know, workhorse shit.",Negative
AMD,"Let me explain this in simple terms. You're asking why a Ferrari has better track times than an 18 wheeler. Both are designed for specific tasks in mind. High end Non X3D CPUs are like 18 wheels. They are designed for specific work. The 9800x3d is like a Ferrari. It's designed to be very good at gaming, but it's slower than the non x3d in multithreaded applications. Ray trace rendering, file exporting,engineering simulation, CAD, ect all favor the ""18 wheeler"" while gaming and some other tasks favor the ""Ferrari"". Just as in real life those two vehicles serve different purposes, with the Ferrari being the better option for a task like a track and the 18 wheeler being the better option for hauling 40k pounds",Neutral
AMD,Yes and Geekbench is a useless benchmark that compares to nothing except other apple cpus as always. I dont even know why they are comparing to an Intel or AMD cpu that focus on SMT.  As always sensensional trash.,Negative
AMD,The bunch of my clients that are asking me to migrate their SM businesses to x86-64 machines from m2-3 macs because of the huge performance gap in their workloads would like to disagree.   As I said on the other comment this post had no reason to compare to a 9950X and not just previous gens of A or M chips.,Negative
AMD,"True, or you could spend half as much on a phone that does everything you need and you'd have $600 towards a GPU in your PC that'd give you way more gaming performance. Maybe I'm out of touch, I don't know anyone in my circle who games on their phones other than one of my boomer parents who plays scrabble and candy crush.. which any phone can play. Just seems like a niche used to justify needing a 1k+ phone.",Negative
AMD,"For me it would just be to have something really lightweight to travel with that still has a decent keyboard built in and a proper OS where I can run dev tools. I think an A19 pro would still make a highly capable little dev machine.  Like, a 13"" MBA is light but it's not ""forget it's in your rucksack"" light.",Positive
AMD,"But you realize what you're saying has nothing to do with the original comment questioning it's sustained performance? He asked about sustained performance, not efficiency of a chip if it had active cooling",Negative
AMD,Cooler than your fingers until 4 minutes have passed? Did you even watch the video you linked?,Neutral
AMD,200 seconds? how about 200 minutes?,Neutral
AMD,I game on on my Steam Deck at 8 watts all the time. 8 watts is quite a bit for a phone.,Neutral
AMD,"Geekbench is mostly synthetic models of real workloads, with a few real world ones scattered throughout but the majority are synthetic. The real ones are ones like clang, which are about as relevant to most people as cinema4d.",Neutral
AMD,"So what should I actually be using when I'm just messing around with overclocks and stuff like that. Is cinebench still fine for that? Or is geekbench better for some reason? Apparently I'm not read up enough to know cb has ""lost its luster"" lmao",Neutral
AMD,"99% of people don't need high IPC in the first place if htat's your argument. you can watch 8k movies on an old cpu just fine.  but we're not talking about that, we're just talking about the raw performance of actual usage. gaming is about the only thing that every day people would notice a difference in cpu speed because bigger fps number more better",Neutral
AMD,Geekbench is terribly weighted and it's rarely representative in the CPU space. You really do want specific workload tests like cinebench or single category suites like SPEC2017. SPEC benches sT and nT separately.,Neutral
AMD,Cinebench reflects the rendering subsystem of Maxon Cinema 4D which is absolutely a real world application with real world utility?  It might not reflect what YOU do with a computer but that's not the same as saying it's not a real workload,Neutral
AMD,"A minority of the Geekbench CPU tests use real applications, most simply model one.",Neutral
AMD,"I don't think the results are that accurate for most users workflows. And it seems even less accurate for the workflows of users who need higher performance cpus. Plus the results are kind of irrelevant when you can just seek out benchmarks of what you specifically want to do. Many android manufacturers have been discovered to be juicing their numbers (which I actually don't think apple is). Using different application suite benchmarks is much better. Sp I geekbench is a mediocre at best general benchmark, but the numbers from general benchmarks are kind of useless. Maybe specific tests in it are good, but I'm not going to bother drilling through it.",Negative
AMD,"Appreciate your reply. People have been too tribal about X86. The software barrier is so much easier to cross than the hardware barrier. I worked at these companies developing code for ARM CPU within the general SoC, so I know it's not impossible for these companies to switch to ARM or even RISCV.   Support has been barebones, but I'll take ARM and RISCV over any X86 architecture, which I still dont understand to this day. Worst still, management is still sticking on X86 but at least diversifying to ARM, and the possibility of RISCV is at least on the table.  Even for gaming, ARM designs are bring considered internally as well, but it takes two hands to clap. Microsoft and the general public have to step in to force these companies into making good products with usable API provided for the respective platforms.  Frankly, I am tired of having subpar products compared to the M4, which runs circles over X86 and open source id a thing now with rosetta emulation. I really dont see why X86 should be the defacto CPU to be used now, and I really truly dislike the narrative ARM/RiSCV should be reserved for low powered devices and such. X86 should be left to legacy devices period, and newer devices should at least be ARM based or even RISCV, which should be the defacto. Sadly, the cash cow is X86. Those 2 architectures are just on the sidelines despite their ease of use and efficiency.",Neutral
AMD,Pretty much every review pitting the M1 onward VS top desktop chips from reputable PC reviewers.  Kinda hard to miss that dead horse.,Neutral
AMD,"Also just an amateur but if I'm going to go out to take some photos I'm going to take my camera with me.   Smartphone cameras have gotten a lot better over the years but I'm pretty sure I could get nicer looking photos from my old retired D3100. Some of my favourite shots I took where on my D3100 using old vintage lenses but I used that camera so much for a long ass time so it's expected. I still use old manual lenses on my new camera, very pleased with the results. A smartphone sensor wouldn't do those lenses justice at all.   Granted with photography it's moistly personal preference.",Positive
AMD,"Modern games are multi-threaded.   We are discussing a mobile chip, used to run mobile apps. Not PC games.",Neutral
AMD,i meant that QC had full custom cores but they flopped before. they bought nuvia to compete vs ARM Stock and Apple,Neutral
AMD,"That is simply looks implausible, the current Strix Halo reaches 5060 performance. either way it has to compete by then vs 6000 series GPUs. Medusa Halo might have a 5070 like GPU but that is 2000€+ laptops, there's no cheap Strix Halo laptops, every single one of them is cheaper to have a normal CPU+Nvidia 5070 which is much faster",Neutral
AMD,"Most APKs will be signed properly. At least that is what happens on windows and mac where similar restrictions are in place. OFC windows it is easy to move past the smartscreen window, but it is pretty rare that i have to.",Neutral
AMD,"We'll be able to install through shizuku I reckon, which is an on-device adb she'll  Still not ideal, but you already need to use shizuku for everything a power user might do, nowadays",Neutral
AMD,The inexorable march towards total information control is dark & dangerous,Neutral
AMD,Not only that but you can say goodbye to using apps that require security frameworks. No more banking apps or pay by phone,Negative
AMD,"And eventually a Google EU account, I imagine, after the EU takes the same legal action against Google.   But not being able to use the google account I've had for 15 years with an Android phone is going to suck. At least you can add multiple gmail accounts to the gmail app, I think.",Negative
AMD,Source?,Neutral
AMD,"Dev account is rather limited. There are apps like AltStore which allow you to sign a package with your dev cert and load it into iPhone. But there's a limit on the number of apps at the same time, I think 2, and it needs to be refreshed every several days, I think 7. And I'm not sure where I would find unsigned distos of apps.",Neutral
AMD,"> Google demanding the verification and setting up the rules that will decide if that verification is valid  I'm currently contributing to a FOSS project that already verified by Google and published into GPlay. I don't think there will be problems installing it outside of PlayStore.    In comparison, this app is impossible to properly run on iPhone as it requires JIT (you don't want to run JVM with interpreter instead).  > There is nothing that would stop them just not verifying apps   This can happen too. But this doesn't equal to ""Google removing sideloading"".  Either way, ADB will still work as it does now.",Neutral
AMD,"IIRC, they don't verify apps, they verify developers and give them certs to sign the app so the certified device will accept them.  As far as I understand.",Neutral
AMD,"Official store verifies apps. 3rd apps won't be verified, the developer signing them will be.",Neutral
AMD,"What about open source apps, github nightlies, patched apps, apks that are no longer mainted but can still be found online but will never get signed. This isn't great. Now Google decide which apps we can use, and how apps can be used, in order to weed out the 0.001% of bogus apps.   I got a train ticket the other day, the train was late, to claim a refund I had to send a picture of the ticket to the train company, but the company that sold me the ticket banned screenshots in their app. Honestly, android was better 10 years ago.",Negative
AMD,unless 100% of certification requests get verified its the same thing as them removing nonstore apps.,Neutral
AMD,"No, it didn't. Lack of regulation and regulation allowing something are effectively the same to Google. Google could have moved to a system as strict as Apple's whenever they wanted pre-EU rulings.  This has nothing to do with the EU and everything to do with their growing anti piracy stance. They are struggling to monetize YouTube and Chrome and their solution has been manifest v3, and this. The point is to get rid of revanced,   Get your head out of your ass. They were cracking down long before the EU ruling. They simply waited pending the results of the lawsuit so they wouldn't get hit with their own lawsuit like Apple. Once the (short term) regulatory confusion was cleared up, they moved to this.  If there had been no lawsuit in the first place, we would have seen this years earlier.   Even ignoring all that, how nonsensical is it to say that it ""created this""? The free market had two options, Apple or Google. How did the free market keep Google from creating a tighter system? Hell, Apple looks better now, so there's real competition, which makes it tougher for Google to implement this, if it really was because of the ""free market"" that they didn't. Apple will pull more users than they would have prior to the EU ruling, making it riskier for Google to make this move.  FrEe MaRKet lmfao",Negative
AMD,"For games.  > something useful?   ""Useful"" and restricted phone hardware/software are incompatible things. I have a proper laptop for productive things.",Neutral
AMD,Did you read my comment?,Neutral
AMD,Nah I've always been kind of uncool /s,Neutral
AMD,You’re referring to the way sideloading works everywhere else. It’s less restrictive than that in the EU. It’s just that alternative app stores haven’t really taken off yet.,Neutral
AMD,"A Mac Mini with an M4 Pro, 12 cores, and a massive GPU onboard with a decent amount of IO while only consuming around 5w at idle and at most 65w at full power.  9950x uses 8x more power at idle and nearly 2/3 the power the M4 Pro uses at peak. That's more than just a little bit of power.",Neutral
AMD,"""a bit"" is like 80 watts on idle for an entire PC on ryzens lol",Neutral
AMD,"I don't think that anyone is confused about that point, it just seemed like a weird tangent from this particular subthread.  But to address this point that you clearly feel very loudly about: the A19 gives us some information about what the M5 will be like. Which very much does appear in full general-purpose computers that will allow you to run whatever software you want on them.",Neutral
AMD,When things change they change rapidly. Companies buy laptops because they run windows not because of the CPU they run as long as the basic desktop apps run all right then they will switch.  The biggest stumbling block is that ARM CPU's on the desktop are expensive.,Neutral
AMD,"Whether someone is buying the chips or buying the devices doesn't matter. AMD sells chips to system integrators, but ultimately those laptops have to be purchased by consumers who *are* comparing them against macbooks.",Neutral
AMD,"Most consumer AMD sales are through OEM devices.  As such apple competes directly with these devices.  So yes apple competes with AMD, if apple sells more laptops then AMD sells less (assuming intel holds steady).   Since the apple silicon introduction to Mac there have been a LOT of people opted for Macs that might have in the past opted for a higher end PC laptop.  This is not just about performance but also perf/W (after all it is a laptop).  Apple sells way more laptops with apple chips in than laptops with AMD chips in them.  The reasons apples market gains have not impacted AMD much is due to intel owning that market not AMD.",Neutral
AMD,"you're desperately moving goal posts to make your point stick.   Performance-wise, which is where the conversation is, AMD pretty much competes directly in a lot of tiers vs Apple (desktop, laptop, etc).",Neutral
AMD,"They said Zen 5 was coming out 1H 2024 using that, and yet it came out later, and the flagships like strix halo, 9950x3d didn't even release till 2025. Idk how AMD remains competitive unless they have the greatest single generation uplift seen in the industry since the m1",Negative
AMD,You do realize these are the same cores on the A19 that will be used on Mac’s that will do the exact same kind of workloads that the AMD chips will do right?,Neutral
AMD,Almost all of those are heavily limited by single threads.,Neutral
AMD,"This ignores that the 9950X is *also* the fastest 1T performance AMD offers. *That's* why the 9950X gets compared in 1T tests.  1T and nT are not opposite sides of the spectrum and to win one, you need to lose the other. Their performance is unrelated to each other. You can have a low-perf 1T cores and a high-perf nT SoC. Just add more weak cores, voila.",Neutral
AMD,">Geekbench is a useless benchmark that compares to nothing except other apple cpus as always  Nonsense. All CPU designers have used Geekbench to test microarchitecture performance in consumer applications: GB6 is a comprehensive consumer workload benchmark (and not enterprise / scientific / professional), and more rigorous.  Just because *you* cannot apply it does not mean Geekbench is now ""useless"". Ironically, Apple is the **only** CPU firm that doesn't publicly use Geekbench. So if you don't like Geekbench scores, Apple ought to be your top pick.  [AMD uses Geekbench](https://www.amd.com/system/files/documents/amd-benchmark-white-paper.pdf).  [Arm **also** uses Geekbench](https://images.anandtech.com/doci/18871/Arm%20Client%20Tech%20Days%20CPU%20Presentation_Final-22.png).  [Intel **also** uses Geekbench.](https://edc.intel.com/content/www/xl/es/products/performance/benchmarks/architecture-day-2021/)  [NUVIA **also** uses Geekbench.](https://medium.com/silicon-reimagined/performance-delivered-a-new-way-part-2-geekbench-versus-spec-4ddac45dcf03)  Of course, companies publicly use Geekbench (and every other benchmark) only when they look good.  //  Good thing it's not just Geekbench, though. The M4 absolutely demolishes Zen5 & Lion Cove in integer & floating point workloads. I suspect the A19 Pro & M5 will do even better.  |CPU|SPECint2017|SPECfp2017|Geomean|%| |:-|:-|:-|:-|:-| |Apple M4 Pro|11.72|17.96|14.51|131%| |AMD 9950X (Zen5)|10.14|15.18|12.41|112%| |Intel 285K (Lion Cove)|9.81|12.44|11.05|100%|  I might predict you're about to start whining about SPEC, too: 3…2….1…go!",Negative
AMD,"Do you think an A19 Pro would make any kind of signifant weight change in a laptop vs a base level M chip though? I mean.. they have M chips in the IPad Pro already with less cores and I don't see how you can get much lighter or thinner in any meaningful way. Cutting out another core or two doesn't really make a difference in terms of direct weight loss since the cores don't really weigh much. I'm assuming you mean since it has less cores they could design it with fewer thermals in mind and overall less weight because of reduced thermal management system. They can and already do that with the M chips. They will simply thermal throttle at different workload lengths.  I guess what we each call 'significant' in terms of weight in this context is pretty subjective. Even the 15in macbook air, IMO, is so insignificant to the overall weight of my bag, it's hard for me to notice if it's there or not. I'm also less concerned with weight than I am with volume in my bag, and I don't see how they could get much tinner without some miracle in materials science for structural integrity.  Now, if they could do that and the A19 Pro chip is significantly cheaper and they passed that onto us, I could see that being nice.",Neutral
AMD,"Did you, u/bb999? The absolutey arrogant ignorance on r/hardware never fails.  1. 200 seconds = 3 minutes 20 seconds, not 4 minutes. At 3:20, the temp meter on the modern smartphone [reads](https://imgur.com/a/oWb7z4r) **35.9C**.  2. The human body temp is **37C**. 3. 35.9C phone < 37.0C body (assuming room temp, as stated).  >100% load at 5W for 200 seconds makes a modern smartphone … still ***cooler*** than your fingers at room temp (<37C).  Do you understand *anything*?",Negative
AMD,"Do you run with the fan on or passively at 8W? I assume you mean 8W from BATT, not 8W from the TDP slider.",Neutral
AMD,I'd bet there are way more software developers than graphics people so clang is a more relevant benchmark than cinebench.,Neutral
AMD,"If you're checking performance differences either should be fine. I'm not very experienced with overclocking, but something longer and continuous like CB is probably better for stability and thermal testing.",Neutral
AMD,"Specific workload tests like cinebench are great for their specific workload. But you need to refer to other benchmarks to get a wider view of overall performance  Ideally CPU reviews should have various specific workload tests and GB6 & SPEC2017. Unfortunately, there aren't many specific workload tests on iOS/Android   Geekbench 5 actually had [very similar correlation to SPEC2017, according to NUVIA it was basically within margin of error](https://medium.com/silicon-reimagined/performance-delivered-a-new-way-part-2-geekbench-versus-spec-4ddac45dcf03)  Geekbench also benches sT and nT separately. Although both GB6 & SPEC2017 have issues with scaling nT  Specific workload tests are far better for testing nT, since many workloads scale differently",Neutral
AMD,"Don’t worry, the M4 is 30% faster than a 285k in SPEC2017, whereas the 9950x is only 12% faster.  And this is the m5 architecture.",Positive
AMD,You could just look into gb's subscores for workloads that matter to you.,Neutral
AMD,These people are on crack or something. If you do any 3d render workflows then the cinebench is obviously applicable and much better than a general benchmark.,Neutral
AMD,"Great and what does that mean? Exactly, nothing.   3D Rendering can be equally done on the GPU, there is no justification to run that on the CPU.   The issue is that not all Cinema4D Projects will be identical in performance, there are massive differences between projects (just see more extensive Blender tests, for each project the relative performance differs).   The Benchmark itself is designed to exclude all other aspects like RAM speed and Cache which makes it purely synthetic. The only reason to run renders on the CPU instead of GPU are projects that exceed your GPUs RAM capacity. But then, Cinebench doesn't tell you anything.   And you'll probably say something like: ""But the results are more in line with other benchmarks""   But that's not really true either. It's similar to other 3D rendering benchmarks and do you know why? Because it's similar in Software. They use intels Embree [Embree Github](https://github.com/RenderKit/embree) which is not only used by Cinema4D but many other apps too. It's also used by: Autodesk, Blender, Corona, Redshift and V-Ray.   [https://www.embree.org/](https://www.embree.org/)  It's exaggerated but it's fair to say that Cinebench is basically a synthetic Benchmark of intels Embree.    That's also a reason why ARM CPUs usually score lower there because Embree is optimized for x86 AVX functions but doesn't really supports ARMs vector functions.",Negative
AMD,"Still rather realistic (it tests web performance, compression, PDF rendering and code compiling) compared to more synthetic benchmarks.",Positive
AMD,It accurate for most normal folk,Neutral
AMD,That should make it easy for you to provide an example then,Neutral
AMD,"I think your issue is when someone provides an argument against you, you completely change the topic, or you provide a claim that isn’t backed up/irrelevant. I think it’s called moving the goalpost?",Negative
AMD,">What you're showing isn't difference between cores but whole package, where x86 CPUs are at major disadvantage due to their memory not being on chip and countless other factors regarding core parking and voltage control.  Irrelevant nonsense on each count.  These power measurements are packages ***all*** *CPUs*. Apple's M2 core (not package) power ***IS*** less than 5.3W.  Memory on package: memory power consumption is *much* lower than CPU power consumption here. Saving ""up to 40%"" on 1-3W of SODIMM DDR4 is like scrapping the paint off a pickup truck to reduce its weight and save money on gas. Memory on package is *only* substantial when the CPU core is extremely efficient.  Core parking & voltage control: AMD & Intel use these, too. Are you illiterate?  //  >If anything compare them with lunar lake, which is closest to any ARM SOC architecture wise.  Read this 100 times so it's tattooed in your obstinate head:  |Cinbench 2024|1T points|Power|Points / W| |:-|:-|:-|:-| |Apple M3 (MoP, TSMC N3B)|141|11.1W|12.7 pts / W| |Intel 258V (MoP, TSMC N3B)|120|22.4W|5.36 pts / W|  [Huh. Memory on package. TSMC N3B. 4 big + 4 little cores. Apple's M3 is **50% less power** and **18% faster**.](https://www.notebookcheck.net/Intel-Lunar-Lake-CPU-analysis-The-Core-Ultra-7-258V-s-multi-core-performance-is-disappointing-but-its-everyday-efficiency-is-good.893405.0.html)  It was never the nodes.  It was never MoP.  It was never core counts.  It was never ""core parking"" lmao.  It was never ""voltage control"" lmao.  Apple does 1T CPU power ***exceedingly well*** after decades of physical layout optimisation, foundry DTCO, front-end (think branch predictors & cache), back-end (think 8x ALUs). Both Intel & AMD are years behind.  //  >defending people who are dead wrong just because you're members of the same cult is called arselickerism.  You might want to get over yourself. Everyone else has.",Negative
AMD,So why are you all comparing a mobile chip to a desktop chip. Hence my original point where this benchmark and comparison is stupid.,Negative
AMD,Yes you can.  You can also use different Google accounts within the play store. It can get around some download restrictions but I'm not sure where the limits are.,Neutral
AMD,"That's a thing?   With android they're are talking about Termux (or proot-distro), which essentially is micro-distro running locally via chroot (so not emulation). There's package manager and all.",Neutral
AMD,Samsung seems to be removing the possibility for one:   https://www.reddit.com/r/Android/comments/1mabuht/samsung_removes_bootloader_unlocking_with_one_ui_8/  But the trend has gone in that direction for a long time.,Neutral
AMD,"It is all speculating whether Google will double down or not, but they definitely will do everything slowly, letting the frog boil comfortably. The favourable (for Google) final destination is already known though. Everything they did so far doesn't really make me think they don't want to build their own walled garden.   Also I can't quite remember the app which I got from another source as APK, but on launch it asked me to go to play store and install it from there.",Neutral
AMD,"> I'm currently contributing to a FOSS project that already verified by Google and published into GPlay.  Congratulations, you are an approved Android developer that Google doesn't care about.  This is not about you or your sanctioned app.  This is about apps Google doesn't like.",Negative
AMD,"Right, but the developers of several of these apps are already operating in a legally grey area, and they likely don't want to doxx themselves to Google lest they have lawyers knocking on their door within 15 minutes.",Negative
AMD,"If you were the dev of certain youtube variants, would you give your data to google?",Neutral
AMD,"You understand incorrectly.  The moment a developer signs an app that Google doesn't like, Google will go after the developer (having real-world info about them to do so) and all apps signed by them.  This is worse than individual apps having to be approved.",Negative
AMD,"Ah yes, expecting to verify as developer by google while at the same time google is trying to sue you for.... disabling ads on youtube.",Negative
AMD,Its the same thing. Theres no incentive to verify developers that werent allowed on the official store.,Neutral
AMD,I use a 13 year old abandonware audiobook player. Its an unsigned APK. It comes from an era before ads were allowed inside apps.,Neutral
AMD,"Yes but it is not the bogus app, it is malicious malware. I think it is hitting billions lost every year to the extent it is affecting society fabric. Open source is not going to be that affected, they just need to apply for a cert, sure it is going to slow them down a couple of days but to save billions of dollars for all the poor soul getting scam? Sounds like a fair compromise. I am a iPhone user as much as I am annoyed on the restriction, the fact that Android caused so many people to get scammed make me feel it is kind of worth it. Would it be better if people just stop believing the scammer? Sure but so far we are losing.",Negative
AMD,You claim useful and restricted phone are incompatible. Yet the thing you brought up when I asked was a useless thing. So you contradicted yourself.  And no one has replied with anything useful.   I use my 5 year old rooted android to have unlimited hotspot via modified IPTables.   Cant do that on my newest z fold 6. So my main phone is an iPhone. Only useful thing I use android for is something not powerful. I don’t really need a powerful device.,Negative
AMD,"I’m envious, I wish I could use Nzb360, heard such good things.",Positive
AMD,So basically you know jack shit about what you're evangelizing for.  Typical.,Negative
AMD,So... why would I use a device that is more restrictive just because I'm not in the right region?,Negative
AMD,"yes and it plugs into the wall. who cares if it uses 65W or 165W. the ryzen box gets better perf on the things that matter to people who use it, and better support for peripherals and addon cards so its just plain better.",Positive
AMD,"My point remains that no matter how good the excavator, it does not compete in the market for the drill. Macs are a separate, parallel market.  You can have a 30% faster CPU in it, but it is clear from the relatively stable OS market share that people replacing their existing Windows/Linux device look for new Windows/Linux devices, and people looking to replace a MacBook look for the newest MacBook. Moves from one to the other are rare among mass market users. This also isn't changing, as there are firm barriers that have never moved. Among others, despite existing for decades, Macs remain impractical in the vast majority of the world, where the OS is just not commonly used or well supported.  From the technical perspective, as a hardware geek I can appreciate the work that Apple teams did on their silicon, and it's great to see those performance gains. People with an old Mac may be tempted to upgrade. But as an average user getting a new work laptop, it is irrelevant to me as we don't use Macs, and the Apple silicon is not available in the open laptop market, so it never existed as an option. I can only realistically choose between AMD and Intel.",Neutral
AMD,Strix Halo redid the whole inter-chiplet fabric which is why it was delayed.  > Idk how AMD remains competitive unless they have the greatest single generation uplift seen in the industry since the m1  Zen6 appears to be on 2nm. So it should easily be more competitive since it will have a node advantage for the first time ever. Up until now Apple had a node advantage. Zen6 flips the tables.,Neutral
AMD,"Yeah, I guess thats the other side of it. Put it in a different cooling and power environment and see how it goes.   Theres not much point in saying you'll do the same work on that chip in a phone chassis as you would do on it if it were in a configuration thats not power and heat limited. OS aside.",Neutral
AMD,"After Effects has multi frame rendering and scales heavily, almost infinitely with core count.   Premiere and Lightroom leverage multithread and lean on GPU heavily.   Blender and V-ray for CPU 3d rendering heavily lean on multicore. In fact all CPU rendering options for all of this scales heavily with multicore and RAM.",Neutral
AMD,"Dunno where you got your spec numbers from, but at least for the 9950x they seem waaay off. Anandtech (clang/gfortran) has them at ~11/17.7, C&C (gcc, faster mem) at 11.9/19.8...",Neutral
AMD,"I think a lot of the weight in an MBA is in the battery. I would hope that a smaller die with fewer cores would get better idle or light-load power, so could get away with a smaller battery and an overall lighter machine.",Neutral
AMD,"Core body temperature is 37°C. Fingers are not your core and they're more like 30°. But that's still irrelevant. The human body generates heat and wants to constantly shed it, thus a comfortable ambient temperature is lower than body temp. In yankee doodle units it's 97 freedom degrees - that's not a comfortable day out. It isn't going to burn you or anything but you will end up with hot, sweaty hands.",Neutral
AMD,Very few software developers work with compilers these days.,Neutral
AMD,Link? nT? 1T?,Neutral
AMD,"Yes, I have. But people dont report or reference those. They report the weighted total combined scores. Like this post.",Neutral
AMD,"There were somewhere under a 500k 3d artists, animators, etc in the entire US last I checked. The remaining 340,000k aren't that worried about how fast a computer can render a scene.",Neutral
AMD,"> 3D Rendering can be equally done on the GPU, there is no justification to run that on the CPU.  Yes there is, when your scene doesn’t fit in VRAM. No, this is not a hypothetical niche case. GPU rendering is not adopted in many film workflows because of VRAM issues. In many cases, GPU renderers have additional feature limitations, ex, in Blender you can’t run custom OSL shaders when using GPU rendering on macOS (choosing macOS here because that’s a platform where VRAM limits don’t apply due to the unified memory)  > The issue is that not all Cinema4D Projects will be identical in performance, there are massive differences between projects (just see more extensive Blender tests, for each project the relative performance differs).  For the most part, these differences are consistent from processor to processor and are mostly the result of the number of computations the scene needs, not different workloads. Ex, if scene A is 30% slower than scene B on processor X, it tends to also be 30% slower on processor Y.  > The Benchmark itself is designed to exclude all other aspects like RAM speed and Cache which makes it purely synthetic. The only reason to run renders on the CPU instead of GPU are projects that exceed your GPUs RAM capacity. But then, Cinebench doesn't tell you anything.  No, that’s just the workload CPU render engines present. I’m sorry if my job does not tax the hardware subsystem you wish it did, but that’s the way it is. Cinebench is quite frankly an excellent proxy for 3D render performance in general, as your remaining paragraphs explain quite well.  If you don’t care about 3D render performance, that’s fine. But saying Cinebench is a synthetic or meaningless benchmark is absurd. It’s an excellent real world benchmark, just maybe one that tests something you don’t personally care about",Neutral
AMD,"Just so we remember what happened here, Antagonin deleted all their replies, but couldn't resist another pathetic personal attack, which was also was deleted within hours:  [https://i.imgur.com/tnDgnUH.png](https://i.imgur.com/tnDgnUH.png)  lmao.",Negative
AMD,Samsung has been a pain to bootloader unlock for at least more than a decade.,Neutral
AMD,Yup. Easy way to say get rid of any and all emulators through legal channels.,Neutral
AMD,"""certain"" youtube variants already require you to have your own patched build, I see no problem of signing them with my certificate to install them onto my device.  For testing ADB exists, removing this will actually hit Google as it's used by a lot (unlike sideloading actually).",Neutral
AMD,"Don't use your ""legit"" certificate to sign YT mods and use ADB instead?  ReVanced already requires users to have their own builds instead of downloading already patched ones by others.",Negative
AMD,"> And no one has replied with anything useful.   Oh, I'm sorry, I didn't get here within 4 minutes to personally give you a list.  I need to use old versions of apps, and sideload them, for a ton of things.  Email apps (old versions of K-9 Mail, as an example) because the functionality in newer versions is broken or because the UI is awful.  Several ""Smart home"" / ""IoT"" apps because the company is dead or the old hardware I have is now ""unsupported"".  For one specific example, I have an old lighting setup and the hub and bulbs work perfectly, but the app was pulled from the store in favor of a new version. The new version of the app won't work with my hub unless I update the hub firmware and lose basic functionality that they put behind a paywall and ""cloud service"" for spying.  File manager apps because Android is fucking awful and constantly changes the file system and breaks things, all in the name of ""protecting"" me when I'm trying to do something as simple as move a file on my device to another folder or organize downloads and attachments from email.  And there are also times where I downloaded an app on one device only to be told it's ""not compatible"" or ""not available"" on another device due to bullshit fake requirements, regional settings, things being pulled from the store for new downloads but still present for existing users / updates, or Google's awful practice of beta testing releases to random subsets of users and hiding them from others (Google's not the only one guilty of this).  If I download it from elsewhere, or extract it from one device that has it, then sideload it onto another device, it almost always works just fine.  Then you have utilities related to networking, ad blocking, etc. or anything requiring root access, or anything centered around enabling/managing root access.  Android is locked down garbage at this point.  AOSP is a non-starter.",Negative
AMD,I never said you should.,Neutral
AMD,How exactly are they supposed to make a better CPU when the current one uses so much power. They can't do better while using this much power already.   Power use does matter because you need to cool it and at some point that becomes impossible.,Negative
AMD,Mac laptops are not a separate as you say.  The majority of laptop buyers are corporate vendors buying large shipments.  Macs are popular in that space.   In the volume laptop market AMD is a long wya behind apple and is in direct competition to apple.,Neutral
AMD,"Zen 5 on 4nm is not more efficient than M2 on 5nm, with their x86/chiplet latency and power problems that's not a given. So node advantage isn't an automatic win for AMD, and it might not be long before m6 launches after zen 6, especially with how long it takes for AMD laptops to come to market.   For example, there are still no AMD laptops with 5080 or 5090 despite them having x3d, and the premium 2-1 OLED laptops are pretty much intel exclusive for years.",Negative
AMD,"[These are from Geekerwan](https://youtu.be/2jEdpCMD5E8?t=185) of their stock 9950X test, where they should be internally consistent. IMO, SPEC ought not to be compared between reviewers and sadly AnandTech never reviewed the Apple M4 (Pro).   For another reference, I checked David Huang[. He shows M4 Pro holds a sizeable lead from the 9950X, yet a notably smaller gap than Geekerwan.](https://blog.hjc.im/spec-cpu-2017) Though looks like he only tests int and shows the M4 Pro with only half the advantage seen in Geekerwan.  |SPECint2017|Apple M4 Pro|AMD R9 9950X|M4 Pro Advt| |:-|:-|:-|:-| |Geekerwan|11.72|10.14|\+15.6%| |David Huang|13.7|12.6|\+8.7%|",Neutral
AMD,"True, the battery is a significant amount of the weight. I'm wondering how much of a difference in idle or light load power draw it would be. I would also benefit from that. I think something that could be an even big game changer in terms of weight would be if Apple used the same silicon-carbon battery tech they are using in Iphone 17 Air in this super light macbook air you're thinking about. As of right now, the Iphone 17 Air is the only Apple product using it and they haven't mentioned using it in any of their other products.. YET. Silicon-carbon batteries offer a 10-20% increase in energy density compared to traditional lithium-ion batteries with graphite anodes, though the theoretical potential is much higher. That could translate to up to 20% thinner while maintaining the same battery capacity. Other manufacturers have already started using it in their phones.. mostly Chinese brands like Xiaomi, Honor, OnePlus, and Nothing. But I haven't seen it used in any laptops yet. I read that the Framework Laptop community has discussed the possibility of carbon-silicon batteries for the Framework 13.. but haven't seen anything solid yet.",Positive
AMD,">In yankee doodle units it's 97 freedom degrees - that's not a comfortable day out. It isn't going to burn you or anything but you will end up with hot, sweaty hands.  That is not applicable here as each body part *perceives* heat differently: studies for fingers and electronics (a common test these days) show that [at 23C ambient](https://asmedigitalcollection.asme.org/electronicpackaging/article-abstract/139/3/030802/372761/Overview-of-Human-Thermal-Responses-to-Warm?redirectedFrom=fulltext) a 36C electronics device held by humans is reported as ""comfortable in sensation"" and ""neutral in temperature"", far below the perception of warmth—that was only reported at 40C, per Fig. 2 and Fig. 3.  >**Fingers thermal sensation scores** with surface temperature. X-axis shows the back-surface temperature of the simulated tablet computer ranging from 34C to 44C and Y axis shows the participants rated scores on fingers’ thermal sensation. The lines in different patterns show the responses in different ambient temperatures of 13C, 23C, and 33C.  ...  >Similarly, as the surface temperature increases, the average thermal discomfort score on fingers increases from neutral to uncomfortable. However, the neutral threshold is different among the ambient temperature. **The neutral rating** is about 42C at 13C ambient, **about 40C at 23C ambient**, and about 38C at 33C ambient.   36C was never perceived as ""hot"" nor ""warm"" in the simulated tablet heater. However, all that being said, a neutral feeling on your fingers is not the same as *cooler*, so I appreciate your appropriate correction.  EDIT: added second interesting quote!",Neutral
AMD,"What are you talking about, do you think the code just directly run on the machine without any translation?",Neutral
AMD,"Did you read anything of what I wrote?   I'm not saying that Cinema4D Benchmarks are synthetic, only Cinebench is.   It's a bit older but here is a reddit thread where Andrei Frumusanu from Anandtech gave his opinion on Geekbench versus Cinebench as a general Benchmark: [https://www.reddit.com/r/hardware/comments/pitid6/eli5\_why\_does\_it\_seem\_like\_cinebench\_is\_now\_the/?utm\_source=share&utm\_medium=web3x&utm\_name=web3xcss&utm\_term=1&utm\_content=share\_button](https://www.reddit.com/r/hardware/comments/pitid6/eli5_why_does_it_seem_like_cinebench_is_now_the/?utm_source=share&utm_medium=web3x&utm_name=web3xcss&utm_term=1&utm_content=share_button)  It covers other aspects that I didn't. The intel Embree thing came in with R23 but the other issues still exist.",Neutral
AMD,From an outside perspective it looks that way. And I never defended him. I genuinely didn’t. It was purely criticism. I really couldn’t care less which chip is faster. Perhaps you think everyone who disagrees with you is irredeemable?,Negative
AMD,Especially the Snapdragon variants.,Neutral
AMD,"I guarantee you are lying about using 99% of the apps you mentioned. And you are ONE person who’s rattled off anything. All of which, 99.99% of people don’t need. Like you.  Also, this just goes to show that you were using bad products and services. How smart is your IOT device if it can only be used by your phone? And you’re using an email service that can’t even make a good client android app. Sounds like you’re making all the wrong mistakes in your life, buddy.   Imagine complaining about android and then using the file system instead of just using a phone that can handle it",Negative
AMD,"1) I can run x86 Windows on ARM Android. Not hard to comprehend.  2) x86 has its advantages, specifically in that it is, by default, much more open  and has much wider support than Apple Silicon.  3) My arguments and positions are the exact opposite of being tribal as I want the most OPEN implementation, not to be stuck with only the OS I'm ""allowed"" to use with the hardware by any one particular corporation. Apple Silicon and iOS is the most proprietary option out there.",Positive
AMD,"they do make better CPUs and the cores get more efficient under load. zen5 can do a lot more at 125W than a zen3 chip. and the beauty of it is, unlike apple silicon, you can scale up the power draw with core count to 350W for epyc zen3, and 500W for epyc zen5 if you need it.  apple silicon is stuck at like 60W peak power draw? it doesnt scale up. yeah the idle is great at like 5w but once again, it plugs into the wall and nobody who needs to use the system to do work instead of sitting idle on their desktop cares.",Positive
AMD,"Macs are pretty rare among business / corporate buyers because the vast majority of users are familiar with Windows, and providing users Windows devices significantly improves productivity and reduces complexity if the alternative is having tons of people with varying degrees of technical abilities having to learn how an OS they've never used operates.  Beyond few very specific clients in the US/Canada/UK, I typically work with companies that don't provide anything but Windows laptops to most staff. And outside of those countries I've never seen a business Mac, as Macs just aren't commonly used there. The entire education system and home compute is handled on Windows by default.  And this has got nothing to do with hardware. Macbooks were killing it around 2013, where their hardware was further ahead of the game - especially with the introduction of the Haswell MacBook Air, which was likely the furthers ahead of others Apple has ever gotten in the laptop game. I learned my lesson when even that did not lead to a change in marketshare - it's been nearly flat for decades, as it's mostly just Mac users only ever upgrading to newer Macs when given reasons to.",Neutral
AMD,"The topic was performance. And idle power / latency issues will improve a lot with the new fabric. That clearly wasn't the focus on desktop (as most people didn't care), but it's getting resolved.  x3d chips not being paired with 5080 and 5090 is stupid. I think everyone can agree with that. OEMs are regarded. Like why even release x3d laptops if you're not going to pair them with the fastest GPUs? Makes no sense.",Neutral
AMD,"36 C at ambient 23 C - that’s ok, literally colder than 36.6C of a healthy body. 42C at ambient 13 C - I can understand that, it’s cold, so your fingers are cold and touching something warm feels pleasant.  But 40C at 23C? 38C at 33C ambient?   33C ambient? That means I’m at home, in T-shirt and underwear, gulping ice cold water.  33C ambient means it’s scorching hot outside and you want to tell me that you’d like to essentially hold somebody with a fever in your hands? I dunno, something fishy is going on with those quotes.",Neutral
AMD,"> if you need it  Well who? Server? Even server wont go that extreme use case 500w easily unless we are talking about gpu. You are cherry picking extreme use case here.   Its efficient? No, but is it more efficient than pervious gen? Yes. But still huge disparity between apple",Neutral
AMD,"As long as they are not using arm, monolithic dies, and non on-package memory I don't see how AMD will beat Apple when it comes to real world efficiency, especially at low power and idle. the M1 on 5nm has been shown to have better perf/Watt single thread curve than lunar lake, which is easily better than what AMD has. x86 laptops still burn up when you close the lid and I don't see any plan to fix that.",Negative
AMD,"> 33C ambient? That means I’m at home, in T-shirt and underwear, gulping ice cold water  i mean that very much depends on where you live",Neutral
AMD,"> The TA95X3D brings the power efficiency of a mobile form factor, but it sacrifices upgradability.  Except those two soldered CPUs have the same chiplet layout as their desktop counterparts, with the same idle power issues.  Is there any sizeable difference to getting a socketed model and setting a power limit?",Negative
AMD,Minisforum is currently selling this type of motherboard but for the 7000 series,Neutral
AMD,"Hello kikimaru024! Please **double check that this submission is original reporting and is not an unverified rumor or repost** that does not rise to the standards of /r/hardware. If this link is reporting on the work of another site/source or is an unverified rumor, please delete this submission. If this warning is in error, please report this comment and we will remove it.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,I think price is also a factor the 9955hx3d model costs 661 usd while the 9950x3d alone costs 670 usd.,Neutral
AMD,"I've been able to compare the 7945HX vs the 7950X, both at a full multi-core load. With the 7945HX outperforming the 7950X by 5%, while drawing 10% less watts at the wall.  Both CPU's/Systems were tuned for maximum performance/watt efficiency.",Neutral
AMD,AOOSTAR's model seems to have 4x SATA but in mATX form factor; Minisforum is only mITX with 0x SATA.,Neutral
AMD,"You can look up reviews of similar CPUs from previous generation (7945HX3D), it's very capable - especially considering it uses ~~lots~~**less** power than the desktop models.   If Minisforum or whomever could make an ITX model with 2x SATA I'd be all over that.",Positive
AMD,Yes but based off desktop,Neutral
AMD,I hope their models will use dimm rams instead of sodimms,Neutral
AMD,"SODIMM is a very sad state of affairs for the minisforum board, considering the (lack of) quality DDR5 SODIMM memory vs the great kits you can get in normal DIMM form factor now.",Negative
AMD,Cheapest sodimm is 75$ for 32gb 5600mhz cl46 model while you can find deals for 6400mhz cl32 dimms 32gb,Neutral
AMD,Yep :'(. 2/3rds the latency!,Neutral
AMD,I'm waiting for the AMD 3100x3d,Neutral
AMD,Waiting for a special edition die-shrink of the 5800x3d as a last hurrah of AM4.,Neutral
AMD,it's not available worldwide :(,Negative
AMD,"I don't really get what AMDs strategy is here. They have gone 5800X3D, 5700X3D and down the stack to 5500X3D while the those earlier higher performing models are largely gone and not made anymore we get lower performing ones introduced.  We get these while X3D is not coming to lower end CPUs in the 7000 and 9000 series. It can not be the case that this is a higher margin way to sell the cache silicon add-on of X3D.  Are AMD just trying to shift old poor quality dies they have laying around?",Negative
AMD,I'm lazy why doesn't gamers nexus just have an average data slide,Negative
AMD,give me 5950x3d to replace my 5950x so it can catch up with my rtx4080 somehow,Neutral
AMD,"AM4 : ""I didn't hear no bell"" 💪",Neutral
AMD,"Charts are missing the 5800XT which is currently around the same price as this ($205 USD). Would've been nice to include it to see if the 8-core non-X3D part is a better value. The 5950X is in the charts, but you never know whether games will act funny due to the 5950X having 2 CCDs.  The 5500X3D has a very poor showing in productivity so perhaps the 5800XT would just be a better all-around pick especially if playing in 4K.  All-core sustained clocks:  * [8C] 5800XT: 4500MHz (via HUB) * [8C] 5800X3D: 4300MHz * [8C] 5700X3D: 4000MHz * [6C] 5600X3D: 4350MHz (This is why the 5600X3D is better than the 5700X3D in some game benchmarks despite the core deficit.) * [6C] 5500X3D: 3950MHz  It seems that the 5500X3D is just poor quality silicon if it can't even hit 4GHz. That's a huge clock speed deficit compared to the 5800XT and 5800X3D. When the 5800X3D first came out, it spanked the 5800X even though it was clocked 200MHz lower all-core (4500MHz vs 4300MHz).  But 5500X3D vs. 5800XT? I'm not convinced the X3D CPU is better. The 5800XT is top-binned silicon and could be OC'd to 4.9-5.0GHz all-core. The 5500X3D is stuck with 2 fewer cores and 3950MHz (you can't OC X3D CPUs). That's a ~1GHz clock difference and I don't believe the +64MB 3D V-cache makes up for this. The 5800XT still has 32MB L3 cache; it's not a Celeron. For those on a budget, the 5800XT looks even more attractive because it has gone on sale for $125-150 and comes with a decent cooler.",Neutral
AMD,"With those crazy fps we currently get, reviewers should focus more on 0.1% lows instead of 1%. 1% were good enough when average framerates were <60.",Negative
AMD,"Its awesome to see AM4 still alive, while Intel requires a new mobo for every generation...",Positive
AMD,"Remember the thread from a couple days ago when everyone said that Hardware Unboxed cherry picked the benchmarks to show that 6 cores CPUs keep up with 8 core CPUs in gaming and even when there's an improvement, it usually wouldn't be large enough to be noticeable with the naked eye? Funny how nearly all the benchmarks here show the same thing.",Neutral
AMD,"I have it, it's really good.",Positive
AMD,Looks like AMD's marketing department figured out how to make product focused videos instead of drama again.,Negative
AMD,how has there been no 7600x3d or something yet thats not a best buy exclusive  i think a 199 USD  x3d chip on the 7 series would be an AMAZING seller,Negative
AMD,Alternative title:   YouTuber who never used AM4 long-term declares AM4 is the GOAT platform because AMD keeps releasing new defective CPUs to maximize profits.,Negative
AMD,"Factory reject CPU that should not exist, on a stone age platform.",Negative
AMD,Backport 3D cache to Bulldozer.,Neutral
AMD,I'm waiting for the budget Ryzen 3 2200G3D,Neutral
AMD,Single core,Neutral
AMD,I'm waiting for the AMD 3150c3d. Let's see how zen1 holds up,Neutral
AMD,"If I'm not mistaken, Zen 2 actually had TSVs and was designed with 3D stacking in mind.  It's possible, possibly with moderate tweaks, that Zen 2 could get 3d vcache.   No idea if the actual implementation on Zen 2 was bugged or not and Zen 3, 4 and 5 are a lot more modern.",Neutral
AMD,"nah, zen4 or zen5 on am4 package. 100% it is doable.",Neutral
AMD,Yet.   AMD has done this in the past with regional or retailer specific launches before quietly releasing to the wider market after a few months.,Neutral
AMD,"> Are AMD just trying to shift old poor quality dies they have laying around?  Pretty much. All of the newer AM4 CPU releases are low-binned CPUs. The exception is the 5800XT which is a slightly faster 5800X. (The 5900XT is a slightly worse 16-core 5950X, not a better 12-core 5900X, adding to the confusion.)  The 5600X3D has higher clock speed than the 5700X3D, but 2 fewer cores. So the silicon is better on the enabled cores, and it wins on some games where clock speed matters more. The 5500X3D has lower clock speed _and_ 2 fewer cores, making it the lowest quality silicon of the bunch.",Neutral
AMD,>Are AMD just trying to shift old poor quality dies they have laying around?  Pretty much. Stockpile dies not good enough for 5700X3D and sell them late.,Negative
AMD,"Given that I thought the x3D packaging has a non zero failure rate.   Like, does breaking the CPU dies by putting the vCache on them allow them to write the broken dies off on taxes? While letting them sit in inventory unsold doesn't?   Like, obviously being able to sell them as working products is better, but. I mean, if the issue is they have unsold 5500 level CPUs, I would be more worried about. Breaking them with the die stacking, which I thought was something that could happen.",Negative
AMD,"My guess is because their business relies on YouTube metrics and if people just skipped to the slide, it could be financially bad for them.   Also, there's nuance that overall average performance can miss. Maybe a part excels/sucks in one genre of game specifically and it skews the average",Negative
AMD,"Better value for gaming - i don't think so  If productivity is a real concern, a high core count dual CCX cpu is probably needed anyway (I also don't think many who used am4 for productivity need/want to change cpu at this point)",Negative
AMD,"Why would you spend a 200 on a slightly binned non-x3d zen3 8-core? Like only ppl who think about am4 are ppl wth zen1/+/2 cpu:s and at that price point, why would you make that upgrade. The whole meme about the XT cpu:s is that it's $50 for letter and nothing else, can just manually OC a 5700X to be near the same or more/less cores for more/less money if you care/don't acre about productivity.  Just don't get a 5700 it's a 5700G without the IGPU, great naming amd!",Neutral
AMD,Yea 0.1% is better for sure. Frame time health would be best but only DF does that.,Positive
AMD,The 0.1% lows are still crazy fps. At this point fps is a solved problem in non RT games.,Negative
AMD,I mean this is no different than if intel released a 14599K with a 5% performance reduction to the 14600k… simply repackaging worse dies on a bad yield CPU isn’t anything special,Negative
AMD,"proper ddr4 ram is very expensive though if u want that, or if u need a new am4 mobo well there are no good left to be bought. only a couple of meh models left in retail. At least when I sourced the retail channels so to speak.",Negative
AMD,"LGA 1700 supported 2 gens, not bad. Raptor Lake was a really strong upgrade except for the RMA rate. Yeah it's not like AM4 which supported 4 gens, but i5 Raptor Lake beats i9 Alder Lake in gaming. Very rarely we see such leap within a single gen.    LGA 1851 was supposed to support 3 gens but became a single gen platform because Meteor Lake-S and Panther Lake-S were cancelled, perhaps for a good reason, which is the chips were very mobile focused and scaled poorly on desktop as shown by ARL-S only achieving low 5GHz and poor ring clock speed and latency.   Considering intel's current financial difficulties, I'm sure it was considered a waste of resource to design and validate desktop chips that aren't amazing on anything but power efficiency. LGA 1851 still supports 3 gens on mobile, where power efficiency matters the most.   LGA 1954 will likely support 3+ gens again, like LGA 1851, but with all the products coming to desktop. The first desktop CPUs for it, Nova Lake-S, are definitely coming. The future gens reportedly will feature unified cores, which is good news for desktop. No more p-core/e-core gap.",Positive
AMD,This is the main issue Intel needs to fix for me. Core2 duo was my last Intel they'll need to do what AMD does,Neutral
AMD,There is Bartlett Lake. Same deal as this. Double interesting too that no one calls AM4 a dead end platform when talking about buying it's low end CPUs,Neutral
AMD,"HUB has consistently claimed that new gen 6 cores are always faster than previous gen 8 cores, dont try to twist facts by changing it to ""keeping up"". And in this video we can clearly see that's not the case in BG 3 where a 5700x is slightly faster than a 7600 even though the latter has higher frequency and uses way faster memory with higher bandwidth.",Neutral
AMD,"The people claiming otherwise literally provided 0 evidence from their own hardware or any benchmarks from any reviewer.  ""You're suppose to have 30 tabs open, 4k utube video, rendering a vid, discord, spotify, netflix and benchmark at 720p""  But 14600k is better than 7600/7500 if you're going to keep ur gpu under 4090 for the next 5 years because you're going to be gpu bound.",Negative
AMD,"I thought that was known for a while now, in even some CPU intensive games only utilize up to 4 cores. That's why it was notable when BF6 beta showed capabilities of  actually using all cores when playing.   I'd understand it if people are more afraid of the future when games suddenly release that has that capability, but in the meantime any 6 core CPU with similar spec should be similar in games with higher core CPUs.",Neutral
AMD,"There is 7600X3D available, maybe not in 'murica, but in Europe it's very available. Obviously not $199 cause why would it be the normal 7600 seems to be $185 USD and the launch price was $299 USD a year ago and it's currently ~$307 converted when removing tax here.",Neutral
AMD,"well it is not that far off, but I would call it more like. Manufacturers gimps the model u want so that they can release a cheaper version of it instead of offering us the older model for less several years later because it would cut into the sales of the current gen models.     The binning costs money so I guess it is cheaper to just not trying to bin the old am4 cpus and just release them as the new sku with the new lesser binning.",Neutral
AMD,I really hope that title shows up on dearrow because it's the only accurate title,Positive
AMD,"That is possibly the most unfavorable, disingenuous way to frame this.   If I ran a successful YouTube channel, and my career was to benchmark hardware and then edit the content - i wouldn't use AM4 either. I'd run a 5090 and at minimum a 9950X3D - possibly even Threadripper. But my career requires Office Suite, a web browser, and RDP, so I use a ThinkPad issued by the company. I'm also sure that GN/HUB probably don't have tons of free time to play games because they have to work more than 40 hours a week to hit their current release schedules.   As for ""releasing new defective CPUs"" - that describes pretty much everything that *isn't* a 9950(X3D) or 285K.  A more reasonable take like ""the release of a 5500X3D doesn't mean AM4 isn't dead. It's just a limited release bin of an existing CPU to clear out the remaining inventory that didn't meet spec"" - I'd 100% agree with you.",Negative
AMD,>releasing new defective CPUs to maximize profits.  You say that like it's a bad thing.,Negative
AMD,and still matching Intel latest in games,Neutral
AMD,Release a `Socket 7` Ryzen CPU with massive 3d cache.,Neutral
AMD,lol. ive forgotten about bulldozer. I dont think ive ever seen one in person.,Negative
AMD,Would have saved bulldozer tbh.,Neutral
AMD,opteron 144,Neutral
AMD,On ddr4 speeds??? You cant balance that even with l3 cache,Negative
AMD,"Ehhh, Ryzen 5 5600X3D was Microcenter-exclusive and never became worldwide. If they had the stock to do a worldwide release I imagine they would have.",Neutral
AMD,Maybe Steve needs to be more entertaining so people want to watch the video for more than just the slide at the end.,Neutral
AMD,"It's impressive how Google is acting more and more like a stereotypical monopolistic mega corporation of a cyberpunk dystopian world as of late.  Google search is absolute sheet, YouTube is getting sheetier, side loading (as we know it) is being killed off on Android next year, and Chrome is... well, Chrome.  That's one reason I've migrated to FireFox, switched to DuckDuckGo, and already looking into installing a custom ROM on my Android with zero Google crap.   And YouTube isn't half bad with UBlock + YT Control Panel but... I've digressed enough!",Negative
AMD,"> The whole meme about the XT cpu:s is that it's $50 for letter and nothing else  The 5800XT has gone [on sale for $159](https://www.reddit.com/r/buildapcsales/comments/1lty6e5/cpu_5800xt_included_1tb_mp44l_m2_ssd_159/) as late as last month. That's why. We're not talking about MSRP prices. The 5800XT hasn't been at MSRP price for a long while now.  The last 5700X sale on r/buildapcsales was 9 months ago. So no, the 5700X isn't easily available for cheap anymore, just like the 5700X3D and 5800X3D. Obviously everyone wants the 5800X3D as the endgame AM4 CPU but now they're $450+ even on Aliexpress.",Neutral
AMD,"Ye, for all intents and purposes AM4 died with the 5800X3D launch. That was the last actual meaningful launch.   Intel will still be selling LGA1700 CPUs for years to come. Doesn't mean it isn't a dead platform, unless Bartlett Lake is actually launched for desktop.",Neutral
AMD,"yep, remember that amd was not willing to enable support for the zen3 cpus on older mobos until the very end of the am4 platforms life. That was something that many including I thought was such a f-you to us that bought their platform.",Negative
AMD,"Imo, the big draw is that AMD keeps manufacturing old parts for a long time, so new parts cause pricing to be pushed further down.  This puts pressure on the used market and older parts. It also means that there will be a much larger amount of Zen 3 in the used market relative to anything else. That reduces the ""end of the line"" proce creep that the strongest parts for a socket often have.",Positive
AMD,"Intel even did that a while ago, with their non e Core CPUs which were announced at the beginning of this year. So, basically the same, just a slightly worse version of the existing product. But in there no one cared",Negative
AMD,That is the most negative thing I have heard about a platform having a 10 year longevity. You are good at finding small details in a sea of positive news. lol,Negative
AMD,"> Raptor Lake was a really strong upgrade except for the RMA rate.  Aside from that, Mrs Lincoln, how was the play?   > but i5 Raptor Lake beats i9 Alder Lake in gaming. Very rarely we see such leap within a single gen.   That's pretty normal, if not underwhelming, for a generational upgrade though? The gap between i5 and i9 in gaming is pretty small to begin with. Hell, depending *which* i5 you get, it's literally just the same ADL silicon.  > LGA 1851 still supports 3 gens on mobile, where power efficiency matters the most.  LGA 1851 is a desktop socket. Mobile uses something entirely different. And what 3 gens are you talking about there? It's just MTL and ARL.   > The future gens reportedly will feature unified cores, which is good news for desktop.  That's probably well after this socket's lifespan.",Positive
AMD,"> Very rarely we see such leap within a single gen.   Not really. For Intel that happened with 11->12->13 gens, for AMD non X3D chips Zen+->Zen2->Zen3->Zen4.",Neutral
AMD,"The 9700X is only 0.5% faster than the 9600X on average at gaming  https://www.techpowerup.com/review/amd-ryzen-5-9600x/18.html  >Compared to the Ryzen 7 9700X the 9600X loses by a small 2% at 720p, but it keeps on gaining as resolution increases and beats it at 1440p and 4K by a wafer thin 0.3 and 0.4%. While that's not exactly conclusive, it's strong evidence that gaming performance between those two processors will be virtually identical.   https://www.techpowerup.com/review/amd-ryzen-5-9600x/29.html",Neutral
AMD,"Yeah, I looked into this very closely many years ago when choosing PC parts and found that the 5600X performed nearly the same as the 5700X and even the 5800X in most games. So too did the 5600X3D, 5700X3D and 5800X3D. And while games may have gotten more CPU intensive, the 9600X is still very good against 9700X.",Positive
AMD,"i play with 2500tabs open with a video playing in the background and playing at the same time, even on the hexa core systems and the perf penalty is not at all that bad, pretty much the same perf as an octa core or if I enable the e cores, heck even worse with the e cores on.  But if u render a video or something with the cpu then I guess it gets a tiny bit better on the higher core count cpu, even though it would still make the game unplayable on a higher core count system but for less time than on a hexa core.  People dont understand how it works.",Neutral
AMD,"Even if the main event loop in those few games you play do not take advantage more than a handful of cores, there are lots of games now doing shader compiles.    Some new games (e.g. Sony, Unreal) do that at first run or after new GPU driver install,  while better ones run it at the background (can cause stutters) and some older games do that between levels.  Shader compile is using most of my threads in my 5800X and cut back of the wait time and possible stutters.  There are other things people do that requires CPU cores. e.g. emulation, virtual machines, video encoding.  Silly to buy/build a PC for single use case and restrict yourself.",Neutral
AMD,The 7600X3D launched in America as well.,Neutral
AMD,>Look guys I know I only do reviews where I test CPUs for a few days with like 1-3 motherboards that are rarely updated with their BIOS revisions and are rarely on but let me tell YOU how great AM4 is.  Absolute cinema. Any sane person with a brain would ignore these tech reviewers.,Positive
AMD,It's also matching the AMD latest 9600x according to the graph.,Neutral
AMD,"does it? 5800x3d lost pretty badly in hubs own testing when they tested against an 12900k and faster ddr5 sticks compared to ddr4 sticks it used before.  I loved my 5800x3d and had no issues what so ever with what same on the net would call amd-dip, but my lga 1700 cpus including my 12700k tuned outperformed my 5800x3d which had tuned b-dies so even higher perf that we see here.",Negative
AMD,My first cpu was an amd Athlon 1.53ghz single core.   Then a phenom X 4 9850.   Then a fx 6100. (Bulldozer),Neutral
AMD,"IIRC, there were Excavator CPUs for AM4. The A12-9800 I think.",Neutral
AMD,"Having any L3 at all could probably have helped Bulldozer, yeah.   At a certain point though, the question becomes, if you make a big enough change to the architecture, when does it stop being that architecture and start being a new one?",Neutral
AMD,"Because Infinity Fabric is bottlenecking hard, there is limited benefit from DDR5 for Zen4 and 5. Keeping old Zen3 era I/O die would cost ~10% gaming performance for non X3D chips, even less for X3D.",Negative
AMD,"I've been wondering for a while if Zen4 or higher could be back ported to AM4 by adding a HBM module as an L4 cache, but I'm sure the engineers at AMD already ran the numbers and it wouldn't be cost effective.",Neutral
AMD,"same as 7600x3d, sad times",Neutral
AMD,"If I'm going to pull out my own wallet I don't need entertainment, I want facts and figures  It was websites but now it's skipping around in YouTube videos and tpu reviews   Ltt and gn are both techtubers but hit different markets imo  But yeah I don't daily drive gn and go there when I buy or well major news breaks",Neutral
AMD,"I feel like he missed an opportunity to poach Emily Young out of LTT or hire them after quitting. They're a great presenter who has a similar tone but feels more engaging and speaks at a more measured pace. Look at the videos where Emily was host or the foil to Linus' energy.  Of course, I'm not sure Steve's ever gonna pull from LTT at any point in his career, but it's quite surprising how, despite his and his team's improvements in writing, the hosting stagnated.",Neutral
AMD,You're allowed to say shit.,Negative
AMD,"4.3 petabytes get uploaded to YT *every day*. I don't think Google's actions surrounding ads or YT Premium have been unreasonable considering that metric.   I pay for YouTube premium because its YT is my most used service. Don't mind the price vs the value.   I also dont think YT's stance on minimum watch time to count as a view is unreasonable. YT's algorithm is based around clicks - and as much as people say we hate click bate titles, they work. Thats more to blame on human nature than Google specifically imo.   But I do also think Google as a company sucks. They lack a unified, creative vision. They extract value from the few hits they had years ago while being unable to replicate past success. They dont care for a tight UX and overall attention to detail, and care only about having a ""good enough"" product to extract user data....  But realistically, only a massive company like Google that has other revenue streams based primarily on user data can even make a service like YT viable at all.   Also I played around with custom ROMS a lot back in the day. If I ever reach the point of taking the next step in de-Googling by life, I'd just switch to iPhone.",Neutral
AMD,"I mean there is a reason Google lost three different anti-trust cases in like the last year or so (one over web search, one over their ad network, one suit filed by Epic games over treatment of third party stores); trials right now are going on to determine the penality Google will face, hopefully the DoJ follows through on some of their more hefty proposed remedies like forcing Google to sell off Chrome.",Negative
AMD,Your just being contrarian hopefully that's just youth as irrational contrarianism isn't a good look for an adult.,Negative
AMD,"They've always been like this, the moment they made Chrome people called this shit from a mile away. They make money from ads they need as many people to see ads and all the things you listed can get in the way of that. Fuck, Google that's why I never used chrome.",Negative
AMD,breh your neck beard needs a trim,Neutral
AMD,"The 5700x is pretty cheap here in Brazil right now through amazon, in comparison to the 5500x3d that is costing more making It the x3d not worth it here",Neutral
AMD,Nah the 5700x3d because it was actually affordable.,Neutral
AMD,At least you can still find AM4 boards at retail at reasonable prices now. You cannot say the same about Intel platforms from 2017.,Neutral
AMD,You are forgetting all the 1600X owners that still see any 5XXX series CPU as a great upgrade.,Positive
AMD,"they did, Q1 this year.",Neutral
AMD,"there was a lot of negativity before am4 got obsolete. The biggest thing is that AMD did not want us to run zen3 cpus on older boards which they let us when the platform was end of line.  That was much more negative than the fact that fast ddr4 sticks and the vast amount of am4 boards that once were avaible are not anymore. How many owners of earlier am4 boards were forced to upgrade to a new board when they went with zen3 cpus when they did not really needed that at all.  That was negative, not that some components are rare now.",Negative
AMD,Why are you bringing up averages when HUB literally brought one game?,Neutral
AMD,omg people were down voting you...wth...,Neutral
AMD,"yea, pretty good huh",Positive
AMD,"never owned a bulldozer/piledriver as at that time a phenom was just as good at the start of those fx cpu life and at the end intel was the only way.     But, my first proper pc was an amd k6 200mhz cpu, which was barley faster than my friends 120mhz pentium. But then athlon came out and omg... those were the times.",Neutral
AMD,Isn't Infinity Fabric speed tied to memory speed though? At the same rate or half the rate or something? Or has there been some big change when I wasn't paying as much attention over the past few years?,Neutral
AMD,7600x3d can be bought in Europe,Neutral
AMD,"YouTube was self-sufficient the last time I checked.   Google isn't doing any of this out of the goodness of their hearts.   There's no such thing as free lunch, a simple fact most people seemingly lack the capacity to appreciate.",Negative
AMD,"> But I do also think Google as a company sucks. They lack a unified, creative vision. They extract value from the few hits they had years ago while being unable to replicate past success. They dont care for a tight UX and overall attention to detail, and care only about having a ""good enough"" product to extract user data....  That description made me think of Microsoft. Strikeout Google and replace with Microsoft and it's just as accurate.",Negative
AMD,"Don't forget constantly killing good products and somehow having 7 overlapping calendar or task services.  But yeah, I don't think YouTube could exist as an independent company. Not unless they dropped above 1080p resolutions and compressed everything to hell. It's insane that a free service provides 8K60 HDR upload and playback.",Negative
AMD,it was?,Neutral
AMD,"what no.... they are crap boards and ddr4 are getting more expensive and they are crap sticks as well. no good am4 stuff left, at least when I looked.",Negative
AMD,"So what, so that the 10 people who has a motherboard that breaks long enough after purchases that had they gone Intel it would be EOL can get one?  That is a extremely niche positive. While motherboards do break more often than CPUs. They as all electronics follow the bathtub curve. If it didn't break in the first year of you owning it, it is not likely to break within the reasonable life time of the product either. Baring some glaring issue like the capacitor plague or specific design flaws with that model.  And if you are using that 2017 platform and upgrading it along the way. You will pay a price for it. A 5800X3D will be held back by PCIe 3.0 before it becomes obsolete as a CPU. Do you then get a new board for a dead platform to gain 4.0? Nullifying the main advantage vs going with a newer contemporary Alder Lake platforms or just getting a at the time new 4.0 AM4 board?",Neutral
AMD,"And why is that? Because AMD started at a very low point. The 1000 > 5000 series jump is not normal in that time period. AMD started at similar performance level of Haswell, which was 4 years old at the time.   Reddit is heavily biased towards entusiasts when it comes to PC hardware discussions. People upgrading every other generation is not the norm.    Had you instead bought a 8700K back in 2017, you would still be using a chip that is perfectly passable today. Sure the fastest AM4 has to offer is considerably faster for things like gaming. But it is not nearly the same leap as someone who jumped on Ryzen 1 and is now comparing vs a 5800X3D. The jump from a 8700K to 5800 series is closer to ""normal"" progress in the 5 year period from 2017 until the last major AM4 launch.  Normally, most consumers would see no reason to upgrade in that time frame. If you bought a 7800X3D at launch, by the time that thing is obsolete for a normal consumer and they would consider a upgrade. You would be looking at a whole new platform. Because the last X3D chip released for AM5 will itself be old news by then.",Neutral
AMD,Which CPU are you referring to?,Neutral
AMD,"Most of the criticisms were that the benchmarks were cherry  picked because the video in question showed one game. Therefore, Gamer's Nexus above as well as the 9600X review from TechPowerUp show an average of games which corroborate HUB's claims. This addresses the criticism of ""cherry picking"" data.  In addition, HUB has made countless of those videos over the years and they had an average of the games. There is a clear trend in the data.  https://youtu.be/7L9rPNSuPCA?si=4zAgnP8t2z1OKEJN",Neutral
AMD,They all bought 9800X3D's and play at 4K and can't admit it was a waste of money.  X3D's only make sense if you are playing on a 5090 at 1080p and only play esports titles. For sane gaming it makes zero difference over a 9600X. We are all GPU limited at the resolutions and settings people actually use.,Negative
AMD,"I went with my dad to buy a Compaq in 98 or 99.. it had an amd k5 533 mhz cpu. Lol. He spent 2500 on that pc lmao. It had 128MB SD ram. We had computers before that, like ibms and stuff, but I wasn't into computers then. But yeah, the thuban amd x6 six core cpu i remember was a true 6 core and people liked it more than bulldozer",Neutral
AMD,You can change the mem clock to IF bus ratio around however you'd need to.  It'd be fine.,Neutral
AMD,not worldwide,Neutral
AMD,TIL the world is US and Europe,Neutral
AMD,">But yeah, I don't think YouTube could exist as an independent company.   Yet their business model is somehow self-sufficient...",Neutral
AMD,"I remember it hitting 170 euros in the netherlands, the 5800x3d was never below 300.",Neutral
AMD,It was like $130 for Ali express at one point.,Neutral
AMD,it was selling for 200CAD at one point (~160USD?),Neutral
AMD,"Yeah the dual rank, Samsung b-dies are long gone. Also, there are not much AM4 mobo options at this point except for budget B550 mobos out there.",Negative
AMD,"> A 5800X3D will be held back by PCIe 3.0 before it becomes obsolete as a CPU.  Doubt.  [5090 loses ~8% at 3.0x16](https://www.techpowerup.com/review/nvidia-geforce-rtx-5090-pci-express-scaling/11.html) in the ~worst (eyeballed) tested game.  [5800X3D vs 9800X3D is down 24% in the same game](https://www.techpowerup.com/review/amd-ryzen-7-9800x3d/18.html).  You could quibble about ""held back"" being a different standard than ""obsolete"" (I am typing this on a Haswell!), or construct scenarios with under-VRAMed GPUs in heavy swapping, or an high-queue-depth IO-bound workload.  I think only the last one would be honest, though, and such workloads are rare.",Negative
AMD,My msi x370 carbon has a word for ya https://www.3dmark.com/3dm/120628256,Neutral
AMD,"Intel released the Raptor lake CPUs, this time without e Cores. Just look at their website",Neutral
AMD,Show me that trend on the 9600x HUB review vs 7700 where the former is significantly faster than the latter on average.,Neutral
AMD,"yep, I only play at 1080p/1440p lowish with an high end gpu/cpu. Actually had two 4090 but now I swapped over both of mine desktop system both intel and amd x3d to 9070xt because of how well they perform at lower res in bf6 beta and warzone.",Neutral
AMD,"It was common here in Sweden as well, to get a pc via the job, here the biggest employer was Volvo/Saab and people from Volvo got to buy/lease whatever it was IBM Aptiva machines. A freind got a pention 75 I think and omg what a machine. But then the development started to move in a blistering fast pace. from 486 to pentium at max 120mhz to 1ghz and accelerator cards and then proper 2d/3d video cards. Now it kinda is at a standstill and only the Halo gpus are actually worth the upgrade but they are locked by the mega extreme pay-wall.  yep, those fx cpus were very meh, and even my friend that hates everything that is intel or nvidia used his phenom until am4 came out.",Neutral
AMD,"Not worldwide yes, but not microcenter exclusive",Neutral
AMD,The point was that its not microcenter exclusive unlike 5600x3d,Neutral
AMD,"For whatever reason mine was going to be 140 but they had it even cheaper with klarna, So I got mine out the door at 123, then I sold my 5600X for 80 bucks   All in all, a $40 upgrade, which is insane",Positive
AMD,"Grats on losing performance on a $1500 graphics card I guess? And before you say ""it's only marginal"". The thing with PCIe is that it does not hit every title evenly.  Rather most are not affected at all while some titles are [severely](https://tpucdn.com/review/nvidia-geforce-rtx-4090-pci-express-scaling/images/metro-exodus-3840-2160.png) impacted.",Negative
AMD,"Are you talking about Bartlett Lake? So far, at least, those are just lesser versions of the 8+16 RPL die. Maybe some ADL mixed in there as well. Also not available at retail.",Neutral
AMD,You can see it in the TechPowerUp benchmarks.  https://www.techpowerup.com/review/amd-ryzen-5-9600x/18.html  A 5600X also consistently outperforms the 3700X.,Neutral
AMD,"You missed the crucial ""never became worldwide"" part of that comment.",Neutral
AMD,"So you call a 12% loss at the very worst severe although it still goes beyond my 4k144 monitor ? Okay, I'll keep that in mind and regret my purchase in silence. 👋",Negative
AMD,"Yes, that’s what I’m talking about.  And no, they are available in retail",Neutral
AMD,Zen 2 is 4 cores per CCX while Zen 3 is 8 cores per CCX. That means the benefits of more cores were nulled by die-to-die latency. All the 6 cores on 5600X exists on one CCX while 3700X is 2x4 cores.,Neutral
AMD,"Techpowerup is a garbage source for cpu benchmark results because they benchmark on low settings instead of max that are more cpu demanding and also hiding % lows metrics per game. Even taking into account those results, 3% difference can barely be called a difference in the first place, way off the mark of significantly faster and more like leaning into flop gen territory.",Negative
AMD,"i didnt miss anything.    I never stated that its worldwide available. I only pointed out that its not microcenter exclusive, so i dont understand why are you trying to correct me so much...",Negative
AMD,"> And no, they are available in retail  Where?",Neutral
AMD,"Very likely that the doubled effective L3 cache size on Zen 3 is the cause of the difference, not CCX-to-CCX (two CCXes are on a single die) latency.",Neutral
AMD,"nah man, it is not that simple. I have not found that many games that actually impact the cpu the higher settings u chose. not in say like bf and wz and the like.   Some games like say gta5 and cp2077 have settings that affect the cpu by increasing the crowd/ai/object population but that issue is not here in mp fps games.",Negative
AMD,Pretty much everywhere. Just search for them.,Neutral
AMD,"I'm pretty sure Ryzen 5700 (5700G without iGPU) which has less L3 performs better than 3700X too, in gaming at least.",Positive
AMD,RT/PT require more cpu power aside other settings like crowd density.,Neutral
AMD,> AMD can confidently ship products across multiple nodes and product families without having to worry about over- or undercapacity at a single fab  Do you remember the chip shortage just a few years ago?  https://en.wikipedia.org/wiki/2020%E2%80%932023_global_chip_shortage  My company is fabless.  You can tell TSMC we want 10 million of this chip and 20 million of that on these dates.  That doesn't mean you are going to get the quantity on the date you want.,Neutral
AMD,">AMD can confidently ship products across multiple nodes and product families without having to worry about over- or undercapacity at a single fab.  They cant. This is why AMD had both CPU and GPU underproduction this year unable to fulfil demand for either. They simply didnt buy enough capacity in advance, which needs to be done years before you actually want the chips.",Negative
AMD,"Only Alder Lake and Raptor Lake at Intel are tied to their process node; Intel's later products - Arrow Lake and Lunar Lake - and those that come later, aren't tied to any specific node.  Intel 7 is fully amortized and will likely be functional only to meet obligations for Alder Lake and Raptor Lake over their entire product life cycles.  Intel 3 is being used for their Xeons, and the Ireland facility that makes them recently announced upgrades for additional capacity. Intel 3 will also be used for the smaller iGPU tile in Panther Lake.  Panther Lake will be on 18A, future Xeons will be on 18A, and Nova Lake will also have 18A as well. Their upcoming E-core Xeon - Clearwater Forest - will also have base tiles for cache and interconnects made on Intel 3.  Seems flexible enough to me.  AMD's situation with TSMC isn't as advantageous as you think. For example they always deprioritize GPUs in favour of CPUs when both of them use the same node. RDNA4 is the most clear-cut example of this.",Neutral
AMD,"Non existent.  That was a thing about a decade ago.  First of all, Intel can still produce in different nodes.  Intel had the problem that they developed both architectures and process nodes together which both had advantages and disadvantages.  You can’t just switch out the process node for an architecture, every architecture is designed for some nodes specifications. The more you focus on one node the better you can use its advantages which is one of the reasons why Intel was typically superior with their chips. But that also comes with drawbacks like Intels 10nm issues.  Intel didn’t stay with basically Kaby lake for years because they wanted to but because they couldn’t switch to ice lake without the 10nm node. And that’s the reason why Intel fell back on the desktop and servers. If you look at their roadmaps, what became Tiger lake H was supposed to come out as Ice lake in like 2018. But with Rocket Lake they backported Ice lake to 14nm which worked more or less well, depending on who you ask. Since then, Intel developed their Architectures with no specific node in mind which makes them as flexible as any fabless company.   I’d say if anything it’s a huge advantage to have your own fabs today because you can determine how much volume you want for yourself.  TSMC is basically a monopoly at that point which also means that all three, NVIDIA, AMD and Intel compete over capacities and that drives up cost.  And then there is Apple. Apple is like a first class customer of TSMC (for good reasons) and they’re the ones to decide how much they leave for the rest.",Neutral
AMD,When Node Development was not a money hog it was good idea to be IDM but as Nodes got expensive and more difficult people started to drop and brings to our present condition of only 3 companies with fabs.  Now for Advantage of fab is margin stacking if you do it correctly you can get  > Margin Stacking   > Volume and you control your supply chain not other companies.  > Custom Process tightly coupled with design allowing you to bend design rules a bit.  Cons > Without enough volume it's a net loss to run fabs. >  Very risky and if you fail you end up like Intel.  Fabs less > Lower risk > Outsourced manufacturing means low Capex requirements. Cons > You are at the mercy of your partner if he screws up your product screws up as well. > IP Issue if your outsourced fab is also a competitor.,Neutral
AMD,"> How big of an advantage is fabless flexibility in practice?  Huge. Not being tied to a fab's success or failure, not being forced to make a direct investment in that yourself, not having to worry about maximizing utilization of that fab, it all adds up. You can see this playing out with Intel real time. They've written down billion of dollars worth of equipment for their newer nodes because of low utilization, and their old nodes are so expensive to manufacture it's a major reason their foundry is so deeply in the red.",Neutral
AMD,It cuts the other way as well. A independent Intel foundry would have it a lot easier attracting customers.   Right now a customer wouldn't know if it isn't always a second priority to whatever intel deems necessary to their own products.   Also there is the question of exposing your roadmap to a company you are directly competing with on products.  A Intel with both design and foundry is only attractive to those who want monopolistic advantages. If a presumably better node were on intels hands it would only serve them and their products.,Neutral
AMD,"Hello! It looks like this might be a question or a request for help that violates [our rules](http://www.reddit.com/r/hardware/about/rules) on /r/hardware. If your post is about a computer build or tech support, please delete this post and resubmit it to /r/buildapc or /r/techsupport. If not please click report on this comment and the moderators will take a look. Thanks!  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/hardware) if you have any questions or concerns.*",Neutral
AMD,">How big of an advantage is fabless flexibility in practice?    This is kind of bagging the question. That there is an advantage or even a choice here.   If you don't have 10 billion to invest in fabs you are a fabless. To basically everyone.   There is no flexibility on leading nodes in this industry. You get TSMC one of the few alternatives.   I imagine TSMCs customers pay what TSMC tells them to, only constrained by the reality they can't get too crazy lest they help support Intel when the customers revolt.   This is not a real thing in practice. Design teams are targeting a node whatever it is.   The cost to switch or support multiple nodes is what it is too. You can switch or hedge different options but there are costs.",Neutral
AMD,"I have a couple of follow-up questions since I’m not an expert on nodes:  * When you say **Intel 7 is fully amortized**, what does that really mean in this context? I get that fabs are expensive and equipment gets “paid off” over time, but since these machines are tightly programmed and occupy massive amounts of space, wouldn’t they still represent opportunity cost if they’re just kept alive to meet obligations? * Also, on the point about products being **“tied” to a node** — could you expand on what that practically means? For example, AMD has shown flexibility in the past, like when they re-lithographed Polaris (RX 580 → RX 590 on 12nm). Would that kind of move count as being “not node tied”? * And for Intel specifically, how should we think about this? Are their designs today still bound in ways that limit shifting across nodes, or is the tile-based approach (like with iGPU/cache tiles on Intel 3 vs. cores on 18A) essentially giving them AMD-like flexibility?  Would love to hear more of your perspective on that.",Neutral
AMD,It's a huge risk to have an in-house fab that doesn't get any outside business. 2 points of failure on design and fab side that stop the whole thing.  Intel got stagnant on the design side and then stumbled on the fab side (which causes more churn on the design side) to end up in the huge mess they are in today.,Neutral
AMD,"So basically with IDM you gain flexibility in design rules and stronger IP security, but you carry all the risk. If you have Intel-level missteps — like missing the mobile wave while PC demand was already declining — the fab overhead turns into billions in write-downs each quarter.  What I find interesting is TSMC’s strategy. They’ve always stuck to the principle of never competing with their customers, unlike Samsung, which builds both its own chips and fabs. That seems to be a big part of why everyone from Apple to AMD has enough trust to let TSMC handle their lithography.  Do you think TSMC’s “pure-play” stance is really the genius move here — the key reason they’ve become the dominant foundry while so many others dropped out?",Neutral
AMD,How behind performance wise are Samsung/GF and SMIC for non-flagship node compared to equvialent nodes like TSMC's N7P/N7+ and N5? As not every company need N2/N3.,Neutral
AMD,"ADL/RPL were designed specifically for Intel 7. Intel 7 was design with ADL/RPL in mind. The two are closely related. It would require a lot of work to bring ADL/RPL to a different node.   One of the main improvements that ARL/LNL brought was internal design changes that make Intel designs in line with industry standards, using industry standard design tools vs previous designs being custom-made for specific internal nodes. This doesnt matter (directly) to the customer, but it does help Intel's engineering efforts easier going forward.    And while yes there's an opportunity cost to continuing to use Intel 7, a lot of Intel 7's cost structure was paying off the massive NRE costs due to its multi-year delay. These costs were fully depreciated H2 of last year, making Intel 7 much more cost effective.   But besides that, fabs need different types of nodes at different price points. The lithograph equipment used for Intel 7 can't just be reporposed for new nodes which use EUV. Its either use it on Intel 7 (or 16) or let it sit unused. Bartlett Lake exists almost entirely just for this reason.",Neutral
AMD,"1. That is basically what it means. As for tooling and machinery, some of them that were intended for the fab expansions that are now either cancelled or have been slowed down, have been repurposed as upgrades for existing manufacturing, so expect those to pay themselves off as well in the future. 2. Since Arrow Lake and Lunar Lake, Intel's core IP, at least on the CPU side, is ""99% process node agnostic"", to quote one of the lead engineers who worked on the Lion Cove P-core that goes into Arrow Lake and Lunar Lake. What it means that they don't have to design with a specific process node in mind, and their workflows and design rules reflect that.  3. Basically the choice of node boils down to performance-power-area - which are intrinsic to the nodes themselves, and cost of wafers, time-to-market, and volume availability. The economic aspect as characterized by the latter, can sometimes be more important in determining which product uses which node. For example, Nova Lake, the desktop successor to Arrow Lake, will use both 18A and some advanced TSMC node for the compute tiles.",Neutral
AMD,TSMC is not unfailiable no one is it's just they are executing better currently if Intel hadn't shot itself so many times in the foot TSMC wouldn't be so dominant. There Is only one rules in semi manufacturing the best node(PPA and yield) wins that's it.,Neutral
AMD,"These are questions with nuanced answers: better in terms of what metric? Power, area, durability, defects?   Most of people could tell you would be under their NDA and other legal restrictions to talk about it.   Your best source of information for something like this would be the Asianometry YouTube channel or similar aggregators because they are probably not bound the same way.    That said they get their information through press releases, announcements, papers, inferring, educated guesses and leaks.",Neutral
AMD,"Exactly, TSMCs last node, N3, was essentially an L. It took a while (and some alterations) for it to really get to healthy yields and performance. If they had competition, there absolutely could’ve been some customers willing to switch. IF. Instead, Intel was on fire and Samsung was busy huffing glue, so the status quo remained.",Neutral
AMD,"Thanks for the constructive reply! Yeah, I’m more of a layman in this space, so I really appreciate the sources you mentioned and I fully understand the NDA limits around these discussions. I’ve always been curious about how much smaller nodes really matter in practice. For example, I use four different 7 nm-class chips across my devices: my desktop (Zen 3) and tablet (Kirin 990 4G) are on TSMC N7, my phone (Kirin 9010) is on SMIC’s 7 nm equivalent, and my laptop runs on Intel 7. I’ve often wondered about the differences between these “same-node” designs, both in terms of marketing vs. actual performance. And looking at newer phones, even the latest 3 nm chips don’t feel like they bring Moore’s Law-level leaps compared to what 5 nm delivered five years ago.  Would love to hear any more insights into how meaningful these node shifts really are in everyday use!",Positive
AMD,Many of the big players never touched N3 and are planning to go straight from N4 to N2. If thats not a failure of N3 i dont know what is.,Neutral
AMD,Intel went to N3 cause Intel fumbled and looked like TSMC caught Intel dealy flu for 3nm lol,Neutral
AMD,"It becomes very specific to the design, any performance changes and the limits of parallelism.   Adding more cores to a single threaded workload will not improve performance.   Adding a larger cache may. Density should help there.   Memories can really benefit from a die shrink because in theory you can have the same design with minor updates just smaller. Then you can have more dies, or arrays of memory cells.   Designs that have multiple dies also have special trade offs in that area. It gets very technical.   Thermal considerations also are really part of this. Many devices are now thermally limited so whatever process reduces heat or helps extract that heat to the cooling design has an impact.   But some designs/workloads are not as effected by this.   And this is not even going into and talking about just improving design because not surprisingly a lot of the performance is made or lost based on the design itself.",Neutral
AMD,"So what you’re saying is that this really becomes a *system-level project*? We can’t judge node progress linearly by one parameter alone — instead, it’s about balancing across a wider set of trade-offs depending on the design goals and the target users. That makes sense, and I guess strong project management across the whole chain is pretty essential for success (Intel’s history being a good example here).  I’m currently studying computer engineering at university and I’d really like to get into the IC design field. Do you have any suggestions on where to start learning more about these trade-offs and the broader design/management aspects? Thanks a lot!",Neutral
AMD,"Lol 5500/4090 is a wild combo. If you don’t want to switch sockets, try to find a used 5700X3D or 5800X3D.",Neutral
AMD,"That might be the craziest GPU/CPU combo I have seen.  As far as AM4 CPUs go, that is the fastest one you can get for gaming and should keep up with the 4090 in just about any game out in 1440p and above.  Depending on how much the 5800x3d costs, it may be a better option to look into AM5 CPUs that would have more longevity. You would need a new board and RAM but generally it is better value to move to AM5 instead of spending $250+ on a good AM4 x3d CPU right now.",Positive
AMD,Get AM5 honestly,Neutral
AMD,"The 5800X3D was discontinued like a year ago.  There is also the 5700X3D, but it was discontinued about a month ago.  See if you can find either of them used for a decent price.",Neutral
AMD,"A 5**6**00 or 5600X is already about a 15% performance lift over the 5500.  A 5800X3D is about 60% faster than the 5500, but is *insanely* priced at the moment, going for nearly $700. The easier to find 5700X3D is 50% faster than the 5500, but still goes for $350 retail at the moment. Though you can find it for around $260 used on eBay.  For around $430 you can get a 7600X, decent B650 motherboard, 32GB of DDR5 6000 CL30, and a cooler which is roughly on par with the 5700X3D, and with another $20 more the 9600X roughly matches the 5800X3D.  [https://pcpartpicker.com/list/CrhWpK](https://pcpartpicker.com/list/CrhWpK)  You can also get a used 7500F off of eBay for around $130, bringing the total cost down below $400. If you can fit an ATX motherboard in your case, the Gigabyte B650 Gaming X AX is also $129 right now, cutting another $20 off that price. Meaning you'd be looking at around $370 for a full AM5 swap with performance roughly on par with the 5700X3D.  I should note the 7500F is only *slightly* slower than the 7600X. Not the same extreme difference as the 5500 and 5600X. The Ryzen 5 5500 is AMD's mobile version of the Zen 3 CPU with the iGPU fused off, where as the 5600 and higher are the desktop Zen 3 with significantly more cache and higher clock speeds. The Ryzen 5 7500F is a 7600 with the iGPU fused off and 100Mhz lower clock speed.",Neutral
AMD,"it would be a nice boost .. if your motherboard supports it, id be sure, and id also check if the board supports pcie4. if it doesnt id probably just get an budget am5 setup cpu/board/ram, and even a budget am5 will outperform a x3d on a 450 board ..",Positive
AMD,"I bought my step-daughter a prebuilt 5500 with a 3060 two years ago. Her father stole money she had been saving for a gaming pc. At that time, that was all I could swing. I’ve since replaced the 5500 with a 5800X3D and swapped out the 3060 with a 7800XT. It is like a completely different machine. But honestly, with prices where they are on the 5800x3d I would just put in a AM5 board and a 7800x3d.",Neutral
AMD,"5700x3d/5800x3d roughly matches 7500f/7600 performance levels. Unfortunately, I dont see those AM4 chips under $260-$300 used very much while the 7500f can be had for as little as $160 on Aliexpress. At this point, I would go straight to AM5 instead of upgrading AM4. Throw in about $100 for an mATX motherboard and $80 for 32gb DDR5, finally sell your old cpu/mobo/RAM for about $120 and honestly you may come out ahead on a current platform with faster memory.",Negative
AMD,"it makes no sense to buy a 5X00x3d anymore since you're gonna pay $300 minimum and it is probably preowned. at that point another $100-200 gives you an am5 mobo/ddr5/cpu.  also, with a 4090 you're not playing at 1080p like the cpu-bound benchmark videos.  just get a $160 ryzen 5800xt instead: [amazon link](https://www.amazon.com/AMD-RyzenTM-5800XT-16-Thread-Processor/dp/B0D6NNDQ92)",Negative
AMD,Being discontinued? The 5800X3D production has been discontinued since 2024. You can't get a good retail price 5800X3D now.,Negative
AMD,Honestly you really should upgrade to AM5 and just get a modern CPU and RAM for that GPU,Neutral
AMD,"I had this dilemma too, upgrading from a 5500, I ultimately decided to just get a 5070ti then wait a few months and fully upgrade to AM5 cause GPU was the first priority for me",Neutral
AMD,"What is your budget? A 7600x bundle will cost about the same as a 5800x3d so I'd just go AM5. If you have the budget, you could go even faster, like a 9700x or 7800x3d. The 4090 is a beastly card. What resolution is your monitor and what games do you play?",Neutral
AMD,"If you buy a new 5800x3d it will be the same cost as a new 7600x3d mobo+cpu+ram combo from microcenter. Gets your foot in the door to the next socket, a good gaming cpu, and 32gb of ram.",Neutral
AMD,Honestly I’d sell the 4090 and get a new setup with a 4080 lol. How much do you actually need the 4090?,Neutral
AMD,i think people who haven't looked at prices in a while assume they cost $200 or $250. but it's like $300 now for any am4 x3d.,Neutral
AMD,i know i got my pc prebuilt as a gift 2 years ago and im just know doing research on it 💔,Neutral
AMD,So crazy,Neutral
AMD,"costs is way more, cpu+mobo+memory vs just a cpu.",Neutral
AMD,I bought one at micro center last week..139$,Neutral
AMD,This is the one !!,Neutral
AMD,would getting a combo mobo cpu ram on newegg also be a good option?,Positive
AMD,i don’t really have a budget if i need to i’ll get it eventually. I have a 1440p monitor right now and i mainly play fps games like marvel rivals and overwatch.,Neutral
AMD,"I’m not really sure. This pc was a prebuilt gift, and i’m just now realizing how weird my gpu/CPU combo is. I’m trying to “fix it” so my fps in games are better and i’m not bottlenecking my cpu",Negative
AMD,You now I understand that!,Neutral
AMD,"With how insane the 5700X3D and especially 5800X3D pricing has gotten, it's not.",Negative
AMD,5700X3D is way too overpriced,Neutral
AMD,Unfortunately the AM4 x3d chips are way too expensive rn. He'll have the ability to sell his AM4 stuff and make some money back too. AM5 just makes too much sense in this market,Negative
AMD,"It could be. Its not always a huge savings vs buying everything on its own, and Newegg limits the combo options. I prefer to use PC part picker, sort price low to high, and buy the highest rated option with the features I want. Heres an example of what I found: [https://pcpartpicker.com/list/VzKkKX](https://pcpartpicker.com/list/VzKkKX)  I made a newegg combo thats very similar but with the 7600 instead of 7600x. The Newegg combo saved about $15 but for a slightly slower clocked CPU.",Neutral
AMD,"A 7600x will be fine in most games, especially at 1440p 144hz but as a fellow fps player I'd get a higher tier for better 1% lows (especially in rivals). A 9700x or 7/9800x3d chip will give you a smoother experience. If you have a 240hz panel then you should definitely get the faster chips.",Positive
AMD,"Nah don’t sell the 4090 they are getting up in value as the time goes by, if you want to save money you can buy some used parts, you could even begin with a 7600x and a decent b650, and later on upgrade to a 7800x3d, also AliExpress has really good deals on cpus, I bought a R7 7700 for 165",Neutral
AMD,"yes as other commentator said, do not sell 4090, it is a very powerful card and it will last very long",Positive
AMD,yeah it’s avg 350-400 right now.,Neutral
AMD,but you don't need to buy those overpriced cpus.,Negative
AMD,£210 here.,Neutral
AMD,"For $400 just go to AM5, AM4 is a dead socket and is 100% not worth spending that kind of money to get more performance out of it.   For $500 you can get a used 7800x3d with a mobo and ram that would beat the 5800x3d in everything while being on the newer platform.",Negative
AMD,"$400 was the price I paid for a 9700x/b650/32gb ram bundle I bought. Not worth getting the AM4 CPUs at those prices, unfortunately.",Negative
AMD,"The comparison is because the performance between the entry AM5 CPUs is roughly on par with the 5\*00X3D CPUs.  I mentioned below that the Ryzen 5600 is already about 15% faster than the Ryzen 5500. The Ryzen 5800XT is maybe the fastest non-X3D AM4 CPU for gaming, but it's 4% faster at best than the Ryzen 5600. Looking at AM4 CPU pricing right now, I'd actually suggest the Ryzen 5700 since it's got 8 cores and slightly higher base clock speeds, and is only $10 more than the 5600. A few weeks ago the 5800XT was on sale for the same price as the 5600, which would have been a great buy. But at it's current price isn't worth the premium.",Neutral
AMD,Then what is he upgrading to?,Neutral
AMD,"on amazon, the 5600 is $140 and the 5800xt is $160.  so if you're sensitive about a $20 diff (hey, get the 5600 then), then surely a full am5 upgrade is not on the table since it's going to be over $500 to swap everything.  i think your point is to just not upgrade at all since it's so marginal (unless you pay big for the x3d) and instead plan on am5 being your next upgrade when you need it.",Neutral
AMD,"nothing. unless they have a specific perf target and the 5600 is too slow somehow, what are they upgrading for.",Negative
AMD,But he wants to upgrade so your comment is not helpfull,Negative
AMD,"everyone wants to upgrade because it's mostly consumerism. their options are pay big for am4 x3d or pay even bigger for am5.  their question has been answered like 20 times, so now we can move on to the commentary about the answers people provided. got it?",Neutral
AMD,"Nope, not helpful.",Neutral
AMD,is your feedback helpful?,Neutral
AMD,Waste of money on a PCIe 5.0 SSD.  The build will work fine though.  Is your key OEM or retail?,Neutral
AMD,"Is the speed difference that insignificant between PCIe 4.0 vs 5.0?  OEM. It was Windows 7 originaly, which got the free update to 10 treatment.",Neutral
AMD,Next gen is looking like a larger jump in performance as there is an actual node shrink. The 7600 is still a decent gpu so if you can wait that would be great,Positive
AMD,"First, answer the following:   - Are you dissatisfied with your gaming performance?   If you aren't, there's no need to upgrade. There's no point in upgrading just simply for the sake of upgrading. The performance difference between the two cards is not that big.   If you are dissatisfied, then just go ahead!  The 9060xt 16gb is an an amazing card for it's price, and if you need the extra VRAM just go for it!   Hopefully this answers your question, and if you have more questions, I'm here to help!",Neutral
AMD,by waiting are you expecting more GPUs to come out next year?,Neutral
AMD,"As someone else said, can it do what  you want it to do? If yes, you don't need to upgrade",Neutral
AMD,"The 9060xt 16gb is pretty great. It looks to be about 50% faster than the 7600 at 1080p with an even larger difference at 1440p. I bought one recently and have been playing around with it (XFX swift triple fan). I think I will sell it on rather than replace my 7900xt, but it is fast and efficient, and FSR4 is legitimately great.   There will be other cards available for sure in a year or so. Nvidia 50 series super cards are coming out in early 2026. Maybe AMD will have udna/rdna5 cards out by the end of next year, time frames there aren't really clear on that yet.  Will any of those offerings be a better value than current offerings? Not easy to say. Maybe, maybe not. The gains from 7600 to 9060xt are there, but no one can really tell you whether that's ""worth it"" to you now vs waiting to see what's coming",Positive
AMD,"Never wait. Buy now when you can get a deal. GPUs keep going up in price, and it has been this way since the mining boom in 2020. What price is the 9060? What price can you get a 9070 non xt for? Those are 2 great choices",Positive
AMD,I'm a a bit dissatisfied at the moment. Got a 3440x1440 display from a friend and having to play games at low or 30fps to handle the screen is a bit disappointing.  Do you consider the performance difference small enough to say playing games will only get marginally better?,Negative
AMD,Well expecting new GPUs to come out for better performance at the price range or have better prices in the current GPUs that are currently at a higher price range.,Positive
AMD,"Its about 50% faster, so you you'd get like 45 fps instead. But remember it also has fsr 4 which is waaaay better then fsr 3",Positive
AMD,"After some double checking I found that it is indeed quite a big jump, as the other comment said. You'll be getting 40-50% more performance, 8gb of VRAM, and FSR4, which could double your fps depending on the title.   I would consider the purchase to be worth it. You might get better value by waiting, but there's no need to spend multiple months at a diminished gaming experience for that.   By the way, maybe try turning down some settings? Depending on which ones, they can have a negligible difference in viewing experience while boosting your fps by tons. I apologise in advance if you're already playing at low settings.",Positive
AMD,gotcha. maybe some good deals when black Friday comes around.,Positive
AMD,45fps doesn't sound that nice tbn. I'll research the fsr4 just to know how much of a difference it is. Thank you for the comment,Negative
AMD,"Unless I am playing some competitive title, framegen is also amazing",Positive
AMD,"Im currently lowering the settings of games as much as I can to keep the GPU around 90%.  I mostly get low-mid graphics and cap to 60fps, on some games cap to 30fps. That is on the 3440x1440 monitor. Im thinking of switching to the other 1080 monitor I got for gaming but I mean it feels kinda disappointing switching monitors just for gaming.  I think I'll get the GPU maybe sell the current one and hope for no regrets.",Neutral
AMD,Fsr4 is ever so slightly better then dlss3 which is great. It would get you above 60fps easy. But yeah sounds like you might need a bit more to not rely on upscaling so much,Positive
AMD,"In your situation I would probably just buy the 9060xt. It's an amazing card for its price, and waiting a year on a bad experience to probably save only around $50 is simply not worth it.",Positive
AMD,"As a workstation?  The case is a hotbox and expensive for it. There are a ton of good modern cases   Nvidia may be better for workstation software, don't know what you're using it for though",Neutral
AMD,https://youtu.be/ImZeedA_wAo?si=p7WQnK8964jl-Bg3  They trade blows depending on the game. Toss a coin or go for the less expensive.,Neutral
AMD,9700x. we are already seeing older 6 cores like the 5600x getting maxed out in borderlands 4,Neutral
AMD,9700x for sure,Neutral
AMD,"yea I watched that video before, I hate the fact that there is no clear winner",Negative
AMD,"Yep, x3d for gaming and 7700/9700 or better for work, streaming, etc.",Positive
AMD,Yeah haha I chose my CPU based on the games I play.,Neutral
AMD,What does 'work' mean in this context?,Neutral
AMD,"Video editing, 3D modeling, rendering, large data analysis, etc.   Basically non-gaming tasks that benefit from additional cores.",Neutral
AMD,"alright, yea i dont need to do those tasks normaly. The X3D seems like the better choice then",Neutral
AMD,Ensure the monitor is plugged directly into the GPU and not the motherboard.,Neutral
AMD,90% chance monitor plugged into the motherboard,Neutral
AMD,Is your monitor plugged into your graphics card or mobo? Did you install the amd drivers for your 9060xt? Did you install the chipset drivers your motherboard?,Neutral
AMD,"Dude, a word of advice for life.   If you gonna ask for help,  provide details.",Neutral
AMD,"This isn’t enough information.  What have you done, what are you seeing - **look at other troubleshooting posts and update your original post.** here are some ideas you can rub your neurons together over:  - Is the monitor plugged into the graphics card (GPU), not the motherboard? - are the GPU’s PCIe power cable fully plugged in? Are all cables seated firmly? - Is the GPU detected in BIOS? Is the BIOS up to date? - Is Windows fully updated? - Is the latest AMD GPU driver installed from AMD’s website (not just Windows Update)? - Did you install the AMD chipset drivers for the B650 motherboard? - are there Any unnecessary apps chewing CPU/GPU  that could be hampering troubleshooting efforts? (Discord overlay, browser tabs, recording software, etc.)? - Check CPU/GPU temps while gaming with something like HWInfo. - Is the GPU running at expected speed and power consumption under load, or stuck at very low speeds? - what about the CPU? - have you tried a cpu and GPU benchmark like cinebench or furmark, respectively? Are scores in line with those from other users with the same processor and GPU, respectively? -  What resolution are you running the games at (1080p, 1440p, 4K)? - what about graphics settings for those games? - Is XMP/EXPO profile enabled in BIOS so the DDR5 actually runs at 6000 MHz? - Is the game installed on an SSD or HDD?  This is not an exhaustive list, but please check and try these things and report back. It’s more valuable in some sense to determine what the issue ***isn’t***.  And if you just delete your post instead of updating with the solution that is bad vibes.",Neutral
AMD,"Not a peep from the OP, I suspect the worst ladies and gentlemen, it was plugged into the motherboard",Negative
AMD,"made the post, first comment told him to plug his monitor to the GPU and not the mobo, OP hasn't said anything since out of shame",Neutral
AMD,"Just wanted to say thank you all for the replies. I missed a few drivers, tweaked my AMD settings and connected my monitor to my fucking graphics card LMAOOOOOO. I knew it was gonna be something stupid. I appreciate all of your help, my wife is very happy now 😊",Negative
AMD,"Did you peel the sticker off of your cpu cooler?   Apart from that, which setting and resolution are you running?",Neutral
AMD,"Could you please elaborate on ""struggles to run""?  What games, resolution, and quality settings?  What FPS are you getting?   Did you compare the GPU's performance to benchmarks with the same games to see if it's actually under-performing?",Neutral
AMD,You definitely plugged your monitor into the motherboard and not the GPU,Neutral
AMD,Check bios and make sure resizable bar is on. There's a few settings on this card that you need to play around with.,Neutral
AMD,Your not using your gpu are you?,Neutral
AMD,Yeah something is wrong. I’m crushing 1440 with a 9600x 9060xt,Negative
AMD,post the picture of where you have plugged in the monitor cable. we'll tell you if its the gpu problem or a connection mistake.,Neutral
AMD,"Check your monitor is actually connected to GPU.   How much thermal paste did you put on the CPU?  Did you remove the plastic cover of the CPU cooler?  Do you have enough case fans to get fresh air?  If its struggling to run most games, there's probably something pretty big that wasn't done right. How hot are the CPU and GPU running?",Neutral
AMD,You need to configure your windows to run the monitor in 144hz instead the 60hz,Neutral
AMD,Guy makes a post asking for help and replies to nobody after an hour,Neutral
AMD,What is the GPU utilization? Maybe run RTSS and tell us.,Neutral
AMD,Did you change your monitor's refresh rate?,Neutral
AMD,"Just wanna throw this out there - ignore any comments mentioning enabling expo or resizable bar. Literally 0% either of those things is causing your pc to run rust at 30fps.   Sure this stuff is something to look at after you’ve figured out what’s causing the issue, but changing a bunch of stuff at once is how you end up having no idea how to fix this the next time it happens.",Negative
AMD,"CPU and GPU can run most anything really well. Check ram, drivers, cooling of the CPU, and that you are plugged into the GPU not the mobo.",Positive
AMD,"Bro, I literally just bought those parts yesterday. Still waiting for them to get here",Neutral
AMD,"Ah, the good side of Reddit",Positive
AMD,I would have gotten an NVIDIA GPU instead. You can generally avoid a lot of frustration by using the best parts and device drivers for your build.,Positive
AMD,Why did you go with the Ryzen 5 7600? That might be your bottle neck right there…,Neutral
AMD,Thank you so much. I can’t believe I overlooked this lmaooooooo. YOU 🫵😁 are amazing,Positive
AMD,The answer always,Neutral
AMD,Classic,Neutral
AMD,That's a BINGO!,Neutral
AMD,Chipset drivers are hella important. I had to reinstall windows on a laptop that the drive died on and wondered why games were running like absolute ass. Turns out there was a bunch of chipset updates stuck in windows “optional” updates that it hadn’t installed yet. Completed those and suddenly it’s back to normal.,Negative
AMD,"Did he enable expo? (Yes I know that is not going to be the major issue, but it does help)  Also what resolution, at 4k a 9060XT is not going to be a high flyer.",Neutral
AMD,The classic blunder,Neutral
AMD,"Glad your problem is solved, happy gaming!",Positive
AMD,lol sorry. Was making the tweaks,Neutral
AMD,Not really that uncommon.,Negative
AMD,"A Ryzen 7600 is not going to bottleneck a 9060XT, something else is going on.",Neutral
AMD,"Lol That cpu can drive almost any gpu on the market. I maybe wouldn't run it with a 5090, but still. It trades blows with the 5800x3d which was the fastest gaming cpu only a few years ago.",Positive
AMD,"We've all been there.   Now, make sure your monitor refresh rate isn't at the default of 60 in Windows Display Settings too! (Assuming you bought a high refresh rate monitor)",Neutral
AMD,"Easy miss when your excited to get a new build up and running, you'll never do it again.  We never learn unless we make mistakes ... even simple ones 👍  Enjoy your new rig 😁",Positive
AMD,Hell yeah enjoy gaming!,Positive
AMD,"One of us,  One of us,  happened to most of us",Neutral
AMD,"Immediately knew this was the problem. On the plus side, you’ll never need to ask Reddit this question again! 😆",Negative
AMD,Now make sure your Windows display settings are at your monitors refresh rate. Might be defaulted to 60 still.,Neutral
AMD,"My first pc build, I didn't even know you needed a gpu to get a picture on the screen.  I was still waiting on mine but had everything else built.  My cpu didn't have an igpu (didn't even know it was a thing) and could not figure out what was happening.  So yeah, we've all been there and no shame in it!",Neutral
AMD,Dude that processor sucks… everything is good but that processor,Negative
AMD,Worst message I've ever seen containing bOtTlEnEcK word. I'd bet that he's using userbenshitmark as a reference...,Negative
AMD,"Good call on checking the refresh rate. Also, make sure your graphics drivers are up to date   that can make a difference too.",Positive
AMD,"Silly question, if you prefer 60 and dont see a need to play higher fps, is there a downside at just leaving it there?",Neutral
AMD,Did you base your wrong statements on userbenchmarks.com,Negative
AMD,The 7600 doesn't suck. It's a great CPU for the price. Pairs just fine with a 9060xt 16GB.,Positive
AMD,"7600 is not X3D chip, but it’s not bad by any means, enough to run anything modern well",Positive
AMD,the 7600 is fine lmao,Positive
AMD,"The performance of the 7600 is practically the same as the 5800X3D in games, just look at the results there. The 7600 is definitely not the problem in this case. This CPU easily pushes a 9060XT.",Positive
AMD,Lol,Neutral
AMD,"I'm not even one of those people who think bottleneck is a bad word.(it is often misunderstood/misused though) The comment just knows nothing about the performance of that cpu though, and doesn't realize that even weaker cpus are still quite capable.",Negative
AMD,"And your BIOS, unless you like getting access violation crashes in some games thanks to a Windows update.",Negative
AMD,A higher refresh rate makes doing basic tasks look MUCH smoother.,Neutral
AMD,60hz is for blind people - try 75 at least or 120/144. Fully different experience.,Neutral
AMD,"Yeah, it heavily depends on games and resolution/settings but generally speaking any GPU below the 5090 are just fine for 1440p/4k with this CPU. Many people still pair a 5090 with their 5800x3d and it does the job for many games while in other games it holds back the 5090 to a noticeable extent.",Positive
AMD,"Is there a downside to setting your monitor refresh rate high, but locking a game to like 60 or something? I dont know anything about computers, but I swear i've heard there can be some stutter problems or something when it comes with framerates and hz on monitors.",Negative
AMD,I mean if you're REALLY concerned about power consuption...   I don't have specific numbers but it shouldn't make too big of a jump.,Neutral
AMD,"No, you can lock in game fps to whatever you like, though I don't see why you would do that if your PC is powerful enough to keep a stable let's say 144fps. But, for example, if you play a single player game on 4k, then capping at 60fps is recommended because of frame rate stability.",Neutral
AMD,Alright thanks!,Positive
AMD,"Depends.... If you tell us your budget and your country, we can cook up a full parts list for ya!",Neutral
AMD,For those games it’s a good combo,Positive
AMD,10GB is a lucky lot,Neutral
AMD,That will be plenty,Positive
AMD,My budget is about 1400-1500€ and i'm from Finland. I'm planning to get these parts and buy the graphics card used.  [https://fi.pcpartpicker.com/list/JgQjzP](https://fi.pcpartpicker.com/list/JgQjzP),Neutral
AMD,For gaming performance the 7800X3D is the best out of all of those - but a 7600 will work just fine.,Positive
AMD,"7600X or 9600X if its just a bit more expensive for gaming. 7500F is also a good, budget option you can consider.",Neutral
AMD,Since you mentioned AutoCAD use id go 9700x. But if it’s MOSTLY gaming then 7800x3d will help with 1% lows in any game.,Neutral
AMD,"normally id say 9700x but the price between 7800x3d and 9700x are just 20 dollars apart, what country are you from??",Neutral
AMD,X3D,Neutral
AMD,I go either 7600X or 7800X3D if you can fit it in the budget.,Neutral
AMD,why not 7700 or 7700X?,Neutral
AMD,"If you have already bought the motherboard (and can’t potentially return it), then your decision is mostly already made: if it’s AM5 socket, then the 7800X3D of the choices you mention. If it’s Intel LGA1851, then the 245kf is the only one on your list that will work. If it’s got a different socket than those, you’ll need a CPU different than what’s on your list.",Neutral
AMD,"Between those options, the 9700x is the best all-around CPU for the price imo.    It excells nowhere but it's pretty good everywhere. From what I've seen it performs 15% less on average in gaming compared to a 9800x3d, depending on the game, the settings  and the resolution.   But if you already bought the motherboard, check its CPU compatibility list since the 245kf won't fit on a mobo the rest of your options would.",Positive
AMD,"7800x3d will work best  9700x isd my second choice.     I wouldn't go below that...*Borderlands 4* requires an 8 core cpu minimum, and won't be the last",Positive
AMD,"Ive tested amd and intel processors except core ultra, based on what ive seen either get intel 14700k or 7800x3d if you want amd, amd non 3ds are 25% slower at gaming.",Negative
AMD,"They dont sell it in my country, thats why.",Negative
AMD,"No, still no mobo, but they are cheaper, the only thing that matter is the cpu.",Neutral
AMD,"Still no mobo an yeah i know that the ultra goes to the lga 1851 and not am5, but the component that conditions the build is the cpu since he still doestn have one.  Ty anyways.",Neutral
AMD,"i see, with a 9070xt, id say a 7800x3d",Neutral
AMD,"Heh, be sure there are motherboards that are more expensive than some CPUs. But since you don’t have one yet, go for the 7800X3D (or even better: 9800X3D) CPU.",Neutral
AMD,"The 9800x3d is a scam in my country since is literally double the price of the 7800x3d and about the mobo the gigabyte aorus ice wifi is the one he is going to, after chosing  the cpu he is going to buy the one with the fitting socket, mostly the am5 variant.",Negative
AMD,i need new cpu because in dota2 12v12 i experience some lag and fps drop to 60 or lower,Negative
AMD,"14600K has same number of P cores as 12400f tho. Do you really think the extra e cores will solve the lag? Have you looked at the Task Manager while this lag is happening? Be sure to right click graph and show all logical CPU cores.   You might throw Task Manager up on second display so you can see it while gaming. You might also try playing with only one display connected. Sometimes a graphics card might struggle to run two displays simultaneously in some situations when the refresh rates are dramatically different. Not sure whether that's the case here, but if you haven't tried it, you won't know for certain.  If it is somehow GPU quirk, you may solve it by running one display off the iGPU but you are using F version without iGPU, so that is not option for you. You should also check whether game is set to full screen or full screen windowed because that might have impact, too. And update your AMD drivers. All the basics.  I think you should do a bit more testing before you conclude you need to throw cash at it. 14600K is a great value with discount tho.",Neutral
AMD,"If you can get a 12700K or 12900K really cheaply (used?) then should help a bit, but otherwise, yeah, go AMD Zen 4 or later, ideally the 9800X3D, it will be a hefty jump in CPU performance for you.",Neutral
AMD,"14600K is a solid upgrade not that hard to cool, especially while gaming. I’d go with that personally.",Positive
AMD,"I tried everything, my last gou is 3070ti and i change gpu theres no difference in playing cs2 and dota2. No increase fps. So ineed new cpu",Negative
AMD,14600k almost same price and with free bf6 game,Neutral
AMD,"Intel 13th and 14th gen have degradation issues, it's on a dead platform, where the 9800X3D is much better and has upgradeability. Also, BF6 only counts if you were going to spend money in it anyway. If you don't care about that game then the game has zero value.  No, getting LGA1700 is a bad idea, except where you can get a really good deal as a temporary thing, and are going to upgrade later to Zen 6 or Intel Nova Lake in a couple years anyway.",Negative
AMD,"Older stuff always gets more expensive for a little while as it gets long in the tooth and stops being produced. This is true of many things. Just look at the 5700X3D.  Eventually it will disappear entirely, then show up on the used market, seriously discounted, like 5 years later.",Negative
AMD,"4x RAID 0 with HDDs? You're joking right? 0 data retention if anything happens to even one of the 4 drives. Either go with RAID 10 or get some SSDs, how much write speed do you need?",Negative
AMD,Thanks! Got afraid that there was something up with the 9995wx and to get the 7995wx. which didnt make sense as on paper pretty much the same with higher turbo clocks and recently released on the newer 4nm,Negative
AMD,thanks edited it a bit lol yea raid 0 for the dream but tbh dont need to raid 0 at all. dont need data retention as once files are on the machine its immediately transferred via 10gig to a storage array that has retention this is more just a workhorse type of pc with a large storage.  its a direct capture machine for imax size films that goes up to.. be a few TB.,Neutral
AMD,"RAID0 for speed, backups for failure if you don’t need high availability.",Neutral
AMD,What makes you think they're hdds?,Neutral
AMD,"I mean, I'm not an expert. This is just a general trend. I suppose there could be some niche feature of the 7 series that was deprecated with 9. But... I doubt it.  Perhaps compatibility? Are 7/9 series not compatible with the same motherboards (I thought they were)?  Like, there's a reason people can still get ANY money at all for i7s from 10 years ago. People are silly and just want an i7 to upgrade their system, even though a modern i3 would demolish it.",Negative
AMD,"10 gig fiber's theoretical max speed is 1.25 GB/s. Most NVME SSDs are capable of delivering that. Still, I'd be considering RAID 10 if you really want 4 drives. What happens if you lose power during one of the transfers for example? And one of the drives decides to die because of a surge or something?",Neutral
AMD,"Even if they're SSDs, RAID 0 is not a good choice.",Negative
AMD,4x 4 TB HP Z Turbo Drive M.2 TLC SSD   lol will need some good warranty,Positive
AMD,yea sadly HP site only allows RAID 1 or 0 pretty poop for such a high end workstation. Maybe i'll just raid 1 take a slight write speed hit. as you said incase something.  raid 10 would be amazing though. ill reach out to HP to see why they dont offer raid 10.  would you recommend win 11 storage spaces two way mirror? its not true raid 10,Negative
AMD,"Ehh.  Don't know the use case so hard to say. If they're backed up, im fine with it.",Neutral
AMD,"You might get overall better performance overall out of fewer, higher quality/enterprise grade ssds.  What's the use case?",Positive
AMD,"Try to avoid any type of software RAID, they're usually a big PITA when something goes wrong. Try to do it over the mobo or get a specific RAID card.",Negative
AMD,writing and transferring a large amount of data direct capture to the workstation full uncompressed ffv1,Neutral
AMD,"that is like the absolute opposite from what ive heard from everyone in the last like 10 years  with hardware raid you you are boned if something  breakd and you can't have exactly the same hardware again ot even some config/array info on the local storage of the hardware, while with software you can just plug the same drives into anything running that software and import",Negative
AMD,would get a better mainboard and 32gb ram. pc is fine for these games but will\ probably get too expensive with the upgrades.  for a few bucks more it should looks like this:\ https://pcpartpicker.com/list/vTmMh7,Neutral
AMD,how much would you be paying for the RX 7600 gpu,Neutral
AMD,Okay i definitely should have said it was a prebuilt pc my bad 😕,Negative
AMD,"Don't know about the price, but this will run the games you want without breaking a sweat. I think it might be overkill, but if you can spend that money, it's a good build yes.",Positive
AMD,I’d look at the prebuilts since they start 1-1.2k and are currently doing a few gens up on CPU and GPU for roughly the same price. You will get more for your buck,Neutral
AMD,What's the price and what are you using it for?,Neutral
AMD,"The CS2241 and the NV3 use QLC flash, which has low durability and becomes noticeably slower after a few years. Get a better drive with TLC flash like the Kioxia Exceria Plus G3, WD Blue SN580/SN5000, TeamGroup MP44L/G50, Patriot P400/VP4300 Lite, or Klevv CRAS C910.",Neutral
AMD,overall price is like 1.5k ill mostly use it for gaming,Neutral
AMD,No deal if that's 1.5k USD,Neutral
AMD,u mean no deal by performance or no deal by overprice,Neutral
AMD,"Both, for 1.5k you should get better specs. These specs should cost less, especially gaming performance wise",Neutral
AMD,"well i live in belarus so specs are way more expensive here , but if u could choose , which specs would u take?",Neutral
AMD,"Idk the Belarus market, maybe it's a good deal there. I'd recommend finding a community of Belarussian PC builders who know local parts prices and availability",Positive
AMD,thanks for the help,Positive
AMD,"I would swap out the cooler for a thermalright 240mm AIO to save like $40 and put that towards a 2tb SSD. Games are regularly over 100gb now and 1tb will fill up crazy fast.   Other than that, nice build :)",Positive
AMD,"Thanks! 😊   Just to clarify — that build isn’t for me, it’s one I built for my boyfriend about 6 months ago. I included it just to show what kind of hardware I'm familiar with.  I'm actually asking for help with a **new build for myself**, and trying to figure out what the best options are within my **€1.000–€1.100 budget (excluding the case)**.  Appreciate the advice though — I’ll definitely keep your SSD tip in mind for my own setup!",Positive
AMD,"Ah I gotcha, I missed that blurb at the top.   [https://pcpartpicker.com/user/hawk7117/saved/XhkTzy](https://pcpartpicker.com/user/hawk7117/saved/XhkTzy)  This comes in at about $1050 and is a pure value beast :)",Positive
AMD,"Ah, thanks for sharing! I’ll take a closer look — looks like a really solid value build. Appreciate you sharing it!",Positive
AMD,"He will go to 95 degrees by default, Watch optimus tech video of ryzen 7000 PBO and your temps will decrease a  lot.",Neutral
AMD,"How hot does it get when you're using the PC normally, like gaming or whatever?",Neutral
AMD,Did you take the plastic layer covering the surface of the sink off? This is 80% correct answer.,Neutral
AMD,Are you sure you removed the protective clear plastic from the aio's heatsink face while installing it?  IS the aio pump and fans running and set on a curve to increase speed with CPU temp?   Is it properly installed with thermal paste evenly covering the whole CPU? Double check you used the correct mounting hardware and installed it according to the instructions.,Neutral
AMD,Normal behavior if that's during full load. Check performance as well to see if that's normal.  EDIT: [For the ignorant](https://www.reddit.com/r/Amd/comments/xqsa2j/95c_ryzen_7000_is_not_the_whole_story_here_is/),Neutral
AMD,thank you,Positive
AMD,"I didn’t play any games yet but when i checked while watching youtube today, it was around 50C",Neutral
AMD,Yeah i did.,Neutral
AMD,yes and yes and yes,Neutral
AMD,"And those are idle/work temps? If idle, it cant be anything else than faulty installation. If stress, check you case cooling,",Negative
AMD,"95c seems high, especially for a 7600X and a 360mm aio, if it is in-fact properly installed and fan curves are set.  What temps are you getting with a normal load that isn't a benchmark, such as gaming?",Neutral
AMD,"X3d Chips are different due to how they PBO (if at all). Every Ryzen 7000 I've owned has done this, it's not exactly throttling because in HWInfo it's not throwing that error. Instead, it just PBO's to max, usually by default, and stays there.",Neutral
AMD,no thats under full load,Neutral
AMD,65 average i’d say,Neutral
AMD,Then there's something wrong with the installation.,Negative
AMD,"You aren't compromising performance because you're already going over your spec for the CPU. That's what PBO is. It's literally an auto overclock, you're not losing performance for overclocking less.",Neutral
AMD,It's usually on automatically... [Read this](https://www.reddit.com/r/Amd/comments/xqsa2j/95c_ryzen_7000_is_not_the_whole_story_here_is/) before you reply for the love of God it's embarrassing,Positive
AMD,This is like when people put the wrong badges on their car lol,Negative
AMD,Holy shit its the ryzen 4070,Negative
AMD,2000w version?,Neutral
AMD,There's literally an article about this,Neutral
AMD,Lisa Su and Jensen Huang are cousins...,Neutral
AMD,The holy Ryzen RTX AI 5090 X Ti,Neutral
AMD,Is Radeon or GeForce?,Neutral
AMD,You and the Radeon guy should swap,Neutral
AMD,Someone who works in the factory that makes them is laughing their asses off right now.,Positive
AMD,The GPU was manufactured with the wrong silicone and it helps it's dysphoria.,Negative
AMD,Is this a Ryzen 3070?,Neutral
AMD,Coé 👋,Neutral
AMD,By this point we are going to get these posts once in two months,Neutral
AMD,another one??,Neutral
AMD,stop reposting,Neutral
AMD,I am getting flashbacks...  https://preview.redd.it/8d98c7yykdqf1.jpeg?width=1200&format=pjpg&auto=webp&s=b0a70a46c3a3308965b152a97595e717ddc53b0e,Neutral
AMD,"Reminds me of Audi or Seat cars using the same parts as Volkswagen. So if you go to an Audi dealer the market up price for parts is insane, compared to buying the same parts for VW or Seat cars.  Same for Lamborghini, some of their cars have same parts as Audi too.",Neutral
AMD,close to 3000w,Neutral
AMD,"No its definitly radeon, theres no 16pinout",Neutral
AMD,both 😎 /s,Neutral
AMD,It's my 9070 xt that came from the box with a GeForce side panel.,Neutral
AMD,"A ""unharmful mistake"" lmao",Neutral
AMD,"eae meu fi, roubei kkkk claro q ia dar créditos né po, tmj",Neutral
AMD,"pardon me, I am not that active in this community 🤷",Neutral
AMD,wdym? never saw anyone posting this atrocious thing here lol,Negative
AMD,Lots of luxury brands are like this. Buick and Cadillac use Chevy parts. Lincolns are mechanically Fords. A Lexus ES350 is a Toyota Avalon. An Acura Integra is a Honda Civic. Infiniti is just Nissan. Land Rover and Jaguar use loads of Ford parts.,Neutral
AMD,Yeah they don't just share parts but whole platforms.  Under the exterior and interiors they are all the same.  I have a golf with a few 'upgrades' from audi.  It's all the same.,Neutral
AMD,probably,Neutral
AMD,That is no longer accurate. Certain amd models now also use that 12vhpwr connector,Neutral
AMD,I have this model Asus 9070 xt mine says Radeon on the side,Neutral
AMD,u legit said credits.....,Neutral
AMD,Lamborghinis also sometimes use parts from ford,Neutral
AMD,Only the sapphire nitro iirc,Neutral
AMD,"i said *here*, if you go into his profile he never posted that here lmao  and he claims to be the owner too. not my fault bruh",Neutral
AMD,ASRock Taichi as well,Neutral
AMD,"Thanks for the ""do not buy"" list.",Negative
AMD,"So first AMD locks their fancy new upscaling to 9000 cards only, and then releases new 7000 cards?   Maybe it'll work if it's cheap, but given AMD's track record of abysmal pricing on release, I doubt this.",Negative
AMD,Another source link  [https://www.pcgamesn.com/amd/radeon-rx-7700-guide](https://www.pcgamesn.com/amd/radeon-rx-7700-guide)    https://preview.redd.it/3vwighnwg6qf1.png?width=1920&format=png&auto=webp&s=7c59554bf451feb3545eb355d282789948f87a84  [https://asrock.com/Graphics-Card/AMD/Radeon%20RX%207700%20Challenger%2016GB/index.asp](https://asrock.com/Graphics-Card/AMD/Radeon%20RX%207700%20Challenger%2016GB/index.asp)     Direct link to the card     If this is cheaper that the 9060 then it would be a good choice for a budget gamer its that rdna3 that hurts this card so its up to the price. Why get it if the 9060xt is close to the same price that is what i am saying.      Time will tell but very cool amd tossed us a new AM4 chip and now a new gpu.,Neutral
AMD,"Not that it makes it any better, but it's not really any different to what NVIDIA did with the GTX 1650 and 1660. If anything I would take this as a sign that FSR4 will be coming to RDNA3.",Neutral
AMD,your motherboard's VRMs are thermal throttling  your screenshot in hwinfo  Thermal Throttling (PROCHOT EXT) usually points to VRMs,Neutral
AMD,"same thing was happening to my ryzen 5 5600  but it wasnt from start of pc, it would happen randomly (like once every month) and restart was fixing that. i never fixed that permanently",Negative
AMD,"VRMs overheating, buying motherboard with heatsink on VRMs important even on entry level CPUs somehow",Neutral
AMD,Does it happen when you're in games/under load? Its normal behaviour for the CPU to clock lower when its not doing anything. And check with something like MSI afteerburner mid game not by alt tabing to task manager,Neutral
AMD,"Yeah i agree. He could put some heat sinks on VRMs and check if this fix it.  He should look for ""PCH and VRM MOS"" in hwinfo to be sure  EDIT. I check and sadly not always there is hwinfo info about vrm sadly :/",Neutral
AMD,Can repair shop fix VRM issue?,Neutral
AMD,It's locked right from the boot. And it stays locked always,Neutral
AMD,some lower end boards do not have a temp sensor on the vrms sadly,Negative
AMD,Have you checked your bios settings? Maybe something is messed up there?,Neutral
AMD,Everything is default there. Do i need to tweak anything?,Neutral
AMD,Try locking the frequency there. And disable turbo/auto boost,Neutral
AMD,Couldn't find auto boost or turbo. The frequency was set to auto. I set it to 3700mhz and still the issue wasn't solved,Neutral
AMD,"Some people report fixing the issue by turning off fast boot in bios, maybe that?",Neutral
AMD,I hope it didn’t cost you more than 1200$,Neutral
AMD,"finally, a ""fist pc"" post that is very modest and not completely over the top",Positive
AMD,Congrats! I recommend getting some more RAM if you find on sale to bring up to 32 gigs but other than that its peak EDIT: yeah i posted this at night I dumb,Positive
AMD,Exactly $1200. On sale from $1500 on New Egg.,Neutral
AMD,"lol, Right? It's nice to see someone keeping it real without breaking the bank! Solid build.",Positive
AMD,It stated he has 32gb alredy,Neutral
AMD,"Thanks. Yeah, that’s eventually the plan, but I’ll probably wait until Black Friday.",Positive
AMD,I misread the vram as ram at first too lol,Neutral
AMD,Fair price for the system! Have fun,Neutral
AMD,"I was going to buy the 9070xt originally, but had some personal problems get in the way. At least the 9060 xt was still within the budget.",Neutral
AMD,"Yeah bro I'm an idiot I thought the VRAM was your ram. I posted this a little late so excuse me lol. As for upgrading your RAM the more you can afford the better. I personally have 64 GB of DDR4 RAM, and i think it's overkill. Tbh you don't have to buy RAM, you have enough already.",Negative
AMD,Oh it's VRAM? Time to change that comment,Neutral
AMD,"Thanks! Basically want to do some light gaming and not really worried about any AAA games. Have a Series S and PS5, but figured might as well get the PC for some older PC games that I'd like to play again.",Positive
AMD,For the pre build I wouldn’t pay more than,Neutral
AMD,I love the way sapphire makes their designs,Positive
AMD,That GPU is THICC.,Neutral
AMD,"as much as I dislike amd gpu, can't deny they have the best looking cards on the market with this new design.",Positive
AMD,"Amazing, but I’m still disappointed at AMD for still not bringing 9070xt at msrp. I can wait more. My 6800xt is still strong at 1440p gaming. I will never pay a gpu over msrp",Negative
AMD,Congrats!,Neutral
AMD,Thats a very pretty GPU you got there :) how much of a performance boost do you get over your RTX3080?  I have a tuf 3080 10gb and plan to upgrade in the bwar future,Positive
AMD,what was your 2007 amd build?,Neutral
AMD,Sapphire makes the best gpus design period sucks they don't make Nvidia  Congratulations on your build happy gaming,Positive
AMD,I have 9060xt version of it its great,Positive
AMD,TEAM red,Neutral
AMD,amd A true competitor to nvidia,Neutral
AMD,"I would have gotten an RTX 5070 Ti or RTX 5080 instead, but hey, whatever floats your boat.",Neutral
AMD,Same I have the pulse 9070xt and it looks really good,Positive
AMD,"Yeah that card is super slick, I actually like it a bit more than the TUF model I have.",Positive
AMD,That plus montech century psu and that $40 antec case would go really well together,Positive
AMD,Yeah but 50c temps and 3300 clocks I’m seeing . Seems strong !,Neutral
AMD,Its a literal brick!,Neutral
AMD,"AMD makes great GPUs, I have one and it works perfectly for 1440p. I also like Nvidia GPUs a lot, the ""problem"" (it's not a real problem) with AMD are the fanboys.",Positive
AMD,I picked this up for £610. Not sure how that tracks with msrp in the UK though !,Neutral
AMD,About 50% in what I’ve played so far and benchmarks,Neutral
AMD,Im guessing by all AMD they meant ATi pre AMD ownership. Im in the same boat. Had an X2 and a 9800 pro last time I was non intel/Nvidia. That was a great build. Still think the 9800 pro was my favorite card for at the time performance leap.,Positive
AMD,I cross shopped a 5070ti but this thing beats it on perf per pound   5080 was a consideration but 300£ more… not sure,Neutral
AMD,Why? 9070xt is good for 1440p gaming.,Positive
AMD,"If I were to ever go the AMD route, it would definitely be with a sapphire card.",Positive
AMD,i like turning on pt and mfg too much and i dont wanna fuck around with optiscaler just for my upscaler either. not to mention if i went with my original plan of a 5080 (ended up with a 5070ti instead due to no stock) there was no amd equiv,Negative
AMD,"The “AMD driver timeout occurred would you like to send report” and “AMD recovered from a black screen and is running in safe mode” error messages have been the worst moments of having the 7900 XTX for me here, unfortunately (yes I’ve done all solutions on the internet).  When it works it’s satisfying having high frames and a cooler PC, when it doesn’t, it makes me want to throw it out the window and shove the GTX 1080 back in lol.",Negative
AMD,"The problem from AMD's perspective, imo, is that AAA games have predominantly been designed for nvidia since GeForce came out. If AMD put a lot of money behind a game development platform (like Unreal Engine level) then they'd compete.",Neutral
AMD,"At least in the US, original MSRP for the baseline models like the Sapphire Pulse / Powercolor Reaper / XFX SWFT was $600 (£445.27).  I waited in line at a Microcenter for a few hours before they opened on launch day to get one at that price.  It was only a few models for that low, the rest were a lot higher; I think the Nitro+ you have was $730 on launch (£541.74).  But after the first wave sold out, it was revealed that the original price was heavily subsidized by AMD paying the retailers to support it and they went up to $700+ minimum for all models.",Neutral
AMD,"There's really only two reasons left to get nvidia gpus over amd, thats CUDA cores/NVENC encoding (if you do a lot of video editing), and having a better adoption rate for dlss over fsr but AMD does seem to be doing better on that front with fsr4.  Reality is if you want to do anything with content creation your life is going to be a LOT easier on Nvidia and its by design as they worked with software devs to ensure they adopted their tech and became the industry standard. In other words, AMD doesn't rly have a chance in that area, and it sucks.",Positive
AMD,9070 XT is GREAT for 1440P and is usually cheaper than 5070Ti.,Positive
AMD,"You made the right choice, if you want raytracing and much raw power, Nvidia is the way to go.",Positive
AMD,"Does it mean, since AMD is not competing with Nvidia, that AMD is a bad choice for AAA games specifically?",Negative
AMD,"Nah not really, some games run better on Nvidia, others run better on AMD.i don't think you can go wrong either way, but for productivity Nvidia is miles better.  As I said, the only thing that I don't like abt AMD are the fanboys, it's full of them here😅",Positive
AMD,"What I mean by that is for AAA games and AI, AMD is more of a secondary/alternative to NVIDIA. Not good or bad.",Neutral
AMD,"Oh, haven’t encountered much of them around here. What are they like?",Neutral
AMD,for that price it's very solid,Positive
AMD,Why's the GPU so warm with nothing open,Neutral
AMD,https://preview.redd.it/4lovsg012eqf1.jpeg?width=5712&format=pjpg&auto=webp&s=f4ad261f26c5bd873b7c8912775f6815c2067455  Even got a Fortnite dub lol,Neutral
AMD,Single fan GPU and theirs only an exhaust fan in the case nothing else,Neutral
AMD,Off to a great start!,Positive
AMD,38° C is really a perfectly fine temp in idle,Positive
AMD,bro cable management pls,Neutral
AMD,"thats a AM4 rig, great job nonetheless.",Positive
AMD,yeah I used the ramrod method haha!,Neutral
AMD,Thanks!,Positive
AMD,"Yes. Nothing insane but its modern entry level hardware, can play anything you want",Positive
AMD,One thing about white is you'll know when it's dirty. Looks good!,Positive
AMD,https://preview.redd.it/vmsynlbfq6qf1.jpeg?width=1906&format=pjpg&auto=webp&s=5d286a7dd2d9873506320a7a08fb69b8623084fb,Neutral
AMD,Love the white and purple combo! Lighting is also nice! I would rate it 9½ /10,Positive
AMD,Is that keyboard really convenient for gaming?,Neutral
AMD,"Nice but the stats where are the stats ? nice ice white, cool monitor stats!  Is this a show room piece?  Looks cool AF !",Positive
AMD,"haha, true 😄😄 thanks very much",Positive
AMD,thanks 🙏🏻,Positive
AMD,"for me, yes. its the dark project one 87 fuji. i have it for a year now, so far so good :))",Positive
AMD,"not sure what you mean by stats, but it’s not a show room piece, i have built it myself 😁 if you want the components list , it’s in description:)",Neutral
AMD,The exact reverse happened to someone else on this sub a few months ago. https://www.reddit.com/r/pcmasterrace/s/1LHwAacScF,Neutral
AMD,"Another one.  It’s an issue that sometimes happens with the asus tuf cards where the labeling is switched around during manufacturing, because the shroud is shared.",Neutral
AMD,Now we wait for the brother card; a Geforce 9070,Neutral
AMD,"So I've heard that Asus, MSI and Gigabyte tend to re-use some of their Nvidia designs for their AMD cards. I guess this confirms it.",Neutral
AMD,I have no idea but that prob rare if its not fake,Negative
AMD,"Rare misprint :D  The cooler design is probably very similar, and for the various plastic shroud parts, identical. Someone made a booboo at the factory and used ""Radeon"" shroud when the line was building ""GeForce"".  Cosmetic issue. I'm sure ASUS will swap it to a proper one if you complain, but not sure if worth the effort.",Neutral
AMD,"Ah look no warranty is void haha - ASUS, probably.",Neutral
AMD,I think during manufacturing the amd gpu case got accidentally switched with the Nividia gpu case. There is nothing wrong with the card this is just an incredibly rare occurrence,Negative
AMD,The Ryzen 4070 lives on,Neutral
AMD,Oh no not again…  ![gif](giphy|2zUn8hAwJwG4abiS0p|downsized),Negative
AMD,"It has to be done: ""Ryzen 5070"" gpu of the century.",Neutral
AMD,If I were you I would still email customer support. Just to check whether this switchup can have any e.g. thermal or mechanical disadvantages and avoid any issue with the support in the future.,Neutral
AMD,AMD Radeon RTX 5070,Neutral
AMD,Why does this always happen on the xx70 cards,Neutral
AMD,Jensen absolutely losing his mind if he sees this.,Neutral
AMD,It has happened again. A SIGN!,Neutral
AMD,A second one???,Neutral
AMD,That's funny. You have a frankenstein build.,Neutral
AMD,I would proud you have the only known 5070 amd Radeon collaboration,Neutral
AMD,That’s awesome! Keep that misprint! You got something unique and valuable.,Positive
AMD,Every day we draw closer to the Ryzen 4070,Positive
AMD,"Out there, there's a TUF 9070 that has the RTX 5070's cooler shroud lol",Neutral
AMD,I say jackpot,Neutral
AMD,What's the pc case?,Neutral
AMD,Finally the Radeon RTX 5070 is here 🥳,Positive
AMD,I think that's awesome. How many of those do you think there are in the world? Less than 100 for sure. You have a rare and unique GPU,Positive
AMD,Another Radeon RTX has struck the subreddit,Neutral
AMD,"Wow… the product quality team of ASUS is a joke at this point. While hilarious that this has happen to two people “so far,” I think I will be abstaining from this brand for a while.",Negative
AMD,"Oh no, that poor 5070 is now eternally cursed. :(",Negative
AMD,https://preview.redd.it/kaervbmj29qf1.png?width=780&format=png&auto=webp&s=1315d79cc8a57d7bd086f6e45625e7d625ba1ce5,Neutral
AMD,This is why I use reddit!! Hillarious!,Neutral
AMD,Maybe these got somehow swapped in the factory and nobody noticed because they're functionally identical,Neutral
AMD,"I can't send links from other subreddits, but the same happened to me, just check out my profile.",Neutral
AMD,I wonder how much this card would go for collectors.,Neutral
AMD,They should swap back plates to fully hide their cards XD  Tho idk if the back plates will allow it,Neutral
AMD,Keeping the labeling consistent would be pretty tuf in that scenario,Neutral
AMD,Reverse already happened 3 months ago https://www.reddit.com/r/pcmasterrace/s/nTiNI1zGnl  @ u/Fantastic-Ad8410,Neutral
AMD,"Its nothing new pr surprising,  it saves money",Neutral
AMD,Doesn't seem to be fake. Shows up as an RTX 5070 in dxdiag and GPU-Z.,Neutral
AMD,"It would save a lot of headaches if you would want to sell it on an online marketplace later, because no matter how you put it in the description, people won't believe it.",Negative
AMD,"It's literally just the writing on it. The part itself (the shroud) is physically identical between several GPUs to cut down on manufacturing costs. This was a production fuck-up where whoever was responsible for supplying the shrouds for final assembly wasn't paying attention to the exact part number. These are assembled by people, but the ones who actually assemble them aren't paid enough to give a fuck if someone from another department screwed up on the job.  Still, should've been caught during quality control.  It's worth contacting them just to whine about it and maybe (unlikely) get a discount for future purchases as an apology.",Negative
AMD,"Someone in the factory probably misread and ordered the 9070 instead of 5070, the same thing has already happened with a 9070xt and 5070ti",Negative
AMD,JONSBO Z20. Cool case but a pain to build in.,Positive
AMD,This warms my cold old heart,Neutral
AMD,Nature is healing,Neutral
AMD,"Yeah, but I feel like they could be a little more strix about these things.",Neutral
AMD,You're not u/Ok_Conversation_2734,Negative
AMD,Nothing wrong with the equivalent Radeon cards tbh,Negative
AMD,"No that's the first case, a pair for it was already found lol. This is another one.",Neutral
AMD,whats the performance on it? And gpus can still appear as a 5070 in those but be like a rtx 3060. Im pretty sure and where did you get it? I think its fake as its 2 whole different companies,Negative
AMD,You can hack the VBIOS to make a GTX 750Ti say it is a RTX 5070.,Neutral
AMD,"Nah this happened a few times now, if anything it might make it more valuable as a collector item…",Neutral
AMD,"True.   Probably trivial to swap that shroud part yourself, so maybe OP should ask if ASUS can send the correct one without having to swap the whole card.",Neutral
AMD,"Yeah, all of this seems a bit asus to me. Are they trying to hide sth?",Negative
AMD,Retarded people can’t feel,Neutral
AMD,asus shares the shroud used so this has happened sometimes,Neutral
AMD,I ran through some tests on 3D Mark and games like Cyberpunk and Stalker 2. In 3D Mark it showed scores similar to other 5070s. Game performance was also similar to some youtube benchmarks I watched. Looks pretty legit.,Positive
AMD,"I would totally love to have a RTX GPU that said ""Radeon"" so people don't know I'm a total w\*ore.",Positive
AMD,"This is asus though, they will probably void the warranty letting them do it themselves",Neutral
AMD,I feel like you're going to have a hard day when you realize you're AI. I hope your journey to self discovery is fruitful.,Positive
AMD,My PC doesn't have a white or yellow hood. Am I doing something wrong?,Negative
AMD,That's a car,Neutral
AMD,/u/bot-sleuth-bot,Neutral
AMD,Maybe a ROG employee made the switch on purpose,Neutral
AMD,"Some can, just means a different part of their brain got stunted. This one tho, the whole brain",Neutral
AMD,ur right i forgot,Neutral
AMD,prob a misprint then,Neutral
AMD,And that's a bot,Neutral
AMD,I heard Jensen Huang is the Prime suspect.,Neutral
AMD,"Yeah, I figured. Why is it getting upvotes though?",Neutral
AMD,Its MAXMIUS performance,Neutral
AMD,More bots,Neutral
AMD,"Your choice and maybe a good one for your needs, but I was just confused when I saw the pic and the biggest thing on it is an enormous Nvidea GPU box.  Did you look into the AMD options and what was your deciding factor in going with an Nvidea card or that specific one?  Last I saw on Jz2c he was rating the RX 9070 XT over Nvidea alternatives in a similar price range, but I've not seen any performance comparisons other than just now checking UserB*******k where that and your card of choice seem quite even.",Neutral
AMD,"You have finally chooses the superior Side. Welcome.   Here we dont:  \-Force Mobo Upgrade every CPU Gen   \-Limit Users too 4 Cores 8 Threads    \-Deliver minimal Generational Upgradea while doubling the Price   \-Rebrand Old CPUs with new names and increase Power draw so much it destroys itself   \-Limit/Exclude Tech like ECC, PCI-E Lanes for CPUs under 500€",Neutral
AMD,So now I am fighting the memory issue. Boot process stops at the memory check. Memory debug light stays lit. Nothing else happens. Do I have a bad memory module? Bad memory channel? Possible bad mother board or CPU? Or is it just the AMD issue that it has to 'train' then memory? Off to try another BIOS update and let it sit while I am at work. Maybe it will be operational when I return.,Negative
AMD,The GPU is actually over a year old now.I still have PTSD from the old 9800 Pro days with ATi (now AMD).  I may upgrade after a while but for now this GPU still fares well in most cases.,Neutral
AMD,Holy glaze,Neutral
AMD,i forgot too add Intels support and partnership with Israel,Neutral
AMD,"These huge companies just want your money, they don't care about you...neither Intel, Nvidia or AMD. Stop treating them like they're your friend.",Negative
AMD,AMD Radeon™ RX 7700 ?,Neutral
AMD,>Failure to launch may be observed while using the Oasis Driver with Windows Mixed Reality headsets.  I need to head over to the r/WindowsMR and see if this allows Oasis to work.,Neutral
AMD,"Boy, that's a lot of intermittent crashes.",Negative
AMD,"Seems like it doesn't appear for 9000 series, also it is tagged as an optional driver.  I guess, if everything works fine for your, don't update. Nothing special regarding this one.",Negative
AMD,u/AMD_Vik can you please inform the boys that Alan Wake 2 with RT/PT crashes on 7900 XTX on I think all branch 25.10 drivers?,Neutral
AMD,"Crashing was fixed for me with the last driver update, not sure if I want to update just yet. Please let me know how it goes for yall!  6750xt",Neutral
AMD,"Still didnt fix the VR issues, damn.",Negative
AMD,My rx550 chilling in the corner:,Neutral
AMD,Need fix for beyond all reason driver timeouts on 6000 series!,Neutral
AMD,> ... disable Radeon™ Anti-Lag as a temporary workaround.  Anti-Lag vs Anti Cheat solutions II:  *Electric Boogaloo*.,Neutral
AMD,How can I backup all by adrenaline settings to do the DDU reinstall workaround to avoid bricking the performance metrics? (New 9070xt owner),Neutral
AMD,Please fix Beyond All Reason crashes on the 7000 series GPUs!,Negative
AMD,The NBA my career crash going to get fixed before NBA2k27,Negative
AMD,"Why is the VR bug on the 7000 series still a thing? It's been 6 months by now, that's honestly unacceptable at this point",Negative
AMD,So… no fix for weird pink pixels appearing in browsers? An issue that appeared after last update for many users?,Negative
AMD,The VR issue still not being fixed is a complete blunder!,Negative
AMD,>Intermittent system crash may be observed while playing World of Warcraft while watching YouTube on Radeon™ RX 7900 GRE graphics products.  Nice.,Positive
AMD,Atualização da AMD 👏👏👏 .... Ansioso pelo FSR Redstone!!!,Neutral
AMD,"I am not able to turn on noise suppression anymore, I think this driver broke it.",Neutral
AMD,Damn I was hoping for a big update on official implementation of FSR 4 to RDNA 2 and 3.,Negative
AMD,"""Crashing may be observed in 2077 with path tracing"" yeah no shit sherlock we all have known that for god knows how long",Neutral
AMD,"At this point, I’m not even going to try. I have zero hope for an artifact fix while hardware acceleration is on.  update : i tried and its not fixed ofc 🤦",Negative
AMD,Has anyone with these previous issues:  https://www.reddit.com/r/AMDHelp/comments/1khzck4/version_2551_update_rx6800_crashes_often/   https://www.reddit.com/r/Amd/comments/1k60kzh/amd_ryzen_chipset_driver_release_notes_70409545/   https://www.reddit.com/r/AMDHelp/comments/1k2a66p/blue_screen_with_the_new_2531/   Tried this driver yet? Especially RX 6700XT? The last driver I am able to use without issue is 24.12.1,Neutral
AMD,25.9.1 has been causing High cpu in games for some reason for me.  I have the 9070xt and 7800x3d.    I had to roll back an update and it fixed it,Negative
AMD,I guess we aren't gonna get more officially approved FSR4 games. AMD should really be adding to the FSR4 list even though they allow every fsr3.1 game to upgrade automatically.   There are many people around the internet still claiming that there are only 85 FSR4 games.,Negative
AMD,Experiencing the same issue in world of Warcraft but I have a 9070xt.,Neutral
AMD,I was getting some microstutters on my 9070 XT yesterday playing HL Alyx but that was on 72 hz. Could it be linked to the same problem mentioned here?,Negative
AMD,Is the gtfo fix coming next driver? Or later in the year?,Neutral
AMD,"AMD noise suppression is not working or activating, when i manually run executable from its folder its greeted by a Windows warning.  This app can't run on your PC with a prompt to close it.   [https://i.imgur.com/YxGSE2B.png](https://i.imgur.com/YxGSE2B.png)",Neutral
AMD,"I need a fix for CS2 Hammer. Keep getting VRAD ERROR when trying to compile a map, drivers/gpu crashed. And not just me another friend with AMD card is having the same issue, was ok few months back",Negative
AMD,"But does this fix the hardware acceleration artifacting in chromium-based applications (i.e. Steam, Discord, and Chromium Browsers)?  Edit: For 7000 series cards",Neutral
AMD,"Adrenalin won't open when I use the ""minimal"" install. It was working fine in 25.9.1",Neutral
AMD,"The long-await news for Oasis support! While it's not yet fully resolved, but great significance in AMD's official note. Whether the remaining journey lies with mbucchia or AMD remains to be seen, but really hope Radeon will finally meet Oasis.",Positive
AMD,I really want to play 2k26,Neutral
AMD,"No fix listed for right clicking in explorer launches the Radeon software, le sigh~",Neutral
AMD,I see World of Warcraft mentioned   Does this mean I can now play the new zone in DX12 mode,Neutral
AMD,Eh that's ok...,Neutral
AMD,"This driver dont work with RX 6800 XT, The installation doesn't complete, the screen doesn't return, and it restarts automatically. It doesn't install properly because there's some bug. This happens with the updated WIN 11 24H2.",Negative
AMD,"Already had my first driver timeout while playing dying light the beast we going this way again, where i cannot play new games because drivers simply aren't ready yet ?",Negative
AMD,This error trying to install 25.9.2 and reverted back to 25.9.1  https://i.redd.it/t2z8q8tb1fqf1.gif  Never had this error before since 2020 (rx 6800 \~> 7900 xtx),Negative
AMD,Bro we are waiting for Redstone that's all,Neutral
AMD,Issues with the Cyberpunk 2077 are since beginning of 2025...just fix them fsf,Negative
AMD,My whole system locks up and have to keep enabling the 9070xt in device manager. Must have reported this multiple times. Switched to bazzite and while Linux has its own issues have yet to have the entire system crash like on windows with amd ugh.,Negative
AMD,The Warzone stutter issue has been taken out of the notes what does this mean ???,Neutral
AMD,"Can u fix ""leaked"" FSR4 compatibility on RDNA2? FSR4 works with RDNA2 on the driver 23.9.1 or bellow and on Linux but 24/25.X.X Windows drivers have terrible ghosting and artifacts. Thanks!",Negative
AMD,"If they're releasing more RDNA3, then that hopefully means they're working hard to backport FSR4 to it. Fingers crossed for them, even though I have a 6950XT lol",Positive
AMD,"hah, i was curious about that too...",Neutral
AMD,"It should, this fix is implemented in a way that avoids the need for EDID hacks as well.",Positive
AMD,What is this? I thought mixed reality is dead? Is there a tool that I can use to make my HP Reverb G2 a functional headset again?,Negative
AMD,yup staying on 25.9.1 for now.,Neutral
AMD,Also on CS2 and POE2!!! Driver not found error. I have tried under volting and so forth in the performance settings and cannot get always stable. Only mostly stable.,Negative
AMD,"Yeah, I honestly wonder the same with my 6700xt. Like, I’m guessing majority support is for 90 series by now, so do I risk it with the new drivers..? It’s not like I have bad performance now and I don’t imagine there’s much juice to squeeze out nor do I have much confidence in optimization boosts.",Negative
AMD,"If you look back to the 25.9.1 reddit post, an AMD rep said that they plan on releasing the VR patch for 25.10.1.  I hope I interpreted the reps response correctly and that this happens.",Neutral
AMD,Are you talking about the one where the screen is tinted green on Oculus Rift S?,Neutral
AMD,And no mention of the guaranteed shutdown after 5-6 hours of SteamVR continuous use since last year's driver..,Negative
AMD,Yes I also have to disable all these settings or VAC will boot me. Anti lag and image sharpening,Neutral
AMD,"There is the ""export settings"" option in the system/preferences tab (top right in adrenalin) and you can re-import the zip file directly to restore setrings after installing new driver.",Neutral
AMD,"Maybe it's implied by you saying ""reinstall workaround"", but unless you're having issues there's really no point wasting time doing driver updates with DDU.",Negative
AMD,"Yes, I also noticed this since several prior driver versions. It's rare in occurrence.",Neutral
AMD,"it was pretty ass to begin with if you mean the audio one, not surprised it doesnt work anymore",Negative
AMD,I wouldn't expect that to happen before Redstone.,Neutral
AMD,It's a new driver version mainly released to support the RX 7700 and provide optimized support for today's launch of *Dying Light: The Beast*.,Positive
AMD,I hope it comes to RDNA2 but I am not holding my breath on it.,Neutral
AMD,"Nope that would happpoen with Redstone, amd would announce that feature,its not to put in a driver.",Neutral
AMD,lol no dude won't be seeing anything on that until the end of the year.,Neutral
AMD,Thank you for your service of trying.,Positive
AMD,"I had high CPU usage with the same card. When I would open task manager, it would go back down. Never could figure it out",Negative
AMD,Did the .2 driver fix it? Cpu temps has been the issue for me in the one before 25.9.1,Neutral
AMD,I have a 7900xt but haven’t been experiencing these crashes since swapping off Windows. I’ve had plenty of keys fail because others are crashing,Neutral
AMD,I legit just swapped my 7900xtx for a 5080 because I crash constantly in dungeons.,Negative
AMD,"The driver workaround is tentatively aligned to an October release, we're still working with the developer on an actual fix.",Neutral
AMD,"Are you asking if AMD mentioning a system crash observed while playing WoW while watching Youtube on RX 7900 GRE as a Known Issue (ie. not fixed) would mean that your issues in WoW are solved by this patch?  I mean... probably not, right? But testing it out won't hurt.",Negative
AMD,"Happens with me on 6700 XT, you can finish the install by installing the driver again but I will just use the 25.8.1 for now.",Neutral
AMD,what is redstone?,Neutral
AMD,Last patch notes AMD said they'd already done everything they can on the driver side and were working with the game devs to fix it.,Neutral
AMD,man...,Neutral
AMD,[https://videocardz.com/newz/amd-launches-radeon-rx-7700-with-2560-cores-and-16gb-memory](https://videocardz.com/newz/amd-launches-radeon-rx-7700-with-2560-cores-and-16gb-memory)   New launch! :D,Neutral
AMD,Well this is exciting! Should be able to try it myself later.,Positive
AMD,"Hi, can you give me some details about the change?  Several users (myself included) reported that they don't see any difference. No one has reported success so far AFAIK.",Negative
AMD,Can you pass this somehow to the Ryzen Master devs?   There has been a bug in Ryzen Master instalation that it will fail when you use specific windows region options instead of US. In my case when set to Bulgarian/BG. I have reported it numerous times and it seems that it is not fixed. I still get thank you coments on my 3 yeard old post about this issue.  https://www.reddit.com/r/AMDHelp/s/Z7HLxnjmpQ,Negative
AMD,Massive kudos to the dev team but it could have been communicated better with the Oasis dev.,Positive
AMD,hi gaben is too busy with his yachts - can you guys do anything to improve the performance of cs2 - thanks =),Positive
AMD,Yes! Well there is if you have an Nvidia card! An MS dev created a project called Oasis which replaces the Windows MR stuff that MS removed when they decided to kill everyone's expensive hardware! The Dev has had no luck getting help from AMD so while he got Nvidia cards working us AMD users have been stuck. This looks like a sign that AMD has indeed been working on fixing the road block. It still doesn't actually work on AMD but hopefully now the Oasis Dev can get it working.   I posted the new driver over there. It is worth following the sub for news/updates as things progress.  [https://www.reddit.com/r/WindowsMR/comments/1nka1it/new\_amd\_driver\_claims\_to\_fix\_failure\_to\_launch/](https://www.reddit.com/r/WindowsMR/comments/1nka1it/new_amd_driver_claims_to_fix_failure_to_launch/),Positive
AMD,Yeah there are no boost in performance since years (6650xt here). I guess just update if you need optimization for a new game that you want to play or a new driver feature.,Neutral
AMD,Ah thanks! guess i will wait just a bit more then,Positive
AMD,"No, i have my VR games with the Quest 2 constantly freeze every couple of seconds for about one second or crash entirely",Negative
AMD,"Thanks, Sir!",Positive
AMD,"Every time I update the drivers via adrenaline, the performance tracking is broken and single sensors (CPU temp i.e.) don't show up anymore.",Negative
AMD,Huh? Noise suppression is awesome and helps tremendously in my recordings.,Positive
AMD,"And that still a big if. I wouldn't hold my hopes up for that. I am just happy, that we got FSR4 on older cards as we did.",Positive
AMD,i did and it has nothing to do with it,Neutral
AMD,What did you end up doing.?,Neutral
AMD,"It looks like virus behavior, of the gen.32 type - I remember it well, it has a very clever mechanism spreading silently between executables.    Only Malwarebytes can remove it.",Neutral
AMD,"9070xt had been incredible for me since march, only started with this issue maybe a couple weeks ago.   I might try rolling back to whatever driver I installed when I assembled the build that worked great for me before raid tonight. Otherwise I’m sure it’ll cause at least a few wipes lol. Hopefully they fix it soon.",Positive
AMD,Amd sorry but your software has a problem with my rx I use windows 11 with a rx 6000 series and every time I turn off and on the pc I have to reset the fans because it loses the configuration can you fix it I am forced to use msi afterburner for the fans and driver only for the amd drivers,Negative
AMD,Yeah a guy can dream  aMD time out issues,Neutral
AMD,Next version of FSR 4. Going to be released in October or November.!,Neutral
AMD,"I havent played with it myself yet, but there are threads of people using FSR4 on RDNA2 GPUs on Windows already if that's what you're asking. I have heard there are lots of issues with ghosting though, which are not present on Linux when doing the same thing, so seems to be a driver issue potentially. I too am holding out hope that 6800XT gets FSR4 support because that visual difference is insane, even looking at performance on FSR4 vs quality on FSR3. And as a software engineer I dont see a reason that isn't feasible, but I doubt AMD would put that out (maybe a setting in optional drivers if they did at all).",Neutral
AMD,"What an odd card. VRAM capacity, speed, and bus width of the 7800 XT, but less CU's (40 vs 54) than the 7700 XT.  Guess they had some leftovers where all MCD's worked but not all CU's on the GCD did.",Neutral
AMD,"Hey there, quoting my colleague from Display:  > The driver will autodetect WMR from EDID and register it for LVR use. Oasis should not do or need to do anything to make the HMD work with SteamVR and LVR",Neutral
AMD,I've passed this on - thank you,Positive
AMD,[We touched upon this here](https://old.reddit.com/r/Amd/comments/1n40oz8/amd_being_difficult_with_wmr_3rd_party_driver/nbw7t4t/?context=3),Neutral
AMD,Probably unrelated but for HTC Vive I had to disable Motion Smoothing in SteamVR or it will gobble up more and more system RAM making game stutter then eventually crash (Half Life Alyx for example).,Negative
AMD,"I have the same issue when using the update via adrenaline.  What worked for me is to manually download the actual driver from the amd homepage. Make sure not to download the ""minimal setup"" which is about 40MB, but the full driver package which is \~800MB. Then I simply install the driver and I don't even need to select ""factory reset"" or do any DDU. Just not using the adrenaline update function fixed it for me.  So maybe you could give that a try.  [AMD Radeon™ RX 9070 XT](https://www.amd.com/en/support/downloads/drivers.html/graphics/radeon-rx/radeon-rx-9000-series/amd-radeon-rx-9070-xt.html) Driver Link",Neutral
AMD,"Hmm weird. I stopped using Adrenalin for CPU metrics a while back, but I do remember experiencing some weird behaviour with Ryzen Master.  Nowadays I just update over the top. There's also a box you can select when doing clean install to preserve settings.",Negative
AMD,"Even after using DDU, I still cant get performance tracking to work when I installed 25.9.1. None of the average FPS in my games track, but hours played does. I use a different overlay than AMD's so is it because I dont have the overlay physically enabled when gaming? Idk who wants to stare at that giant thing on the side of their screen anyway while actually playing",Negative
AMD,Krisp and Nvidia broadcast are vastly superior and I say this as a 9070 XT day 1 owner  It's a no contest if you have tried them all  Especially in live situations like voice or video calls  Genuinely terrible in comparison,Negative
AMD,Just minimized task manager. Only troubleshooting I really did was to make sure it wasn't malware.  I thought it could have been a process ending itself when task manager opened but I didn't find anything. I know the latest Monster Hunter was a game it happened on.,Negative
AMD,Hmm. It doesn't sound like a driver problem.,Negative
AMD,Oh wtf.. support for rx 7900xt?,Neutral
AMD,The ghosting issue is fixed with driver downgrade to 23.9.1  Edit: [https://youtu.be/c3YWjqpQu-U](https://youtu.be/c3YWjqpQu-U),Neutral
AMD,In line with the latest 5600F launch  Totally unclear why this production process is still alive and who is gonna buy this,Negative
AMD,Im sure the core clocks overclock like crazy if you're willing to change power limit a bit,Neutral
AMD,More VRAM than the 7700 XT… wtf. Just goes to show the XT model should have had 16 GB from the start.,Neutral
AMD,"Thank you. I'll have to look more closely, it probably requires changes on my side as well.",Neutral
AMD,"Yeah i saw people mention that but its not enabled for Oculus or Windows MR headsets, which explains why i couldn't even find that setting.",Negative
AMD,"Been on green over over 20 years until I gave the big middle finger last month and retired my evga 1080ti Hybrid FTW and jump on the Red Boat. So i'm very used to the DDU process with Nvidia even though I do the clean up option.  As such, what's the best process for team Red?      uninstall and purge all remnants process?     clean uninstall option with new one?     Do I do update via the old one or download new one and run it?  If I tweaked my fan settings and start doing under volting, I will have to start over with that each driver update?   I already installed 9070xt on new 24h2 windows 12 install on my nvme drive last month. So I'm asking how to ""update"" radeon drivers properly",Neutral
AMD,"That's a crazy opinion. I've found Krisp to be absolute dogshit in comparison, like almost unusable. Nvidia is definitely superior, but it's such a small difference I'd struggle to tell if I wasn't knowing to pay attention.  ninja-edit: I wonder if it's different with the 9000 series? I've used Noise Suppression on 6000 and 7000 series only.",Negative
AMD,Sometimes this surge in resource usage can also be noise suppression from the GPU driver conflicting with Windows or other sound-related software coming from the motherboard.,Neutral
AMD,I said that the fans don't work sometimes with adrenaline software not that the drivers don't work to fix their broken software,Negative
AMD,"I hope for you, at least a light version.",Positive
AMD,maybe but dont hope too much,Neutral
AMD,"I stand corrected, thanks for letting me know!",Positive
AMD,I'm imagining it's less new production and more getting any leftover bins that couldn't be used for previous 7000 series cards for whatever reason,Neutral
AMD,"Many people around the globe want to play games and have salaries of about 15 dollars per day. Budget PC may be big IMO, If they produce those chips, someone is buying them",Positive
AMD,It will compete with the 5060 and has double the VRAM. Worth it if it's under 300 dollars.,Positive
AMD,lol even my 6900 xt and 6800 xt came with 16gb,Neutral
AMD,"All good - for whatever it's worth, we successfully initialised our WMR devices using the private driver you'd sent over. Not sure if anything relevant had changed between then and now.",Positive
AMD,"I only switched to AMD a couple years ago myself (last card was HD4890 from 2009) so I was hesitant to jump myself after being with Nvidia for the better part of 15 years. However, performance and stability have been entirely faultless with my 7900XT.  Anyway, regarding driver updates, I've never had any trouble just installing straight over the top of the existing driver via the Adrenalin interface. It used to be that you just download and run the new version directly from Adrenalin, but at some point in the last few months I've got some AMD Install Manager on my PC that does it instead, so YMMV if you have that or not.  I usually _don't_ perform a clean install (I think the tick box is labelled ""Factory Reset"" on the installer window), but there should be an option to preserve settings/config if you do want to do this. I have done it before and it's kept all of my bindings and stuff.  So yeah, tl;dr DDU is definitely unnecessary IMO. Just download the new version and install over.",Neutral
AMD,"Just DDU's ur older driver and install the new, if you're having problems there's some troubleshoot that u can try, but for only installation use ddu, also you can save ALL your configuration in one document, on the AMD app, everytime you do a DDU, just use that document to put everything of your fan and unvervolt configuration. Also, don't upgrade your driver if you're not having problem.",Neutral
AMD,"It's most likely this given how late this is coming out and there doesn't seem to be a DIY release. Plus, this card has the same hardware cost as the 7800 XT while performing slower than the 9060 XT and 7700 XT. So there's really no reason besides salvage.",Negative
AMD,If they have professional SKUs on the same silicon they may be obligated to keep the production line going and keep accumulating strange bins,Neutral
AMD,"While this might be true, building a cheaper product on the newest node and technology would be an answer too.   I don’t think the demand for amd gpus is much higher than supply. Where is the 9060 non xt? Or gre",Neutral
AMD,Also FSR4 is closer and closer to be fully working on RDNA3. So that would be a big boost too,Positive
AMD,"Thanks. Yours had a special mode disabling EDID override, which I now need to make part of the workflow.",Positive
AMD,The WoW driver issue on 7xxx series isn't new. The game has been spontaneously crashing in dungeons for the year I've been back to playing it.,Negative
AMD,"Nah, I'll do someone's earlier comment and just use the clean uninstall on the new driver download",Neutral
AMD,"True, I didn't think of it that way. Professional GPU generations typically have longer periods between new releases I'm pretty sure, so bins that don't work for that use case could also be the reason for this release",Neutral
AMD,"I imagine AMD can get more profit from selling a 7800 XT for 450-500 than a 9060 XT for 370, despite the pricier node. They also may not have the supply to make 9060 XT's to satisfy the entire global market.  This card, however, has the same hardware cost of the 7800 XT while performing slower than the 9060 XT 16GB. So it's likely only being made due to defective dies.",Neutral
AMD,"We automatically register with LVR.  You don't need to do anything for the WMR HMD, SteamVR will should see it as any other HMD today.  However, OS is _not_ aware of the display since, like all LVR HMDs, it is hidden from the OS (not that SteamVR cares, they use LVR).",Neutral
AMD,"Only issue I’ve had with WoW was when using RSR and alt tabbing between windows. However I’m still using 6900XT. Waiting for AMD to release something a bit more high end GPU wise to make the leap to AM5. My Nitro 6900 XT SE has still been keeping up with most games on ultra/high graphics, occasionally taking advantage of Optiscalar/DLSS swapper to squeeze a bit more juice out of it. The 9070 XT seems like a good first release with UDNA cores, now it’s time to see how far they can push their tech with their next major GPU release ;) Tempted to start installing driver without software to see if it helps avoid the extra complications that many experience.",Neutral
AMD,"Thanks. That explains the new behavior that users are seeing, where the initial step (unlocking) fails to see the device entirely. I need to make that step conditional to GPU. There's another possible issues, where some headsets require a USB command to power the display on. Right now, I block vrcompositor from starting until I can enumerate the device (using NVAPI). unless I can somehow get the proper LVR API to enumerate D2D devices, I will need a timer or something arbitrary.",Neutral
AMD,"I was able to confirm this behavior, thanks. I need to coordinate another change with Valve and some slight tuning in my driver, but with that it should work in the end!",Positive
AMD,"Hi u/AMD_Aric (and u/AMD_Vik),  I was able to use driver 25.9.2 with some headsets successfully. However I have had no luck with the HP Reverb G1. SteamVR crashes immediately inside of LVR, and that is even before my Oasis driver is loaded.  (Edited: Corrected, only the HP Reverb G1 seems to have issues so far. HP Reverb G2 appears to work)  Here is a core dump: [https://drive.google.com/file/d/1uXePxj7YZ6mC0XjwkBBGaLcRh17QVHaw/view?usp=sharing](https://drive.google.com/file/d/1uXePxj7YZ6mC0XjwkBBGaLcRh17QVHaw/view?usp=sharing)  The error that is printed on the console:  `2 [VulkanLoaderFunctionTable]   Error: C:\constructicon\builds\gfx\one\25.10\drivers\liquidvr\private\impl\src\mantle_dyn.cpp(1451):alvr::VulkanLoaderFunctionTable::CreateVulkanDevice MANTLE_ERROR(0x3ba20978) vkCreateDevice() failed`  While I don't have access to the D2D API, I have traced it to ""ALVRDisplayFactory::CreateManager()"".  ~~Note that the HP Reverb & Reverb G2 use a ""delayed"" power up sequence, where a USB command needs to be sent in order to fully power up the display. I am not sure whether that is related. I attempted to send the command long before LVR is initialized, but the issue persists.~~ Edited: Not the issue, given that HP Reverb G2 was confirmed to work in the end.  Thank you.",Neutral
AMD,"Also, [u/AMD\_Aric](https://www.reddit.com/user/AMD_Aric/) and [u/AMD\_Vik](https://www.reddit.com/user/AMD_Vik/) (sorry for back-to back messages), I wanted to report that outside of this crash with HP Reverb G1, I have now had several users reporting success with the updated AMD driver and the updated Oasis driver. Success with HP Reverb G2, Lenovo Explorer, Acer AH101 and Samsung Odyssey... So things are looking pretty good. Thank you for the support.",Positive
AMD,"Handheld as in you can hold it in you hands, yes.  Handheld as it is a self contained unit with a battery like you expect, no  Closest analogy is an old GameGear with the external AC adapter, because the 6 AA batteries didn't last",Neutral
AMD,"Powerful, but not quite the hardware or software I'm looking for, yet.",Neutral
AMD,"$1448 who is it made for, handheld prices are outrageous",Negative
AMD,"Just watched a video comparing it to other handhelds. This thing only start to shine when you push it into the 60-70W territory, otherwise gets beat by the Claw 8 at low to mid tdp and equivalent at 30W. Goes to prove that this chip isn’t at all well suited for a handheld unless you just keep it plugged in with fans full blast.",Negative
AMD,"Wow....umm....i haven't been keeping up with handhelds. 1500 for a handheld? That's crazy.  *clicks link*  Oh...and there is a $2300 version too, lol.",Negative
AMD,"Despite what everyone is saying here, I say innovation is still innovation at the end of the day. The more companies outside of Valve develop and release new ideas of how a handheld should be, the faster the development cycle until we hit a new standard.  This reminds me of the late 1970s and early 1980s hardware race before the video game crash and the eventual reset by Nintendo.  But yes, as a steam deck owner, I have not seen another handheld that in my mind is the undisputed deck killer in terms of pricing, performance, convenience, etc. The lack of dual track pads to me is the deal breaker for non deck handhelds for the games I play.",Neutral
AMD,"I get it's a handheld and technically isn't in the same market, but I can build a far more capable PC for the same price. I think $1448 is pushing into mid-range desktop and semi-professional laptop territory.  Nothing against Strix Halo or this little guy (I'll be buying a Framework Desktop mainboard with a Max+ 395 soon), but spending that much on a handheld PC doesn't make much sense to me if it's only for on-the-go gaming. I like the idea of Strix Halo as a platform cause it draws a fine line between consumer enthusiast hardware and professional workstations.  I think most OEMs are forgetting why the Steam Deck made so much sense. I will almost always favor the Deck cause it's a third of the price, it's limitations don't bother me that much since I'm on a smaller screen, and I don't get a broken heart if I brick it from the occasional drop or hard brunt.  I can fix the Deck myself as well, or I can send it off to a repair center, or buy another one and I would still be spending less than a GPD WIN 5. I'd be afraid to scratch the little guy and I'm not entirely sure how reliable or merciful GPD's warranties are.",Positive
AMD,"I don't mind the idea as most of the time when I'm handheld gaming I'm probably in bed, and could deal with the weight if I had to use the battery (rather than you know, waiting until I got to the hotel or something to kick back with no battery), but the rest of the device in particular the screen is kinda lacklustre on a device of this price.",Neutral
AMD,$1448 😬 No thanks,Positive
AMD,So they expect to sell a lot of these things?,Neutral
AMD,"If I'm spending over $1k, I'd rather by a gaming laptop.",Neutral
AMD,Waiting on Handheld with mature FSR4,Neutral
AMD,For $1500 I can get an Odin 2 portal and upgrade my network gear for 1440p 120fps streaming.,Neutral
AMD,I think even gaming laptops might be more usefull,Positive
AMD,I think if I had the cash for this I'd just get a big ass GPU,Neutral
AMD,"Lol mannn you brought back memories....the Game gear with that nice display used to EAT BATTERIES ALIVE, it was like 3 hours i think and they were done....lol **6** AA batteries done in 3-5 hours was diabolical.",Positive
AMD,"Yep, it's pretty pathetic at the lower end of the [power scale](https://www.youtube.com/watch?v=_VMlqtbFaxM&t=193s). It literally more than doubles in performance from 15w to 30w so even with a system that drew 3w for every other component combined it'd take a 99wh battery to get even 3 hours at the 30w setting. At 15w it should still show gains over the Z2E but they'll be so paltry that it would do nothing to justify the extreme price increase over the already exorbitant price. It's hilarious that these handheld manufacturers are both pricing themselves out and pushing the envelope in such ridiculous ways (like this SoC scales linearly until 55w, it's not an ideal handheld chip).",Negative
AMD,The Atari Lynx was worse. I think we’d get maybe an hour out of 6 AAs when we’d play Chip’s Challenge.,Negative
AMD,"if you down clock it to 15 watt, it will still power full, while not power hungry",Neutral
AMD,That last word seems ominous.,Neutral
AMD,"For all 500 of the people who didn't join the 4 million others who bought Steam Decks or who didn't join the 150 million others who bought Switches. This product is for a niche subset of an already niche community, people who value being able to hold a console with less than half the performance of a comparably priced gaming laptop and don't care that they also have to keep it plugged in.",Neutral
AMD,"It is for  people that also replace their workstation/gaming rig with that thing. It is not an additional device, it is the only device you need.",Neutral
AMD,I want strix halo and all the next gens of this chip inside a Zephyrus G14/16 laptop. With a big battery and great cooling it will legit be the perfect windows laptop,Positive
AMD,>spending that much on a handheld PC doesn't make much sense to me if it's only for on-the-go gaming  Agreed.... and if I ever took it outside it'll be another expensive item to keep an eye on and I'll probably have to add it to the home insurance (which covers valuables being carried on person while outside of the home),Negative
AMD,"you can't compare it with steam deck, valve sold it at loss",Neutral
AMD,"It has detachable batteries on the back, so you can just use 2. It's still lighter than the LEGO 2 with them, plus they can be kept on your body or chair instead of clipped on. And the LEGO 2 is 1350$.  Not that I'm getting this either, maybe if it had fsr 4, but I'd get this over the LEGO 2 which has basically no performance bump over what we've had for ages. I'd lose the battery within a week, though. But probably same for the lego controllers.",Neutral
AMD,"I'm very much waiting for a configuration with this chip that I like, even at a high cost. For this one, things like the external battery and screen are iffy, and of course I want to be certain that even if it comes with Windows, it's an OEM that will mainstream necessary changes into the Linux kernel for me to run SteamOS or similar on it.",Positive
AMD,It has swappable batteries. You only need to keep them plugged in. It's what I used to do with phones before companies got greedy. I never plugged my old phones in.,Neutral
AMD,"with the current technology there is no way to replace your entire workstation/gaming rig, most laptops even dont get close, i feel like a handheld is supposed to compliment your gaming rig, its supposed to be a device that allows you to take your already big gaming library on the go. Me personally even if i was replacing my workstation/gaming rig, would not spend a dime over 800$/whatever currency you use, on a handheld like this",Negative
AMD,"They never said that. People just speculate.  The only thing they ever said was that it was hard to meet their price. I doubt they sold it at a loss as they dropped the LCD prices further when the OLED came out instead of placing the OLED above it. And they removed the lower storage specs like 64 and 256 gb, when the storage upgrades were clearly overpriced for the end-user compared to buying your own SSD. So those were obviously much lower margin than the 512 gb ones which they kept.  Also their prices aren't even consistent across countries. The refurbs are decently cheaper in Canada than USA. And you still get free shipping and Canadian shipping is brootal. I doubt they'd do regional pricing if they were already losing money.",Negative
AMD,"Selling at a loss doesn't really make a difference for the end user. At the end of the day it's a $400-650 (USD) handheld versus a nearly $1500 rival that *is* better in the hardware department. What I'm getting at is justifying the roughly thousand dollar premium is baffling to me (and I'm not alone in that opinion).  That said, I'm willing to hear your take. By all means.",Negative
AMD,"Not exactly the most convenient thing, the device will need to be hibernated/shutdown per [GPD themselves](https://youtu.be/ms4An5aqMMg) in order to ""hot swap"" the batteries. There are several applications on Windows that do not take very kindly to being hibernated in my experience with the ROG Ally so not being able to simply plug it in and swap is pretty lackluster. Moreover, even with hot swappable batteries the outlet needs to be nearly as accessible as it is with the laptop either way (or someone needs to lug around a bag of heavy batteries). It also weighs near 50% more than a deck with a battery (almost a whole kilogram).",Negative
AMD,"yes, how you can compete with someone that sell its product at loss. do you think RnD are free ? logistic are free ?  now tell me how do GPD make money ? if they sell their hardware at loss ? I'm willing to hear your take. By all means",Neutral
AMD,"Valve chooses to sell the Steam Deck at a low profit or loss (even then that's speculation. Gabe Newell said in interviews selling the Deck at its low price was ""painful"", but he never specifically said Valve lost money on each Deck). It's called being a loss leader. It's a very common business strategy, and Valve did so with the intent of increasing Steam store sales. It's not an uncommon business strategy if it attracts new customers to a brand or encourages the sale of other products within said brand. Even then the end product is the end product.  At the end of the day the Deck is still a much cheaper handheld that makes sense for the market it created. I don't see the argument here about a company choosing to sell a product at a loss when the main criticism is that a competitor, the GPD WIN 5, is almost $1500. Heck, it still doesn't even have an internal battery!  GPD used hardware IMO that could have been used for other devices (mini PC or laptop workstations, which Strix Halo makes so much more sense to use) instead of a gaming handheld that I gotta worry about taking out on the streets cause it costs freaking $1500. I can't even fully utilize the performance of the Win 5 unless I max it out at 70W (which is 50W below what Strix Halo is capable of). and kill that weird little external battery pack in an hour.  Valve Steam Deck? I just gotta make sure it's charged and still works. I own a 1TB OLED config ($650). Love the little guy like I used to love my old Nintendo DS and 3DS. It ain't up to date hardware wise (I wish we had a Deck sequel with more current APUs), but at the end of the day I bought it because much more expensive handhelds with more modern APUs didn't make any sense for what they cost.",Neutral
AMD,What a strange late addition to the line up. Bunch of left over Navi 32 chips laying around I suppose. Interesting that they’d change the memory configuration though.,Positive
AMD,"Itll be a 5060 competitor but with 16gb, since its on an older node means it should be cheaper too, idk why people are complqining, this is great. If they put it at 279 it wouldnt be so bad",Positive
AMD,Pricing will dictate how well this product does.    $225 and it will sell like hotcakes. $270 is okay value and it will entice possible 5060 buyers with the extra VRAM at the expense of features and a small amount of performance. $300 and it won't move the needle.,Positive
AMD,"Um, so basically a Radeon RX 6750 XT?   With those CU and SP counts, it would be interesting to see the real uplift per clock between RDNA 1, 2 and 3.",Neutral
AMD,"Kinda wondering how this might compare to a 9060 XT 16gb, which I'm considering as an upgrade from my rx6600.",Neutral
AMD,"AMD probably wants to capitalize on all the 8GB GPU hate. If you want to get a cheap card with 16GB RAM, this is it.",Neutral
AMD,But... why?,Neutral
AMD,"This is great for compute workloads, where only memory bandwidth and capacity matters. I'd happily trade in my 7700 XT for a 7700.",Positive
AMD,"New rx 6800?  If its at the same price point and performs better im all for it, otherwise meh.",Positive
AMD,why tho?,Neutral
AMD,9090xtx when,Neutral
AMD,I really wish AMD would release a true high to extreme end GPU. As someone who has a 4090 I really want to change to AMD. I have zero intention of buy anything from Nvidia.  I have an AMD CPU and would love to get away from Nvidia and their drivers that are coded with faecal matter on a wall somewhere. But I cant without taking a major downgrade. Even if it was a 5-10% downgrade I would probably consider it.  I get it that they are going for the mid range where most people sit. But there are still a decent portion of people who own 4080s and up.,Negative
AMD,AMD does the same thing with leftover Ryzen cpu chips.   Not surprising…,Neutral
AMD,Where does this compare to 9070 xt and nvidia? I am not up to speed with amd,Neutral
AMD,"Even with FSR4 coming and currently runnable on RDNA3, what’s the puro pose of getting this over a 6070XT (16Gb). Obviously it’s better than a 6070xt 8GB with the VRAM. But that’s about the only edge I see.",Neutral
AMD,"Leftover silicon ...  Well, if the price is very low on the market this could be OK for budget gaming  Now that we know FSR4 actually works fine on RDNA3 ...",Positive
AMD,"very simple really amd recognized that there is a market for it I bet you those will sell very well in some countries (africa, asia ) not every one can aford a 9000 card as usual if the price is right, there seems to be a dissonance between the west and all the other countries $200-500 is a lot of money if your wage is $1000 or less monthly.",Positive
AMD,AMD did more to the environment with all these launches than all green parties combined /s,Neutral
AMD,"But what about FSR? Anyway, power requirement is too high.",Neutral
AMD,"Gee, AMD finally decided to launch the real 7600 card.",Positive
AMD,Stupid name tbh. Call it RX 7800 instead,Negative
AMD,"How come AMD always does this? I don't think i remember Nvidia ever dropping a GPU this late that seems like it was made from defective left over dies, intel sometimes does this with CPUs but to a far less extent.",Negative
AMD,Why not especially when you give it 16gb a cheap price and its looking like fsr4 works fine with 7000 series. this is what you do with left over silicon. its expensive to make so you really need to sell it.,Neutral
AMD,"If there was any indication it was going to sub-$300, nobody would be mocking it. But, if that were the case, they'd already be teasing the price.",Neutral
AMD,Lmao $280? Not even the 9060xt 8GB has that low or an MSRP.,Negative
AMD,"Well, same number of CUs (and SE, SA etc) but more RAM, newer process, RDNA 3 design with all that that implies. If AMD finally releases FSR 4 for RDNA 3 officially, it will be significantly better.",Positive
AMD,With 16 gb vram yes,Neutral
AMD,"I was thinking rx 6800. A rx 7800 would be too pricey for what it is i imagine, so rx 7700 it is.",Neutral
AMD,"That's what I'm excited to see because while the 7600 and 6650 xt have the same specs, the 7600 seems to not be a full rdna 3. I'm sure the uplift will still be extremely disappointing but it'll be interesting to see nonetheless",Positive
AMD,Did this same upgrade from rx 6600 powercolor fighter to 9060 xt 16 Gb Saphire pulse. The 9060xt is amazing and a much better GPU that does 1440p smooth as butter.,Positive
AMD,This is OEM only. You can't buy it,Negative
AMD,"Garbage can or a paying customer, where would you put your leftovers?",Neutral
AMD,Use every silicone available.,Neutral
AMD,"binning. its why when you see AMD release lower and lower F stack cpus, it gets slower and smaller with like the 5600F and 9500F. AMD has to spend time collecting the defect chips that either have core defects, or clock like shit, and then sell them as a cheaper SKU. it takes awhile to stockpile these rejects due to TSMC's good yields.",Negative
AMD,Leftover cores I assume.  Also if they price it well it will become an absolute deal. Especially after AMD confirmed that they made FSR4 work on RDNA3 gpus.,Positive
AMD,Defective dies that didn't meet standard. Took a while to stockpile them.,Negative
AMD,They did with the 7900xtx and it just didn't perform competitively. That being said the 9070 xt outpaces the 5070 ti and sometimes comes damn close to the 5080 in performance. For 1440p it's a great graphics card,Neutral
AMD,It's gonna be like a 4060 ti or a bit faster. It's not gonna beat the original 7700 XT in performance.,Neutral
AMD,FSR4 works unofficially on RDNA3 in Linux and now in Windows. I’d be shocked if official support isn’t added soon.,Neutral
AMD,its slower than 7700xt so 7800 wouldnt make sense,Negative
AMD,Or 7750XT,Neutral
AMD,"> How come AMD always does this?   die bins with AMD seem to be like, they shelve them until they hit a critical mass of them and then make a product. Like the 5600X3D",Negative
AMD,"Because AMD doesn't make as many chips as Nvidia, and their designs tend to maximize TSMC's node yields. As a result it takes a very long time to accumulate enough defective dies and make a SKU out of it.  If you look at how Nvidia bins their GPUs, you'll notice they don't sell you the full die most of the time. That's because their design philosophy is fundamentally different from AMD's, aka fast and loose, which allows them to make several SKUs out of the same chip within one generation. Making several times more GPUs than AMD also helps.",Neutral
AMD,"> Nvidia ever dropping a GPU this late that seems like it was made from defective left over dies  They do, remember the 3050 6GB?",Negative
AMD,"I remember the GTX 465 was a salvage gf100 (like gtx470 and up)   Meanwhile GTX 460 was gf104   There was also the GTX 460 SE ""slow edition"" with a reduced cuda core count. In my opinion it should have been named GTS 455.",Neutral
AMD,"3060 8GB, 3050 6 GB",Neutral
AMD,"They do, but they tend to stick THOSE GPUs  to OEMs and markets like China",Neutral
AMD,"They do. Its just if nivida is selling 10x as many, they build up enough defective chips 10x faster, so they can release a bin somewhere much faster.  Everyone die harvests silicon, and stacks it until they have enough to sell.  This type of stuff often just gets released into the oem channel, rather then a retail diy product. And it often goes unannounced.  Nvidia's current flagship, the 5090 is a defective cut. The good chips go into server. Nothing wrong with this.",Neutral
AMD,"unless they will have no more 8gb gpu so any from new lineup will have at least 12GB and any mid-lvl gonna have 16GB , maybe they just want 7700 to have some kind of lifespan with 12GB it would have barerly anything",Neutral
AMD,"There is no way that it will cost more than 9060 XT because it will be slower (9060 XT is few % faster than 7700 XT on avarage) and 9060 XT costs 299$, my guess is that it will be around 250$ or maybe even lower.",Negative
AMD,"Yeah, but a 9060 is better performance than the alleged 7700 in all ways but the vram, better rt and fsr4",Positive
AMD,The 7700 XT is a 6800 replacement. It's definitely lower in performance than that.,Negative
AMD,Ah. Damn.,Negative
AMD,Minimize waste and maximize profit. I can get behind this.,Neutral
AMD,I seldom keep my leftovers for two years.,Neutral
AMD,"The 7900 XTX was never gonna perform that well. It required game wise optimization by developers to take advantage of the new structure and that was never gonna happen. In the handful of games where that was leveraged it beat the 4090 but otherwise it was a 4080 competitor. Tbf, AMD did seem surprised by the low performance gain given their projections but I'm not sure why. They were just implementing chiplets in GPUs for the first time and should've known that there would be difficulties getting it to work properly.",Negative
AMD,2 years later we should see 9500x3d,Neutral
AMD,Like Radeon 9550.,Neutral
AMD,"Geforce 1010 - Pascal chip that launched in 2021. Yeah, they can do it too, but I think in that case it is mostly that they wanted something to replace their ancient 710/720 chips. Not sure that those 1010s were quite as broken as that, so they likely just cut functioning units to hit a price point.",Neutral
AMD,"I thought about that, but can't think of another example, I guess the 10 billion versions of the GT 710 and 730 can technically count but they're kinda irrelevant, AMD had like the 6750 GRE, 7900 GRE, 9070GRE and now the 7700, all are late releases that are cheaper cutdown versions of a certain die that presumably didn't hit its target performance, Nvidia does super refreshes which are better versions of the same die but rarely does the opposite.      Also look at how many new AM4 CPUs made from leftover silicon are releasing it's crazy.",Neutral
AMD,"2060 12GB is a better example but yea I don't see Nvidia launching new 40 series GPUs anytime soon, it's certainly rare.",Neutral
AMD,">There is no way that it will cost more than 9060 XT  You must have forgotten last year when amd released the 5900xt, a cpu thats slightly slower than a 5950x, but they charged more than the current price of a 5950x. Amd is known for launching products at prices that don't make sense.",Negative
AMD,"One can always hope. Then again, it has double the memory bandwidth of the 9060 XT, and we live in the age of AI so...",Neutral
AMD,ASRock has confirmed they are selling the RX 7700 for the DIY market. So looks like you will be able to buy one.,Positive
AMD,AM4 leftovers: hold my beer,Neutral
AMD,They probably needed to stockpile chips to fit this SKU for a long enough period of time to justify a production run of these cards.,Neutral
AMD,"I'm still on AM4, waiting till 2027 for the latest 5200x3d to drop.",Neutral
AMD,"The 1630 as well, launched in 2022 lol",Neutral
AMD,"Radeon RX 6400 (RDNA 2, Navi 24) in 2022 while the RX 6000 series launched in Nov 2020  Radeon R9 370 (2015) was based on the HD 7850/7870 chips from 2012 (GCN 1.0, “Pitcairn”) relaunched 3 years later, even though Fiji/Fury (GCN 3.0) was already out.  Radeon RX 550X, 560X, 570X, 580X (2018 OEM), these were straight rebrands of the 2016–2017 Polaris cards but by 2018, Vega was already old news and RDNA 1 was about to drop (2019).  Radeon R7 250X (2014), rebrand of HD 7770 (2012), came 2 years later, while GCN 2.0 products were already out.",Neutral
AMD,"Yeah, but the 9060 XT has 300 as it's MSRP. The 5950x is lower now of course but it's MSRP was much higher than the 5900XT.  Now, what you say could happen but it's much less likely imo. This card shouldn't be anymore the 250 and that might be pushing it still.",Neutral
AMD,Well rdna 3 didn't sell as well as they thought. They also ordered from tsmc during the crypto boom so they probably ordered a huge volume from them only to be left high and dry when the mining phase ended. Thats one of the reasons why they made the 7900 gre. Too many navi 31 dies cause the 7900 xtx and 7900 xt werent selling that well.,Negative
AMD,"That is probably the most charitable way to put that. Although I highly doubt the ""stockpiling"" was deliberate. RDNA3 didn't sell well, and AMD underestimated the demand for RDNA4 because of it, meaning they are losing free money. I do believe they are in a lurch now, so we get this. But if RDNA3 had sold better, these would have been sold as whatever bin they could sell them at long ago.",Negative
AMD,I cant believe I built a system on AM4 at launch in 2017 and it's still going so strong 8 years later  I will ride this platform out till DDR6,Positive
AMD,We gonna have AM6 and there will be another AM4 X3d refresh trust,Positive
AMD,For $200  ...with performance below their own cards launched 1/2 decade earlier for $150,Neutral
AMD,You can't really compare the msrp of a product made in 2020 to the time they launched the 5900xt. Nobody is gonna price a produce that was below 350 dollars to an 800 dollar msrp that was discounted 3+ years. The 5900xt was just a blunder because nobody would pay more for a product that gives you less.,Negative
AMD,"This usually works by the manufacturers taking chips that were rejected for use in other products, like chips that failed validation for 7800 XT or 7700 XT, and repurposing them into new lower end SKUs where they can be sold. It is certainly possible that these are good chips they simply couldn't sell - that has happened in the past, and sometimes you could unlock them with a BIOS flash - but AMD is pretty competently run these days for the most part, so I doubt they had a whole ton of unsold 7800 XT GPUs that they are dumping this way. Could be, but I think the better guess is that these chips are the duds, or they're targeting a specific market like how AMD dumped the 5500X3D in Brazil.",Neutral
AMD,Same here. Motherboard and ram from 2017. Started as a 1600 system and was able to drop in a 5600x as a easy upgrade. Picked up a 6750 xt gpu along the way. Never had a PC keep up this long. Specially as a perpetual mid tier gaming system.,Neutral
AMD,Same 5800x still kicking arse!!! roar!,Neutral
AMD,"If these were released a year ago, or even around CES 2025, I'd have believed they were defective chips that AMD is binning as a new SKU. But I'm not sure I believe that now. I think chances are better than even that some/many of these could have been full 7800XTs, and that is a shame.",Negative
AMD,Same starting point  1600 => 2700x => 5700x3D  &nbsp;  Also had a RX580 => 3060ti upgrade  My friend hooked me up with the GPU at msrp in Jan 2021  &nbsp;  Until we get 10th gen exclusive games I think I'll be fine to just wait it out,Neutral
AMD,Well maybe the purchasers are about to get a hell of a deal if they can be unlocked with a BIOS flash!,Positive
AMD,Why are they comparing Zen 5 to 14th gen when Arrow lake with vPro is already everywhere in the big vendors?,Neutral
AMD,What is the msrp?,Neutral
AMD,"Hey OP — /r/AMD is in manual approval mode, this means **all submissions are automatically removed and must first be approved before they are visible to others**. This is done to prevent spam, scams, excessive self-promotion and other rule-breaking posts.  Your post will be approved, provided it follows the subreddit [rules](https://www.reddit.com/r/Amd/about/rules/).  **Posts regarding purchase advice, PC build questions or technical support will not be approved.** If you are looking for purchasing advice, have a PC build question or technical support problem, please visit the [Q3 2025, PC Build Questions, Purchase Advice and Technical Support Megathread](https://www.reddit.com/r/Amd/comments/1lomnxt/pc_build_questions_purchase_advice_and_technical/).   *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Neutral
AMD,Under 200 USD,Neutral
AMD,- A product by AMD - A consistent naming scheme easy to understand  Choose one.,Neutral
AMD,All wild rumours.,Neutral
AMD,"Being confident in your own products and not using a naming system near identical to the one used by your competitors would be a good start, but that's wishful thinking.",Neutral
AMD,They make the name confusing on purpose obviously,Neutral
AMD,I mean we already know they can't do that because when they don't everyone just assumes AMD is the cheapo option even if its the fastest CPU you can buy.,Negative
AMD,"Yeah, one lf the reasons is that it's easier to sell old products as brand new or make inferior products look not so bad.",Neutral
AMD,"Sad. It's likely the same dumb consumers who believe the ""bigger number = better"" marketing BS.",Negative
AMD,"I mean the Ryzen 3 7 9... solved that problem people understand that because Intel has hammered it into their brains. It's just the extra names that are crazy, useless and confusing.  Literally better if they had renamed ti Aizen (oh wait thats a bad guy)",Negative
AMD,"I admit the Ryzen 3/5/7/9 branding is a good exception and just works. But I also dislike the ""Ryzen AI"" word salad and now Radeon RX x0y0 (XT) instead of RX xy00 (XT) system of previous generations that worked just fine.",Negative
AMD,"OK, so here I go, Ryzen 9825 next year we get Ryzen 9826 ..... whoooooooo amazing.  3 7 9 can be number of cores  1-9 can be perf range segment, eg a 16 core ultra low power is an 91 a 2 core ultra low power is a 31  Netbook has a 3125  Gaming desktop has 9925  Simplified but still clear naming is GOOD for marketing. It helps people buy the right thing and be happy with what they got...  applies for GPUs too, AMD's GPU stack is not more than 10 thick and for some they can ignore the naming eg OEM / Embedded can get a different name if it needs to get out of the consumer marketing stack.  I think Prizm would be a good brand name to reduce Radeon Pro \[AI\] R\[W\]\[WX\]xxxx naming down to something sane. under then same nomenclature. So AMD Prizm 9926 would be the top workstation GPU next year.  I think the market control groups are partly to blame for this whole issue... because a name is NOT supposed to really convey all the information, its only supposed to clearly identify a card. But they are probably asking people questions like ""how does this GPU make you feel"" or ""do you think this \[insert long forgettable GPU name\] is fast at AI"" type questions ....",Positive
AMD,They just keep finding silicon down the back of the couch,Neutral
AMD,Lisa Su became CEO of AMD in 2014. Coincidence?,Neutral
AMD,how about you make some more 5800xds,Neutral
AMD,"So now AMDs F designation doesn't mean non working APU...? (Well kind of, but the original also didn't have working APU) Great naming convention going on.",Positive
AMD,"let's make it 10 years, chief. AM4 is the best platform in the history of mankind.",Positive
AMD,Never gonna give AM4 up   Never gonna let AM4 down  Wow that's lots of 5600...   5600X / 5600G / 5600 / 5600X3D / 5600GT / 5600XT / 5600T / 5600F,Positive
AMD,Bloody hell AMD must be sitting on mountains of AM4 chips.,Negative
AMD,"Re-releasing old CPUs with slightly different clocks is not extending the AM4 platform  The last actually new CPU on AM4 was the 5800X3D in early 2022 (or the 5600X3D in 2023, though that one's not retail).   Everything after that was just a re-release with slightly different clock speeds and prices.",Neutral
AMD,keep AM4 alive,Neutral
AMD,Please bring a new high-end CPU for AM4: 5950X3D,Neutral
AMD,ryzen 5599.9x when?,Neutral
AMD,AM4ever,Neutral
AMD,AM4: Never gonna die!,Neutral
AMD,I really thought Socket 7/Super Socket 7 would be the king of longevity forever… and here we are still talking about AM4 still. So cool!,Positive
AMD,"Its rare having this long support, in one way you sell less mboards and why mboard prices gone up a lot for am5.  atm with a 9800x3d on the same mboard I got 4! years ago on am5 and its been working great since",Positive
AMD,Ryzen 5 5000 hierarchy if anyone needs it since it's become an entire generation of its own now:     5600X3D   5500X3D   5600XT   5600X   5600T   5600   5600F     Ryzen 7 5700/5700G (lol)     5600GT   5600G     5500GT   5600GE   5500,Neutral
AMD,goated cpu support,Neutral
AMD,Just gimme a 5950x3d for Christ's sake,Neutral
AMD,AM4 : I haven't heard the belt yet.,Neutral
AMD,Goated platform.,Neutral
AMD,AM4 refuses to die.,Neutral
AMD,AM4: I didn't hear no bell!,Neutral
AMD,Why not just make more 5800x3d CPUs? I mean that’s probably what most AM4 owners want.  Anyway not surprised by this release when you realise manufacturers are still making AM4 motherboards. I mean my AM4 Gigabyte motherboard was only made this year.,Neutral
AMD,"Bullshit, just print more 5800X3D, we'll buy all of them",Negative
AMD,I mean my am4 has been playing borderlands 4 just fine. Lol. And I only have a 5600x,Positive
AMD,This is just a rebrand lol. The upgrade path stopped in 2020,Neutral
AMD,I’d like to think that this is just left over silicon that couldn’t quite make the cut as a 5600 or 5600X.,Negative
AMD,"Wonder how long will be the support for AM5. Just made myself a new pc a month ago, but hopefully I won't have to get to AM6 when I want to upgrade the CPU.",Neutral
AMD,"If the rumors are true, AMD are now planning to keep AM5 alive until at least Zen 7. So it has the potential to be just as epic.",Positive
AMD,They can't keep getting away with this!!!!!,Neutral
AMD,Interesting,Positive
AMD,"Honestly I love it. Keeps motherboards out of landfills and means there's new chips with new warranties entering the market, increasing the supply of used am4 chips, lowering used prices further. I love it. Honestly amd absolutely nailed ryzen. I've been onboard the day I swapped out my 6 year old ryzen 7 1700x for a 5900xt and got a massive upgrade, without having to buy anything other than a new chip. Freaking awesome",Positive
AMD,F for fail,Neutral
AMD,"Considering my B450 board got a BIOS update this month, I would suggest yes, this socket has been well supported.  FYI MSI B450-a Max  [https://www.msi.com/Motherboard/B450-A-PRO-MAX/support](https://www.msi.com/Motherboard/B450-A-PRO-MAX/support)",Neutral
AMD,"I might be uneducated in this space but does anyone care to explain what the point of making those low end cpus is? I am struggling to find any AM4 X3D card for under 700€ here in Austria for years now (new , used is 400+€). Why is AMD not producing more of the AM4 X3D cpus instead of giving us outdated and weak version of already existing low/mid tier cpus?  Wouldnt it support am4 way more if they just continued the AM4 X3D cards since the demand is still HIGH AF even after years?   Im so confused",Negative
AMD,I’m still waiting for my 5900X3DSSS,Neutral
AMD,it's not worth it anymore imo  DDR4 3600mhz CL16 is now more expensive than DDR5 6000Mhz CL30...  no point in paying a premium for outdated tech,Negative
AMD,"This needs to stop being treated as news at this point. Come back to me if they put a mobile APU with Z3 or Z4 cores out on an AM4 package, instead of finding new ways to get smoke blown up their arses for repackaging defective chips.",Negative
AMD,Dead on arrival product... Interesting. Mid range old tech CPU rebranded as new content...,Positive
AMD,This really just means 9 years of AMD staying in the budget market.,Neutral
AMD,"I don’t care about the CPUs, it’s kinda annoying that they keep changing details in the name which can make huge differences for potential buyers.  But claiming 9 years of platform support for AM4 is just ridiculous. There hasn’t been a new architecture since 2020 on AM4.",Negative
AMD,May as well make a 5300 and 5400. Bring back Ryzen 3 lmao,Neutral
AMD,Could they... Find some more 5700x3d please 🥺,Neutral
AMD,"""You know, this just fell out of the TSMC truck...""",Neutral
AMD,I'm waiting for Athlon 250 X3D,Neutral
AMD,Absolutely not she’s the best thing that could’ve happened. I know am5 isn’t going to have this level of support but god damn is it a good run. Am6 Might have more longevity than am5 due to more pin outs allowing for a higher data transfer rate.,Positive
AMD,Ryzen 5000 series launched more or less at the same time as Apple's first M chip. Coincidence?,Neutral
AMD,I'm assuming they're aligning the naming of everything up.,Neutral
AMD,amd naming scheme was always akin to someone on serius acid.  Just look at their laptop chip. You basically need a degree to decipher what the product actually is.,Negative
AMD,I think it stood for that for a couple years at least,Neutral
AMD,I see the opportunity for them to do a 10th anniversary exclusive cpu sku,Neutral
AMD,"Don't leave out 5650G, 5650GE, 5655G, 5655GE!",Neutral
AMD,"They are just handing out 5600 series like Oprah.  You get a 5600 series, you get a 5600 series, EVERYONE GETS A 5600 SERIES!",Neutral
AMD,"> Never gonna give AM4 up  > Never gonna let AM4 down  *Never gonna stop binning down, and desert you!*",Neutral
AMD,"Several of the chips they’ve done this with have been relatively low volume. They don’t really expect to sell a lot of these though so it doesn’t matter, it’s just otherwise wasted silicon.  *Usually* this kind of thing would have just resulted in a lower binning of the already existing stack rather than making a new SKU. An i5 being dropped to an i3 for example. That doesn’t make news though for obvious reasons.  (Even AMD themselves used to, and likely still do, do that as an aside.)",Negative
AMD,Wouldn't the last am4 one technically be the 5700x3d? In 2024.,Neutral
AMD,"Extending a platform ≠ new CPU.  And 5600X3D was also not a new CPU, it's the same die as 5800X3D that doesn't meet all the requirements to be a 5800X3D, just like how this one is using the same 8-core die as 5700X.",Neutral
AMD,"While that's true, there still is a huge amount of overhead for adding a new sku, whether it is new silicon or not. You need a new agesa and you have to get all your board partners to update decade-old products as well, which also helps encourage security updates for boards that might have otherwise aged out of support.   AMD absolutely deserves the applause they're getting for this.",Neutral
AMD,"Its an extension of the socket as its adding more silicon for the socket to the market, 5600f exist because its a 5600 that cant meet the clock speeds reliably so making a new sku at a cheaper price is the obvious choice. Just like how the 5600X is a higher binned 5600 while the entire ryzen 5 line is actually a lower binned ryzen 7 where 2 cores on the CCD are not reliable in some way.",Neutral
AMD,Releasing the same shit with different clock speeds probably helps availability,Neutral
AMD,"And what is your point? This is what AMD does. In 2005 I bought an AMD Sempron 2200, it was an Athlon XP at a different bus speed in clock speed that didn't technically exist it was just using up old cores. Nothing wrong with this.",Neutral
AMD,It's still a new SKU even if it's based on older tech.  Of course something like a 5950X3D or some kind of Franken-Ryzen with Zen4/5 cores and the Zen3 IO die would be even more interesting...,Positive
AMD,Aww man I could only wish,Neutral
AMD,This would be so badass,Neutral
AMD,IIRC S462 was 6 years. Longer than those.,Neutral
AMD,"Yeah, considering the vast amount of AM4 boards that have been sold, and are waiting for the ultimate upgrade, it would sell super well.  I could even see a 6000 series becoming a thing, slightly higher clocks at a reduced power consumption. And a 6800X3D.   One can dream.",Positive
AMD,I would on that so damn fast!,Positive
AMD,it cannibalizes AM5,Neutral
AMD,2022,Neutral
AMD,great point.,Positive
AMD,5100x3d is exactly what the world needs right now,Neutral
AMD,5300G exists. It is also a Ryzen 3.  [AMD Ryzen 3 5300G Specs | TechPowerUp CPU Database](https://www.techpowerup.com/cpu-specs/ryzen-3-5300g.c2473).  was looking for them on Aliexpress and yes they are available there.,Neutral
AMD,Loads on AliExpress if you want one badly,Neutral
AMD,"I am a bit of nut for AM3. Especially the Athlon II series. Would absolutely buy one fo those.  Unironically, I am a collector of them.",Neutral
AMD,unlike other ceos (cough cough boeing) lisa is an electrical engineer and knows what she's doing,Neutral
AMD,why wouldn’t this happen to am5? really asking here,Neutral
AMD,💯💯💯,Neutral
AMD,"Sure, across gens the F designations is now indicating a cheaper similar performance chip. It is just that within the 5000 series the naming is completely messed up. Between the 5600(X), 5700G, 5700, and now 5600F, I don't see how anyone is supposed to make heads or tails of it if they aren't deeply into it.",Negative
AMD,And they change the scheme regularly which invalidates their previous press materials k on how to decode them,Neutral
AMD,5950x3D.,Neutral
AMD,And the 5605G,Neutral
AMD,I'd like to think somewhere AMD's marketing department is going hey we're missing a CPU in this $5 margin and engineers go we got you.,Negative
AMD,if intel found those chips they would've released a new platform.,Neutral
AMD,Any X3D AM4 CPU after the 5800X3D would be considered a lowered binning of that CPU. The 5700X3D just can't clock as high as the 5800X3D but the performance difference is negligible.,Neutral
AMD,Got one on launch day put my 5600x in a different rig😁,Positive
AMD,"Most (all?) of the releases past 5800x3d have not required a bios update, because they’re existing chips as far as the board is concerned.  AM4 AGESA updates are strictly security and have been since 2022 (the aforementioned 5800x3d), the CPU release don’t require an update for the same reason as above.  Edit: the only overhead required of this is making packaging, really. Down binned chips from silicon that does not meet requirements has been a thing for ages, the silicon would have gotten shipped whether they called it a 5600 or a 5500.",Neutral
AMD,"> you have to get all your board partners to update decade-old products as well, which also helps encourage security updates for boards that might have otherwise aged out of support.  that reminds me, I need to check for MB bios updates",Neutral
AMD,>AMD absolutely deserves the applause they're getting for this  For what? The consumer does not benefit from this in any way.  Yeah it's more effort for them than just lowering the price of existing SKUs. But that doesn't make it any better. We should be wondering why AMD is doing this.,Negative
AMD,Putting some of the die shrunk zen3+ mobile chips on AM4 would have been interesting but that probably isn't doable as I think they only support ddr5 and not ddr4  Apparently they were using them for testing or something on AM5 https://videocardz.com/newz/amd-confirms-zen-3-rembrandt-apu-with-navi-graphics-was-tested-on-am5-socket,Neutral
AMD,Still though the fact they’re still supporting AM4 even now is a good thing.,Positive
AMD,"Still zen 3 and in my own experience, 5800x3d is not much of an upgrade from 5800x unless all you do is play cs2 at 720p low",Neutral
AMD,Gold!,Neutral
AMD,"At the very least, there needs to be a Zen 3 Ryzen 3 part with PCIe 4.0 and 16 MB of L3 cache, because the Ryzen 3 3100 is the last truly great Ryzen 3, along with the 3300X. Both of those have PCIe 4.0 and 16mb of L3 cache.  The terribly conceived Ryzen 3 4100, has PCIe 3.0 and 8 MB of L3 cache.  Even the Ryzen 5 5500 only has PCIe. 3.0 and ...I think maybe 8 MB of L3 cache.  So a Zen 3 architecture version of the Ryzen 3 3100 would be an amazing budget gaming CPU and would have no competition in ""that class"", if it was predictably priced.",Neutral
AMD,A dual core cpu with 3d cache. For gamers on a TIGHT budget. Comming soon in limited supply spring  2027!,Neutral
AMD,That would be an APU.,Neutral
AMD,Yeah but now priced at like 290 euros...   They used to be reasonably priced at like half of that.,Neutral
AMD,"Honestly I dream of being in a similar place as Lisa, I want to manufacture motherboards and am trying to get into that field",Positive
AMD,Oh yeah that doesnt make sense whatsoever considering 4 of those have no iGPU but only one has that in its naming scheme.,Negative
AMD,Don't forget the 5600G! Which is more closely related to a 5500 than the 5600/X,Neutral
AMD,If they make a 5950x3d ill marry amd,Neutral
AMD,And the PRO 5645,Neutral
AMD,I'd still say it was an extension of am4 due to the value it provided over the 5800x3d though. It allowed many am4 gamers on 3000 series to upgrade without breaking the bank.,Positive
AMD,"400 MHz is not ""negligible"".",Neutral
AMD,I can say with 100% certainty that my B550 Tomahawk was on the newest bios prior to the release of the 5600x3d in 2023 and required a bios update to support it.,Neutral
AMD,"if it ain't broken, don't fix it.",Negative
AMD,"They're obviously not doing it out of the goodness in their heart, but I'd still say consumers benefit. Many are still on Zen 1 and Zen 2 and don't have the budget to move to a completely new platform. With Windows 10 support ending, this also allows those on unsupported CPUs to upgrade to a supported CPU. Of course there's always the second-hand market but without new CPUs coming out, those prices would be much higher too (e.g. the 5700X3D really punctured the second-hand market for the 5800X3D).  As for why they don't just lower prices on existing SKU's, it's pretty obvious. Lower clock speeds and a few disabled cores allows them to increase yields, which is important in such a price-sensitive segment with probably very thin margins.",Neutral
AMD,">  5800x3d is not much of an upgrade from 5800x unless all you do is play cs2 at 720p low  Even at 1440p there is a *massive* difference in 1% lows and frame-time smoothness going from a 5700x to a 5700x3d, in a wide multitude of games.",Neutral
AMD,"By that logic a 7700X is also no better than a 5800X for a gaming PC.  The 5800X3D is \~30% faster than the 5800X/5950X in gaming.  If you play PubG, Tarkov, Warhammer Space Marine 2, racing games and sims, flight sim, in these games it can and does make a significant difference if you have a good GPU.   If you're GPU limited, like in most AAA games, then yeah it wouldn't make a difference, but neither would a 9800X3D.",Neutral
AMD,Wow 🤦‍♂️,Positive
AMD,"APU with 3D V-Cache would be fire tho Also weren’t 200 and 400 the APUs, at least during the last 2000 and 3000 Zen and Zen+ lineups?",Neutral
AMD,"No, it wouldn't. There is no G, and the ""100"" number has no history of being an APU. That's two separate signals that it would most certainly not be an APU.  I have a Ryzen 3 3100, the last truly great Ryzen 3, along with the 3300X. Both of those have PCIe 4.0 and 16mb of L3 cache, while the Ryzen 3 4100, has PCIe 3.0 and 8 MB of L3 cache.  Even the Ryzen 5 5500 only has PCIe. 3.0 and ...I think maybe 8 MB of L3 cache.",Neutral
AMD,"Keep an eye on eBay, I snagged mine two months ago for $160, Bent pens none broken I spent about 20 minutes with a razor blade and a mechanical pencil straightening them. Big upgrade from my 5500.",Positive
AMD,"I have one in one of my computers, quite a capable little unit. Hits 4.6 without trying. IGP is good enough for a media box, both instreaming and in light gaming.",Positive
AMD,Dual CCD x3D would be awesome.. 5950X6D lmao,Positive
AMD,"Same CPU. It offers the same thing and is not an extension to the platform.  Whether they just lower the price of the 5800X3D or they re-release it as a 5700X3D to quietly lower the price, does not really matter in the end.  Especially AMD is known for lowering the prices. The 2700X launched at 330 and 2 years later it was 130 on sale on prime day. But that didn't make it a new CPU or an extension to the platform.  I would consider the 5600X3D an extension though, as it offered something that was not offered before (even though it is a cut down 5800X3D). But it was not retail (with a few small exceptions), so it arguably doesn't count.",Neutral
AMD,"If you look at real-world benchmarks, it's negligible.",Neutral
AMD,unless a newer bios has security patches,Neutral
AMD,They are never cheaper than the already available Ryzen 5000 SKUs  That's the point. This does literally nothing.  It was the same thing with the Ryzen 5600 GT (or the XT SKUs). Re-releases of the same CPU for the 5th time like that do nothing.   But people love to blindly fanboy over AMD.,Negative
AMD,Fair enough.  But I’d like to see an APU done up as an X3D part though.  5700GX3D.,Neutral
AMD,"I was using a 5600G in my home server for a while after retiring it from a desktop, though I've now found and upgraded to a cheap 5950X. It was pretty satisfactory while I used it",Positive
AMD,5950x9D,Neutral
AMD,"I disagree, it is an extension as it's yet another product available for the platform.   When they are out of production it makes a larger difference as this does genuinely extend the availability of similar cpus for that platform extending it's commercial life as people can still pick up new cpus for it, even if performance is pretty close in comparison.",Neutral
AMD,Have you actually compared the prices of the 5800x3d and the 5700x3d like ever?,Neutral
AMD,"What is this 'cheap 5950X', internet stranger?  ha ha ha I love it!",Positive
AMD,"Nah, 5950X10D for 10 year anniversary. :D  But for real- AMD teased a 9950X3D dual CCD, then gave up on it as they fixed the scheduling issues present since the first dual CCD x3D cpus in 7000 series/EPYC Milan-X. They even claimed a limited SKU but didnt.  By now, probably all the good 5950X/5800X3D CCDs have probably been used up, but a limited run of 5950X3D or 5950X3D(2) would be such a boon for AM4 enthusiasts like me.  I moved on to 9950X (non X3D for my workload) delidded and watercooled, but god damn would I love to see a AM4 revitalization.",Neutral
AMD,"I don't mean that the 5950X is ""cheap"", but that I personally was lucky enough to find an amazing deal on one",Positive
AMD,"Looks very clean mate. I always tell people that dust, pets, smoke and vape juice are killers of computer systems. If you maintain your computer well it will last you upwards of 10 years if not longer. Keep your PC clean and it will serve you a long time!",Positive
AMD,"Meshify C is such a great looking case, I've still got mine even though everything inside has been replaced at some point.",Positive
AMD,"6600 series are goated, got my 6600 Challenger model last year and it’s still tanking modern games like a champ. Even Dying Light The Beast runs fine at high settings.",Positive
AMD,If only 6600xt had 10gb vram i never would’ve dreamed of upgrading,Neutral
AMD,Theres nothing wrong with those specs. Just pair it with a nice monitor and you're in for a good time with just about any game.,Positive
AMD,Fellow Meshify C user,Neutral
AMD,Your build and its history is *very* similar to mine - I even have the Meshify C! I bought a B450 motherboard in 2018 and started off with a Ryzen 5 2600 and an 8GB RX 580. That held me for a while before my 580 died during Covid. I managed to get my hands on an RTX 3050 for a somewhat reasonable price at the time before I eventually sold it after upgrading to an RX 6650 XT. I also upgraded my 2600 to a 5600 around 2022 and it’s still going strong (also using a Wraith Prism I got from a buddy). I just recently upgraded the 6650 XT to a 16GB 9060 XT. The build is really putting in work and AM4’s longevity is no joke.,Positive
AMD,Its much better than my 480p machine,Positive
AMD,What’s going on with that cooler for your GPU? Looks like some weird pitting,Negative
AMD,"When I got the Meshify C, I really liked that it was a shorter case. Then graphics cards started getting more power hungry and bigger. Now it's limiting the cards I'm able to buy. Still a good case though.",Positive
AMD,better than my current pc,Positive
AMD,"That is more than fine! I use 5600 and 6650xt for 1440p gaming and it's fine. It all depends on what you play. If you ignore UE5 slop, I consider it 1440p gaming pc.",Positive
AMD,Aged like fine wine! Looks great!,Positive
AMD,"slap a 5700X3D and a 9060XT in there, and it will live forever",Neutral
AMD,"Mine is Ryzen 5700x. MSI B450 Tomahawk, and Sapphire RX 470 Pulse, and the 5700x was my most recent upgrade after like 7 years from 1500x ... all because of Win11.",Positive
AMD,"My man, treat yourself and upgrade from the stock cooler. Despite its name, the Wraith Stealth is loud as shit lol",Negative
AMD,"Mine started as a 2600x + RX580. Now its a 5800x3D and 3080FE. No plans to upgrade anytime soon. Even then, since its an X470 and limited to Gen 3 PCIE, not sure if I did upgrade GPU what I'd go to, so hoping to make it to AM6 before I upgrade. Had a chance at a 3090FE for $600 two years ago that I should have jumped on. They sell for more than that now, and the 24GB would have kept me going for some time, while the 3080fe has dropped in price since, so not worth the price difference now.  I have 2 other AM4 builds, one on my TV: AB320 with a 5600x and 6700xt, though it is barely used since getting a Steam Deck years ago, the other in SFF that i use for browsing/light gaming and my kids use for Roblox here and there: B350 with a 5600gt, APU only.  AM4 was/is amazing.",Neutral
AMD,"AMD has really great longevity , my rx 570 i bought it from someqhere its been mined and its still good",Positive
AMD,Nice. I built this almost exact pc 3-4 years ago and its been a very stable reliable build. Runs everything well enough. Can do 1440p most of the time.,Positive
AMD,"I've got the same case as well! Started with 5600x + 6900xt, now upgraded to 9800x3d on b850m aorus elite, still the same gpu but planning to upgrade in the near future. The case is great, easy to work with and clean.",Positive
AMD,Similar situation here. Used to have R5 2600 + 1650. Now I have R5 5600XT + 6600XT ITX challenger. Hope to keep It for several years to come.,Neutral
AMD,"why is it on a oven plate in the first 2 pictures lol, it has feet",Neutral
AMD,meanwhile I am still running on i7-2600K + 1660Ti,Neutral
AMD,"Ayeeee.. We have the same case I think, mine is white but there's no GPU just 6x 18TB drives 🤣",Neutral
AMD,"How is it so clean, I can never get rid of fine dust",Negative
AMD,Wow How is it possible that the GPU is also oxidized,Neutral
AMD,Solid computer! I love AMD!,Positive
AMD,That machine is my dream pc rn lol,Neutral
AMD,I got a 5800X3D and a 6900XT - the latter of which I bought on sale recently (before 9000-series launch) for $450. I basically have the flagship tier of everything you have.,Neutral
AMD,I still have the same case! Love fractal,Positive
AMD,Still looking good,Positive
AMD,What is about PCs out in the sun that makes it look so good?,Positive
AMD,"I upgrade a couple years ago, I had a 1600af and a 1070, now a 5600 and 4060. Wish I made more money to upgrade but it handles things surprisingly well. Hell my laptop i got used is better in the CPU, and has the same GPU.",Positive
AMD,"I'm still using a Meshify C too. Built my PC in 2019, 3600+5700XT. Have since upgraded to a 5800x3d and managed to cram a Powercolor Red Devil 6800XT in there. Case size is a bit of a limiter these days",Neutral
AMD,Ehhh... fractal case. Very underrated brand imo. Just a little small for new cards!,Positive
AMD,I have the same case! I'm sticking with my 5800X3D for awhile.,Neutral
AMD,Meshify C gang ✊✊,Neutral
AMD,"Ahh a fellow Meshify C mini enjoyer, take my upvote",Neutral
AMD,"I dropped a 5600x3d (the microcenter exclusive chip) into a friend's a320, AM4 really is the GOAT",Neutral
AMD,love the looks of a  wraight prism cooler have it on my 5700x3D,Positive
AMD,Give it 5700X3D or 5800X3D if you can find one (probably used) and 9070XT and let it go on adventures with you for another few years.,Neutral
AMD,Great case.  Live the Fractal Meshifys.,Positive
AMD,Those Fractal Ion PSUs are quite underrated,Neutral
AMD,"Dang we got almost similar specs. I got a b350-M l, 5600, and I  got a 6800 gpu a few years back. It can handle 1440 pretty well but its showing its age with recent titles.   Biggest value upgrade you can do my friend: get a better cpu cooler. Just get a cheap one with a bigass heatsink. It makes a big difference on those temps. I got mine for like $25.",Positive
AMD,A fellow 5600 + 6600 user!,Neutral
AMD,"The Asrock 6600(XT) cards are AMAZING. Quiet, cool, overclockable, really good budget cards. And running 3200MT/s on four DIMMs is a good job, well done there. I literally just recieved my new NAS mobo in the mail, a B550. AM4 really is a goated socket.  Until my 4080S arrived (very late), I was running an RX6600 at QHD 180Hz. It was a really good experience, to be honest, and most games were >90FPS with choice settings, with a few (eg. REPO, Minecraft, DRG) getting up to 180.",Positive
AMD,Still a good build & looks like it's going strong 💪!  ![gif](giphy|N7iJLlhvaNm8Z4lKEq|downsized),Positive
AMD,"1080p machine? I have an extremely similar setup here, same exact case, same generation of CPU (5950x, a beast for CPU heavy workloads, but not much of a difference when it comes to gaming), and a 5700 XT instead of 6600 XT (should come out to roughly the same performance). I have no problem running 2k on this machine, sometimes even triple 2k.  Granted, I'm also not exactly making a habit out of chasing the absolute most recent and demanding titles at ""ultra"" settings. That said, whenever I do try a newer title, I really don't feel like I'm uncomfortably running into my hardware's limit all the time the way it was back when I was still running on an RX 580.",Neutral
AMD,"Wait til you jump to AM5   Going from a 5600x to a 7900x and eventually a 7800x3d was the best decision I ever made, general tasks are lightning fast and I got a free 13 percent performance lift on my 3080 10gb",Positive
AMD,vape juice is horrible. what i find strange is a lot of vapers won't believe that vape juice creates a residue that lands on things and starts to accumulate getting thicker and thicker. it kills electronics by covering up electrical traces and contacts. Also dust will stick to it.,Negative
AMD,10 years? I have a working 16 years old.,Neutral
AMD,"God I feel old, a 10yrs old PC in my eyes is a core 2 duo or core 2 quad.. The fact this is 7yrs old is mad.",Neutral
AMD,You think 10 years is a long time? I've had terminals in the industrial field running MS-DOS that only crapped out a couple years ago.,Neutral
AMD,My bulldozer system is still working 👍,Positive
AMD,"Agreed! I do my best to take care of the system, hopefully many more years of use to come",Positive
AMD,"I have the same case but in white. I like it, but… don’t at the same time? I’ve always been kinda meh about the looks of it. Like it’s alright but kinda always wanted something different. Cant quite put my finger on it.",Negative
AMD,"Fell in love with mine when I first bought it 4 or 5 years ago, and I’ll probably buy another newer model for my next pc. Fractal in general, I usually recommend them to anyone I talk to considering building a pc",Positive
AMD,"I use my RX6600 to play older games in 4K on my TV. Runs decently well, I have to reduce details...",Positive
AMD,"circa 2022 i kept saying the 6600 was the best value and it kept getting better and better.  it's alright these days, though it's not as cheap as it used to be. but in 2023-2024 it was arguably the best value video card you could get new. with maybe the A580 in a (somewhat) distant second.",Positive
AMD,"Exactly, I got the 6600xt two months ago. A massive upgrade from my RX580, but after buying some newer games I quickly realized that vram is no joke. It's a shame, because the gpu is fast, in some games fast enough for 1440p. But the vram is limiting.",Negative
AMD,"Sell the 6600XT for 140€, buy a 6700XT for 200€ and at 1080p you are good to go for at least 5 years more :D",Positive
AMD,"Perfectly fine for 4k gaming as well.  I've been running an older 3600 and its doing perfectly fine.  You just can't have every single setting on ""Ultra""..",Positive
AMD,"That's awesome, this platform really is great for longevity. How does the 5600 + 9060xt perform together ?",Positive
AMD,"Good eye, there's a small bit of corrosion on some of the heatsink fins - I bought the GPU cheap off a miner. It's been running cool and flawlessly for the couple years I've owned it though!",Positive
AMD,"Yeah I do like the compact size, but I figure I'll run into that issue if and when I upgrade GPU hahaha",Neutral
AMD,Thank you!,Positive
AMD,"I worry 5700X3D/5800X3D chips will always cost a premium aftermarket since many people will have the same idea to keep their AM4 systems going, but otherwise that is the plan!",Negative
AMD,"5700x3d/5800x3d is a really stupid idea when they're going for 250-400$ USED, and are being faked. When even very budget am5 cpus outperform them for brand new. 5800x/xt is really the only good end-game am4 cpu option.",Negative
AMD,Hahahaa I've considered upgrading but with a high airflow case the system is pretty quiet and runs fairly cool too. I'm a big fan of the look of this cooler - but if I find a good deal I might just upgrade,Positive
AMD,"He is using the Wraith PRISM now. I've used it with a 5600X in ECO with +200MHz PBO and the fan never had to ramp even with long game sessions.   That said, I would not argue that a $30-$35 dual tower isn't a great investment.",Neutral
AMD,"That's awesome, I scored a really cheap ITX AM4 machine off Facebook marketplace and have also thought about using that as a TV/console PC. AM4 really is amazing",Positive
AMD,"Yeah such a great case, looks really timeless and elegant.",Positive
AMD,Loooooool i dont know it was just outside so I thought maybe its better than concrete HAHAHA,Positive
AMD,Damn what do you need that much storage for👀,Negative
AMD,"Hahaha I just dusted it before this picture, that's why it's outside",Neutral
AMD,"It was used for mining before I bought it, snagged it cheap",Neutral
AMD,"Thanks for the kind words! Agreed performance on these cards really is great, I plan on keeping this system going for as long as I can!",Positive
AMD,"Yeah a lot of comments tell me they're running 1440p on this is GPU which is awesome to hear, but I don't plan on upgrading my monitors so it'll remain a 1080p system for now hahaha",Positive
AMD,"This happened to my brother, vape juice all on top of his GPU and his TV also died, took off the back panel and it was wet behind it, like a slime film, disgusting",Neutral
AMD,"That's almost unbelievable though. Do they not open a window? Do they just sit in a cloud of smoke 16 hours a day?  I have pictures of my 10 year old PC before an upgrade, with some of the most disgusting buildup of dust and cobwebs, and I've never had anything like that, layers of wet juice or smoky tar or otherwise.  [https://www.reddit.com/r/LinusTechTips/comments/1jgku6w/12\_year\_old\_dust\_with\_a\_case\_that\_could\_haunt/](https://www.reddit.com/r/LinusTechTips/comments/1jgku6w/12_year_old_dust_with_a_case_that_could_haunt/)  There it is lol. What a disgrace.",Negative
AMD,I've got a working Compaq Armada M700 (pentium 3 1ghz baybee) still working with the stock 11gb hdd 25 years later.  Amazing how far hardware can go.,Positive
AMD,In my experience the newer boards last even longer than older tech if you use a proper PSU and dust off.,Positive
AMD,"I built my 4790k 32g RAM, 980ti, ASUS Maximus Hero machine in 2014 and is still going strong, running 24/7 as my NAS. Absolute workhorse.",Positive
AMD,How'd you get him to work? My 16 year old only sits in the house and smokes weed.,Neutral
AMD,"FWIW, those OEM AMD CPU coolers are noisy as hell.   One of the no-brainer upgrades is buying an after-market cooler.   For example: I have a cooler from DeepCool which cost like €40. Can't remember the exact model but it came highly recommended by the *Hardware Unboxed* YouTube channel.",Neutral
AMD,I have the white one too. I can't imagine replacing it with anything but a Meshify 2 compact in white.,Neutral
AMD,> the A580  Is this a typo? Didn't you mean the RX580?,Neutral
AMD,I'm at 1440p and they pair together great.  [Hardware Canucks actually has a video](https://www.youtube.com/watch?v=NqRTVzk2PXs) showcasing how well older CPUs (including the 5600X which performs within the margin of error of the 5600) pair with the 9060 XT and the 5060 Ti.,Positive
AMD,"Fyi, I managed to fit a 9070xt Red Devil inside the Meshify. Together with the 5700x3d, it is an  astonishing capable and efficient system.",Positive
AMD,>Thank you!  You're welcome!,Positive
AMD,"I have a non XD, 5800 X and it's still super fast and efficient. It's going for $180 on Amazon now. Best upgrade for my B350, and technically not supported officially. It's allowed me to stretch the board for probably one more iteration.",Positive
AMD,Just some uncompressed 4K movies ;) I have a home theatre system and streaming quality is ass. Gotta seed your torrents,Negative
AMD,"see, proving my point, how is it unbelievable, where do you think the cloud of vape goes after it disperses. Even in a well ventilated area, it will still accumulate in the vicinity where the person is vaping.",Neutral
AMD,I have a Toshiba Satellite 4000CDT I occasionally use for retro computing. That thing is older than me.,Neutral
AMD,I'm sorry to say but you failed at parenting.,Neutral
AMD,Well the wraith prism is honestly not a bad CPU cooler. Copper base and 4 heat pipes. Funny enough it was made by Cooler Master. For a 5600 you can definitely run it under a reasonably quiet fan curve. But I agree if you spend like $35 you can get a peerless assassin and the thing can probably run at min fan speed the entire time.,Positive
AMD,"My only gripe with it is that GPU these days are so damn long, plus no USB-C ports on the case. Other than that, amazing case I love mine",Negative
AMD,"not a typo. the arc A580 was about on par with the 6600 in some games, though the drivers and reliance on rebar made it a distant second",Neutral
AMD,Awesome I'll give it a watch - thanks!,Positive
AMD,"Probably not a ""worthy"" upgrade for ~$200 dollars though. It'll be hardly noticeable while gaming, coming from a 5600x.",Negative
AMD,"actually 5800xt is 140-160 brand new as well on Amazon. Just a refreshed, newer and slightly-faster 5800x. If you want anything better budget b850m+9600x is the way to go.",Neutral
AMD,"It's only a joke, but sadly I do know some neighborhood kids where I used to live that did just that. Their parents were basically door mats.",Negative
AMD,"I had a wraith prism for a while, it was cooling my 6 core 3400G just fine. I wouldnt use it on anything above an 8 core though.",Neutral
AMD,"> Well the wraith prism is honestly not a bad CPU cooler.   Absolutely.   I don't believe I inferred that it's ""bad"". just that it's too noisy for its performance. That's all.",Positive
AMD,"Right, my fault that I wasn't clear but I wasn't recommending buying one. Just that the price is dropping on them, since when I purchased mine it was on sale for $350. I could not find 5800X3D for sale currently unless severely overpriced but a 7800X3D is going for $350 currently.",Negative
AMD,"I wasn't clear in my comment but wasn't actually recommending it, just using it for comparison. See my other reply to the same topic.",Neutral
AMD,"Oh trust me, I live in a country where that is very prevalent. Kids become parents way before they're ready to even look after themselves. It's disgusting.",Negative
AMD,"As someone from Central America, I 100% agree.",Neutral
AMD,Budget Ryzen3 when?,Neutral
AMD,"Never. 8400F is 100 usd, it's practically ryzen 3",Neutral
AMD,"This post has been flaired as a rumor.   Rumors may end up being true, completely false or somewhere in the middle.  Please take all rumors and any information not from AMD or their partners with a grain of salt and degree of skepticism.",Negative
AMD,If it's INT8 it should work on RDNA2 onwards,Neutral
AMD,Hopefully AMD will release this officially for the 7000 cards soon.,Positive
AMD,Ancient Gameplays got it working    https://youtube.com/watch?v=iOndUPO9_NE,Neutral
AMD,Fsr4 at similar fps to fsr3 quality or native without the shimmering / aliasing... Sign me up,Neutral
AMD,What's the point of FSR 4 on rtx 3000?,Neutral
AMD,[Results on a RX6800 (FPS included)](https://imgur.com/a/41t9yAD),Neutral
AMD,"Yep, I got it implemented in decky framegen via optiscaler, no weird proton version or fp8 hack needed. You can even use on steam deck, though it’s still somewhat heavy:  https://github.com/xXJSONDeruloXx/Decky-Framegen/releases",Neutral
AMD,RX 6000 series ?,Neutral
AMD,"Yeah and now we also likely know why AMD wasn't going forward with this on RDNA3. While the quality is good, the performance impact is significant. It's hardly any faster than native full-res rendering.  What would actually be needed for older GPUs is an even simpler ML model. That would probably result in a quality somewhere between FSR 3.1 and FSR 4.0, but with less computational cost.",Negative
AMD,Would this mean that a 7900xtx would smash a 9070xt by a good margin?,Neutral
AMD,"I checked it out in FFVII Rebirth.  It's worse quality than XESS Currently: Static-y, shimmering, unstable.  BUT  Much more fine detail is preserved (Both 100% and 66% scale)  at the SAME PERFORMANCE as XESS.  FSR3 in FFVII Rebirth sucks. Tons of ghosting, I straight up refuse to use it, so with some tweaking it might actually be usable, and definitely will be better than XESS with a little work.  All on a 6900 XT.",Negative
AMD,"I tested it on a RX6800. shadows were noisy, FPS went from 100 to 80 (FSR3 vs FSR4) in Cyberpunk 2077 @ quality settings and 3440x1440. I will stick to using XeSS in Cyberpunk. use DLSS swapper to keep whatever you use up to date.  [Here's the results ](https://imgur.com/a/41t9yAD)",Neutral
AMD,Can we have this for RX 6000 series?,Neutral
AMD,Can anyone compare this with dlss 4 in darktide?,Neutral
AMD,I got me a 3080 12gb. Be sweet if it worked on mine lmfao,Positive
AMD,"I thought using FSR4 on 7000 series and lower cards didnt get better performance, if anything games ran worse?",Negative
AMD,"Took only one month for a random user to make it work, AMD is a joke.",Negative
AMD,Do you mean vk khr shader float16 int8 extension?,Neutral
AMD,It's not good on RDNA 3—only a 2-3% performance increase in balanced mode. It should actually decrease performance when used as a filter at native resolution. You need to run in performance mode to see any performance increase.,Neutral
AMD,"it does work on RDNA2, check r/radeon lots of people are already using it on windows",Neutral
AMD,Technically 7nm Vega products (Renoir/Cezanne/Radeon VII) should also be capable. They also have dp4a support.,Neutral
AMD,It does! I tried it and only got -10% performance penalty when switching from 3.1.5 to 4.0.2  Only downside afaik is it only works properly on linux atm  Edit: forgot to mention I'm using an rx6700 xt with optiscaler,Neutral
AMD,"Update: I have tried on a RX 6800 + 9900K on W10  Here's a sample pic showing FSR4 Performance, optimized settings with RT reflections, producing 80 fps at 1440p at the river docks [https://imgur.com/a/JI3AQl2](https://imgur.com/a/JI3AQl2)  *(Quality and Balanced look even better of course but this is to show that even Performance which is enough for RT still looks great)*  It's a bit of a pain because at the moment you need old drivers or take some files from those and swap them in the latest drivers, otherwise you get blocked by games saying it's too old  **I do not recommend** attempting to fiddle with drivers because although it worked I got some black screens. Let's say it's experimental for now, not practical  That nevertheless gave me enough time to try FSR4 in Cyberpunk 2077 for a while and it looks GREAT  Better than XeSS, almost no shimmering, much better details in the distance, even down to Performance it looks great at 1440p. I was able to get **60+ fps with optimized settings and RT Reflections ON ... at the (heavy on perf) market near the start ... on a RX 6800 with an old CPU !**  **117 fps without RT**  **118 fps with RT + FSR FG** (nukems mod via Optiscaler)  I haven't tried with XeFG yet but it should be even slightly more refined since XeFG is a bit superior not in perceived performance gains but in smoothness consistency  This level of graphics quality on a such a low end system relatively to those settings is unprecedented on RDNA2  So yeah FSR4 on RDNA2 is really, really worthwhile, the ""not enough performance"" claim from AMD is BS, because even a 6800 with a weak CPU is enough to benefit from FSR4  Next I'll try on my 6950XT, I know in advance it'll be awesome <3  Note that despite the software displaying 4.0.2 this leaked INT8 version is in fact an older model, like not even real 4.0.0 but something preliminary, so if the port went official we would have even better performance and picture quality with newer FSR4 models  **DO IT AMD ! :D ... fingers crossed this will be included in Redstone**",Positive
AMD,Probably when the Redstone thing comes out they’ll release it for 7000.,Neutral
AMD,"Been using OptiScaler to emulate FSR 4.0 and spoof DLSS on an Asrock 7900 XTX Creator. Can still be hit or miss, but my experience has been fairly stable.",Neutral
AMD,Right now it’s significantly slower. What I saw was a 10 to 15 percent boost over native  on 7000 series for balanced.,Neutral
AMD,Proof of Concept kind of thing. It's like what if you run DLSS 4 on RDNA4,Neutral
AMD,there's always far cry 6 only supporting fsr,Neutral
AMD,if you want to use framegen on an rtx 3000 fsr is your only option. At least with fsr4 it'll look better than with fsr3,Positive
AMD,"FSR 4 AA looks significantly better than DLSS 4 AA in many games, especially those that use a lot of dithering (i.e. almost every UE game).",Positive
AMD,"At the end of the day, FSR and DLSS are upscalers using machine learning/AI models so it's not a wild concept the technologies can be interchangeable.",Neutral
AMD,"None, it will be slow. Maybe just show that it's possible.",Neutral
AMD,"For me, I could use it to get a little boost in FPS while at the same time possibly fixing some shimmering issues I get in some games even with Native. In an Ancient Gameplays video he showed some image quality benefits in Cyberpunk using FSR4 on RDNA3 over Native.",Positive
AMD,Is this on Windows or Linux? Ppl on here saying Windows has ghosting issues with RDNA 2,Neutral
AMD,On RDNA 3 cards at 4K quality the difference between FSR 3 and FSR 4 is very small. 5% difference in performance. Only when doing 1080P Performance mode is it not as good. But seriously next to nobody is running 540P res upscaling anyways but its still better than native even at that low of a res.  Most people upscale at 4K and 1440P which FSR 4 has very little performance impact. So this benefit of running FSR 4 on RDNA 3 is incredible and the performance is still great. For instance I get 190 FPS running FSR 3 at Cyberpunk 1440P and with FSR 4 now im getting 180 FPS. Not that big of a difference with a massive boost in image quality.,Neutral
AMD,"As an RX 7900 XTX user, I am not concerned about the performance penalty when using FSR4. I'd only want FSR 4 for image quality purposes. ""...Hardly any faster than native full-res rendering"" is perfect if an RDNA3 user aims for better image quality compared to FSR 3.1.",Neutral
AMD,"Idk the performance uplift for me at 4k was significant, cyberpunk maxed out no rt I get about 40fps on my 6800 XT, with FSR4 I was capping at 60fps with vsync, more than acceptable. This was under windows 10.",Neutral
AMD,"Did you check in Windows or Linux? People are saying that Windows isn't suitable for RDNA 2, and they're getting better results in Linux.",Neutral
AMD,"It already works for it, that's the very topic...",Neutral
AMD,Just use DLSS? If you're talking about FSR frame gen you can already use it on 3000 series if the game has it,Neutral
AMD,"That was the earlier Linux hack\~y experiment's issue  This is different, it's a genuine converter made by AMD that works on Windows  The performance uplift seems comparable to XeSS  Plenty of comments apparently mistaking the two events for the same",Neutral
AMD,"On 7000 cards it increases performance, just not as well as it does on the 9000 cards.",Neutral
AMD,"Lol he just compiled AMD code. If they wanted it out for 6000/7000 series cards, it would be.",Neutral
AMD,Why pay people to do something the community will do for you and then you can learn from and do and get credit for while paying less people?   Lisa's a genius at not paying people.,Negative
AMD,"I got it working on my AM4 rig on Linux. For some reason this sub keeps deleting any picture I post though (and why does the format have to be .gif?). Anyway latency cost on my 6950XT sits at around 2.6ms in Oblivion Remastered. It looks better compared to XESS in the distance, much less shimmering. At least at the balanced preset, quality preset gives me crazy shimmering near the edges of my screen for some reason.  EDIT : works perfectly in Cyberpunk2077.. I feel like quality and performance are good enough that AMD could do an official release, if they just mention performance might be less compared to FSR3.x I really see no issue. Anyway, genie's out of the bottle now, your move AMD ;).",Neutral
AMD,"It will be a good upgrade for steamdeck and other handhelds.    I think AMD may be planning to launch this soon, perhaps when redstone comes out.",Positive
AMD,"From what I gathered on Windows it has a lot of ghosting, but not on Linux (maybe to do with the drivers?).",Neutral
AMD,"I'm using it in Cronos the New Dawn right now (on an RX 6600). Same performance as XeSS (which makes me wonder why they didn't release it for RDNA 2/3) but it's shimmering and ghosting incessantly even when standing still. It's bugged for now on Windows on RDNA 2, so If you have an RDNA 2 GPU, don't bother testing it on Windows. I expected to see a cleaner image at least when standing still, but it's even worse than the worst FSR implementations",Negative
AMD,Only Vega20(Radeon VII) supports INT8 - DP4a.,Neutral
AMD,"Technically they can, practically - 85ms on Linux (5600G).",Neutral
AMD,It works on windows too,Positive
AMD,Is it worth it?  I also have a 6700xt  So we are getting better quality image at the cost of performance?,Neutral
AMD,How does the image quality compare to native in that scenario?,Neutral
AMD,FSR FG doesn't need FSR  They are independent,Neutral
AMD,"Some games unlink the upscaling and frame-gen, allowing you to use DLSS upscaling with FSR frame-gen. For other games you can use the dlssg-to-fsr3 mod.",Neutral
AMD,XeSS 2.1 fg works on most cards. It was pretty good on my 2080ti. But FSR is in more games than XeSS,Positive
AMD,Do rtx 3000 users not have dlss framegen?,Neutral
AMD,Unless they can use tensor cores for int 8 acceleration.,Neutral
AMD,"Interesting, A question   So this only affects Upscaling?  So if I have a 165hz/ 1440p Monitor  To see any benefit from this i would have to set my screen res as 1080 for it to ""Upscale"" to 1440 right?  This comes to my question, On my 7800XT with optimised settings, I can run most games at high quality at 1440p and still get around 100FPS in quite a wide variety of games, I was never a fan of FSR3 because it looked kinda wacky and i never seemed to notice the performance increase   So by doing this method of FSR4 am I going to see way better performance AND the benefits of 1440p? Or is it better to just stick with native? Im so confused",Neutral
AMD,woah its working on the 6000 series too?,Neutral
AMD,Oh yes!!! Now that's some good news!!,Positive
AMD,"Not in thw state it is rn, almost 0 perfomance gains from native resolution, it makes no sense",Negative
AMD,"No ghosting with 2 years old Windows driver **23.9.1** but the issue starts with the next version up to current  So it's not that it *can't* work fine on Windows RDNA2, it's a driver regression issue  Either AMD will fix the current driver, or ppl will mod it specifically for this purpose ?  We'll see  In the meantime there's Linux indeed",Neutral
AMD,"I can't confirm if what your saying is true but it could be because FSR4 on linux is the FP8 model (running with FP8 emulation).  Afaik FP8 is the ""final"" model. Int8 was developed first and then AMD developed the FP8, probably because of image quality and/or performance.",Neutral
AMD,I heard ppl say because of some kind of emulation windows can't do but Linux does,Neutral
AMD,"MPO issue probably has to do with how MPO functions, and perhaps MPO is not enabled yet on linux or maybe it is and its simply functioning better, i do know linux was working on MPO couple of months ago.",Neutral
AMD,Obviously to make people upgrade their gpus.   If they released it instantly they'd get even less sales of their already small amount of gpus shipped,Negative
AMD,"Oh huh. I guess there's either an INT24 fallback, or the compiler generates one for that hardware then. Because I've screenshots on Renoir.",Neutral
AMD,Haven't tried it on windows but I heard reports it has flickering/ghosting issues on RDNA2 but works perfectly on RDNA3,Neutral
AMD,Where to download this from?,Neutral
AMD,With terrible ghosting issues.,Negative
AMD,"Honestly I think its a no brainer, it only cost about the same as dp4a xess which i've been using anyway while having far superior image quality",Positive
AMD,"Depends on the game. If native uses TAA, it could look better as TAA tends to be a little blurry.",Positive
AMD,"nah, 4000 and 5000 only",Neutral
AMD,"Yep, buggy on windows however but fine on Linux.",Neutral
AMD,"It's still a bit rough apparently (gonna try on my 6800 and 6950XT asap)  **EDIT: the ghosting issue on RDNA2 is due to drivers, apparently it works fine on some old drivers like 23.5.1 or 'newer' like 23.9.1, so it's not DOA ... either use those 2 years old drivers or wait for someone to come up with a fix for modern drivers**  After all it's a leak, but solid proof it's working, you'll find videos of that on YT already  Some ppl even running it on RDNA1 lol  I'm guessing this INT8 converter exists because they needed to develop something like that for the PS5 Pro's Amethyst FSR4 backport planned for Q1 release, last I've heard  And PS5/Pro hardware is basically custom RDNA2 with some RDNA3 changes and extra tweaks  So it's logical, I honestly just didn't expect it to be 'releasing' for RDNA2, in fact I already had lost hope for RDNA3 lol  Now I'm waiting for Optiscaler team to make this impeccable  Picture this : soon we will be able to casually enjoy both XeFG and FSR4 on 6000 series ha ha :)",Neutral
AMD,"That's just straight up not accurate. Functionally equivalent performance to XESS on a 6900 XT, both as Native AA, and 66% res scale (Tested in FF VII Rebirth.)  With work in optiscaler to configure it appropriately, it means better quality and frame rates, for not that much more cost. (95fps FSR3, 87 XESS and FSR4.)  I already use XESS since it has less ghosting and shimmering than FSR3, in FFVII Rebirth, as my specific example.  [Edit] For anyone who doesn't already know this: Performance hit is going to vary from game to game. Some better, some worse.",Neutral
AMD,"The problem is specific to the INT8 model on Windows on RDNA2. The ghosting isn't present when using the INT8 model or FP8 model on Linux with RDNA2, although FP8 on RDNA2 performs like crap and is totally unusable.",Negative
AMD,"WMMA emulation is working on Linux, but missing on Windows.  It'll still run without it, but you get artifacts.",Neutral
AMD,What is that,Neutral
AMD,"Hmm. I tested it on my old Vega laptop, and it simply doesn’t work in-game, the screen keeps flickering black and showing severe artifacts.   However, it worked perfectly on my 7900XTX after following the Ancient Gameplays tutorial.",Negative
AMD,"I believe it's emulation, which is pretty bad, like trying to run Xess on very old GPUs without DP4a.",Negative
AMD,"I mean +10-15% perf and might look a little better in some cases sounds like nothing but win, especially for singleplayer games.    I've been meaning to get around to a CP2077 playthrough since I bought a 4k OLED screen, but 4k is a lot of work for a 7900XT.  An extra 10-15% would sure help there.",Positive
AMD,time to see how it does on the steamdeck now...,Neutral
AMD,That's because it's a 6900 XT. The Steam Deck struggles to actually gain much from compute-heavy upscaling like XESS because it's so limited. You see more upscaler gains on high end chips like the 6900 XT.,Neutral
AMD,TIL a 6900xt is quasi comparable to a steam deck APU.  You have to comprehend when you read you know.  Like before you reply.,Neutral
AMD,Has anyone checked if the same issue occurs with RDNA1 or pascal? Apparently this leaked FSR 4 version works on all GPUs like FSR 3.1 does.,Neutral
AMD,how do we get it installed on linux?,Neutral
AMD,"RIP my 6950XT, guess I gotta switch over to Linux come next month",Neutral
AMD,"Yeah that's what i meant, do you think there's a way it can be used on windows? The emulation i meant",Neutral
AMD,XeSS has a dedicated INT24 fallback which older GPUs rely on.,Neutral
AMD,"Wait, 10-15% when compared to fsr3, not native.  Wrt native,  it is giving a good boost",Positive
AMD,Completely agree. I was just commenting that it won’t be as much as FSR3 or FSR4 on a 9070XT.,Neutral
AMD,No? They can already enable FSR FG,Neutral
AMD,You can already run FSR Frame gen on Nvidia cards. I can turn it on in The Finals and I have a DLSS Transformer on DLAA enabled tho it feels awful so I never enable it. 70 (locked) FPS is more than enough for me (sweet spot between power usage and latency. I can probably live with 60),Neutral
AMD,Yeah IME fsr3 is never worth it on the steam deck nor is xess. The perf gains are at best a few frames for significant visual downgrades. Given fsr4 is more computationally heavy I doubt it's even useful for steam deck but there may be specific use cases,Negative
AMD,"Considering they're both RDNA2, Yeah, they're pretty comparable.  Architecturally, They have the exact same disadvantages when running FSR4.  Maybe you should do some reading before you reply.",Neutral
AMD,"Got a crash with a 1080Ti in the 2 games I tested, both natively and via optiscalar.  They were Tlou1 (native) and Wuchang (optiscalar). I know 1080Ti does support INT8, but as for why it won't run FSR4, nobody knows. Could be a driver issue but they will never fix it.  Apparently it does run on a 1660, could be that it requires true async compute.",Negative
AMD,Works on all gpus with dedicated ai hardware as of now,Neutral
AMD,"Optiscaler is your best bet for both.  I'm not entirely sure of the process for the FP8 model, but INT8 should be easy to do with the latest Optiscaler, just copy and replace the file in the OP with the one in the Optiscaler directory, copy the contents of the Optiscaler directory to the directory with the main .exe for your game and run the setup_linux.sh script.",Neutral
AMD,Why?,Neutral
AMD,"It would probably have to be included in the drivers, so it would be up to AMD to actually implement.",Neutral
AMD,"Ohhh, that definitely changes things!",Neutral
AMD,Based on what I have seen at least some games it gives barely any boost vs native,Neutral
AMD,They aren't. There's minimum requirements for a reason.  4CU isn't the same as 32 or 48 is it?  Architecturally doesn't mean a lot bud.  All that means is it can run it.  It does NOT mean it will benefit.  Who cares if it can run it if it reduces FPS below native?,Negative
AMD,thank you,Positive
AMD,"Windows 10 support ends next month, so I'm gonna have to upgrade to Windows 11. But I have been considering giving Linux a try on a spare 500gb SSD drive I have.  Edit: Downvoted for Linux, classy",Neutral
AMD,"Can you share the names?  This weekend, I can probably try them out in both linux and windows using my 7800XT  Another question - for these games, was XESS also behaving the same?",Neutral
AMD,"Switch bro. I switched to cachyOS and games like KCDII and helldivers have better 1% than windows (you'll need some tinkering, like PortProton for non steam games so keep your windows in the meantime to play.",Positive
AMD,RDR2 runs significantly worse on fsr 4 than native on my 7800xt,Negative
AMD,LMAO. an article about a reddit post,Neutral
AMD,Saving you a click:  https://www.reddit.com/r/radeon/s/8NLhxTwb9y,Neutral
AMD,"Somehow, Vega 56 returned",Neutral
AMD,"> Looks inside  > Just an increase in power limit pretty much  Slow news day, eh?",Negative
AMD,Still remember modding my Radeon 9500 to a 9700...good to see something similar decades later.,Positive
AMD,Now we get a reddit post about an article about a reddit post.  I cant wait for the article about this post,Neutral
AMD,Should have cut out ad-filled middleman and linked directly to original Reddit post.,Neutral
AMD,"Best part is that the old school Ati Radeon 9800 (non-pro, and maybe even some 9700 models) was flashable to 9800 XT bios too https://www.overclockers.com/forums/threads/how-to-softmod-your-9800np-pro-into-xt.270479/",Neutral
AMD,"Could also flash a RX 5700 with a 5700XT BIOS, they have almost identical components. AMD even confirmed it was possible, but you were doing it at your own risk. While most people said it worked, it also had lead to high temps and noises.Probably wasn't good for the longevity of the card either.",Neutral
AMD,"Its an older meme sir, I was about to updoot it.",Neutral
AMD,> https://www.reddit.com/r/radeon/s/8NLhxTwb9y  with adjustments to my voltage and memory clocks.,Neutral
AMD,What a fascinating development.  Someone should write an article.,Neutral
AMD,"thats awesome, never forget ati",Positive
AMD,"Don't worry, we'll get a second article from WCCFTech or Tom's reporting on VC's post about the Reddit thread, which will then be submitted here.",Neutral
AMD,No need when ai can do it for free.,Neutral
AMD,Next: Gamer Nexus making a video about WCCFTech linking to VC which linked to Reddit post about getting free 25% boost in overall performance,Neutral
AMD,"I mean, that’s what X3D CPUs really excel at.  Not surprising that marketing catches up",Positive
AMD,I wish I could hit 1000 FPS with it lol,Positive
AMD,In the menus or what,Neutral
AMD,Hey I get about 1200 fps on java Minecraft!,Neutral
AMD,"I can play Doom 64 at 1000 FPS with my 5700X3D, and World of Tanks at around 600 FPS",Neutral
AMD,Too bad modern GPUs can't even run games at 1080p60 maxed out... This headline means less than nothing until that changes. UE5 really fucked gaming,Negative
AMD,"Not gonna read it but let me take a guess, in minesweeper?",Neutral
AMD,Now show me the 1000Hz monitor to make it mean something.,Neutral
AMD,and twitchy little kids will lap this up.,Positive
AMD,"9800X3D and 5090 to play Counter Strike or Valorant at 1000 FPS, there must be dozens of people doing that",Neutral
AMD,Loaded up Halo 1 and got 900fps uncapped. I really should upgrade.,Positive
AMD,Lord knows FarmVille loves the frames.,Neutral
AMD,Unreal Tournament 2003 maybe,Neutral
AMD,can anyone see a 1000 FPS for gaming?,Neutral
AMD,Stardew Valley 1000 FPS experience :),Positive
AMD,"I mean, the original doom probably could",Neutral
AMD,"Might be cool for professional grade gamers but for the 99% of of us, this is a useless target. Also if your paying over $700 for your GPU and gaming at 1080P, your wasting your money.",Negative
AMD,1080p 24” OLED when?,Neutral
AMD,BuT tHe HuMaN eYe CaN't SeE pAsT 60 fPs.,Neutral
AMD,"Interesting... I recently asked ai to give a fps number we would need for emulating crts phosphorus flow effect and fade, plus line scan. You know so we can play old school games the best lol.  It said we need 1000fps. Now I don't know for sure, but it seems likely around there. Would be cool if someone made a 4k/8k oled (or micro led) screen that simply had this feature built into the display so all you need to do is feed it the frame data. (though trusting manufacturer to not screw that up, good luck lol)",Neutral
AMD,I hit that with my I5… on some loading screens.,Neutral
AMD,if it gives me 1000 fps in iracing then ill be happy,Positive
AMD,BORDERLANDS 4  ![gif](giphy|GpyS1lJXJYupG),Neutral
AMD,* Disclaimer - Results might be closer to 10fps in Borderlands 4,Neutral
AMD,I still find 9800x3d expensive.  We need some competition.,Neutral
AMD,"5800x3d with a 4090 I get 1000+ fps in menus and average 700-900 in game. I don’t doubt if I had the newer chip it’d be pushing 1000 in game. So I actually don’t hate this marketing since it’s a valid claim. For 1080p esports gaming, their chips really are among the best at getting such high framerates, especially at their given low power usage.  But if you care about esports gaming you’ll mainly want to pay attention to 1% fps lows. Good news is that the lows also are helped out lots by the cache. I get 1% lows always above my monitor’s 390 hz refresh rate so I’m a happy camper.",Positive
AMD,yup its a beautiful thing,Neutral
AMD,"Hmm, i see barely ""outrage"" about it here unlike if NVIDIA did those claims, also there is not a single world where these CPU's could hit 1000FPS in CS2, not even remotely close and especially not while in an actual match, so its completely meaningless.  But its the classic marketing ""up to"" thing just like sales with ""up to 70%"", can mean anything from 1 to 1000 FPS, but its okay if AMD does it, not so much if others :D  Just complete marketing bullshit like NVIDIA with MFG enabled in benchmark comparison sheets to previous generations without MFG.",Negative
AMD,Getting 2000FPS on the original GTA Vice City and only using 1% of CPU power.   ![gif](giphy|FP7g0JkFYO4gK9o4vr),Neutral
AMD,My 7700X hits 1000 FPS… in CS Source :),Positive
AMD,Just make sure you don't buy our GPU as well...,Negative
AMD,Seems like something that is gonna turn into a class action lawsuit eventually because of verbiage,Negative
AMD,"More impressive, it plays Crysis, max settings @ 10 FPS",Positive
AMD,Pretty sure apex doesnt even go past 300fps xD,Negative
AMD,"*except in UE5 games, that engine will make all hardware look like it's only good for 720p",Negative
AMD,How are these lies legal,Neutral
AMD,Edit/Update: My apologies to those I have and potentially will offend/disturb with my internet joke.  Remove the fake frame insertion and AI upscaling...what do you have left in 2025? Approximately 37fps  These CPUs are profoundly overcapable for the current landscape lmao.,Negative
AMD,Not surprising that marketing *caches* up  I'll see myself out now,Negative
AMD,literally it does,Neutral
AMD,Well that would require playing Valorant or CS source.  Not too many other games out there that are *that* optimized.,Negative
AMD,I hit over 1000 in Minecraft with a 7950x3d (just for fun testing),Neutral
AMD,Only game I play that has 1000+ fps is osu. But that game is just a bunch of png images. I get 3000 with a midrange pc 😂,Neutral
AMD,I hit around 900 on 2xko,Neutral
AMD,Valorant low settings firing range,Neutral
AMD,"Oh wow, how many chunks render distance?",Neutral
AMD,Valorant uses UE5.,Neutral
AMD,it's nothing to do with ue5 and everything to do with publishers rushing devs and not giving them the resources to make finished products. Use your brain,Negative
AMD,"UE5 isn’t the problem, it’s the scapegoat.",Negative
AMD,I swear Unity must be paying for comments like this to show up under every Reddit thread that’s vaguely about modern game performance,Negative
AMD,Demanding games have always been demanding lmao. No one was getting 1000fps 10 years ago either,Negative
AMD,"It was inevitable though. Stronger hardware often brings less optimized games because devs can rely on the brute forcing that improved hardware provides.  When average people are okay with 1080p 30 fps still, there’s no reason for devs to optimize for 1080p 60fps or more if it won’t really impact sales.",Negative
AMD,Wdym? Valorant is UE5 and it can run on 1000fps even on amd,Negative
AMD,"I’ve been playing with UE5 editor a bit, I think it’s an amazing engine. I could make something that runs at 300fps, or 1 frame every 2 minutes (extreme photo realism) it totally depends how you use it. If you just throw in a bunch of extremely high poly assets into a scene , and max out the ray tracing setting of course it’s going to be slow.",Positive
AMD,What are we talking about here? 8gb 5060's?,Neutral
AMD,Randy Pitchford says stop being poor and run a Threadripper for UE5 titles.  He once worked a shoe shine in a mall to afford his first threadripper.,Neutral
AMD,"What are you talking about, I get 4k60 with my 7800 all day long.",Neutral
AMD,Probably aimed at esports titles.,Neutral
AMD,"Yes, this is for e-sports like valo cs2 fortnite",Neutral
AMD,https://www.tomshardware.com/monitors/gaming-monitors/tcl-demonstrates-4k-gaming-monitor-with-a-1000-hz-refresh-rate,Neutral
AMD,"show me a person with .125 ms reaction time and explain to me 8000hz gaming keyboards and mice lol.  input latency freaks go nuts for this stuff and it sells lol, they are just getting in on that segment of the hype train.  Every tiny link in the chain, right?",Negative
AMD,Most of the doom games are very well optimized actually...,Positive
AMD,"Actually a couple of years ago, some people managed to run Doom Eternal at 1000fps (with liquid nitrogen).",Neutral
AMD,"> if your paying over $700 for your GPU and gaming at 1080P, your wasting your money.  Unless your goal is frames and not resolution",Negative
AMD,"it isnt even for them, the difference between 240fps freesynced vs just 1000 fps is not there, and additionally uncapping your framerate is worse for input latency, you need some headroom so it might be even worse,  purely mathematically, the difference between 1000 fps and 240 is 3ms, added on top of basically base cost of \~15-20ms, and the fact that your monitor is definitely not 1000hz makes that difference even smaller, its negligible  edit: if you don't believe me watch the video i attached in the response below",Negative
AMD,24 ☝️🤓,Neutral
AMD,im pretty sure someone else has done the numbers on that.,Neutral
AMD,"Because these are not lies. In the games listed that I play(CS2, Valorant) I can get > 1000 fps with my 7950x3D playing at competitive settings, my peak FPS in valorant while in game is in fact 1700+.  A 9800x3D can get way more than a 7950x3D in those eSports title, so it's not wrong at all for eSport games.",Neutral
AMD,"There are no lies.  It’s actually what X3D are really good at. They run e sports Games at low resolutions and low settings with a fast GPU.  And if you do so, you’ll see 1000 FPS with X3D CPUs in some cases.  It’s actually what they’re best at.  Games that already run at high framerates typically benefit a lot from faster memory and/or bigger caches.",Positive
AMD,What's lie lol,Neutral
AMD,"Nah, in those cases and games tested you can actually get multiple hundred fps.   I get 1440p max settings still 280+ frames in Overwatch.. And thats a heavier game compared to valorant or league of legends.   Modern high budget games just tend to be shit.",Negative
AMD,Cache me outside how bout dat,Neutral
AMD,"If I'm not mistaken, while the original x3D cached up, the 9000 series caches down for better thermals.",Neutral
AMD,Microsoft Solitaire,Neutral
AMD,"True.  Regarding well performing games, Overwatch and Rainbow Six come to my mind too. Not sure if it counts, but Battlefield 4 can hit quite high framerates as well. Nowhere near 1000 though.  It's worth noting Overwatch has a 600FPS cap. I think the framerate could exceed the cap.",Positive
AMD,"It's not an optimisation issue   Many games have so much more stuff going on than Val/CS. So their CPU requirements will always be more demanding, which translates to lower fps.  So even if you had the entirety of Nvidia, AMD, and any 5 game studios of your choosing working on it like their lives depended on it - they couldn't get it to run at 1k fps on current hardware without gutting some of those features.",Negative
AMD,3D pinball must be running at 50000 fps then.,Neutral
AMD,does it actually get to 1k fps there?  only game i know that gets to 1k fps and it matters is osu.,Neutral
AMD,"Finally, I can play with 1000 fps in firing range",Positive
AMD,2,Neutral
AMD,"When I get home I will check out the settings and get back to you. I recently purchased a 5090 and have it paired with a 9950x3d. I also use process lasso to experiment with programs to set specific affinities for CPU parameters. If I leave game mode on it puts the frequency cores to sleep essentially disregarding lasso all together, but if you configure process lasso correctly and experiment with leaving the virtual cores on and then trying off you get a mixed bag of results. Some games get a massive boost from turning off virtual cores and some have no difference. As for in game settings I will get back to you later today",Neutral
AMD,"And the performance got mostly better with the switch, and the size on disk went down",Positive
AMD,As does Fortnite.,Neutral
AMD,"He meant ""good looking UE5 game"". Yeah Valorant uses UE5 but it looks like shit which is fair because its a competitive game.",Negative
AMD,Valorant uses UE5 in name only. All key UE5 features have been removed in Riot's variant of UE5 to make Valorant. They just upgraded to get more technical support from Epic regarding the latest toolset.,Neutral
AMD,"Compare to Deadlock and Overwatch, valorant look trash and got lower fps",Negative
AMD,"VaLOrAnT usES UE5. Just stfu, compare the graphics and scale of valorant to the UE5 games that people are complaining about",Negative
AMD,"It's not like Valorant is a graphically demanding game  Destruction? No. Small maps, there are effects but they are very bland",Negative
AMD,"It’s likely both.  A lot of the performance issues we’ve been seeing are likely a product of UE5 including tools that make it easy to make things “pretty” in extremely inefficient ways (i.e. if you know what you’re doing you can get extremely close to the same look with much better performance).  So artists make things look like they want, and nobody ever comes through and “fixes” things to actually be performant.",Negative
AMD,It's not always publishers it can also be lazy devs.,Negative
AMD,"Which is crazy, because unity is even worse most of the time lol   At the end of the day, they're both just tools. It's up the developer to utilize the tools properly. But Unity has a lot more poorly-made shovelware",Negative
AMD,"It's the tagline, they see it once and repeat it.   Then the next kid sees their comment, repeats it.   Ad infinitum",Neutral
AMD,"It's not like having a proprietary engine is a guarantee, just look at recent Capcom games. Seen people praise the Decima (Horizon/Death Stranding) engine saying it should be used everywhere, with a sample size of 2 studios as the only evidence it would work everywhere between any hands.",Neutral
AMD,"That's rather ironic, since everyone here seems to be praising Unreal Engine!",Neutral
AMD,"Valorant doesn't use any of the key UE5 features like Lumen, Nanite and VSM. So, it's not comparable to UE5 games that do use these features.",Neutral
AMD,"Plus, bigger studios have all the man power and skill to interface with the rendering pipeline in a way that can increase performance a lot. They just never get the time to do so.   Developers know how to make stuff fast. It just takes time, time that no one wants to pay for. They need to shit out content and then move on.",Neutral
AMD,can these guys just release a commercial 4k 480hz+ model already? i want one thanks.,Negative
AMD,"A faster mouse still makes you faster even with a slow response time.  Your mouse will be part of that response time.   So for someone with 150ms response time (among the best esports players)   A 500 Hz mouse will add 2 ms making it 152 ms   A 1000 Hz mouse will add 1 ms making it 151 ms   A 8000 Hz mouse will add .125 ms making it 150.125  On a game like CS2 where your inputs are timestamped. If both you and the enemy have the same reaction time but you have faster hardware, you get the kill. On a game like Valorant (as far as i know who doesn't use timestamps) you hope that that latency reduction brings you into the earliest 7.8 ms  server update window (128 tick server).",Neutral
AMD,"More Hz also means more motion fluidity, so it's easier to spot details when you move the mouse around fast.",Positive
AMD,"It’s not all about reaction time, it’s about fluid input. It’s been proven for over a decade that 1000hz usb polling rates can stutter on high refresh rate screens.",Neutral
AMD,"super high monitor refresh rates is about improved motion clarity, input lag is not a major selling point (for people that actually know what they're buying into).  i run 165hz and i still have trouble following some in-your-face movement in overwatch.",Negative
AMD,"Yeah exactly  Ive hit 1000fps in modern ports of those early boomer shooters like gzdoom and whatever the name of that one game with lo-wang cutting people up with katanas, kung fu and uzis was",Neutral
AMD,"Doom Eternal blows me away, no other game has worked well on phenomenally low specs.   My buddy's 750ti and Quad-core Athleon can run it at 1080p Low settings 30-45 fps, or 720p Low settings 55-60 fps.   That's incredible!",Positive
AMD,"Yo wthhh  Eternal has to be one of the most optimized games of the modern era, shame its successor is so unlike that, requiring RT and therefore performing infinitely worse",Negative
AMD,"Then it's a pointless goal, you're eyes cannot tell the different between 240 and 1000fps. Most people can't tell much beyond 144 besides placebo, but some professionals actually can.   The dude chilling at platinum can't see shit and just pretends it'll make them better.",Negative
AMD,"> uncapping your framerate is worse for input latency  This is untrue. Uncapping your fps will likely cause tearing, though idk what that looks like at 1000fps. However, by its nature, more frames means less latency.",Negative
AMD,"The difference is there, stop the cap",Neutral
AMD,That's like claiming the RTX 3090 is an 8K GPU just because it can play Minecraft at 8k  This is as bad as Nvidia's marketing calling the 3090 the first 8k gaming GPU,Negative
AMD,Lmao.,Neutral
AMD,"https://old.reddit.com/r/Games/comments/1nd1dpz/former_ms_engineer_dave_plummer_admits_he/  Only older versions, they've capped all of the built-in games now.",Neutral
AMD,"What you made me remember! I had that game on windows 98.  Edit: my bad, I read it wrong as paintball 3D. On windows 98 i had it and Roswell pinball",Negative
AMD,"With no bots i get 1700+ in range, with bots 1200+ for valorant range for my 7950X3D.",Neutral
AMD,Ngl one of the reasons I am building that exact system is to play minecraft with modpacks and shader. Idk I keep coming back to that game ever since the beta,Positive
AMD,Crazy how ugly Valorant is *today* compared to Overwatch 1 in 2016.,Neutral
AMD,"Okay, I'll survive with my trash 700+ fps in 4K.",Negative
AMD,skill issues,Neutral
AMD,"A tool is only as effective as your skill with it, blame the devs (and more so publishers) not the engine.",Neutral
AMD,"If you want a UE5 game with destruction that performs well look at The Finals.  At 1440p Max settings i get a steady 60 FPS, if i drop it to low i can easily do 144 fps while still maintaining native resolution.",Neutral
AMD,"Which is still user error. An engine is a tool, if you don't use it properly it's not going to work very well for you",Negative
AMD,"A lot of the time, devs are being forced to rush games by publishers. No dev wants to have 3 months of crunch time just to get a game out the door.",Negative
AMD,Unity has dotNET framework (operating under the Mono runtime) overhead.,Neutral
AMD,"kid? probably a bunch of dumb covid gamers who have no clue how low fps games were back in the old times, unless you turned all gfx options down to look like shit just to maybe hit 100 fps",Negative
AMD,"What? In a way who cares, he was talking about ue5 not some plug-ins INSIDE ue5",Negative
AMD,"Also we have to consider the reason game dev studios will use UE5 over a more specialised in house engine - because UE5 removes development overheads.  It's natural that if a studio is trying to cut development costs, they likely aren't allocating much resource into engine optimisation work either.  It's why we see small indie studios getting more potential out of UE5 performance:visual wise than the AAA studios. Because the AAA studios are largely just brute forcing it.",Neutral
AMD,A faster monitor works the same because it's also adding to your total latency. (It's about showing you the enemy as early as possible and not about reacting within a single frame). The impact would also be even bigger. A 120 Hz display adds 8.3 ms (more than the Valorant window specified earlier) whereas a 560hz monitor would add less than 2 ms to your total combined latency.,Neutral
AMD,"\>""If both you and the enemy have the same reaction time but you have faster hardware, you get the kill.""  Unless valve intends to run its esports brand more like F1 or similarly technology driven sports, that sounds like a pretty big trade-off for getting rid of the classical systems. Just my initial reaction, my frame of mind. I understand the reason why it's done this way, I think it could of course be improved for the reasons above. Of course the better player should get the benefit, and timing it how they do now improves that. but on the end where your mouse counts..",Neutral
AMD,"The whole reaction time argument is nonsense because when it comes to the mouse polling rate, frame rate, and monitor refreshrate it's all about the latency between your input (mouse motion) and what you perceive on the screen. That's different from reaction time by about an order of magnitude. Removing a few ms polling delay, a few ms render delay, and a few ms monitor delay is now a significant fraction and easily noticeable.",Neutral
AMD,shadow warrior?,Neutral
AMD,"Blurbusters has shown that there’s significant motion clarity gains up past 1000fps. It’s more than just “it looks smooth” but that there is minimal blurring, judder, ghosting, etc.   Besides that, as fps goes up, latency goes down. At 1000fps, you’ve got a ms between frames. Meaning the game state will update 1ms after you do something. For esports pros in particular, this is a significant advantage. For average joe gamers, it’ll feel pretty good.   Additionally, at 1000fps, you’ve could do BFI at 500fps, or CRT simulation at  333fps and it’d be pretty magical.",Positive
AMD,"You call it pointless but provide the counterargument in your own reply.  I play competitive games 95% of the time and my goal with upgrades is to have stable 300+ FPS on my 1080p monitor, which I won't change until my 30s when my priorities shift.  The goal is a more responsive game and motion clarity. I'm perfectly content with my 240hz monitor and a PC that can uphold FPS at that number, but I'm sure I would see a difference at 500 or 1000 hz. The limit isn't my eyes, it's my wallet.   When you look at time between frames, you reduce it by 8 ms going from 60 to 120hz, and by another 7ms when going from 120 to 1000hz. You can see that there are harsh diminishing returns on all of this.",Neutral
AMD,"Agreed. 144Hz is the sweet spot.   The difference between 60 and 144 Hz is 10ms.   The difference between 144 and 500 Hz is half that much i.e 5ms.   6ms if we move up to 1,000 Hz.   It's all marketing bullshit!",Negative
AMD,bs take. I have a crt monitor and had a 240hz oled and I could tell the crt was sharper in motion at a paltry 960p.,Neutral
AMD,"nope, it's a common misconception, uncapping framerate overfills the render queue,    nvidia reflex (or amd anti lag) tries to minimize that effect, and it works pretty well but it's still not perfect, capping your framerate 10-15%  below your gpus capability is recommended,    optimum has a pretty in depth video with testing latencies, [https://www.youtube.com/watch?v=WHBMxOPAqWc](https://www.youtube.com/watch?v=WHBMxOPAqWc)  eg. rtx 2060 (from the video, sadly images not allowed)    120 frame cap (85% gpu load) - 24ms latency   no cap, 147 fps (99% gpu load) - 32 ms latency",Neutral
AMD,"its really not, with a 1000hz monitor sure it would be worth it due to better smoothness, but otherwise, in a vacuum the difference between 360 fps and 1000 fps is 1.7ms (in practice its always a bit higher), your total system latency is as i said 15-20ms at best, usually a bit higher too, not even including your human reaction time which is not gonna be smaller than 100ms even for the best pro players,  we use high refresh rate monitors for smoothness, it makes aiming easier, above certain framerate the latency difference is negligible",Negative
AMD,But competitive settings in competitive games are a real thing that people want unlike 8k minecraft,Neutral
AMD,"No it's not, because on the same slide (if you cared to open the article and look at it) mentions the games it runs +1000fps at.  Like it's not even vague, it's clearly stated in the slide...",Neutral
AMD,Is that the article about pinball? I read that one the other day,Neutral
AMD,I get 1100 all low settings in range and 700 with bots on a 9800x3d. I genuinely don’t believe you.,Neutral
AMD,"You and me both, it's a killer system and if you have the funds and are due for an upgrade I highly recommend it. You'll be able to play anything.",Positive
AMD,that system is wildly overkill even for ATM10 with shaders. nothing wrong with overkill though,Negative
AMD,"Expected response from a valorant kid, good luck with graduating high school pal",Positive
AMD,"144 fps on low is not much  The game looks mid. The roof tiles are 2D, same as the ground, destruction isn't really destruction, it's just walls disappearing but barely any crumbling  It just has good lightning and reflections, take that away and the game looks like it could have been released 10 years ago. Mirrors Edge gets close (besides the objects missing)",Negative
AMD,I just wanted to point out that it can also be incompetence from the developer and not exclusively publisher's fault.,Negative
AMD,>probably a bunch of dumb covid gamers who have no clue how low fps games were back in the old times  What on earth are you on about?  PC gaming has always been about high frame rates and the only thing that has really changed over time is what is considered to be a high frame rate.  It was console gamers (and terrible console ports) who were stuck in 30fps hell for forever and a day.  I still remember benchmarking my x800xt back around early 2005 and spending a good few hours trying to overclock my CRT to get more than 75Hz out of it (no luck as there wasn't enough bandwidth over VGA).,Negative
AMD,"Low fps? I was playing Star Wars Battlefront in 60fps in 2005...  Actually, I'm pretty sure that game ran at 45 on some consoles!",Neutral
AMD,"These are not plugins. These are the key components advertised by Epic themselves that makes UE5 a better engine than UE4. What Valorant uses is a variant of UE5 with all of these components removed, essentially making the engine comparable to UE4.   When somebody is talking about UE5 games having performance issues, they imply it's those UE5 games that use these key features. Valorant is an exception, not the norm.",Neutral
AMD,"I get your point but for online matches this is mostly overshadowed by ping. So it's mostly a bigger deal for LAN matches but at the bigger ones, everyone uses the same monitors and the players all have these superfast peripherals.",Neutral
AMD,"Ah yes, that mustve been the one",Positive
AMD,"Its very much diminishing returns, though.",Neutral
AMD,There is a 1ms difference between 500FPS and 1000FPS in a chain that takes about 20-30ms minimum in a theoretical best case scenario.  There is no human who will ever be able to tell the difference between that.,Neutral
AMD,There is a very noticeable difference between 144hz and 240hz,Neutral
AMD,Delay isn’t the only benefit with higher refresh rates.,Neutral
AMD,"A lot of that depends on where the bottleneck is. At 1080p on a higher end gpu, that’s not going to be the gpu",Neutral
AMD,Then when are they going back to CRT monitors to actually utilize those insane refresh rates that don't exist on anything modern.,Negative
AMD,"Go optimize your system, probably heavily filled with bloat and badly optimized.  https://imgur.com/a/JGZRw2o",Negative
AMD,"I was really looking forward to BL4 at high settings, but that kinda... is an exception   It's not like the 60 series is coming any time soon",Negative
AMD,"I want like shaders and far rendering distances and still have 120+ fps, minecraft sure is demanding  Obviously it's not the only game I would play on it, heard BL4 again has a bad performance launch",Negative
AMD,waaaaaaaaaa so mature waaaaa,Neutral
AMD,Del boy chatting about valorant. Time for bed unc,Neutral
AMD,"That is possible, though it seems like that's rarer than not. Studios at the very least have 1 senior developer.   There are also times, like with Jalopy, where the developer did release a broken update, but the publisher is preventing them from fixing it.",Neutral
AMD,"No, you can't take out stuff from ue5 so it becomes ue4  And yea, they aren't truly plugins, but it's easier to compare it to those",Neutral
AMD,"good point indeed, they are on much more even ground sitting in the same room with essentially equal gear heh",Positive
AMD,"I am old, in A B testing I can't tell the difference between 90 and 120 fps.",Neutral
AMD,"Just like there's a very noticeable difference between 30 and 33 FPS.   The difference between 240 and 144 Hz is 2.77ms. 3ms, if we round it off.   Same goes to 30 (3.33ms) and 33 Hz (30.30ms) i.e 3ms.   The real difference is input latency, not perceptible image smoothness.",Neutral
AMD,"in normal scenarios probably not, but we're talking about 1000 fps here, valorant shouldn't be an issue but i think cs2 is a bit harder on the gpu",Neutral
AMD,"You don’t need 1000hz monitor to benefit from 1000 fps. It’s going to be smoother on a 240hz monitor, mainly because the reason you’d want 1000 fps is so that your 1% lows are still above 240hz.   Also, you’re getting a more recently updated frame at 1000fps than 240fps.",Neutral
AMD,CRT monitors don't have high refresh rates.  Just getting 100Hz with a CRT was crazy high.  Modern LCD panels have surpassed CRT for refresh rate significantly.,Neutral
AMD,what mb? do you have x3d gaming / turbo whatever enabled in bios?,Neutral
AMD,"Even on a clean install I never got that my g. Are you actually looking at bots or are you just looking at a random wall? And what fps do get in gameplay? As for me, when I load into range(standing in default position) I get around 1100 and turning on practice bots I get 700-800. While shooting them fast it can dip towards 500 range. The benchmarks I saw on yt seem consistent with this.",Negative
AMD,"it’s also not that well optimized when you add tons of mods, so don’t be too surprised if you still get some performance issues with shaders on ultra",Negative
AMD,You good bro? 😬,Positive
AMD,its so funny when kids get triggered about their favourite game getting flamed,Negative
AMD,"I have no idea why Reddit fetishizes developers. They're human. As is human nature, they'll choose the easiest way out and be incompetent if it's an option.  It's like y'all think they should never have accountability. There's not always some big evil saturday morning cartoon villain publisher or manager around the corner.",Negative
AMD,">No, you can't take out stuff from ue5 so it becomes ue4  I never said this. Removing the mentioned key features makes UE5 ""comparable"" to UE4. I never said that it literally become UE4. The reason Riot upgraded to UE5 is to have access to the latest toolset and technical support from Epic themselves. Aside from that, whatever internal variant of UE5 that Riot uses is comparable to UE4. It's just UE5 in label only.",Neutral
AMD,"Valid point, this way gaming gets cheaper by every year haha  Except borderlands 4",Positive
AMD,"MSI X670P, did curve optimizer and curve shaper and tightened memory timings for 6200mhz. Tried +100 and +200 for pbo but cpu basically ignores my suggestion.  Debloated win11 with < 100 process at idle, valorant runs in windowed borderless with exploit protection mostly disabled for valorant except control flow guard because thats essential for valorant to run.  Set default cpu sets to only target non x3d cores so no programs will use x3d cores by default, valorant and other games are manually set onto x3d core via process lasso, gamebar completely removed for further debloat and remove potential conflicts.  Driver affinity and interrupt mostly spread across non x3d core except for primary gpu driver using gointerruptpolicy, gpu driver running on last 2 x3d core because i ran out of cores to throw drivers to.",Neutral
AMD,As an outside observer you seem to be the one crashing out over Valorant of all games.,Negative
AMD,"Think I played 2 hours on release. I'm nearly 30, I play League and Satisfactory instead",Neutral
AMD,"I'm not fetishizing them. I'm just saying it can be more than just ""lazy devs"".",Negative
AMD,Then you get glasses and the cost goes up.,Neutral
AMD,"Do you have any tips or guides for gointerruptpolicy? Never used it before. I have set up a good process lasso and did my CPU tuning and RAM timings. Also, how did you debloat w11, what tool? Winaero tweaker?",Neutral
AMD,I value your input,Neutral
AMD,"It matters more on which USB ports you use for your keyboard and mouse, the minute difference in clock speed does not matter as much, ideally you would want to use the USB ports that are wired directly to the CPU instead of chipset. You can figure which USB is which by looking at your mobo manual.  In my case however I have my keyboard using the CPU USB but my mouse connected to a USB that is routed to the first chipset(second chipset incurs even more latency), as for some reason I am having issues with mouse disconnects when I plug it to the CPU USB slot(most likely due to my mouse being 4k polling rate).",Neutral
AMD,"Does it even count as a ""launch"" if there is no price?",Neutral
AMD,I doubt there is any real volume of these chips,Negative
AMD,2x the price for 1.15 better performance is a bad deal.,Negative
AMD,"So if Z2 handhelds are double Z1 handhelds price, does it mean Z3 handhelds are going to be triple Z2 handhelds price?   Soon handhelds will just die out because the price is so astronomically high, it will become a niche hobby product..",Neutral
AMD,No one is asking for AMD put expensive NPUs into these mobile APUs.,Negative
AMD,"$1350 for the Legion Go 2 is a complete joke, at that point you might as well get the $1400 GPD WIN 5 with the AI MAX 385 for significantly more performance.  You might think that you would rather not have to carry an additional external battery's worth of weight, but the Go 2 weighs the same as the WIN 5 **with** the battery attached to the back. And the thing with having a external battery is that you can rest it on a table or in your pocket/bag instead of having all that extra weight on your palms.",Negative
AMD,"Just give it 6 months, be plenty of deals going when they don't sell at higher pricing. Just like OLED TV's.",Neutral
AMD,"This was how x86 handheld flopped in the past. Performance was ok, battery life not so good but managable. But the price, why would I want to spend this much for just a handheld? Then after sales goes down they will start complainning there is not enough market. The cycle will repeat.",Negative
AMD,"I was really hyped for Z2 (before any leaks). I was hoping for a much better performance for a slightly higher price. Surely it was possible.   Instead we get 2x price for like what, 10% improvement? Bruh... Literally there is absolutely no point in buying Z2 handhelds. It's better to go for Intel in the high end, Z1 in mid tier or Steam Deck.   On the bright side - at least I won't get any handheld before we get FSR4 in them lmao.",Positive
AMD,"It's a shame no one is looking at making a cheaper SOC for these handhelds. The soaring costs of the latest nodes probably isn't helping.  Ideally AMD would even just retape Aerith to a newer node so that it's smaller, cheaper per chip and uses less power. This means a smaller battery and less cooling hardware needed.  Then take the LCD steam deck. Make it smaller, maybe remove the trackpads (not sure how much those cost. They're great but not strictly necessary for many games). Cheaper WIFI..doesn't need to be 7. Sell it for less money and still make a profit.  But the rising costs of the latest nodes probably means retaping Aerith is not cost effective. AMD would probably rather use those wafers for higher margin products anyway.  But man: a solid 299 or 349 PC handheld would be great. A PC Handheld equivalent of something like the Pocket fit would be class.",Negative
AMD,"z1 was cheap, z2 is expensive. i guess manufacturers are testing to see what price point they can get away with",Neutral
AMD,Just spreading FUD to distract from the launches of embedded EPYC and Ryzen Pro that are decimating the competition,Neutral
AMD,Can we have a Switch Lite sized handheld with Z1 or Z2 Extreme?,Neutral
AMD,I'm getting ally x z1extreme for $400 and I waited enough for z2 which is wayyyy too much expensive,Neutral
AMD,and no meaningful feature upgrades. these devices don't make any sense.,Negative
AMD,I don’t really understand what rationale person is “frustrated” over this for that reason,Negative
AMD,Still Lost to intel 258v in Avg fps & low tdp performance per watt. Thats embarrassing,Neutral
AMD,"These new device prices are inflating the handheld market too, which sucks.  I picked up an ROG Ally Z1E Open box at BestBuy for $323 in February.  Now they’re $551 open box…",Negative
AMD,"I'm actually legitimately amazed that they're bothering. Weren't sales volumes really low for the first round of these things?  Then again, the fact that Asus only hopped in with the Xbox blessing is probably writing on the wall for that.",Neutral
AMD,Handheld ecosystem goes in a wrong direction. Time for Valve to comeback and set the standard again lol.,Negative
AMD,"It’s already a niche product really. After years of being out there are fewer than 6-7 million handhelds sold, vast majority of which are steam decks. The switch 2 sold over 6 million in a month and a half, last figures I can see being early August.",Neutral
AMD,"tell that to nvidia, ppl still buy their astronomically high gpus",Negative
AMD,"Someone here said that handhelds offered ""infinitely better"" gaming experiences compared to laptops. If it's that good, then why is there no outrage over a $1300 laptop? There's an implicit message going on, something many of us aren't willing to openly admit.",Neutral
AMD,They already are pretty niche. I don’t think the total for all of them has crossed 10 million even after a few years,Neutral
AMD,"I mean, they're already such niche products that it's likely hard to justify their development, production, and sale as is. [The Verge](https://www.theverge.com/pc-gaming/618709/steam-deck-3-year-anniversary-handheld-gaming-shipments-idc) wrote an article detailing whose products comprised which portions of the pie and apparently (per IDC estimates) Valve could have a share of near 66% of all PC gaming handhelds sold. As the prices creep further and further it'll be reduced to only those who really need their gaming to be handheld (and who can justify splurging for the extra performance). Everyone else is going to consider gaming laptops as crossover devices that beat the everloving shit out of even the best gaming handhelds AMD currently offers and likely Steam Decks (if anything) to scratch the itch without breaking the bank.",Neutral
AMD,Microsoft did :(,Neutral
AMD,Interesting username,Positive
AMD,At that price point you would be looking at a full laptop instead.,Neutral
AMD,"120 hz, 1080p, not oled, and 7"" so what are you really doing with that extra performance if you can't display it. Plus ik I'll lose the battery within a week",Negative
AMD,"That was what I kept saying, 6 months ago.",Neutral
AMD,"Exactly. There is a combination of multiple factors that must match in a sensible way. Over-priced devices, power-hungry devices, lackluster capabilities.. There are many ways to botch a form factor. It seems these device manufacturers are too eager to be milking the gaming market.",Negative
AMD,"there is a cheaper option, but the problem is that Lenovo IMO charged too much for it.  while not a night and day advancement compared to the steam deck, the Z2 Go cpu found in the Lenovo Legion Go S is in that pool of reasonable SOC's for handheld.  I personally believe that the biggest factor holding back AMD on the handheld front is that their CPUs are 8+ core CCDs when a handheld needs less cores at a lower power consumption to dump most of the ~10-20w power to the igpu instead.   The steamdeck worked great because the 4c/8t Zen 2 with 8 CU RDNA2 had a good power/perf ratio for a handheld.  the original Z1E was a 8c/16t zen 4 cpu wit 12 RDNA3 CUs which only marginally performed better than the steamdeck on the same power consumption, and only started to shine more once you got past 18W to 28W, which starts to get to the point where its not a practical handheld.  The Z2 Go was a modest upgrade to Aerith. its 4c/8t Zen 3, with 12 RDNA2 Cus. so same core config as steam deck, but with zen 3 instead of zen 2, and 4 more CU's for that slight more gpu grunt to cover those edge cases the steam deck might have barely missed.  The problem was, Lenovo charged 600$ for it. and had it been a bit lower, could have been an okay mid generation choice when compared to the steam deck.  my personal hope for the market is that the third tier AMD CPU core type that is rumored to be true. AMD does Full core, C cores, and supposedly a 3rd core type optimized for 1W/core usage. a 2:2:2 core configuration would IMO be a solid handheld CPU size option. if the latter doesnt exist, a handheld would far be better off with more Zen C cores than standard ones. probably a 2:4c config.",Negative
AMD,"What competition is ryzen pro (limited to 12 cores, most of the range being Zen 4) decimating?",Neutral
AMD,The people who pre-ordered and have had their orders delayed. Says so in the first and second paraphrase.,Neutral
AMD,Intel is on a better node.,Positive
AMD,Is that true? I was considering an MSI Claw 8 AI for Intel's better Moonlight streaming feature compatibility (YUV 4:4:4 decoding).,Positive
AMD,"It isn't embarrassing, the Intel 258v is outstanding at efficiency/lower wattages by design.  It's like you're saying It's a pathetic chip that is still beating AMD.",Negative
AMD,"It really is, considering that AMD won't really have anything new in (low powered) igpu space until sometime in 2027 with UDNA.",Neutral
AMD,"Lunar Lake features on-package memory and GPU cache which greatly improves battery life and stability. Z2E is basically HX370 with less CPU cores, AMD knows they can just rebrand their existing APUs and get away with it because most people prefer AMD in handhelds anyway.",Positive
AMD,"Thats competition, its a pretty good thing. Its only embarrassing for people who had to relieve their buyers constipation without looking at the competition.",Negative
AMD,I imagine they’re waiting until they can get a UDNA APU.,Neutral
AMD,I am waiting for the steam deck 2 to get into handheld. I won’t buy the switch 2 for nintendo’s greediness. Got a switch 1 with enough games I had purchased a long time ago and will go through this backlog until deck 2.,Negative
AMD,"They will, once there's an actual hardware successor. The difference is that these manufacturers only make money at the time of the initial hardware sale, so if they want more money, they need to sell you new hardware. As a result, they're just throwing whatever at the wall, whether it actually has a true benefit or not, just to say it's new, and you should get this one now, too.  Valve on the other hand is making its money off of game sales. The hardware is just a gateway drug. There's no real pressure to push out an iteration every year or sooner. The only reason they even did the Deck OLED was because they had done all the work, and with no true APU successor even in the works, they decided to just release what they had, instead of holding it indefinitely.",Neutral
AMD,"Unfortunately they can get away with overpricing their GPUs because they’re the market leader by a wide margin (though AMD is giving them a run for their money this gen, pun very much intended)",Negative
AMD,Because they do more than gaming.,Neutral
AMD,"Sadge, just tack that on the list of reasons Linux is doing it better I guess.",Neutral
AMD,"Not saying it isn't an absurd price, but (most) people aren't buying handhelds just because of the price. Hypothetically, if only those 2 options existed and I had to choose between a $750 laptop and a $1500 handheld with the same performance, I'd go with the handheld. Thank fuck the Steam Deck exists.",Negative
AMD,">	so what are you really doing with that extra performance if you can’t display it  You certainly ain't getting more than 40-60FPS in modern AAA titles on the Go 2, whereas you can absolutely achieve triple digits with the AI MAX series iGPUs which have similar performance to 60-class GPUs.  Also the Go 2 has a 1920x1200 display, which is basically the 16:10 version of 1080p. Due to the larger display the pixel density is actually worse at 257ppi compared to the WIN 5's 315ppi, so you're more likely to spot pixels on the Go 2.  If you only care about having a OLED display and not performance then just get a OLED Steam Deck at less than half the price - the performance of the Go 2 is only slightly better than the Steam Deck anyways.",Negative
AMD,"The implication was that no rationale person would spend their money on a bad deal, and so there’d be no reason to be frustrated about the launch",Negative
AMD,"They are also a complete SOC. CPU, GPU, and RAM on one chip.",Neutral
AMD,I mean it's not intel's fault. AMD just wants to sell old silicon for premium prices,Negative
AMD,Competing products regardless,Neutral
AMD,"Its good, but msi software is extremely buggy & slow. Device is extremely quiet, great speakers etc.   A8 with Z2 extreme is a ""downgraded"" version. Cheap build, way smaller heatsink, 5-7c hotter than 258v. Noisier fan",Negative
AMD,"I think the ""embarrassing"" aspect is more to do with how Intel has struggled historically for decades with designing and supporting decent iGPUs, and AMD has long held the position of having superior iGPUs.  Now, AMD releases this new ""extreme"" product with some of their best ""small handheld"" tech (only said for the sake of the GPD Win 5) and it is getting matched or beaten by Intels iGPU in a roughly equivalent form factor.      It's crazy times, but the prices are going too high.",Negative
AMD,"Yea I think so too. They wait for a great tech leap to execute their vision like they always do.  I think they actually said exactly this in an interview about the next steam deck, i don't remember which interview tho.",Neutral
AMD,"Any ideas, when the UDNA APUs can be expected? I always assumed Steam would target 2027 for a SD2 once they reach FSR4 capability",Neutral
AMD,..waiting for the Strix Corona 9070S :p,Neutral
AMD,"Hopefully Valve are working with Intel and Nvidia instead.  That would be a far better outcome for the hand-held PC market, as AMD clearly sees no competition right now.",Positive
AMD,Getting a refurbished steamdeck from valve would be my choice right now.  I dont think the steamdeck 2 is coming soon and being patient for a refurbed steamdeck helps save a good chunk of money.,Neutral
AMD,valve os very slow with hardware. I don't think we even hear much about a steamdeck two for like 2 more years.,Negative
AMD,AMD is also the market leader in handhelds,Neutral
AMD,Pc handhelds are just pcs that can do everything a pc can do. They just happen to be drizzling shits at almost everything. Dreadful at gaming. Horrible as a mobile pc. It's even failing at being a handheld.,Negative
AMD,"But those older iGPUs never tried to have good performance, they were just display output hardware (like the iGPUs in desktop Zen5). Useful for what they were designed for (your GPU died, or for office work). It's only recently that Intel entered the ""real"" GPU market, so they now have the tech to have much better iGPUs.",Neutral
AMD,Well now desktop UDNA has been rumored for an early 2027 launch which would likely put APUs 12-18months afterwards.,Neutral
AMD,"if we make the assumption that medusa series of products are using rdna 3.5/4 for whats functionally all of 2026, then don't expect any RDNA5/UDNA products to release at the earliest, 2027.  I'm on the boat that Valve should consider what Sony will put in their theoretical PS6 Handheld(if true) and try to get something on a similar boat if they cared about longevity of the chip. If all devs had the playstation handheld in mind, if the steam deck 2 manages to be a similar performance tier, then it would be a no brainer to support it for the lifetime of the next gen devices.",Neutral
AMD,I would be truly shocked if contracts for Deck 2 weren’t signed a year or two ago and AMD is already working on a chip..,Neutral
AMD,"I am ok with my switch for now. Got quite a huge backlog that I only go through when I am on the go so yea, got lots of time to wait.",Positive
AMD,"They're actively vying for it, where Nvidia's top GPU offering walks AMD's top GPU offering like a dog the Z2E doesn't really offer much over the 258v that it either matches in price or exceeds.",Neutral
AMD,"Nintendo has been the handheld leader for over 3 decades. Since releasing the Switches with Nvidia tegra chips, it makes Nvidia the defacto leader. AMD is offering other brands opportunities to compete. Companies need to get their s*** together, because Nintendo makes it look easy, but making a well rounded, ergonomic, and price-competitive handheld is a lot harder than people think.  Closest competitor is probably the steam deck.",Positive
AMD,"The Llano APU release was like, 2011, with Trinity shortly after, and you could 720p game on low with those with a good experience.      It isn't as if Intel wasn't actually trying since that time, to be competitive.  They had ranges of EU-config iGPUs since then up to over 40eu on some chips.  They would have little reason to stack so much iGPU is they weren't intending to stab at low-end gaming to compete with the APU parts from AMD, because you wouldn't be doing professional application development at that time with anything less than a fire pro or a quadro.     Don't know why I'd get downvotes for literally recounting history, but do what you must.  But consider, I am over the moon with how these Intel chips and gpus are performing.  I just remember what it was like when Intel was saying, in the mid 2010s, oh we have a 30% uplift with our iGPU performance!  And they were still getting trounced by two generational old APUs.  Like I said, we've come a long way. They just need to get freaking Big Battlemage released, if they're going to.",Neutral
AMD,Amd usually announces notebook chips at CES in January. In 2026 we will get a Zen 5 refresh (Grogon Point) and then at CES 2027 Medusa Point (Zen6+rdna5/udna). New systems then usually are available starting in late April/May.,Neutral
AMD,"Perhaps  On the other hand Intel and Nvidia have been working on their first SOC for a year also, so who knows  I'm just coping here. I don't like where the handheld PC market is going right now with only AMD as the hardware supplier.",Negative
AMD,Nintendo is a console theyre not targeting the same markets.  Nintendo is brand loyalty from legacy names,Neutral
AMD,"If you don't think people who want to game on the go with a handheld form factor are considering the Switch 2 then you're off your rocker, it's outsold every single PC handheld combined within mere months of release. The Steam Deck is the closest PC handheld competitor and it's been trying for almost half a decade, so unless the Switch 2 just isn't a handheld then the Switch 2 is **the** market leading handheld (unless we include its predecessor).",Positive
AMD,"The vast majority of people who buy handhelds couldn't care less about the tech details. If the market is handhelds, Nintendo & nVidia are playing in it and unfortunately beating the shit out of everyone except mobile phones.",Negative
AMD,"exactly no one cares, its ppl buying nintendo for nintendo brand.  Steamdeck is its own market not in direct competition",Negative
AMD,I would love one but they are still too expensive.,Negative
AMD,Dozens of mini PCs. But no laptops.,Neutral
AMD,"Everytime I see a post about strix halo and there a few poster said ""reee too expensive reee"" i just :rofl:.   Hmm I wonder if AMD has a plan for something like 890M class but runs quad channel ram (with camm2 slot) in the future.",Neutral
AMD,1000€ is the max I'm willing to pay for the 128GB version. So I guess I'll wait for a long time.,Neutral
AMD,"I wish I could figure out a better cooling solution for my EVO 2, I know it’s within spec but I really don’t like sustaining 90c.",Positive
AMD,Most of the 128gb models in Canada are like $2700+  Nope.  That APU with RAM is under $400 raw cost. They can FK off with that markup,Neutral
AMD,Well I guess now we know where they're all going,Neutral
AMD,How many of which from a tier 1 OEM with real global volume?,Neutral
AMD,"I will be honest. I hate AI. I do use AI as a web developer, as you get clients that give you one sentence and say, build a website, you know what you are doing. Does not matter how easy it will make my life, I will not use it outside of my work. Speak to people here, and compared to the older generations, they are not even in our league when it comes to knowledge. I had a client whose email I had to fix. She told me to do this, as that is what ChatGPT says is the problem.   I told her, with all due respect, don't tell me how to do my work because AI says that is the problem. I showed her what CHATGPT said was the opposite of what the actual problem was. I don't mean this to be offensive, but people are dumb and lazy. They never flex their brains as AI is there to think for them. Does not matter how big AI becomes, I will never adopt it into my daily life.",Negative
AMD,"> I guess this seals the deal that Strix Halo and its successor (Medusa Halo) will never be gaming-first products  And thats it. They’ll try to push these to businesses since they have the money and volume, only to be hit by the reality that they will either spend money on cloud GPU and CUDA than paying $1,500 upfront for an unproven toy.   AMD on AI business is a failure beyond inference. They don’t the ecosystem, nor price advantage, nor availability, nor the marketing.",Negative
AMD,And who are these actually for actually? I don't see the purpose of these for excel and outlook business users,Negative
AMD,"Expensive mini PCs kind of defeat the point of the appeal of a mini PC. Cheap while being effective enough to do stuff.  At that price point, why not just get a laptop?",Negative
AMD,Framework motherboard for 1600,Neutral
AMD,HP has one....,Neutral
AMD,"Framework and AMD have openly stated that CAMM doesn't have the memory latency signal integrity required for Strix Halo. its the main reason why Framework has soldered ram, despite having a company mission to have more modular parts for repairability/sustainability",Negative
AMD,GMKtek and Beelink are differentiating their version by overlocking with voltage.  Undervolt it and it should last way longer. Framework went with AMD’s stock I believe and it never goes above 75c.,Neutral
AMD,No. The APU is $600. Regular 128 GB of ddr5 alone is $400. The BOM for APU+Ram is $1000.,Neutral
AMD,"Yeah, and then realize that it wasn't just spawned out of nowhere. The designs didn't appear to someone in a dream. Millions, if not billions went into the R&D and that cost needs to be made up + there needs to be profit overall.  Then realize that these chips are extremely efficient and the CPU and GPU that you get will cost quite a bit to get anyways  (you'd need an rtx 5060 + 9950x to get the same performance and they'd be less efficient than this is).   And then you need to get the motherboard and lpddr ram that it requires and that needs to be soldered on and the cooling solutions and case etc.   Overall, the price isn't too outrageous.",Neutral
AMD,"What do you mean raw cost? You mean it takes $400 to just make? How will they make money if they sell it for the amount it costs to make? It's a huge chip, of course it's expensive.",Negative
AMD,Yields won’t be huge for a package this size.  Supply low = high price.,Neutral
AMD,"Yes. And there is ASUS one. Both were announced long ago, like the first mini PCs. But since then, we've been getting more mini PCs, but no laptops. According to the article, it isn't really ""dozens"" yet, but AFAIK, for Strix Halo we're now at 17 mini PCs vs. 2 laptops, 2 handhelds and 2 desktop mainboards.",Neutral
AMD,Is it $600? I heard murmurs of it being slightly over $700 for the 395. I need actual numbers.,Neutral
AMD,"I need to see the source for this because the 395+ and 8x16gb cost to the resellers is $400 according to what I remember . The 32gb configs run about $45 wholesale and 128gb are closer to $120 to $150. The 308mm die of 4nm is about the same. Meaning, even if AMD adds 25% markup, it's still $400.   Whatever old silicon resellers add is not worth $1600. My admiration for AMD stands as I think this tech id amazing, but the integrators are ripping people off",Neutral
AMD,rtx 5060 has double the memory bandwidth of this thing and 5x the “AI TOPS” due to native support for small int/fp types.,Neutral
AMD,> you'd need an rtx 5060 + 9950x to get the same performance   Even a laptop RTX 4060 beats it.,Neutral
AMD,"These resellers volume discount price is cheap. The chip with soc memory included. That's the part that's revolutionary. The rest is already existing silicon that's low in demand. If you don't understand BOM vs markup then I can't help you and no, 5x bom cost is a stupid price",Neutral
AMD,What are those Ryzen AI Max 395 Desktop Mainboards? The Framework? It‘s still not officially out (can only be pre ordered) and costs 2000€ with 128GB RAM (all other options would make no sense). The other one?,Negative
AMD,"There's genuine excitement for large RAM and high power envelopes, so the mini PCs are getting more attention. 128GB in a z13 should have an impact on battery life and the package does not run at maximum design power even when plugged in.  Some of the mini computers also have better storage options and I saw someone claiming they got an eGPU working despite AMD saying Halo doesn't support discrete graphics. My guess is there's no provision for a regular dGPU but the USB4 works fine?",Positive
AMD,It was $600 at the start of the year.  Probably went up to $700+ by now.,Neutral
AMD,A laptop RTX 4070 does but I believe that's when both are power limited. It can match a desktop RTX 4060 to get the same performance as the chip does. I don't know why there's an rtx 5060 in my comment. I meant the RTX 4060.,Neutral
AMD,Can it beat it when only running on battery?,Neutral
AMD,"Hard to be sure of the specifics. They are using a more expensive package and motherboard, plus accounting for initial development costs and a low starting volume of units shipped. The internals run at much lower base power than the 9955HX, this thing is a complete fork.  Framework 32GB model is like $1100 usd for a complete system, $800 for only motherboard and the higher memory configurations are where they will take their profits from. I could see a BOM on the mainboard approaching $600+ assembled. People want the 64GB and 128GB systems and will pay market prices to get them.  Yes there is profit-taking on the higher end models, that's how companies profit off of consumers and make money to develop new products. Original Steam Deck was made to break-even at $400 usd acording to Valve, the midrange and top models added like a hundred dollars and hundred-eighty dollars of additional profit. Massive sales of the 512GB Deck gave Valve an incentive to develop the OLED refresh model which has also sold well.  In terms of price, performance and device form factors I am hanging a lot of hope on Medusa Halo. Assuming that MH is not canceled and uses a newer RDNA version, it could be exactly what I am looking for. If it has 50% more memory channels for the 'big' Halo and will accept 192GB of RAM, that thing could be a great system. Too many rumors, claims of hardware changes and memory upgrades. Strix Halo is fine but I'm holding out for the v2.",Neutral
AMD,"The Framework one and the Sixunited STHT1. Similarly, also for the mini PCs, 9 of the 17 are not officially out yet.",Neutral
AMD,They have been delivering the framework to those that pre ordered for at least three weeks.  So backordered is more accurate despite them having not updated the website description.,Neutral
AMD,yeah usb4 egpu as per normal i think,Neutral
AMD,"Sure the 13'' and 14'' inch laptop sizes and weights might limit TDP, and thus performance, battery life might not be too great (though from what I've read the HP ZBook Ultra 14 G1a is doing fine wrt. to battery life, and has great performance compared to other laptops - haven'T seen a laptop vs. mini PC benchmark Strix Halo benchmark comparison yet).  But I don't see why that translates into a huge number of mini PCs being announced. For me, an expected solution would be for laptop vendors to release larger, heavier 15'' laptops with higher TDP or larger batteries, etc.  After all, Strix Halo is currently the only non-Apple APU in its class. So if your company is not called Apple, but wants to offer the best CPU performance in a laptop, Strix Halo is the only practical choice.",Neutral
AMD,"There's no way.  The GPU/IO die is the most expensive part to produce, and it's 308mm² which is smaller than the Navi48 chip in a 9070XT. The rest of it is two compute dies at 70.6mm². Board partners can build the entire 9070XT including VRAM, cooler, power delivery etc and sell at $600.  So the question is how much are Minisforum etc buying the APUs from AMD, but AMD's BOM on the APU is definitely not $600.",Negative
AMD,"I mean, how much is power efficiency even worth?  Oh right, there is... all of... that :: gestures to Apple as a company ::",Neutral
AMD,"Larger laptops tend toward having a dGPU so the first models with Halo were smaller. Then you can compare them against Razer blades and other compact designs. It wasn't a bad idea but I keep seeing people talk about Homelab and how much storage can they attach to these things. Any design with a low core count or less than 64GB of RAM may as well be a regular Ryzen 370 (and those models have the motherboard support for a dGPU).  Side note, I saw that Corsair has a 385 with 64GB for $1600 and I was a little tempted.  Rumors have been floating about a possible ""big Halo"" and ""little Halo"" if the line is continued. If true, little Halo should be cheaper than current stuff and big Halo could be much faster. This also depends whether we see AMD shipping as small as possibly 2nm product in their consumer division. I trust MLID even less than his net worth, but 2nm could give us a 'small' monolithic APU that's 50% faster than a 370 in both multicore and graphics performance...if you can afford it. Rumors about the big chip could mean a higher max TDP but also 40% or more improvement in performance over Strix Halo. If it has working FSR4 it could be competitive even in the 17in laptop space.",Neutral
AMD,"Your comment has been removed, likely because it contains trollish, political, rude or uncivil language, such as insults, racist or other derogatory remarks.  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/Amd) if you have any questions or concerns.*",Negative
AMD,We are not talking about AMD BOM.  You can't put a value on a chip that way anyway.  Billions of R&D go into chip design that AMD had to recoup.,Negative
AMD,"Apple has amazing hardware that I really wish I could utilize but the closed ecosystem doesn't allow me to use the apps I would want. (Still no MacOS on their ipads either, which would actually convert me.)",Negative
AMD,"can someone explain how this might effect me, as a long term holder, as if I am a golden retriever?",Neutral
AMD,The MTL/ARL/LNL Dates are all wrong for CPU launch it's an OEM roadmap not Intel.,Negative
AMD,Medusa Ryzen... Sounds like something I'd rather avoid in real life.,Neutral
AMD,Panther Lake is slipping to Q2 2026 now?    It was originally supposed to launch in Q4 2025,Neutral
AMD,Just hodl until you get the biscuits,Neutral
AMD,"If leak is correct it's bad news.   Panther lake q2 2026, Nova lake q2 2027.    It means 18A will arrive later than TSMC 2N.    Nova lake will likely arrive after Ryzen successors.    Combine that with other performance rumors where there isn't a huge performance uplift. It's bad news.",Negative
AMD,"The dates aren't wrong. As you said, It's the OEM roadmap, and they follow a Q2 release cadence.",Neutral
AMD,Gate All around and Backside power was supposed to be in 2024...  Now it's q2 2026. At this point it's 4N7Y... Which is what we call TSMC cadence.,Neutral
AMD,"why even panther laker when it was only for mobile, cancelled it and just released nova lake next year",Negative
AMD,"This isn't Intel and AMD's road map, this is an ""undisclosed OEM's"" road map.   This doesnt tell us anything about PTL's launch. Just when this OEM will refresh their laptops to have PTL, and its normal for OEMs to launch refreshed models at certain times.   Dell, for example, will usually do Spring, so this may be them. Surface is notorious for being 3 or so quarters after CPU launch.  Look at where MTL is on this schedule, for example. Also around Q2, even though it had been on store shelves technically since the previous year's Q4.  Edit: Looking at the roadmap a second time, this OEM seems to always target a Q2 launch cycle: ADL, RPL, MTL, ARL, PTL, and NVL all follow it. LNL is the only outlier.",Neutral
AMD,I thought Arrow Lake refresh was in the cards for 2025.,Neutral
AMD,Wasn’t Panther Lake going to use TSMC for a portion of the volume?,Neutral
AMD,Yeah I forget to to mention wrong In the sense that it is not Intel CPU Launch OEM roadmap.,Negative
AMD,This entire thing is a mobile roadmap so why are you here?,Negative
AMD,"This ""undisclosed OEM"" is Lenovo by the looks of the font used. Also the timeline makes sense. For example Lunar Lake is shown to be in very late 2024. Lenovo had only one model of Lunar Lake available at that time - the Yoga Aura Edition.",Neutral
AMD,"Yeah if it's Surface roadmap, it's a nothing burger.",Neutral
AMD,Yeah some OEMs don't refresh their lineups until the new generation is already launched,Neutral
AMD,I think it was for the iGPU but I don’t remember. I just know the majority of PTL is 18A. I saw a rumor that they were using Intel 3 for the iGPU also so idk.,Neutral
AMD,I also think it's Lenovo. Q2 is when they refresh their ThinkPad line,Neutral
AMD,The 4 Xe3 core iGPU will use Intel 3. The 12 Xe3 core iGPU will use N3E.,Neutral
AMD,"Wonder if Intel will do stacking two bLLCs on a single CPU tile like rumoured as an option with AMD's Zen 6. I iknow this is basically both Zen 5 CCDs having each an 3D v-Cache and NVL seemingly going to have that option now. But two bLLCs meaning 288MBs of stacked cache + cache in compute die would be insane and crazy for gaming.  Regardless I'd love a P Core only RZL SKU with much stacked cache as possible, or heck just 48MB 12P Griffin Cove + 144MB L3 bLLC would be sweet.",Neutral
AMD,"I like how they just throw random words around to pad their ""article"".",Neutral
AMD,The only reason I don't have a dual CCD ryzen is the lack of X3D on the 2nd CCD.  I would 100% be in for a 16 or 24 (when they go 12 core CCD) core dual X3D.,Neutral
AMD,"""leaks""",Neutral
AMD,"When are people going to realize ""Leaks"" are marketing ploys to get eyes on product. Come on people.",Negative
AMD,"Considering the latency is not even expected to be any good with BLLC, AMD is seemingly quite a careful opponent for Intel indeed",Negative
AMD,Scared of their rumor?  Lets release our rumor!,Neutral
AMD,I don't think 2x bLLC could fit on the package. Sounds like nonsense to me.,Negative
AMD,it seems as if right now the best option for consumers is to just build a cheap 1700 or AM4 system and call it a day for the next 4-5 years or so. but that's just me,Neutral
AMD,"My god.  The only reason huge caches benefit gaming, is to benefit rushes to shipment by the publisher, so that game engineers either don't have to optimize, or they can use garbage like Unity, or both.  This is going to make so many mediocre developers better... except no it won't.  It'll just get the market flooded with more garbage.  Also... diminishing returns, and I doubt AMD will put this in Zen 5 -- seems like a trash rumor, to me.",Negative
AMD,"3D cache will eventually become full of assets and dip, if and onlyif IMC is bottlenecked by random memory access.  - the ability for cache to transfer data from memory, to IMC, to cache has a FPS hit (cache hit-rate). This is due to the bottleneck of a chiplet-based IOD: compromising memory latency with cache hit-rate. Eventually the cache fills and relying on IMC to feed the cores anyway.  You can easily test this by monitoring 1% lows and increasing memory latency (lowering DDR4 B-die clocks/DDR5 hynix and increasing timings) to see how Cache holds up when full of assets while memory is being bottlenecked-   What games need is 10-12 P-Cores and an IMC capable of high speeds. Within a monolithic die to reduce relying on cache hit-rate and IOD chiplet bottlenecks.",Neutral
AMD,This is all they needed to sell for gaming again and yet they gave us all the e cores no one asked for jfc.,Negative
AMD,bLLC is not stacked cache,Neutral
AMD,What do you consider random? The article was perfectly clear.,Neutral
AMD,Do you have a specific application in mind? I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.,Neutral
AMD,Some rumors of 16-core dual X3D chip actually surfaced yesterday...,Neutral
AMD,"They would have trouble fitting just 1 bLLC onto a package, 2 bLLC might only be viable for something like a server chip with a much larger socket.   It may be possible once Intel starts 3D stacking they're CPUs though, but we have to wait and see.",Neutral
AMD,Yeah. Definitely just you,Neutral
AMD,You could literally make that claim with any CPU performance increase.,Neutral
AMD,Speak for yourself. Skymont ecores have raptor lake IPC. We need more of those,Neutral
AMD,"What CPU do you have? If its Zen 4 or slower, your cores are same or weaker than Arrowlake E cores in IPC",Neutral
AMD,This is correct. Stacked cashe comes later. I'd say around 1 to 2 years after. And that comes with stacked cores as well if they go with that path.,Neutral
AMD,"Correlating asset pop in with memory latency is utterly nonsensical.   Edit: Lmao, blocked.  Also, to the comment below, the creator of that video clearly has no clue what there're talking about. There's no ""bottleneck between cache and memory"" with v-cache. The opposite, if anything. Just another ragebait youtuber who doesn't know how to run a test.",Negative
AMD,"> I thought two CCDs with 3D cache would still face the same issues if the app tries to use cores across CCDs.  It does, but i'd rather every core be equal in terms of per-core cache in a local sense. Thread scheduler can handle the CCD split (usually)  There are definitely workloads that benefit from X3D but either scale to many threads or are let down by poor OS scheduling.  It could even be a boon to gaming with well thought out core pinning of game threads.",Neutral
AMD,He has a fair argument with the horrible optimization of modern video games but any innovation in the CPU space is a good innovation at this fucking point,Negative
AMD,Id love a 200 e core cpu with lots of pcie lanes for my home server. But I do not want mixed cores.,Neutral
AMD,Yea but I’m not paying for e cores that are barely faster than my current skylake cores. I just want p cores only.,Negative
AMD,"IPC is just 1 factor  Zen 4 clocks much higher, and got a far superior low latency caching subsystem   No one sane thinks that Skymont can actually beat Zen 4 in most workloads Only if you downclock Zen 4... and guve it slow ram to increase latency",Neutral
AMD,"Now pay attention to the memory bottleneck of chiplet IMC architectures on x3d chips:P https://youtu.be/Q-1W-VxWgsw?si=JVokm7iLScb7xynU&t=700  This simple 1% lows test indicates how texture-pop can impact frametime, directly. Since assets are first stored in memory, then fed to cache, any bottleneck between Cache - to memory will cause delays when loading asset thus 1% FPS... measured temporally(in this case).",Neutral
AMD,The bad optimisation critics aremostly people trying to sound smart by rambling things they dont understand.  Game engines are very optimized these days.   Its funny they mostly pick on Unreal Engine 5 as the prime example... That is probably the most complete game engine out there. So naturally its hard to make all that stuff work well togetter... They have litterally some of the best coders and researchers in the business ... Id love to see you do a better job.,Negative
AMD,Mixed cores are awesome if the scheduler would do what i want and there lies the problem and i dont think even AI can solve that yet.,Negative
AMD,"I said IPC of Zen 4 is the SAME and weaker than Zen 4 is weaker than Skymont in IPC. No more no less. As for clocks, the core scales to max 5.2ghz (default 4.6ghz), certainly overall slower than Zen 4. IPC is a comparison about throughput at a given clockspeed. Its a give to compare you will need to match the clocks.  Funny you you should mention latencies, considering that core to core (even accross clusters), latencies of Skymont are better than Lion Cove? So if latency is the issue, it has nothing to do with Skymont and everything to do with the trash uncore they have based on the Meteorlake SOC. Even workstation Crestmont on a different package performs better. Furthermore skymont IPC is measured based on Arrowlake. Arrowlake is worse latency than Raptorlake and Zen already. Latencies have been accounted for.",Negative
AMD,"I play Warthunder almost daily. It’s ridiculously fucking optimized.   I got cyberpunk when it came out, it was a shit show. Elden ring? Mediocre.  Escape from tarkov? Fucking laughable.  Call of duty? 500 terabytes. Besides this small list,  I’ll wait for battlefield 6 (which by the way also has a history of releasing unoptimized)  The one thing I will say is that developers do work hard after releases to optimize their games. I’m just so use to playing Warthunder which almost always works, everyday, perfectly, with no complaints. lol",Negative
AMD,"Do tell how a bigger cache makes games faster, if you are aware of some details.  And, of course, how doubling the already tripled cache will yield perf improvements worth the huge amount of extra money to make such chips.",Neutral
AMD,Any efforts of Intel to improve efficiency and even performance on old products is a pro-consumer move. Good to see.,Positive
AMD,It's not sorcery. Its just Intel doing the game developers work.,Negative
AMD,"Is it updated? Last time I tried it, it didn't have an uninstaller and not many games were supported. In the ones I tried, there was no difference at all.",Negative
AMD,"It's very specific scenarios where this can unlock new performance, most games it won't do anything or even lower performance.     So they can't *really expand it* since it's an uncommon scenario. That's why we have so few games that benefit from it.",Negative
AMD,Never count out Intel. They have some very talented people over there.,Positive
AMD,"Regarding the cache optimizations you pointed out, I have a question. Can you test all APO supported games you own and capture the utilization of the E-cores ?  My understanding is that Intel is keeping 1 E-core per cluster active and parking the remaining 3 E-cores. So the lone E-core of each cluster has full access to the 4mb shared L2. For the i9 they can have 4 such solitary E-cores that have their own cache and hence do not touch the L3 cache. The 4 E-cores handle background task while the 8 P-cores with the full 36mb L3 cache deal with the games.  Please correct me if I am wrong.  Also, what cpu cooler do you use ?",Neutral
AMD,What about this post saying APO havent gotten anything in over a year.. https://www.reddit.com/r/intel/s/tTKCn1CnDZ,Negative
AMD,Does anyone know if changing the Windows power plan affects performance? What Windows power plan do you recommend for the i7 14700kf? Balanced? High performance?,Neutral
AMD,I wonder why APO isn't just tied to game mode. That way it would work for all games from ADL to ARL.,Negative
AMD,"or just get rid of E cores. I know all the non engineers in here think they are great, but real world they are crap, this is just another in a long line of examples of why intel needs to ditch them.   anyway go ahead and downvoted me becuse being correct in this sub is looked down upon.",Negative
AMD,A few years ago you'd have to tweak the hell out of AMD CPUs to get comparable performance to Intel equivalents. Now the tables have turned.  In either case those who know how to tweak will get top performance not matter the platform.     Kudos,Negative
AMD,Let me teach you a trick.... there is a hack to disable e cores for every game using Intel compatibility tool.... you can try yourself to see if it works,Neutral
AMD,I will follow all!,Neutral
AMD,Congratulations on getting a CPU that didn’t fry itself.  Consider yourself one of the few lucky ones.,Neutral
AMD,"It looks like Intel APO almost completely disables the efficiency cores for the game when you look at the screenshots I posted below with per core activity.  But this performance boost likely cannot just be gained by turning off the efficiency cores in the BIOS yourself I wager, because it would be too simple.",Negative
AMD,How is this doing the game developers work?,Neutral
AMD,"It's updated as far as I know, but not for every platform.  If you're on Arrow Lake you have the new games that were added, but if you're still on Raptor Lake then you only have the original titles.     This sucks because I own BG3, which is one of the new titles they added recently along with Cyberpunk 2077.",Negative
AMD,"They have added a few new games, but it appears you need to have Arrow Lake to be able to apply APO on them which is BS!  They need to have a better system than this.       The DTT driver should be downloadable from Intel, and not from the motherboard manufacturers who cannot be trusted to make the latest version available.",Negative
AMD,The intel software team is pure black magic when they allowed to work on crack.,Negative
AMD,"Here is a screenshot showing CPU activity across all cores.  This is with HT disabled BTW, and the cooler is an Id FROZN A720.  It looks like the efficiency cores are not engaged in this particular game when APO is enabled.  And Metro Exodus is the only game that I own in the current lineup for the 14900K.  I know they've added some games for Arrow Lake (like Cyberpunk, Baldur's Gate 3) but those games still aren't available yet for the 14900K     [https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png](https://imagizer.imageshack.com/v2/3528x1985q90/923/hh8Guk.png)",Neutral
AMD,"Raptor Lake isn't getting new games for APO.  Which strikes me as greedy, since I'm sure it would work fine. Intel originally said that APO wouldn't work at all on 13th gen, which turned out to be completely false.",Negative
AMD,"When combined with Nova Lake's bLLC, it could give Intel a major edge over AMD's X3D.       And yeah, limiting it to certain games and certain CPUs is a real downer for the technology, so hopefully Intel can find a way to streamline and expand it when Nova Lake launches.",Positive
AMD,"AFAIK, it's recommended to use Balanced mode for proper use of the efficiency cores if you have a Raptor Lake or Arrow Lake CPU",Neutral
AMD,Balanced in full load will just do the same thing as High Performance.,Neutral
AMD,"It's capable of a few things, but the main mechanism from what I've seen is that it temporarily disables three out of four cores in each E-core cluster by forcing them into the lowest power state. The result is that the remaining active core has access to each cluster's full L2 cache (2MB on 12th-gen, 4MB on 13th/14th-gen). L2 cache isn't shared with P-cores (L3 is global), so this can really minimize E-core cache evictions before they're forced into slower memory, and games do love themselves some cache.  It's a genuinely clever way of maximizing available resources and I really wish they'd allow user control over its features, but it seems to be pretty tightly leashed to/by the team that developed it. It obviously wouldn't benefit every situation, such as particularly low/high thread occupancy situations, but it's pretty rough to have the option tied to quarterly updates for one or two specific games.",Neutral
AMD,Or... Just process lasso.,Neutral
AMD,Because it’s optimizations on how it can efficiently use the cpu.,Neutral
AMD,Must the App be downloaded? Or it's automatic after installing the PPM and DTT drivers?,Neutral
AMD,"All of this stuff is super situational. For when it’s supported for my 14th G, I love it.  To call it better than X3D though? That’s a bit much.",Positive
AMD,They are they came up with ML for their upscaller before the red team did and even better than FSR as a dump upscaler and they also did a good RT implementation. But the hardware still has raw horse power issues and they already use a big die for what it can do.  In the 80's or they shammed The DEC Alpha team on subnormal floating point operations with the 8087 FPU. Intel was doing way more precession without having to round so earlier on a consumer FPU while the Alpha CPU was a Mainframe one. Intel also gave it up to help estandardize the way CPUs handle floating point types.  Intel compilers were also black magic becuase they optimized so much C and C++ for Intel specifically but there was always controversy causeless AMD x86 couldn't take part of those optimizations,Neutral
AMD,Were there any background tasks running when the screenshot was captured ? I want to see how the E-cores are assigned for the background tasks under APO.,Neutral
AMD,In certain games. It's not universal at all. There would be scenarios where it gets an edge but if AMD is pushing as hard as rumors state they may still have the better CPUs overall.,Neutral
AMD,"Great explanation that makes sense!  It definitely has to do with the cache, that much is true.   A Raptor Lake CPU with 8 P cores and no efficiency cores, with 36MB of L3 would perform exceptionally well in games.   But the efficiency cores have their uses as well in highly parallel tasks.",Positive
AMD,"Yeah it's a pretty cool, yet simple idea. It would be really nice to give users more granular control natively.",Positive
AMD,Are the e cores clustered in order so we would be able to manually achieve this with process lasso?,Neutral
AMD,Why should game devs be automatically expected to specifically optimize their game for specific architectures of Intel's CPUs? It's not on them to do so.,Negative
AMD,You need to download the Intel Application Optimization app from the Windows store,Neutral
AMD,"I never said it was better than X3D, only that it is useful as a foil.       Remember that Nova Lake will have bLLC, which is the answer to X3D.  But in conjunction with APO, it may give Intel the edge and help them regain the gaming crown in the next cycle provided they can streamline and expand the technology.",Positive
AMD,"Actually, it is better than X3D in almost every GPU/CPU combo where the GPU throttles. Intel 14th gen almost universally wins 1% lows and frequently FPS - almost universally in 4k gaming, but as I said, when the GPU throttles at any resolution, which is almost always, Intel wins.",Positive
AMD,"Eh, they're very good but they benefited vastly from AMDs mistakes and the lessons learned from it. Remember AMD didn't have any ai hardware for their first 2 gens on RDNA and RDNA 3 didn't have enough for FSR4 either. It's not that intel were better at producing a ML upscaler. They just had the hardware for it from the get go so they could implement it quicker.  They did make Xess though which is miles ahead of FSR 3 even on AMD systems (on average. FSR3 is pretty good when modded and can be competitive) so they do have some extremely talented people there. I hope they fund them properly and don't go the AMD route of having Ryzen and Radeon not work together due to pride.",Positive
AMD,"Just  Steam, MSI Afterburner, Windows Security, Nvidia App and my sound card command center",Neutral
AMD,Where to download this APO,Neutral
AMD,Except its THE game devs job to optimize games for multiple cpus and gpus.,Negative
AMD,It’s literally their job to do so? wtf you talking about?,Negative
AMD,AMD also has Heterogeneous CPUs and even more so ARM cpus.  Devs should start to do this optimizations to schedule the stronger cores.,Negative
AMD,"Will do, I got a 265K. Performance is already great tbh.",Positive
AMD,I love Intel but with how the company is doing right now I’m in a “I’ll believe it when I see it” type of skepticism.,Neutral
AMD,"Sorry I am making unreasonable requests. Can you test the game with APO on, and a few background tasks ? Like a few browser tabs, a youtube video playing, discord etc ? I am interested in the E-core usage, particularly the spread of E-core usage.   Thanks.",Neutral
AMD,"You need to enable it in the BIOS, me ale sure the DTT driver is installed and then download the app from the Windows 11 store",Neutral
AMD,"Which they already do, and are often overworked and underappreciated for.   But sure, it's their fault they didn't optimize the game even more for a subset of already relatively high end CPUs....",Negative
AMD,"You are massively oversimplifying. A lot of games that support APO, like Metro Exodus EE OP used as an example, were released before Intel released CPUs with P and E cores. Expecting studios to rework and optimize old games for new CPUs is very naive.",Neutral
AMD,"They should be focusing on optimizations that help the vast majority of all CPUs, especially older/lower end ones.   It's not their job to specifically help one companies latest CPU architectures because they couldn't figure out how to create a core that has both good area and performance.",Negative
AMD,That's not simply what APO does.,Neutral
AMD,"You should have the full list of current APO optimized titles, unlike myself which only has the original list LOL!",Neutral
AMD,"Everyone should be skeptical, sure, but I choose to be hopefully optimistic also.",Positive
AMD,"I booted it up again, this time with three Chrome tabs open including one that had an active YouTube video playing.  The efficiency cores were definitely being utilized for the background tasks I believe as I could see a small bit of load going from core to core.     [https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png](https://imagizer.imageshack.com/v2/3621x2037q90/923/gLnqlo.png)",Neutral
AMD,i am not talking about older games. i am talking about newer games.... really dude?,Neutral
AMD,Ok…all CPUs include Intel CPUs…I don’t get your thought process,Neutral
AMD,"According to Intel, the app is entirely optional. The DTT driver already includes APO, the app is just an interface to control it and disable APO, if needed.",Neutral
AMD,Thanks a lot.,Positive
AMD,> i am not talking about older games. i am talking about newer games.... really dude?  New games seldom have problems with P and E cores. That is why if you took at minute to look at games that APO support you would notice almost all were released before 2022 before release of Alder lake CPUs. Because older P and E unaware games struggle. Don't really dude me if you are this clueless.,Negative
AMD,"APO includes specific optimizations made for specific Intel skus, not general optimizations that help pretty much all processors.",Neutral
AMD,That is not true whatsoever. Games do not properly utilize the p and e cores and unreal 5 is very known for this.,Negative
AMD,Ok…? If there is a specific problem then they need to optimize it for that hardware. Literally a devs job. Still don’t get your thought process,Negative
AMD,"Dude, Unreal 5 runs like crap on pretty much all hardware. It stutters on AMD and nVidia graphics. Unreal 5 doesn't count.",Neutral
AMD,"There isn't a specific problem, it still works lol, just not as well as it could on a specific architecture.   They shouldn't be wasting their finite resources/time on optimizing specifically for an already relatively high end and well performing architecture, but rather attempting more generalized optimizations that help all users.   It really shouldn't be too hard to understand.",Neutral
AMD,"Very cool. Cheers Intel! As a 6900XT owner, I use Xess whenever it's available. The DP4A version got very solid once V1.3 hit.  Hopefully more devs implement it.",Positive
AMD,"I like how Intel still support DP4A model. My meteor lake chip gonna love this XeSS2 with FG and LL.  Meanwhile Amd give middle finger to their consumer who don't have radeon 9000 series, FSR4 doesn't works at all on their previous GPU including the flagship rx 7000 series.",Positive
AMD,Did the DP4A version also improve from 1.3 to 2.0?,Neutral
AMD,"I thought it was a mix of both shader model 6.4+ and DP4a, not either or",Neutral
AMD,Okay but why would I want to use that instead of NVIDIA DLSS?,Neutral
AMD,It’s the least you should get after not getting FSR4.,Negative
AMD,You'd use it over FSR if that's available too?,Neutral
AMD,Except you NEVER got acces to the DP4a version tho. Intel cross vendor locked the DP4a path to their igpus and only allowed Shader Model 6.4 version for other gpus.,Neutral
AMD,"I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4.   On Linux they managed to emulate it using the FP16 instruction, which is available on RDNA3, but it seriously impacts performance and introduces 1,5ms of input latency, compared to the 0,5/0,8ms of latency on RDNA4 with hardware FP8.",Negative
AMD,And they expect people who bought previous RDNA to buy more RDNA,Neutral
AMD,Not by a significant amount.,Neutral
AMD,DP4a only relevant for intel igpus as its crpss vendor locked by intel. Other gpus only get the Shader Model 6.4 version.,Neutral
AMD,for the games that dont support DLSS,Neutral
AMD,"If you own a card that doesn't support dlss.   This is an extra option, it doesn't affect your choice to run dlss or not.",Neutral
AMD,10 series cards will benefit from this,Neutral
AMD,It's great news for gpu that can't use dlss because fsr3 is a blurry shitfest.,Positive
AMD,"I mean it gives better image quality than FSR 3.1, so I know I definitely do.",Positive
AMD,Sure. The image quality is generally better. I'd only use FSR if I was trying to squeeze every last drop of performance out of my GPU since it's faster than Xess DP4a. But Xess will be tried first.,Positive
AMD,Do you have a link? Anything I have seen points to the GPU using Xess DP4A unless it's an ARC GPU. So it supports RDNA2 and newer on AMD and Turing and newer on Nvidia.  https://www.pcgamer.com/intel-xess-compatibility-nvidia-amd/  And I do literally run Xess on my 6900XT (currently using it for F7 Rebirth through Optiscaler). So if I'm not running Xess DP4a then what am I running?,Neutral
AMD,">I don't want to defend AMD, but 7000 series lacks the FP8 instruction to hardware accelerate FSR4   Sure, but they can make FSR4 for non hardware accelerated like Intel right? FSR 3.1 is so bad even when compared to Intel XeSS 1.3 DP4A, but Amd refuse to improve it because they want to force people to buy radeon 9000 series.",Negative
AMD,"Amd really all in when it comes to copying Nvidia. Guess what? The more you buy, the more you save!",Positive
AMD,DP4a is cross vendor.   XMX is Arc only.,Neutral
AMD,DLSS is the widely adopted upscaler in the current market. I'd be shocked if a major game in 2025 releases without DLSS at the very least.,Neutral
AMD,1080ti heard no bell,Neutral
AMD,"You run the Shader Model 6.4 version which i stated. There was a discussiona bout DP4a being cross vendor locked but for the life of me i cannot find it again. The article you linked is from 2021, in early 2022 intel cross vendor locked DP4a. The main article of the post clearly states Shader Model 6.4.",Neutral
AMD,where did amd touch you bud?,Neutral
AMD,"Yeah more and more games are adding DLSS support, so I'm not sure when I would need Xess",Neutral
AMD,"That's true but in edge cases where even the lightest Xess option is still too heavy, FSR may still deliver a better overall experience.   E.g. CP2077 on the steam Deck in Dog Town. When I was testing some stuff out (trying to see how close I could get my Deck to the Switch 2 build) I found Xess to be a unsuitable. Even on the lightest Xess mode, the performance hit was a bit too noticeable. FSR doesn't look as good as Xess but it runs better and you can get away with a bit more upscaling artifacting on the Decks relatively small screen. Just rendering at a lower res and letting the screen upscale was also a solid option in that scenario.  As I say though, that's an edge case where you really just need outright speed. If your GPU doesn't support DLSS or FSR4 then using Xess makes sense in the vast majority of cases.",Neutral
AMD,Are you sure you're not getting the DP4A/Shader Model 6.4 and Arc exclusive version mixed up? DP4a was added to Shader Model 6.4. https://github.com/microsoft/DirectXShaderCompiler/wiki/Shader-Model-6.4.  There is a separate path of Xess that is better than DP4A but that only works on Arc Cards.,Neutral
AMD,"It's good to see finally Intel working with game developer to bring optimization to Intel platform. This time not only optimization for Arc GPU, but also for Intel Core CPU like proper CPU scheduling for P and E cores to make sure the game runs on the right thread.  I think this also the reason why they never update APO game support again. Obviously optimizing their platform by working directly with game developer is better than doing it by themself.  I really hope we got optimization like this for more titles.",Positive
AMD,News like this will never been posted on Yahoo Finance or Stocktwits... Instead the news from Short-Sellers and Paid Bashers ...,Negative
AMD,"Im not impressed, my i9 12900k is bottlenecking my 4080 like crazy in bf6 beta :s  Im only getting 100-140 fps on my cpu while my gpu wants me to have 200. Any kind soul who can explain how i increase this?  My cpu is right out the box, i have not done any OC what so ever",Negative
AMD,13700k vs 9800x3d BF6 Beta menu cpu fps comparison (105 vs 370 fps) [https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8](https://youtube.com/shorts/SxtVlcGbQX0?si=2s4A50cFoTL3luj8),Neutral
AMD,You sound like an intel stock holder lmao The markets don't care about gaming at all.  They only care about AI slop,Negative
AMD,"the game is just really latency sensitive, i dont think it will even change in the full release sadly  i kinda wonder what makes the game so demanding on CPUs, the mechanics and physics arent much better compared to older BF (like BF4) that barely use modern CPUs beyond 20 - 30%",Negative
AMD,"Dealing with same issue on a 285k + 5090. dipping to 125 fps curing certain scenarios specifically on the cairo map at point C. MY system should not struggle at all at 1440p to reach 144 fps, but the 285k is getting absolutely destroyed by bf6 and I have no idea what the issue is,",Negative
AMD,"It's not like game runs bad on Intel CPU which has P and E cores, i will be lying if i say that. It's about maximizing CPU performance. CPU demanding game will runs with much higher FPS when it knows which thread should be utilized.    This is why game with APO enable like R6S on i9-14900K at 1080p low settings, it got performance increase up to 100FPS which is massive.",Neutral
AMD,"Most games are optimized for consoles which are natively eight cores, and a lot are ported over to PC and so you basically need eight fast cores in whatever CPU you happen to have, and the rest don't really do much. That's why Intel HEDT and AMD's Threadripper aren't of much use in this regard.",Neutral
AMD,"No, that's because and isn't really better for gaming unless you get their x3d variants. Those have a different style of memory that is better for gaming in particular but can underperform when it comes to workload tasks. Idk why 🤷",Negative
AMD,"Yeah and in a game like bf, I only care about the 1% low staying at minimum my monitor refresh rate 🫠 But I have not spend that much time optimizing my settings so I think I can work around it",Neutral
AMD,"I found a ""fix""   Dynamic render scale, I set it to 75, so when shit really hits the fan, it just lowers the render scale for just enough time for me to not drop below my monitor refreshrate, and it happens so fast I don't see the change in gfx",Negative
AMD,"From what I understand, many games are generally fairly latency sensitive. The fast cache on the X3D chips basically helps with that.",Positive
AMD,Thank you :),Positive
AMD,Wouldn't the 300 series actually be Arrow Lake Refresh?,Neutral
AMD,"Videocardz is wrong      This chip will likely be called ""Core Ultra 5 445k"" it's an 8+16 die with bLLC + 4LPe   (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")    Ultra 3 has 4P + 4E + 4LPe  Ultra 5 has the 1x 8+16 tile + 4LPE   Ultra 7 has 8+6 P-Cores and 16+8 E-Cores +4LPe   Ultra 9 had 2x 8+16 tiles + 4LPe tile   Ultra 300 series will be reserved for mobile only Panther Lake and/or Arrow Lake Refresh   Ultra 400 will almost certainly be Nova Lake.",Neutral
AMD,I think this competition will tight up alot in the next gen for both intel and AMD.  Intel has had a superior memory controller for awhile now and AMD will finally address this in Zen 6.  AMD has better cache setup with VCache which Intel will finally address in NL.  May the superior arch win!,Positive
AMD,This is pat's work,Neutral
AMD,This is normal Nova Lake or the bLLC version? I heard the bLLC version has separate SKUs.,Neutral
AMD,8 Big cores with loads of cache and then a couple of E cores with their own cache for background tasks would be great.,Positive
AMD,Large L3 cache without reducing latency will be fun to watch.,Neutral
AMD,Is Intel finally making a comeback with their cpus? I hope so,Positive
AMD,the specs sure do shift a lot..,Neutral
AMD,it's just a bunch of cores glued together - intel circa 2016 probably,Neutral
AMD,Not sure I can trust Intel again after the BS around Arrow Lake and the lack of ongoing LGA 1851 upgrade path.,Negative
AMD,"Now we are talking Intel! If they can hold on to these chips, not melting themselves, then it's gonna be a good fight",Positive
AMD,Bout time,Neutral
AMD,"Like I commented before, hope (but really don't believe) this is NOT the product that is only 10% faster than arrowlake because liquidation awaits them otherwise",Negative
AMD,Will it be available in fall of this year or 26Q1?,Neutral
AMD,So it only took them like 5 years to come up with a luke-warm response. cool cool. It's especially bad as Intel actually introduced something similar (eDRAM LLC) into the x86 space many years ago.,Negative
AMD,Noob question:  Is this their 16th gen chips?,Neutral
AMD,Planning on upgrading from my 13700K to whatever the top end Nova Lake is,Neutral
AMD,Only Core i3 can have 65watt ? no more i5 at 65Watt ?,Neutral
AMD,"For some reason it seems to me that all these new different micro cores are cores from ten years ago at the level of Broadwell or Lynnfield, but made on thin nm to fit everything under one lid lmao",Neutral
AMD,Cool.  I have said for a while now that is want both the extra cache AND a few lesser cores to offload stuff to.  The only alternative that has this atm is the 9900x3d and the 9950x3d.  Next gen when im buying new stuff then perhaps intel will have an alternative to those then.,Positive
AMD,"The 52 cores is exciting, but I would probably use Linux as my main OS and run Windows in a VM. Plenty of cores and plenty of RAM and it would be a great machine.  A 52 core desktop beast with an Intel graphics card with 128 GB VRAM would be a killer machine. LLMs would run like lightening.",Positive
AMD,"Intel mocking the chiplet design, describing it as gluing processors together and then doing it themselves - this screams that energy once again.",Negative
AMD,So Nova Lake IS happening? This talk about skipping to next node had me anxious..,Negative
AMD,"So alder lake on cache steroids ?  If the heavier core count chips don’t have the LLc cache, and if this chip is faster at gaming than the heavier core count chips, how will Intel market it?   If they call it a core ultra 5 or 7, then that will make the core ultra 9 seem less of a halo product.  Will be interesting to see how they market it.",Neutral
AMD,"E core need to get lost for gaming, we need 9800x3d rival, purely for gaming !",Neutral
AMD,"""E-Cores"",  Ewww, Gross.",Neutral
AMD,"**Hi,**  I’m currently running a 9700K paired with a 5070 Ti. Do you think I should upgrade to Arrow Lake Refresh now, or hold out for Nova Lake?  The CPU’s clearly starting to bottleneck a bit, but it’s not super noticeable yet.  Is it worth waiting for Nova Lake, or better to just go with Arrow Lake Refresh when it drops?",Neutral
AMD,NVL will definitely be the 400 series. PTL is 300.,Positive
AMD,"Naming can be changed, but the most important thing is, this isn't the upcoming arrow lake refresh  I hope arrow lake refresh fix the memory controller latency tho",Neutral
AMD,"Yes, I expect the 300 series to be Arrow Lake refresh and Panther Lake.",Neutral
AMD,"> (Not sure what suffix Intel will use to denote bLLC SKU maybe ""L"")  Maybe they will bring back the ""C"" from i7-5775C.   Behold, the Core Ultra 7 465KFC",Neutral
AMD,I suspect it'll be like:  495K: 16+32.  485K: 12/14+24/32.  475K: 8+16.  465K: 8+12.  Etc.,Neutral
AMD,Seeing all those different cores on a chip does look cool. But I presume this idea will end up being butchered by Windows not being able to schedule things properly,Neutral
AMD,"So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake? Or in 2027? Either way Zen 6 vs Nova Lake will be interesting.",Positive
AMD,Taking the 'L' to their own CPUs would generally be considered a bad marketing move.,Negative
AMD,"We'll see how far Zen 6 takes us on memory. Strix has improvements, hopefully that wasn't all of it though.",Neutral
AMD,"If intel doesn't screw itself with stupid tdp, reliability issues, or bad relative performance",Negative
AMD,More like *despite* him.,Neutral
AMD,All the SKUs rumored so far are BLLC,Neutral
AMD,"They have graduated from background tasks. At this point, even LPE cores can handle apps like Microsoft teams by themselves with no issue never mind the E cores on the ring bus",Neutral
AMD,"If the scheduler can really support this. As soon as a game would hit the e cores, it could turn bad.",Negative
AMD,Why not 8 P cores with HT + even more cache and no e cores at all,Neutral
AMD,"A large L3 will help a ton with latency. It's why AMD has so much success with it using chiplets which have a ton of extra latency. Reduces all those higher latency trips out to RAM. Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile and if that's the case this L3 might be a really big hit for gaming, etc.",Positive
AMD,If latency slightly increases or stays the same due to adopting Pantherlake optimizations and the newer Foveros then that might be enough to narrow the gap,Neutral
AMD,I assume the cache is based on Lunar Lake's memory side cache which doesn't bring much if any noticeable latency over Arrowlake H,Neutral
AMD,"Also done with them. Changing socket every \~3 years was also bad, but this is disgusting.",Negative
AMD,"Not sure how it could be, given all the extra cores.  That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  I expect NVL to be a big power/thermals/multithreading win, than anything else.",Neutral
AMD,Nova lake? More like 26Q4.,Neutral
AMD,Only Pantherlake for mobile,Neutral
AMD,"It doesn’t matter so long as all major vendors will automatically ship with whatever Intel puts out. Lenovo, dell, and HP carry a grand total of *zero* desktop Zen 5 (granite ridge) machine across their consumer, gaming, and business line. You heard that right, *zero* among the global big three.",Neutral
AMD,"Since when they are ""mocking chiplet design""? Have you forgot Intel is the first when it comes to gluing chip together? Intel did this with Core 2 Quad. Meanwhile Amd copying that with ryzen but sadly people don't want to admit it.",Neutral
AMD,It is really happening. The only question is when? Or can they release it on the next year without delay?,Neutral
AMD,"Core Ultra 9 has 48 cores (and 4 LPE cores). It slaps you in the face as halo SKU (unless it becomes entry level threadripper target?).  Currently rumored:  (8+16E+BLLC, 8+16E)  8+16E+BLLC  8+12E+BLLC",Neutral
AMD,The i9 sku is rumored to be two 8+16 tiles. So slotting in an extra bLLC 8+16 tile instead of a normal 8+16 one for the top sku doesn't sound impossible.,Neutral
AMD,Probably same way AMD did the 5800X3D 8 core which was slower than the 5800X at productivity and had less cores than the 12 core 5900X and 16 core 5950X.,Neutral
AMD,"E cores are the future, P cores days are numbered.",Neutral
AMD,ARL refresh is not worth it. Go AMD or wait for NVL if you want.,Negative
AMD,If you have to upgrade now for gaming get a 9800x3d from AMD otherwise wait and see what AMD Zen 6 and Nova Lake can do.,Neutral
AMD,"You just missed some pretty killer deals around Prime Day; that would have been the ideal time to go with Arrow Lake.  The 'refresh' will offer better performance, but I highly doubt the value difference will be significant.     You can definitely wait a bit.",Negative
AMD,"I would like the naming with suffix ""C"" for cache at the end or maybe ""G"" for gaming.",Neutral
AMD,"W11 has (mostly) fixed the scheduling issues, in fact the scheduler was already pretty good by the time Raptor Lake launched. Sometimes it can mess things up but it's quite rare nowadays. The only time I have seen some small issues is with legacy single threaded software.",Neutral
AMD,"Why would it be butchered? the issues with big/little are long gone now - but I guess to your type none of that matters - getting upvotes for blind anti-MS zealotry is all that matters.  but given that this is an intel sub, i am surprised its not merely blind anti intel zealotry instead.",Negative
AMD,">So if Arrow Lake Refresh comes out this year, next year we'll get Nova Lake?  Yes. Intel has confirmed NVL will launch in 2026.",Neutral
AMD,The former ceo,Neutral
AMD,"Raichu has repeatedly talked about BLLC ""variants"", so I wouldn't assume that all skus have the large L3.   Plus, it seems extremely wasteful to have all the skus have BLLC.",Neutral
AMD,"Yeah, Skymont are more like space efficient P cores that are down clocked and don't have all of the features. IPC is really good.",Positive
AMD,This is wrong. Arrow Lake already uses E cores for gaming to no performance hit.   12-14th Gen sure. But not with the new E cores and no HT. You’re using dated info.,Negative
AMD,"A number of public tests have shown that, even with all the P-cores disabled, ARL is still pretty good in gaming.  It's not the E-cores.",Positive
AMD,No different to 12-14th gen then.,Neutral
AMD,"windows pretty much gave up on this and just allows you to set to performance mode, which always tries to grab a P core first.",Negative
AMD,"Or that I forgot they don't have HT on the newer chips.   To be honest I was meaning 14th gen P and E cores with more cache not the new gen stuff. We know the IPC, memory controller and core speed is enough it's just the cache that makes it fall behind AMD in gaming.",Neutral
AMD,E core is 30% faster than hyper threading,Positive
AMD,HT is worse than E cores,Negative
AMD,>Intel's Nova Lake processors are expected to feature the memory controller integrated back into the CPU tile  They aren't,Neutral
AMD,It will enlarge the ontile L3 cache most likely with a cache only tile with high performance die to die connect. No 3d stacking until 18A-PT is available for production which will likely be well after Nova Lake tapeout which should be done now or with in the next few months in order to ramp up production in early 2026 and be on the market by late 2026.,Neutral
AMD,">That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point.  If you are in the lead, maybe this argument is valid. However Intel is behind Apple in ST and behind AMD in gaming. They have plenty of room to improve.",Neutral
AMD,"> That said, for lower core count workloads, like gaming, I doubt gains of more than 10% are achievable by anyone, at this point  And yet we see that constantly.",Negative
AMD,"Ze3vsZen2, Zen 4vsZen3, Zen 5 X3D vs Zen 4 X3D, Alderlake vs RocketLake, Raptorlake vs AlderLake, Lunarlake/Arrowlake-H (Mobile) vs Meteorlake U and H, Gracemont vs Tremont, Skymont vs Gracemont and Crestmont, Many Arm or Arm based designs, etc",Neutral
AMD,ARM chips regularly do this sometimes on a yearly basis.,Neutral
AMD,A lot of gamers have no clue that Intel is actually doing well with ARL for OEM laptops/dekstops as ARL is basically a laptop design warmed over for desktop chip usage.,Neutral
AMD,Core 2 quads and Ryzen chiplets are totally different ways of doing MCMs  C2Q didnt have an IO die and it pretty much worked like having a 2 socket motherboard      IO die is what enabled much lower latency and the ability to connect much more CPU dies together (EPYC 9005 got 16 CCDs),Negative
AMD,https://www.pcgamer.com/intel-slide-criticizes-amd-for-using-glued-together-dies-in-epyc-processors/,Negative
AMD,"> They stopped trying to sell capacity to external customers and they will try again with 14a. It's not like they had a lot of capacity to spare. It's not even enough for themselves  Intel claims they can double to 1.5x their EUV capacity in arizona and ireland, if customers asked for it, in 1-2 years.   They built out all they needed for their own internal 18A demand too.",Neutral
AMD,"It shouldn't become an entry level workstation part since the Ryzen 9 will have 24c/48t to go against it.   48 cores (plus the 4 LPe, but those shouldn't factor in too much) should handily beat 48 threads, but not by enough to reach Threadripper territory.",Neutral
AMD,Most likely the bLLC variant wil be single compute tile and single cache tile as this would be cheaper for now than putting bLLC cache in base tile like Amd did. They can do that when 18A-PT is avaliable which will probably be after Nova Lake goes into production with 18A or maybe 18A-P,Neutral
AMD,"Lmao, some of my games still runs on e cores",Neutral
AMD,People are still disabling e cores for more performance.,Neutral
AMD,"Varients because there are two tiles. One with BLLV, one without. Full chip had 1 BLLC chip and 1 without. No actual SKU (not tile) without BLLC in some way so far",Neutral
AMD,"They are nothing like P-core as designed with no hyperthreading, area/space efficienct and integrated into clusters of 4 e-cores with shared L2 cache",Neutral
AMD,fym no different they're 50% faster,Positive
AMD,No. ARL architecture is definitely not optimized for latency but hiding memory access which favours multithreading instead of gaming. Hopefully Nova Lake will optimize all the latecy issues with ARL arch and decrease latency to L3 considerably aswell,Negative
AMD,"Yes, but in games, it's the P-cores that are meant to be utilized. Also, more than eight cores or 16 threads aren't really necessary for gaming. The 9950x3d isn't faster, than the 9900x3d.  My point is that the space taken up by the E-cores could instead be used for additional cache, not?",Neutral
AMD,I had heard this both ways and never really know who to trust until I see it. Anyhow if that's true the large L3 should be even more helpful for things like gaming as it will keep those higher latency hits down. AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help.,Neutral
AMD,"As a advice, Nova Lake will further increase memory latency.",Negative
AMD,In which gen iteration?,Neutral
AMD,Editorialized. But its funny because said quote is attributed to AMD when Intel made Core2Duo (or Quad?),Neutral
AMD,"If its is with 10% of threadripper 9960x at half the price, it will definitely hurt the low end threadripper cpus a lot for productivity/creator types who would love a more reasonable priced high performance entry level workstaion. Low end workstation is probably bigger market than high end gaming desktops anyways.",Negative
AMD,"Another factor is that Apple has booked out all the N2 capacity for the better part of a year before AMD can touch it.  Nova Lake will have been out for a while, before AMD shows up",Neutral
AMD,Process lasso can let you just designate by task so the E cores can do background and side jobs. People use process lasso on the 7950x3d to make the X3D chiplet run games with those 8 cores and the other 8 core chiplet for everything else.,Neutral
AMD,What issue did you run into with a 9800x3d? It has one CCD and no small cores,Neutral
AMD,"Nowadays there are very few cases where disabling E cores is extremely beneficial, sometimes they are a detriment to performance but more often than not they are useful. Not to mention the E core design team is more competent than the P core team and are quickly catching up in performance.    The E cores in Alder Lake had relatively poor ST performance but in Arrow Lake they are much faster and closer to the P cores in raw performance. Every new generation the gap keeps shrinking and it will make the disabling of E cores less and less relevant.    Sometimes the Windows scheduler will wrongly assign an E core to a ST bound task, even if a P core is available, but it doesn't happen often enough to be a serious issue.",Neutral
AMD,"yes, but not because of windows scheduling - its because you can OC higher with them off - better temps and easier stability - some people disable HT for the same.",Neutral
AMD,They don't pay attention,Negative
AMD,"No. Intel needs to do what Amd did and add seperate bLLC just for gaming. As long as latency is reduced to the L3 cache including bLLC, gaming performance will definitely increase as 285K and 9950x have similar performance in both gaming and productivity with small lead going to Amd. But x3d totally dominates ARL beacsue of the obvious huge cache difference and lower overall latency to their cache implementation.",Neutral
AMD,That is more so bc the 9950x3d isn’t really a 16 core cpu it is two 8 cores,Neutral
AMD,"There was one article a while back that reported something like ""intel goes back to integrated mc"", not understanding that panther lake is a fundamentally different product than the tile based desktop products.",Neutral
AMD,How do you think they're doing 2 compute tiles if the memory controller is on one?,Neutral
AMD,">I had heard this both ways and never really know who to trust until I see it.  I'm curious, which leakers have said otherwise?   You could still have mem latency improvements without the mem controller being on the compute tile too, so the situation there can improve either way.   >AMD has always had higher latency due to the same kind of design with a separate memory controller so we have pretty good data on how extra cache can help  Yup, and ARL has higher mem latency than AMD even, so obviously there is room to improve even without having to move the mem controller back.",Neutral
AMD,"Intel will definitely decrease latency with Nova lake and as long as they add a bigger L3 cache variant, gaming performance will increase further to challenge AMD",Positive
AMD,"If you go straight to the source, the quote they used is directly on Intel's slides. Intel literally wrote that, regardless of who said it first.   [https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg](https://cdn.mos.cms.futurecdn.net/pPtQr2PKSemDqkhahtz8VN.jpg)",Neutral
AMD,"I mean that it won't be priced like an HEDT part. It'll definitely perform like current generation lower end Threadrippers, but so will the 11950x.",Neutral
AMD,"NVL high end desktop skus are rumored to be on N2 as well. Intel has confirmed NVL desktop will be on TSMC, and Intel has also confirmed some NVL compute tiles will be external.   It's very possible Zen 6 DT is out before NVL-S, and it's very likely that NVL-S and Zen 6 launch within a few months of each other, if that even.   Also, if AMD wants to pay for TSMC N2 capacity, TSMC will build it out. Just a couple days ago we had a [report](https://www.digitimes.com/news/a20250724PD213/tsmc-2nm-capacity-2028-3nm.html) about TSMC being very aggressive with N2 scale out, and the reasoning here is two fold- first of all, 2026 is technically the 2nd year of N2 HVM, and also because a *lot* of major SOCs, other than just Apple and AMD, will be on N2- demand for N2 is high, and build out will be too.",Neutral
AMD,AMD is one of the leading N2 early adopters. Both TSMC and AMD announced this.,Neutral
AMD,That didn't stop Intel with N3B,Neutral
AMD,The recent Warhammer space marine game is a game that benefited from disabling e cores. I am not sure the disparity between arrow and alder in that game tho.,Neutral
AMD,">Intel needs to do what Amd did and add seperate bLLC just for gaming.  Thats what I tried to say.  >My point is that the space taken up by the E-cores could instead be used for additional cache, not?  Or isn't that enough? AMD stacks the cache directly on top (9000 Series) or under (9000 series) the CPU cores, right?",Neutral
AMD,"Yes but IF Intel decides to do quad channel for AI nonsense at least for flagship model, Amd threadripper will be stomped over as I will definitely buy the Nova Lake part just for the price/performance compared to 9960x threadripper. The motherboard selection alone for Z890 is absolutely amazing and RDIMM prices aretwice as expensive for the same amount of memory. I expect with CAMM2 that max ram capaciity will go for 512GB and maybe 1TB for Nova Lake aswell. Even with 2 channel, cudimm will go above 10,000 MTS and maybe 12000 if we are lucky on CAMM2 modules by 2027 as CAMM2 definitely being used for DDR6   I do feel Intel has I/O advantage beyong gaming and that market is getting bigger than hardcore gamers all the time!! A decked out 256GB Arrow Lake z890 based computer is way cheaper than same memory sized threadripper 9960x.",Positive
AMD,"I don't recall Intel stating that NVL core chips would be at TSMC, only that some of the eventual package would be... which implies iGPU, to me, and that wouldn't be N2 (possibly N4, since Battlemage is N5, and N4 would be open and cheaper than N3 nodes).  Do you have a source, where they directly claim the NVL cores are coming from TSMC?",Neutral
AMD,"The game is a dumpster fire when it comes to CPU utilization, it hammers even top tier gaming chips like the 9800X3D. I'm not surprised if the E cores can bottleneck the performance even further, most likely some of the game's heaviest threads get (wrongly) assigned to the E cores.    The scheduling in W11 has been significantly improved over the last few years but it's not 100% perfect either. Legacy software in particular, which typically utilizes 1 core, often ends up running in an E core and as a result you can't take advantage of the higher ST performance a P core provides. In that case you need stuff like Process Lasso to fix the problem. It doesn't happen very often but when it does it's very annoying.",Negative
AMD,"Yup.   >. Then as you look forward, to our next-generation product for client after that, Nova Lake will actually have die both inside and outside for that process. So, you'll actually see compute tiles inside and outside.  Q4 2024 earnings call.",Neutral
AMD,"That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.  Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  That seems like something they couldn't have known in 2024.",Neutral
AMD,"18A is about as good clockspeed as N2, but density is still around N3 level. To make a new architecture, it makes sense for them to target N2 if AMD does the same which they have",Neutral
AMD,">That seems really strange, since ARL was designed for both 18A and N3, and they just went with N3 at the time.   20A, and only the lower end 6+8 dies are rumored to be fabbed on that node.   >Now that 18A is ready, and capacity is reserved for Intel themselves, you'd think NVL would just plain be fabbed there.  Not competitive enough.   >Maybe the 18A fabs are so busy with Clearwater Lake orders, that they would go with TSMC again?  Low end client products like WLC, and even the lower end 4+8 NVL compute tile, is rumored to be on 18A still.",Neutral
AMD,"It doesn't make sense, for either of them, due to the cost of being an early adopter",Negative
AMD,Will it also introduce Lunar Lake successor?,Neutral
AMD,I wonder if this could become avaliable on LGA 1954 because that'd be a good alternative for budget builds potentially if the iGPU is potent enough.,Positive
AMD,Is anyone left?,Neutral
AMD,I would love to see a Nova Lake-A (strix halo like) APU with an 8+16 tile + 20Xe3 cores which is equal to 2560 FP32 lanes or 40 AMD CU's with 16-32mb of memory side cache to handle igpu bandwidth demands,Positive
AMD,"Wonder what node such a product, if it does exist, would use for the iGPU tile.   AFAIK, rumor is that the iGPU tile for the standard parts would be on 18A or an 18A variant. However, if this is a flagship part, I would assume they would be willing to pay the extra cost to use N2...   However that would also presumably involve porting the architecture over to another node, which I don't think Intel has the resources, or wants to go through the effort, to do so.   Also wonder if such a product will finally utilize the ADM cache that has been rumored since like, forever.   And this is a bit of a tangent, but I swear at this point in the leak cycle for a new Intel product, there's always usually a ton of different skus that are leaked that may or may not launch - right now we have NVL bLLC, NVL standard, and now this, and a bunch of them end up not launching (whether they were cut internally, or the leaks were just made up). So it would be pretty interesting to see if any of the more specialized rumored NVL skus end up actually launching.",Neutral
AMD,These designs are going to be the go-to for creator and mid-range gaming at some point in the future. Having it all integrated together should provide opportunities for optimization and form factors that are difficult with discrete cards,Positive
AMD,How long do I have to wait for something that is more powerful than an AMD 7840HS and still cheaper?,Neutral
AMD,Good to hear! I have a strix halo asus tablet the 390 one and its only got 32GB ram but its fast!  Would love to see an intel version…,Positive
AMD,"This is great move by Intel since they aren't making dGPU for laptop so they can get some marketshare by making strong Intel ecosystem. If Nova Lake AX released then people don't need to buy overpriced laptop with Nvidia dGPU anymore, not to mention with Nvidia shitty small vram.",Positive
AMD,"I'm still using Alder Lake, I suppose all these chips will be released in 2026 ?",Neutral
AMD,That's Panther Lake in a few months,Neutral
AMD,Lunar lake was always confirmed even by Intel as a 1 off design with soldered memory and many of its design choices.,Neutral
AMD,"No, the memory config wouldn't work.",Neutral
AMD,No not really.  It's pretty f'n bleak atm.,Negative
AMD,"That isn't that different from the AI MAX 395+ already released, which has 16 Zen 5 cores and 40CUs and some MALL cache (I think 20MB) to help with bandwidth.  For this to be a meaningful upgrade, you'd want more than 20 Xe3 cores, which means you need more bandwidth, which means you need probably LPDDR6 memory to get it high enough as otherwise the cache demands would be prohibitive and LPDDR6 memory isn't coming to client until probably 2027.",Neutral
AMD,It's N2.  Sadly Intel Inside has almost entirely become TSMC Inside,Negative
AMD,"Nova Lake-AX will not be released, and DLLC will not be released, and nova Lake will not be released.",Neutral
AMD,"Unless they can find some way to bring down the cost.....these aren't gonna get much traction, the same applies to Strix Point of course. While more efficient than your typical 2000$ laptop with a decent GPU.....the traditional laptop is still gonna outperform them based on pure specs.  You get more battery hour and less heat but people who use these kind of laptop are used to having it plugged 24/7. Feels more like experimental models but they have a long way to go before they see proper adoption. Particularly the cost.",Negative
AMD,"> mid-range gaming  Strix halo is only in $2000 systems. high end price, mid-range performance.  Good for stock margins i guess.",Positive
AMD,I'm sure that Xe2 in Lunar Lake is faster than the 7840HS iGPU on average if you run it with the max performance mode in the OEM software that comes with your laptop.,Positive
AMD,The 7850HS can't compete with the Xe2 igpu because it's bandwidth starved.   The 8mb of memory side cache insulating the 4mb of L2 from main memory allows the Xe2 Arc 140V igpu in LL clocked at 1950mhz to beat the RDNA 3.5 890m clocked at 2950mhz as it only has 4mb of L2 as a last level of cache before hitting memory.   TLDR: RDNA 3.5 is bandwidth starved on strix point due to lacking memory side cache,Neutral
AMD,PTL's not really a LNL successor.,Neutral
AMD,"Yeah, it was just to prove a point that ARM is overrated.",Neutral
AMD,"I hear you, questioning my decision to return under Pat’s hire-back spending spree. Dodged this one… but this is hitting differently.",Neutral
AMD,What about an 8 + 16 big LLC tile and 32Xe3 cores (4096 FP32 lanes or ~60 AMD CU's) + 64mb of memory side cache?   Or even better a mid range part with an 8+16 tile and 16-20Xe3 cores + 16-32mb of memory side cache,Neutral
AMD,Just read about the JEDEC spec for lpddr6 of 14400mt/s.  That is wild!,Neutral
AMD,Who said it's It's entirely TSMC ? they have been pretty open about majority of tiles Intel the CPU Tile will be shared with N2 ofc though,Neutral
AMD,"It should be Intel here, TSMC there.  Or a little bit of Intc, a little bit of TSMC.",Neutral
AMD,IDM 2.0: Where we proudly declare our fabs are world-class—while quietly handing the crown jewels to TSMC.,Neutral
AMD,"The BOM is lower, so the question is where the markup is coming from.",Neutral
AMD,Consoles have big APUs and they have better value than anything. I know they're subsidized however.,Positive
AMD,A mid range Nova Lake-A SKU with a 6+12 tile and 16/20Xe3 cores with 16-32mb of memory side cache would be sick,Neutral
AMD,Thats the problem with AMD sticking an additional 8 cores instead of infinity cache to fix the bandwidth bottleneck.,Negative
AMD,>still cheaper,Neutral
AMD,"It's the closest that we will get to a Lunar Lake successor   Really, panther lake is a combined successor to both Arrow Lake-H and Lunar Lake",Neutral
AMD,"Low volume and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference.  Combine that with not moving a lot of them compared to the rest of the lineup, and each one of those units has to make up more of the costs from things like bad dies or general spin up costs for a new chip.",Negative
AMD,It already has 32MB infinity cache.,Neutral
AMD,The 7840HS is cheaper because it is older.,Neutral
AMD,"It competes in the U/H lanes, so what is today ""ARL""-U and ARL-H. But yes, in practice it should bring *most* of the LNL goodness over, but PTL-U is still not a true successor in the original ~10W envelope LNL was designed for.",Neutral
AMD,"> and a giant chip package most likely. That GPU tile is going to be expensive. Look at Strix Halo for reference   It's no bigger than the equivalent dGPU, and you save on memory, package, and platform costs. Half the point of these things is to use the integrated cost advantage to better compete with Nvidia dGPUs.",Negative
AMD,The new Ryzen AI 250/260 with 780M is still cheaper because it doesn't have copilot+ .,Neutral
AMD,I will see the performance envelope of PTL U and decide should dump my LNL or not,Neutral
AMD,"The low volume part of that quote is important. If you're making heaps of them, each can take up a little bit of the startup costs for that design. If you only make a few, those costs get concentrated on what you do make.  Low volume also means a single defect is promotionally more of your possible units, and hits the margins harder. You do get more total defects in a larger run, but at sufficiently high quantities of those defects, you start making lower bins to recover some of the defects. If you hardly make the volume to cover one widespread SKU, you aren't likely to have enough to make a cheaper one with any degree of availability. At some point you hit a threshold where it's not worth selling what would be a lower bin because there's not enough of them.",Neutral
AMD,Because putting a 40 TOPs NPU for the Copilot+ certification is costly.,Neutral
AMD,"It doesn't have a 40 TOPS NPU, it's the same 16TOPS one in the 8840HS it's literally just another rebrand. It's cheap because it's from 2023.",Neutral
AMD,Exactly. Hence your Xe2 lunar lake being faster is irrelevant to the question because it won't be cheaper regardless of age when they have a NPU that takes close to 1/3 of die space.,Negative
AMD,I think I didn't say anything that deviates from what you just said.,Neutral
AMD,"Maybe I misread or misunderstood, I thought you said the 260/250 had a 40TOPS NPU",Neutral
AMD,enjoy rhythm aware outgoing practice bike attempt library versed cake   *This post was mass deleted and anonymized with [Redact](https://redact.dev/home)*,Neutral
AMD,Haha wow that is hilariously bad.   And here I thought ASUS newer bios update was bad due to some higher temps,Negative
AMD,"The new Gigabyte BIOS also has booting issues for me, and memory is even more unstable.  At least the UI is entirely in English now.",Negative
AMD,"who needs quality control, what can go wrong?",Negative
AMD,"This is why you don't rush to update software, let others do the testing for two weeks",Neutral
AMD,Gigabyte is trash,Negative
AMD,The Elon Musk method,Neutral
AMD,"> “Yeah just push it to production nothing’s gonna happen, no need to QA/QC this”  Seems to be the motto of a lot of the tech world",Neutral
AMD,"Yup indeed.   I would even go as far as one full month   Hence, why I learned to avoid installing new Windows Update automatically",Neutral
AMD,Intel really needs to be able to compete with X3D or they're going to continue getting dominated in the enthusiast consumer market. I like Intel CPUs and was happy with my 12600K for awhile but X3D finally swayed me to switch over.,Positive
AMD,"Intel has had plans for big ass L4 cache for almost a decade now, just that it never made it past the design board.  Supposed to be marketed as Adamantium. But it got ZBB’d every time I suppose due to cost.  For Intel to implement Adamantium, regular manufacturing yield has to be good enough I.e cost is low so they can splurge on L4.  Of course now they are forced to go this way irrespective of cost. I’d love 16p + L4 CPU.",Negative
AMD,"Honestly, good. I've been using AMD for a while now but we need healthy competition in the CPU space for gaming otherwise AMD will see a clear opportunity to bring prices up",Positive
AMD,"Something interesting is that the extra cache isn't rumored to be on a base tile (like it is with Zen 5X3D), but rather directly in the regular compute tile itself.   On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.   I think Intel atp desperately needs a X3D competitor. Their market share and especially revenue share in the desktop segment as a whole has been ""cratering"" (compared to how they are doing vs AMD in their other segments) for a while now...",Neutral
AMD,Hasn’t this been on their roadmap for a while now? I’m pretty sure they said 2027 is when they’ll have their version of x3D on the market,Neutral
AMD,"These core count increases could be a godsend at the low end and in the midrange. If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming, Intel will have a massive advantage on price.  That being said, if leaked Zen 6 clocks (albeit they’re from MLID, so should be taken with a grain of salt) are accurate, Nova Lake could lose to vanilla Zen 6 in gaming by a solid 5-10% anyway.",Positive
AMD,"Funny how non of this news posted on reddit hardware sub or even allowed to be posted. Guest what? R amdhardware will always be amdhardware! It's painfully obvious that unbearable toxic landfills sub is extremely biased to Amd. Meanwhile all Intel ""bad rumors"" got posted there freely which is really BS!  I still remember i got banned from that trash sub for saying ""People need to touch grass and stop pretending like AMD is still underdog because they aren't"" and the Amd mods sure really mad after seeing my comment got 100+ upvotes for saying the truth, but that doesn't matter anymore because i also ban those trash sub!",Negative
AMD,Intel should be ahead of the curve on things not looking to compete on previously created tech,Neutral
AMD,"Very fine-tuned ARL-S almost reach 9800X3D performance. Extra cache could help to close the gap   Given people are willing to overpay for price-inflated 9800X3D, I wonder if it could work given buyers need an entirely new platform. 9800X3D users are fine for a pretty long time like 5800X3D users did",Positive
AMD,"Lol, requires a new socket. Intel is such trash.",Negative
AMD,Intel will simply always be better than amd,Positive
AMD,"AMD gains tremendously from X3D/v$ because the L3 cache runs at core speeds and thus is fairly low latency, Intel hasn't seen such low latency L3 caches since skylake, which also has much smaller sizes, so the benefits of this could be much less than what AMD sees.   Only one way to find out, but I advise some heavy skepticism on the topic of ""30% more gaming perf from 'intel's v$'""",Positive
AMD,"Either more cache or resurrecting the HEDT X-series... Doesn't matter, as long as there is an affordable high-end product line.",Neutral
AMD,"The 12600k was a fine chip, but AMD had the ace up Its sleeve. I upgraded from a 12600k to a 7950x3d and it was one of the best PC upgrades I ever made.",Positive
AMD,I mean 9800x3D and 14900K offers basically the same performance in the enthusiast segment. Going forward though it would be nice to have more cache so normal users doesn't have to do any sort of memory overclocking just to match 9000x3D in gaming.,Neutral
AMD,4070 ti won’t cut it man - upgrade!,Negative
AMD,Broadwell could have been so interesting had it planned out.,Positive
AMD,"I want a 32 Core/64 Thread 3.40 GHz Core i9-like CPU. Not Xeon like with Quad-Channel and stuff, just 40 PCIe 5.0 lanes and 32 Power-Cores instead of little.big design. 😬",Neutral
AMD,">Otherwise AMD will see a clear opportunity to bring prices up  AMD already did, as you can see zen 5 x3d is overpriced as hell especially the 8 core CPU. Zen 5 is overpriced compared to zen 4 which is already more expensive than zen 3. Not to mention they did shady business like keep doing rebranding old chip as the new series to fools people into thinking it was new architecture when it wasn't and sell it with higher price compared to chip on the same architecture in old gen.  Intel surely needed to kick Amd ass because Amd keep milking people with the same 6 and 8 cores CPU over and over with price increases too! Not to mention radeon is the same by following nvidia greedy strategy.  Edit: Some mad Amd crowd going to my history just to downvote every of my comments because they are salty as hell, i won't be surprised if there are from trash sub r/hardware. But truth to be told, your downvote won't change anything!!",Negative
AMD,"Even though it's not stacked, I believe it's still going to fix the last level cache latency issue MTL and ARL have.   Ryzen CPUs have lower L3 latency than Intel because each CCX gets their own independent L3, unlike Intel's shared L3. Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3, so possibly giving the existing cores/tiles their own independent L3, improving latency and bandwidth over shared L3.  But one thing intrigues me. If this cache level has lower latency than shared L3, wouldn't this more properly be called L2.5 or something below L3 rather than last level cache? Will NVL even still have shared L3 like the previous Intel CPUs? I know the rumor that it will have shared L2 per two cores, but we know nothing of the L3 configuration.",Neutral
AMD,"> On one hand, this shouldn't cause any thermal and Fmax implications like 3D stacking has created for AMD's chips, however doing this would prob also make the latency hit of increasing L3 capacity worse too.  It is already a non-issue since AMD moved the 3D V-Cache to underneath the compute tile.",Neutral
AMD,"Adamantaium was on the interposer, did they change plans?",Neutral
AMD,"Don't remember them saying anything like that, but by around that time their 18A packaging is supposed to be ready for 3D stacking.",Neutral
AMD,"Nova lake= skip of it's just as good as zen, you would be looking at 2 gens after that and then swap from AM5 to intel.",Positive
AMD,"> If a 4+8-core Ultra 3 425K can match an 8+0 core Ryzen AI 5 competing product in gaming  Doubt that since it'll probably lack hyperthreading and the E-Cores are slower, even 6C12T CPUs are starting to hit their limits in games in the last few years, faster cores won't help if there's much less resources to go around, it kinda feels like intel went backwards when they removed hyperthreading without increasing the P-Core count.",Negative
AMD,Intel managed to run Sandy Bridge's ring bus clock speeds at core clocks which resulted in 30 cycles of L3 latency.   Haswell disaggreated core and ring clocks allowing for additional power savings.   Arrow Lake's L3 latency is 80 cycles with a ring speed of 3.8ghz,Neutral
AMD,"I'd like to see the HEDT X-series come back too, but Intel would have to come up with something that would be competitive in that area.  It's not hard to see why Intel dropped the series when you take a look at the Sapphire Rapids Xeon-W lineup they would have likely been based off of.  I think AMD would also do well to offer something that's a step above the Ryzen lineup, rather than a leap above it like the current Threadrippers.",Neutral
AMD,Well it was a downgrade on system snappiness as intel have way higher random reads than amd.,Positive
AMD,> I mean 9800x3D and 14900K offers basically the same performance  LMAO,Neutral
AMD,"Huh? 9800x3d is universally known to be like 20-25 percent faster, even in 1 percent lows.   https://www.techspot.com/review/2931-amd-ryzen-9800x3d-vs-intel-core-14900k/",Neutral
AMD,Maybe that is your experience.  Neverthelesss if you compare most gamers who switched to 9800x3D they report a significantly noticeable uplift in fps and 0.1 fps. Maybe a negligible few reported a decrease. And this has very likely nothing to do with the x3D CPU but other causes.,Neutral
AMD,"Ah you’re missing the final piece. As far as i’m aware this pretty much requires controlling the OS as well (or at least solid OS support). Consoles get their own custom operating system, Apple built a new version of MacOS for M chips. Intel and AMD though don’t control windows.",Neutral
AMD,UMA is such a hassle That's why I don't see it much except for calculation purposes (HPC/AI)...,Negative
AMD,"Application developers are supposed to try to avoid copies from GPU memory to CPU memory, instead letting it stay in the GPU memory as much as possible",Neutral
AMD,">so there is still the cost of useless copies between system RAM vs allocated GPU ram.    There is none, AMDGPU drivers have supported GTT memory since forever, so static allocation part is just to reduce burden for app developers but if you use GTT memory you can do zero-copy CPU+GPU hybrid processing.",Negative
AMD,"Intel needs something decent because AMD has taken a page out of intel (up to gen7) playbook, same cores no changes. Intel now provides more cores but it's the 100% core increase Vs AMD 50% and bLLC that should shake things up, hopefully they keep the temperature down as I don't want to have to replace case and get a 360mm rad just to not throttle, and not ever again do a 13th and 14th gen degradation show.   If all goes well going back to intel for a few years then AMD, brand loyalty is for suckers, buy what's best for performance and value. Hopefully intel i5 has 12P cores and i7 18-20P cores that would be nice to have",Neutral
AMD,"bLLC is just a big-ass L3$ and since Intel does equal L3 slices per coherent ring stop, it'll be 6\*12 or 12\*12 with each slice doubling or quadrupling. The rumor is 144MB so quadrupled per slice, probably 2x ways and 2x sets to keep L3 latency under control.",Neutral
AMD,"Intel and AMD have effectively the same client L3 strategy. It's only allocated local to one compute die. Intel just doesn't have any multi-compute die parts till NVL.   > Now in NVL, the BLLC configuration will replace half of the P-core and E-core tiles with L3  8+16 is one tile, in regardless of how much cache they attach to it",Neutral
AMD,It is a massive issue for amd. You're voltage limited like crazy as electron migration kills the 3D cache really fucking fast. 1.3V is already dangerous voltage for the cache.,Negative
AMD,"I still think there's a slight impact (the 9800x3d only boosts up to 5.2GHz vs the 5.5GHz of the 9700x), but compared to Zen 4, the issue does seem to have been lessened, yes.   And even with Zen 4, the Fmax benefit from not using 3D V-cache using comparable skus was also only single digits anyways.",Neutral
AMD,"Adamantium was always rumored to be an additional L4 cache IIRC, and what Intel appears to be doing with NVL is just adding more L3 (even though ig Intel is calling their old L3 the new L4 cache? lol).   I don't think Intel can also build out Foveros-Direct at scale just yet, considering they are having problems launching it for just CLF too.",Neutral
AMD,"I'm an e-core hater but arrow lake e-cores are really performant and make up for the loss of HT. arl/nvl 4+8 would wildly beat 6c12t adl/rpl.  HT was always a fallacy anyway. If you load up every thread, your best possible performance is ~60% of a core for a games main-thread.  I would much rather pin main-thread to best p-core in a dedicated fashion and let the other cores handle sub threads. Much better 1% lows if we optimize for arrow lake properly (still doesn't hold a candle to 9800X3D with HT disabled though).",Neutral
AMD,"Yeah, I somewhat agree with this. I suppose it depends if Intel’s latency problem with their P+E core design is at all a fixable one - 4c/8t is still shockingly serviceable for gaming, but 4c/4t absolutely is not.",Neutral
AMD,It's the same ratio as 285K 8P+16E vs AMD 16P and we know that 285K is competitive despite no hyperthreading,Neutral
AMD,"Sooo a few months ago, I helped a buddy of mine troubleshoot a black screen issue on his newly built 9800X3D and RTX 5090 rig, a fairly common issue with Nvidia’s latest GPUs.  While working on his PC, I'd notice a series of odd and random hiccups. For example, double clicking a window to maximize it would cause micro freezes. His monitor runs at 240Hz, and the cursor moves very smoothly, but dragging a window around felt like it was refreshing at 60Hz. Launching League of Legends would take upwards of 10+ seconds, and loading the actual game would briefly drop his FPS to the low 20s before going back to normal. Waking the system from sleep had a noticeable 2-3 seconds delay before the (wired) keyboard would respond, which is strange, considering the keyboard input was what wake the system up in the first place.  Apparently, some of these things also happen to him on his old 5800X3D system, and he thoughts that these little quirks were normal.  I did my due diligence on his AMD setup: updated the BIOS and chipset drivers, enabled EXPO profile, made sure Game Bar was enabled, set the power mode to Balanced. Basically, all the little things you need to do to get the X3D chip to play nice and left.  But man... I do not want to ever be on an AMD system.",Neutral
AMD,did they measure responsiveness and timed the click to action? and was it significantly different? how much difference are we talking about?,Neutral
AMD,"can you explain exactly what you're talking about here? are you talking about a situation where the system needs to do random reads from an ssd? aka: boot time, initial game load time?",Neutral
AMD,"How was ""system snappiness"" measured?",Neutral
AMD,No.,Neutral
AMD,"Now both AMD and Intel chips are “disaggregated “ which means between cpu and other agents like memory controllers, pcie, and storage there is higher latency than the 12/13/14th gen parts. AMD has higher latency due to the larger distances involved on the package.  Also Intel is not really improving the CPU core much. There won’t be a compelling reason to upgrade from a 14700 until DDR6 comes out. At least not in desktop. Nova lake high cache parts will cost $600 or more so value/dollar will be low.",Negative
AMD,So? Major upgrade for everything else,Neutral
AMD,"I had an 12600 not k. I had the opposite experience, I upgraded to a 7800x3d and the snappiness was a night and day upgrade. I can recommend a x3d to anyone. Pair that cpu with Windows 11 IoT LTSC and you have a winner <3",Positive
AMD,"Meant to say ""Gaming Performance""  >Higher avg on X3D  >similar or same 1% lows on both platforms >Higher .1% lows on Intel.",Neutral
AMD,"any comment that starts with ""I mean..""  I never go any further, its like some weird reddit think where everyone with ignorant comments seems to start out with this,  at least often anyway.",Negative
AMD,"""Enthusiast Segment"" my good sir. All the benches you see are poorly configured or stock 14900K. With tuning it's a different story. Intel craptorlake scales with fast ram.",Neutral
AMD,"As a long time AMD user I know that Intel needs to be tuned to perform best. So when you tune the 14900K or even 285K you get like 20% performance uplift vs stock. X3D just performs great out of the box because of the huge L3 Cache. At the very least if you do not like microstutters or frame drops and want consistent gaming performance Intel 14th gen is superior vs current AMD's offering. Anyone with a specific board like Apex, Lightning, Tachyon, or even Gigabyte Refresh boards + i7/i9 13-14th gen with decent memory controller can achieve similar gaming experience. I'm speaking from experience since I also have a fully tuned 9950x3D/5090 on my testbench. For productivity task Intel feels much better to use as well. I feel like Intel is just better optimized for Windows and Productivity too.",Positive
AMD,"Actually Intel thermal is already better than Amd ever since Arrow Lake and Lunar Lake released. Even Core Ultra 7 258V is arround 10c cooler than Amd Z2E and Strix Point on the same watt.   On MSI Claw 8 AI+, Lunar Lake temp at 20w is just arround 62c while the Amd version is arround 70c. I wouldn't have a doubt Nova Lake and Panther Lake will also have good thermal because it will have 18A node with BPD and RibbonFET GAA which is more advance than traditional silicon when it comes to power delivery and efficiency.",Positive
AMD,Ah so bLLC on both tiles is a possible configuration? Any chance Intel actually goes for this?,Neutral
AMD,You can very simply get 9800x3D to 5.4 with little effort,Neutral
AMD,I haven't seen any of those issues on AMD where the underlying cause wouldn't also cause those issues on Intel.,Negative
AMD,"7800x3d here and never had these issues, came from intel",Neutral
AMD,Difference between 85MBps and 140MBps in q1t1 random reads and writes.,Neutral
AMD,Lets just ignore the whitepaper WD and Intel did about this.,Neutral
AMD,So what configuration (tuning and ram settings) can a 14900k match a 9800x3d?,Neutral
AMD,That (Apple Silicon is good) and UMA are different stories I already know that Apple Silicon is good,Positive
AMD,On which benchmark(s) / metrics?,Neutral
AMD,"Haven't kept up with mobile since AMD 5000 and intel 10th gen, all I remember is intel needing XTU undervolting and then intel blocking XTU so using third party undervolt programs, AMD like I said 5000 never needed undervolting.   Desktop side, AMD 7000 is a hot mess, like seriously ridiculous. Seems that was sorted on 9000,    Let's wait and see on nova lake and zen6, like I said, brand loyalty is stupid, bought into AM5, so it's cheaper for me to go for zen6 with 244Mb of L3, but no am6-7 that will be intel.",Negative
AMD,"In theory, yes. For packaging reasons and market segmentation, probably not.",Neutral
AMD,"what's that on in terms of percentage, or seconds to person? was it noticeable?  i'm not techy enough, but is random reads and writes for clicking things and accessing data, and less so on copying and pasting a file?",Neutral
AMD,where is this whitepaper,Neutral
AMD,"You can send white papers all day but if most people buy these for gaming or productivity, AMD is winning in both categories.",Neutral
AMD,Stock clocks 5.7/4.4/5.0 HT Off With DDR5 7600-8000 on Windows 10 22H2 or Windows 11 23H2 is enough to match 9800x3D at 5.4ghz with 6000 c28/6200 c28,Neutral
AMD,So MLID leaked in his most recent video about this (and he ranks it with a blue color code - 'very high confidence')  What do you think about this?,Neutral
AMD,[https://youtu.be/0dOjvdOOq04?t=283](https://youtu.be/0dOjvdOOq04?t=283) This explains it.  Gonna find the whitepapers link again.,Neutral
AMD,Based on what evidence?   I looked online for a few moments and found:  (1) Buildzoid from actually hardcore  overlocking did a 12 hour live stream where they couldn't even get a 8000 mhz overclock stable on the 14900k. No wonder why people haven't benched this lol  https://www.youtube.com/live/bCis9x_2IL0?si=ht3obVoBLcRFCyXI  (2) Plenty of benchmarks where an overclocked 9800x3d is about 10 percent faster than an overclocked 14900k with 7600 ddr5,Neutral
AMD,"Haven't you been listening? The conversation is strange (Confused) You first brought up the story of UMA, right?",Neutral
AMD,"Be civil and follow Reddiquette, uncivil language, slurs and insults will result in a ban.",Neutral
AMD,"That channel has a lot of videos and even on this specific video it would help if you point the specific time you're referring to.  Now regarding AI, I assume you are talking about token generation speed and not prompt processing or training (for which Macs are lagging due to weak GPU compute).  I happen to have expertise in optimizing AI algorithms (see https://www.reddit.com/user/Karyo_Ten/comments/1jin6g5/memorybound_vs_computebound_deep_learning_llms/ )  The short answer is that consumers' PCs have been stuck with dual-channel RAM for very very long and with DDR5 the memory bandwidth is just 80GB/s to 100GB/s with overclocked memory.  Weakest M4 starts at 250GB/s or so and M4 Pro 400GB/s and M4 Max 540GB/s.  Slowest GPUs have 250GB/s, midrange about 800GB/s and 3090~4090 have 1000~1100GB/s with 5090 having 1800GB/s bandwidth. Laptop GPUs probably have 500~800GB/s bandwidth  LLMs token generation scales linearly with memory bandwidth, compute doesn't matter on any CPU/GPU from the past 5 years.  So by virtue of their fast memory Macs are easily 3x to 8x faster than PC on LLMs.  The rest is quite different though which is why what benchmark is important.",Neutral
AMD,"I'm highly suspicious, for packaging reasons if nothing else. I'm not going to call it impossible, but that would put enormous strain on the X dimension. Think about it, it's something on the order of 3-4x what the socket was originally for.  And obviously goes without saying, but MLID ""very high confidence"" doesn't mean shit.",Negative
AMD,"Based on my own testing , the games I play and benchmark results. Bullzoid is not relevant to this conversation. You can keep downvoting me for speaking the truth it's okay. I can't blame you because youtube and mainstream techspot are 99% misinformation. If you believe 9800x3D is 30% faster then go buy it, nobody is stopping you. I have investments with AMD and you're doing me a favor by supporting them.",Neutral
AMD,Just to be clear compute and hardware support for things like tensor cores have a massive impact. HBM is king but on older cards like the mi100 (released five years ago) can be out paced by a mid range card like the 4070.  All I wanted to convey is llm and token generation is a complex topic with limitations and struggles beyond memory bandwidth.,Neutral
AMD,"I haven't downvoted you at all, what are you even talking about.   You want people to believe in some conspiracy theory where any other information is a lie, and only you provide the ""real truth"".",Negative
AMD,"Besides, UMA wasn't first developed by Apple. Even if Intel introduces it, the software side or the software framework… Moreover, the OS side has to deal with it, so it is necessary to consider it a little. That's what you said earlier",Neutral
AMD,"I take no side there. I'm a dev, I want my code to be the fastest on all platforms  I have: - M4 Max so I can optimize on ARM and MacOS - Ryzen 9950X so I can optimize with AVX512 - in the process of buying an Intel 265K so I can tune multithreaded code to heterogeneous architecture.  The problem of Intel and AMD is segmentation between consumer and pro.  If Intel and AMD want to be competitive on AI they need 8-channel DDR5 (for 350~400GB/s), except that it's either professional realm (Threadripper's are 8-channel and EPYC's are 12-channel) with $800~1000 motherboards and $1500 CPUs and $1000 of RAM.  Or they make custom designsvwith soldered LPDDR5 like the current Ryzen AI Max 395, but it's still a paltry 256GB/s.  Now consumer _need_ fast memory. Those NPUs are worthless if the data doesn't get fetched fast enough. So I expect the next-gen CPUs (Zen 6 and Nova Lake) to be quad-channel by default (~200GB/s with DDR5) so they are at least in the same ballpark as M4 chip (but still 2x slower than M4 Pro and Max).  I also expect more soldered LPDDR5 builds in the coming year.",Neutral
AMD,"Have you not seen HardwareUnboxed 9070 XT Finewine video? Tell me why should I trust someone like that? I'm just sayin if you want real information test it yourself. I just don't trust product influencers like GamersNexus, HUB , Jayz2cents and other mainstream channels. Id rather buy the product and test it myself. Im not forcing you to believe what I said, its for people that actually knows what Raptorlake is capable off. If you want to see a properly tuned 14900k check out facegamefps on youtube. This is a very capable platform.",Neutral
AMD,"This subreddit is in manual approval mode, which means that **all submissions are automatically removed and must first be approved before they are visible**. Your post will only be approved if it concerns news or reviews related to Intel Corporation and its products or is a high quality discussion thread. Posts regarding purchase advice, cooling problems, technical support, etc... will not be approved. **If you are looking for purchasing advice please visit /r/buildapc. If you are looking for technical support please visit /r/techsupport or see the pinned /r/Intel megathread where Intel representatives and other users can assist you.**  *I am a bot, and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose/?to=/r/intel) if you have any questions or concerns.*",Neutral
AMD,Is it safe to install an Arrow Lake CPU without a third-party contact frame over the long run?,Neutral
AMD,"Edit: solved. Windows is capable of disabling e-cores on boot. The setting to turn them back on in MSCONFIG, boot tab, cores dropdown. Whatever feature causes this, I do not know but would love to find out, and furthermore their is no legitimate purpose for this feature, and Intel should prevent Microsoft from doing this, in my opinion, if there is any legal means available. Unless it interferes with Intel’s revenue.  Poor performance, incredibly low benchmarks. Beginning to suspect bad CPU.  I have tried bone stock with cleared CMOS settings. I have loaded multiple overclocking profiles, clearing CMOS in between. I have tried undervolting, and adjusting Load Line. I have changed Lite Load settings. I have tried with and without XMP enabled. Changed windows power plans. Nothing gets my cinebench r23 score above 11k. CPU-Z benchmarks it as 43% as fast as the previous generation CPU. I never overclocked this cpu, and I changed to the updated bios with new microcode the day it was available. CPU Purchased from Intel, retail box.  Windows 11 Build 26100.4484  CPU: Intel I9 14900K, stock clock, 360 AIO liquid cooling  RAM: Patriot Viper Venom DDR5 32GB (2x16) 7200MT/S CL34  GPU: MSI Ventus NVIDIA RTX 4080S  Motherboard: MSI Z790-S Pro Wifi with most recent bios  Storage: WD Black 2TB NVME on CPU; Toshiba 20GB X300 Pro  PSU: NZXT C1000 PSU (2022)  Display: Samsung Odyssey G93SC  I am at my wits end with this thing. I first noticed games crashing to desktop a few months ago. I thought it was just poorly coded (Helldivers 2). I checked my FPS in the game, and while previously I had to frame limit it 10 144FPS, I was now getting close to 60-80FPS.  Is my CPU a toaster or is there something I'm missing?  CPU-Z output, and HWiNFO64 sensor readings:  [https://imgur.com/a/ROuZOKS](https://imgur.com/a/ROuZOKS)",Neutral
AMD,"I bought a 265kf CPU from Amazon. On the checkout page it clearly stated that my purchase qualified for the Intel Spring Gaming Bundle and that I would receive an email with a Master Key. Well, I didn't receive anything and I spent all day being transferred from one Amazon support staff to another to no avail.  The promotion is literally still active and if I try to buy the CPU again it shows the same offer.  But somehow no one on Amazon or Intel support can tell me why I didn't get the email.",Neutral
AMD,Does anyone know if the upcoming Bartlett Lake-S 12 p-core no e-core CPU will suffer from the same stability issues as Intel 13th gen and 14th gen CPUs?,Negative
AMD,Any news to share regarding this link? https://www.reddit.com/r/intel/s/Bg4QnVzIdD,Neutral
AMD,"Bug report: Latest ARC driver 32.0.101.6972 causes crashing using Speed Sync  I get crashing in ALL games when attempting to use the new Speed Sync option on this driver.  Specs are:  Ryzen 9700x, Arc B580, 32GB DDR5 RAM, 2tb SSD.  Monitor is non VRR compatible (older gsync monitor ASUS PG348Q)  Hope this gets resolved as it sounds like a great feature.",Negative
AMD,"Hello,  I own a i9-14900K, mobo MSI MAG Z790 Tomahawk. I am wondering if it’s possible a specific program can cause my cores to be power limit exceeded, and is there any fix to that? Currently it drops the cpu speed to 0.8 GHz. This one program LeagueClient.exe for a game, League of Legends, has recently started to cause this problem, the only fix is to restart the computer. I have updated my bios, changed power limits within the bios, disabled c state, disabled EIST, disabled e-cores, tried to go into safe boot but the program won’t launch, reinstalled multiple times.",Negative
AMD,Has the degradation of the 13th and 14th gen CPU’s been fixed yet?,Neutral
AMD,"Been getting the following error A LOT while playing Expedition 33, and now after 18 hours I can't start the game without it crashing:  `LowLevelFatalError [File:C:\Hw5\Engine\Source\Runtime\RenderCore\Private\ShaderCodeArchive.cpp] [Line: 413] DecompressShaderWithOodleAndExtraLogging(): Could not decompress shader group with Oodle. Group Index: 760 Group IoStoreHash:52bcbf8ac813e7ee35697309 Group NumShaders: 29 Shader Index: 9301 Shader In-Group Index: 760 Shader Hash: 3BF30C4C9852D0D23B2DF59B4396FCC76BC3A80. The CPU (13th Gen Intel(R) Core(TM) i7-13700KF) may be unstable; for details see` [`http://www.radgametools.com/oodleintel.htm`](http://www.radgametools.com/oodleintel.htm)     Am I just screwed because I bought the wrong generation of Intel CPUs 2 years ago?",Neutral
AMD,"I'm not sure if this matters, but when running `garuda-inxi` on my laptop, it tells me that my i3-8130U is Coffee Lake Gen 8, not Kaby Lake Gen 9.5. I've seen similar problems reported with users using CPU-Z, is this just a known bug or is there something else going on?  I'm going to be tweaking my CPU performance soon, so I'd like to make sure of the CPU capabilities/options first.",Negative
AMD,"I've been planing to build a PC from scratch for video editing and Ultra 7 265k and Arc B580 are good in my price bracket. Now, I'm worried with all the talk of Intel potentially going bankrupt or having layoffs, is it a good idea to still buy their products? Will we lose support with drivers and stuff like that?",Neutral
AMD,My gaming laptop Intel core i7-12700h runs at a constant temperature 95 . Doesn't matter if I'm playing a high end game like cyberpunk or some indie game like hollow knight. Is this thing suppose to always run at this temp?,Negative
AMD,"con el nuevo microcodigo de intel , los juegos de ubisoft (ASSASINS CRREED ODYSSE) tienen tiempos  de carga excecivamente altos, diria que al menos 10 veces mas de lo normal, Al devolverr la bios al microcodigo anterior todo funciona bien, cuando lo van a parchar?",Neutral
AMD,is it worth the upgrade for ai preformance cus my 5070 ti wont work for some reason so now my cpu is my main accelertor and user so should i upgrade to ultra 9? and i  will remove 5070 ti from my flair soon so yeah .,Negative
AMD,"ok now this is something. my Intel HD Graphics Control Panel is no longer there? no idea how long its been gone but i clearly remember it being there at a point.  using a Lenovo G510(i7-4700MQ, HD Graphics 4600)   Windows 10 Home 22H2 (Build 19045.6216)  windows apps in settings shows the intel driver but not the control panel   drivers from [lenovo's website](https://pcsupport.lenovo.com/in/en/products/laptops-and-netbooks/lenovo-g-series-laptops/lenovo-g510-notebook/20238/downloads/ds103802-intel-vga-driver-for-windows-10-64-bit-lenovo-g410-g510?category=Display%20and%20Video%20Graphics) are dated 16jul 2015. version seems to be 10.18.15.4240   drivers from [intel's website](https://www.intel.com/content/www/us/en/download/18388/intel-graphics-driver-for-windows-10-15-40-4th-gen.html?wapkw=intel%20hd%20graphics%204600) are dated 9jan 2015. version is 15.40.7.64.4279  now whats funny is that my device manager shows that my driver is dated 8mar 2017 which is version 20.19.15.4624   i also have another driver that i can see in the update drivers menu(drivers already present on my device) along with this one which is dated 29sep 2016 version 20.19.15.4531  i have tried reinstalling the driver from the update driver menu using the 8mar 2017 version, no change at all.  my drivers dont seem to be DCH drivers.   intel also says that they [discontinued the ms store version of the control panel](https://www.intel.com/content/www/us/en/support/articles/000058733/graphics.html) anyway.  this is all that i could think of writing here. any other details required just ask.   any help would be good lol",Neutral
AMD,"my i5-14600KF is being throttled at low temps and refuses to go past 0.8ghz of clock speed. Nothing I do seems to get it to stop throttling. According to throttlestop i have a red EDP OTHER ongoing throttle under CORE and RING. My average CPU temp is 31 degrees C across all cores and im getting 0.69 Voltage to my CPU  CPU: Intel i5-14600KF stock settings no overclock   GPU: Intel Arc B580 ONIX Odyssey BAR resizing enabled   Motherboard Gigabyte Ultra Durable Z790 S WIFI DDR4   RAM: Corsair Vengeance DDR4 16 GB x 2 (32GB)   Storage: 1TB Corsair MP600 CORE XT SSD + 2 TB WD Black SN770   PSU: Cooler Master MWE Gold 850 V2  EDIT: NEW INFO ACQUIRED   When running in safe mode and when booting into BIOS settings my CPU acts normally and receives typical voltage. Something running on my computer is throttling my CPU as i boot into windows. If i open Task Manager quickly after booting, I see system interrupts consume a mild amount of CPU before quickly going away, rather than sticking around when their is an ongoing hardware issue. I have reason to believe a program, either maliciously or due to error, is fucking with my CPU. Also worth noting is that Intel Graphics Software reports my CPU utilization as far higher than task manager, anywhere between 2-50% higher.",Negative
AMD,"Currently in the planning/purchasing phase of a small NVR/Steam Cache server. Information on VROC on X299 is pretty limited. so far I've seen mixed information on the drives supported. Before I purchase x4 Intel P4510 drives, I was hoping someone on here has a similar configuration that works.  The mobo manual states that only Intel based drives are supported but doesn't clarify which intel drives. Also saw on the intel forum that X299 CPU raid is further limited to only Optane based NVME drives. This drive will not be booted to, and I dont want to do a windows based raid.  My planned specs are:  CPU: 10900X  Mobo: X299 Taichi CLX - One of the few that seems to support bifurcation & VROC  Drive: Intel P4510 1TB  VROC Key: VRoc Standard  Are the Intel P4510 supported for Vroc on the x299 platform?",Neutral
AMD,"My Intel I210 ethernet device has device id 1531 meaning unprogrammed. The freebsd ethernet driver does not work with 1531. It needs device id 1533 meaning programmed. (Can I use a different driver? No, it's an embedded system that only supports this driver.)  I was linked this:  https://www.intel.com/content/www/us/en/content-details/334026/intel-ethernet-controller-i210-i211-faq.html  but 1) have no idea how to do it 2) cannot access the .bin file and tool it requires  Does anyone have ELI5 steps for getting the device to show devid 1533? Where can I get the .bin?",Negative
AMD,"The RMA process for Intel is absolutely atrocious. I don't know if  anyone can give advice on this but here is what is happening:  Based in Germany, for geolocation info. I was one of the early adopters of the 13th gen processors, but as I don't follow tech news too strongly I didn't find out about the issues with these chips until autumn 2024 when the issues I was having with my PC escalated to a point I couldn't ignore any further. Identified the CPU as the likely culprit and started the RMA process.  Firstly, Intel would not offer any solution where I could continue to use my PC whilst they analysed my CPU. As I use my PC for work, having it out of action for weeks/months was simply not an option, so I was forced to pause the RMA ticket whilst I saved up for a new cpu way ahead of my expected timeline.  With that aside, I reopened my ticket and the requirements they lay out are near impossible to meet - they wanted a clear photo of the matrix on the front of the chip and the matrix on the pcb itself.  The front of the chip was simple enough but on this series Intel printed the matrix in dark grey on a dark green pcb. It's barely visible just with the naked eye and I've tried so many ways to take a picture of the matrix with my smartphone and nothing I do is getting a clear picture.  I don't understand how your average consumer can possibly meet this requirement - solutions online apparently suggest purchasing a special type of expensive scanner or a macro lens for your smart phone? Which is ridiculous to me.  There is no way they cannot verify my chip with the rest of the information I have been able to give them, as well as far as I am aware, the matrix on the side of the chip on the 13th gen is literally the same as the one on the front of the chip.  The whole experience is proving to be awful, time consuming, feels like it should be illegal and has completely put me off ever using Intel again, or recommending it to anyone I help spec builds for.",Negative
AMD,"Hi, I have a Lenovo Ideapad with an Intel CPU and Intel GPU.  I find Windows font rendering unreadably faint. I have an astigmatism, and a light sensitivity, so even with prescription sunglasses I can't use dark mode, or bright screens, respectively.  I've tried finding Intel Graphics Software app settings which might help. The contrast settings quickly get too bright for my eyes, so they don't fix this. The right gamma settings might help though, one profile to make medium shades darker to make text bolder, and one to make them lighter to make images bolder.  I tried the support site here, but it has buggy scrolling which triggers my migraines, and it doesn't work with Firefox's reader view:  https://www.intel.com/content/www/us/en/support/products/80939/graphics.html#211011  How can I find or create these profiles? The Intel Graphics Software app doesn't seem to have an option to create profiles.  P.S. I can use the Windows Display Calibrator; if I ignore the instructions, and turn it as dark as possible during gamme, I get readable text at the end; if I turn it as light as possible, I get clearer images. But I can't see  a way to save those and switch without going through the whole rigamarole again.",Negative
AMD,"Can someone explain to me the differences between all the different names of the Intel CPUs, I’m new to laptops and am trying to learn. What’s the difference between 155H, 255H, and Meteor/Lunar/Arrow Lake and which one is “better”? All the different names get so confusing to me",Neutral
AMD,"I've been chasing down a crashing issue in a Unity program. Memtest86+ ran for 24 hours and cleared 18 passes so I was confident it's not RAM, and I started setting core affinity in an elimination pattern to see if it might be CPU related. I have discovered it crashes within minutes if I set affinity to Core 8, but it's rock solid on Cores 0-7 (haven't individually tested any of the cores past 8 yet but I have it on 9 now and it seems fine so far).  I have a 13900KF (not Overclocked) which is part of the batch that had the microcode issue. My BIOS was updated, but I am now suspicious of this core. Is it likely I should RMA this? Is there something else I should try first?  So far the system itself has been stable but this one Unity program crashes consistently, and I was having tab crashes in both Firefox and Chrome that also resolved when I removed Core 8 from their affinity...  The CPU passed the Intel CPU Diagnostic tool but because the crashes are so specific to core 8 it makes me suspicious.",Neutral
AMD,"i'm trying to get a specific driver for the storage but i can't findd a site with the driver, only setup exes (my old computer will not let it work and the new one needs the driver to install windows). is there a place where i could find the driver itself?",Negative
AMD,"Hi There. I been trying to get a replacement cpu for my 14700 and i chose option 2 to pay the 25 dollor fee (nonrefundable) etc. but my case now has a new number, a differnt worker. and he said that a credit card specilist is gonna called me and im supposed to give him my credit card infomation  edit 1: He also said the intel website isnt secured/safe for creditcard transaction",Negative
AMD,"Hi, I have Intel AX200 160Mhz Gig+ in 3 PC's (on board Gigabyte B550 x 2 & X570). My Router supports 160Mhz, it's enabled and was working getting me up to 900Mbps on my Gigabit plan. and in the last few months I've noticed the 160Mhz option in the intel wifi drivers has disappeared. I discovered this as I noticed the household PC's were struggling to get above 600Mbps most of the time. I double checked the router setting, nothing changed there 160Mhz still eneabled, only changes over the last couple of moths were 2 bios updates each for the motherboards and Intel wifi driver updates. Has anyone else had the 160Mhz option disapear? Any tips on how to get it and my full network performance back?",Neutral
AMD,"u/Progenitor3  yes, it is generally safe to install an Arrow Lake CPU without a third-party contact frame over the long run. CPUs are designed to function properly with the standard mounting mechanisms provided by the manufacturer. Third-party contact frames are optional and may offer additional stability or cooling benefits, but they are not necessary for the safe operation of the CPU. Always ensure proper installation according to the manufacturer's guidelines to maintain optimal performance and safety.",Neutral
AMD,"u/TerminalCancerMan  Intel cannot comment or interpret results from third party benchmark tools. Run [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html) to confirm if there are any issues with the CPU. You may try this, If the motherboard BIOS allows, disable Turbo and run the system to see if the instability continues.  If the instability ceases with Turbo disabled, it is likely that the processor need a replacement.",Neutral
AMD,"u/Progenitor3  Just fill in your info, and it’ll automatically create a ticket for you. Our team that handles those items will get in touch within 3–5 business days.   [Software Advantage Program](https://softwareoffer.intel.com/Support)",Neutral
AMD,Gotta wait for real-world tests to know for sure tho.,Neutral
AMD,"u/Special_Ad_7146 To stay on top of Intel news, **visit** our [Newsroom](https://newsroom.intel.com/).",Neutral
AMD,"u/Hippieman100  just checking can you confirm which software you're using? From what I’ve seen in the latest [ReleaseNotes\_101.6972.pdf](https://downloadmirror.intel.com/861295/ReleaseNotes_101.6972.pdf), there doesn’t seem to be a “Speed Sync” feature in the current version that comes with this driver. It does support V-Sync and Adaptive Sync though. Just wanted to clarify are you referring to the older Intel Arc Control software or the Intel Graphics Command Center? Just making sure we’re on the same page!",Neutral
AMD,"u/TheCupaCupa to better understand and isolate the issue, I kindly ask for some additional information. Please find the details requested below:   1. When the CPU drops to 0.8 GHz, do you notice any **error messages or warnings** in Windows or BIOS? 2. Does the issue happen **only with LeagueClient.exe**, or have you seen it with other programs too? 3. Have you checked **CPU temperatures and power draw** when the issue occurs? 4. Can you check **Event Viewer** for any critical errors or warnings around the time of the slowdown? 5. Are you using any **custom power plans** in Windows, or is it set to Balanced/High Performance? 6. Is **Intel Turbo Boost** enabled in BIOS? 7. **When did this issue first start happening?** Has it occurred before? 8. Have you made any software or hardware changes to the system recently?   Once I receive this information, I will be able to properly assess the situation and provide further assistance.",Neutral
AMD,"u/Alloyd11 Not all 13th and 14th generation processors show instability issues. Just to better assist you are you planning to buy or use one of these processors, or do you need help with your current system?  Let me know how I can support you!",Neutral
AMD,"u/amitsly “crashes” is a pretty broad term, and not every system issue points directly to the CPU. There are quite a few steps we go through to fully isolate the problem before concluding it’s a processor fault.  To help us assist you better, could you please share a bit more info about the crashes?  * When did the issue first start happening? * Have you made any recent changes to the system either hardware or software? * Is there any visible physical damage to the system? * What troubleshooting steps have you already tried? * Have you noticed any signs of overheating? * Have you tested the processor in another working system, or tried swapping it out to see if the issue follows the CPU?  The more details you can provide, the quicker we can get to the bottom of it!",Neutral
AMD,"u/Jay_377 its  likely comes from Intel’s naming convention. The i3-8130U is part of the 8th gen, but it falls under “Kaby Lake Refresh” (for mobile chips), not “Coffee Lake” (which is for desktops). Tools like `garuda-inxi` or `CPU-Z` might label it differently based on how they categorize architectures, not a bug, just naming differences.  [Intel® Core™ i3-8130U Processor](https://www.intel.com/content/www/us/en/products/sku/137977/intel-core-i38130u-processor-4m-cache-up-to-3-40-ghz/specifications.html)",Neutral
AMD,It is because Kaby Lake CPU's are 7th gen processors for laptops and 8th gen to 9th are Coffee Lake check rhis link for better understanding:https://www.intel.com/content/www/us/en/ark/products/codename/97787/products-formerly-coffee-lake.html,Neutral
AMD,"u/Reality_Bends33 Intel has been around for a long time and is known for making solid, reliable products, so you can feel confident about choosing them for your PC build. The Ultra 7 265k and Arc B580 are great picks for video editing, offering the performance you need without breaking the bank. While there’s been some discussion about Intel facing challenges, remember that big companies like Intel usually keep up with driver updates and support, even if they’re going through changes. The tech world is always evolving, and Intel is investing in new technologies to stay ahead. So, you’re likely to get the support you need for your products.",Positive
AMD,"u/unknownboy101 It’s normal for your processor to heat up during heavy tasks like gaming. Intel CPUs are built to manage heat by adjusting power and speed, so they stay safe and avoid damage. However, running at a constant **95°C** on your Intel Core i7-12700H even during light gaming is **not ideal** and could indicate a cooling issue. While Intel CPUs are designed to handle high temperatures and will throttle performance to avoid damage, consistently running near the thermal limit can shorten the lifespan of your components and affect performance. **Feel free to check out this article for more info or steps to try. Just a heads-up this is specifically meant for boxed-type processors. You can still take a look, but I strongly recommend reaching out to your laptop’s manufacturer to get help with the overall system configuration.**  [Overheating Symptoms and Troubleshooting for Intel® Boxed Processors](https://www.intel.com/content/www/us/en/support/articles/000005791/processors/intel-core-processors.html)  [Is It Bad If My Intel® Processor Frequently Approaches or Reaches Its...](https://www.intel.com/content/www/us/en/support/articles/000058679/processors.html)",Neutral
AMD,"u/Frost-sama96 Tenga en cuenta que solo puedo apoyarlo en inglés. He utilizado una herramienta de traducción web para traducir esta respuesta, por lo tanto, puede haber alguna traducción inexacta     **To help me dig a little deeper into the issue, could you share a few details?**  * What’s the **make and model** of your system? Is it a **laptop or desktop**? * Do you remember **when the issue first started** happening? * Which **BIOS version** are you referring to, the one that works fine? If you can share the exact version , that’d be super helpful.",Neutral
AMD,"u/Fluid-Analysis-2354 If your 5070 Ti isn't functioning and your CPU is now your main accelerator, upgrading to the Ultra 9 could be beneficial. The Ultra 9 offers enhanced AI performance, which can significantly improve your computing tasks. If AI capabilities are a priority for you, the upgrade is worth considering.",Positive
AMD,"u/BestSpaceBot , As with all good things, your product has reached the end of its interactive technical support life. However, you can find [Intel® Core™ i7-4700MQ Processor](https://www.intel.com/content/www/us/en/products/sku/75117/intel-core-i74700mq-processor-6m-cache-up-to-3-40-ghz/specifications.html) recommendations at [Intel Community forums](https://community.intel.com/) and additional information at the [Discontinued Products](https://www.intel.com/content/www/us/en/support/articles/000005733/graphics.html) other community members may still offer helpful insights or suggestions.. It is our pleasure to continue to serve you with the next generation of Intel innovation at [Intel.com](http://www.intel.com/). You may also visit this article for more details [Changes in Customer Support and Servicing Updates for Select Intel®...](https://www.intel.com/content/www/us/en/support/articles/000022396/processors.html)",Neutral
AMD,"u/ken10wil   To help figure out what’s going on with your CPU throttling issue, I’d like to ask a few quick questions that’ll help me dig deeper:  * When did you first start noticing the problem? * Have you made any changes to your system recently like installing new software, updating drivers, or swapping hardware? * Is there any visible damage to your PC or loose connections? * Have you updated your BIOS to the latest version for your Gigabyte Z790 board? * Did you try resetting the BIOS to default settings to see if that helps? * Have you tried reapplying thermal paste to the CPU? Just to rule out any cooling contact issues. * Are there any startup programs or services that might be messing with your CPU? * What background processes pop up right after boot? You can check Task Manager or use Reliability Monitor to trace anything unusual. * Can you run the [Intel® Processor Diagnostic Tool](https://www.intel.com/content/www/us/en/download/15951/intel-processor-diagnostic-tool.html) and let me know if it passes or fails?  Let me know what you find happy to help you troubleshoot further once we have a bit more info!",Neutral
AMD,u/PM_pics_of_your_roof   Interesting thanks for pointing that out! Let me check this on my end and I’ll post an update here once I have accurate information.,Positive
AMD,u/UC20175 Let me check this on my end and I’ll post an update here once I have accurate information.,Neutral
AMD,"I don't think that there it is awful, I mean in the past where the issue blew out and many consumers are reporting. People at Intel can barely accommodate them but they still give the replacement needed for the consumer.   If i am right, that image is necessary for them to get your serial number and other stuff. Unless you have the box of the processor, or you did not purchase it as a tray then I think you'll be fine.",Neutral
AMD,"Hi, I think you'll need to connect with your laptop manufacturer or Microsoft to help you customize an ideal setting for your display. But could you share your full laptop model?",Neutral
AMD,I think this will help you https://www.intel.com/content/www/us/en/processors/processor-numbers.html,Neutral
AMD,"u/Tagracat  May I confirm the exact error message you're seeing during the crash? Also, have you had a chance to contact the developer of the software in question? Regarding the BIOS, could you double-check if you're using the patch addressing instability issues specifically version 0x12F provided by your motherboard manufacturer? Since the Intel Processor Diagnostic Tool passed, not all crashes may be CPU-related.   Looking forward to your response!",Neutral
AMD,"u/Maleficent_Apple4169   What specific storage device are we talking about here? Is it an NVMe SSD, RAID controller, or something else? And what's the make/model of the unit in question? Knowing the exact hardware might help me point you to a more specific solution.",Neutral
AMD,Guess checking with your motherboard manufacturer,Neutral
AMD,"u/ChiChiKiller Just to be clear, what you've described sounds like a possible **fraud or phishing attempt**. Please **do not share your credit card information** with anyone claiming to be from Intel unless you're communicating directly through our **official support channels**. Intel will never say that our website is not secure we take security very seriously, and this kind of messaging is often used by bad actors trying to steal personal information.  I’ve sent you a **private message in your inbox** please check it when you can. Also, could you kindly provide the details of your **initial ticket** and the **new case number** you mentioned through inbox? I’ll investigate this immediately on my end.  Don’t worry I’ll keep you updated as soon as I have more information. For your privacy, I’ll continue communicating with you through inbox so you don’t have to post sensitive details publicly.  Thanks again for bringing this to our attention!",Negative
AMD,I fixed the problem. It was windows msconfig disabling e-cores on boot. You should forbid them from doing this.,Negative
AMD,"I can't remember which (not at my pc), I think it's intel graphics command center. I get a list of v-sync options, off, on, smooth, smart and speed (from memory). Speed was not available in previous versions of the software/driver. Underneath there is an FPS limiter and a control for latency improvement (I can't remember the name) with off, on, and on + boost options available.",Neutral
AMD,"Just got back to my PC, it's actually neither Arc Control or Intel Graphics Command Center. I'm using Intel® Graphics Software (25.26.1602.2), should I be using something else?",Neutral
AMD,"Hello, thank you and I'll try my best.  1. No error messages or warnings in Windows, unsure how to check BIOS. 2. Yes, currently I have only found this happens when LeagueClient.exe is started. 3. Using HWInfo64, right when I open LeagueClient.exe, all P and E cores have ""Yes"" in the Current column for ""Power Limit Exceeded"". Core temperatures are: Current- 33C Minimum- 31C Max- 72C Average- 42C. CPU Package Power, minimum is 65.699 W, maximum is 129.230 W. Upon starting the program, the maximum value doesn't change. 4. For Event Viewer, no warnings come up when LeagueClient.exe is started. 5. I have tried setting it to Balanced, High Performance, but I'm mainly on Ultimate Performance. 6. Yes, Intel Turbo Boost is enabled. 7. The issue first started happening 2 days ago, 07/25/25. No it has not occurred before. 8. I have not made any new hardware changes to the system. I did have a windows update, 2025-07 Cumulative Update Preview for Windows 11 Version 24H2 for x64 based Systems (KB5062660) (261.000.4770) installed on 07/25/25, however I installed this update later on during the day after the problem had already started.",Neutral
AMD,"Well, I didn't immediately blame the CPU. The crash message specifically mentions the issue that point the finger to the CPU, plus the provided link a saying that's the root cause.  Anyhow, here are more details:  1. The issue first started happening about 2 hours into my Expedition 33 playthrough and has happened at least 20 times since (in about 18 hours of gameplay) 2. I have not made any changes whatsoever to software or hardware before it started happening. Last night, after the 20th crash, I did run an update for various drivers & the BIOS (including the 0x12F update) using Gigabyte's CC software. 3. No physical damage that I could observe through the PC's glass window 4. I have tried everything the internet had to offer about this specific issue regarding Expedition 33. This includes verifying game files, changing config and settings, reinstalling, lowering the CPU clock speed and more.  5. I didn't notice the temperature when the crashes happened but I'll be on the look out. 6. I don't have a way to swap out the CPU nor do I have a replacement CPU",Neutral
AMD,"Weird, mine isn't in that list.",Negative
AMD,"I5 14600k desktop  When i try Odysse ACC, \*game\* I notice the start up and loading in to the game was so slow, like 8 or 10 minutes to Load.    \--Versión 182011.06 MB2025/05/21SHA-256 ：493D40A2351EED41FCF60E51346B9065880E87F986C1FD1FB1A3008E8C68DA26  ""Update the Intel microcode to version 0x12F to further improve system conditions that may contribute to Vmin Shift instability in Intel 13th and 14th Gen desktop-powered systems.   Updating this BIOS will simultaneously update the corresponding Intel ME to version 16.1.32.2473v3. Please note after you update this BIOS, the ME version remains the updated one even if you roll back to an older BIOS later. ""-- With this BIOS frrom ASUS page, The game do that   But, when i rollback to>   Versión 181211.04 MB2025/03/28SHA-256 ：98528167115E0B51B83304212FB0C7F7DD2DBB86F1C21833454E856D885C7EA0  ""Improve system performance and stability      Updating this BIOS will simultaneously update the corresponding Intel ME to version 16.1.32.2473v3. Please note after you update this BIOS, the ME version remains the updated one even if you roll back to an older BIOS later.""  Intel microcode 0x12B t  The game Load A lot faster and run well.  I dont know what this happens, but is a error caused by New microcode",Negative
AMD,"\- I started noticing the problem between august the 19th and august the 20th.  \- I updated the system around the time I \*noticed\* the problem, but I am not sure if that is when the issue started. I reverted the machine to a state prior to the update and the problem still occurs. Any other driver updates or software installs happened after I had already noticed the problem, and were initiated trying to solve the problem  \- There is no visible damage to my pc or wires  \- my Bios is up to date, and aside from turning on resizable BAR months ago for my GPU, my bios settings are default. I reset to default and turned resizable BAR on again just to be sure, and the issue still occurred.  \- I have not, mostly because it would be a hassle and because the CPU has remained very cool. I have manually overclocked it as a temporary solution to the problem and even under these conditions it is averaging 35-40 degrees Celsius when idle and hovers around 50 degrees when under stress  \- While the issue did not occur in Safe Mode, I am not sure which program is causing the throttling. I have disabled all installed startup programs and still get throttled.  \- No unusual processes pop up right after boot, though the sum of all processes hits 15% CPU utilization, stays there for a while and then the CPU throttling begins (all happens in less than 30 seconds after boot). I could see what happens after disabling even the security related startup programs and see if there is any difference  \- I pass the PDT tests but I have abysmal performance on various benchmark tests, and the PDT takes a long time to complete. Overclocking leads to more expected results.",Neutral
AMD,"Thank you. I spent some more time looking and it appears VROC on x299 was discontinued sometime in the past two years? Seems intel pushed people towards RST. I guess the question still stands since VROC on X299 has moved into sustaining mode, so drives that maybe came out during that time should still work.      [https://www.intel.com/content/www/us/en/support/articles/000026106/memory-and-storage/datacenter-storage-solutions.html](https://www.intel.com/content/www/us/en/support/articles/000026106/memory-and-storage/datacenter-storage-solutions.html)",Neutral
AMD,What I've gathered from support so far is documents needed by section 2.14/2.15 of https://www.intel.com/content/www/us/en/content-details/334026/intel-ethernet-controller-i210-i211-faq.html are behind a premier account registration. I'll try registering an account.,Neutral
AMD,"I'm mostly referring to the fact they've designed the matrix to be printed near invisibly with it being absolutely miniscule and basically the same colour as the pcb, then made it a mandatory requirement for the RMA process.   It reeks of a company trying to dodge responsibility for faulty products by making the hoops so difficult to jump through theres no reasonable expectation the average consumer will be capable of complying.   Why do I need to go out and purchase a macro lens for my smart phone? Its putting me out of even more money than I already am.",Negative
AMD,Ideapad 1 15IAU7.,Neutral
AMD,"No error message per se... the program just closes and there is an event in the event viewer. I chatted with the dev and it looked like it pointed to a virtual memory issue, but it ONLY occurs when bound to core 8. (or all cores including 8)  turns out I was on BIOS 16.02, NOT 17.03 which specifically addresses 0x12F. I will update that next!",Neutral
AMD,it's a NVMe m2 SSD. i don't think it has a specific make/model because I built the computer myself,Neutral
AMD,"u/Hippieman100 **Intel Graphics Command Center** and **Intel Arc Control** have been the go-to software for many users, but they’re being phased out soon, and support will be limited moving forward.  Since system (not your PC) includes an **Intel Arc B580**, I highly recommend switching to **Intel Graphics Software** instead. This newer software is bundled with the graphics driver package, which you can find at the link I’ll provide-[Intel® Arc™ & Iris® Xe Graphics - Windows\*](https://www.intel.com/content/www/us/en/download/785597/intel-arc-iris-xe-graphics-windows.html). Before making the switch, please take a moment to read through all the details and driver descriptions on that page.  Also, based on my checks, it looks like you're currently using **driver version 25.26.1602.2 on your PC**, which is outdated.  Let me know if you have any questions.",Neutral
AMD,"u/TheCupaCupa If the motherboard BIOS allows, disable Turbo and run the system to see if the issues continues.  If the instability ceases with Turbo disabled, please let me know.",Neutral
AMD,"u/amitsly For further analysis, please provide the crash dump or log files generated by the game. You can follow the guide and scroll down to the section titled **""Crashing/Freezing Issues/BSOD""-**[Need help? Reporting a bug or issue? - PLEASE READ THIS FIRST! - Intel Community](https://community.intel.com/t5/Intel-ARC-Graphics/Need-help-Reporting-a-bug-or-issue-with-Arc-GPU-PLEASE-READ-THIS/m-p/1494429#M5057) for instructions.  Once you’ve obtained the files, kindly notify me so I can send you a private message to collect the logs.  For isolation purposes, please try the following step and let me know the outcome:   **If your motherboard BIOS allows it, disable Turbo Boost and observe whether the system crashes continues.**",Neutral
AMD,Yeah that's weird try checking qith laptop manufacturer about it,Negative
AMD,"u/ken10wil  Thanks for the detailed info - this is really helpful for narrowing things down. What I'm seeing here points to a software issue rather than hardware failure. The sudden onset timeline, passing PDT tests, and the fact that safe mode works fine all suggest you're dealing with a software or driver problem.   Your CPU temps are totally normal too, so I can rule out thermal throttling. That 15% CPU usage spike right before throttling kicks in is actually a big clue - something's definitely triggering this behavior.     [Information about Temperature for Intel® Processors](https://www.intel.com/content/www/us/en/support/articles/000005597/processors.html)  [How to Know the Idle Temperature of Intel® Processor](https://www.intel.com/content/www/us/en/support/articles/000090343/processors.html)  [What Is Throttling and How Can It Be Resolved?](https://www.intel.com/content/www/us/en/support/articles/000088048/processors.html)  For overclocking: Note that the motherboard does have an impact on the ability to overclock. Some are better quality and more capable than others. Furthermore, the way the motherboard is set up also impacts the overclocking ability of a particular system (such as liquid cooler vs fan). Please note that if the system was overclocked, including voltage/frequency beyond the processor supported specifications, your processor voids warranty.   **Next steps to try:** • **Check Windows power settings** \- sometimes updates mess with power profiles. Set to ""High Performance"" and see if that helps •   **Look at that 15% CPU spike** \- open Task Manager right after boot and sort by CPU usage to catch what's eating those cycles •   **Try disabling Windows security temporarily** \- sometimes antivirus can cause weird throttling behavior •   **Check Event Viewer** \- look for any error messages around the time throttling starts • **Check Reliability Monitor** \- go to Control Panel > Security and Maintenance > Reliability Monitor to spot any anomaly issues or critical events around August 19-20thThe fact that overclocking fixes it temporarily suggests your CPU is being artificially limited by software, not hardware.   Since you've already tried the obvious stuff like BIOS reset and driver rollbacks, you're probably looking at a Windows service or background process gone rogue. Keep me posted on what you find with that CPU usage spike and reliability monitor - those are likely our smoking guns!",Neutral
AMD,"u/PM_pics_of_your_roof  You're absolutely correct about VROC on X299 being discontinued. Intel moved VROC for X299 platforms into sustaining mode within the past two years and has shifted focus toward Intel RST (Rapid Storage Technology) for consumer applications. While VROC is no longer actively developed, it remains supported in sustaining mode for existing users, which means NVMe drives that were released during VROC's active development period (roughly 2017-2022) should still function properly. However, newer drives may work but won't receive official validation or certification. For anyone building new systems, Intel recommends using RST instead of VROC, but existing X299 users can continue using VROC with supported drives from the qualified vendor list. ***Support is now limited to sustaining mode with no new features or drive certifications planned, as confirmed in the support article you referenced.***",Neutral
AMD,"u/UC20175  Per your inquiry:  The necessary tools and firmware files, but they are only accessible through Intel’s Resource and Design Center (RDC), which requires a Premier account.  1. Register for an Intel RDC Premier Account 2. Visit: Intel RDC Registration Guide [https://www.intel.com/content/www/us/en/support/articles/000058073/programs/resource-and-documentation-center.html](https://www.intel.com/content/www/us/en/support/articles/000058073/programs/resource-and-documentation-center.html) 3. Use a corporate email address for faster approval. 4. From the RDC, search following Content IDs:  EEPROM Access Tool (EAT) – Content ID: 572162  [https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=572162](https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=572162)  Production NVM Images for I210/I211 – Content ID: 513655  [https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=513655](https://www.intel.com/content/www/us/en/secure/design/internal/content-details.html?DocID=513655)",Neutral
AMD,I guess you are the only one who is having trouble on that part. I mean I haven't seen anyone get mad about it. So I don't really seem to find any fault I mean it is their preventive measure for fake processors that are out there. Just ask for some help to take a picture it's not that hard,Negative
AMD,"I think you should revert the Gpu setting to default. Then start changing some settings in the Microsoft setting. There are settings for fonts, brightness, and such. There is also some setting for the smoothness of the animation in the Windows",Neutral
AMD,I suggest getting a replacement if the issue is the same. But make it sure you update your bios first to the latest version and set it to intel default settings.,Neutral
AMD,"u/Tagracat I’m okay to proceed with a replacement if the recommendation has already been tried and you're still experiencing the same issue.  Just let me know if you're good to move forward, and I’ll send you a personal message to help facilitate a possible RMA.",Neutral
AMD,"u/Maleficent_Apple4169 NVMe drivers are typically **chipset/motherboard-based**, not SSD-specific. The driver you need depends on your motherboard's chipset, not the SSD brand.     Here are my recommendation;    Identify Your Motherboard Chipset:  Check motherboard manual/box for chipset info (Z690, B550, X570, etc.)    Universal NVMe Driver Sources:  Windows 10/11 usually has built-in NVMe support  Try Windows installation without additional drivers first  If needed, use Microsoft's generic NVMe drivers you may contact Microsoft for additional guidance.    Motherboard Manufacturer Support:  Visit your motherboard brand's website (ASUS, MSI, Gigabyte, etc.)  Download chipset/storage drivers for your specific board model    Most likely solution, download Intel RST drivers or AMD chipset drivers based on your motherboard's chipset  this will provide the NVMe controller drivers needed for Windows installation.",Neutral
AMD,"You have misunderstood, I am using intel graphics software already as I said, 25.26.1602.2 is the version number of that software. My driver's are up to date according to intel graphics software.",Neutral
AMD,"Hello,    great news, it solved itself. I opened the program this morning and everything was fine. If this happens again, I will try disabling Turbo and see what happens. Thank you for the help!",Positive
AMD,So what is the QVL list for x299 VROC? I can’t find that information. I can find QVL for C621,Neutral
AMD,"So I was able to download the files from the RDC. Currently when I run        eeupdate64e.efi /NIC=2 /DATA Dev_Start_I210_Copper_NOMNG_4Mb_A2.bin  It says ""Only /INVM* and /MAC commands are valid for this adapter. Unsupported."" This is for three 8086-1531 Intel(R) I210 Blank NVM Device. Presumably in order to program them with device id 1533, I need to run a different command or use a different tool?  Thank you for your help, I feel like it is very close to working.  Edit: I believe it works, using /INVMUPDATE",Neutral
AMD,"[This is literally what I am dealing with.](https://imgur.com/a/WBowyjO)  I'm not even worried about posting this image because please tell me how to get a better photo than this with a regular smart phone, and you can barely see there's anything there.",Negative
AMD,"Yes, I've turned off the system animations, and the blinking cursors.  There aren't settings to switch fonts, to replace thin/faint fonts with bold ones. The folks developing Windows like Segoe Ui, maybe they can read it. There are regedit hacks, but I'm not sure how to do them. There is also Winaero Tweaker, which can switch some fonts, but doesn't work on some older apps with unreadable text. There is MacType, which can bolden text without switching fonts, but doesn't work everywhere either.  There are settings to scale fonts, or scale everything, or reduce resolution. I've tried every combination of these. I've reduced resolution, since it does work everywhere. But I can't go below 1280x720, and I end up breaking some interfaces anyway. There is also the magnifier, but it gives me a migraine.  Here's the thing. If text is too thin/faint, making it 2x as bold will take less than 2x the screen space; making it 2x as big will take 4x the screen space.  I'm surprised how well tweaking gamma has worked. So far it has worked everywhere with dark text on light images, and it's worked very well.",Negative
AMD,I'm now on the new BIOS (with Intel default settings) and the crashing is still occurring on Core 8. Alas... I was really hoping that would fix it.  What are the next steps?,Neutral
AMD,"u/Hippieman100 Ah, I see I overlooked that part! Looks like this is the installer version, my apologies for the confusion since we were discussing three different software options earlier.  Alrighty, since you're using the latest version now, how can I help? Are you running into any issues with the new application, or is there a specific feature that's not working as expected?",Neutral
AMD,"u/TheCupaCupa Great to hear it fixed itself! If it happens again, trying Turbo off sounds like a good plan.",Positive
AMD,"u/PM_pics_of_your_roof  You're encountering a common issue, Intel doesn't maintain a centralized QVL (Qualified Vendor List) specifically for X299 VROC at the platform level. Since X299 is a consumer chipset, the VROC compatibility lists were typically maintained by individual motherboard manufacturers rather than Intel directly. The C621 QVL you found is for Intel's server/workstation chipset, which has more formal validation processes. For X299 VROC compatibility, you'll need to check with your specific motherboard manufacturer (ASUS, MSI, Gigabyte, etc.) as they would have maintained their own compatibility lists during VROC's active period. However, since X299 VROC is now in sustaining mode, many manufacturers may no longer actively update these lists. Your best bet is to search for your specific motherboard model's support page or contact the manufacturer directly, though given the discontinued status, this information may be limited or archived.",Neutral
AMD,Relax 😅 and follow this:https://www.intel.com/content/www/us/en/support/articles/000005609/processors.html,Neutral
AMD,"Yeah switching fonts, I haven't tried it. But in windows settings you can change its size and such. Try looking at the Microsoft community or support changing it. But I am glad that adjusting the gamma helps.",Neutral
AMD,u/Tagracat  Kindly follow the article linked below to request a warranty replacement. You can use this Reddit thread as a reference for faster processing.  [How To Submit an Online Warranty Request](https://www.intel.com/content/www/us/en/support/articles/000098501/programs.html),Neutral
AMD,"Thank you for the reply. Sadly asrock doesn’t have a QVL for nvme drives in U.2 form factor or enterprise grade drives, just m.2. Let alone a QVL for VROC support. Looks like I’ll be the test subject for this.",Negative
AMD,"That is what I'm following, I provided support everything on that list, including decoding the matrix on the front of the chip but they don't proceed with the ticket until they get a picture of the code on the pcb which is nigh on impossible.  I wound up taking it into the office to use a high resolution scanner thats used to digitise paintings and just about got something readable. I remain firm in my stance that whilst wanting to verify a serial number isn't outrageous, making the process in which you verify said information virtually impossible is.   I simply don't understand how regular people can take such a detailed photo using smartphone technology. My phone is 4 years old and was absolutely not capable (Samsung S20+)",Negative
AMD,u/PM_pics_of_your_roof  **Best of luck with your configuration!** Your pioneering work might just pave the way for others looking to implement similar setups.,Positive
AMD,"u/Danderlyon  **just sent you a message, check your inbox when you get a chance!**",Neutral
AMD,"Just wanted to follow up for anyone seeing this or is in a similar boat. I can confirm that Intel DC P4510 drives will work in a VROC Raid using a standard key.      Not sure if intel support can confirm but it appears the ASROCK X299 Taichi CLX supports VROC Raid across multiple VMD Domains, which is supposed to be locked behind Xeon scalable CPU/Chipsets. I need to buy two more drives and try to add them to my array to confirm.",Neutral
AMD,"LPCAMM2 and CAMM2 are not the same thing, CAMM2 utilizes the same DDR modules as conventional DIMMs, LPCAMM2 it for laptops and it utilizes LPDDR modules, you don't want LPDDR memory on your desktop, it has awful timings/latency.",Neutral
AMD,">Faster DDR5 DRAM chips are coming, even at high capacities, but existing DIMM standard is a bottleneck due to signal degradation.   Intel is already running MRDIMMs at 8800 MT/s with 12800 MT/s planned on a conventional DIMM socket. The DIMM format can handle the frequency, but bottleneck is the DRAM itself, so MRDIMMs basically RAID two 4400 MT/s memories (and in future 6400 MT/s) on a single DIMM to get around the the limited speed of the DRAM. Signal integrity is evidently not a problem past 10 GT/s, so don't expect CAMM to suddenly unlock more performance from existing DDR5 DRAMs or to make the performance hit from multiple modules dramatically smaller (although it may help somewhat).   >Does that mean it would be prudent to wait a bit with new MoBos purchase ?   IMO no, at least not with DDR5. We will see if DDR6 at >12 GT/s runs into issues with DIMM, but that is still a ways off.    >LP/CAMM2 apparently brings many other benefits, not just frequency bump. It shoudl be compatible with LP/DDR6, allow line-load-reducing, clk regen ( like CUDIMM), MRDIMM data rate doubling (friggin cool!), registering, ECC etcetc   The format allows those memory types, but putting a CAMM with LPDDR into a system that doesn't support LPDDR is not going to work. Similarly to DIMM, the CPU still has to support a feature, so don't expect things like MRDIMMs to suddenly work on consumer hardware.",Neutral
AMD,"We need more speed, further, faster, farther. Where is the easy button for all this?",Negative
AMD,"I'm at Computex right now talking with both motherboard makers and memory makers. At this point, they're not seeing a ton of demand for CAMM2 modules in the enthusiast space, so I don't think this year will be the year for mainstream adoption. I could see adoption picking up for laptops and for full systems (see the ASUS TUF T500 that launched a couple days ago).   I don't have a crystal ball, but I'd almost think the major transition will happen with DDR6 - at this point, if you have DDR5 in a DIMM already and are upgrading your rig while reusing the same DDR5... why would you re-buy it as CAMM2?",Neutral
AMD,">you don't want LPDDR memory on your desktop, it has awful timings/latency.  Ryzen AI Max uses LPDDR5 and achieves 256GB/s memory bandwidth vs 100GB/s at most for 2-channel DDR5 on regular PC.  Apple M4 is the same. M4 Pro is 400GB/s and M4 Max is 540GB/s. Ultra is 800GB/s reaching recent GPU speed.",Negative
AMD,"Yeah, this is unfortunately the big problem.. Convincing mobo makers to switch.  I think it will have to start with Laptops right? And it'll have to come from consumers not wanting normal SODIMMs or soldered RAM.. So maybe a few years yet :( .",Negative
AMD,"Yeah and they still have awful memory timings, you are confusing memory channels and bandwidth with memory latency.",Negative
AMD,"Latency kills any graphics application that need to display at 60fps, 144fps or more, one blip and frames drop.",Negative
AMD,"Guess, I'll stay on bios v1701 on my Strix Z890-H then. I have already set my cache ratio to 40 and the NGU and D2D to 32...  My E-cores are set to 48 and my RAM is running at 8200MT/s in gear 2 with the XMP Tweaked profile.  Haven't touched any voltage settings and my system is stable. I have no idea if this can be improved but I'm quite happy with the performance at the moment.",Neutral
AMD,How to retain my previous undervolting settings along with this new S200 Boost mode? I have an ASRock Z890 Riptide WiFi board and a Core Ultra 9 285K.,Neutral
AMD,I'm having issues since I enabled S200 boost mode. I can't seem to get my RAM to run stable without xmp. If I run both RAM settings and boost mode I hard crash at random. What RAM settings do you guys use when using boost mode?,Negative
AMD,"Yes for xtu if 200s is off, but once 200s boost is active, xtu doesn't let you do anything. No undervolting, no multiplier. Even if vmd and undervolt protection is off, xtu wom't let you. 200s boost is useless, just use performance intel bios setting, xpm profile and undervolt in xtu amd get a bigger boost than 200s boost.",Negative
AMD,"Yeah, 200s boost is trash. I had better performance with an undervolting 0 OC, than no possible undervolting with XTU. The 265k sees no benifit from this. I disabled it, undervolted, and added 53x and 47x and have better perf.",Negative
AMD,"Sigh, I reverted to old bios only allow me to use xtu.",Neutral
AMD,"200s boost is basically Intel sanctioned overclocking that does not void your warranty. Ive enabled the feature, and I'm pretty happy with how much my ram latency dropped (9.6ns) and how I'm also getting improved 1% lows as a result.   You are free to disable it, over clock and tune until your heart's content. But calling it useless is a bit of a stretch.",Negative
AMD,Are you able to undervolt using xtu even 200s boost is disabled?,Neutral
AMD,"Rumors and alleged hardware id captures suggest that yes, more discrete GPU models are coming.  It appears that Intel has cancelled the higher end Battlemage Xe2 GPUs though, which considering the performance of the B580, it's a shame if we don't get to see big Battlemage.      But Celestial is allegedly part of the tile set for the upcoming Panther Lake chips as Xe3 cores, and Druid (Xe4) is supposedly pretty far along in production if not close to being finished.     But all of that is just rumors at this point.  Who the heck knows what will happen?  The company got a new CEO and it looks like he's trying to make Intel slim down a bit.",Neutral
AMD,I'm not sure Intel itself knows that all things considered,Neutral
AMD,"dgpus/igpus are a marketable side effect of producing server farm gpus, so probably.",Neutral
AMD,Even most people at Intel probably don’t know at this point.  Intel’s new CEO did say that Intel will get rid of its “non-core business” to focus on its “core business” and “expand that using AI and Software 2.0”.,Neutral
AMD,"Intel's [got job listings for the Arc division](https://www.pcguide.com/news/intel-isnt-giving-up-on-discrete-gpus-as-new-job-listings-reveal-the-company-is-shooting-for-much-much-higher/) and have stated they're ""shooting for much much higher"" so presumably we're at least going to get Celestial and Druid. If they both flop then maybe Arc'll get canned but otherwise I'm fairly certain Arc's here to stay.",Neutral
AMD,* I see similarities between the engineering philosophy of Intel GPU's to AMD  Bro can share me some of your weed ?,Neutral
AMD,Given that Tan has said that he believes in you need to ship products to learn from them.  I don't see him canceling many chip programs that have future product development needs.  Intel needs ship products to get people into their eco system and GPU development is needed for AI chip development.  Either way I would not expect much rumors until closer to Thursday and Intel announces earnings.,Neutral
AMD,"In the land of HPC, AI workloads, GPU is king. If they stop making GPU then they stop being relevant. They already goofed up big time by not making GPU last two decades. If they deprioritise dGPU now then they will never be able to catch up.",Negative
AMD,"I do hope Intel continues.  Intel HAS to develop shaders and tensors for iGPU and NPUs anyway. Might as well get more out of it by releasing dGPU cards as well.  Nvidia completely abandoned the low end of the market. It's likely a low margin product, but with high volumes. The market used to be mostly sub 300 € GPUs.  As for mid tier and high tier card, Nvidia sells their VRAM like it's made of money, and AMD is hopelessly behind in software. Words can't describe how bad AMD is at accelerating anything.   Give how quickly Intel figured out driver, I am more hopeful Intel can figure out an acceleration stack to make use of their NPU units.",Neutral
AMD,"My best is they'll slim down the division and not release discrete cards. They need to keep the r and d up to make their igpu competitive.   Intel need to save money first and foremost, so I would think layoffs are coming fairly soon",Neutral
AMD,"I hope they keep trying. While not perfect (of course)  , they had good showing with ""only"" 2 gens of products. But they really need to fix gpu usage, which seems kind of low right now. They would need to fix this before launching high end gpus, because right now going larger doesn't seem to scale very well for their arch.",Positive
AMD,"Do they even produce B570/580?   Not even available, or for a stupid price.",Negative
AMD,"> But Celestial is allegedly part of the tile set for the upcoming Panther Lake chips as Xe3 cores  PTL has an Xe3 iGPU. Celestial is the name for a (once) future dGPU line. Xe3-derived dGPUs are dead, however. The last shred of hope would be something Xe4-based years down the line.",Neutral
AMD,And it has also allowed intel to make proper integrated graphics on the laptop side,Positive
AMD,"Clearly,  As we know, network administrators pass their time by playing video games on server farms they manage.",Neutral
AMD,"He mostly means, Mobileye etc. Not GPU's... Though that might not also make the cut.  I think new CEO is more likely to split the foundry than to cut GPU's given their importance in AI. I don't think either of these is going to happen.",Negative
AMD,"Yeah, but GPUs and WiFi chips are both part of the intel package, even if they dropped discrete gpus they still need to develop igpus and ai accelerators, so it's probably not actually that big of a cost",Neutral
AMD,"Yep, GPUs were Pat’s project, most likely gone with the new guy’s focus on essentials.",Neutral
AMD,"> AMD is hopelessly behind in software  If Nvidia drivers are anything to go by, they may be trying to catch up to AMD's Glory days.",Negative
AMD,"How can they save money if they have to develop the software parts all by themselves ? They can't, and that's why they need as much help from the open-source community as possible. That means putting GPUs into their hands.",Neutral
AMD,“Hardware Unboxed said the same.” 🤣,Neutral
AMD,Here’s to hoping that the overhead issue gets fixed soon so that we could have better Intel gpu’s,Positive
AMD,"Or much simpler explanation. It didn't scale well to make economic sense?  If anything overhead issue is less of an issue on a premium higher performance cards, as people are more likely to overpay for their cpu in higher price ranges.",Negative
AMD,"Overhead issue is when we use older cpus like 9400 or 3600  If intel is to make high end GPU , it would be obviously bought buy those having good enough cpu as well like 5700x or 12600k  It would be stupid imo to buy a high end GPU (whether it's amd .intel or nvidia ) with older CPUs  The real reason could be potential b770 wasn't giving satisfactory performance uplift to be considered as mid-high end GPU that's why they cancelled b770 and now looking forward c770 i.e celestial",Negative
AMD,"I have not seen anything that indicates that it's cancelled except -more often wrong than right- Moore's law is dead.  In fact Intel product head, said that they would continue with discrete in this January. Though didn't specify if they will ever go for a higher end card.",Negative
AMD,"I can't find any good info saying Celestial cards with Xe3/Xe3p are cancelled or dead.  Peterson himself said the work for Celestial dGPUs is finished, and they have been spotted in data pulls on Linux.  I don't believe either one of those things mean there will never be a Xe3/Xe3p dGPU.",Negative
AMD,stop watching Moore's law is dead and stating his videos as facts,Neutral
AMD,"He is referring to any money-losing side project.  To survive, Intel needs to tighten its belt to conserve money, just as AMD did between 2011 and 2017.  There is no way that Intel would abandon GPU development, as ML/AI is a very lucrative market, but most users here are talking specifically about gaming GPUs.",Neutral
AMD,"There will still be Intel GPUs for ML/Al (because that’s where the money is), but I don’t think that that is what most users here are talking about.",Neutral
AMD,"> Or much simpler explanation. It didn't scale well to make economic sense?  That's pretty much the entire Arc lineup  >If anything overhead issue is less of an issue on a premium higher performance cards, as people are more likely to overpay for their cpu in higher price ranges.  Imagine that it makes twice as many draw calls.  Even the Ryzen 7 9800X3D is going to be overloaded.",Negative
AMD,Imagine that it makes twice as many draw calls.  Even the Ryzen 7 9800X3D is going to be overloaded.,Negative
AMD,"> I have not seen anything that indicates that it's cancelled except -more often wrong than right- Moore's law is dead.  That would be a broken clock right twice a day kind of situation. Gelsinger cancelled Celestial a few months before he was fired.   > In fact Intel product head, said that they would continue with discrete in this January  The wording was ""continued investment"", which means precisely nothing at all. A single future driver update for BMG would count.",Negative
AMD,"> Peterson himself said the work for Celestial dGPUs is finished  No, he said nothing at all about dGPUs. He didn't even name Celestial at all. His comments were that Xe3 was essentially finished, because that's the PTL iGPU, and we know that's more or less done from a hardware standpoint.",Neutral
AMD,Who ever said I got that from him?,Negative
AMD,"Not to mention, nVidia is also coming to eat Intel's lunch, with an APU of its own (and surely there'll be consumer versions too).   It would be absolutely suicidal for Intel to get rid of its GPU business unit.",Negative
AMD,"You don't know what you're talking about, you never did, I always read your posts and you act as if you know what you're talking about.  Intel's job hiring proves you're just a clueless redditor making things up https://www.pcguide.com/news/intel-isnt-giving-up-on-discrete-gpus-as-new-job-listings-reveal-the-company-is-shooting-for-much-much-higher/",Negative
AMD,"Fair enough, but that still does not make the point that there will be no Celestial dGPU release.  If xe3 is all the way done and allegedly popping up on high performance test strings on Linux, it would follow that sometime soon we will see Celestial in the graphics card market.     Again, there seems to be no evidence that Celestial is cancelled on dGPUs.  The only evidence found is that the uArch is developed and will, in tile form, be an igpu.",Neutral
AMD,"Lmao, people continue to be in denial about the things everyone else everyone knows. Tell me, is it still ""FUD"" that Intel's using N3 for ARL? Or that BMG is 4060-tier?  Btw, you can't even find that job link.",Negative
AMD,"No *public* evidence, at least. Intel has been cagey on this for a reason. Would simplify things if they just admit what their roadmap (or lack thereof) is.",Negative
AMD,"Notice how you started to move the goal posts, you can literally find the job postings yourself with a simple Google search.  Also, maybe you should do something else in life than sitting on reddit for over a decade spewing BS.",Negative
AMD,"Yeah if they layed out a real honest roadmap and stuck with it, it would be better for everyone.",Positive
AMD,I’m running 1703 with the Z890 EXTREME. Zero issues.,Negative
AMD,Release date for 200s boost bios?,Neutral
AMD,"I don't usually get the board list/release notes until Friday. However, roll out starts during that week, so you can start checking Tuesday - Thursday, but I don't have a better date for you at this time.",Neutral
AMD,it's available if you have an apex:  https://dlcdnets.asus.com/pub/ASUS/mb/BIOS/ROG-MAXIMUS-Z890-APEX-ASUS-1801.ZIP  probably soon otherwise tho,Neutral
AMD,Thank you but actually i am looking for Z890 TUF-PLUS.,Positive
AMD,Intel 200S boost available on version 2001  [https://dlcdnets.asus.com/pub/ASUS/mb/BIOS/TUF-GAMING-Z890-PLUS-WIFI-ASUS-2001.ZIP?model=TUF%20GAMING%20Z890-PLUS%20WIFI](https://dlcdnets.asus.com/pub/ASUS/mb/BIOS/TUF-GAMING-Z890-PLUS-WIFI-ASUS-2001.ZIP?model=TUF%20GAMING%20Z890-PLUS%20WIFI),Neutral
AMD,"One thing also to note is that because of the way our website syncs, sometimes you might find a newer BIOS on a different regional website (although this doesn't happen often). So, if you see someone say they see it, and you don't, try to find out which regional website they saw the BIOS update listed.",Neutral
AMD,b860 tiene soporte para 200s boost?,Neutral
